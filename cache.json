{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Sparks: Inspiration for Science Writing using Language Models. (arXiv:2110.07640v1 [cs.HC])","link":"http://arxiv.org/abs/2110.07640","description":"<p>Large-scale language models are rapidly improving, performing well on a wide\nvariety of tasks with little to no customization. In this work we investigate\nhow language models can support science writing, a challenging writing task\nthat is both open-ended and highly constrained. We present a system for\ngenerating \"sparks\", sentences related to a scientific concept intended to\ninspire writers. We find that our sparks are more coherent and diverse than a\ncompetitive language model baseline, and approach a human-created gold\nstandard. In a study with 13 PhD students writing on topics of their own\nselection, we find three main use cases of sparks: aiding with crafting\ndetailed sentences, providing interesting angles to engage readers, and\ndemonstrating common reader perspectives. We also report on the various reasons\nsparks were considered unhelpful, and discuss how we might improve language\nmodels as writing support tools.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gero_K/0/1/0/all/0/1\">Katy Ilonka Gero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1\">Vivian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chilton_L/0/1/0/all/0/1\">Lydia B. Chilton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GlobalWoZ: Globalizing MultiWoZ to Develop Multilingual Task-Oriented Dialogue Systems. (arXiv:2110.07679v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07679","description":"<p>Much recent progress in task-oriented dialogue (ToD) systems has been driven\nby available annotation data across multiple domains for training. Over the\nlast few years, there has been a move towards data curation for multilingual\nToD systems that are applicable to serve people speaking different languages.\nHowever, existing multilingual ToD datasets either have a limited coverage of\nlanguages due to the high cost of data curation, or ignore the fact that\ndialogue entities barely exist in countries speaking these languages. To tackle\nthese limitations, we introduce a novel data curation method that generates\nGlobalWoZ -- a large-scale multilingual ToD dataset globalized from an English\nToD dataset for three unexplored use cases. Our method is based on translating\ndialogue templates and filling them with local entities in the target-language\ncountries. We release our dataset as well as a set of strong baselines to\nencourage research on learning multilingual ToD systems for real use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bosheng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljunied_S/0/1/0/all/0/1\">Sharifah Mahani Aljunied</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Scale Substitution-based Word Sense Induction. (arXiv:2110.07681v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07681","description":"<p>We present a word-sense induction method based on pre-trained masked language\nmodels (MLMs), which can cheaply scale to large vocabularies and large corpora.\nThe result is a corpus which is sense-tagged according to a corpus-derived\nsense inventory and where each sense is associated with indicative words.\nEvaluation on English Wikipedia that was sense-tagged using our method shows\nthat both the induced senses, and the per-instance sense assignment, are of\nhigh quality even compared to WSD methods, such as Babelfy. Furthermore, by\ntraining a static word embeddings algorithm on the sense-tagged corpus, we\nobtain high-quality static senseful embeddings. These outperform existing\nsenseful embeddings techniques on the WiC dataset and on a new outlier\ndetection dataset we developed. The data driven nature of the algorithm allows\nto induce corpora-specific senses, which may not appear in standard sense\ninventories, as we demonstrate using a case study on the scientific domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eyal_M/0/1/0/all/0/1\">Matan Eyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadde_S/0/1/0/all/0/1\">Shoval Sadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taub_Tabib_H/0/1/0/all/0/1\">Hillel Taub-Tabib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Document-Level Information Extraction Right for the Right Reasons. (arXiv:2110.07686v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07686","description":"<p>Document-level information extraction is a flexible framework compatible with\napplications where information is not necessarily localized in a single\nsentence. For example, key features of a diagnosis in radiology a report may\nnot be explicitly stated, but nevertheless can be inferred from the report's\ntext. However, document-level neural models can easily learn spurious\ncorrelations from irrelevant information. This work studies how to ensure that\nthese models make correct inferences from complex text and make those\ninferences in an auditable way: beyond just being right, are these models\n\"right for the right reasons?\" We experiment with post-hoc evidence extraction\nin a predict-select-verify framework using feature attribution techniques.\nWhile this basic approach can extract reasonable evidence, it can be\nregularized with small amounts of evidence supervision during training, which\nsubstantially improves the quality of extracted evidence. We evaluate on two\ndomains: a small-scale labeled dataset of brain MRI reports and a large-scale\nmodified version of DocRED (Yao et al., 2019) and show that models'\nplausibility can be improved with no loss in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_D/0/1/0/all/0/1\">Dhruv Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Suyash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_A/0/1/0/all/0/1\">Abhijeet Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_R/0/1/0/all/0/1\">R. Nick Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Stance Detection Topic-Independent and Cross-topic Generalizable? -- A Reproduction Study. (arXiv:2110.07693v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07693","description":"<p>Cross-topic stance detection is the task to automatically detect stances\n(pro, against, or neutral) on unseen topics. We successfully reproduce\nstate-of-the-art cross-topic stance detection work (Reimers et. al., 2019), and\nsystematically analyze its reproducibility. Our attention then turns to the\ncross-topic aspect of this work, and the specificity of topics in terms of\nvocabulary and socio-cultural context. We ask: To what extent is stance\ndetection topic-independent and generalizable across topics? We compare the\nmodel's performance on various unseen topics, and find topic (e.g. abortion,\ncloning), class (e.g. pro, con), and their interaction affecting the model's\nperformance. We conclude that investigating performance on different topics,\nand addressing topic-specific vocabulary and context, is a future avenue for\ncross-topic stance detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reuver_M/0/1/0/all/0/1\">Myrthe Reuver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morante_R/0/1/0/all/0/1\">Roser Morante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokkens_A/0/1/0/all/0/1\">Antske Fokkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CCQA: A New Web-Scale Question Answering Dataset for Model Pre-Training. (arXiv:2110.07731v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07731","description":"<p>With the rise of large-scale pre-trained language models, open-domain\nquestion-answering (ODQA) has become an important research topic in NLP. Based\non the popular pre-training fine-tuning approach, we posit that an additional\nin-domain pre-training stage using a large-scale, natural, and diverse\nquestion-answering (QA) dataset can be beneficial for ODQA. Consequently, we\npropose a novel QA dataset based on the Common Crawl project in this paper.\nUsing the readily available schema.org annotation, we extract around 130\nmillion multilingual question-answer pairs, including about 60 million English\ndata-points. With this previously unseen number of natural QA pairs, we\npre-train popular language models to show the potential of large-scale\nin-domain pre-training for the task of question-answering. In our experiments,\nwe find that pre-training question-answering models on our Common Crawl\nQuestion Answering dataset (CCQA) achieves promising results in zero-shot, low\nresource and fine-tuned settings across multiple tasks, models and benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1\">Patrick Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aghajanyan_A/0/1/0/all/0/1\">Armen Aghajanyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Sonal Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Mitigating Spurious Correlations for Improving Robustness in NLP Models. (arXiv:2110.07736v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07736","description":"<p>Recently, NLP models have achieved remarkable progress across a variety of\ntasks; however, they have also been criticized for being not robust. Many\nrobustness problems can be attributed to models exploiting spurious\ncorrelations, or shortcuts between the training data and the task labels.\nModels may fail to generalize to out-of-distribution data or be vulnerable to\nadversarial attacks if spurious correlations are exploited through the training\nprocess. In this paper, we aim to automatically identify such spurious\ncorrelations in NLP models at scale. We first leverage existing\ninterpretability methods to extract tokens that significantly affect model's\ndecision process from the input text. We then distinguish \"genuine\" tokens and\n\"spurious\" tokens by analyzing model predictions across multiple corpora and\nfurther verify them through knowledge-aware perturbations. We show that our\nproposed method can effectively and efficiently identify a scalable set of\n\"shortcuts\", and mitigating these leads to more robust models in multiple\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianlu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hindsight: Posterior-guided training of retrievers for improved open-ended generation. (arXiv:2110.07752v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07752","description":"<p>Many text generation systems benefit from using a retriever to retrieve\npassages from a textual knowledge corpus (e.g., Wikipedia) which are then\nprovided as additional context to the generator. For open-ended generation\ntasks (like generating informative utterances in conversations) many varied\npassages may be equally relevant and we find that existing methods that jointly\ntrain the retriever and generator underperform: the retriever may not find\nrelevant passages even amongst the top-10 and hence the generator may not learn\na preference to ground its generated output in them. We propose using an\nadditional guide retriever that is allowed to use the target output and \"in\nhindsight\" retrieve relevant passages during training. We model the guide\nretriever after the posterior distribution Q of passages given the input and\nthe target output and train it jointly with the standard retriever and the\ngenerator by maximizing the evidence lower bound (ELBo) in expectation over Q.\nFor informative conversations from the Wizard of Wikipedia dataset, with\nposterior-guided training, the retriever finds passages with higher relevance\nin the top-10 (23% relative improvement), the generator's responses are more\ngrounded in the retrieved passage (19% relative improvement) and the end-to-end\nsystem produces better overall output (6.4% relative improvement).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paranjape_A/0/1/0/all/0/1\">Ashwin Paranjape</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1\">Omar Khattab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1\">Christopher Potts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1\">Matei Zaharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multilingual Bag-of-Entities Model for Zero-Shot Cross-Lingual Text Classification. (arXiv:2110.07792v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07792","description":"<p>We present a multilingual bag-of-entities model that effectively boosts the\nperformance of zero-shot cross-lingual text classification by extending a\nmultilingual pre-trained language model (e.g., M-BERT). It leverages the\nmultilingual nature of Wikidata: entities in multiple languages representing\nthe same concept are defined with a unique identifier. This enables entities\ndescribed in multiple languages to be represented using shared embeddings. A\nmodel trained on entity features in a resource-rich language can thus be\ndirectly applied to other languages. Our experimental results on cross-lingual\ntopic classification (using the MLDoc and TED-CLDC datasets) and entity typing\n(using the SHINRA2020-ML dataset) show that the proposed model consistently\noutperforms state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishikawa_S/0/1/0/all/0/1\">Sosuke Nishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1\">Isao Echizen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContraQA: Question Answering under Contradicting Contexts. (arXiv:2110.07803v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07803","description":"<p>With a rise in false, inaccurate, and misleading information in propaganda,\nnews, and social media, real-world Question Answering (QA) systems face the\nchallenges of synthesizing and reasoning over contradicting information to\nderive correct answers. This urgency gives rise to the need to make QA systems\nrobust to misinformation, a topic previously unexplored. We study the risk of\nmisinformation to QA models by investigating the behavior of the QA model under\ncontradicting contexts that are mixed with both real and fake information. We\ncreate the first large-scale dataset for this problem, namely Contra-QA, which\ncontains over 10K human-written and model-generated contradicting pairs of\ncontexts. Experiments show that QA models are vulnerable under contradicting\ncontexts brought by misinformation. To defend against such a threat, we build a\nmisinformation-aware QA system as a counter-measure that integrates question\nanswering and misinformation detection in a joint fashion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alternative Input Signals Ease Transfer in Multilingual Machine Translation. (arXiv:2110.07804v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07804","description":"<p>Recent work in multilingual machine translation (MMT) has focused on the\npotential of positive transfer between languages, particularly cases where\nhigher-resourced languages can benefit lower-resourced ones. While training an\nMMT model, the supervision signals learned from one language pair can be\ntransferred to the other via the tokens shared by multiple source languages.\nHowever, the transfer is inhibited when the token overlap among source\nlanguages is small, which manifests naturally when languages use different\nwriting systems. In this paper, we tackle inhibited transfer by augmenting the\ntraining data with alternative signals that unify different writing systems,\nsuch as phonetic, romanized, and transliterated input. We test these signals on\nIndic and Turkic languages, two language families where the writing systems\ndiffer but languages still share common features. Our results indicate that a\nstraightforward multi-source self-ensemble -- training a model on a mixture of\nvarious signals and ensembling the outputs of the same model fed with different\nsignals during inference, outperforms strong ensemble baselines by 1.3 BLEU\npoints on both language families. Further, we find that incorporating\nalternative inputs via self-ensemble can be particularly effective when\ntraining set is small, leading to +5 BLEU when only 5% of the total training\ndata is accessible. Finally, our analysis demonstrates that including\nalternative signals yields more consistency and translates named entities more\naccurately, which is crucial for increased factuality of automated systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Simeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1\">Chau Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Fast and Slow Models for Efficient Semantic Code Search. (arXiv:2110.07811v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07811","description":"<p>The goal of natural language semantic code search is to retrieve a\nsemantically relevant code snippet from a fixed set of candidates using a\nnatural language query. Existing approaches are neither effective nor efficient\nenough towards a practical semantic code search system. In this paper, we\npropose an efficient and accurate semantic code search framework with cascaded\nfast and slow models, in which a fast transformer encoder model is learned to\noptimize a scalable index for fast retrieval followed by learning a slow\nclassification-based re-ranking model to improve the performance of the top K\nresults from the fast retrieval. To further reduce the high memory cost of\ndeploying two separate models in practice, we propose to jointly train the fast\nand slow model based on a single transformer encoder with shared parameters.\nThe proposed cascaded approach is not only efficient and scalable, but also\nachieves state-of-the-art results with an average mean reciprocal ranking (MRR)\nscore of 0.7795 (across 6 programming languages) as opposed to the previous\nstate-of-the-art result of 0.713 MRR on the CodeSearchNet benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-learning via Language Model In-context Tuning. (arXiv:2110.07814v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07814","description":"<p>The goal of meta-learning is to learn to adapt to a new task with only a few\nlabeled examples. To tackle this problem in NLP, we propose $\\textit{in-context\ntuning}$, which recasts adaptation and prediction as a simple sequence\nprediction problem: to form the input sequence, we concatenate the task\ninstruction, the labeled examples, and the target input to predict; to\nmeta-train the model to learn from in-context examples, we fine-tune a\npre-trained language model (LM) to predict the target label from the input\nsequences on a collection of tasks.\n</p>\n<p>We benchmark our method on two collections of text classification tasks: LAMA\nand BinaryClfs. Compared to first-order MAML which adapts the model with\ngradient descent, our method better leverages the inductive bias of LMs to\nperform pattern matching, and outperforms MAML by an absolute $6\\%$ AUC ROC\nscore on BinaryClfs, with increasing advantage w.r.t. model size. Compared to\nnon-fine-tuned in-context learning (i.e. prompting a raw LM), in-context tuning\ndirectly learns to learn from in-context examples. On BinaryClfs, in-context\ntuning improves the average AUC-ROC score by an absolute $10\\%$, and reduces\nthe variance with respect to example ordering by 6x and example choices by 2x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sheng Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Neural Machine Translation:Can Linguistic Hierarchies Help?. (arXiv:2110.07816v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07816","description":"<p>Multilingual Neural Machine Translation (MNMT) trains a single NMT model that\nsupports translation between multiple languages, rather than training separate\nmodels for different languages. Learning a single model can enhance the\nlow-resource translation by leveraging data from multiple languages. However,\nthe performance of an MNMT model is highly dependent on the type of languages\nused in training, as transferring knowledge from a diverse set of languages\ndegrades the translation performance due to negative transfer. In this paper,\nwe propose a Hierarchical Knowledge Distillation (HKD) approach for MNMT which\ncapitalises on language groups generated according to typological features and\nphylogeny of languages to overcome the issue of negative transfer. HKD\ngenerates a set of multilingual teacher-assistant models via a selective\nknowledge distillation mechanism based on the language groups, and then distils\nthe ultimate multilingual model from those assistants in an adaptive way.\nExperimental results derived from the TED dataset with 53 languages demonstrate\nthe effectiveness of our approach in avoiding the negative transfer effect in\nMNMT, leading to an improved translation performance (about 1 BLEU score on\naverage) compared to strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_F/0/1/0/all/0/1\">Fahimeh Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1\">Wray Buntine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Lan Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DirectQuote: A Dataset for Direct Quotation Extraction and Attribution in News Articles. (arXiv:2110.07827v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07827","description":"<p>Quotation extraction and attribution are challenging tasks, aiming at\ndetermining the spans containing quotations and attributing each quotation to\nthe original speaker. Applying this task to news data is highly related to\nfact-checking, media monitoring and news tracking. Direct quotations are more\ntraceable and informative, and therefore of great significance among different\ntypes of quotations. Therefore, this paper introduces DirectQuote, a corpus\ncontaining 19,760 paragraphs and 10,279 direct quotations manually annotated\nfrom online news media. To the best of our knowledge, this is the largest and\nmost complete corpus that focuses on direct quotations in news texts. We ensure\nthat each speaker in the annotation can be linked to a specific named entity on\nWikidata, benefiting various downstream tasks. In addition, for the first time,\nwe propose several sequence labeling models as baseline methods to extract and\nattribute quotations simultaneously in an end-to-end manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanchi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models. (arXiv:2110.07831v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07831","description":"<p>Backdoor attacks, which maliciously control a well-trained model's outputs of\nthe instances with specific triggers, are recently shown to be serious threats\nto the safety of reusing deep neural networks (DNNs). In this work, we propose\nan efficient online defense mechanism based on robustness-aware perturbations.\nSpecifically, by analyzing the backdoor training process, we point out that\nthere exists a big gap of robustness between poisoned and clean samples.\nMotivated by this observation, we construct a word-based robustness-aware\nperturbation to distinguish poisoned samples from clean samples to defend\nagainst the backdoor attacks on natural language processing (NLP) models.\nMoreover, we give a theoretical analysis about the feasibility of our\nrobustness-aware perturbation-based defense method. Experimental results on\nsentiment analysis and toxic detection tasks show that our method achieves\nbetter defending performance and much lower computational costs than existing\nonline defense methods. Our code is available at\nhttps://github.com/lancopku/RAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span Detection for Aspect-Based Sentiment Analysis in Vietnamese. (arXiv:2110.07833v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07833","description":"<p>Aspect-based sentiment analysis plays an essential role in natural language\nprocessing and artificial intelligence. Recently, researchers only focused on\naspect detection and sentiment classification but ignoring the sub-task of\ndetecting user opinion span, which has enormous potential in practical\napplications. In this paper, we present a new Vietnamese dataset (UIT-ViSD4SA)\nconsisting of 35,396 human-annotated spans on 11,122 feedback comments for\nevaluating the span detection in aspect-based sentiment analysis. Besides, we\nalso propose a novel system using Bidirectional Long Short-Term Memory (BiLSTM)\nwith a Conditional Random Field (CRF) layer (BiLSTM-CRF) for the span detection\ntask in Vietnamese aspect-based sentiment analysis. The best result is a 62.76%\nF1 score (macro) for span detection using BiLSTM-CRF with embedding fusion of\nsyllable embedding, character embedding, and contextual embedding from\nXLM-RoBERTa. In future work, span detection will be extended in many NLP tasks\nsuch as constructive detection, emotion recognition, complaint analysis, and\nopinion mining. Our dataset is freely available at\nhttps://github.com/kimkim00/UIT-ViSD4SA for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kim Thi-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_S/0/1/0/all/0/1\">Sieu Khai Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Luong Luc Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_P/0/1/0/all/0/1\">Phuc Huynh Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc-Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Fine-Grained Entity Typing. (arXiv:2110.07837v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07837","description":"<p>The growth of cross-lingual pre-trained models has enabled NLP tools to\nrapidly generalize to new languages. While these models have been applied to\ntasks involving entities, their ability to explicitly predict typological\nfeatures of these entities across languages has not been established. In this\npaper, we present a unified cross-lingual fine-grained entity typing model\ncapable of handling over 100 languages and analyze this model's ability to\ngeneralize to languages and entities unseen during training. We train this\nmodel on cross-lingual training data collected from Wikipedia hyperlinks in\nmultiple languages (training languages). During inference, our model takes an\nentity mention and context in a particular language (test language, possibly\nnot in the training languages) and predicts fine-grained types for that entity.\nGeneralizing to new languages and unseen entities are the fundamental\nchallenges of this entity typing setup, so we focus our evaluation on these\nsettings and compare against simple yet powerful string match baselines.\nExperimental results show that our approach outperforms the baselines on unseen\nlanguages such as Japanese, Tamil, Arabic, Serbian, and Persian. In addition,\nour approach substantially improves performance on unseen entities (even in\nunseen languages) over the baselines, and human evaluation shows a strong\nability to predict relevant types in these settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Selvaraj_N/0/1/0/all/0/1\">Nila Selvaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1\">Yasumasa Onoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESPnet2-TTS: Extending the Edge of TTS Research. (arXiv:2110.07840v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07840","description":"<p>This paper describes ESPnet2-TTS, an end-to-end text-to-speech (E2E-TTS)\ntoolkit. ESPnet2-TTS extends our earlier version, ESPnet-TTS, by adding many\nnew features, including: on-the-fly flexible pre-processing, joint training\nwith neural vocoders, and state-of-the-art TTS models with extensions like\nfull-band E2E text-to-waveform modeling, which simplify the training pipeline\nand further enhance TTS performance. The unified design of our recipes enables\nusers to quickly reproduce state-of-the-art E2E-TTS results. We also provide\nmany pre-trained models in a unified Python interface for inference, offering a\nquick means for users to generate baseline samples and build demos.\nExperimental evaluations with English and Japanese corpora demonstrate that our\nprovided models synthesize utterances comparable to ground-truth ones,\nachieving state-of-the-art TTS performance. The toolkit is available online at\nhttps://github.com/espnet/espnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_R/0/1/0/all/0/1\">Ryuichi Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshimura_T/0/1/0/all/0/1\">Takenori Yoshimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peter Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeki_T/0/1/0/all/0/1\">Takaaki Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yooncheol Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasuda_Y/0/1/0/all/0/1\">Yusuke Yasuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamichi_S/0/1/0/all/0/1\">Shinnosuke Takamichi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Endorsement for Multi-Document Abstractive Summarization. (arXiv:2110.07844v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07844","description":"<p>A crucial difference between single- and multi-document summarization is how\nsalient content manifests itself in the document(s). While such content may\nappear at the beginning of a single document, essential information is\nfrequently reiterated in a set of documents related to a particular topic,\nresulting in an endorsement effect that increases information salience. In this\npaper, we model the cross-document endorsement effect and its utilization in\nmultiple document summarization. Our method generates a synopsis from each\ndocument, which serves as an endorser to identify salient content from other\ndocuments. Strongly endorsed text segments are used to enrich a neural\nencoder-decoder model to consolidate them into an abstractive summary. The\nmethod has a great potential to learn from fewer examples to identify salient\ncontent, which alleviates the need for costly retraining when the set of\ndocuments is dynamically adjusted. Through extensive experiments on benchmark\nmulti-document summarization datasets, we demonstrate the effectiveness of our\nproposed method over strong published baselines. Finally, we shed light on\nfuture research directions and discuss broader challenges of this task using a\ncase study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lebanoff_L/0/1/0/all/0/1\">Logan Lebanoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Segmentation-based News Summarization. (arXiv:2110.07850v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07850","description":"<p>In this paper, we bring a new way of digesting news content by introducing\nthe task of segmenting a news article into multiple sections and generating the\ncorresponding summary to each section. We make two contributions towards this\nnew task. First, we create and make available a dataset, SegNews, consisting of\n27k news articles with sections and aligned heading-style section summaries.\nSecond, we propose a novel segmentation-based language generation model adapted\nfrom pre-trained language models that can jointly segment a document and\nproduce the summary for each section. Experimental results on SegNews\ndemonstrate that our model can outperform several state-of-the-art\nsequence-to-sequence generation models for this new task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07855","description":"<p>Abstract Meaning Representation (AMR) parsing translates sentences to the\nsemantic representation with a hierarchical structure, which is recently\nempowered by pretrained encoder-decoder models. However, the flat\nsentence-to-AMR training paradigm impedes the representation learning of\nconcepts and relations in the deeper AMR sub-graph. To make the\nsequence-to-sequence models better adapt to the inherent AMR structure, we\npropose a hierarchical curriculum learning (HCL) which consists of (1)\nstructure-level curriculum (SC) and (2) instance-level curriculum (IC). SC\nswitches progressively from shallow to deep AMR sub-graphs while IC transits\nfrom easy to hard AMR instances during training. Extensive experiments show\nthat BART trained with HCL achieves the state-of-the-art performance on the\nAMR-2.0 and AMR-3.0 benchmark, and significantly outperforms baselines on the\nstructure-dependent evaluation metrics and hard instances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Low-dimensional Intrinsic Task Subspace via Prompt Tuning. (arXiv:2110.07867v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07867","description":"<p>How can pre-trained language models (PLMs) learn universal representations\nand effectively adapt to broad NLP tasks differing a lot superficially? In this\nwork, we empirically find evidences indicating that the adaptations of PLMs to\nvarious tasks can be reparameterized as optimizing only a few free parameters\nin a common low-dimensional intrinsic task subspace, which may help us\nunderstand why PLMs could easily adapt to various NLP tasks with small-scale\ndata. Specifically, to find such a subspace and examine its universality, we\nresort to the recent success of prompt tuning and decompose the soft prompts of\nmultiple NLP tasks into the same low-dimensional nonlinear subspace, then we\nlearn to adapt the PLM to unseen tasks or data by only tuning parameters in the\nsubspace. We dub this pipeline as intrinsic prompt tuning (IPT). In\nexperiments, we study diverse few-shot NLP tasks and surprisingly find that in\na 5-dimensional subspace found with 100 random tasks, by only tuning 5 free\nparameters, we can recover 87% and 65% of the full prompt tuning performance\nfor 100 seen tasks (using different training data) and 20 unseen tasks,\nrespectively, showing great generalization ability of the found intrinsic task\nsubspace. Besides being an analysis tool, IPT could further bring practical\nbenefits, such as improving the prompt tuning stability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socially Aware Bias Measurements for Hindi Language Representations. (arXiv:2110.07871v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07871","description":"<p>Language representations are an efficient tool used across NLP, but they are\nstrife with encoded societal biases. These biases are studied extensively, but\nwith a primary focus on English language representations and biases common in\nthe context of Western society. In this work, we investigate the biases present\nin Hindi language representations such as caste and religion associated biases.\nWe demonstrate how biases are unique to specific language representations based\non the history and culture of the region they are widely spoken in, and also\nhow the same societal bias (such as binary gender associated biases) when\ninvestigated across languages is encoded by different words and text spans.\nWith this work, we emphasize on the necessity of social-awareness along with\nlinguistic and grammatical artefacts when modeling language representations, in\norder to understand the biases encoded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malik_V/0/1/0/all/0/1\">Vijit Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishi_A/0/1/0/all/0/1\">Akihiro Nishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer. (arXiv:2110.07904v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07904","description":"<p>As pre-trained language models have gotten larger, there has been growing\ninterest in parameter-efficient methods to apply these models to downstream\ntasks. Building on the PromptTuning approach of Lester et al. (2021), which\nlearns task-specific soft prompts to condition a frozen language model to\nperform downstream tasks, we propose a novel prompt-based transfer learning\napproach called SPoT: Soft Prompt Transfer. SPoT first learns a prompt on one\nor more source tasks and then uses it to initialize the prompt for a target\ntask. We show that SPoT significantly boosts the performance of PromptTuning\nacross many tasks. More importantly, SPoT either matches or outperforms\nModelTuning, which fine-tunes the entire model on each individual task, across\nall model sizes while being more parameter-efficient (up to 27,000x fewer\ntask-specific parameters). We further conduct a large-scale study on task\ntransferability with 26 NLP tasks and 160 combinations of source-target tasks,\nand demonstrate that tasks can often benefit each other via prompt transfer.\nFinally, we propose a simple yet efficient retrieval approach that interprets\ntask prompts as task embeddings to identify the similarity between tasks and\npredict the most transferable source tasks for a given novel target task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Speech Recognition using Knowledge Transfer across Learning Processes. (arXiv:2110.07909v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07909","description":"<p>Multilingual end-to-end(E2E) models have shown a great potential in the\nexpansion of the language coverage in the realm of automatic speech\nrecognition(ASR). In this paper, we aim to enhance the multilingual ASR\nperformance in two ways, 1)studying the impact of feeding a one-hot vector\nidentifying the language, 2)formulating the task with a meta-learning objective\ncombined with self-supervised learning (SSL). We associate every language with\na distinct task manifold and attempt to improve the performance by transferring\nknowledge across learning processes itself as compared to transferring through\nfinal model parameters. We employ this strategy on a dataset comprising of 6\nlanguages for an in-domain ASR task, by minimizing an objective related to\nexpected gradient path length. Experimental results reveal the best\npre-training strategy resulting in 3.55% relative reduction in overall WER. A\ncombination of LEAP and SSL yields 3.51% relative reduction in overall WER when\nusing language ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_R/0/1/0/all/0/1\">Rimita Lahiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumatani_K/0/1/0/all/0/1\">Kenichi Kumatani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_E/0/1/0/all/0/1\">Eric Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Level and Direction of Phonetic Dialect Change in the Northern Netherlands. (arXiv:2110.07918v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07918","description":"<p>This article reports ongoing investigations into phonetic change of dialect\ngroups in the northern Netherlandic language area, particularly the Frisian and\nLow Saxon dialect groups, which are known to differ in vitality. To achieve\nthis, we combine existing phonetically transcribed corpora with dialectometric\napproaches that allow us to quantify change among older male dialect speakers\nin a real-time framework. A multidimensional variant of the Levenshtein\ndistance, combined with methods that induce realistic phonetic distances\nbetween transcriptions, is used to estimate how much dialect groups have\nchanged between 1990 and 2010, and whether they changed towards Standard Dutch\nor away from it. Our analyses indicate that language change is a slow process\nin this geographical area. Moreover, the Frisian and Groningen dialect groups\nseem to be most stable, while the other Low Saxon varieties (excluding the\nGroningen dialect group) were shown to be most prone to change. We offer\npossible explanations for our findings, while we discuss shortcomings of the\ndata and approach in detail, as well as desiderata for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buurke_R/0/1/0/all/0/1\">Raoul Buurke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekeres_H/0/1/0/all/0/1\">Hedwig Sekeres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heeringa_W/0/1/0/all/0/1\">Wilbert Heeringa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knooihuizen_R/0/1/0/all/0/1\">Remco Knooihuizen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieling_M/0/1/0/all/0/1\">Martijn Wieling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap: Cross-Lingual Summarization with Compression Rate. (arXiv:2110.07936v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07936","description":"<p>Cross-lingual Summarization (CLS), converting a document into a cross-lingual\nsummary, is highly related to Machine Translation (MT) task. However, MT\nresources are still underutilized for the CLS task. In this paper, we propose a\nnovel task, Cross-lingual Summarization with Compression rate (CSC), to benefit\ncross-lingual summarization through large-scale MT corpus. Through introducing\ncompression rate, we regard MT task as a special CLS task with the compression\nrate of 100%. Hence they can be trained as a unified task, sharing knowledge\nmore effectively. Moreover, to bridge these two tasks smoothly, we propose a\nsimple yet effective data augmentation method to produce document-summary pairs\nwith different compression rates. The proposed method not only improves the\nperformance of CLS task, but also provides controllability to generate\nsummaries in desired lengths. Experiments demonstrate that our method\noutperforms various strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_K/0/1/0/all/0/1\">Kai Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Causal Influences on Publication Trends and Behavior: A Case Study of the Computational Linguistics Community. (arXiv:2110.07938v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07938","description":"<p>Drawing causal conclusions from observational real-world data is a very much\ndesired but challenging task. In this paper we present mixed-method analyses to\ninvestigate causal influences of publication trends and behavior on the\nadoption, persistence, and retirement of certain research foci --\nmethodologies, materials, and tasks that are of interest to the computational\nlinguistics (CL) community. Our key findings highlight evidence of the\ntransition to rapidly emerging methodologies in the research community (e.g.,\nadoption of bidirectional LSTMs influencing the retirement of LSTMs), the\npersistent engagement with trending tasks and techniques (e.g., deep learning,\nembeddings, generative, and language models), the effect of scientist location\nfrom outside the US, e.g., China on propensity of researching languages beyond\nEnglish, and the potential impact of funding for large-scale research programs.\nWe anticipate this work to provide useful insights about publication trends and\nbehavior and raise the awareness about the potential for causal inference in\nthe computational linguistics and a broader scientific community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glenski_M/0/1/0/all/0/1\">Maria Glenski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkova_S/0/1/0/all/0/1\">Svitlana Volkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't speak too fast: The impact of data bias on self-supervised speech models. (arXiv:2110.07957v1 [eess.AS])","link":"http://arxiv.org/abs/2110.07957","description":"<p>Self-supervised Speech Models (S3Ms) have been proven successful in many\nspeech downstream tasks, like ASR. However, how pre-training data affects S3Ms'\ndownstream behavior remains an unexplored issue. In this paper, we study how\npre-training data affects S3Ms by pre-training models on biased datasets\ntargeting different factors of speech, including gender, content, and prosody,\nand evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB\nBenchmark. Our experiments show that S3Ms have tolerance toward gender bias.\nMoreover, we find that the content of speech has little impact on the\nperformance of S3Ms across downstream tasks, but S3Ms do show a preference\ntoward a slower speech rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracing Origins: Coref-aware Machine Reading Comprehension. (arXiv:2110.07961v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07961","description":"<p>Machine reading comprehension is a heavily-studied research and test field\nfor evaluating new pre-trained models and fine-tuning strategies, and recent\nstudies have enriched the pre-trained models with syntactic, semantic and other\nlinguistic information to improve the performance of the model. In this paper,\nwe imitated the human's reading process in connecting the anaphoric expressions\nand explicitly leverage the coreference information to enhance the word\nembeddings from the pre-trained model, in order to highlight the coreference\nmentions that must be identified for coreference-intensive question answering\nin QUOREF, a relatively new dataset that is specifically designed to evaluate\nthe coreference-related performance of a model. We used an additional BERT\nlayer to focus on the coreference mentions, and a Relational Graph\nConvolutional Network to model the coreference relations. We demonstrated that\nthe explicit incorporation of the coreference information in fine-tuning stage\nperformed better than the incorporation of the coreference information in\ntraining a pre-trained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Baorong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scribosermo: Fast Speech-to-Text models for German and other Languages. (arXiv:2110.07982v1 [cs.CL])","link":"http://arxiv.org/abs/2110.07982","description":"<p>Recent Speech-to-Text models often require a large amount of hardware\nresources and are mostly trained in English. This paper presents Speech-to-Text\nmodels for German, as well as for Spanish and French with special features: (a)\nThey are small and run in real-time on microcontrollers like a RaspberryPi. (b)\nUsing a pretrained English model, they can be trained on consumer-grade\nhardware with a relatively small dataset. (c) The models are competitive with\nother solutions and outperform them in German. In this respect, the models\ncombine advantages of other approaches, which only include a subset of the\npresented features. Furthermore, the paper provides a new library for handling\ndatasets, which is focused on easy extension with additional datasets and shows\nan optimized way for transfer-learning new languages using a pretrained model\nfrom another language with a similar alphabet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bermuth_D/0/1/0/all/0/1\">Daniel Bermuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poeppel_A/0/1/0/all/0/1\">Alexander Poeppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_W/0/1/0/all/0/1\">Wolfgang Reif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Multi-task Learning for Disaster Tweet Categorisation. (arXiv:2110.08010v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08010","description":"<p>Social media has enabled people to circulate information in a timely fashion,\nthus motivating people to post messages seeking help during crisis situations.\nThese messages can contribute to the situational awareness of emergency\nresponders, who have a need for them to be categorised according to information\ntypes (i.e. the type of aid services the messages are requesting). We introduce\na transformer-based multi-task learning (MTL) technique for classifying\ninformation types and estimating the priority of these messages. We evaluate\nthe effectiveness of our approach with a variety of metrics by submitting runs\nto the TREC Incident Streams (IS) track: a research initiative specifically\ndesigned for disaster tweet classification and prioritisation. The results\ndemonstrate that our approach achieves competitive performance in most metrics\nas compared to other participating runs. Subsequently, we find that an ensemble\napproach combining disparate transformer encoders within our approach helps to\nimprove the overall effectiveness to a significant extent, achieving\nstate-of-the-art performance in almost every metric. We make the code publicly\navailable so that our work can be reproduced and used as a baseline for the\ncommunity for future work in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nulty_P/0/1/0/all/0/1\">Paul Nulty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillis_D/0/1/0/all/0/1\">David Lillis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Proficiency with Implicit User Representations. (arXiv:2110.08011v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08011","description":"<p>We introduce the problem of proficiency modeling: Given a user's posts on a\nsocial media platform, the task is to identify the subset of posts or topics\nfor which the user has some level of proficiency. This enables the filtering\nand ranking of social media posts on a given topic as per user proficiency.\nUnlike experts on a given topic, proficient users may not have received formal\ntraining and possess years of practical experience, but may be autodidacts,\nhobbyists, and people with sustained interest, enabling them to make genuine\nand original contributions to discourse. While predicting whether a user is an\nexpert on a given topic imposes strong constraints on who is a true positive,\nproficiency modeling implies a graded scoring, relaxing these constraints. Put\nanother way, many active social media users can be assumed to possess, or\neventually acquire, some level of proficiency on topics relevant to their\ncommunity. We tackle proficiency modeling in an unsupervised manner by\nutilizing user embeddings to model engagement with a given topic, as indicated\nby a user's preference for authoring related content. We investigate five\nalternative approaches to model proficiency, ranging from basic ones to an\nadvanced, tailored user modeling approach, applied within two real-world\nbenchmarks for evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Breitwieser_K/0/1/0/all/0/1\">Kim Breitwieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahnala_A/0/1/0/all/0/1\">Allison Lahnala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_C/0/1/0/all/0/1\">Charles Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flek_L/0/1/0/all/0/1\">Lucie Flek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1\">Martin Potthast</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crisis Domain Adaptation Using Sequence-to-sequence Transformers. (arXiv:2110.08015v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08015","description":"<p>User-generated content (UGC) on social media can act as a key source of\ninformation for emergency responders in crisis situations. However, due to the\nvolume concerned, computational techniques are needed to effectively filter and\nprioritise this content as it arises during emerging events. In the literature,\nthese techniques are trained using annotated content from previous crises. In\nthis paper, we investigate how this prior knowledge can be best leveraged for\nnew crises by examining the extent to which crisis events of a similar type are\nmore suitable for adaptation to new events (cross-domain adaptation). Given the\nrecent successes of transformers in various language processing tasks, we\npropose CAST: an approach for Crisis domain Adaptation leveraging\nSequence-to-sequence Transformers. We evaluate CAST using two major\ncrisis-related message classification datasets. Our experiments show that our\nCAST-based best run without using any target data achieves the state of the art\nperformance in both in-domain and cross-domain contexts. Moreover, CAST is\nparticularly effective in one-to-one cross-domain adaptation when trained with\na larger language model. In many-to-one adaptation where multiple crises are\njointly used as the source domain, CAST further improves its performance. In\naddition, we find that more similar events are more likely to bring better\nadaptation performance whereas fine-tuning using dissimilar events does not\nhelp for adaptation. To aid reproducibility, we open source our code to the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Congcong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nulty_P/0/1/0/all/0/1\">Paul Nulty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lillis_D/0/1/0/all/0/1\">David Lillis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Modeling for Dialogue Disentanglement. (arXiv:2110.08018v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08018","description":"<p>Tangled multi-party dialogue context leads to challenges for dialogue reading\ncomprehension, where multiple dialogue threads flow simultaneously within the\nsame dialogue history, thus increasing difficulties in understanding a dialogue\nhistory for both human and machine. Dialogue disentanglement aims to clarify\nconversation threads in a multi-party dialogue history, thus reducing the\ndifficulty of comprehending the long disordered dialogue passage. Existing\nstudies commonly focus on utterance encoding with carefully designed feature\nengineering-based methods but pay inadequate attention to dialogue structure.\nThis work designs a novel model to disentangle multi-party history into\nthreads, by taking dialogue structure features into account. Specifically,\nbased on the fact that dialogues are constructed through successive\nparticipation of speakers and interactions between users of interest, we\nextract clues of speaker property and reference of users to model the structure\nof a long dialogue record. The novel method is evaluated on the Ubuntu IRC\ndataset and shows state-of-the-art experimental results in dialogue\ndisentanglement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xinbei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Emotion-Cause Pair Extraction in Conversations. (arXiv:2110.08020v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08020","description":"<p>Emotion cause analysis has received considerable attention in recent years.\nPrevious studies primarily focused on emotion cause extraction from texts in\nnews articles or microblogs. It is also interesting to discover emotions and\ntheir causes in conversations. As conversation in its natural form is\nmultimodal, a large number of studies have been carried out on multimodal\nemotion recognition in conversations, but there is still a lack of work on\nmultimodal emotion cause analysis. In this work, we introduce a new task named\nMultimodal Emotion-Cause Pair Extraction in Conversations, aiming to jointly\nextract emotions and their associated causes from conversations reflected in\nmultiple modalities (text, audio and video). We accordingly construct a\nmultimodal conversational emotion cause dataset, Emotion-Cause-in-Friends,\nwhich contains 9,272 multimodal emotion-cause pairs annotated on 13,509\nutterances in the sitcom Friends. We finally benchmark the task by establishing\na baseline system that incorporates multimodal features for emotion-cause pair\nextraction. Preliminary experimental results demonstrate the potential of\nmultimodal information fusion for discovering both emotions and causes in\nconversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fanfan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianfei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StreaMulT: Streaming Multimodal Transformer for Heterogeneous and Arbitrary Long Sequential Data. (arXiv:2110.08021v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08021","description":"<p>This paper tackles the problem of processing and combining efficiently\narbitrary long data streams, coming from different modalities with different\nacquisition frequencies. Common applications can be, for instance, long-time\nindustrial or real-life systems monitoring from multimodal heterogeneous data\n(sensor data, monitoring report, images, etc.). To tackle this problem, we\npropose StreaMulT, a Streaming Multimodal Transformer, relying on cross-modal\nattention and an augmented memory bank to process arbitrary long input\nsequences at training time and run in a streaming way at inference. StreaMulT\nreproduces state-of-the-art results on CMU-MOSEI dataset, while being able to\ndeal with much longer inputs than other models such as previous Multimodal\nTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pellegrain_V/0/1/0/all/0/1\">Victor Pellegrain</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1\">Myriam Tami</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Batteux_M/0/1/0/all/0/1\">Michel Batteux</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a> (2) ((1) Institut de Recherche Technologique SystemX, (2) Universit&#xe9; Paris-Saclay, CentraleSup&#xe9;lec, MICS)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniDS: A Unified Dialogue System for Chit-Chat and Task-oriented Dialogues. (arXiv:2110.08032v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08032","description":"<p>With the advances in deep learning, tremendous progress has been made with\nchit-chat dialogue systems and task-oriented dialogue systems. However, these\ntwo systems are often tackled separately in current methods. To achieve more\nnatural interaction with humans, a dialogue agent needs to be capable of both\nchatting and accomplishing tasks. To this end, we propose a unified dialogue\nsystem (UniDS) with the two aforementioned skills. In particular, we design a\nunified dialogue data schema, compatible for both chit-chat and task-oriented\ndialogues, and we train UniDS with mixed dialogue data from a pretrained\nchit-chat dialogue model. Without adding extra parameters to SOTA baselines,\nUniDS can alternatively handle chit-chat and task-oriented dialogues in a\nunified framework. Experimental results demonstrate that the proposed UniDS\nworks comparably well as the pure chit-chat system, and it outperforms\nstate-of-the-art task-oriented dialogue systems. More importantly, UniDS\nachieves better robustness as it is able to smoothly switch between two types\nof dialogues. These results demonstrate the feasibility and potential of\nbuilding an one-for-all dialogue system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xinyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yajiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Natural Language Adversarial Examples through An Improved Beam Search Algorithm. (arXiv:2110.08036v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08036","description":"<p>The research of adversarial attacks in the text domain attracts many\ninterests in the last few years, and many methods with a high attack success\nrate have been proposed. However, these attack methods are inefficient as they\nrequire lots of queries for the victim model when crafting text adversarial\nexamples. In this paper, a novel attack model is proposed, its attack success\nrate surpasses the benchmark attack methods, but more importantly, its attack\nefficiency is much higher than the benchmark attack methods. The novel method\nis empirically evaluated by attacking WordCNN, LSTM, BiLSTM, and BERT on four\nbenchmark datasets. For instance, it achieves a 100\\% attack success rate\nhigher than the state-of-the-art method when attacking BERT and BiLSTM on IMDB,\nbut the number of queries for the victim models only is 1/4 and 1/6.5 of the\nstate-of-the-art method, respectively. Also, further experiments show the novel\nmethod has a good transferability on the generated adversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tengfei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zhaocheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hanping Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dingmeng Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Semantics: An Opportunity for Effective 6G Communications. (arXiv:2110.08049v1 [cs.IT])","link":"http://arxiv.org/abs/2110.08049","description":"<p>Recently, semantic communications are envisioned as a key enabler of future\n6G networks. Back to Shannon's information theory, the goal of communication\nhas long been to guarantee the correct reception of transmitted messages\nirrespective of their meaning. However, in general, whenever communication\noccurs to convey a meaning, what matters is the receiver's understanding of the\ntransmitted message and not necessarily its correct reconstruction. Hence,\nsemantic communications introduce a new paradigm: transmitting only relevant\ninformation sufficient for the receiver to capture the meaning intended can\nsave significant communication bandwidth. Thus, this work explores the\nopportunity offered by semantic communications for beyond 5G networks. In\nparticular, we focus on the benefit of semantic compression. We refer to\nsemantic message as a sequence of well-formed symbols learned from the\n\"meaning\" underlying data, which have to be interpreted at the receiver. This\nrequires a reasoning unit, here artificial, on a knowledge base: a symbolic\nknowledge representation of the specific application. Therefore, we present and\ndetail a novel architecture that enables representation learning of semantic\nsymbols for effective semantic communications. We first discuss theoretical\naspects and successfully design objective functions, which help learn effective\nsemantic encoders and decoders. Eventually, we show promising numerical results\nfor the scenario of text transmission, especially when the sender and receiver\nspeak different languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sana_M/0/1/0/all/0/1\">Mohamed Sana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strinati_E/0/1/0/all/0/1\">Emilio Calvanese Strinati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jurassic is (almost) All You Need: Few-Shot Meaning-to-Text Generation for Open-Domain Dialogue. (arXiv:2110.08094v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08094","description":"<p>One challenge with open-domain dialogue systems is the need to produce\nhigh-quality responses on any topic. We aim to improve the quality and coverage\nof Athena, an Alexa Prize dialogue system. We utilize Athena's response\ngenerators (RGs) to create training data for two new neural Meaning-to-Text\nRGs, Athena-GPT-Neo and Athena-Jurassic, for the movies, music, TV, sports, and\nvideo game domains. We conduct few-shot experiments, both within and\ncross-domain, with different tuning set sizes (2, 3, 10), prompt formats, and\nmeaning representations (MRs) for sets of WikiData KG triples, and dialogue\nacts with 14 possible attribute combinations. Our evaluation uses BLEURT and\nhuman evaluation metrics, and shows that with 10-shot tuning, Athena-Jurassic's\nperformance is significantly better for coherence and semantic accuracy.\nExperiments with 2-shot tuning on completely novel MRs results in a huge\nperformance drop for Athena-GPT-Neo, whose semantic accuracy falls to 0.41, and\nwhose untrue hallucination rate increases to 12%. Experiments with dialogue\nacts for video games show that with 10-shot tuning, both models learn to\ncontrol dialogue acts, but Athena-Jurassic has significantly higher coherence,\nand only 4% untrue hallucinations. Our results suggest that Athena-Jurassic can\nreliably produce outputs of high-quality for live systems with real users. To\nour knowledge, these are the first results demonstrating that few-shot tuning\non a massive language model can create NLGs that generalize to new domains, and\nproduce high-quality, semantically-controlled, conversational responses\ndirectly from MRs and KG triples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reed_L/0/1/0/all/0/1\">Lena Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cecilia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_A/0/1/0/all/0/1\">Angela Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liren Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_M/0/1/0/all/0/1\">Marilyn Walker</a> (Natural Language and Dialogue Systems Lab, University of California, Santa Cruz)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Bot: Prompt-Based Learning for Dialogue Systems. (arXiv:2110.08118v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08118","description":"<p>Learning to converse using only a few examples is a great challenge in\nconversational AI. The current best conversational models, which are either\ngood chit-chatters (e.g., BlenderBot) or goal-oriented systems (e.g., MinTL),\nare language models (LMs) fine-tuned on large conversational datasets. Training\nthese models is expensive, both in terms of computational resources and time,\nand it is hard to keep them up to date with new conversational skills. A simple\nyet unexplored solution is prompt-based few-shot learning (Brown et al. 2020)\nwhich does not require gradient-based fine-tuning but instead uses a few\nexamples in the LM context as the only source of learning. In this paper, we\nexplore prompt-based few-shot learning in dialogue tasks. We benchmark LMs of\ndifferent sizes in nine response generation tasks, which include four\nknowledge-grounded tasks, a task-oriented generations task, three open-chat\ntasks, and controlled stylistic generation, and five conversational parsing\ntasks, which include dialogue state tracking, graph path generation, persona\ninformation extraction, document retrieval, and internet query generation. The\ncurrent largest released LM (GPT-J-6B) using prompt-based few-shot learning,\nand thus requiring no training, achieves competitive performance to fully\ntrained state-of-the-art models. Moreover, we propose a novel prompt-based\nfew-shot classifier, that also does not require any fine-tuning, to select the\nmost appropriate prompt given a dialogue history. Finally, by combining the\npower of prompt-based few-shot learning and a Skill Selector, we create an\nend-to-end chatbot named the Few-Shot Bot (FSB), which automatically selects\nthe most appropriate conversational skill, queries different knowledge bases or\nthe internet, and uses the retrieved knowledge to generate a human-like\nresponse, all using only few dialogue examples per skill.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1\">Genta Indra Winata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking Down Multilingual Machine Translation. (arXiv:2110.08130v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08130","description":"<p>While multilingual training is now an essential ingredient in machine\ntranslation (MT) systems, recent work has demonstrated that it has different\neffects in different multilingual settings, such as many-to-one, one-to-many,\nand many-to-many learning. These training settings expose the encoder and the\ndecoder in a machine translation model with different data distributions. In\nthis paper, we examine how different varieties of multilingual training\ncontribute to learning these two components of the MT model. Specifically, we\ncompare bilingual models with encoders and/or decoders initialized by\nmultilingual training. We show that multilingual training is beneficial to\nencoders in general, while it only benefits decoders for low-resource languages\n(LRLs). We further find the important attention heads for each language pair\nand compare their correlations during inference. Our analysis sheds light on\nhow multilingual translation models work and also enables us to propose methods\nto improve performance by training with highly related languages. Our\nmany-to-one models for high-resource languages and one-to-many models for LRL\noutperform the best results reported by Aharoni et al. (2019).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1\">Ting-Rui Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating diverse extraction pathways using iterative predictions for Multilingual Open Information Extraction. (arXiv:2110.08144v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08144","description":"<p>In this paper we investigate a simple hypothesis for the Open Information\nExtraction (OpenIE) task, that it may be easier to extract some elements of an\ntriple if the extraction is conditioned on prior extractions which may be\neasier to extract. We successfully exploit this and propose a neural\nmultilingual OpenIE system that iteratively extracts triples by conditioning\nextractions on different elements of the triple leading to a rich set of\nextractions. The iterative nature of MiLIE also allows for seamlessly\nintegrating rule based extraction systems with a neural end-to-end system\nleading to improved performance. MiLIE outperforms SOTA systems on multiple\nlanguages ranging from Chinese to Galician thanks to it's ability of combining\nmultiple extraction pathways. Our analysis confirms that it is indeed true that\ncertain elements of an extraction are easier to extract than others. Finally,\nwe introduce OpenIE evaluation datasets for two low resource languages namely\nJapanese and Galician.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubio_D/0/1/0/all/0/1\">Daniel O&#xf1;oro Rubio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Tembras_V/0/1/0/all/0/1\">Vanesa Rodriguez-Tembras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamoto_M/0/1/0/all/0/1\">Makoto Takamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models. (arXiv:2110.08151v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08151","description":"<p>Recent studies have shown that multilingual pretrained language models can be\neffectively improved with cross-lingual alignment information from Wikipedia\nentities. However, existing methods only exploit entity information in\npretraining and do not explicitly use entities in downstream tasks. In this\nstudy, we explore the effectiveness of leveraging entity representations for\ndownstream cross-lingual tasks. We train a multilingual language model with 24\nlanguages with entity representations and show the model consistently\noutperforms word-based pretrained models in various cross-lingual transfer\ntasks. We also analyze the model and the key insight is that incorporating\nentity representations into the input allows us to extract more\nlanguage-agnostic features. We also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset. We show that entity-based prompt\nelicits correct factual knowledge more likely than using only word\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1\">Ikuya Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kronecker Decomposition for GPT Compression. (arXiv:2110.08152v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08152","description":"<p>GPT is an auto-regressive Transformer-based pre-trained language model which\nhas attracted a lot of attention in the natural language processing (NLP)\ndomain due to its state-of-the-art performance in several downstream tasks. The\nsuccess of GPT is mostly attributed to its pre-training on huge amount of data\nand its large number of parameters (from ~100M to billions of parameters).\nDespite the superior performance of GPT (especially in few-shot or zero-shot\nsetup), this overparameterized nature of GPT can be very prohibitive for\ndeploying this model on devices with limited computational power or memory.\nThis problem can be mitigated using model compression techniques; however,\ncompressing GPT models has not been investigated much in the literature. In\nthis work, we use Kronecker decomposition to compress the linear mappings of\nthe GPT-22 model. Our Kronecker GPT-2 model (KnGPT2) is initialized based on\nthe Kronecker decomposed version of the GPT-2 model and then is undergone a\nvery light pre-training on only a small portion of the training data with\nintermediate layer knowledge distillation (ILKD). Finally, our KnGPT2 is\nfine-tuned on down-stream tasks using ILKD as well. We evaluate our model on\nboth language modeling and General Language Understanding Evaluation benchmark\ntasks and show that with more efficient pre-training and similar number of\nparameters, our KnGPT2 outperforms the existing DistilGPT2 model significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edalati_A/0/1/0/all/0/1\">Ali Edalati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahaei_M/0/1/0/all/0/1\">Marzieh Tahaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">James J. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization. (arXiv:2110.08168v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08168","description":"<p>Transformer-based models have achieved state-of-the-art performance on short\ntext summarization. However, they still struggle with long-input summarization.\nIn this paper, we present a new approach for long-input summarization: Dynamic\nLatent Extraction for Abstractive Summarization. We jointly train an extractor\nwith an abstractor and treat the extracted text snippets as the latent\nvariable. We propose extractive oracles to provide the extractor with a strong\nlearning signal. We introduce consistency loss, which encourages the extractor\nto approximate the averaged dynamic weights predicted by the generator. We\nconduct extensive tests on two long-input summarization datasets, GovReport\n(document) and QMSum (dialogue). Our model significantly outperforms the\ncurrent state-of-the-art, including a 6.21 ROUGE-2 improvement on GovReport and\na 2.13 ROUGE-1 improvement on QMSum. Further analysis shows that the dynamic\nweights make our generation process highly interpretable. Our code will be\npublicly available upon publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models. (arXiv:2110.08173v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08173","description":"<p>Knowledge probing is crucial for understanding the knowledge transfer\nmechanism behind the pre-trained language models (PLMs). Despite the growing\nprogress of probing knowledge for PLMs in the general domain, specialised areas\nsuch as biomedical domain are vastly under-explored. To catalyse the research\nin this direction, we release a well-curated biomedical knowledge probing\nbenchmark, MedLAMA, which is constructed based on the Unified Medical Language\nSystem (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs\nand probing approaches on our benchmark, reaching at most 3% of acc@10. While\nhighlighting various sources of domain-specific challenges that amount to this\nunderwhelming performance, we illustrate that the underlying PLMs have a higher\npotential for probing tasks. To achieve this, we propose Contrastive-Probe, a\nnovel self-supervised contrastive probing approach, that adjusts the underlying\nPLMs without using any probing data. While Contrastive-Probe pushes the acc@10\nto 28%, the performance gap still remains notable. Our human expert evaluation\nsuggests that the probing performance of our Contrastive-Probe is still\nunder-estimated as UMLS still does not include the full spectrum of factual\nknowledge. We hope MedLAMA and Contrastive-Probe facilitate further\ndevelopments of more suited probing techniques for this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_C/0/1/0/all/0/1\">Charlotte Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixQG: Neural Question Generation with Mixed Answer Types. (arXiv:2110.08175v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08175","description":"<p>Asking good questions is an essential ability for both human and machine\nintelligence. However, existing neural question generation approaches mainly\nfocus on the short factoid type of answers. In this paper, we propose a neural\nquestion generator, MixQG, to bridge this gap. We combine 9 question answering\ndatasets with diverse answer types, including yes/no, multiple-choice,\nextractive, and abstractive answers, to train a single generative model. We\nshow with empirical results that our model outperforms existing work in both\nseen and unseen domains and can generate questions with different cognitive\nlevels when conditioned on different answer types. Our code is released and\nwell-integrated with the Huggingface library to facilitate various downstream\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1\">Lidiya Murakhovs&#x27;ka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color. (arXiv:2110.08182v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08182","description":"<p>Recent work has raised concerns about the inherent limitations of text-only\npretraining. In this paper, we first demonstrate that reporting bias, the\ntendency of people to not state the obvious, is one of the causes of this\nlimitation, and then investigate to what extent multimodal training can\nmitigate this issue. To accomplish this, we 1) generate the Color Dataset\n(CoDa), a dataset of human-perceived color distributions for 521 common\nobjects; 2) use CoDa to analyze and compare the color distribution found in\ntext, the distribution captured by language models, and a human's perception of\ncolor; and 3) investigate the performance differences between text-only and\nmultimodal models on CoDa. Our results show that the distribution of colors\nthat a language model recovers correlates more strongly with the inaccurate\ndistribution found in text than with the ground-truth, supporting the claim\nthat reporting bias negatively impacts and inherently limits text-only\ntraining. We then demonstrate that multimodal models can leverage their visual\ntraining to mitigate these effects, providing a promising avenue for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paik_C/0/1/0/all/0/1\">Cory Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroca_Ouellette_S/0/1/0/all/0/1\">St&#xe9;phane Aroca-Ouellette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roncone_A/0/1/0/all/0/1\">Alessandro Roncone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm. (arXiv:2110.08190v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08190","description":"<p>Various pruning approaches have been proposed to reduce the footprint\nrequirements of Transformer-based language models. Conventional wisdom is that\npruning reduces the model expressiveness and thus is more likely to underfit\nthan overfit compared to the original model. However, under the trending\npretrain-and-finetune paradigm, we argue that pruning increases the risk of\noverfitting if pruning was performed at the fine-tuning phase, as it increases\nthe amount of information a model needs to learn from the downstream task,\nresulting in relative data deficiency. In this paper, we aim to address the\noverfitting issue under the pretrain-and-finetune paradigm to improve pruning\nperformance via progressive knowledge distillation (KD) and sparse pruning.\nFurthermore, to mitigate the interference between different strategies of\nlearning rate, pruning and distillation, we propose a three-stage learning\nframework. We show for the first time that reducing the risk of overfitting can\nhelp the effectiveness of pruning under the pretrain-and-finetune paradigm.\nExperiments on multiple datasets of GLUE benchmark show that our method\nachieves highly competitive pruning performance over the state-of-the-art\ncompetitors across different pruning ratio constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dongkuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_I/0/1/0/all/0/1\">Ian E.H. Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sung-en Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Mimi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why don't people use character-level machine translation?. (arXiv:2110.08191v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08191","description":"<p>We present a literature and empirical survey that critically assesses the\nstate of the art in character-level modeling for machine translation (MT).\nDespite evidence in the literature that character-level systems are comparable\nwith subword systems, they are virtually never used in competitive setups in\nWMT competitions. We empirically show that even with recent modeling\ninnovations in character-level natural language processing, character-level MT\nsystems still struggle to match their subword-based counterparts both in terms\nof translation quality and training and inference speed. Character-level MT\nsystems show neither better domain robustness, nor better morphological\ngeneralization, despite being often so motivated. On the other hand, they tend\nto be more robust towards source side noise and the translation quality does\nnot degrade with increasing beam size at decoding time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1\">Helmut Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BBQ: A Hand-Built Bias Benchmark for Question Answering. (arXiv:2110.08193v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08193","description":"<p>It is well documented that NLP models learn social biases present in the\nworld, but little work has been done to show how these biases manifest in\nactual model outputs for applied tasks like question answering (QA). We\nintroduce the Bias Benchmark for QA (BBQ), a dataset consisting of\nquestion-sets constructed by the authors that highlight \\textit{attested}\nsocial biases against people belonging to protected classes along nine\ndifferent social dimensions relevant for U.S. English-speaking contexts. Our\ntask evaluates model responses at two distinct levels: (i) given an\nunder-informative context, test how strongly model answers reflect social\nbiases, and (ii) given an adequately informative context, test whether the\nmodel's biases still override a correct answer choice. We find that models\nstrongly rely on stereotypes when the context is ambiguous, meaning that the\nmodel's outputs consistently reproduce harmful biases in this setting. Though\nmodels are much more accurate when the context provides an unambiguous answer,\nthey still rely on stereotyped information and achieve an accuracy 2.5\npercentage points higher on examples where the correct answer aligns with a\nsocial bias, with this accuracy difference widening to 5 points for examples\ntargeting gender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nangia_N/0/1/0/all/0/1\">Nikita Nangia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_V/0/1/0/all/0/1\">Vishakh Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_J/0/1/0/all/0/1\">Jana Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Htut_P/0/1/0/all/0/1\">Phu Mon Htut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multitask Prompted Training Enables Zero-Shot Task Generalization. (arXiv:2110.08207v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08207","description":"<p>Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks. It has been hypothesized that this is\na consequence of implicit multitask learning in language model training. Can\nzero-shot generalization instead be directly induced by explicit multitask\nlearning? To test this question at scale, we develop a system for easily\nmapping general natural language tasks into a human-readable prompted form. We\nconvert a large set of supervised datasets, each with multiple prompts using\nvarying natural language. These prompted datasets allow for benchmarking the\nability of a model to perform completely unseen tasks specified in natural\nlanguage. We fine-tune a pretrained encoder-decoder model on this multitask\nmixture covering a wide variety of tasks. The model attains strong zero-shot\nperformance on several standard datasets, often outperforming models 16x its\nsize. Further, our approach attains strong performance on a subset of tasks\nfrom the BIG-Bench benchmark, outperforming models 6x its size. All prompts and\ntrained models are available at github.com/bigscience-workshop/promptsource/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1\">Lintang Sutawika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiegler_A/0/1/0/all/0/1\">Arnaud Stiegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scao_T/0/1/0/all/0/1\">Teven Le Scao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_A/0/1/0/all/0/1\">Arun Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shanya Sharma Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szczechla_E/0/1/0/all/0/1\">Eliza Szczechla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Datta_D/0/1/0/all/0/1\">Debajyoti Datta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jonathan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mike Tian-Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manica_M/0/1/0/all/0/1\">Matteo Manica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Thomas Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neeraj_T/0/1/0/all/0/1\">Trishala Neeraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozen_J/0/1/0/all/0/1\">Jos Rozen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason Alan Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teehan_R/0/1/0/all/0/1\">Ryan Teehan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bers_T/0/1/0/all/0/1\">Tali Bers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1\">Thomas Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Identity Preserving Normal to Dysarthric Voice Conversion. (arXiv:2110.08213v1 [cs.SD])","link":"http://arxiv.org/abs/2110.08213","description":"<p>We present a voice conversion framework that converts normal speech into\ndysarthric speech while preserving the speaker identity. Such a framework is\nessential for (1) clinical decision making processes and alleviation of patient\nstress, (2) data augmentation for dysarthric speech recognition. This is an\nespecially challenging task since the converted samples should capture the\nseverity of dysarthric speech while being highly natural and possessing the\nspeaker identity of the normal speaker. To this end, we adopted a two-stage\nframework, which consists of a sequence-to-sequence model and a nonparallel\nframe-wise model. Objective and subjective evaluations were conducted on the\nUASpeech dataset, and results showed that the method was able to yield\nreasonable naturalness and capture severity aspects of the pathological speech.\nOn the other hand, the similarity to the normal source speaker's voice was\nlimited and requires further improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halpern_B/0/1/0/all/0/1\">Bence Mark Halpern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Violeta_L/0/1/0/all/0/1\">Lester Phillip Violeta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1\">Odette Scharenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Speech Synthesis For Speech-To-Speech Translation. (arXiv:2110.08214v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08214","description":"<p>In a speech-to-speech translation (S2ST) pipeline, the text-to-speech (TTS)\nmodule is an important component for delivering the translated speech to users.\nTo enable incremental S2ST, the TTS module must be capable of synthesizing and\nplaying utterances while its input text is still streaming in. In this work, we\nfocus on improving the incremental synthesis performance of TTS models. With a\nsimple data augmentation strategy based on prefixes, we are able to improve the\nincremental TTS quality to approach offline performance. Furthermore, we bring\nour incremental TTS system to the practical scenario in combination with an\nupstream simultaneous speech translation system, and show the gains also carry\nover to this use-case. In addition, we propose latency metrics tailored to S2ST\napplications, and investigate methods for latency reduction in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialFact: A Benchmark for Fact-Checking in Dialogue. (arXiv:2110.08222v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08222","description":"<p>Fact-checking is an essential tool to mitigate the spread of misinformation\nand disinformation, however, it has been often explored to verify formal\nsingle-sentence claims instead of casual conversational claims. To study the\nproblem, we introduce the task of fact-checking in dialogue. We construct\nDialFact, a testing benchmark dataset of 22,245 annotated conversational\nclaims, paired with pieces of evidence from Wikipedia. There are three\nsub-tasks in DialFact: 1) Verifiable claim detection task distinguishes whether\na response carries verifiable factual information; 2) Evidence retrieval task\nretrieves the most relevant Wikipedia snippets as evidence; 3) Claim\nverification task predicts a dialogue response to be supported, refuted, or not\nenough information. We found that existing fact-checking models trained on\nnon-dialogue data like FEVER fail to perform well on our task, and thus, we\npropose a simple yet data-efficient solution to effectively improve\nfact-checking performance in dialogue. We point out unique challenges in\nDialFact such as handling the colloquialisms, coreferences, and retrieval\nambiguities in the error analysis to shed light on future research in this\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Prakhar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Visual Question Generation. (arXiv:2110.08226v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08226","description":"<p>In traditional Visual Question Generation (VQG), most images have multiple\nconcepts (e.g. objects and categories) for which a question could be generated,\nbut models are trained to mimic an arbitrary choice of concept as given in\ntheir training data. This makes training difficult and also poses issues for\nevaluation -- multiple valid questions exist for most images but only one or a\nfew are captured by the human references. We present Guiding Visual Question\nGeneration - a variant of VQG which conditions the question generator on\ncategorical information based on expectations on the type of question and the\nobjects it should explore. We propose two variants: (i) an explicitly guided\nmodel that enables an actor (human or automated) to select which objects and\ncategories to generate a question for; and (ii) an implicitly guided model that\nlearns which objects and categories to condition on, based on discrete latent\nvariables. The proposed models are evaluated on an answer-category augmented\nVQA dataset and our quantitative results show a substantial improvement over\nthe current state of the art (over 9 BLEU-4 increase). Human evaluation\nvalidates that guidance helps the generation of questions that are\ngrammatically coherent and relevant to the given image and objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vedd_N/0/1/0/all/0/1\">Nihir Vedd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Data Integration for Named Entity Disambiguation in Biomedical Text. (arXiv:2110.08228v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08228","description":"<p>Named entity disambiguation (NED), which involves mapping textual mentions to\nstructured entities, is particularly challenging in the medical domain due to\nthe presence of rare entities. Existing approaches are limited by the presence\nof coarse-grained structural resources in biomedical knowledge bases as well as\nthe use of training datasets that provide low coverage over uncommon resources.\nIn this work, we address these issues by proposing a cross-domain data\nintegration method that transfers structural knowledge from a general text\nknowledge base to the medical domain. We utilize our integration scheme to\naugment structural resources and generate a large biomedical NED dataset for\npretraining. Our pretrained model with injected structural knowledge achieves\nstate-of-the-art performance on two benchmark medical NED datasets: MedMentions\nand BC5CDR. Furthermore, we improve disambiguation of rare entities by up to 57\naccuracy points.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1\">Maya Varma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_L/0/1/0/all/0/1\">Laurel Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leszczynski_M/0/1/0/all/0/1\">Megan Leszczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent-based Product Collections for E-commerce using Pretrained Language Models. (arXiv:2110.08241v1 [cs.IR])","link":"http://arxiv.org/abs/2110.08241","description":"<p>Building a shopping product collection has been primarily a human job. With\nthe manual efforts of craftsmanship, experts collect related but diverse\nproducts with common shopping intent that are effective when displayed\ntogether, e.g., backpacks, laptop bags, and messenger bags for freshman bag\ngifts. Automatically constructing a collection requires an ML system to learn a\ncomplex relationship between the customer's intent and the product's\nattributes. However, there have been challenging points, such as 1) long and\ncomplicated intent sentences, 2) rich and diverse product attributes, and 3) a\nhuge semantic gap between them, making the problem difficult. In this paper, we\nuse a pretrained language model (PLM) that leverages textual attributes of\nweb-scale products to make intent-based product collections. Specifically, we\ntrain a BERT with triplet loss by setting an intent sentence to an anchor and\ncorresponding products to positive examples. Also, we improve the performance\nof the model by search-based negative sampling and category-wise positive pair\naugmentation. Our model significantly outperforms the search-based baseline\nmodel for intent-based product matching in offline evaluations. Furthermore,\nonline experimental results on our e-commerce platform show that the PLM-based\nmethod can construct collections of products with increased CTR, CVR, and\norder-diversity compared to expert-crafted collections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hiun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Min Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongjun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyun Dong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1\">Dongpil Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jeeseung Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dong Wook Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Ji Ae Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_R/0/1/0/all/0/1\">Rak Yeong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dubber: Dubbing for Silent Videos According to Scripts. (arXiv:2110.08243v1 [eess.AS])","link":"http://arxiv.org/abs/2110.08243","description":"<p>Dubbing is a post-production process of re-recording actors' dialogues, which\nis extensively used in filmmaking and video production. It is usually performed\nmanually by professional voice actors who read lines with proper prosody, and\nin synchronization with the pre-recorded videos. In this work, we propose\nNeural Dubber, the first neural network model to solve a novel automatic video\ndubbing (AVD) task: synthesizing human speech synchronized with the given\nsilent video from the text. Neural Dubber is a multi-modal text-to-speech (TTS)\nmodel that utilizes the lip movement in the video to control the prosody of the\ngenerated speech. Furthermore, an image-based speaker embedding (ISE) module is\ndeveloped for the multi-speaker setting, which enables Neural Dubber to\ngenerate speech with a reasonable timbre according to the speaker's face.\nExperiments on the chemistry lecture single-speaker dataset and LRS2\nmulti-speaker dataset show that Neural Dubber can generate speech audios on par\nwith state-of-the-art TTS models in terms of speech quality. Most importantly,\nboth qualitative and quantitative evaluations show that Neural Dubber can\ncontrol the prosody of synthesized speech by the video, and generate\nhigh-fidelity speech temporally synchronized with the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1\">Chenxu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Q/0/1/0/all/0/1\">Qiao Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tingle Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tricks for Training Sparse Translation Models. (arXiv:2110.08246v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08246","description":"<p>Multi-task learning with an unbalanced data distribution skews model learning\ntowards high resource tasks, especially when model capacity is fixed and fully\nshared across all tasks. Sparse scaling architectures, such as BASELayers,\nprovide flexible mechanisms for different tasks to have a variable number of\nparameters, which can be useful to counterbalance skewed data distributions. We\nfind that that sparse architectures for multilingual machine translation can\nperform poorly out of the box, and propose two straightforward techniques to\nmitigate this - a temperature heating mechanism and dense pre-training.\nOverall, these methods improve performance on two multilingual translation\nbenchmarks compared to standard BASELayers and Dense scaling baselines, and in\ncombination, more than 2x model convergence speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dua_D/0/1/0/all/0/1\">Dheeru Dua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhosale_S/0/1/0/all/0/1\">Shruti Bhosale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1\">Vedanuj Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1\">Angela Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. (arXiv:2110.08247v1 [cs.CR])","link":"http://arxiv.org/abs/2110.08247","description":"<p>Backdoor attacks are a kind of emergent security threat in deep learning.\nWhen a deep neural model is injected with a backdoor, it will behave normally\non standard inputs but give adversary-specified predictions once the input\ncontains specific backdoor triggers. Current textual backdoor attacks have poor\nattack performance in some tough situations. In this paper, we find two simple\ntricks that can make existing textual backdoor attacks much more harmful. The\nfirst trick is to add an extra training task to distinguish poisoned and clean\ndata during the training of the victim model, and the second one is to use all\nthe clean training data rather than remove the original clean data\ncorresponding to the poisoned data. These two tricks are universally applicable\nto different attack models. We conduct experiments in three tough situations\nincluding clean data fine-tuning, low poisoning rate, and label-consistent\nattacks. Experimental results show that the two tricks can significantly\nimprove attack performance. This paper exhibits the great potential harmfulness\nof backdoor attacks. All the code and data will be made public to facilitate\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct simultaneous speech to speech translation. (arXiv:2110.08250v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08250","description":"<p>We present the first direct simultaneous speech-to-speech translation\n(Simul-S2ST) model, with the ability to start generating translation in the\ntarget speech before consuming the full source speech content and independently\nfrom intermediate text representations. Our approach leverages recent progress\non direct speech-to-speech translation with discrete units. Instead of\ncontinuous spectrogram features, a sequence of direct representations, which\nare learned in a unsupervised manner, are predicted from the model and passed\ndirectly to a vocoder for speech synthesis. The simultaneous policy then\noperates on source speech features and target discrete units. Finally, a\nvocoder synthesize the target speech from discrete units on-the-fly. We carry\nout numerical studies to compare cascaded and direct approach on Fisher\nSpanish-English dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng-Jen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Phillip Koehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Order-Free Tag Relations for Context-Aware Recommendation. (arXiv:2012.02957v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.02957","description":"<p>Tag recommendation relies on either a ranking function for top-$k$ tags or an\nautoregressive generation method. However, the previous methods neglect one of\ntwo seemingly conflicting yet desirable characteristics of a tag set:\norderlessness and inter-dependency. While the ranking approach fails to address\nthe inter-dependency among tags when they are ranked, the autoregressive\napproach fails to take orderlessness into account because it is designed to\nutilize sequential relations among tokens. We propose a sequence-oblivious\ngeneration method for tag recommendation, in which the next tag to be generated\nis independent of the order of the generated tags and the order of the ground\ntruth tags occurring in training data. Empirical results on two different\ndomains, Instagram and Stack Overflow, show that our method is significantly\nsuperior to the previous approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeonghwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Suwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyon Myaeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trankit: A Light-Weight Transformer-based Toolkit for Multilingual Natural Language Processing. (arXiv:2101.03289v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.03289","description":"<p>We introduce Trankit, a light-weight Transformer-based Toolkit for\nmultilingual Natural Language Processing (NLP). It provides a trainable\npipeline for fundamental NLP tasks over 100 languages, and 90 pretrained\npipelines for 56 languages. Built on a state-of-the-art pretrained language\nmodel, Trankit significantly outperforms prior multilingual NLP pipelines over\nsentence segmentation, part-of-speech tagging, morphological feature tagging,\nand dependency parsing while maintaining competitive performance for\ntokenization, multi-word token expansion, and lemmatization over 90 Universal\nDependencies treebanks. Despite the use of a large pretrained transformer, our\ntoolkit is still efficient in memory usage and speed. This is achieved by our\nnovel plug-and-play mechanism with Adapters where a multilingual pretrained\ntransformer is shared across pipelines for different languages. Our toolkit\nalong with pretrained models and code are publicly available at:\nhttps://github.com/nlp-uoregon/trankit. A demo website for our toolkit is also\navailable at: <a href=\"http://nlp.uoregon.edu/trankit.\">this http URL</a> Finally, we create a demo video\nfor Trankit at: https://youtu.be/q0KGP3zGjGc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra-High Dimensional Sparse Representations with Binarization for Efficient Text Retrieval. (arXiv:2104.07198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07198","description":"<p>The semantic matching capabilities of neural information retrieval can\nameliorate synonymy and polysemy problems of symbolic approaches. However,\nneural models' dense representations are more suitable for re-ranking, due to\ntheir inefficiency. Sparse representations, either in symbolic or latent form,\nare more efficient with an inverted index. Taking the merits of the sparse and\ndense representations, we propose an ultra-high dimensional (UHD)\nrepresentation scheme equipped with directly controllable sparsity. UHD's large\ncapacity and minimal noise and interference among the dimensions allow for\nbinarized representations, which are highly efficient for storage and search.\nAlso proposed is a bucketing method, where the embeddings from multiple layers\nof BERT are selected/merged to represent diverse linguistic aspects. We test\nour models with MS MARCO and TREC CAR, showing that our models outperforms\nother sparse models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jang_K/0/1/0/all/0/1\">Kyoung-Rok Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Junmo Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_G/0/1/0/all/0/1\">Giwon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myaeng_S/0/1/0/all/0/1\">Sung-Hyon Myaeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joohee Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1\">Taewon Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1\">Heecheol Seo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DWUG: A large Resource of Diachronic Word Usage Graphs in Four Languages. (arXiv:2104.08540v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08540","description":"<p>Word meaning is notoriously difficult to capture, both synchronically and\ndiachronically. In this paper, we describe the creation of the largest resource\nof graded contextualized, diachronic word meaning annotation in four different\nlanguages, based on 100,000 human semantic proximity judgments. We thoroughly\ndescribe the multi-round incremental annotation process, the choice for a\nclustering algorithm to group usages into senses, and possible - diachronic and\nsynchronic - uses for this dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlechtweg_D/0/1/0/all/0/1\">Dominik Schlechtweg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebi_N/0/1/0/all/0/1\">Nina Tahmasebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengchen_S/0/1/0/all/0/1\">Simon Hengchen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubossarsky_H/0/1/0/all/0/1\">Haim Dubossarsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Rank Subspaces for Unsupervised Entity Linking. (arXiv:2104.08737v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08737","description":"<p>Entity linking is an important problem with many applications. Most previous\nsolutions were designed for settings where annotated training data is\navailable, which is, however, not the case in numerous domains. We propose a\nlight-weight and scalable entity linking method, Eigenthemes, that relies\nsolely on the availability of entity names and a referent knowledge base.\nEigenthemes exploits the fact that the entities that are truly mentioned in a\ndocument (the \"gold entities\") tend to form a semantically dense subset of the\nset of all candidate entities in the document. Geometrically speaking, when\nrepresenting entities as vectors via some given embedding, the gold entities\ntend to lie in a low-rank subspace of the full embedding space. Eigenthemes\nidentifies this subspace using the singular value decomposition and scores\ncandidate entities according to their proximity to the subspace. On the\nempirical front, we introduce multiple strong baselines that compare favorably\nto (and sometimes even outperform) the existing state of the art. Extensive\nexperiments on benchmark datasets from a variety of real-world domains showcase\nthe effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akhil Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Duran_A/0/1/0/all/0/1\">Alberto Garc&#xed;a-Dur&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphTMT: Unsupervised Graph-based Topic Modeling from Video Transcripts. (arXiv:2105.01466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01466","description":"<p>To unfold the tremendous amount of multimedia data uploaded daily to social\nmedia platforms, effective topic modeling techniques are needed. Existing work\ntends to apply topic models on written text datasets. In this paper, we propose\na topic extractor on video transcripts. Exploiting neural word embeddings\nthrough graph-based clustering, we aim to improve usability and semantic\ncoherence. Unlike most topic models, this approach works without knowing the\ntrue number of topics, which is important when no such assumption can or should\nbe made. Experimental results on the real-life multimodal dataset MuSe-CaR\ndemonstrates that our approach GraphTMT extracts coherent and meaningful topics\nand outperforms baseline methods. Furthermore, we successfully demonstrate the\napplicability of our approach on the popular Citysearch corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stappen_L/0/1/0/all/0/1\">Lukas Stappen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Jason Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable agent communication from scratch (with a generic visual processor emerging on the side). (arXiv:2106.04258v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04258","description":"<p>As deep networks begin to be deployed as autonomous agents, the issue of how\nthey can communicate with each other becomes important. Here, we train two deep\nnets from scratch to perform realistic referent identification through\nunsupervised emergent communication. We show that the largely interpretable\nemergent protocol allows the nets to successfully communicate even about object\ntypes they did not see at training time. The visual representations induced as\na by-product of our training regime, moreover, show comparable quality, when\nre-used as generic visual features, to a recent self-supervised learning model.\nOur results provide concrete evidence of the viability of (interpretable)\nemergent deep net communication in a more realistic scenario than previously\nconsidered, as well as establishing an intriguing link between this field and\nself-supervised visual learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dessi_R/0/1/0/all/0/1\">Roberto Dess&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baroni_M/0/1/0/all/0/1\">Marco Baroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraining Linear-chain CRFs to Regular Languages. (arXiv:2106.07306v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.07306","description":"<p>A major challenge in structured prediction is to represent the\ninterdependencies within output structures. When outputs are structured as\nsequences, linear-chain conditional random fields (CRFs) are a widely used\nmodel class which can learn \\textit{local} dependencies in the output. However,\nthe CRF's Markov assumption makes it impossible for CRFs to represent\ndistributions with \\textit{nonlocal} dependencies, and standard CRFs are unable\nto respect nonlocal constraints of the data (such as global arity constraints\non output labels). We present a generalization of CRFs that can enforce a broad\nclass of constraints, including nonlocal ones, by specifying the space of\npossible output structures as a regular language $\\mathcal{L}$. The resulting\nregular-constrained CRF (RegCCRF) has the same formal properties as a standard\nCRF, but assigns zero probability to all label sequences not in $\\mathcal{L}$.\nNotably, RegCCRFs can incorporate their constraints during training, while\nrelated models only enforce constraints during decoding. We prove that\nconstrained training is never worse than constrained decoding, and show\nempirically that it can be substantially better in practice. Additionally, we\ndemonstrate a practical benefit on downstream tasks by incorporating a RegCCRF\ninto a deep neural model for semantic role labeling, exceeding state-of-the-art\nresults on a standard dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papay_S/0/1/0/all/0/1\">Sean Papay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1\">Sebastian Pad&#xf3;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemEval-2021 Task 11: NLPContributionGraph -- Structuring Scholarly NLP Contributions for a Research Knowledge Graph. (arXiv:2106.07385v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07385","description":"<p>There is currently a gap between the natural language expression of scholarly\npublications and their structured semantic content modeling to enable\nintelligent content search. With the volume of research growing exponentially\nevery year, a search feature operating over semantically structured content is\ncompelling. The SemEval-2021 Shared Task NLPContributionGraph (a.k.a. 'the NCG\ntask') tasks participants to develop automated systems that structure\ncontributions from NLP scholarly articles in the English language. Being the\nfirst-of-its-kind in the SemEval series, the task released structured data from\nNLP scholarly articles at three levels of information granularity, i.e. at\nsentence-level, phrase-level, and phrases organized as triples toward Knowledge\nGraph (KG) building. The sentence-level annotations comprised the few sentences\nabout the article's contribution. The phrase-level annotations were scientific\nterm and predicate phrases from the contribution sentences. Finally, the\ntriples constituted the research overview KG. For the Shared Task,\nparticipating systems were then expected to automatically classify contribution\nsentences, extract scientific terms and relations from the sentences, and\norganize them as KG triples.\n</p>\n<p>Overall, the task drew a strong participation demographic of seven teams and\n27 participants. The best end-to-end task system classified contribution\nsentences at 57.27% F1, phrases at 46.41% F1, and triples at 22.28% F1. While\nthe absolute performance to generate triples remains low, in the conclusion of\nthis article, the difficulty of producing such data and as a consequence of\nmodeling it is highlighted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">S&#xf6;ren Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_T/0/1/0/all/0/1\">Ted Pedersen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion analysis and detection during COVID-19. (arXiv:2107.11020v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.11020","description":"<p>Crises such as natural disasters, global pandemics, and social unrest\ncontinuously threaten our world and emotionally affect millions of people\nworldwide in distinct ways. Understanding emotions that people express during\nlarge-scale crises helps inform policy makers and first responders about the\nemotional states of the population as well as provide emotional support to\nthose who need such support. We present CovidEmo, ~3K English tweets labeled\nwith emotions and temporally distributed across 18 months. Our analyses reveal\nthe emotional toll caused by COVID-19, and changes of the social narrative and\nassociated emotions over time. Motivated by the time-sensitive nature of crises\nand the cost of large-scale annotation efforts, we examine how well large\npre-trained language models generalize across domains and timeline in the task\nof perceived emotion prediction in the context of COVID-19. Our analyses\nsuggest that cross-domain information transfers occur, yet there are still\nsignificant gaps. We propose semi-supervised learning as a way to bridge this\ngap, obtaining significantly better performance using unlabeled data from the\ntarget domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sosea_T/0/1/0/all/0/1\">Tiberiu Sosea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_C/0/1/0/all/0/1\">Chau Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekle_A/0/1/0/all/0/1\">Alexander Tekle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12079","description":"<p>Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation to perform reasoning\nand provide consistent and explainable answers. We illustrate the system using\na COVID-19 vaccine information case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08102","description":"<p>Understanding speaker's feelings and producing appropriate responses with\nemotion connection is a key communicative skill for empathetic dialogue\nsystems. In this paper, we propose a simple technique called Affective Decoding\nfor empathetic response generation. Our method can effectively incorporate\nemotion signals during each decoding step, and can additionally be augmented\nwith an auxiliary dual emotion encoder, which learns separate embeddings for\nthe speaker and listener given the emotion base of the dialogue. Extensive\nempirical studies show that our models are perceived to be more empathetic by\nhuman evaluations, in comparison to several strong mainstream methods for\nempathetic responding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chengkun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruizhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morality-based Assertion and Homophily on Social Media: A Cultural Comparison between English and Japanese Languages. (arXiv:2108.10643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10643","description":"<p>Moral psychology is a domain that deals with moral identity, appraisals and\nemotions. Previous work has primarily focused on moral development and the\nassociated role of culture. Knowing that language is an inherent element of a\nculture, we used the social media platform Twitter to compare moral behaviors\nof Japanese tweets with English tweets. The five basic moral foundations, i.e.,\nCare, Fairness, Ingroup, Authority and Purity, along with the associated\nemotional valence were compared between English and Japanese tweets. The tweets\nfrom Japanese users depicted relatively higher Fairness, Ingroup, and Purity,\nwhereas English tweets expressed more positive emotions for all moral\ndimensions. Considering moral similarities in connecting users on social media,\nwe quantified homophily concerning different moral dimensions using our\nproposed method. The moral dimensions Care, Authority and Purity for English\nand Ingroup, Authority and Purity for Japanese depicted homophily on Twitter.\nOverall, our study uncovers the underlying cultural differences with respect to\nmoral behavior in English- and Japanese-speaking users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rishemjit Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_A/0/1/0/all/0/1\">Akiko Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_S/0/1/0/all/0/1\">S.R.S. Iyengar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasahara_K/0/1/0/all/0/1\">Kazutoshi Sasahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04726","description":"<p>Deep neural models for low-resource named entity recognition (NER) have shown\nimpressive results by leveraging distant super-vision or other meta-level\ninformation (e.g. explanation). However, the costs of acquiring such additional\ninformation are generally prohibitive, especially in domains where existing\nresources (e.g. databases to be used for distant supervision) may not exist. In\nthis paper, we present a novel two-stage framework (AutoTriggER) to improve NER\nperformance by automatically generating and leveraging \"entity triggers\" which\nare essentially human-readable clues in the text that can help guide the model\nto make better decisions. Thus, the framework is able to both create and\nleverage auxiliary supervision by itself. Through experiments on three\nwell-studied NER datasets, we show that our automatically extracted triggers\nare well-matched to human triggers, and AutoTriggER improves performance over a\nRoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a\nlow resource setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvam_R/0/1/0/all/0/1\">Ravi Kiran Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus. (arXiv:2109.13348v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13348","description":"<p>The current UMLS (Unified Medical Language System) Metathesaurus construction\nprocess for integrating over 200 biomedical source vocabularies is expensive\nand error-prone as it relies on the lexical algorithms and human editors for\ndeciding if the two biomedical terms are synonymous. Recent advances in Natural\nLanguage Processing such as Transformer models like BERT and its biomedical\nvariants with contextualized word embeddings have achieved state-of-the-art\n(SOTA) performance on downstream tasks. We aim to validate if these approaches\nusing the BERT models can actually outperform the existing approaches for\npredicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks\nwith LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with\nthe biomedical BERT embeddings extracted from each BERT model using different\nways of extraction. In the Transformer architecture, we evaluate the use of the\ndifferent biomedical BERT models that have been pre-trained using different\ndatasets and tasks. Given the SOTA performance of these BERT models for other\ndownstream tasks, our experiments yield surprisingly interesting results: (1)\nin both model architectures, the approaches employing these biomedical\nBERT-based models do not outperform the existing approaches using Siamese\nNetwork with BioWordVec embeddings for the UMLS synonymy prediction task, (2)\nthe original BioBERT large model that has not been pre-trained with the UMLS\noutperforms the SapBERT models that have been pre-trained with the UMLS, and\n(3) using the Siamese Networks yields better performance for synonymy\nprediction when compared to using the biomedical BERT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_G/0/1/0/all/0/1\">Goonmeet Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_H/0/1/0/all/0/1\">Hong Yung Yip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javangula_V/0/1/0/all/0/1\">Vishesh Javangula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenreider_O/0/1/0/all/0/1\">Olivier Bodenreider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00534","description":"<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Technology for Everyone: Automatic Speech Recognition for Non-Native English with Transfer Learning. (arXiv:2110.00678v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.00678","description":"<p>To address the performance gap of English ASR models on L2 English speakers,\nwe evaluate fine-tuning of pretrained wav2vec 2.0 models (Baevski et al., 2020;\nXu et al., 2021) on L2-ARCTIC, a non-native English speech corpus (Zhao et al.,\n2018) under different training settings. We compare \\textbf{(a)} models trained\nwith a combination of diverse accents to ones trained with only specific\naccents and \\textbf{(b)} results from different single-accent models. Our\nexperiments demonstrate the promise of developing ASR models for non-native\nEnglish speakers, even with small amounts of L2 training data and even without\na language model. Our models also excel in the zero-shot setting where we train\non multiple L2 datasets and test on a blind L2 test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shibano_T/0/1/0/all/0/1\">Toshiko Shibano</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyi Zhang</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mia Taige Li</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Haejin Cho</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Sullivan_P/0/1/0/all/0/1\">Peter Sullivan</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a> (1) ((1) University of British Columbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoEfication: Conditional Computation of Transformer Models for Efficient Inference. (arXiv:2110.01786v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01786","description":"<p>Transformer-based pre-trained language models can achieve superior\nperformance on most NLP tasks due to large parameter capacity, but also lead to\nhuge computation cost. Fortunately, we observe that most inputs only activate a\ntiny ratio of neurons of large Transformer-based models during inference.\nHence, we propose to transform a large model into its mixture-of-experts (MoE)\nversion with equal model size, namely MoEfication, which could accelerate\nlarge-model inference by conditional computation based on the sparse activation\nphenomenon. MoEfication consists of two steps: (1) splitting the parameters of\nfeed-forward neural networks (FFNs) into multiple parts as experts, and (2)\nbuilding expert routers to decide which experts will be used for each input.\nExperimental results show that the MoEfied models can significantly reduce\ncomputation cost, e.g., only activating 20% FFN parameters of a\n700-million-parameter model without performance degradation on several\ndownstream tasks including text classification and machine reading\ncomprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internal Language Model Adaptation with Text-Only Data for End-to-End Speech Recognition. (arXiv:2110.05354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05354","description":"<p>Text-only adaptation of an end-to-end (E2E) model remains a challenging task\nfor automatic speech recognition (ASR). Language model (LM) fusion-based\napproaches require an additional external LM during inference, significantly\nincreasing the computation cost. To overcome this, we propose an internal LM\nadaptation (ILMA) of the E2E model using text-only data. Trained with\naudio-transcript pairs, an E2E model implicitly learns an internal LM that\ncharacterizes the token sequence probability which is approximated by the E2E\nmodel output after zeroing out the encoder contribution. During ILMA, we\nfine-tune the internal LM, i.e., the E2E components excluding the encoder, to\nminimize a cross-entropy loss. To make ILMA effective, it is essential to train\nthe E2E model with an internal LM loss besides the standard E2E loss.\nFurthermore, we propose to regularize ILMA by minimizing the Kullback-Leibler\ndivergence between the output distributions of the adapted and unadapted\ninternal LMs. ILMA is the most effective when we update only the last linear\nlayer of the joint network. ILMA enables a fast text-only adaptation of the E2E\nmodel without increasing the run-time computational cost. Experimented with\n30K-hour trained transformer transducer models, ILMA achieves up to 34.9%\nrelative word error rate reduction from the unadapted baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Brief Introduction to Automatic Differentiation for Machine Learning. (arXiv:2110.06209v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06209","description":"<p>Machine learning and neural network models in particular have been improving\nthe state of the art performance on many artificial intelligence related tasks.\nNeural network models are typically implemented using frameworks that perform\ngradient based optimization methods to fit a model to a dataset. These\nframeworks use a technique of calculating derivatives called automatic\ndifferentiation (AD) which removes the burden of performing derivative\ncalculations from the model designer. In this report we describe AD, its\nmotivations, and different implementation approaches. We briefly describe\ndataflow programming as it relates to AD. Lastly, we present example programs\nthat are implemented with Tensorflow and PyTorch, which are two commonly used\nAD frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1\">Davan Harrison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation. (arXiv:2110.06354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06354","description":"<p>Recent years have witnessed the dramatic growth of paper volumes with plenty\nof new research papers published every day, especially in the area of computer\nscience. How to glean papers worth reading from the massive literature to do a\nquick survey or keep up with the latest advancement about a specific research\ntopic has become a challenging task. Existing academic search engines such as\nGoogle Scholar return relevant papers by individually calculating the relevance\nbetween each paper and query. However, such systems usually omit the\nprerequisite chains of a research topic and cannot form a meaningful reading\npath. In this paper, we introduce a new task named Reading Path Generation\n(RPG) which aims at automatically producing a path of papers to read for a\ngiven query. To serve as a research benchmark, we further propose SurveyBank, a\ndataset consisting of large quantities of survey papers in the field of\ncomputer science as well as their citation relationships. Each survey paper\ncontains key phrases extracted from its title and multi-level reading lists\ninferred from its references. Furthermore, we propose a\ngraph-optimization-based approach for reading path generation which takes the\nrelationship between papers into account. Extensive evaluations demonstrate\nthat our approach outperforms other baselines. A Real-time Reading Path\nGeneration System (RePaGer) has been also implemented with our designed model.\nTo the best of our knowledge, we are the first to target this important\nresearch problem. Our source code of RePaGer system and SurveyBank dataset can\nbe found on here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiayuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangyang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual COVID-19 Fake News Detection. (arXiv:2110.06495v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06495","description":"<p>The COVID-19 pandemic poses a great threat to global public health.\nMeanwhile, there is massive misinformation associated with the pandemic which\nadvocates unfounded or unscientific claims. Even major social media and news\noutlets have made an extra effort in debunking COVID-19 misinformation, most of\nthe fact-checking information is in English, whereas some unmoderated COVID-19\nmisinformation is still circulating in other languages, threatening the health\nof less-informed people in immigrant communities and developing countries. In\nthis paper, we make the first attempt to detect COVID-19 misinformation in a\nlow-resource language (Chinese) only using the fact-checked news in a\nhigh-resource language (English). We start by curating a Chinese real&amp;fake news\ndataset according to existing fact-checking information. Then, we propose a\ndeep learning framework named CrossFake to jointly encode the cross-lingual\nnews body texts and capture the news content as much as possible. Empirical\nresults on our dataset demonstrate the effectiveness of CrossFake under the\ncross-lingual setting and it also outperforms several monolingual and\ncross-lingual fake news detectors. The dataset is available at\nhttps://github.com/YingtongDou/CrossFake.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jiangshu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1\">Yingtong Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Limeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and the growth of margin. To counteract this deficiency, we\npropose to reward well-classified examples with additive bonuses to revive\ntheir contribution to learning. This counterexample theoretically addresses\nthese three issues. We empirically support this claim by directly verify the\ntheoretical results or through the significant performance improvement with our\ncounterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that\nbecause our idea can solve these three issues, we can deal with complex\nscenarios, such as imbalanced classification, OOD detection, and applications\nunder adversarial attacks. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Speaker-aware Parallel Hierarchical Attentive Encoder-Decoder Model for Multi-turn Dialogue Generation. (arXiv:2110.06823v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06823","description":"<p>This paper presents a novel open-domain dialogue generation model emphasizing\nthe differentiation of speakers in multi-turn conversations. Differing from\nprior work that solely relies on the content of conversation history to\ngenerate a response, we argue that capturing relative social relations among\nutterances (i.e., generated by either the same speaker or different persons)\nbenefits the machine capturing fine-grained context information from a\nconversation history to improve context coherence in the generated response.\nGiven that, we propose a speaker-aware Parallel Hierarchical Attentive\nEncoder-Decoder (PHAED) model that aims to model each utterance with the\nawareness of its speaker and contextual associations with the same speaker's\nprevious messages. Specifically, in a conversation involving two speakers, we\nregard the utterances from one speaker as responses and those from the other as\nqueries. After understanding queries via our encoder with inner-query and\ninter-query encodings, our decoder reuses the hidden states of previously\ngenerated responses, instead of reconstructing these by the encoder, to\ngenerate a new response. Our empirical results show that PHAED outperforms the\nstate-of-the-art in both automatic and human evaluations. Furthermore, our\nablation study shows that dialogue models with speaker tokens can generally\ndecrease the possibility of generating non-coherent responses regarding the\nconversation context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Ming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junli Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HumBugDB: A Large-scale Acoustic Mosquito Dataset. (arXiv:2110.07607v1 [cs.SD])","link":"http://arxiv.org/abs/2110.07607","description":"<p>This paper presents the first large-scale multi-species dataset of acoustic\nrecordings of mosquitoes tracked continuously in free flight. We present 20\nhours of audio recordings that we have expertly labelled and tagged precisely\nin time. Significantly, 18 hours of recordings contain annotations from 36\ndifferent species. Mosquitoes are well-known carriers of diseases such as\nmalaria, dengue and yellow fever. Collecting this dataset is motivated by the\nneed to assist applications which utilise mosquito acoustics to conduct surveys\nto help predict outbreaks and inform intervention policy. The task of detecting\nmosquitoes from the sound of their wingbeats is challenging due to the\ndifficulty in collecting recordings from realistic scenarios. To address this,\nas part of the HumBug project, we conducted global experiments to record\nmosquitoes ranging from those bred in culture cages to mosquitoes captured in\nthe wild. Consequently, the audio recordings vary in signal-to-noise ratio and\ncontain a broad range of indoor and outdoor background environments from\nTanzania, Thailand, Kenya, the USA and the UK. In this paper we describe in\ndetail how we collected, labelled and curated the data. The data is provided\nfrom a PostgreSQL database, which contains important metadata such as the\ncapture method, age, feeding status and gender of the mosquitoes. Additionally,\nwe provide code to extract features and train Bayesian convolutional neural\nnetworks for two key tasks: the identification of mosquitoes from their\ncorresponding background environments, and the classification of detected\nmosquitoes into species. Our extensive dataset is both challenging to machine\nlearning researchers focusing on acoustic identification, and critical to\nentomologists, geo-spatial modellers and other domain experts to understand\nmosquito behaviour, model their distribution, and manage the threat they pose\nto humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiskin_I/0/1/0/all/0/1\">Ivan Kiskin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinka_M/0/1/0/all/0/1\">Marianne Sinka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobb_A/0/1/0/all/0/1\">Adam D. Cobb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rafique_W/0/1/0/all/0/1\">Waqas Rafique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lawrence Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilli_D/0/1/0/all/0/1\">Davide Zilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutteridge_B/0/1/0/all/0/1\">Benjamin Gutteridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dam_R/0/1/0/all/0/1\">Rinita Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinos_T/0/1/0/all/0/1\">Theodoros Marinos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Msaky_D/0/1/0/all/0/1\">Dickson Msaky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaindoa_E/0/1/0/all/0/1\">Emmanuel Kaindoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killeen_G/0/1/0/all/0/1\">Gerard Killeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herreros_Moya_E/0/1/0/all/0/1\">Eva Herreros-Moya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_K/0/1/0/all/0/1\">Kathy J. Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen J. Roberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Structure from 2D Microscopy images using Deep Learning. (arXiv:2110.07608v1 [q-bio.QM])","link":"http://arxiv.org/abs/2110.07608","description":"<p>Understanding the structure of a protein complex is crucial indetermining its\nfunction. However, retrieving accurate 3D structures from microscopy images is\nhighly challenging, particularly as many imaging modalities are\ntwo-dimensional. Recent advances in Artificial Intelligence have been applied\nto this problem, primarily using voxel based approaches to analyse sets of\nelectron microscopy images. Herewe present a deep learning solution for\nreconstructing the protein com-plexes from a number of 2D single molecule\nlocalization microscopy images, with the solution being completely\nunconstrained. Our convolutional neural network coupled with a differentiable\nrenderer predicts pose and derives a single structure. After training, the\nnetwork is dis-carded, with the output of this method being a structural model\nwhich fits the data-set. We demonstrate the performance of our system on two\nprotein complexes: CEP152 (which comprises part of the proximal toroid of the\ncentriole) and centrioles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Blundell_B/0/1/0/all/0/1\">Benjamin J. Blundell</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Sieben_C/0/1/0/all/0/1\">Christian Sieben</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Manley_S/0/1/0/all/0/1\">Suliana Manley</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rosten_E/0/1/0/all/0/1\">Ed Rosten</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chng_Q/0/1/0/all/0/1\">QueeLim Ch&#x27;ng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Cox_S/0/1/0/all/0/1\">Susan Cox</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-contact Atrial Fibrillation Detection from Face Videos by Learning Systolic Peaks. (arXiv:2110.07610v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07610","description":"<p>Objective: We propose a non-contact approach for atrial fibrillation (AF)\ndetection from face videos. Methods: Face videos, electrocardiography (ECG),\nand contact photoplethysmography (PPG) from 100 healthy subjects and 100 AF\npatients are recorded. All the videos in the healthy group are labeled as\nhealthy. Videos in the patient group are labeled as AF, sinus rhythm (SR), or\natrial flutter (AFL) by cardiologists. We use the 3D convolutional neural\nnetwork for remote PPG measurement and propose a novel loss function\n(Wasserstein distance) to use the timing of systolic peaks from contact PPG as\nthe label for our model training. Then a set of heart rate variability (HRV)\nfeatures are calculated from the inter-beat intervals, and a support vector\nmachine (SVM) classifier is trained with HRV features. Results: Our proposed\nmethod can accurately extract systolic peaks from face videos for AF detection.\nThe proposed method is trained with subject-independent 10-fold\ncross-validation with 30s video clips and tested on two tasks. 1)\nClassification of healthy versus AF: the accuracy, sensitivity, and specificity\nare 96.16%, 95.71%, and 96.23%. 2) Classification of SR versus AF: the\naccuracy, sensitivity, and specificity are 95.31%, 98.66%, and 91.11%.\nConclusion: We achieve good performance of non-contact AF detection by learning\nsystolic peaks. Significance: non-contact AF detection can be used for\nself-screening of AF symptom for suspectable populations at home, or\nself-monitoring of AF recurrence after treatment for the chronical patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sun_Z/0/1/0/all/0/1\">Zhaodong Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junttila_J/0/1/0/all/0/1\">Juhani Junttila</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tulppo_M/0/1/0/all/0/1\">Mikko Tulppo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seppanen_T/0/1/0/all/0/1\">Tapio Sepp&#xe4;nen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaobai Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-deep Networks. (arXiv:2110.07641v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07641","description":"<p>Depth is the hallmark of deep neural networks. But more depth means more\nsequential computation and higher latency. This begs the question -- is it\npossible to build high-performing \"non-deep\" neural networks? We show that it\nis. To do so, we use parallel subnetworks instead of stacking one layer after\nanother. This helps effectively reduce depth while maintaining high\nperformance. By utilizing parallel substructures, we show, for the first time,\nthat a network with a depth of just 12 can achieve top-1 accuracy over 80% on\nImageNet, 96% on CIFAR10, and 81% on CIFAR100. We also show that a network with\na low-depth (12) backbone can achieve an AP of 48% on MS-COCO. We analyze the\nscaling rules for our design and show how to increase performance without\nchanging the network's depth. Finally, we provide a proof of concept for how\nnon-deep networks could be used to build low-latency recognition systems. Code\nis available at https://github.com/imankgoyal/NonDeepNetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Ankit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochkovskiy_A/0/1/0/all/0/1\">Alexey Bochkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Detection In Collaborative Learning Environments. (arXiv:2110.07646v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07646","description":"<p>We study the problem of detecting talking activities in collaborative\nlearning videos. Our approach uses head detection and projections of the\nlog-magnitude of optical flow vectors to reduce the problem to a simple\nclassification of small projection images without the need for training\ncomplex, 3-D activity classification systems. The small projection images are\nthen easily classified using a simple majority vote of standard classifiers.\nFor talking detection, our proposed approach is shown to significantly\noutperform single activity systems. We have an overall accuracy of 59% compared\nto 42% for Temporal Segment Network (TSN) and 45% for Convolutional 3D (C3D).\nIn addition, our method is able to detect multiple talking instances from\nmultiple speakers, while also detecting the speakers themselves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1\">Wenjing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattichis_M/0/1/0/all/0/1\">Marios S. Pattichis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celedon_Pattichis_S/0/1/0/all/0/1\">Sylvia Celed&#xf3;n-Pattichis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LopezLeiva_C/0/1/0/all/0/1\">Carlos L&#xf3;pezLeiva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Analysis of CNN Robustness. (arXiv:2110.07667v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07667","description":"<p>While convolutional neural networks (CNNs) have found wide adoption as\nstate-of-the-art models for image-related tasks, their predictions are often\nhighly sensitive to small input perturbations, which the human vision is robust\nagainst. This paper presents Perturber, a web-based application that allows\nusers to instantaneously explore how CNN activations and predictions evolve\nwhen a 3D input scene is interactively perturbed. Perturber offers a large\nvariety of scene modifications, such as camera controls, lighting and shading\neffects, background modifications, object morphing, as well as adversarial\nattacks, to facilitate the discovery of potential vulnerabilities. Fine-tuned\nmodel versions can be directly compared for qualitative evaluation of their\nrobustness. Case studies with machine learning experts have shown that\nPerturber helps users to quickly generate hypotheses about model\nvulnerabilities and to qualitatively compare model behavior. Using quantitative\nanalyses, we could replicate users' insights with other CNN architectures and\ninput images, yielding new insights about the vulnerability of adversarially\ntrained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sietzen_S/0/1/0/all/0/1\">Stefan Sietzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1\">Judy Borowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waldner_M/0/1/0/all/0/1\">Manuela Waldner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Imitation Experience via Equivariant Representations. (arXiv:2110.07668v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07668","description":"<p>The robustness of visual navigation policies trained through imitation often\nhinges on the augmentation of the training image-action pairs. Traditionally,\nthis has been done by collecting data from multiple cameras, by using standard\ndata augmentations from computer vision, such as adding random noise to each\nimage, or by synthesizing training images. In this paper we show that there is\nanother practical alternative for data augmentation for visual navigation based\non extrapolating viewpoint embeddings and actions nearby the ones observed in\nthe training data. Our method makes use of the geometry of the visual\nnavigation problem in 2D and 3D and relies on policies that are functions of\nequivariant embeddings, as opposed to images. Given an image-action pair from a\ntraining navigation dataset, our neural network model predicts the latent\nrepresentations of images at nearby viewpoints, using the equivariance\nproperty, and augments the dataset. We then train a policy on the augmented\ndataset. Our simulation results indicate that policies trained in this way\nexhibit reduced cross-track error, and require fewer interventions compared to\npolicies trained using standard augmentation methods. We also show similar\nresults in autonomous visual navigation by a real ground robot along a path of\nover 500m.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dhruv Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuwajerwala_A/0/1/0/all/0/1\">Alihusein Kuwajerwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shkurti_F/0/1/0/all/0/1\">Florian Shkurti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Appearance Editing with Free-viewpoint Neural Rendering. (arXiv:2110.07674v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07674","description":"<p>We present a neural rendering framework for simultaneous view synthesis and\nappearance editing of a scene from multi-view images captured under known\nenvironment illumination. Existing approaches either achieve view synthesis\nalone or view synthesis along with relighting, without direct control over the\nscene's appearance. Our approach explicitly disentangles the appearance and\nlearns a lighting representation that is independent of it. Specifically, we\nindependently estimate the BRDF and use it to learn a lighting-only\nrepresentation of the scene. Such disentanglement allows our approach to\ngeneralize to arbitrary changes in appearance while performing view synthesis.\nWe show results of editing the appearance of a real scene, demonstrating that\nour approach produces plausible appearance editing. The performance of our view\nsynthesis approach is demonstrated to be at par with state-of-the-art\napproaches on both real and synthetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gera_P/0/1/0/all/0/1\">Pulkit Gera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+KT_A/0/1/0/all/0/1\">Aakash KT</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirikonda_D/0/1/0/all/0/1\">Dhawal Sirikonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakurikar_P/0/1/0/all/0/1\">Parikshit Sakurikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1\">P.J. Narayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shaping embodied agent behavior with activity-context priors from egocentric video. (arXiv:2110.07692v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07692","description":"<p>Complex physical tasks entail a sequence of object interactions, each with\nits own preconditions -- which can be difficult for robotic agents to learn\nefficiently solely through their own experience. We introduce an approach to\ndiscover activity-context priors from in-the-wild egocentric video captured\nwith human worn cameras. For a given object, an activity-context prior\nrepresents the set of other compatible objects that are required for activities\nto succeed (e.g., a knife and cutting board brought together with a tomato are\nconducive to cutting). We encode our video-based prior as an auxiliary reward\nfunction that encourages an agent to bring compatible objects together before\nattempting an interaction. In this way, our model translates everyday human\nexperience into embodied agent skills. We demonstrate our idea using egocentric\nEPIC-Kitchens video of people performing unscripted kitchen activities to\nbenefit virtual household robotic agents performing various complex tasks in\nAI2-iTHOR, significantly accelerating agent learning. Project page:\n<a href=\"http://vision.cs.utexas.edu/projects/ego-rewards/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1\">Tushar Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASK: Adaptively Selecting Key Local Features for RGB-D Scene Recognition. (arXiv:2110.07703v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07703","description":"<p>Indoor scene images usually contain scattered objects and various scene\nlayouts, which make RGB-D scene classification a challenging task. Existing\nmethods still have limitations for classifying scene images with great spatial\nvariability. Thus, how to extract local patch-level features effectively using\nonly image labels is still an open problem for RGB-D scene recognition. In this\npaper, we propose an efficient framework for RGB-D scene recognition, which\nadaptively selects important local features to capture the great spatial\nvariability of scene images. Specifically, we design a differentiable local\nfeature selection (DLFS) module, which can extract the appropriate number of\nkey local scenerelated features. Discriminative local theme-level and\nobject-level representations can be selected with the DLFS module from the\nspatially-correlated multi-modal RGB-D features. We take advantage of the\ncorrelation between RGB and depth modalities to provide more cues for selecting\nlocal features. To ensure that discriminative local features are selected, the\nvariational mutual information maximization loss is proposed. Additionally, the\nDLFS module can be easily extended to select local features of different\nscales. By concatenating the local-orderless and global structured multi-modal\nfeatures, the proposed framework can achieve state-of-the-art performance on\npublic RGB-D scene recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhitong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gray Matter Segmentation in Ultra High Resolution 7 Tesla ex vivo T2w MRI of Human Brain Hemispheres. (arXiv:2110.07711v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07711","description":"<p>Ex vivo MRI of the brain provides remarkable advantages over in vivo MRI for\nvisualizing and characterizing detailed neuroanatomy. However, automated\ncortical segmentation methods in ex vivo MRI are not well developed, primarily\ndue to limited availability of labeled datasets, and heterogeneity in scanner\nhardware and acquisition protocols. In this work, we present a high resolution\n7 Tesla dataset of 32 ex vivo human brain specimens. We benchmark the cortical\nmantle segmentation performance of nine neural network architectures, trained\nand evaluated using manually-segmented 3D patches sampled from specific\ncortical regions, and show excellent generalizing capabilities across whole\nbrain hemispheres in different specimens, and also on unseen images acquired at\ndifferent magnetic field strength and imaging sequences. Finally, we provide\ncortical thickness measurements across key regions in 3D ex vivo human brain\nimages. Our code and processed datasets are publicly available at\nhttps://github.com/Pulkit-Khandelwal/picsl-ex-vivo-segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khandelwal_P/0/1/0/all/0/1\">Pulkit Khandelwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sadaghiani_S/0/1/0/all/0/1\">Shokufeh Sadaghiani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravikumar_S/0/1/0/all/0/1\">Sadhana Ravikumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lim_S/0/1/0/all/0/1\">Sydney Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arezoumandan_S/0/1/0/all/0/1\">Sanaz Arezoumandan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peterson_C/0/1/0/all/0/1\">Claire Peterson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_E/0/1/0/all/0/1\">Eunice Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bedard_M/0/1/0/all/0/1\">Madigan Bedard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Capp_N/0/1/0/all/0/1\">Noah Capp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ittyerah_R/0/1/0/all/0/1\">Ranjit Ittyerah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Migdal_E/0/1/0/all/0/1\">Elyse Migdal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_G/0/1/0/all/0/1\">Grace Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kopp_E/0/1/0/all/0/1\">Emily Kopp</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loja_B/0/1/0/all/0/1\">Bridget Loja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_E/0/1/0/all/0/1\">Eusha Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prabhakaran_K/0/1/0/all/0/1\">Karthik Prabhakaran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mizsei_G/0/1/0/all/0/1\">Gabor Mizsei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gabrielyan_M/0/1/0/all/0/1\">Marianna Gabrielyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schuck_T/0/1/0/all/0/1\">Theresa Schuck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_J/0/1/0/all/0/1\">John Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ohm_D/0/1/0/all/0/1\">Daniel Ohm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_E/0/1/0/all/0/1\">Edward Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Trojanowski_J/0/1/0/all/0/1\">John Q. Trojanowski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McMillan_C/0/1/0/all/0/1\">Corey McMillan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grossman_M/0/1/0/all/0/1\">Murray Grossman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Irwin_D/0/1/0/all/0/1\">David Irwin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tisdall_M/0/1/0/all/0/1\">M. Dylan Tisdall</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Das_S/0/1/0/all/0/1\">Sandhitsu R. Das</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wisse_L/0/1/0/all/0/1\">Laura E.M. Wisse</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wolk_D/0/1/0/all/0/1\">David A. Wolk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yushkevich_P/0/1/0/all/0/1\">Paul A. Yushkevich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Scene Reconstruction and Object Detection System for Assisting Autonomous Vehicle. (arXiv:2110.07716v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07716","description":"<p>In the current computer vision era classifying scenes through video\nsurveillance systems is a crucial task. Artificial Intelligence (AI) Video\nSurveillance technologies have been advanced remarkably while artificial\nintelligence and deep learning ascended into the system. Adopting the superior\ncompounds of deep learning visual classification methods achieved enormous\naccuracy in classifying visual scenes. However, the visual classifiers face\ndifficulties examining the scenes in dark visible areas, especially during the\nnighttime. Also, the classifiers face difficulties in identifying the contexts\nof the scenes. This paper proposed a deep learning model that reconstructs dark\nvisual scenes to clear scenes like daylight, and the method recognizes visual\nactions for the autonomous vehicle. The proposed model achieved 87.3 percent\naccuracy for scene reconstruction and 89.2 percent in scene understanding and\ndetection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1\">Md Foysal Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hay-Youn Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dae-Seong Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Human-guided Conditional Variational Generative Modeling for Automated Urban Planning. (arXiv:2110.07717v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07717","description":"<p>Urban planning designs land-use configurations and can benefit building\nlivable, sustainable, safe communities. Inspired by image generation, deep\nurban planning aims to leverage deep learning to generate land-use\nconfigurations. However, urban planning is a complex process. Existing studies\nusually ignore the need of personalized human guidance in planning, and spatial\nhierarchical structure in planning generation. Moreover, the lack of\nlarge-scale land-use configuration samples poses a data sparsity challenge.\nThis paper studies a novel deep human guided urban planning method to jointly\nsolve the above challenges. Specifically, we formulate the problem into a deep\nconditional variational autoencoder based framework. In this framework, we\nexploit the deep encoder-decoder design to generate land-use configurations. To\ncapture the spatial hierarchy structure of land uses, we enforce the decoder to\ngenerate both the coarse-grained layer of functional zones, and the\nfine-grained layer of POI distributions. To integrate human guidance, we allow\nhumans to describe what they need as texts and use these texts as a model\ncondition input. To mitigate training data sparsity and improve model\nrobustness, we introduce a variational Gaussian embedding mechanism. It not\njust allows us to better approximate the embedding space distribution of\ntraining data and sample a larger population to overcome sparsity, but also\nadds more probabilistic randomness into the urban planning generation to\nimprove embedding diversity so as to improve robustness. Finally, we present\nextensive experiments to validate the enhanced performances of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_P/0/1/0/all/0/1\">Pauline Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Leilei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bowen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack across Datasets. (arXiv:2110.07718v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07718","description":"<p>It has been observed that Deep Neural Networks (DNNs) are vulnerable to\ntransfer attacks in the query-free black-box setting. However, all the previous\nstudies on transfer attack assume that the white-box surrogate models possessed\nby the attacker and the black-box victim models are trained on the same\ndataset, which means the attacker implicitly knows the label set and the input\nsize of the victim model. However, this assumption is usually unrealistic as\nthe attacker may not know the dataset used by the victim model, and further,\nthe attacker needs to attack any randomly encountered images that may not come\nfrom the same dataset. Therefore, in this paper we define a new Generalized\nTransferable Attack (GTA) problem where we assume the attacker has a set of\nsurrogate models trained on different datasets (with different label sets and\nimage sizes), and none of them is equal to the dataset used by the victim\nmodel. We then propose a novel method called Image Classification Eraser (ICE)\nto erase classification information for any encountered images from arbitrary\ndataset. Extensive experiments on Cifar-10, Cifar-100, and TieredImageNet\ndemonstrate the effectiveness of the proposed ICE on the GTA problem.\nFurthermore, we show that existing transfer attack methods can be modified to\ntackle the GTA problem, but with significantly worse performance compared with\nICE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanhao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Certified Patch Robustness via Smoothed Vision Transformers. (arXiv:2110.07719v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07719","description":"<p>Certified patch defenses can guarantee robustness of an image classifier to\narbitrary changes within a bounded contiguous region. But, currently, this\nrobustness comes at a cost of degraded standard accuracies and slower inference\ntimes. We demonstrate how using vision transformers enables significantly\nbetter certified patch robustness that is also more computationally efficient\nand does not incur a substantial drop in standard accuracy. These improvements\nstem from the inherent ability of the vision transformer to gracefully handle\nlargely masked images. Our code is available at\nhttps://github.com/MadryLab/smoothed-vit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander M&#x105;dry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules. (arXiv:2110.07720v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07720","description":"<p>Training from scratch is the most common way to build a Convolutional Neural\nNetwork (CNN) based model. What if we can build new CNN models by reusing parts\nfrom previously build CNN models? What if we can improve a CNN model by\nreplacing (possibly faulty) parts with other parts? In both cases, instead of\ntraining, can we identify the part responsible for each output class (module)\nin the model(s) and reuse or replace only the desired output classes to build a\nmodel? Prior work has proposed decomposing dense-based networks into modules\n(one for each output class) to enable reusability and replaceability in various\nscenarios. However, this work is limited to the dense layers and based on the\none-to-one relationship between the nodes in consecutive layers. Due to the\nshared architecture in the CNN model, prior work cannot be adapted directly. In\nthis paper, we propose to decompose a CNN model used for image classification\nproblems into modules for each output class. These modules can further be\nreused or replaced to build a new model. We have evaluated our approach with\nCIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet\nmodels and found that enabling decomposition comes with a small cost (2.38% and\n0.81% for top-1 and top-5 accuracy, respectively). Also, building a model by\nreusing or replacing modules can be done with a 2.3% and 0.5% average loss of\naccuracy. Furthermore, reusing and replacing these modules reduces CO2e\nemission by ~37 times compared to training the model from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rangeet Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EMDS-7: Environmental Microorganism Image Dataset Seventh Version for Multiple Object Detection Evaluation. (arXiv:2110.07723v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07723","description":"<p>The Environmental Microorganism Image Dataset Seventh Version (EMDS-7) is a\nmicroscopic image data set, including the original Environmental Microorganism\nimages (EMs) and the corresponding object labeling files in \".XML\" format file.\nThe EMDS-7 data set consists of 41 types of EMs, which has a total of 2365\nimages and 13216 labeled objects. The EMDS-7 database mainly focuses on the\nobject detection. In order to prove the effectiveness of EMDS-7, we select the\nmost commonly used deep learning methods (Faster-RCNN, YOLOv3, YOLOv4, SSD and\nRetinaNet) and evaluation indices for testing and evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hechen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Ao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_H/0/1/0/all/0/1\">Hongzan Sunand Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multifocal Stereoscopic Projection Mapping. (arXiv:2110.07726v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07726","description":"<p>Stereoscopic projection mapping (PM) allows a user to see a three-dimensional\n(3D) computer-generated (CG) object floating over physical surfaces of\narbitrary shapes around us using projected imagery. However, the current\nstereoscopic PM technology only satisfies binocular cues and is not capable of\nproviding correct focus cues, which causes a vergence--accommodation conflict\n(VAC). Therefore, we propose a multifocal approach to mitigate VAC in\nstereoscopic PM. Our primary technical contribution is to attach electrically\nfocus-tunable lenses (ETLs) to active shutter glasses to control both vergence\nand accommodation. Specifically, we apply fast and periodical focal sweeps to\nthe ETLs, which causes the \"virtual image'\" (as an optical term) of a scene\nobserved through the ETLs to move back and forth during each sweep period. A 3D\nCG object is projected from a synchronized high-speed projector only when the\nvirtual image of the projected imagery is located at a desired distance. This\nprovides an observer with the correct focus cues required. In this study, we\nsolve three technical issues that are unique to stereoscopic PM: (1) The 3D CG\nobject is displayed on non-planar and even moving surfaces; (2) the physical\nsurfaces need to be shown without the focus modulation; (3) the shutter glasses\nadditionally need to be synchronized with the ETLs and the projector. We also\ndevelop a novel compensation technique to deal with the \"lens breathing\"\nartifact that varies the retinal size of the virtual image through focal length\nmodulation. Further, using a proof-of-concept prototype, we demonstrate that\nour technique can present the virtual image of a target 3D CG object at the\ncorrect depth. Finally, we validate the advantage provided by our technique by\ncomparing it with conventional stereoscopic PM using a user study on a\ndepth-matching task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kimura_S/0/1/0/all/0/1\">Sorashi Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwai_D/0/1/0/all/0/1\">Daisuke Iwai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punpongsanon_P/0/1/0/all/0/1\">Parinya Punpongsanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_K/0/1/0/all/0/1\">Kosuke Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning of Neural Collision Handler for Complex 3D Mesh Deformations. (arXiv:2110.07727v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07727","description":"<p>We present a robust learning algorithm to detect and handle collisions in 3D\ndeforming meshes. Our collision detector is represented as a bilevel deep\nautoencoder with an attention mechanism that identifies colliding mesh\nsub-parts. We use a numerical optimization algorithm to resolve penetrations\nguided by the network. Our learned collision handler can resolve collisions for\nunseen, high-dimensional meshes with thousands of vertices. To obtain stable\nnetwork performance in such large and unseen spaces, we progressively insert\nnew collision data based on the errors in network inferences. We automatically\nlabel these data using an analytical collision detector and progressively\nfine-tune our detection networks. We evaluate our method for collision handling\nof complex, 3D meshes coming from several datasets with different shapes and\ntopologies, including datasets corresponding to dressed and undressed human\nposes, cloth simulations, and human hand poses acquired using multiview capture\nsystems. Our approach outperforms supervised learning methods and achieves\n$93.8-98.1\\%$ accuracy compared to the groundtruth by analytic methods.\nCompared to prior learning methods, our approach results in a $5.16\\%-25.50\\%$\nlower false negative rate in terms of collision checking and a $9.65\\%-58.91\\%$\nhigher success rate in collision handling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qingyang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zherong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_B/0/1/0/all/0/1\">Breannan Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training Molecular Graph Representation with 3D Geometry. (arXiv:2110.07728v1 [q-bio.QM])","link":"http://arxiv.org/abs/2110.07728","description":"<p>Molecular graph representation learning is a fundamental problem in modern\ndrug and material discovery. Molecular graphs are typically modeled by their 2D\ntopological structures, but it has been recently discovered that 3D geometric\ninformation plays a more vital role in predicting molecular functionalities.\nHowever, the lack of 3D information in real-world scenarios has significantly\nimpeded the learning of geometric graph representation. To cope with this\nchallenge, we propose the Graph Multi-View Pre-training (GraphMVP) framework\nwhere self-supervised learning (SSL) is performed by leveraging the\ncorrespondence and consistency between 2D topological structures and 3D\ngeometric views. GraphMVP effectively learns a 2D molecular graph encoder that\nis enhanced by richer and more discriminative 3D geometry. We further provide\ntheoretical insights to justify the effectiveness of GraphMVP. Finally,\ncomprehensive experiments show that GraphMVP can consistently outperform\nexisting graph SSL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_S/0/1/0/all/0/1\">Shengchao Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lasenby_J/0/1/0/all/0/1\">Joan Lasenby</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Classification: Directly Training Spiking Neural Networks for Semantic Segmentation. (arXiv:2110.07742v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07742","description":"<p>Spiking Neural Networks (SNNs) have recently emerged as the low-power\nalternative to Artificial Neural Networks (ANNs) because of their sparse,\nasynchronous, and binary event-driven processing. Due to their energy\nefficiency, SNNs have a high possibility of being deployed for real-world,\nresource-constrained systems such as autonomous vehicles and drones. However,\nowing to their non-differentiable and complex neuronal dynamics, most previous\nSNN optimization methods have been limited to image recognition. In this paper,\nwe explore the SNN applications beyond classification and present semantic\nsegmentation networks configured with spiking neurons. Specifically, we first\ninvestigate two representative SNN optimization techniques for recognition\ntasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic\nsegmentation datasets. We observe that, when converted from ANNs, SNNs suffer\nfrom high latency and low performance due to the spatial variance of features.\nTherefore, we directly train networks with surrogate gradient learning,\nresulting in lower latency and higher performance than ANN-SNN conversion.\nMoreover, we redesign two fundamental ANN segmentation architectures (i.e.,\nFully Convolutional Networks and DeepLab) for the SNN domain. We conduct\nexperiments on two public semantic segmentation benchmarks including the PASCAL\nVOC2012 dataset and the DDD17 event-based dataset. In addition to showing the\nfeasibility of SNNs for semantic segmentation, we show that SNNs can be more\nrobust and energy-efficient compared to their ANN counterparts in this domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chough_J/0/1/0/all/0/1\">Joshua Chough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1\">Priyadarshini Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep learning model for classification of diabetic retinopathy in eye fundus images based on retinal lesion detection. (arXiv:2110.07745v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07745","description":"<p>Diabetic retinopathy (DR) is the result of a complication of diabetes\naffecting the retina. It can cause blindness, if left undiagnosed and\nuntreated. An ophthalmologist performs the diagnosis by screening each patient\nand analyzing the retinal lesions via ocular imaging. In practice, such\nanalysis is time-consuming and cumbersome to perform. This paper presents a\nmodel for automatic DR classification on eye fundus images. The approach\nidentifies the main ocular lesions related to DR and subsequently diagnoses the\nillness. The proposed method follows the same workflow as the clinicians,\nproviding information that can be interpreted clinically to support the\nprediction. A subset of the kaggle EyePACS and the Messidor-2 datasets, labeled\nwith ocular lesions, is made publicly available. The kaggle EyePACS subset is\nused as a training set and the Messidor-2 as a test set for lesions and DR\nclassification models. For DR diagnosis, our model has an area-under-the-curve,\nsensitivity, and specificity of 0.948, 0.886, and 0.875, respectively, which\ncompetes with state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+delaPava_M/0/1/0/all/0/1\">Melissa delaPava</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rios_H/0/1/0/all/0/1\">Hern&#xe1;n R&#xed;os</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodriguez_F/0/1/0/all/0/1\">Francisco J. Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Perdomo_O/0/1/0/all/0/1\">Oscar J. Perdomo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gonzalez_F/0/1/0/all/0/1\">Fabio A. Gonz&#xe1;lez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Knights\": First Place Submission for VIPriors21 Action Recognition Challenge at ICCV 2021. (arXiv:2110.07758v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07758","description":"<p>This technical report presents our approach \"Knights\" to solve the action\nrecognition task on a small subset of Kinetics-400 i.e. Kinetics400ViPriors\nwithout using any extra-data. Our approach has 3 main components:\nstate-of-the-art Temporal Contrastive self-supervised pretraining, video\ntransformer models, and optical flow modality. Along with the use of standard\ntest-time augmentation, our proposed solution achieves 73% on\nKinetics400ViPriors test set, which is the best among all of the other entries\nVisual Inductive Priors for Data-Efficient Computer Vision's Action Recognition\nChallenge, ICCV 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dave_I/0/1/0/all/0/1\">Ishan Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biyani_N/0/1/0/all/0/1\">Naman Biyani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_B/0/1/0/all/0/1\">Brandon Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Rohit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Reconstruction of Curvilinear Structures with Stereo Matching DeepConvolutional Neural Networks. (arXiv:2110.07766v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07766","description":"<p>Curvilinear structures frequently appear in microscopy imaging as the object\nof interest. Crystallographic defects, i.e., dislocations, are one of the\ncurvilinear structures that have been repeatedly investigated under\ntransmission electron microscopy (TEM) and their 3D structural information is\nof great importance for understanding the properties of materials. 3D\ninformation of dislocations is often obtained by tomography which is a\ncumbersome process since it is required to acquire many images with different\ntilt angles and similar imaging conditions. Although, alternative stereoscopy\nmethods lower the number of required images to two, they still require human\nintervention and shape priors for accurate 3D estimation. We propose a fully\nautomated pipeline for both detection and matching of curvilinear structures in\nstereo pairs by utilizing deep convolutional neural networks (CNNs) without\nmaking any prior assumption on 3D shapes. In this work, we mainly focus on 3D\nreconstruction of dislocations from stereo pairs of TEM images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Altingovde_O/0/1/0/all/0/1\">Okan Alting&#xf6;vde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishchuk_A/0/1/0/all/0/1\">Anastasiia Mishchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganeeva_G/0/1/0/all/0/1\">Gulnaz Ganeeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oveisi_E/0/1/0/all/0/1\">Emad Oveisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_C/0/1/0/all/0/1\">Cecile Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Homomorphic Encryption in Medical Imaging. (arXiv:2110.07768v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07768","description":"<p>In this technical report, we explore the use of homomorphic encryption (HE)\nin the context of training and predicting with deep learning (DL) models to\ndeliver strict \\textit{Privacy by Design} services, and to enforce a zero-trust\nmodel of data governance. First, we show how HE can be used to make predictions\nover medical images while preventing unauthorized secondary use of data, and\ndetail our results on a disease classification task with OCT images. Then, we\ndemonstrate that HE can be used to secure the training of DL models through\nfederated learning, and report some experiments using 3D chest CT-Scans for a\nnodule detection task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dutil_F/0/1/0/all/0/1\">Francis Dutil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+See_A/0/1/0/all/0/1\">Alexandre See</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jorio_L/0/1/0/all/0/1\">Lisa Di Jorio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandelier_F/0/1/0/all/0/1\">Florent Chandelier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4D flight trajectory prediction using a hybrid Deep Learning prediction method based on ADS-B technology: a case study of Hartsfield-Jackson Atlanta International Airport(ATL). (arXiv:2110.07774v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07774","description":"<p>The core of any flight schedule is the trajectories. In particular, 4D\ntrajectories are the most crucial component for flight attribute prediction. In\nparticular, 4D trajectories are the most crucial component for flight attribute\nprediction. Each trajectory contains spatial and temporal features that are\nassociated with uncertainties that make the prediction process complex. Today\nbecause of the increasing demand for air transportation, it is compulsory for\nairports and airlines to have an optimized schedule to use all of the airport's\ninfrastructure potential. This is possible using advanced trajectory prediction\nmethods. This paper proposes a novel hybrid deep learning model to extract the\nspatial and temporal features considering the uncertainty of the prediction\nmodel for Hartsfield-Jackson Atlanta International Airport(ATL). Automatic\nDependent Surveillance-Broadcast (ADS-B) data are used as input to the models.\nThis research is conducted in three steps: (a) data preprocessing; (b)\nprediction by a hybrid Convolutional Neural Network and Gated Recurrent Unit\n(CNN-GRU) along with a 3D-CNN model; (c) The third and last step is the\ncomparison of the model's performance with the proposed model by comparing the\nexperimental results. The deep model uncertainty is considered using the\nMont-Carlo dropout (MC-Dropout). Mont-Carlo dropouts are added to the network\nlayers to enhance the model's prediction performance by a robust approach of\nswitching off between different neurons. The results show that the proposed\nmodel has low error measurements compared to the other models (i.e., 3D CNN,\nCNN-GRU). The model with MC-dropout reduces the error further by an average of\n21 %.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahfienya_H/0/1/0/all/0/1\">Hesam Sahfienya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Regan_A/0/1/0/all/0/1\">Amelia C. Regan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuroView: Explainable Deep Network Decision Making. (arXiv:2110.07778v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07778","description":"<p>Deep neural networks (DNs) provide superhuman performance in numerous\ncomputer vision tasks, yet it remains unclear exactly which of a DN's units\ncontribute to a particular decision. NeuroView is a new family of DN\narchitectures that are interpretable/explainable by design. Each member of the\nfamily is derived from a standard DN architecture by vector quantizing the unit\noutput values and feeding them into a global linear classifier. The resulting\narchitecture establishes a direct, causal link between the state of each unit\nand the classification decision. We validate NeuroView on standard datasets and\nclassification tasks to show that how its unit/class mapping aids in\nunderstanding the decision-making process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barberan_C/0/1/0/all/0/1\">CJ Barberan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Improved Semi-Supervised Semantic Segmentation in Satellite Images. (arXiv:2110.07782v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07782","description":"<p>Remote sensing data is crucial for applications ranging from monitoring\nforest fires and deforestation to tracking urbanization. Most of these tasks\nrequire dense pixel-level annotations for the model to parse visual information\nfrom limited labeled data available for these satellite images. Due to the\ndearth of high-quality labeled training data in this domain, there is a need to\nfocus on semi-supervised techniques. These techniques generate pseudo-labels\nfrom a small set of labeled examples which are used to augment the labeled\ntraining set. This makes it necessary to have a highly representative and\ndiverse labeled training set. Therefore, we propose to use an active\nlearning-based sampling strategy to select a highly representative set of\nlabeled training data. We demonstrate our proposed method's effectiveness on\ntwo existing semantic segmentation datasets containing satellite images: UC\nMerced Land Use Classification Dataset and DeepGlobe Land Cover Classification\nDataset. We report a 27% improvement in mIoU with as little as 2% labeled data\nusing active learning sampling strategies over randomly sampling the small set\nof labeled training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shasvat Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_D/0/1/0/all/0/1\">Debasmita Ghose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DG-Labeler and DGL-MOTS Dataset: Boost the Autonomous Driving Perception. (arXiv:2110.07790v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07790","description":"<p>Multi-object tracking and segmentation (MOTS) is a critical task for\nautonomous driving applications. The existing MOTS studies face two critical\nchallenges: 1) the published datasets inadequately capture the real-world\ncomplexity for network training to address various driving settings; 2) the\nworking pipeline annotation tool is under-studied in the literature to improve\nthe quality of MOTS learning examples. In this work, we introduce the\nDG-Labeler and DGL-MOTS dataset to facilitate the training data annotation for\nthe MOTS task and accordingly improve network training accuracy and efficiency.\nDG-Labeler uses the novel Depth-Granularity Module to depict the instance\nspatial relations and produce fine-grained instance masks. Annotated by\nDG-Labeler, our DGL-MOTS dataset exceeds the prior effort (i.e., KITTI MOTS and\nBDD100K) in data diversity, annotation quality, and temporal representations.\nResults on extensive cross-dataset evaluations indicate significant performance\nimprovements for several state-of-the-art methods trained on our DGL-MOTS\ndataset. We believe our DGL-MOTS Dataset and DG-Labeler hold the valuable\npotential to boost the visual perception of future transportation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiwen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yixin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xingyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_F/0/1/0/all/0/1\">Feng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongfang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Occupancy Estimation from Thermal Images. (arXiv:2110.07796v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07796","description":"<p>We propose a non-intrusive, and privacy-preserving occupancy estimation\nsystem for smart environments. The proposed scheme uses thermal images to\ndetect the number of people in a given area. The occupancy estimation model is\ndesigned using the concepts of intensity-based and motion-based human\nsegmentation. The notion of difference catcher, connected component labeling,\nnoise filter, and memory propagation are utilized to estimate the occupancy\nnumber. We use a real dataset to demonstrate the effectiveness of the proposed\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zishan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaki_D/0/1/0/all/0/1\">Dipankar Chaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhdari_A/0/1/0/all/0/1\">Abdallah Lakhdari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abusafia_A/0/1/0/all/0/1\">Amani Abusafia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouguettaya_A/0/1/0/all/0/1\">Athman Bouguettaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EFENet: Reference-based Video Super-Resolution with Enhanced Flow Estimation. (arXiv:2110.07797v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07797","description":"<p>In this paper, we consider the problem of reference-based video\nsuper-resolution(RefVSR), i.e., how to utilize a high-resolution (HR) reference\nframe to super-resolve a low-resolution (LR) video sequence. The existing\napproaches to RefVSR essentially attempt to align the reference and the input\nsequence, in the presence of resolution gap and long temporal range. However,\nthey either ignore temporal structure within the input sequence, or suffer\naccumulative alignment errors. To address these issues, we propose EFENet to\nexploit simultaneously the visual cues contained in the HR reference and the\ntemporal information contained in the LR sequence. EFENet first globally\nestimates cross-scale flow between the reference and each LR frame. Then our\nnovel flow refinement module of EFENet refines the flow regarding the furthest\nframe using all the estimated flows, which leverages the global temporal\ninformation within the sequence and therefore effectively reduces the alignment\nerrors. We provide comprehensive evaluations to validate the strengths of our\napproach, and to demonstrate that the proposed framework outperforms the\nstate-of-the-art methods. Code is available at\nhttps://github.com/IndigoPurple/EFENet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mengqi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Purification through Representation Disentanglement. (arXiv:2110.07801v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07801","description":"<p>Deep learning models are vulnerable to adversarial examples and make\nincomprehensible mistakes, which puts a threat on their real-world deployment.\nCombined with the idea of adversarial training, preprocessing-based defenses\nare popular and convenient to use because of their task independence and good\ngeneralizability. Current defense methods, especially purification, tend to\nremove ``noise\" by learning and recovering the natural images. However,\ndifferent from random noise, the adversarial patterns are much easier to be\noverfitted during model training due to their strong correlation to the images.\nIn this work, we propose a novel adversarial purification scheme by presenting\ndisentanglement of natural images and adversarial perturbations as a\npreprocessing defense. With extensive experiments, our defense is shown to be\ngeneralizable and make significant protection against unseen strong adversarial\nattacks. It reduces the success rates of state-of-the-art \\textbf{ensemble}\nattacks from \\textbf{61.7\\%} to \\textbf{14.9\\%} on average, superior to a\nnumber of existing methods. Notably, our defense restores the perturbed images\nperfectly and does not hurt the clean accuracy of backbone models, which is\nhighly desirable in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Lanqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bihan Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTQ-SL: Exploring the Sub-layerwise Post-training Quantization. (arXiv:2110.07809v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07809","description":"<p>Network quantization is a powerful technique to compress convolutional neural\nnetworks. The quantization granularity determines how to share the scaling\nfactors in weights, which affects the performance of network quantization. Most\nexisting approaches share the scaling factors layerwisely or channelwisely for\nquantization of convolutional layers. Channelwise quantization and layerwise\nquantization have been widely used in various applications. However, other\nquantization granularities are rarely explored. In this paper, we will explore\nthe sub-layerwise granularity that shares the scaling factor across multiple\ninput and output channels. We propose an efficient post-training quantization\nmethod in sub-layerwise granularity (PTQ-SL). Then we systematically experiment\non various granularities and observe that the prediction accuracy of the\nquantized neural network has a strong correlation with the granularity.\nMoreover, we find that adjusting the position of the channels can improve the\nperformance of sub-layerwise quantization. Therefore, we propose a method to\nreorder the channels for sub-layerwise quantization. The experiments\ndemonstrate that the sub-layerwise quantization with appropriate channel\nreordering can outperform the channelwise quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chenhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenguang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiankun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiankun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gait-based Frailty Assessment using Image Representation of IMU Signals and Deep CNN. (arXiv:2110.07821v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07821","description":"<p>Frailty is a common and critical condition in elderly adults, which may lead\nto further deterioration of health. However, difficulties and complexities\nexist in traditional frailty assessments based on activity-related\nquestionnaires. These can be overcome by monitoring the effects of frailty on\nthe gait. In this paper, it is shown that by encoding gait signals as images,\ndeep learning-based models can be utilized for the classification of gait type.\nTwo deep learning models (a) SS-CNN, based on single stride input images, and\n(b) MS-CNN, based on 3 consecutive strides were proposed. It was shown that\nMS-CNN performs best with an accuracy of 85.1\\%, while SS-CNN achieved an\naccuracy of 77.3\\%. This is because MS-CNN can observe more features\ncorresponding to stride-to-stride variations which is one of the key symptoms\nof frailty. Gait signals were encoded as images using STFT, CWT, and GAF. While\nthe MS-CNN model using GAF images achieved the best overall accuracy and\nprecision, CWT has a slightly better recall. This study demonstrates how image\nencoded gait data can be used to exploit the full potential of deep learning\nCNN models for the assessment of frailty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arshad_M/0/1/0/all/0/1\">Muhammad Zeeshan Arshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dawoon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Mina Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_H/0/1/0/all/0/1\">Hyungeun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinwook Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mun_K/0/1/0/all/0/1\">Kyung-Ryoul Mun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding and Improving Robustness of Vision Transformers through Patch-based Negative Augmentation. (arXiv:2110.07858v1 [cs.LG])","link":"http://arxiv.org/abs/2110.07858","description":"<p>We investigate the robustness of vision transformers (ViTs) through the lens\nof their special patch-based architectural structure, i.e., they process an\nimage as a sequence of image patches. We find that ViTs are surprisingly\ninsensitive to patch-based transformations, even when the transformation\nlargely destroys the original semantics and makes the image unrecognizable by\nhumans. This indicates that ViTs heavily use features that survived such\ntransformations but are generally not indicative of the semantic class to\nhumans. Further investigations show that these features are useful but\nnon-robust, as ViTs trained on them can achieve high in-distribution accuracy,\nbut break down under distribution shifts. From this understanding, we ask: can\ntraining the model to rely less on these features improve ViT robustness and\nout-of-distribution performance? We use the images transformed with our\npatch-based operations as negatively augmented views and offer losses to\nregularize the training away from using non-robust features. This is a\ncomplementary view to existing research that mostly focuses on augmenting\ninputs with semantic-preserving transformations to enforce models' invariance.\nWe show that patch-based negative augmentation consistently improves robustness\nof ViTs across a wide set of ImageNet based robustness benchmarks. Furthermore,\nwe find our patch-based negative augmentation are complementary to traditional\n(positive) data augmentation, and together boost the performance further. All\nthe code in this work will be open-sourced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1\">Balaji Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1\">Alex Beutel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Receptive Field Broadening and Boosting for Salient Object Detection. (arXiv:2110.07859v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07859","description":"<p>Salient object detection requires a comprehensive and scalable receptive\nfield to locate the visually significant objects in the image. Recently, the\nemergence of visual transformers and multi-branch modules has significantly\nenhanced the ability of neural networks to perceive objects at different\nscales. However, compared to the traditional backbone, the calculation process\nof transformers is time-consuming. Moreover, different branches of the\nmulti-branch modules could cause the same error back propagation in each\ntraining iteration, which is not conducive to extracting discriminative\nfeatures. To solve these problems, we propose a bilateral network based on\ntransformer and CNN to efficiently broaden local details and global semantic\ninformation simultaneously. Besides, a Multi-Head Boosting (MHB) strategy is\nproposed to enhance the specificity of different network branches. By\ncalculating the errors of different prediction heads, each branch can\nseparately pay more attention to the pixels that other branches predict\nincorrectly. Moreover, Unlike multi-path parallel training, MHB randomly\nselects one branch each time for gradient back propagation in a boosting way.\nAdditionally, an Attention Feature Fusion Module (AF) is proposed to fuse two\ntypes of features according to respective characteristics. Comprehensive\nexperiments on five benchmark datasets demonstrate that the proposed method can\nachieve a significant performance improvement compared with the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingcan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Changqun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chenxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaowu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single volume lung biomechanics from chest computed tomography using a mode preserving generative adversarial network. (arXiv:2110.07878v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07878","description":"<p>Local tissue expansion of the lungs is typically derived by registering\ncomputed tomography (CT) scans acquired at multiple lung volumes. However,\nacquiring multiple scans incurs increased radiation dose, time, and cost, and\nmay not be possible in many cases, thus restricting the applicability of\nregistration-based biomechanics. We propose a generative adversarial learning\napproach for estimating local tissue expansion directly from a single CT scan.\nThe proposed framework was trained and evaluated on 2500 subjects from the\nSPIROMICS cohort. Once trained, the framework can be used as a\nregistration-free method for predicting local tissue expansion. We evaluated\nmodel performance across varying degrees of disease severity and compared its\nperformance with two image-to-image translation frameworks - UNet and Pix2Pix.\nOur model achieved an overall PSNR of 18.95 decibels, SSIM of 0.840, and\nSpearman's correlation of 0.61 at a high spatial resolution of 1 mm3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chaudhary_M/0/1/0/all/0/1\">Muhammad F. A. Chaudhary</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerard_S/0/1/0/all/0/1\">Sarah E. Gerard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Christensen_G/0/1/0/all/0/1\">Gary E. Christensen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cooper_C/0/1/0/all/0/1\">Christopher B. Cooper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schroeder_J/0/1/0/all/0/1\">Joyce D. Schroeder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffman_E/0/1/0/all/0/1\">Eric A. Hoffman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reinhardt_J/0/1/0/all/0/1\">Joseph M. Reinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advances and Challenges in Deep Lip Reading. (arXiv:2110.07879v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07879","description":"<p>Driven by deep learning techniques and large-scale datasets, recent years\nhave witnessed a paradigm shift in automatic lip reading. While the main thrust\nof Visual Speech Recognition (VSR) was improving accuracy of Audio Speech\nRecognition systems, other potential applications, such as biometric\nidentification, and the promised gains of VSR systems, have motivated extensive\nefforts on developing the lip reading technology. This paper provides a\ncomprehensive survey of the state-of-the-art deep learning based VSR research\nwith a focus on data challenges, task-specific complications, and the\ncorresponding solutions. Advancements in these directions will expedite the\ntransformation of silent speech interface from theory to practice. We also\ndiscuss the main modules of a VSR pipeline and the influential datasets.\nFinally, we introduce some typical VSR application concerns and impediments to\nreal-world scenarios as well as future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oghbaie_M/0/1/0/all/0/1\">Marzieh Oghbaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabaghi_A/0/1/0/all/0/1\">Arian Sabaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemifard_K/0/1/0/all/0/1\">Kooshan Hashemifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akbari_M/0/1/0/all/0/1\">Mohammad Akbari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolyNet: Polynomial Neural Network for 3D Shape Recognition with PolyShape Representation. (arXiv:2110.07882v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07882","description":"<p>3D shape representation and its processing have substantial effects on 3D\nshape recognition. The polygon mesh as a 3D shape representation has many\nadvantages in computer graphics and geometry processing. However, there are\nstill some challenges for the existing deep neural network (DNN)-based methods\non polygon mesh representation, such as handling the variations in the degree\nand permutations of the vertices and their pairwise distances. To overcome\nthese challenges, we propose a DNN-based method (PolyNet) and a specific\npolygon mesh representation (PolyShape) with a multi-resolution structure.\nPolyNet contains two operations; (1) a polynomial convolution (PolyConv)\noperation with learnable coefficients, which learns continuous distributions as\nthe convolutional filters to share the weights across different vertices, and\n(2) a polygonal pooling (PolyPool) procedure by utilizing the multi-resolution\nstructure of PolyShape to aggregate the features in a much lower dimension. Our\nexperiments demonstrate the strength and the advantages of PolyNet on both 3D\nshape classification and retrieval tasks compared to existing polygon\nmesh-based methods and its superiority in classifying graph representations of\nimages. The code is publicly available from\nhttps://myavartanoo.github.io/polynet/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1\">Mohsen Yavartanoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_S/0/1/0/all/0/1\">Shih-Hsuan Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1\">Reyhaneh Neshatavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Unsupervised Domain Adaptive Re-Identification via Source-Guided Selection of Pseudo-Labeling Hyperparameters. (arXiv:2110.07897v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07897","description":"<p>Unsupervised Domain Adaptation (UDA) for re-identification (re-ID) is a\nchallenging task: to avoid a costly annotation of additional data, it aims at\ntransferring knowledge from a domain with annotated data to a domain of\ninterest with only unlabeled data. Pseudo-labeling approaches have proven to be\neffective for UDA re-ID. However, the effectiveness of these approaches heavily\ndepends on the choice of some hyperparameters (HP) that affect the generation\nof pseudo-labels by clustering. The lack of annotation in the domain of\ninterest makes this choice non-trivial. Current approaches simply reuse the\nsame empirical value for all adaptation tasks and regardless of the target data\nrepresentation that changes through pseudo-labeling training phases. As this\nsimplistic choice may limit their performance, we aim at addressing this issue.\nWe propose new theoretical grounds on HP selection for clustering UDA re-ID as\nwell as method of automatic and cyclic HP tuning for pseudo-labeling UDA\nclustering: HyPASS. HyPASS consists in incorporating two modules in\npseudo-labeling methods: (i) HP selection based on a labeled source validation\nset and (ii) conditional domain alignment of feature discriminativeness to\nimprove HP selection based on source samples. Experiments on commonly used\nperson re-ID and vehicle re-ID datasets show that our proposed HyPASS\nconsistently improves the best state-of-the-art methods in re-ID compared to\nthe commonly used empirical HP setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubourvieux_F/0/1/0/all/0/1\">Fabian Dubourvieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1\">Ang&#xe9;lique Loesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainouz_S/0/1/0/all/0/1\">Samia Ainouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canu_S/0/1/0/all/0/1\">St&#xe9;phane Canu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Infer Kinematic Hierarchies for Novel Object Instances. (arXiv:2110.07911v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07911","description":"<p>Manipulating an articulated object requires perceiving itskinematic\nhierarchy: its parts, how each can move, and howthose motions are coupled.\nPrevious work has explored per-ception for kinematics, but none infers a\ncomplete kinematichierarchy on never-before-seen object instances, without\nrelyingon a schema or template. We present a novel perception systemthat\nachieves this goal. Our system infers the moving parts ofan object and the\nkinematic couplings that relate them. Toinfer parts, it uses a point cloud\ninstance segmentation neuralnetwork and to infer kinematic hierarchies, it uses\na graphneural network to predict the existence, direction, and typeof edges\n(i.e. joints) that relate the inferred parts. We trainthese networks using\nsimulated scans of synthetic 3D models.We evaluate our system on simulated\nscans of 3D objects, andwe demonstrate a proof-of-concept use of our system to\ndrivereal-world robotic manipulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Rashid_H/0/1/0/all/0/1\">Hameed Abdul-Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_M/0/1/0/all/0/1\">Miles Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbatematteo_B/0/1/0/all/0/1\">Ben Abbatematteo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1\">Daniel Ritchie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining CNNs With Transformer for Multimodal 3D MRI Brain Tumor Segmentation With Self-Supervised Pretraining. (arXiv:2110.07919v1 [eess.IV])","link":"http://arxiv.org/abs/2110.07919","description":"<p>We apply an ensemble of modified TransBTS, nnU-Net, and a combination of both\nfor the segmentation task of the BraTS 2021 challenge. In fact, we change the\noriginal architecture of the TransBTS model by adding Squeeze-and-Excitation\nblocks, an increasing number of CNN layers, replacing positional encoding in\nTransformer block with a learnable Multilayer Perceptron (MLP) embeddings,\nwhich makes Transformer adjustable to any input size during inference. With\nthese modifications, we are able to largely improve TransBTS performance.\nInspired by a nnU-Net framework we decided to combine it with our modified\nTransBTS by changing the architecture inside nnU-Net to our custom model. On\nthe Validation set of BraTS 2021, the ensemble of these approaches achieves\n0.8496, 0.8698, 0.9256 Dice score and 15.72, 11.057, 3.374 HD95 for enhancing\ntumor, tumor core, and whole tumor, correspondingly. Our code is publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dobko_M/0/1/0/all/0/1\">Mariia Dobko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kolinko_D/0/1/0/all/0/1\">Danylo-Ivan Kolinko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Viniavskyi_O/0/1/0/all/0/1\">Ostap Viniavskyi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yelisieiev_Y/0/1/0/all/0/1\">Yurii Yelisieiev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Generation using Texture Co-occurrence and Spatial Self-Similarity for Debiasing. (arXiv:2110.07920v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07920","description":"<p>Classification models trained on biased datasets usually perform poorly on\nout-of-distribution samples since biased representations are embedded into the\nmodel. Recently, adversarial learning methods have been proposed to disentangle\nbiased representations, but it is challenging to discard only the biased\nfeatures without altering other relevant information. In this paper, we propose\na novel de-biasing approach that explicitly generates additional images using\ntexture representations of oppositely labeled images to enlarge the training\ndataset and mitigate the effect of biases when training a classifier. Every new\ngenerated image contains similar spatial information from a source image while\ntransferring textures from a target image of opposite label. Our model\nintegrates a texture co-occurrence loss that determines whether a generated\nimage's texture is similar to that of the target, and a spatial self-similarity\nloss that determines whether the spatial details between the generated and\nsource images are well preserved. Both generated and original training images\nare further used to train a classifier that is able to avoid learning unknown\nbias representations. We employ three distinct artificially designed datasets\nwith known biases to demonstrate the ability of our method to mitigate bias\ninformation, and report competitive performance over existing state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Myeongkyun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_D/0/1/0/all/0/1\">Dongkyu Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luna_M/0/1/0/all/0/1\">Miguel Luna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Kyung Soo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">June Hong Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sang Hyun Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Preserving Triplet Mining for Stabilizing the Triplet Loss in Vehicle Re-identification. (arXiv:2110.07933v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07933","description":"<p>Object appearances often change dramatically with pose variations. This\ncreates a challenge for embedding schemes that seek to map instances with the\nsame object ID to locations that are as close as possible. This issue becomes\nsignificantly heightened in complex computer vision tasks such as\nre-identification(re-id). In this paper, we suggest these dramatic appearance\nchanges are indications that an object ID is composed of multiple natural\ngroups and it is counter-productive to forcefully map instances from different\ngroups to a common location. This leads us to introduce Relation Preserving\nTriplet Mining (RPTM), a feature matching guided triplet mining scheme, that\nensures triplets will respect the natural sub-groupings within an object ID. We\nuse this triplet mining mechanism to establish a pose-aware, well-conditioned\ntriplet cost function. This allows a single network to be trained with fixed\nparameters across three challenging benchmarks, while still providing\nstate-of-the-art re-identification results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Adhiraj Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugalingam_K/0/1/0/all/0/1\">Kuruparan Shanmugalingam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wen-Yan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Generating Identifiable Virtual Faces. (arXiv:2110.07986v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07986","description":"<p>Face anonymization with generative models have become increasingly prevalent\nsince they sanitize private information by generating virtual face images,\nensuring both privacy and image utility. Such virtual face images are usually\nnot identifiable after the removal or protection of the original identity. In\nthis paper, we formalize and tackle the problem of generating identifiable\nvirtual face images. Our virtual face images are visually different from the\noriginal ones for privacy protection. In addition, they are bound with new\nvirtual identities, which can be directly used for face recognition. We propose\nan Identifiable Virtual Face Generator (IVFG) to generate the virtual face\nimages. The IVFG projects the latent vectors of the original face images into\nvirtual ones according to a user specific key, based on which the virtual face\nimages are generated. To make the virtual face images identifiable, we propose\na multi-task learning objective as well as a triplet styled training strategy\nto learn the IVFG. Various experiments demonstrate the effectiveness of the\nIVFG for generate identifiable virtual face images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuowen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose-guided Generative Adversarial Net for Novel View Action Synthesis. (arXiv:2110.07993v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07993","description":"<p>We focus on the problem of novel-view human action synthesis. Given an action\nvideo, the goal is to generate the same action from an unseen viewpoint.\nNaturally, novel view video synthesis is more challenging than image synthesis.\nIt requires the synthesis of a sequence of realistic frames with temporal\ncoherency. Besides, transferring the different actions to a novel target view\nrequires awareness of action category and viewpoint change simultaneously. To\naddress these challenges, we propose a novel framework named Pose-guided Action\nSeparable Generative Adversarial Net (PAS-GAN), which utilizes pose to\nalleviate the difficulty of this task. First, we propose a recurrent\npose-transformation module which transforms actions from the source view to the\ntarget view and generates novel view pose sequence in 2D coordinate space.\nSecond, a well-transformed pose sequence enables us to separatethe action and\nbackground in the target view. We employ a novel local-global spatial\ntransformation module to effectively generate sequential video features in the\ntarget view using these action and background features. Finally, the generated\nvideo features are used to synthesize human action with the help of a 3D\ndecoder. Moreover, to focus on dynamic action in the video, we propose a novel\nmulti-scale action-separable loss which further improves the video quality. We\nconduct extensive experiments on two large-scale multi-view human action\ndatasets, NTU-RGBD and PKU-MMD, demonstrating the effectiveness of PAS-GAN\nwhich outperforms existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xianhang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunchang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S Rawat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramid Correlation based Deep Hough Voting for Visual Object Tracking. (arXiv:2110.07994v1 [cs.CV])","link":"http://arxiv.org/abs/2110.07994","description":"<p>Most of the existing Siamese-based trackers treat tracking problem as a\nparallel task of classification and regression. However, some studies show that\nthe sibling head structure could lead to suboptimal solutions during the\nnetwork training. Through experiments we find that, without regression, the\nperformance could be equally promising as long as we delicately design the\nnetwork to suit the training objective. We introduce a novel voting-based\nclassification-only tracking algorithm named Pyramid Correlation based Deep\nHough Voting (short for PCDHV), to jointly locate the top-left and bottom-right\ncorners of the target. Specifically we innovatively construct a Pyramid\nCorrelation module to equip the embedded feature with fine-grained local\nstructures and global spatial contexts; The elaborately designed Deep Hough\nVoting module further take over, integrating long-range dependencies of pixels\nto perceive corners; In addition, the prevalent discretization gap is simply\nyet effectively alleviated by increasing the spatial resolution of the feature\nmaps while exploiting channel-space relationships. The algorithm is general,\nrobust and simple. We demonstrate the effectiveness of the module through a\nseries of ablation experiments. Without bells and whistles, our tracker\nachieves better or comparable performance to the SOTA algorithms on three\nchallenging benchmarks (TrackingNet, GOT-10k and LaSOT) while running at a\nreal-time speed of 80 FPS. Codes and models will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tingfa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shenwang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junjie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaGNET: Uniform Sampling from Deep Generative Network Manifolds Without Retraining. (arXiv:2110.08009v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08009","description":"<p>Deep Generative Networks (DGNs) are extensively employed in Generative\nAdversarial Networks (GANs), Variational Autoencoders (VAEs), and their\nvariants to approximate the data manifold, and data distribution on that\nmanifold. However, training samples are often obtained based on preferences,\ncosts, or convenience producing artifacts in the empirical data distribution\ne.g., the large fraction of smiling faces in the CelebA dataset or the large\nfraction of dark-haired individuals in FFHQ. These inconsistencies will be\nreproduced when sampling from the trained DGN, which has far-reaching potential\nimplications for fairness, data augmentation, anomaly detection, domain\nadaptation, and beyond. In response, we develop a differential geometry based\nsampler -- coined MaGNET -- that, given any trained DGN, produces samples that\nare uniformly distributed on the learned manifold. We prove theoretically and\nempirically that our technique produces a uniform distribution on the manifold\nregardless of the training set distribution. We perform a range of experiments\non various datasets and DGNs. One of them considers the state-of-the-art\nStyleGAN2 trained on FFHQ dataset, where uniform sampling via MaGNET increases\ndistribution precision and recall by 4.1% &amp; 3.0% and decreases gender bias by\n41.2%, without requiring labels or retraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayun_A/0/1/0/all/0/1\">Ahmed Imtiaz Humayun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Channel and Weight Pruning for Model Acceleration on Moblie Devices. (arXiv:2110.08013v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08013","description":"<p>For practical deep neural network design on mobile devices, it is essential\nto consider the constraints incurred by the computational resources and the\ninference latency in various applications. Among deep network acceleration\nrelated approaches, pruning is a widely adopted practice to balance the\ncomputational resource consumption and the accuracy, where unimportant\nconnections can be removed either channel-wisely or randomly with a minimal\nimpact on model accuracy. The channel pruning instantly results in a\nsignificant latency reduction, while the random weight pruning is more flexible\nto balance the latency and accuracy. In this paper, we present a unified\nframework with Joint Channel pruning and Weight pruning (JCW), and achieves a\nbetter Pareto-frontier between the latency and accuracy than previous model\ncompression approaches. To fully optimize the trade-off between the latency and\naccuracy, we develop a tailored multi-objective evolutionary algorithm in the\nJCW framework, which enables one single search to obtain the optimal candidate\narchitectures for various deployment requirements. Extensive experiments\ndemonstrate that the JCW achieves a better trade-off between the latency and\naccuracy against various state-of-the-art pruning methods on the ImageNet\nclassification dataset. Our codes are available at\nhttps://github.com/jcw-anonymous/JCW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Sheryl Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tensor-to-Image: Image-to-Image Translation with Vision Transformers. (arXiv:2110.08037v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08037","description":"<p>Transformers gain huge attention since they are first introduced and have a\nwide range of applications. Transformers start to take over all areas of deep\nlearning and the Vision transformers paper also proved that they can be used\nfor computer vision tasks. In this paper, we utilized a vision\ntransformer-based custom-designed model, tensor-to-image, for the image to\nimage translation. With the help of self-attention, our model was able to\ngeneralize and apply to different problems without a single modification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunduc_Y/0/1/0/all/0/1\">Yi&#x11f;it G&#xfc;nd&#xfc;&#xe7;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on ML Defense Models Competition. (arXiv:2110.08042v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08042","description":"<p>Due to the vulnerability of deep neural networks (DNNs) to adversarial\nexamples, a large number of defense techniques have been proposed to alleviate\nthis problem in recent years. However, the progress of building more robust\nmodels is usually hampered by the incomplete or incorrect robustness\nevaluation. To accelerate the research on reliable evaluation of adversarial\nrobustness of the current defense models in image classification, the TSAIL\ngroup at Tsinghua University and the Alibaba Security group organized this\ncompetition along with a CVPR 2021 workshop on adversarial machine learning\n(https://aisecure-workshop.github.io/amlcvpr2021/). The purpose of this\ncompetition is to motivate novel attack algorithms to evaluate adversarial\nrobustness more effectively and reliably. The participants were encouraged to\ndevelop stronger white-box attack algorithms to find the worst-case robustness\nof different defenses. This competition was conducted on an adversarial\nrobustness evaluation platform -- ARES (https://github.com/thu-ml/ares), and is\nheld on the TianChi platform\n(https://tianchi.aliyun.com/competition/entrance/531847/introduction) as one of\nthe series of AI Security Challengers Program. After the competition, we\nsummarized the results and established a new adversarial robustness benchmark\nat https://ml.cs.tsinghua.edu.cn/ares-bench/, which allows users to upload\nadversarial attack algorithms and defense models for evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qi-An Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiayu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">XiaoFeng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yunrui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xitong Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Daquan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiadong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhennan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1\">Jiequan Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaogang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengguang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Layer Pseudo-Supervision for Histopathology Tissue Semantic Segmentation using Patch-level Classification Labels. (arXiv:2110.08048v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08048","description":"<p>Tissue-level semantic segmentation is a vital step in computational\npathology. Fully-supervised models have already achieved outstanding\nperformance with dense pixel-level annotations. However, drawing such labels on\nthe giga-pixel whole slide images is extremely expensive and time-consuming. In\nthis paper, we use only patch-level classification labels to achieve tissue\nsemantic segmentation on histopathology images, finally reducing the annotation\nefforts. We proposed a two-step model including a classification and a\nsegmentation phases. In the classification phase, we proposed a CAM-based model\nto generate pseudo masks by patch-level labels. In the segmentation phase, we\nachieved tissue semantic segmentation by our proposed Multi-Layer\nPseudo-Supervision. Several technical novelties have been proposed to reduce\nthe information gap between pixel-level and patch-level annotations. As a part\nof this paper, we introduced a new weakly-supervised semantic segmentation\n(WSSS) dataset for lung adenocarcinoma (LUAD-HistoSeg). We conducted several\nexperiments to evaluate our proposed model on two datasets. Our proposed model\noutperforms two state-of-the-art WSSS approaches. Note that we can achieve\ncomparable quantitative and qualitative results with the fully-supervised\nmodel, with only around a 2\\% gap for MIoU and FwIoU. By comparing with manual\nlabeling, our model can greatly save the annotation time from hours to minutes.\nThe source code is available at: \\url{https://github.com/ChuHan89/WSSS-Tissue}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Jinhai Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingling Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_S/0/1/0/all/0/1\">Su Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_L/0/1/0/all/0/1\">Lixu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Huan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zeyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xiaomei Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_G/0/1/0/all/0/1\">Guoqiang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_C/0/1/0/all/0/1\">Changhong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes. (arXiv:2110.08059v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08059","description":"<p>When designing Convolutional Neural Networks (CNNs), one must select the size\nof the convolutional kernels before training. Recent works show CNNs benefit\nfrom different kernel sizes at different layers, but exploring all possible\ncombinations is unfeasible in practice. A more efficient approach is to learn\nthe kernel size during training. However, existing works that learn the kernel\nsize have a limited bandwidth. These approaches scale kernels by dilation, and\nthus the detail they can describe is limited. In this work, we propose\nFlexConv, a novel convolutional operation with which high bandwidth\nconvolutional kernels of learnable kernel size can be learned at a fixed\nparameter cost. FlexNets model long-term dependencies without the use of\npooling, achieve state-of-the-art performance on several sequential datasets,\noutperform recent works with learned kernel sizes, and are competitive with\nmuch deeper ResNets on image benchmark datasets. Additionally, FlexNets can be\ndeployed at higher resolutions than those seen during training. To avoid\naliasing, we propose a novel kernel parameterization with which the frequency\nof the kernels can be analytically controlled. Our novel kernel\nparameterization shows higher descriptive power and faster convergence speed\nthan existing parameterizations. This leads to important improvements in\nclassification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1\">David W. Romero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruintjes_R/0/1/0/all/0/1\">Robert-Jan Bruintjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomczak_J/0/1/0/all/0/1\">Jakub M. Tomczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik J. Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogendoorn_M/0/1/0/all/0/1\">Mark Hoogendoorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reliable Shot Identification for Complex Event Detection via Visual-Semantic Embedding. (arXiv:2110.08063v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08063","description":"<p>Multimedia event detection is the task of detecting a specific event of\ninterest in an user-generated video on websites. The most fundamental challenge\nfacing this task lies in the enormously varying quality of the video as well as\nthe high-level semantic abstraction of event inherently. In this paper, we\ndecompose the video into several segments and intuitively model the task of\ncomplex event detection as a multiple instance learning problem by representing\neach video as a \"bag\" of segments in which each segment is referred to as an\ninstance. Instead of treating the instances equally, we associate each instance\nwith a reliability variable to indicate its importance and then select reliable\ninstances for training. To measure the reliability of the varying instances\nprecisely, we propose a visual-semantic guided loss by exploiting low-level\nfeature from visual information together with instance-event similarity based\nhigh-level semantic feature. Motivated by curriculum learning, we introduce a\nnegative elastic-net regularization term to start training the classifier with\ninstances of high reliability and gradually taking the instances with\nrelatively low reliability into consideration. An alternative optimization\nalgorithm is developed to solve the proposed challenging non-convex non-smooth\nproblem. Experimental results on standard datasets, i.e., TRECVID MEDTest 2013\nand TRECVID MEDTest 2014, demonstrate the effectiveness and superiority of the\nproposed method to the baseline algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Minnan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chen Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Quality Control of Vacuum Insulated Glazing by Convolutional Neural Network Image Classification. (arXiv:2110.08079v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08079","description":"<p>Vacuum Insulated Glazing (VIG) is a highly thermally insulating window\ntechnology, which boasts an extremely thin profile and lower weight as compared\nto gas-filled insulated glazing units of equivalent performance. The VIG is a\ndouble-pane configuration with a submillimeter vacuum gap between the panes and\ntherefore under constant atmospheric pressure over their service life. Small\npillars are positioned between the panes to maintain the gap, which can damage\nthe glass reducing the lifetime of the VIG unit. To efficiently assess any\nsurface damage on the glass, an automated damage detection system is highly\ndesirable. For the purpose of classifying the damage, we have developed,\ntrained, and tested a deep learning computer vision system using convolutional\nneural networks. The classification model flawlessly classified the test\ndataset with an area under the curve (AUC) for the receiver operating\ncharacteristic (ROC) of 100%. We have automatically cropped the images down to\ntheir relevant information by using Faster-RCNN to locate the position of the\npillars. We employ the state-of-the-art methods Grad-CAM and Score-CAM of\nexplainable Artificial Intelligence (XAI) to provide an understanding of the\ninternal mechanisms and were able to show that our classifier outperforms\nResNet50V2 for identification of crack locations and geometry. The proposed\nmethods can therefore be used to detect systematic defects even without large\namounts of training data. Further analyses of our model's predictive\ncapabilities demonstrates its superiority over state-of-the-art models\n(ResNet50V2, ResNet101V2 and ResNet152V2) in terms of convergence speed,\naccuracy, precision at 100% recall and AUC for ROC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riedel_H/0/1/0/all/0/1\">Henrik Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokdad_S/0/1/0/all/0/1\">Sleheddine Mokdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulz_I/0/1/0/all/0/1\">Isabell Schulz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocer_C/0/1/0/all/0/1\">Cenk Kocer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosendahl_P/0/1/0/all/0/1\">Philipp Rosendahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1\">Jens Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Michael A. Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drass_M/0/1/0/all/0/1\">Michael Drass</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Aggregation Network for Fast MR Imaging. (arXiv:2110.08080v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08080","description":"<p>Magnetic resonance (MR) imaging is a commonly used scanning technique for\ndisease detection, diagnosis and treatment monitoring. Although it is able to\nproduce detailed images of organs and tissues with better contrast, it suffers\nfrom a long acquisition time, which makes the image quality vulnerable to say\nmotion artifacts. Recently, many approaches have been developed to reconstruct\nfull-sampled images from partially observed measurements in order to accelerate\nMR imaging. However, most of these efforts focus on reconstruction over a\nsingle modality or simple fusion of multiple modalities, neglecting the\ndiscovery of correlation knowledge at different feature level. In this work, we\npropose a novel Multi-modal Aggregation Network, named MANet, which is capable\nof discovering complementary representations from a fully sampled auxiliary\nmodality, with which to hierarchically guide the reconstruction of a given\ntarget modality. In our MANet, the representations from the fully sampled\nauxiliary and undersampled target modalities are learned independently through\na specific network. Then, a guided attention module is introduced in each\nconvolutional stage to selectively aggregate multi-modal features for better\nreconstruction, yielding comprehensive, multi-scale, multi-modal feature\nfusion. Moreover, our MANet follows a hybrid domain learning framework, which\nallows it to simultaneously recover the frequency signal in the $k$-space\ndomain as well as restore the image details from the image domain. Extensive\nexperiments demonstrate the superiority of the proposed approach over\nstate-of-the-art MR image reconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fe_H/0/1/0/all/0/1\">Huazhu Fe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of Lung CT Scores of Systemic Sclerosis by Cascaded Regression Neural Networks. (arXiv:2110.08085v1 [eess.IV])","link":"http://arxiv.org/abs/2110.08085","description":"<p>Visually scoring lung involvement in systemic sclerosis from CT scans plays\nan important role in monitoring progression, but its labor intensiveness\nhinders practical application. We proposed, therefore, an automatic scoring\nframework that consists of two cascaded deep regression neural networks. The\nfirst (3D) network aims to predict the craniocaudal position of five\nanatomically defined scoring levels on the 3D CT scans. The second (2D) network\nreceives the resulting 2D axial slices and predicts the scores. We used 227 3D\nCT scans to train and validate the first network, and the resulting 1135 axial\nslices were used in the second network. Two experts scored independently a\nsubset of data to obtain intra- and interobserver variabilities and the ground\ntruth for all data was obtained in consensus. To alleviate the unbalance in\ntraining labels in the second network, we introduced a sampling technique and\nto increase the diversity of the training samples synthetic data was generated,\nmimicking ground glass and reticulation patterns. The 4-fold cross validation\nshowed that our proposed network achieved an average MAE of 5.90, 4.66 and\n4.49, weighted kappa of 0.66, 0.58 and 0.65 for total score (TOT), ground glass\n(GG) and reticular pattern (RET), respectively. Our network performed slightly\nworse than the best experts on TOT and GG prediction but it has competitive\nperformance on RET prediction and has the potential to be an objective\nalternative for the visual scoring of SSc in CT thorax studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1\">Jingnan Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Staring_M/0/1/0/all/0/1\">Marius Staring</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hernandez_Giron_I/0/1/0/all/0/1\">Irene Hern&#xe1;ndez-Gir&#xf3;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kroft_L/0/1/0/all/0/1\">Lucia J.M. Kroft</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schouffoer_A/0/1/0/all/0/1\">Anne A. Schouffoer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoel_B/0/1/0/all/0/1\">Berend C. Stoel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trade-offs of Local SGD at Scale: An Empirical Study. (arXiv:2110.08133v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08133","description":"<p>As datasets and models become increasingly large, distributed training has\nbecome a necessary component to allow deep neural networks to train in\nreasonable amounts of time. However, distributed training can have substantial\ncommunication overhead that hinders its scalability. One strategy for reducing\nthis overhead is to perform multiple unsynchronized SGD steps independently on\neach worker between synchronization steps, a technique known as local SGD. We\nconduct a comprehensive empirical study of local SGD and related methods on a\nlarge-scale image classification task. We find that performing local SGD comes\nat a price: lower communication costs (and thereby faster training) are\naccompanied by lower accuracy. This finding is in contrast from the\nsmaller-scale experiments in prior work, suggesting that local SGD encounters\nchallenges at scale. We further show that incorporating the slow momentum\nframework of Wang et al. (2020) consistently improves accuracy without\nrequiring additional communication, hinting at future directions for\npotentially escaping this trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1\">Jose Javier Gonzalez Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Mike Rabbat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Tailed, Multi-Headed, Spatial Dynamic Memory refined Text-to-Image Synthesis. (arXiv:2110.08143v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08143","description":"<p>Synthesizing high-quality, realistic images from text-descriptions is a\nchallenging task, and current methods synthesize images from text in a\nmulti-stage manner, typically by first generating a rough initial image and\nthen refining image details at subsequent stages. However, existing methods\nthat follow this paradigm suffer from three important limitations. Firstly,\nthey synthesize initial images without attempting to separate image attributes\nat a word-level. As a result, object attributes of initial images (that provide\na basis for subsequent refinement) are inherently entangled and ambiguous in\nnature. Secondly, by using common text-representations for all regions, current\nmethods prevent us from interpreting text in fundamentally different ways at\ndifferent parts of an image. Different image regions are therefore only allowed\nto assimilate the same type of information from text at each refinement stage.\nFinally, current methods generate refinement features only once at each\nrefinement stage and attempt to address all image aspects in a single shot.\nThis single-shot refinement limits the precision with which each refinement\nstage can learn to improve the prior image. Our proposed method introduces\nthree novel components to address these shortcomings: (1) An initial generation\nstage that explicitly generates separate sets of image features for each word\nn-gram. (2) A spatial dynamic memory module for refinement of images. (3) An\niterative multi-headed mechanism to make it easier to improve upon multiple\nimage aspects. Experimental results demonstrate that our Multi-Headed Spatial\nDynamic Memory image refinement with our Multi-Tailed Word-level Initial\nGeneration (MSMT-GAN) performs favourably against the previous state of the art\non the CUB and COCO datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seshadri_A/0/1/0/all/0/1\">Amrit Diggavi Seshadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravindran_B/0/1/0/all/0/1\">Balaraman Ravindran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Fine-grained Layout Analysis for the Historical Tibetan Document Based on the Instance Segmentation. (arXiv:2110.08164v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08164","description":"<p>Accurate layout analysis without subsequent text-line segmentation remains an\nongoing challenge, especially when facing the Kangyur, a kind of historical\nTibetan document featuring considerable touching components and mottled\nbackground. Aiming at identifying different regions in document images, layout\nanalysis is indispensable for subsequent procedures such as character\nrecognition. However, there was only a little research being carried out to\nperform line-level layout analysis which failed to deal with the Kangyur. To\nobtain the optimal results, a fine-grained sub-line level layout analysis\napproach is presented. Firstly, we introduced an accelerated method to build\nthe dataset which is dynamic and reliable. Secondly, enhancement had been made\nto the SOLOv2 according to the characteristics of the Kangyur. Then, we fed the\nenhanced SOLOv2 with the prepared annotation file during the training phase.\nOnce the network is trained, instances of the text line, sentence, and titles\ncan be segmented and identified during the inference stage. The experimental\nresults show that the proposed method delivers a decent 72.7% AP on our\ndataset. In general, this preliminary research provides insights into the\nfine-grained sub-line level layout analysis and testifies the SOLOv2-based\napproaches. We also believe that the proposed methods can be adopted on other\nlanguage documents with various layouts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Penghai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhengqi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuqi Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The World of an Octopus: How Reporting Bias Influences a Language Model's Perception of Color. (arXiv:2110.08182v1 [cs.CL])","link":"http://arxiv.org/abs/2110.08182","description":"<p>Recent work has raised concerns about the inherent limitations of text-only\npretraining. In this paper, we first demonstrate that reporting bias, the\ntendency of people to not state the obvious, is one of the causes of this\nlimitation, and then investigate to what extent multimodal training can\nmitigate this issue. To accomplish this, we 1) generate the Color Dataset\n(CoDa), a dataset of human-perceived color distributions for 521 common\nobjects; 2) use CoDa to analyze and compare the color distribution found in\ntext, the distribution captured by language models, and a human's perception of\ncolor; and 3) investigate the performance differences between text-only and\nmultimodal models on CoDa. Our results show that the distribution of colors\nthat a language model recovers correlates more strongly with the inaccurate\ndistribution found in text than with the ground-truth, supporting the claim\nthat reporting bias negatively impacts and inherently limits text-only\ntraining. We then demonstrate that multimodal models can leverage their visual\ntraining to mitigate these effects, providing a promising avenue for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paik_C/0/1/0/all/0/1\">Cory Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aroca_Ouellette_S/0/1/0/all/0/1\">St&#xe9;phane Aroca-Ouellette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roncone_A/0/1/0/all/0/1\">Alessandro Roncone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1\">Katharina Kann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crop Rotation Modeling for Deep Learning-Based Parcel Classification from Satellite Time Series. (arXiv:2110.08187v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08187","description":"<p>While annual crop rotations play a crucial role for agricultural\noptimization, they have been largely ignored for automated crop type mapping.\nIn this paper, we take advantage of the increasing quantity of annotated\nsatellite data to propose the first deep learning approach modeling\nsimultaneously the inter- and intra-annual agricultural dynamics of parcel\nclassification. Along with simple training adjustments, our model provides an\nimprovement of over 6.6 mIoU points over the current state-of-the-art of crop\nclassification. Furthermore, we release the first large-scale multi-year\nagricultural dataset with over 300,000 annotated parcels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quinton_F/0/1/0/all/0/1\">F&#xe9;lix Quinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guided Point Contrastive Learning for Semi-supervised Point Cloud Semantic Segmentation. (arXiv:2110.08188v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08188","description":"<p>Rapid progress in 3D semantic segmentation is inseparable from the advances\nof deep network models, which highly rely on large-scale annotated data for\ntraining. To address the high cost and challenges of 3D point-level labeling,\nwe present a method for semi-supervised point cloud semantic segmentation to\nadopt unlabeled point clouds in training to boost the model performance.\nInspired by the recent contrastive loss in self-supervised tasks, we propose\nthe guided point contrastive loss to enhance the feature representation and\nmodel generalization ability in semi-supervised setting. Semantic predictions\non unlabeled point clouds serve as pseudo-label guidance in our loss to avoid\nnegative pairs in the same category. Also, we design the confidence guidance to\nensure high-quality feature learning. Besides, a category-balanced sampling\nstrategy is proposed to collect positive and negative samples to mitigate the\nclass imbalance problem. Extensive experiments on three datasets (ScanNet V2,\nS3DIS, and SemanticKITTI) show the effectiveness of our semi-supervised method\nto improve the prediction quality with unlabeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shaoshuai Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chi-Wing Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention meets Geometry: Geometry Guided Spatial-Temporal Attention for Consistent Self-Supervised Monocular Depth Estimation. (arXiv:2110.08192v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08192","description":"<p>Inferring geometrically consistent dense 3D scenes across a tuple of\ntemporally consecutive images remains challenging for self-supervised monocular\ndepth prediction pipelines. This paper explores how the increasingly popular\ntransformer architecture, together with novel regularized loss formulations,\ncan improve depth consistency while preserving accuracy. We propose a spatial\nattention module that correlates coarse depth predictions to aggregate local\ngeometric information. A novel temporal attention mechanism further processes\nthe local geometric information in a global context across consecutive images.\nAdditionally, we introduce geometric constraints between frames regularized by\nphotometric cycle consistency. By combining our proposed regularization and the\nnovel spatial-temporal-attention module we fully leverage both the geometric\nand appearance-based consistency across monocular frames. This yields\ngeometrically meaningful attention and improves temporal depth stability and\naccuracy compared to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruhkamp_P/0/1/0/all/0/1\">Patrick Ruhkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daoyi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanzhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shared Visual Representations of Drawing for Communication: How do different biases affect human interpretability and intent?. (arXiv:2110.08203v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08203","description":"<p>We present an investigation into how representational losses can affect the\ndrawings produced by artificial agents playing a communication game. Building\nupon recent advances, we show that a combination of powerful pretrained encoder\nnetworks, with appropriate inductive biases, can lead to agents that draw\nrecognisable sketches, whilst still communicating well. Further, we start to\ndevelop an approach to help automatically analyse the semantic content being\nconveyed by a sketch and demonstrate that current approaches to inducing\nperceptual biases lead to a notion of objectness being a key feature despite\nthe agent training being self-supervised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1\">Daniela Mihai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1\">Jonathon Hare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Diverse Feature Priors. (arXiv:2110.08220v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08220","description":"<p>To improve model generalization, model designers often restrict the features\nthat their models use, either implicitly or explicitly. In this work, we\nexplore the design space of leveraging such feature priors by viewing them as\ndistinct perspectives on the data. Specifically, we find that models trained\nwith diverse sets of feature priors have less overlapping failure modes, and\ncan thus be combined more effectively. Moreover, we demonstrate that jointly\ntraining such models on additional (unlabeled) data allows them to correct each\nother's mistakes, which, in turn, leads to better generalization and resilience\nto spurious correlations. Code available at\nhttps://github.com/MadryLab/copriors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsipras_D/0/1/0/all/0/1\">Dimitris Tsipras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Visual Question Generation. (arXiv:2110.08226v1 [cs.LG])","link":"http://arxiv.org/abs/2110.08226","description":"<p>In traditional Visual Question Generation (VQG), most images have multiple\nconcepts (e.g. objects and categories) for which a question could be generated,\nbut models are trained to mimic an arbitrary choice of concept as given in\ntheir training data. This makes training difficult and also poses issues for\nevaluation -- multiple valid questions exist for most images but only one or a\nfew are captured by the human references. We present Guiding Visual Question\nGeneration - a variant of VQG which conditions the question generator on\ncategorical information based on expectations on the type of question and the\nobjects it should explore. We propose two variants: (i) an explicitly guided\nmodel that enables an actor (human or automated) to select which objects and\ncategories to generate a question for; and (ii) an implicitly guided model that\nlearns which objects and categories to condition on, based on discrete latent\nvariables. The proposed models are evaluated on an answer-category augmented\nVQA dataset and our quantitative results show a substantial improvement over\nthe current state of the art (over 9 BLEU-4 increase). Human evaluation\nvalidates that guidance helps the generation of questions that are\ngrammatically coherent and relevant to the given image and objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vedd_N/0/1/0/all/0/1\">Nihir Vedd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fire Together Wire Together: A Dynamic Pruning Approach with Self-Supervised Mask Prediction. (arXiv:2110.08232v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08232","description":"<p>Dynamic model pruning is a recent direction that allows for the inference of\na different sub-network for each input sample during deployment. However,\ncurrent dynamic methods rely on learning a continuous channel gating through\nregularization by inducing sparsity loss. This formulation introduces\ncomplexity in balancing different losses (e.g task loss, regularization loss).\nIn addition, regularization-based methods lack transparent tradeoff\nhyperparameter selection to realize computational budget. Our contribution is\ntwofold: 1) decoupled task and pruning training. 2) Simple hyperparameter\nselection that enables FLOPs reduction estimation before training. We propose\nto predict a mask to process k filters in a layer based on the activation of\nits previous layer. We pose the problem as a self-supervised binary\nclassification problem. Each mask predictor module is trained to predict if the\nlog-likelihood of each filter in the current layer belongs to the top-k\nactivated filters. The value k is dynamically estimated for each input based on\na novel criterion using the mass of heatmaps. We show experiments on several\nneural architectures, such as VGG, ResNet, and MobileNet on CIFAR and ImageNet\ndatasets. On CIFAR, we reach similar accuracy to SOTA methods with 15% and 24%\nhigher FLOPs reduction. Similarly in ImageNet, we achieve a lower drop in\naccuracy with up to 13% improvement in FLOPs reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elkerdawy_S/0/1/0/all/0/1\">Sara Elkerdawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoushi_M/0/1/0/all/0/1\">Mostafa Elhoushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1\">Nilanjan Ray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Dubber: Dubbing for Silent Videos According to Scripts. (arXiv:2110.08243v1 [eess.AS])","link":"http://arxiv.org/abs/2110.08243","description":"<p>Dubbing is a post-production process of re-recording actors' dialogues, which\nis extensively used in filmmaking and video production. It is usually performed\nmanually by professional voice actors who read lines with proper prosody, and\nin synchronization with the pre-recorded videos. In this work, we propose\nNeural Dubber, the first neural network model to solve a novel automatic video\ndubbing (AVD) task: synthesizing human speech synchronized with the given\nsilent video from the text. Neural Dubber is a multi-modal text-to-speech (TTS)\nmodel that utilizes the lip movement in the video to control the prosody of the\ngenerated speech. Furthermore, an image-based speaker embedding (ISE) module is\ndeveloped for the multi-speaker setting, which enables Neural Dubber to\ngenerate speech with a reasonable timbre according to the speaker's face.\nExperiments on the chemistry lecture single-speaker dataset and LRS2\nmulti-speaker dataset show that Neural Dubber can generate speech audios on par\nwith state-of-the-art TTS models in terms of speech quality. Most importantly,\nboth qualitative and quantitative evaluations show that Neural Dubber can\ncontrol the prosody of synthesized speech by the video, and generate\nhigh-fidelity speech temporally synchronized with the video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_C/0/1/0/all/0/1\">Chenxu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_Q/0/1/0/all/0/1\">Qiao Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1\">Tingle Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuping Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance, Successes and Limitations of Deep Learning Semantic Segmentation of Multiple Defects in Transmission Electron Micrographs. (arXiv:2110.08244v1 [cs.CV])","link":"http://arxiv.org/abs/2110.08244","description":"<p>In this work, we perform semantic segmentation of multiple defect types in\nelectron microscopy images of irradiated FeCrAl alloys using a deep learning\nMask Regional Convolutional Neural Network (Mask R-CNN) model. We conduct an\nin-depth analysis of key model performance statistics, with a focus on\nquantities such as predicted distributions of defect shapes, defect sizes, and\ndefect areal densities relevant to informing modeling and understanding of\nirradiated Fe-based materials properties. To better understand the performance\nand present limitations of the model, we provide examples of useful evaluation\ntests which include a suite of random splits, and dataset size-dependent and\ndomain-targeted cross validation tests. Overall, we find that the current model\nis a fast, effective tool for automatically characterizing and quantifying\nmultiple defect types in microscopy images, with a level of accuracy on par\nwith human domain expert labelers. More specifically, the model can achieve\naverage defect identification F1 scores as high as 0.8, and, based on random\ncross validation, have low overall average (+/- standard deviation) defect size\nand density percentage errors of 7.3 (+/- 3.8)% and 12.7 (+/- 5.3)%,\nrespectively. Further, our model predicts the expected material hardening to\nwithin 10-20 MPa (about 10% of total hardening), which is about the same error\nlevel as experiments. Our targeted evaluation tests also suggest the best path\ntoward improving future models is not expanding existing databases with more\nlabeled images but instead data additions that target weak points of the model\ndomain, such as images from different microscopes, imaging conditions,\nirradiation environments, and alloy types. Finally, we discuss the first phase\nof an effort to provide an easy-to-use, open-source object detection tool to\nthe broader community for identifying defects in new images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_R/0/1/0/all/0/1\">Ryan Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Mingren Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_W/0/1/0/all/0/1\">Wei Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greaves_J/0/1/0/all/0/1\">Jacob RC Greaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donglin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zeming Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zitong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Field_K/0/1/0/all/0/1\">Kevin G. Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_D/0/1/0/all/0/1\">Dane Morgan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Stream Dynamic Video Summarization. (arXiv:1812.00108v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1812.00108","description":"<p>With vast amounts of video content being uploaded to the Internet every\nminute, video summarization becomes critical for efficient browsing, searching,\nand indexing of visual content. Nonetheless, the spread of social and\negocentric cameras creates an abundance of sparse scenarios captured by several\ndevices, and ultimately required to be jointly summarized. In this paper, we\ndiscuss the problem of summarizing videos recorded independently by several\ndynamic cameras that intermittently share the field of view. We present a\nrobust framework that (a) identifies a diverse set of important events among\nmoving cameras that often are not capturing the same scene, and (b) selects the\nmost representative view(s) at each event to be included in a universal\nsummary. Due to the lack of an applicable alternative, we collected a new\nmulti-view egocentric dataset, Multi-Ego. Our dataset is recorded\nsimultaneously by three cameras, covering a wide variety of real-life\nscenarios. The footage is annotated by multiple individuals under various\nsummarization configurations, with a consensus analysis ensuring a reliable\nground truth. We conduct extensive experiments on the compiled dataset in\naddition to three other standard benchmarks that show the robustness and the\nadvantage of our approach in both supervised and unsupervised settings.\nAdditionally, we show that our approach learns collectively from data of varied\nnumber-of-views and orthogonal to other summarization methods, deeming it\nscalable and generic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elfeki_M/0/1/0/all/0/1\">Mohamed Elfeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liqiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-the-fly Global Embeddings Using Random Projections for Extreme Multi-label Classification. (arXiv:1912.08140v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1912.08140","description":"<p>The goal of eXtreme Multi-label Learning (XML) is to automatically annotate a\ngiven data point with the most relevant subset of labels from an extremely\nlarge vocabulary of labels (e.g., a million labels). Lately, many attempts have\nbeen made to address this problem that achieve reasonable performance on\nbenchmark datasets. In this paper, rather than coming-up with an altogether new\nmethod, our objective is to present and validate a simple baseline for this\ntask. Precisely, we investigate an on-the-fly global and structure preserving\nfeature embedding technique using random projections whose learning phase is\nindependent of training samples and label vocabulary. Further, we show how an\nensemble of multiple such learners can be used to achieve further boost in\nprediction accuracy with only linear increase in training and prediction time.\nExperiments on three public XML benchmarks show that the proposed approach\nobtains competitive accuracy compared with many existing methods. Additionally,\nit also provides around 6572x speed-up ratio in terms of training time and\naround 14.7x reduction in model-size compared to the closest competitors on the\nlargest publicly available dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_Y/0/1/0/all/0/1\">Yashaswi Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation. (arXiv:2010.01663v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.01663","description":"<p>Most methods for medical image segmentation use U-Net or its variants as they\nhave been successful in most of the applications. After a detailed analysis of\nthese \"traditional\" encoder-decoder based approaches, we observed that they\nperform poorly in detecting smaller structures and are unable to segment\nboundary regions precisely. This issue can be attributed to the increase in\nreceptive field size as we go deeper into the encoder. The extra focus on\nlearning high level features causes the U-Net based approaches to learn less\ninformation about low-level features which are crucial for detecting small\nstructures. To overcome this issue, we propose using an overcomplete\nconvolutional architecture where we project our input image into a higher\ndimension such that we constrain the receptive field from increasing in the\ndeep layers of the network. We design a new architecture for image\nsegmentation- KiU-Net which has two branches: (1) an overcomplete convolutional\nnetwork Kite-Net which learns to capture fine details and accurate edges of the\ninput, and (2) U-Net which learns high level features. Furthermore, we also\npropose KiU-Net 3D which is a 3D convolutional architecture for volumetric\nsegmentation. We perform a detailed study of KiU-Net by performing experiments\non five different datasets covering various image modalities like ultrasound\n(US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic\nand fundus images. The proposed method achieves a better performance as\ncompared to all the recent methods with an additional benefit of fewer\nparameters and faster convergence. Additionally, we also demonstrate that the\nextensions of KiU-Net based on residual blocks and dense blocks result in\nfurther performance improvements. The implementation of KiU-Net can be found\nhere: https://github.com/jeya-maria-jose/KiU-Net-pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sindagi_V/0/1/0/all/0/1\">Vishwanath A. Sindagi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hacihaliloglu_I/0/1/0/all/0/1\">Ilker Hacihaliloglu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral Image Classification -- Traditional to Deep Models: A Survey for Future Prospects. (arXiv:2101.06116v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.06116","description":"<p>Hyperspectral Imaging (HSI) has been extensively utilized in many real-life\napplications because it benefits from the detailed spectral information\ncontained in each pixel. Notably, the complex characteristics i.e., the\nnonlinear relation among the captured spectral information and the\ncorresponding object of HSI data make accurate classification challenging for\ntraditional methods. In the last few years, Deep Learning (DL) has been\nsubstantiated as a powerful feature extractor that effectively addresses the\nnonlinear problems that appeared in a number of computer vision tasks. This\nprompts the deployment of DL for HSI classification (HSIC) which revealed good\nperformance. This survey enlists a systematic overview of DL for HSIC and\ncompared state-of-the-art strategies of the said topic. Primarily, we will\nencapsulate the main challenges of traditional machine learning for HSIC and\nthen we will acquaint the superiority of DL to address these problems. This\nsurvey breakdown the state-of-the-art DL frameworks into spectral-features,\nspatial-features, and together spatial-spectral features to systematically\nanalyze the achievements (future research directions as well) of these\nframeworks for HSIC. Moreover, we will consider the fact that DL requires a\nlarge number of labeled training examples whereas acquiring such a number for\nHSIC is challenging in terms of time and cost. Therefore, this survey discusses\nsome strategies to improve the generalization performance of DL strategies\nwhich can provide some future guidelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_M/0/1/0/all/0/1\">Muhammad Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shabbir_S/0/1/0/all/0/1\">Sidrah Shabbir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1\">Swalpa Kumar Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazzara_M/0/1/0/all/0/1\">Manuel Mazzara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Distefano_S/0/1/0/all/0/1\">Salvatore Distefano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sandwich Batch Normalization: A Drop-In Replacement for Feature Distribution Heterogeneity. (arXiv:2102.11382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.11382","description":"<p>We present Sandwich Batch Normalization (SaBN), a frustratingly easy\nimprovement of Batch Normalization (BN) with only a few lines of code changes.\nSaBN is motivated by addressing the inherent feature distribution heterogeneity\nthat one can be identified in many tasks, which can arise from data\nheterogeneity (multiple input domains) or model heterogeneity (dynamic\narchitectures, model conditioning, etc.). Our SaBN factorizes the BN affine\nlayer into one shared sandwich affine layer, cascaded by several parallel\nindependent affine layers. Concrete analysis reveals that, during optimization,\nSaBN promotes balanced gradient norms while still preserving diverse gradient\ndirections -- a property that many application tasks seem to favor. We\ndemonstrate the prevailing effectiveness of SaBN as a drop-in replacement in\nfour tasks: conditional image generation, neural architecture search (NAS),\nadversarial training, and arbitrary style transfer. Leveraging SaBN immediately\nachieves better Inception Score and FID on CIFAR-10 and ImageNet conditional\nimage generation with three state-of-the-art GANs; boosts the performance of a\nstate-of-the-art weight-sharing NAS algorithm significantly on NAS-Bench-201;\nsubstantially improves the robust and standard accuracies for adversarial\ndefense; and produces superior arbitrary stylized results. We also provide\nvisualizations and analysis to help understand why SaBN works. Codes are\navailable at: https://github.com/VITA-Group/Sandwich-Batch-Normalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1\">Xinyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collapsible Linear Blocks for Super-Efficient Super Resolution. (arXiv:2103.09404v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.09404","description":"<p>With the advent of smart devices that support 4K and 8K resolution, Single\nImage Super Resolution (SISR) has become an important computer vision problem.\nHowever, most super resolution deep networks are computationally very\nexpensive. In this paper, we propose Super-Efficient Super Resolution (SESR)\nnetworks that establish a new state-of-the-art for efficient super resolution.\nOur approach is based on linear overparameterization of CNNs and creates an\nefficient model architecture for SISR. With theoretical analysis, we uncover\nthe limitations of existing overparameterization methods and show how the\nproposed method alleviates them. Detailed experiments across six benchmark\ndatasets demonstrate that SESR achieves similar or better image quality than\nstate-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate\n(MAC) operations. As a result, SESR can be used on constrained hardware to\nperform x2 (1080p to 4K) and x4 (1080p to 8K) SISR. Towards this, we simulate\nhardware performance numbers for a commercial mobile Neural Processing Unit\n(NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the\nchallenges faced by super resolution on AI accelerators and demonstrate that\nSESR is significantly faster (e.g., 6x-8x higher FPS) than existing models on\nmobile-NPUs. The code for this work is available at\nhttps://github.com/ARM-software/sesr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhardwaj_K/0/1/0/all/0/1\">Kartikeya Bhardwaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milosavljevic_M/0/1/0/all/0/1\">Milos Milosavljevic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeil_L/0/1/0/all/0/1\">Liam O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gope_D/0/1/0/all/0/1\">Dibakar Gope</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matas_R/0/1/0/all/0/1\">Ramon Matas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chalfin_A/0/1/0/all/0/1\">Alex Chalfin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suda_N/0/1/0/all/0/1\">Naveen Suda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_L/0/1/0/all/0/1\">Lingchuan Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loh_D/0/1/0/all/0/1\">Danny Loh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Adversarial Robustness of Vision Transformers. (arXiv:2103.15670v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15670","description":"<p>Following the success in advancing natural language processing and\nunderstanding, transformers are expected to bring revolutionary changes to\ncomputer vision. This work provides the first and comprehensive study on the\nrobustness of vision transformers (ViTs) against adversarial perturbations.\nTested on various white-box and transfer attack settings, we find that ViTs\npossess better adversarial robustness when compared with convolutional neural\nnetworks (CNNs). This observation also holds for certified robustness. We\nsummarize the following main observations contributing to the improved\nrobustness of ViTs:\n</p>\n<p>1) Features learned by ViTs contain less low-level information and are more\ngeneralizable, which contributes to superior robustness against adversarial\nperturbations.\n</p>\n<p>2) Introducing convolutional or tokens-to-token blocks for learning low-level\nfeatures in ViTs can improve classification accuracy but at the cost of\nadversarial robustness.\n</p>\n<p>3) Increasing the proportion of transformers in the model structure (when the\nmodel consists of both transformer and CNN blocks) leads to better robustness.\nBut for a pure transformer model, simply increasing the size or adding layers\ncannot guarantee a similar effect.\n</p>\n<p>4) Pre-training on larger datasets does not significantly improve adversarial\nrobustness though it is critical for training ViTs.\n</p>\n<p>5) Adversarial training is also applicable to ViT for training robust models.\n</p>\n<p>Furthermore, feature visualization and frequency analysis are conducted for\nexplanation. The results show that ViTs are less sensitive to high-frequency\nperturbations than CNNs and there is a high correlation between how well the\nmodel learns low-level features and its robustness against different\nfrequency-based perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Rulin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhouxing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOON: Scenario Oriented Object Navigation with Graph-based Exploration. (arXiv:2103.17138v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.17138","description":"<p>The ability to navigate like a human towards a language-guided target from\nanywhere in a 3D embodied environment is one of the 'holy grail' goals of\nintelligent robots. Most visual navigation benchmarks, however, focus on\nnavigating toward a target from a fixed starting point, guided by an elaborate\nset of instructions that depicts step-by-step. This approach deviates from\nreal-world problems in which human-only describes what the object and its\nsurrounding look like and asks the robot to start navigation from anywhere.\nAccordingly, in this paper, we introduce a Scenario Oriented Object Navigation\n(SOON) task. In this task, an agent is required to navigate from an arbitrary\nposition in a 3D embodied environment to localize a target following a scene\ndescription. To give a promising direction to solve this task, we propose a\nnovel graph-based exploration (GBE) method, which models the navigation state\nas a graph and introduces a novel graph-based exploration approach to learn\nknowledge from the graph and stabilize training by learning sub-optimal\ntrajectories. We also propose a new large-scale benchmark named From Anywhere\nto Object (FAO) dataset. To avoid target ambiguity, the descriptions in FAO\nprovide rich semantic scene information includes: object attribute, object\nrelationship, region description, and nearby region description. Our\nexperiments reveal that the proposed GBE outperforms various state-of-the-arts\non both FAO and R2R datasets. And the ablation studies on FAO validates the\nquality of the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengda Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypercorrelation Squeeze for Few-Shot Segmentation. (arXiv:2104.01538v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01538","description":"<p>Few-shot semantic segmentation aims at learning to segment a target object\nfrom a query image using only a few annotated support images of the target\nclass. This challenging task requires to understand diverse levels of visual\ncues and analyze fine-grained correspondence relations between the query and\nthe support images. To address the problem, we propose Hypercorrelation Squeeze\nNetworks (HSNet) that leverages multi-level feature correlation and efficient\n4D convolutions. It extracts diverse features from different levels of\nintermediate convolutional layers and constructs a collection of 4D correlation\ntensors, i.e., hypercorrelations. Using efficient center-pivot 4D convolutions\nin a pyramidal architecture, the method gradually squeezes high-level semantic\nand low-level geometric cues of the hypercorrelation into precise segmentation\nmasks in coarse-to-fine manner. The significant performance improvements on\nstandard few-shot segmentation benchmarks of PASCAL-5i, COCO-20i, and FSS-1000\nverify the efficacy of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_J/0/1/0/all/0/1\">Juhong Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dahyun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Representation Learning and Novel Category Discovery on Single- and Multi-modal Data. (arXiv:2104.12673v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12673","description":"<p>This paper studies the problem of novel category discovery on single- and\nmulti-modal data with labels from different but relevant categories. We present\na generic, end-to-end framework to jointly learn a reliable representation and\nassign clusters to unlabelled data. To avoid over-fitting the learnt embedding\nto labelled data, we take inspiration from self-supervised representation\nlearning by noise-contrastive estimation and extend it to jointly handle\nlabelled and unlabelled data. In particular, we propose using category\ndiscrimination on labelled data and cross-modal discrimination on multi-modal\ndata to augment instance discrimination used in conventional contrastive\nlearning approaches. We further employ Winner-Take-All (WTA) hashing algorithm\non the shared representation space to generate pairwise pseudo labels for\nunlabelled data to better predict cluster assignments. We thoroughly evaluate\nour framework on large-scale multi-modal video benchmarks Kinetics-400 and\nVGG-Sound, and image benchmarks CIFAR10, CIFAR100 and ImageNet, obtaining\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xuhui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yukun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1\">Bradley Green</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning. (arXiv:2105.04906v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04906","description":"<p>Recent self-supervised methods for image representation learning are based on\nmaximizing the agreement between embedding vectors from different views of the\nsame image. A trivial solution is obtained when the encoder outputs constant\nvectors. This collapse problem is often avoided through implicit biases in the\nlearning architecture, that often lack a clear justification or interpretation.\nIn this paper, we introduce VICReg (Variance-Invariance-Covariance\nRegularization), a method that explicitly avoids the collapse problem with a\nsimple regularization term on the variance of the embeddings along each\ndimension individually. VICReg combines the variance term with a decorrelation\nmechanism based on redundancy reduction and covariance regularization, and\nachieves results on par with the state of the art on several downstream tasks.\nIn addition, we show that incorporating our new variance term into other\nmethods helps stabilize the training and leads to performance improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bardes_A/0/1/0/all/0/1\">Adrien Bardes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LuvHarris: A Practical Corner Detector for Event-cameras. (arXiv:2105.11443v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11443","description":"<p>There have been a number of corner detection methods proposed for event\ncameras in the last years, since event-driven computer vision has become more\naccessible. Current state-of-the-art have either unsatisfactory accuracy or\nreal-time performance when considered for practical use, for example when a\ncamera is randomly moved in an unconstrained environment. In this paper, we\npresent yet another method to perform corner detection, dubbed look-up\nevent-Harris (luvHarris), that employs the Harris algorithm for high accuracy\nbut manages an improved event throughput. Our method has two major\ncontributions, 1. a novel \"threshold ordinal event-surface\" that removes\ncertain tuning parameters and is well suited for Harris operations, and 2. an\nimplementation of the Harris algorithm such that the computational load per\nevent is minimised and computational heavy convolutions are performed only\n\"as-fast-as-possible\", i.e. only as computational resources are available. The\nresult is a practical, real-time, and robust corner detector that runs more\nthan 2.6x the speed of current state-of-the-art; a necessity when using\nhigh-resolution event-camera in real-time. We explain the considerations taken\nfor the approach, compare the algorithm to current state-of-the-art in terms of\ncomputational performance and detection accuracy, and discuss the validity of\nthe proposed approach for event cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glover_A/0/1/0/all/0/1\">Arren Glover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinale_A/0/1/0/all/0/1\">Aiko Dinale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_L/0/1/0/all/0/1\">Leandro De Souza Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_S/0/1/0/all/0/1\">Simeon Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartolozzi_C/0/1/0/all/0/1\">Chiara Bartolozzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection. (arXiv:2106.01178v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.01178","description":"<p>In this paper, we introduce the task of multi-view RGB-based 3D object\ndetection as an end-to-end optimization problem. To address this problem, we\npropose ImVoxelNet, a novel fully convolutional method of 3D object detection\nbased on monocular or multi-view RGB images. The number of monocular images in\neach multi-view input can variate during training and inference; actually, this\nnumber might be unique for each multi-view input. ImVoxelNet successfully\nhandles both indoor and outdoor scenes, which makes it general-purpose.\nSpecifically, it achieves state-of-the-art results in car detection on KITTI\n(monocular) and nuScenes (multi-view) benchmarks among all methods that accept\nRGB images. Moreover, it surpasses existing RGB-based 3D object detection\nmethods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark\nfor multi-view 3D object detection. The source code and the trained models are\navailable at https://github.com/saic-vul/imvoxelnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rukhovich_D/0/1/0/all/0/1\">Danila Rukhovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorontsova_A/0/1/0/all/0/1\">Anna Vorontsova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konushin_A/0/1/0/all/0/1\">Anton Konushin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural Network Robustness via Persistency of Excitation. (arXiv:2106.02078v5 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.02078","description":"<p>Improving adversarial robustness of neural networks remains a major\nchallenge. Fundamentally, training a neural network via gradient descent is a\nparameter estimation problem. In adaptive control, maintaining persistency of\nexcitation (PoE) is integral to ensuring convergence of parameter estimates in\ndynamical systems to their true values. We show that parameter estimation with\ngradient descent can be modeled as a sampling of an adaptive linear\ntime-varying continuous system. Leveraging this model, and with inspiration\nfrom Model-Reference Adaptive Control (MRAC), we prove a sufficient condition\nto constrain gradient descent updates to reference persistently excited\ntrajectories converging to the true parameters. The sufficient condition is\nachieved when the learning rate is less than the inverse of the Lipschitz\nconstant of the gradient of loss function. We provide an efficient technique\nfor estimating the corresponding Lipschitz constant in practice using extreme\nvalue theory. Our experimental results in both standard and adversarial\ntraining illustrate that networks trained with the PoE-motivated learning rate\nschedule have similar clean accuracy but are significantly more robust to\nadversarial attacks than models trained using current state-of-the-art\nheuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1\">Kaustubh Sridhar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1\">Oleg Sokolsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParticleAugment: Sampling-Based Data Augmentation. (arXiv:2106.08693v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.08693","description":"<p>We present an automated data augmentation approach for image classification.\nWe formulate the problem as Monte Carlo sampling where our goal is to\napproximate the optimal augmentation policies. We propose a particle filtering\nscheme for the policy search where the probability of applying a set of\naugmentation operations forms the state of the filter. We measure the policy\nperformance based on the loss function difference between a reference and the\nactual model, which we afterwards use to re-weight the particles and finally\nupdate the policy. In our experiments, we show that our formulation for\nautomated augmentation reaches promising results on CIFAR-10, CIFAR-100, and\nImageNet datasets using the standard network architectures for this problem. By\ncomparing with the related work, our method reaches a balance between the\ncomputational cost of policy search and the model performance. Our code will be\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsaregorodtsev_A/0/1/0/all/0/1\">Alexander Tsaregorodtsev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Affective XAI: Facial Affect Analysis for Understanding Explainable Human-AI Interactions. (arXiv:2106.08761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08761","description":"<p>As machine learning approaches are increasingly used to augment human\ndecision-making, eXplainable Artificial Intelligence (XAI) research has\nexplored methods for communicating system behavior to humans. However, these\napproaches often fail to account for the emotional responses of humans as they\ninteract with explanations. Facial affect analysis, which examines human facial\nexpressions of emotions, is one promising lens for understanding how users\nengage with explanations. Therefore, in this work, we aim to (1) identify which\nfacial affect features are pronounced when people interact with XAI interfaces,\nand (2) develop a multitask feature embedding for linking facial affect signals\nwith participants' use of explanations. Our analyses and results show that the\noccurrence and values of facial AU1 and AU4, and Arousal are heightened when\nparticipants fail to use explanations effectively. This suggests that facial\naffect analysis should be incorporated into XAI to personalize explanations to\nindividuals' interaction styles and to adapt explanations based on the\ndifficulty of the task performed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerdan_L/0/1/0/all/0/1\">Luke Guerdan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raymond_A/0/1/0/all/0/1\">Alex Raymond</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1\">Hatice Gunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PICCOLO: Point Cloud-Centric Omnidirectional Localization. (arXiv:2108.06545v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06545","description":"<p>We present PICCOLO, a simple and efficient algorithm for omnidirectional\nlocalization. Given a colored point cloud and a 360 panorama image of a scene,\nour objective is to recover the camera pose at which the panorama image is\ntaken. Our pipeline works in an off-the-shelf manner with a single image given\nas a query and does not require any training of neural networks or collecting\nground-truth poses of images. Instead, we match each point cloud color to the\nholistic view of the panorama image with gradient-descent optimization to find\nthe camera pose. Our loss function, called sampling loss, is point\ncloud-centric, evaluated at the projected location of every point in the point\ncloud. In contrast, conventional photometric loss is image-centric, comparing\ncolors at each pixel location. With a simple change in the compared entities,\nsampling loss effectively overcomes the severe visual distortion of\nomnidirectional images, and enjoys the global context of the 360 view to handle\nchallenging scenarios for visual localization. PICCOLO outperforms existing\nomnidirectional localization algorithms in both accuracy and stability when\nevaluated in various environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1\">Changwoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hojun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"edge-SR: Super-Resolution For The Masses. (arXiv:2108.10335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10335","description":"<p>Classic image scaling (e.g. bicubic) can be seen as one convolutional layer\nand a single upscaling filter. Its implementation is ubiquitous in all display\ndevices and image processing software. In the last decade deep learning systems\nhave been introduced for the task of image super-resolution (SR), using several\nconvolutional layers and numerous filters. These methods have taken over the\nbenchmarks of image quality for upscaling tasks. Would it be possible to\nreplace classic upscalers with deep learning architectures on edge devices such\nas display panels, tablets, laptop computers, etc.? On one hand, the current\ntrend in Edge-AI chips shows a promising future in this direction, with rapid\ndevelopment of hardware that can run deep-learning tasks efficiently. On the\nother hand, in image SR only few architectures have pushed the limit to extreme\nsmall sizes that can actually run on edge devices at real-time. We explore\npossible solutions to this problem with the aim to fill the gap between classic\nupscalers and small deep learning configurations. As a transition from classic\nto deep-learning upscaling we propose edge-SR (eSR), a set of one-layer\narchitectures that use interpretable mechanisms to upscale images. Certainly, a\none-layer architecture cannot reach the quality of deep learning systems.\nNevertheless, we find that for high speed requirements, eSR becomes better at\ntrading-off image quality and runtime performance. Filling the gap between\nclassic and deep-learning architectures for image upscaling is critical for\nmassive adoption of this technology. It is equally important to have an\ninterpretable system that can reveal the inner strategies to solve this problem\nand guide us to future improvements and better understanding of larger\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michelini_P/0/1/0/all/0/1\">Pablo Navarrete Michelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunhua Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xingqun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic labelling of urban point clouds using data fusion. (arXiv:2108.13757v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13757","description":"<p>In this paper we describe an approach to semi-automatically create a labelled\ndataset for semantic segmentation of urban street-level point clouds. We use\ndata fusion techniques using public data sources such as elevation data and\nlarge-scale topographical maps to automatically label parts of the point cloud,\nafter which only limited human effort is needed to check the results and make\namendments where needed. This drastically limits the time needed to create a\nlabelled dataset that is extensive enough to train deep semantic segmentation\nmodels. We apply our method to point clouds of the Amsterdam region, and\nsuccessfully train a RandLA-Net semantic segmentation model on the labelled\ndataset. These results demonstrate the potential of smart data fusion and\nsemantic segmentation for the future of smart city planning and management.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bloembergen_D/0/1/0/all/0/1\">Daan Bloembergen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eijgenstein_C/0/1/0/all/0/1\">Chris Eijgenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Extreme Value Theory for Open Set Video Domain Adaptation. (arXiv:2109.00522v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00522","description":"<p>With the advent of media streaming, video action recognition has become\nprogressively important for various applications, yet at the high expense of\nrequiring large-scale data labelling. To overcome the problem of expensive data\nlabelling, domain adaptation techniques have been proposed that transfers\nknowledge from fully labelled data (i.e., source domain) to unlabelled data\n(i.e., target domain). The majority of video domain adaptation algorithms are\nproposed for closed-set scenarios in which all the classes are shared among the\ndomains. In this work, we propose an open-set video domain adaptation approach\nto mitigate the domain discrepancy between the source and target data, allowing\nthe target data to contain additional classes that do not belong to the source\ndomain. Different from previous works, which only focus on improving accuracy\nfor shared classes, we aim to jointly enhance the alignment of shared classes\nand recognition of unknown samples. Towards this goal, class-conditional\nextreme value theory is applied to enhance the unknown recognition.\nSpecifically, the entropy values of target samples are modelled as generalised\nextreme value distributions, which allows separating unknown samples lying in\nthe tail of the distribution. To alleviate the negative transfer issue, weights\ncomputed by the distance from the sample entropy to the threshold are leveraged\nin adversarial learning in the sense that confident source and target samples\nare aligned, and unconfident samples are pushed away. The proposed method has\nbeen thoroughly evaluated on both small-scale and large-scale cross-domain\nvideo datasets and achieved the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuoxiao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1\">Mahsa Baktashmotlagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Oriented Object Detection in Aerial Images Based on Area Ratio of Parallelogram. (arXiv:2109.10187v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10187","description":"<p>Rotated object detection is a challenging task in aerial images since the\nobjects in aerial images are displayed in arbitrary directions and are\nfrequently densely packed. Although considerable progress has been made, there\nare still challenges that existing regression-based rotation detectors suffer\nfrom the representation ambiguity. In this paper, we propose a simple,\npractical framework to optimize the bounding box regression for rotating\nobjects. Rather than directly regressing the five parameters (coordinates of\nthe central point, width, height, and rotation angle) or the four vertices, we\nemploy the area ratio of the parallelogram (ARP) to describe a multi-oriented\nobject accurately. Specifically, ARP regresses coordinates of the center point,\nheight, and width of the oriented object's minimum circumscribed rectangle and\nthree area ratios. It may facilitate learning offset and avoid the issue of\nangular periodicity or label points sequence for oriented objects. To further\nremedy the confusion issue of nearly horizontal objects, the area ratio between\nthe object and its minimal circumscribed rectangle has been used to guide the\nselection of horizontal or oriented detection for each object. The rotation\nefficient IOU loss (R-EIOU) connects the flat bounding box with the three area\nratios and improves the accuracy of the rotating bounding. Experimental results\non remote sensing datasets, including HRSC2016, DOTA, and UCAS-AOD, show that\nour method achieves superior detection performance than many state-of-the-art\napproaches. The code and model will be coming with the paper published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed-supervised segmentation: Confidence maximization helps knowledge distillation. (arXiv:2109.10902v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.10902","description":"<p>Despite achieving promising results in a breadth of medical image\nsegmentation tasks, deep neural networks require large training datasets with\npixel-wise annotations. Obtaining these curated datasets is a cumbersome\nprocess which limits the application in scenarios where annotated images are\nscarce. Mixed supervision is an appealing alternative for mitigating this\nobstacle, where only a small fraction of the data contains complete pixel-wise\nannotations and other images have a weaker form of supervision. In this work,\nwe propose a dual-branch architecture, where the upper branch (teacher)\nreceives strong annotations, while the bottom one (student) is driven by\nlimited supervision and guided by the upper branch. Combined with a standard\ncross-entropy loss over the labeled pixels, our novel formulation integrates\ntwo important terms: (i) a Shannon entropy loss defined over the\nless-supervised images, which encourages confident student predictions in the\nbottom branch; and (ii) a Kullback-Leibler (KL) divergence term, which\ntransfers the knowledge of the strongly supervised branch to the\nless-supervised branch and guides the entropy (student-confidence) term to\navoid trivial solutions. We show that the synergy between the entropy and KL\ndivergence yields substantial improvements in performance. We also discuss an\ninteresting link between Shannon-entropy minimization and standard pseudo-mask\ngeneration, and argue that the former should be preferred over the latter for\nleveraging information from unlabeled pixels. Quantitative and qualitative\nresults on two publicly available datasets demonstrate that our method\nsignificantly outperforms other strategies for semantic segmentation within a\nmixed-supervision framework, as well as recent semi-supervised approaches.\nMoreover, we show that the branch trained with reduced supervision and guided\nby the top branch largely outperforms the latter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Tactile Grasp Force Sensing Using Fingernail Imaging via Deep Neural Networks. (arXiv:2109.15231v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15231","description":"<p>This paper has introduced a novel approach for the real-time estimation of 3D\ntactile forces exerted by human fingertips via vision only. The introduced\napproach is entirely monocular vision-based and does not require any physical\nforce sensor. Therefore, it is scalable, non-intrusive, and easily fused with\nother perception systems such as body pose estimation, making it ideal for HRI\napplications where force sensing is necessary. The introduced approach consists\nof three main modules: finger tracking for detection and tracking of each\nindividual finger, image alignment for preserving the spatial information in\nthe images, and the force model for estimating the 3D forces from coloration\npatterns in the images. The model has been implemented experimentally, and the\nresults have shown a maximum RMS error of 8.4% (for the entire range of force\nlevels) along all three directions. The estimation accuracy is comparable to\nthe offline models in the literature, such as EigneNail, while, this model is\ncapable of performing force estimation at 30 frames per second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fallahinia_N/0/1/0/all/0/1\">Navid Fallahinia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascaro_S/0/1/0/all/0/1\">Stephen Mascaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEACh: Task-driven Embodied Agents that Chat. (arXiv:2110.00534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00534","description":"<p>Robots operating in human spaces must be able to engage in natural language\ninteraction with people, both understanding and executing instructions, and\nusing conversation to resolve ambiguity and recover from mistakes. To study\nthis, we introduce TEACh, a dataset of over 3,000 human--human, interactive\ndialogues to complete household tasks in simulation. A Commander with access to\noracle information about a task communicates in natural language with a\nFollower. The Follower navigates through and interacts with the environment to\ncomplete tasks varying in complexity from \"Make Coffee\" to \"Prepare Breakfast\",\nasking questions and getting additional information from the Commander. We\npropose three benchmarks using TEACh to study embodied intelligence challenges,\nand we evaluate initial models' abilities in dialogue understanding, language\ngrounding, and task execution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Padmakumar_A/0/1/0/all/0/1\">Aishwarya Padmakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Ayush Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_P/0/1/0/all/0/1\">Patrick Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_Chen_A/0/1/0/all/0/1\">Anjali Narayan-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gella_S/0/1/0/all/0/1\">Spandana Gella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piramuthu_R/0/1/0/all/0/1\">Robinson Piramuthu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDR-cGAN: Single LDR to HDR Image Translation using Conditional GAN. (arXiv:2110.01660v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01660","description":"<p>The prime goal of digital imaging techniques is to reproduce the realistic\nappearance of a scene. Low Dynamic Range (LDR) cameras are incapable of\nrepresenting the wide dynamic range of the real-world scene. The captured\nimages turn out to be either too dark (underexposed) or too bright\n(overexposed). Specifically, saturation in overexposed regions makes the task\nof reconstructing a High Dynamic Range (HDR) image from single LDR image\nchallenging. In this paper, we propose a deep learning based approach to\nrecover details in the saturated areas while reconstructing the HDR image. We\nformulate this problem as an image-to-image (I2I) translation task. To this\nend, we present a novel conditional GAN (cGAN) based framework trained in an\nend-to-end fashion over the HDR-REAL and HDR-SYNTH datasets. Our framework uses\nan overexposed mask obtained from a pre-trained segmentation model to\nfacilitate the hallucination task of adding details in the saturated regions.\nWe demonstrate the effectiveness of the proposed method by performing an\nextensive quantitative and qualitative comparison with several state-of-the-art\nsingle-image HDR reconstruction techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raipurkar_P/0/1/0/all/0/1\">Prarabdh Raipurkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_R/0/1/0/all/0/1\">Rohil Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Shanmuganathan Raman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum pixel representations and compression for $N$-dimensional images. (arXiv:2110.04405v2 [quant-ph] UPDATED)","link":"http://arxiv.org/abs/2110.04405","description":"<p>We introduce a novel and uniform framework for quantum pixel representations\nthat overarches many of the most popular representations proposed in the recent\nliterature, such as (I)FRQI, (I)NEQR, MCRQI, and (I)NCQI. The proposed QPIXL\nframework results in more efficient circuit implementations and significantly\nreduces the gate complexity for all considered quantum pixel representations.\nOur method only requires a linear number of gates in terms of the number of\npixels and does not use ancilla qubits. Furthermore, the circuits only consist\nof Ry gates and CNOT gates making them practical in the NISQ era. Additionally,\nwe propose a circuit and image compression algorithm that is shown to be highly\neffective, being able to reduce the necessary gates to prepare an FRQI state\nfor example scientific images by up to 90% without sacrificing image quality.\nOur algorithms are made publicly available as part of QPIXL++, a Quantum Image\nPixel Library.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Amankwah_M/0/1/0/all/0/1\">Mercy G. Amankwah</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Camps_D/0/1/0/all/0/1\">Daan Camps</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bethel_E/0/1/0/all/0/1\">E. Wes Bethel</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Beeumen_R/0/1/0/all/0/1\">Roel Van Beeumen</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Perciano_T/0/1/0/all/0/1\">Talita Perciano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Adaptive Cosine Estimator (LACE) for Image Classification. (arXiv:2110.05324v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05324","description":"<p>In this work, we propose a new loss to improve feature discriminability and\nclassification performance. Motivated by the adaptive cosine/coherence\nestimator (ACE), our proposed method incorporates angular information that is\ninherently learned by artificial neural networks. Our learnable ACE (LACE)\ntransforms the data into a new \"whitened\" space that improves the inter-class\nseparability and intra-class compactness. We compare our LACE to alternative\nstate-of-the art softmax-based and feature regularization approaches. Our\nresults show that the proposed method can serve as a viable alternative to\ncross entropy and angular softmax approaches. Our code is publicly available:\nhttps://github.com/GatorSense/LACE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peeples_J/0/1/0/all/0/1\">Joshua Peeples</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCurley_C/0/1/0/all/0/1\">Connor McCurley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_S/0/1/0/all/0/1\">Sarah Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_D/0/1/0/all/0/1\">Dylan Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zare_A/0/1/0/all/0/1\">Alina Zare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Well-classified Examples are Underestimated in Classification with Deep Neural Networks. (arXiv:2110.06537v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.06537","description":"<p>The conventional wisdom behind learning deep classification models is to\nfocus on bad-classified examples and ignore well-classified examples that are\nfar from the decision boundary. For instance, when training with cross-entropy\nloss, examples with higher likelihoods (i.e., well-classified examples)\ncontribute smaller gradients in back-propagation. However, we theoretically\nshow that this common practice hinders representation learning, energy\noptimization, and the growth of margin. To counteract this deficiency, we\npropose to reward well-classified examples with additive bonuses to revive\ntheir contribution to learning. This counterexample theoretically addresses\nthese three issues. We empirically support this claim by directly verify the\ntheoretical results or through the significant performance improvement with our\ncounterexample on diverse tasks, including image classification, graph\nclassification, and machine translation. Furthermore, this paper shows that\nbecause our idea can solve these three issues, we can deal with complex\nscenarios, such as imbalanced classification, OOD detection, and applications\nunder adversarial attacks. Code is available at:\nhttps://github.com/lancopku/well-classified-examples-are-underestimated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guangxiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenkai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuancheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do We Need to Directly Access the Source Datasets for Domain Generalization?. (arXiv:2110.06736v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.06736","description":"<p>Domain generalization (DG) aims to learn a generalizable model from multiple\nknown source domains for unknown target domains. Tremendous data distributed\nacross lots of places/devices nowadays that can not be directly accessed due to\nprivacy protection, especially in some crucial areas like finance and medical\ncare. However, most of the existing DG algorithms assume that all the source\ndatasets are accessible and can be mixed for domain-invariant semantics\nextraction, which may fail in real-world applications. In this paper, we\nintroduce a challenging setting of training a generalizable model by using\ndistributed source datasets without directly accessing them. We propose a novel\nmethod for this setting, which first trains a model on each source dataset and\nthen conduct data-free model fusion that fuses the trained models\nlayer-by-layer based on their semantic similarities, which aggregates different\nlevels of semantics from the distributed sources indirectly. The fused model is\nthen transmitted and trained on each dataset, we further introduce cross-layer\nsemantic calibration for domain-invariant semantics enhancement, which aligns\nfeature maps between the fused model and a fixed local model with an attention\nmechanism. Extensive experiments on multiple DG datasets show the significant\nperformance of our method in tackling this challenging setting, which is even\non par or superior to the performance of the state-of-the-art DG approaches in\nthe standard DG setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning by Estimating Twin Class Distributions. (arXiv:2110.07402v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07402","description":"<p>We present TWIST, a novel self-supervised representation learning method by\nclassifying large-scale unlabeled datasets in an end-to-end way. We employ a\nsiamese network terminated by a softmax operation to produce twin class\ndistributions of two augmented images. Without supervision, we enforce the\nclass distributions of different augmentations to be consistent. In the\nmeantime, we regularize the class distributions to make them sharp and diverse.\nSpecifically, we minimize the entropy of the distribution for each sample to\nmake the class prediction for each sample assertive and maximize the entropy of\nthe mean distribution to make the predictions of different samples diverse. In\nthis way, TWIST can naturally avoid the trivial solutions without specific\ndesigns such as asymmetric network, stop-gradient operation, or momentum\nencoder. Different from the clustering-based methods which alternate between\nclustering and learning, our method is a single learning process guided by a\nunified loss function. As a result, TWIST outperforms state-of-the-art methods\non a wide range of tasks, including unsupervised classification, linear\nclassification, semi-supervised learning, transfer learning, and some dense\nprediction tasks such as detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TDACNN: Target-domain-free Domain Adaptation Convolutional Neural Network for Drift Compensation in Gas Sensors. (arXiv:2110.07509v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2110.07509","description":"<p>Sensor drift is a long-existing unpredictable problem that deteriorates the\nperformance of gaseous substance recognition, calling for an antidrift domain\nadaptation algorithm. However, the prerequisite for traditional methods to\nachieve fine results is to have data from both nondrift distributions (source\ndomain) and drift distributions (target domain) for domain alignment, which is\nusually unrealistic and unachievable in real-life scenarios. To compensate for\nthis, in this paper, deep learning based on a target-domain-free domain\nadaptation convolutional neural network (TDACNN) is proposed. The main concept\nis that CNNs extract not only the domain-specific features of samples but also\nthe domain-invariant features underlying both the source and target domains.\nMaking full use of these various levels of embedding features can lead to\ncomprehensive utilization of different levels of characteristics, thus\nachieving drift compensation by the extracted intermediate features between two\ndomains. In the TDACNN, a flexible multibranch backbone with a multiclassifier\nstructure is proposed under the guidance of bionics, which utilizes multiple\nembedding features comprehensively without involving target domain data during\ntraining. A classifier ensemble method based on maximum mean discrepancy (MMD)\nis proposed to evaluate all the classifiers jointly based on the credibility of\nthe pseudolabel. To optimize network training, an additive angular margin\nsoftmax loss with parameter dynamic adjustment is utilized. Experiments on two\ndrift datasets under different settings demonstrate the superiority of TDACNN\ncompared with several state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuelin Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yan_J/0/1/0/all/0/1\">Jia Yan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_Z/0/1/0/all/0/1\">Zehuan Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Peng_X/0/1/0/all/0/1\">Xiaoyan Peng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tian_Y/0/1/0/all/0/1\">Yutong Tian</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Duan_S/0/1/0/all/0/1\">Shukai Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild. (arXiv:2110.07604v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07604","description":"<p>Recent history has seen a tremendous growth of work exploring implicit\nrepresentations of geometry and radiance, popularized through Neural Radiance\nFields (NeRF). Such works are fundamentally based on a (implicit) volumetric\nrepresentation of occupancy, allowing them to model diverse scene structure\nincluding translucent objects and atmospheric obscurants. But because the vast\nmajority of real-world scenes are composed of well-defined surfaces, we\nintroduce a surface analog of such implicit models called Neural Reflectance\nSurfaces (NeRS). NeRS learns a neural shape representation of a closed surface\nthat is diffeomorphic to a sphere, guaranteeing water-tight reconstructions.\nEven more importantly, surface parameterizations allow NeRS to learn (neural)\nbidirectional surface reflectance functions (BRDFs) that factorize\nview-dependent appearance into environmental illumination, diffuse color\n(albedo), and specular \"shininess.\" Finally, rather than illustrating our\nresults on synthetic scenes or controlled in-the-lab capture, we assemble a\nnovel dataset of multi-view images from online marketplaces for selling goods.\nSuch \"in-the-wild\" multi-view image sets pose a number of challenges, including\na small number of views with unknown/rough camera estimates. We demonstrate\nthat surface-based neural reconstructions enable learning from such data,\noutperforming volumetric neural rendering-based reconstructions. We hope that\nNeRS serves as a first step toward building scalable, high-quality libraries of\nreal-world shape, materials, and illumination. The project page with code and\nvideo visualizations can be found at https://jasonyzhang.com/ners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jason Y. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gengshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn-to-Race: A Multimodal Control Environment for Autonomous Racing. (arXiv:2103.11575v3 [cs.RO] CROSS LISTED)","link":"http://arxiv.org/abs/2103.11575","description":"<p>Existing research on autonomous driving primarily focuses on urban driving,\nwhich is insufficient for characterising the complex driving behaviour\nunderlying high-speed racing. At the same time, existing racing simulation\nframeworks struggle in capturing realism, with respect to visual rendering,\nvehicular dynamics, and task objectives, inhibiting the transfer of learning\nagents to real-world contexts. We introduce a new environment, where agents\nLearn-to-Race (L2R) in simulated competition-style racing, using multimodal\ninformation--from virtual cameras to a comprehensive array of inertial\nmeasurement sensors. Our environment, which includes a simulator and an\ninterfacing training framework, accurately models vehicle dynamics and racing\nconditions. In this paper, we release the Arrival simulator for autonomous\nracing. Next, we propose the L2R task with challenging metrics, inspired by\nlearning-to-drive challenges, Formula-style racing, and multimodal trajectory\nprediction for autonomous driving. Additionally, we provide the L2R framework\nsuite, facilitating simulated racing on high-precision models of real-world\ntracks. Finally, we provide an official L2R task dataset of expert\ndemonstrations, as well as a series of baseline experiments and reference\nimplementations. We make all code available:\nhttps://github.com/learn-to-race/l2r.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herman_J/0/1/0/all/0/1\">James Herman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1\">Anirudh Koul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skabelkin_A/0/1/0/all/0/1\">Alexey Skabelkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhukov_I/0/1/0/all/0/1\">Ivan Zhukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumskoy_M/0/1/0/all/0/1\">Max Kumskoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nyberg_E/0/1/0/all/0/1\">Eric Nyberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2106.13948","description":"<p>Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}