{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling. (arXiv:2207.11280v1 [cs.LG])","link":"http://arxiv.org/abs/2207.11280","description":"<p>We present PanGu-Coder, a pretrained decoder-only language model adopting the\nPanGu-Alpha architecture for text-to-code generation, i.e. the synthesis of\nprogramming language solutions given a natural language problem description. We\ntrain PanGu-Coder using a two-stage strategy: the first stage employs Causal\nLanguage Modelling (CLM) to pre-train on raw programming language data, while\nthe second stage uses a combination of Causal Language Modelling and Masked\nLanguage Modelling (MLM) training objectives that focus on the downstream task\nof text-to-code generation and train on loosely curated pairs of natural\nlanguage program definitions and code functions. Finally, we discuss\nPanGu-Coder-FT, which is fine-tuned on a combination of competitive programming\nproblems and code with continuous integration tests. We evaluate PanGu-Coder\nwith a focus on whether it generates functionally correct programs and\ndemonstrate that it achieves equivalent or better performance than similarly\nsized models, such as CodeX, while attending a smaller context window and\ntraining on less data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Christopoulou_F/0/1/0/all/0/1\">Fenia Christopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gritta_M/0/1/0/all/0/1\">Milan Gritta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guchun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yinpeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1\">Bo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1\">Li Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuchi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Guangtai Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiansheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Fairness in Speech Recognition: Discovery and mitigation of performance disparities. (arXiv:2207.11345v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11345","description":"<p>As for other forms of AI, speech recognition has recently been examined with\nrespect to performance disparities across different user cohorts. One approach\nto achieve fairness in speech recognition is to (1) identify speaker cohorts\nthat suffer from subpar performance and (2) apply fairness mitigation measures\ntargeting the cohorts discovered. In this paper, we report on initial findings\nwith both discovery and mitigation of performance disparities using data from a\nproduct-scale AI assistant speech recognition system. We compare cohort\ndiscovery based on geographic and demographic information to a more scalable\nmethod that groups speakers without human labels, using speaker embedding\ntechnology. For fairness mitigation, we find that oversampling of\nunderrepresented cohorts, as well as modeling speaker cohort membership by\nadditional input variables, reduces the gap between top- and bottom-performing\ncohorts, without deteriorating overall recognition accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_M/0/1/0/all/0/1\">Murugesan Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1\">I-Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1\">Brian King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_K/0/1/0/all/0/1\">Katherine Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saboowala_M/0/1/0/all/0/1\">Melissa Saboowala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_K/0/1/0/all/0/1\">Karan Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1\">Andreas Stolcke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Grounded Conversational Data Augmentation with Generative Conversational Networks. (arXiv:2207.11363v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11363","description":"<p>While rich, open-domain textual data are generally available and may include\ninteresting phenomena (humor, sarcasm, empathy, etc.) most are designed for\nlanguage processing tasks, and are usually in a non-conversational format. In\nthis work, we take a step towards automatically generating conversational data\nusing Generative Conversational Networks, aiming to benefit from the breadth of\navailable language and knowledge data, and train open domain social\nconversational agents. We evaluate our approach on conversations with and\nwithout knowledge on the Topical Chat dataset using automatic metrics and human\nevaluators. Our results show that for conversations without knowledge\ngrounding, GCN can generalize from the seed data, producing novel conversations\nthat are less relevant but more engaging and for knowledge-grounded\nconversations, it can produce more knowledge-focused, fluent, and engaging\nconversations. Specifically, we show that for open-domain conversations with\n10\\% of seed data, our approach performs close to the baseline that uses 100%\nof the data, while for knowledge-grounded conversations, it achieves the same\nusing only 1% of the data, on human ratings of engagingness, fluency, and\nrelevance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations. (arXiv:2207.11401v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11401","description":"<p>Visual Entailment with natural language explanations aims to infer the\nrelationship between a text-image pair and generate a sentence to explain the\ndecision-making process. Previous methods rely mainly on a pre-trained\nvision-language model to perform the relation inference and a language model to\ngenerate the corresponding explanation. However, the pre-trained\nvision-language models mainly build token-level alignment between text and\nimage yet ignore the high-level semantic alignment between the phrases (chunks)\nand visual contents, which is critical for vision-language reasoning. Moreover,\nthe explanation generator based only on the encoded joint representation does\nnot explicitly consider the critical decision-making points of relation\ninference. Thus the generated explanations are less faithful to visual-language\nreasoning. To mitigate these problems, we propose a unified Chunk-aware\nAlignment and Lexical Constraint based method, dubbed as CALeC. It contains a\nChunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical\nConstraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence\nstructure inherent in language and various image regions to build chunk-aware\nsemantic alignment. Relation inferrer uses an attention-based reasoning network\nto incorporate the token-level and chunk-level vision-language representations.\nLeCG utilizes lexical constraints to expressly incorporate the words or chunks\nfocused by the relation inferrer into explanation generation, improving the\nfaithfulness and informativeness of the explanations. We conduct extensive\nexperiments on three datasets, and experimental results indicate that CALeC\nsignificantly outperforms other competitor models on inference accuracy and\nquality of generated explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Document-level Relation Extraction by Entity Knowledge Injection. (arXiv:2207.11433v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11433","description":"<p>Document-level relation extraction (RE) aims to identify the relations\nbetween entities throughout an entire document. It needs complex reasoning\nskills to synthesize various knowledge such as coreferences and commonsense.\nLarge-scale knowledge graphs (KGs) contain a wealth of real-world facts, and\ncan provide valuable knowledge to document-level RE. In this paper, we propose\nan entity knowledge injection framework to enhance current document-level RE\nmodels. Specifically, we introduce coreference distillation to inject\ncoreference knowledge, endowing an RE model with the more general capability of\ncoreference reasoning. We also employ representation reconciliation to inject\nfactual knowledge and aggregate KG representations and document representations\ninto a unified space. The experiments on two benchmark datasets validate the\ngeneralization of our entity knowledge injection framework and the consistent\nimprovement to several document-level RE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zitao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weijian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facing Changes: Continual Entity Alignment for Growing Knowledge Graphs. (arXiv:2207.11436v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11436","description":"<p>Entity alignment is a basic and vital technique in knowledge graph (KG)\nintegration. Over the years, research on entity alignment has resided on the\nassumption that KGs are static, which neglects the nature of growth of\nreal-world KGs. As KGs grow, previous alignment results face the need to be\nrevisited while new entity alignment waits to be discovered. In this paper, we\npropose and dive into a realistic yet unexplored setting, referred to as\ncontinual entity alignment. To avoid retraining an entire model on the whole\nKGs whenever new entities and triples come, we present a continual alignment\nmethod for this task. It reconstructs an entity's representation based on\nentity adjacency, enabling it to generate embeddings for new entities quickly\nand inductively using their existing neighbors. It selects and replays partial\npre-aligned entity pairs to train only parts of KGs while extracting\ntrustworthy alignment for knowledge augmentation. As growing KGs inevitably\ncontain non-matchable entities, different from previous works, the proposed\nmethod employs bidirectional nearest neighbor matching to find new entity\nalignment and update old alignment. Furthermore, we also construct new datasets\nby simulating the growth of multilingual DBpedia. Extensive experiments\ndemonstrate that our continual alignment method is more effective than\nbaselines based on retraining or inductive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yuanning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yiqiao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kexin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\mu\\text{KG}$: A Library for Multi-source Knowledge Graph Embeddings and Applications. (arXiv:2207.11442v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11442","description":"<p>This paper presents $\\mu\\text{KG}$, an open-source Python library for\nrepresentation learning over knowledge graphs. $\\mu\\text{KG}$ supports joint\nrepresentation learning over multi-source knowledge graphs (and also a single\nknowledge graph), multiple deep learning libraries (PyTorch and TensorFlow2),\nmultiple embedding tasks (link prediction, entity alignment, entity typing, and\nmulti-source link prediction), and multiple parallel computing modes\n(multi-process and multi-GPU computing). It currently implements 26 popular\nknowledge graph embedding models and supports 16 benchmark datasets.\n$\\mu\\text{KG}$ provides advanced implementations of embedding techniques with\nsimplified pipelines of different tasks. It also comes with high-quality\ndocumentation for ease of use. $\\mu\\text{KG}$ is more comprehensive than\nexisting knowledge graph embedding libraries. It is useful for a thorough\ncomparison and analysis of various embedding models and tasks. We show that the\njointly learned embeddings can greatly help knowledge-powered downstream tasks,\nsuch as multi-hop knowledge graph question answering. We will stay abreast of\nthe latest developments in the related fields and incorporate them into\n$\\mu\\text{KG}$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xindi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zequn Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter. (arXiv:2207.11500v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11500","description":"<p>The recent advances in natural language processing have yielded many exciting\ndevelopments in text analysis and language understanding models; however, these\nmodels can also be used to track people, bringing severe privacy concerns. In\nthis work, we investigate what individuals can do to avoid being detected by\nthose models while using social media platforms. We ground our investigation in\ntwo exposure-risky tasks, stance detection and geotagging. We explore a variety\nof simple techniques for modifying text, such as inserting typos in salient\nwords, paraphrasing, and adding dummy social media posts. Our experiments show\nthat the performance of BERT-based models fined tuned for stance detection\ndecreases significantly due to typos, but it is not affected by paraphrasing.\nMoreover, we find that typos have minimal impact on state-of-the-art geotagging\nmodels due to their increased reliance on social networks; however, we show\nthat users can deceive those models by interacting with different users,\nreducing their performance by almost 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dogan_D/0/1/0/all/0/1\">Dilara Dogan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altun_B/0/1/0/all/0/1\">Bahadir Altun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zengin_M/0/1/0/all/0/1\">Muhammed Said Zengin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutlu_M/0/1/0/all/0/1\">Mucahid Kutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsayed_T/0/1/0/all/0/1\">Tamer Elsayed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vaccine Discourse on Twitter During the COVID-19 Pandemic. (arXiv:2207.11521v1 [cs.CY])","link":"http://arxiv.org/abs/2207.11521","description":"<p>Since the onset of the COVID-19 pandemic, vaccines have been an important\ntopic in public discourse. The discussions around vaccines are polarized as\nsome see them as an important measure to end the pandemic, and others are\nhesitant or find them harmful. This study investigates posts related to\nCOVID-19 vaccines on Twitter and focuses on those which have a negative stance\ntoward vaccines. A dataset of 16,713,238 English tweets related to COVID-19\nvaccines was collected covering the period from March 1, 2020, to July 31,\n2021. We used the Scikit-learn Python library to apply a support vector machine\n(SVM) classifier to identify the tweets with a negative stance toward the\nCOVID-19 vaccines. A total of 5,163 tweets were used to train the classifier,\nout of which a subset of 2,484 tweets were manually annotated by us and made\npublicly available. We used the BERTtopic model to extract and investigate the\ntopics discussed within the negative tweets and how they changed over time. We\nshow that the negativity with respect to COVID-19 vaccines has decreased over\ntime along with the vaccine roll-outs. We identify 37 topics of discussion and\npresent their respective importance over time. We show that popular topics\nconsist of conspiratorial discussions such as 5G towers and microchips, but\nalso contain legitimate concerns around vaccination safety and side effects as\nwell as concerns about policies. Our study shows that even unpopular opinions\nor conspiracy theories can become widespread when paired with a widely popular\ndiscussion topic such as COVID-19 vaccines. Understanding the concerns and the\ndiscussed topics and how they change over time is essential for policymakers\nand public health authorities to provide better and in-time information and\npolicies, to facilitate vaccination of the population in future similar crises.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lindelof_G/0/1/0/all/0/1\">Gabriel Lindel&#xf6;f</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aledavood_T/0/1/0/all/0/1\">Talayeh Aledavood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_B/0/1/0/all/0/1\">Barbara Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supporting peace negotiations in the Yemen war through machine learning. (arXiv:2207.11528v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11528","description":"<p>Today's conflicts are becoming increasingly complex, fluid and fragmented,\noften involving a host of national and international actors with multiple and\noften divergent interests. This development poses significant challenges for\nconflict mediation, as mediators struggle to make sense of conflict dynamics,\nsuch as the range of conflict parties and the evolution of their political\npositions, the distinction between relevant and less relevant actors in\npeace-making, or the identification of key conflict issues and their\ninterdependence. International peace efforts appear ill-equipped to\nsuccessfully address these challenges. While technology is already being\nexperimented with and used in a range of conflict related fields, such as\nconflict predicting or information gathering, less attention has been given to\nhow technology can contribute to conflict mediation. This case study\ncontributes to emerging research on the use of state-of-the-art machine\nlearning technologies and techniques in conflict mediation processes. Using\ndialogue transcripts from peace negotiations in Yemen, this study shows how\nmachine-learning can effectively support mediating teams by providing them with\ntools for knowledge management, extraction and conflict analysis. Apart from\nillustrating the potential of machine learning tools in conflict mediation, the\npaper also emphasises the importance of interdisciplinary and participatory,\nco-creation methodology for the development of context-sensitive and targeted\ntools and to ensure meaningful and responsible implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">M. Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lier_F/0/1/0/all/0/1\">F.A. Van Lier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Reasoning Behind Classification Predictions with BERT for Fake News Detection. (arXiv:2207.11562v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11562","description":"<p>Fake news detection has become a major task to solve as there has been an\nincreasing number of fake news on the internet in recent years. Although many\nclassification models have been proposed based on statistical learning methods\nshowing good results, reasoning behind the classification performances may not\nbe enough. In the self-supervised learning studies, it has been highlighted\nthat a quality of representation (embedding) space matters and directly affects\na downstream task performance. In this study, a quality of the representation\nspace is analyzed visually and analytically in terms of linear separability for\ndifferent classes on a real and fake news dataset. To further add\ninterpretability to a classification model, a modification of Class Activation\nMapping (CAM) is proposed. The modified CAM provides a CAM score for each word\ntoken, where the CAM score on a word token denotes a level of focus on that\nword token to make the prediction. Finally, it is shown that the naive BERT\nmodel topped with a learnable linear layer is enough to achieve robust\nperformance while being compatible with CAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Daesoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context based lemmatizer for Polish language. (arXiv:2207.11565v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11565","description":"<p>Lemmatization is the process of grouping together the inflected forms of a\nword so they can be analysed as a single item, identified by the word's lemma,\nor dictionary form. In computational linguistics, lemmatisation is the\nalgorithmic process of determining the lemma of a word based on its intended\nmeaning. Unlike stemming, lemmatisation depends on correctly identifying the\nintended part of speech and meaning of a word in a sentence, as well as within\nthe larger context surrounding that sentence. As a result, developing efficient\nlemmatisation algorithm is the complex task. In recent years it can be observed\nthat deep learning models used for this task outperform other methods including\nmachine learning algorithms. In this paper the polish lemmatizer based on\nGoogle T5 model is presented. The training was run with different context\nlengths. The model achieves the best results for polish language lemmatisation\nprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karwatowski_M/0/1/0/all/0/1\">Michal Karwatowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1\">Marcin Pietron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Reasoning for Out-of-distribution Multimodal Sentiment Analysis. (arXiv:2207.11652v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11652","description":"<p>Existing studies on multimodal sentiment analysis heavily rely on textual\nmodality and unavoidably induce the spurious correlations between textual words\nand sentiment labels. This greatly hinders the model generalization ability. To\naddress this problem, we define the task of out-of-distribution (OOD)\nmultimodal sentiment analysis. This task aims to estimate and mitigate the bad\neffect of textual modality for strong OOD generalization. To this end, we\nembrace causal inference, which inspects the causal relationships via a causal\ngraph. From the graph, we find that the spurious correlations are attributed to\nthe direct effect of textual modality on the model prediction while the\nindirect one is more reliable by considering multimodal semantics. Inspired by\nthis, we devise a model-agnostic counterfactual framework for multimodal\nsentiment analysis, which captures the direct effect of textual modality via an\nextra text model and estimates the indirect one by a multimodal model. During\nthe inference, we first estimate the direct effect by the counterfactual\ninference, and then subtract it from the total effect of all modalities to\nobtain the indirect effect for reliable prediction. Extensive experiments show\nthe superior effectiveness and generalization ability of our proposed\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Teng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Liqiang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoWeird: Weird Translational Scoring Function Identified by Random Search. (arXiv:2207.11673v1 [cs.AI])","link":"http://arxiv.org/abs/2207.11673","description":"<p>Scoring function (SF) measures the plausibility of triplets in knowledge\ngraphs. Different scoring functions can lead to huge differences in link\nprediction performances on different knowledge graphs. In this report, we\ndescribe a weird scoring function found by random search on the open graph\nbenchmark (OGB). This scoring function, called AutoWeird, only uses tail entity\nand relation in a triplet to compute its plausibility score. Experimental\nresults show that AutoWeird achieves top-1 performance on ogbl-wikikg2 data\nset, but has much worse performance than other methods on ogbl-biokg data set.\nBy analyzing the tail entity distribution and evaluation protocol of these two\ndata sets, we attribute the unexpected success of AutoWeird on ogbl-wikikg2 to\ninappropriate evaluation and concentrated tail entity distribution. Such\nresults may motivate further research on how to accurately evaluate the\nperformance of different link prediction methods for knowledge graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hansi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Quanming Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Mandarin Speech Recogntion with Block-augmented Transformer. (arXiv:2207.11697v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11697","description":"<p>Recently Convolution-augmented Transformer (Conformer) has shown promising\nresults in Automatic Speech Recognition (ASR), outperforming the previous best\npublished Transformer Transducer. In this work, we believe that the output\ninformation of each block in the encoder and decoder is not completely\ninclusive, in other words, their output information may be complementary. We\nstudy how to take advantage of the complementary information of each block in a\nparameter-efficient way, and it is expected that this may lead to more robust\nperformance. Therefore we propose the Block-augmented Transformer for speech\nrecognition, named Blockformer. We have implemented two block ensemble methods:\nthe base Weighted Sum of the Blocks Output (Base-WSBO), and the\nSqueeze-and-Excitation module to Weighted Sum of the Blocks Output (SE-WSBO).\nExperiments have proved that the Blockformer significantly outperforms the\nstate-of-the-art Conformer-based models on AISHELL-1, our model achieves a CER\nof 4.35\\% without using a language model and 4.10\\% with an external language\nmodel on the testset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaoming Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Huifeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Liuwei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jie Hao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11716","description":"<p>Semantic similarity analysis and modeling is a fundamentally acclaimed task\nin many pioneering applications of natural language processing today. Owing to\nthe sensation of sequential pattern recognition, many neural networks like RNNs\nand LSTMs have achieved satisfactory results in semantic similarity modeling.\nHowever, these solutions are considered inefficient due to their inability to\nprocess information in a non-sequential manner, thus leading to the improper\nextraction of context. Transformers function as the state-of-the-art\narchitecture due to their advantages like non-sequential data processing and\nself-attention. In this paper, we perform semantic similarity analysis and\nmodeling on the U.S Patent Phrase to Phrase Matching Dataset using both\ntraditional and transformer-based techniques. We experiment upon four different\nvariants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by\nperforming K-Fold Cross-Validation. The experimental results demonstrate our\nmethodology's enhanced performance compared to traditional techniques, with an\naverage Pearson correlation score of 0.79.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nemani_P/0/1/0/all/0/1\">Praneeth Nemani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollala_S/0/1/0/all/0/1\">Satyanarayana Vollala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anti-Overestimation Dialogue Policy Learning for Task-Completion Dialogue System. (arXiv:2207.11762v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11762","description":"<p>A dialogue policy module is an essential part of task-completion dialogue\nsystems. Recently, increasing interest has focused on reinforcement learning\n(RL)-based dialogue policy. Its favorable performance and wise action decisions\nrely on an accurate estimation of action values. The overestimation problem is\na widely known issue of RL since its estimate of the maximum action value is\nlarger than the ground truth, which results in an unstable learning process and\nsuboptimal policy. This problem is detrimental to RL-based dialogue policy\nlearning. To mitigate this problem, this paper proposes a dynamic partial\naverage estimator (DPAV) of the ground truth maximum action value. DPAV\ncalculates the partial average between the predicted maximum action value and\nminimum action value, where the weights are dynamically adaptive and\nproblem-dependent. We incorporate DPAV into a deep Q-network as the dialogue\npolicy and show that our method can achieve better or comparable results\ncompared to top baselines on three dialogue datasets of different domains with\na lower computational load. In addition, we also theoretically prove the\nconvergence and derive the upper and lower bounds of the bias compared with\nthose of other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1\">Chang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenpeng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Sentiment-Aware Conversational Agent. (arXiv:2207.11774v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11774","description":"<p>In this paper, we propose an end-to-end sentiment-aware conversational agent\nbased on two models: a reply sentiment prediction model, which leverages the\ncontext of the dialogue to predict an appropriate sentiment for the agent to\nexpress in its reply; and a text generation model, which is conditioned on the\npredicted sentiment and the context of the dialogue, to produce a reply that is\nboth context and sentiment appropriate. Additionally, we propose to use a\nsentiment classification model to evaluate the sentiment expressed by the agent\nduring the development of the model. This allows us to evaluate the agent in an\nautomatic way. Both automatic and human evaluation results show that explicitly\nguiding the text generation model with a pre-defined set of sentences leads to\nclear improvements, both regarding the expressed sentiment and the quality of\nthe generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dias_I/0/1/0/all/0/1\">Isabel Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_P/0/1/0/all/0/1\">Patr&#xed;cia Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Luisa Coheur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancements to the BOUN Treebank Reflecting the Agglutinative Nature of Turkish. (arXiv:2207.11782v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11782","description":"<p>In this study, we aim to offer linguistically motivated solutions to resolve\nthe issues of the lack of representation of null morphemes, highly productive\nderivational processes, and syncretic morphemes of Turkish in the BOUN Treebank\nwithout diverging from the Universal Dependencies framework.\n</p>\n<p>In order to tackle these issues, new annotation conventions were introduced\nby splitting certain lemmas and employing the MISC (miscellaneous) tab in the\nUD framework to denote derivation. Representational capabilities of the\nre-annotated treebank were tested on a LSTM-based dependency parser and an\nupdated version of the BoAT Tool is introduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marsan_B/0/1/0/all/0/1\">B&#xfc;&#x15f;ra Mar&#x15f;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akkurt_S/0/1/0/all/0/1\">Salih Furkan Akkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_M/0/1/0/all/0/1\">Muhammet &#x15e;en</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurbuz_M/0/1/0/all/0/1\">Merve G&#xfc;rb&#xfc;z</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gungor_O/0/1/0/all/0/1\">Onur G&#xfc;ng&#xf6;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozates_S/0/1/0/all/0/1\">&#x15e;aziye Bet&#xfc;l &#xd6;zate&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uskudarli_S/0/1/0/all/0/1\">Suzan &#xdc;sk&#xfc;darl&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gungor_T/0/1/0/all/0/1\">Tunga G&#xfc;ng&#xf6;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozturk_B/0/1/0/all/0/1\">Balk&#x131;z &#xd6;zt&#xfc;rk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArmanEmo: A Persian Dataset for Text-based Emotion Detection. (arXiv:2207.11808v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11808","description":"<p>With the recent proliferation of open textual data on social media platforms,\nEmotion Detection (ED) from Text has received more attention over the past\nyears. It has many applications, especially for businesses and online service\nproviders, where emotion detection techniques can help them make informed\ncommercial decisions by analyzing customers/users' feelings towards their\nproducts and services. In this study, we introduce ArmanEmo, a human-labeled\nemotion dataset of more than 7000 Persian sentences labeled for seven\ncategories. The dataset has been collected from different resources, including\nTwitter, Instagram, and Digikala (an Iranian e-commerce company) comments.\nLabels are based on Ekman's six basic emotions (Anger, Fear, Happiness, Hatred,\nSadness, Wonder) and another category (Other) to consider any other emotion not\nincluded in Ekman's model. Along with the dataset, we have provided several\nbaseline models for emotion classification focusing on the state-of-the-art\ntransformer-based language models. Our best model achieves a macro-averaged F1\nscore of 75.39 percent across our test dataset. Moreover, we also conduct\ntransfer learning experiments to compare our proposed dataset's generalization\nagainst other Persian emotion datasets. Results of these experiments suggest\nthat our dataset has superior generalizability among the existing Persian\nemotion datasets. ArmanEmo is publicly available for non-commercial use at\nhttps://github.com/Arman-Rayan-Sharif/arman-text-emotion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzaee_H/0/1/0/all/0/1\">Hossein Mirzaee</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Peymanfard_J/0/1/0/all/0/1\">Javad Peymanfard</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Moshtaghin_H/0/1/0/all/0/1\">Hamid Habibzadeh Moshtaghin</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Zeinali_H/0/1/0/all/0/1\">Hossein Zeinali</a> (1) ((1) Amirkabir University of Technology, (2) Iran University of Science and Technology, (3) Allameh Tabataba&#x27;i University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gumbel-Attention for Multi-modal Machine Translation. (arXiv:2103.08862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.08862","description":"<p>Multi-modal machine translation (MMT) improves translation quality by\nintroducing visual information. However, the existing MMT model ignores the\nproblem that the image will bring information irrelevant to the text, causing\nmuch noise to the model and affecting the translation quality. This paper\nproposes a novel Gumbel-Attention for multi-modal machine translation, which\nselects the text-related parts of the image features. Specifically, different\nfrom the previous attention-based method, we first use a differentiable method\nto select the image information and automatically remove the useless parts of\nthe image features. Experiments prove that our method retains the image\nfeatures related to the text, and the remaining parts help the MMT model\ngenerates better translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hailong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOLEMAN: Mention-Only Linking of Entities with a Mention Annotation Network. (arXiv:2106.07352v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2106.07352","description":"<p>We present an instance-based nearest neighbor approach to entity linking. In\ncontrast to most prior entity retrieval systems which represent each entity\nwith a single vector, we build a contextualized mention-encoder that learns to\nplace similar mentions of the same entity closer in vector space than mentions\nof different entities. This approach allows all mentions of an entity to serve\nas \"class prototypes\" as inference involves retrieving from the full set of\nlabeled entity mentions in the training set and applying the nearest mention\nneighbor's entity label. Our model is trained on a large multilingual corpus of\nmention pairs derived from Wikipedia hyperlinks, and performs nearest neighbor\ninference on an index of 700 million mentions. It is simpler to train, gives\nmore interpretable predictions, and outperforms all other systems on two\nmultilingual entity linking benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_N/0/1/0/all/0/1\">Nicholas FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botha_J/0/1/0/all/0/1\">Jan A. Botha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1\">Daniel Gillick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1\">Daniel M. Bikel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwiatkowski_T/0/1/0/all/0/1\">Tom Kwiatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues. (arXiv:2108.13811v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13811","description":"<p>The goal of dialogue relation extraction (DRE) is to identify the relation\nbetween two entities in a given dialogue. During conversations, speakers may\nexpose their relations to certain entities by explicit or implicit clues, such\nevidences called \"triggers\". However, trigger annotations may not be always\navailable for the target data, so it is challenging to leverage such\ninformation for enhancing the performance. Therefore, this paper proposes to\nlearn how to identify triggers from the data with trigger annotations and then\ntransfers the trigger-finding capability to other datasets for better\nperformance. The experiments show that the proposed approach is capable of\nimproving relation extraction performance of unseen relations and also\ndemonstrate the transferability of our proposed trigger-finding model across\ndifferent domains and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Po-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shang-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Irrationality of Neural Rationale Models. (arXiv:2110.07550v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07550","description":"<p>Neural rationale models are popular for interpretable predictions of NLP\ntasks. In these, a selector extracts segments of the input text, called\nrationales, and passes these segments to a classifier for prediction. Since the\nrationale is the only information accessible to the classifier, it is plausibly\ndefined as the explanation. Is such a characterization unconditionally correct?\nIn this paper, we argue to the contrary, with both philosophical perspectives\nand empirical evidence suggesting that rationale models are, perhaps, less\nrational and interpretable than expected. We call for more rigorous and\ncomprehensive evaluations of these models to ensure desired properties of\ninterpretability are indeed achieved. The code can be found at\nhttps://github.com/yimingz89/Neural-Rationale-Analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yiming Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Booth_S/0/1/0/all/0/1\">Serena Booth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yilun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do You Know Your Audience? Toward Socially-aware Question Generation. (arXiv:2110.08445v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08445","description":"<p>When writing, a person may need to anticipate questions from their audience,\nbut different social groups may ask very different types of questions. If\nsomeone is writing about a problem they want to resolve, what kind of follow-up\nquestion will a domain expert ask, and could the writer better address the\nexpert's information needs by rewriting their original post? In this paper, we\nexplore the task of socially-aware question generation. We collect a data set\nof questions and posts from social media, including background information\nabout the question-askers' social groups. We find that different social groups,\nsuch as experts and novices, consistently ask different types of questions. We\ntrain several text-generation models that incorporate social information, and\nwe find that a discrete social-representation model outperforms the text-only\nmodel when different social groups ask highly different questions from one\nanother. Our work provides a framework for developing text generation models\nthat can help writers anticipate the information expectations of highly\ndifferent social groups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stewart_I/0/1/0/all/0/1\">Ian Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models. (arXiv:2110.08536v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08536","description":"<p>Distilling state-of-the-art transformer models into lightweight student\nmodels is an effective way to reduce computation cost at inference time. The\nstudent models are typically compact transformers with fewer parameters, while\nexpensive operations such as self-attention persist. Therefore, the improved\ninference speed may still be unsatisfactory for real-time or high-volume use\ncases. In this paper, we aim to further push the limit of inference speed by\ndistilling teacher models into bigger, sparser student models -- bigger in that\nthey scale up to billions of parameters; sparser in that most of the model\nparameters are n-gram embeddings. Our experiments on six single-sentence text\nclassification tasks show that these student models retain 97% of the\nRoBERTa-Large teacher performance on average, and meanwhile achieve up to 600x\nspeed-up on both GPUs and CPUs at inference time. Further investigation reveals\nthat our pipeline is also helpful for sentence-pair classification tasks, and\nin domain generalization settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qinyuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khabsa_M/0/1/0/all/0/1\">Madian Khabsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaech_A/0/1/0/all/0/1\">Aaron Jaech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13214","description":"<p>Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Structured Inference with Randomization. (arXiv:2112.03638v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.03638","description":"<p>Deep discrete structured models have seen considerable progress recently, but\ntraditional inference using dynamic programming (DP) typically works with a\nsmall number of states (less than hundreds), which severely limits model\ncapacity. At the same time, across machine learning, there is a recent trend of\nusing randomized truncation techniques to accelerate computations involving\nlarge sums. Here, we propose a family of randomized dynamic programming (RDP)\nalgorithms for scaling structured models to tens of thousands of latent states.\nOur method is widely applicable to classical DP-based inference (partition,\nmarginal, reparameterization, entropy) and different graph structures (chains,\ntrees, and more general hypergraphs). It is also compatible with automatic\ndifferentiation: it can be integrated with neural networks seamlessly and\nlearned with gradient-based optimizers. Our core technique approximates the\nsum-product by restricting and reweighting DP on a small subset of nodes, which\nreduces computation by orders of magnitude. We further achieve low bias and\nvariance via Rao-Blackwellization and importance sampling. Experiments over\ndifferent graphs demonstrate the accuracy and efficiency of our approach.\nFurthermore, when using RDP for training a structured variational autoencoder\nwith a scaled inference network, we achieve better test likelihood than\nbaselines and successfully prevent posterior collapse. code at:\nhttps://github.com/FranxYao/RDP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John P. Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal data matters: language model pre-training over structured and unstructured electronic health records. (arXiv:2201.10113v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.10113","description":"<p>The massive amount of electronic health records (EHR) has created enormous\npotential in improving healthcare. Clinical codes (structured data) and\nclinical narratives (unstructured data) are two important textual modalities in\nEHR. Clinical codes convey diagnostic and treatment information during the\nhospital, and clinical notes carry narratives of clinical providers for patient\nencounters. They do not exist in isolation and can complement each other in\nmost real-life clinical scenarios. However, most existing EHR-oriented studies\neither focus on a particular modality or integrate data from different\nmodalities in a straightforward manner, which ignores the intrinsic\ninteractions between them. To address these issues, we proposed a Medical\nMultimodal Pre-trained Language Model, named MedM-PLM, to learn enhanced EHR\nrepresentations over structured and unstructured data. In MedM-PLM, two\nTransformer-based neural network components are firstly adopted to learn\nrepresentative characteristics from each modality. A cross-modal module is then\nintroduced to model their interactions. We pre-trained MedM-PLM on the\nMIMIC-III dataset and verified the effectiveness of the model on three\ndownstream clinical tasks, i.e., medication recommendation, 30-day readmission\nprediction and ICD coding. Extensive experiments demonstrate the power of\nMedM-PLM compared with state-of-the-art methods. Further analyses and\nvisualizations show the robustness of our model, which could potentially\nprovide more comprehensive interpretations for clinical decision-making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yongshuai Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Buzhou Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Wikipedia Help Offline Reinforcement Learning?. (arXiv:2201.12122v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12122","description":"<p>Fine-tuning reinforcement learning (RL) models has been challenging because\nof a lack of large scale off-the-shelf datasets as well as high variance in\ntransferability among different environments. Recent work has looked at\ntackling offline RL from the perspective of sequence modeling with improved\nresults as result of the introduction of the Transformer architecture. However,\nwhen the model is trained from scratch, it suffers from slow convergence\nspeeds. In this paper, we look to take advantage of this formulation of\nreinforcement learning as sequence modeling and investigate the transferability\nof pre-trained sequence models on other domains (vision, language) when\nfinetuned on offline RL tasks (control, games). To this end, we also propose\ntechniques to improve transfer between these domains. Results show consistent\nperformance gains in terms of both convergence speed and reward on a variety of\nenvironments, accelerating training by 3-6x and achieving state-of-the-art\nperformance in a variety of tasks using Wikipedia-pretrained and GPT2 language\nmodels. We hope that this work not only brings light to the potentials of\nleveraging generic sequence modeling techniques and pre-trained models for RL,\nbut also inspires future work on sharing knowledge between generative modeling\ntasks of completely different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1\">Yutaro Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility. (arXiv:2202.02312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02312","description":"<p>Vision-language navigation (VLN), in which an agent follows language\ninstruction in a visual environment, has been studied under the premise that\nthe input command is fully feasible in the environment. Yet in practice, a\nrequest may not be possible due to language ambiguity or environment changes.\nTo study VLN with unknown command feasibility, we introduce a new dataset\nMobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete\na natural language command in a mobile app. Mobile apps provide a scalable\ndomain to study real downstream uses of VLN methods. Moreover, mobile app\ncommands provide instruction for interactive navigation, as they result in\naction sequences with state changes via clicking, typing, or swiping. MoTIF is\nthe first to include feasibility annotations, containing both binary\nfeasibility labels and fine-grained labels for why tasks are unsatisfiable. We\nfurther collect follow-up questions for ambiguous queries to enable research on\ntask uncertainty resolution. Equipped with our dataset, we propose the new\nproblem of feasibility prediction, in which a natural language instruction and\nmultimodal app environment are used to predict command feasibility. MoTIF\nprovides a more realistic app dataset as it contains many diverse environments,\nhigh-level goals, and longer action sequences than prior work. We evaluate\ninteractive VLN methods using MoTIF, quantify the generalization ability of\ncurrent approaches to new app environments, and measure the effect of task\nfeasibility on navigation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arsan_D/0/1/0/all/0/1\">Deniz Arsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sanjna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ranjitha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04800","description":"<p>Humans have remarkable capacity to reason abductively and hypothesize about\nwhat lies beyond the literal content of an image. By identifying concrete\nvisual clues scattered throughout a scene, we almost can't help but draw\nprobable inferences beyond the literal scene based on our everyday experience\nand knowledge about the world. For example, if we see a \"20 mph\" sign alongside\na road, we might assume the street sits in a residential area (rather than on a\nhighway), even if no houses are pictured. Can machines perform similar visual\nreasoning?\n</p>\n<p>We present Sherlock, an annotated corpus of 103K images for testing machine\ncapacity for abductive reasoning beyond literal image contents. We adopt a\nfree-viewing paradigm: participants first observe and identify salient clues\nwithin images (e.g., objects, actions) and then provide a plausible inference\nabout the scene, given the clue. In total, we collect 363K (clue, inference)\npairs, which form a first-of-its-kind abductive visual reasoning dataset. Using\nour corpus, we test three complementary axes of abductive reasoning. We\nevaluate the capacity of models to: i) retrieve relevant inferences from a\nlarge candidate corpus; ii) localize evidence for inferences via bounding\nboxes, and iii) compare plausible inferences to match human judgments on a\nnewly-collected diagnostic corpus of 19K Likert-scale judgments. While we find\nthat fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong\nbaselines, significant headroom exists between model performance and human\nagreement. Data, models, and leaderboard available at\n<a href=\"http://visualabduction.com/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semantic Embeddings for Ontology Subsumption Prediction. (arXiv:2202.09791v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.09791","description":"<p>Automating ontology construction and curation is an important but challenging\ntask in knowledge engineering and artificial intelligence. Prediction by\nmachine learning techniques such as contextual semantic embedding is a\npromising direction, but the relevant research is still preliminary especially\nfor expressive ontologies in Web Ontology Language (OWL). In this paper, we\npresent a new subsumption prediction method named BERTSubs for classes of OWL\nontology. It exploits the pre-trained language model BERT to compute contextual\nembeddings of a class, where customized templates are proposed to incorporate\nthe class context (e.g., neighbouring classes) and the logical existential\nrestriction. BERTSubs is quite general, being able to predict multiple kinds of\nsubsumers including named classes and existential restrictions from the same\nontology or another ontology. Extensive evaluation on five real-world\nontologies for three different subsumption tasks has shown the effectiveness of\nthe templates and that BERTSubs can dramatically outperform the baselines that\nuse (literal-aware) knowledge graph embeddings, non-contextual word embeddings\nand the state-of-the-art OWL ontology embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yuxia Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1\">Ernesto Jimenez-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniDU: Towards A Unified Generative Dialogue Understanding Framework. (arXiv:2204.04637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04637","description":"<p>With the development of pre-trained language models, remarkable success has\nbeen witnessed in dialogue understanding (DU). However, current DU approaches\nusually employ independent models for each distinct DU task without considering\nshared knowledge across different DU tasks. In this paper, we propose a unified\ngenerative dialogue understanding framework, named {\\em UniDU}, to achieve\neffective information exchange across diverse DU tasks. Here, we reformulate\nall DU tasks into a unified prompt-based generative model paradigm. More\nimportantly, a novel model-agnostic multi-task training strategy (MATS) is\nintroduced to dynamically adapt the weights of diverse tasks for best knowledge\nsharing during training, based on the nature and available data of each task.\nExperiments on ten DU datasets covering five fundamental DU tasks show that the\nproposed UniDU framework largely outperforms task-specific well-designed\nmethods on all tasks. MATS also reveals the knowledge-sharing structure of\nthese tasks. Finally, UniDU obtains promising performance in the unseen\ndialogue domain, showing the great potential for generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuncong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Su Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Redwood: Using Collision Detection to Grow a Large-Scale Intent Classification Dataset. (arXiv:2204.05483v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05483","description":"<p>Dialog systems must be capable of incorporating new skills via updates over\ntime in order to reflect new use cases or deployment scenarios. Similarly,\ndevelopers of such ML-driven systems need to be able to add new training data\nto an already-existing dataset to support these new skills. In intent\nclassification systems, problems can arise if training data for a new skill's\nintent overlaps semantically with an already-existing intent. We call such\ncases collisions. This paper introduces the task of intent collision detection\nbetween multiple datasets for the purposes of growing a system's skillset. We\nintroduce several methods for detecting collisions, and evaluate our methods on\nreal datasets that exhibit collisions. To highlight the need for intent\ncollision detection, we show that model performance suffers if new data is\nadded in such a way that does not arbitrate colliding intents. Finally, we use\ncollision detection to construct and benchmark a new dataset, Redwood, which is\ncomposed of 451 ntent categories from 13 original intent classification\ndatasets, making it the largest publicly available intent classification\nbenchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1\">Kevin Leach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction. (arXiv:2206.05238v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05238","description":"<p>The most prominent tasks in emotion analysis are to assign emotions to texts\nand to understand how emotions manifest in language. An observation for NLP is\nthat emotions can be communicated implicitly by referring to events, appealing\nto an empathetic, intersubjective understanding of events, even without\nexplicitly mentioning an emotion name. In psychology, the class of emotion\ntheories known as appraisal theories aims at explaining the link between events\nand emotions. Appraisals can be formalized as variables that measure a\ncognitive evaluation by people living through an event that they consider\nrelevant. They include the assessment if an event is novel, if the person\nconsiders themselves to be responsible, if it is in line with the own goals,\nand many others. Such appraisals explain which emotions are developed based on\nan event, e.g., that a novel situation can induce surprise or one with\nuncertain consequences could evoke fear. We analyze the suitability of\nappraisal theories for emotion analysis in text with the goal of understanding\nif appraisal concepts can reliably be reconstructed by annotators, if they can\nbe predicted by text classifiers, and if appraisal concepts help to identify\nemotion categories. To achieve that, we compile a corpus by asking people to\ntextually describe events that triggered particular emotions and to disclose\ntheir appraisals. Then, we ask readers to reconstruct emotions and appraisals\nfrom the text. This setup allows us to measure if emotions and appraisals can\nbe recovered purely from text and provides a human baseline. Our comparison of\ntext classification methods to human annotators shows that both can reliably\ndetect emotions and appraisals with similar performance. Therefore, appraisals\nconstitute an alternative computational emotion analysis paradigm and further\nimprove the categorization of emotions in text with joint models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning. (arXiv:2207.01780v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2207.01780","description":"<p>Program synthesis or code generation aims to generate a program that\nsatisfies a problem specification. Recent approaches using large-scale\npretrained language models (LMs) have shown promising results, yet they have\nsome critical limitations. In particular, they often follow a standard\nsupervised fine-tuning procedure to train a code generation model only from the\npairs of natural-language problem descriptions and ground-truth programs. Such\nparadigm largely ignores some important but potentially useful signals in the\nproblem specification such as unit tests, which thus often results in poor\nperformance when solving complex unseen coding tasks. To address the\nlimitations, we propose \"CodeRL\", a new framework for program synthesis tasks\nthrough pretrained LMs and deep reinforcement learning (RL). Specifically,\nduring training, we treat the code-generating LM as an actor network, and\nintroduce a critic network that is trained to predict the functional\ncorrectness of generated programs and provide dense feedback signals to the\nactor. During inference, we introduce a new generation procedure with a\ncritical sampling strategy that allows a model to automatically regenerate\nprograms based on feedback from example unit tests and critic scores. For the\nmodel backbones, we extended the encoder-decoder architecture of CodeT5 with\nenhanced learning objectives, larger model sizes, and better pretraining data.\nOur method not only achieves new SOTA results on the challenging APPS\nbenchmark, but also shows strong zero-shot transfer capability with new SOTA\nresults on the simpler MBPP benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotmare_A/0/1/0/all/0/1\">Akhilesh Deepak Gotmare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Whois? Deep Author Name Disambiguation using Bibliographic Data. (arXiv:2207.04772v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2207.04772","description":"<p>As the number of authors is increasing exponentially over years, the number\nof authors sharing the same names is increasing proportionally. This makes it\nchallenging to assign newly published papers to their adequate authors.\nTherefore, Author Name Ambiguity (ANA) is considered a critical open problem in\ndigital libraries. This paper proposes an Author Name Disambiguation (AND)\napproach that links author names to their real-world entities by leveraging\ntheir co-authors and domain of research. To this end, we use a collection from\nthe DBLP repository that contains more than 5 million bibliographic records\nauthored by around 2.6 million co-authors. Our approach first groups authors\nwho share the same last names and same first name initials. The author within\neach group is identified by capturing the relation with his/her co-authors and\narea of research, which is represented by the titles of the validated\npublications of the corresponding author. To this end, we train a neural\nnetwork model that learns from the representations of the co-authors and\ntitles. We validated the effectiveness of our approach by conducting extensive\nexperiments on a large dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boukhers_Z/0/1/0/all/0/1\">Zeyd Boukhers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahubali_N/0/1/0/all/0/1\">Nagaraj Asundi Bahubali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ILASR: Privacy-Preserving Incremental Learning for Automatic Speech Recognition at Production Scale. (arXiv:2207.09078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.09078","description":"<p>Incremental learning is one paradigm to enable model building and updating at\nscale with streaming data. For end-to-end automatic speech recognition (ASR)\ntasks, the absence of human annotated labels along with the need for privacy\npreserving policies for model building makes it a daunting challenge. Motivated\nby these challenges, in this paper we use a cloud based framework for\nproduction systems to demonstrate insights from privacy preserving incremental\nlearning for automatic speech recognition (ILASR). By privacy preserving, we\nmean, usage of ephemeral data which are not human annotated. This system is a\nstep forward for production levelASR models for incremental/continual learning\nthat offers near real-time test-bed for experimentation in the cloud for\nend-to-end ASR, while adhering to privacy-preserving policies. We show that the\nproposed system can improve the production models significantly(3%) over a new\ntime period of six months even in the absence of human annotated labels with\nvarying levels of weak supervision and large batch sizes in incremental\nlearning. This improvement is 20% over test sets with new words and phrases in\nthe new time period. We demonstrate the effectiveness of model building in a\nprivacy-preserving incremental fashion for ASR while further exploring the\nutility of having an effective teacher model and use of large batch sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_G/0/1/0/all/0/1\">Gopinath Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_G/0/1/0/all/0/1\">Gurpreet Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eakin_A/0/1/0/all/0/1\">Aaron Eakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1\">Anit Kumar Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1\">Jasha Droppo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlin_A/0/1/0/all/0/1\">Andy Oberlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandanoor_B/0/1/0/all/0/1\">Buddha Nandanoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_P/0/1/0/all/0/1\">Prahalad Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitpure_P/0/1/0/all/0/1\">Pankaj Sitpure</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Brain tumor detection using artificial convolutional neural networks. (arXiv:2207.11248v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11248","description":"<p>In this paper, a convolutional neural network (CNN) was used to classify NMR\nimages of human brains with 4 different types of tumors: meningioma, glioma and\npituitary gland tumors. During the training phase of this project, an accuracy\nof 100% was obtained, meanwhile, in the evaluation phase the precision was 96%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Melchor_J/0/1/0/all/0/1\">Javier Melchor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sotelo_B/0/1/0/all/0/1\">Balam Sotelo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vera_J/0/1/0/all/0/1\">Jorge Vera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corral_H/0/1/0/all/0/1\">Horacio Corral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rich Feature Distillation with Feature Affinity Module for Efficient Image Dehazing. (arXiv:2207.11250v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11250","description":"<p>Single-image haze removal is a long-standing hurdle for computer vision\napplications. Several works have been focused on transferring advances from\nimage classification, detection, and segmentation to the niche of image\ndehazing, primarily focusing on contrastive learning and knowledge\ndistillation. However, these approaches prove computationally expensive,\nraising concern regarding their applicability to on-the-edge use-cases. This\nwork introduces a simple, lightweight, and efficient framework for single-image\nhaze removal, exploiting rich \"dark-knowledge\" information from a lightweight\npre-trained super-resolution model via the notion of heterogeneous knowledge\ndistillation. We designed a feature affinity module to maximize the flow of\nrich feature semantics from the super-resolution teacher to the student\ndehazing network. In order to evaluate the efficacy of our proposed framework,\nits performance as a plug-and-play setup to a baseline model is examined. Our\nexperiments are carried out on the RESIDE-Standard dataset to demonstrate the\nrobustness of our framework to the synthetic and real-world domains. The\nextensive qualitative and quantitative results provided establish the\neffectiveness of the framework, achieving gains of upto 15\\% (PSNR) while\nreducing the model size by $\\sim$20 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1\">Anushri Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S%2E_N/0/1/0/all/0/1\">Nisha J. S.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopi_V/0/1/0/all/0/1\">Varun P. Gopi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PieTrack: An MOT solution based on synthetic data training and self-supervised domain adaptation. (arXiv:2207.11325v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11325","description":"<p>In order to cope with the increasing demand for labeling data and privacy\nissues with human detection, synthetic data has been used as a substitute and\nshowing promising results in human detection and tracking tasks. We participate\nin the 7th Workshop on Benchmarking Multi-Target Tracking (BMTT), themed on\n\"How Far Can Synthetic Data Take us\"? Our solution, PieTrack, is developed\nbased on synthetic data without using any pre-trained weights. We propose a\nself-supervised domain adaptation method that enables mitigating the domain\nshift issue between the synthetic (e.g., MOTSynth) and real data (e.g., MOT17)\nwithout involving extra human labels. By leveraging the proposed multi-scale\nensemble inference, we achieved a final HOTA score of 58.7 on the MOT17 testing\nset, ranked third place in the challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shenghua He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Youbao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1\">Sanliang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junjie Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Ruei-Sung Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Swin Transformers for Egocentric Video Understanding @ Ego4D Challenges 2022. (arXiv:2207.11329v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11329","description":"<p>We implemented Video Swin Transformer as a base architecture for the tasks of\nPoint-of-No-Return temporal localization and Object State Change\nClassification. Our method achieved competitive performance on both challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Escobar_M/0/1/0/all/0/1\">Maria Escobar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daza_L/0/1/0/all/0/1\">Laura Daza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Cristina Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Graph Reasoning for Multi-person 3D Pose Estimation. (arXiv:2207.11341v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11341","description":"<p>Multi-person 3D pose estimation is a challenging task because of occlusion\nand depth ambiguity, especially in the cases of crowd scenes. To solve these\nproblems, most existing methods explore modeling body context cues by enhancing\nfeature representation with graph neural networks or adding structural\nconstraints. However, these methods are not robust for their single-root\nformulation that decoding 3D poses from a root node with a pre-defined graph.\nIn this paper, we propose GR-M3D, which models the \\textbf{M}ulti-person\n\\textbf{3D} pose estimation with dynamic \\textbf{G}raph \\textbf{R}easoning. The\ndecoding graph in GR-M3D is predicted instead of pre-defined. In particular, It\nfirstly generates several data maps and enhances them with a scale and depth\naware refinement module (SDAR). Then multiple root keypoints and dense decoding\npaths for each person are estimated from these data maps. Based on them,\ndynamic decoding graphs are built by assigning path weights to the decoding\npaths, while the path weights are inferred from those enhanced data maps. And\nthis process is named dynamic graph reasoning (DGR). Finally, the 3D poses are\ndecoded according to dynamic decoding graphs for each detected person. GR-M3D\ncan adjust the structure of the decoding graph implicitly by adopting soft path\nweights according to input data, which makes the decoding graphs be adaptive to\ndifferent input persons to the best extent and more capable of handling\nocclusion and depth ambiguity than previous methods. We empirically show that\nthe proposed bottom-up approach even outperforms top-down methods and achieves\nstate-of-the-art results on three 3D pose datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhongwei Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qiansheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Dongmei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Impartial Take to the CNN vs Transformer Robustness Contest. (arXiv:2207.11347v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11347","description":"<p>Following the surge of popularity of Transformers in Computer Vision, several\nstudies have attempted to determine whether they could be more robust to\ndistribution shifts and provide better uncertainty estimates than Convolutional\nNeural Networks (CNNs). The almost unanimous conclusion is that they are, and\nit is often conjectured more or less explicitly that the reason of this\nsupposed superiority is to be attributed to the self-attention mechanism. In\nthis paper we perform extensive empirical analyses showing that recent\nstate-of-the-art CNNs (particularly, ConvNeXt) can be as robust and reliable or\neven sometimes more than the current state-of-the-art Transformers. However,\nthere is no clear winner. Therefore, although it is tempting to state the\ndefinitive superiority of one family of architectures over another, they seem\nto enjoy similar extraordinary performances on a variety of tasks while also\nsuffering from similar vulnerabilities such as texture, background, and\nsimplicity biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinto_F/0/1/0/all/0/1\">Francesco Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep neural network heatmaps capture Alzheimer's disease patterns reported in a large meta-analysis of neuroimaging studies. (arXiv:2207.11352v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11352","description":"<p>Deep neural networks currently provide the most advanced and accurate machine\nlearning models to distinguish between structural MRI scans of subjects with\nAlzheimer's disease and healthy controls. Unfortunately, the subtle brain\nalterations captured by these models are difficult to interpret because of the\ncomplexity of these multi-layer and non-linear models. Several heatmap methods\nhave been proposed to address this issue and analyze the imaging patterns\nextracted from the deep neural networks, but no quantitative comparison between\nthese methods has been carried out so far. In this work, we explore these\nquestions by deriving heatmaps from Convolutional Neural Networks (CNN) trained\nusing T1 MRI scans of the ADNI data set, and by comparing these heatmaps with\nbrain maps corresponding to Support Vector Machines (SVM) coefficients. Three\nprominent heatmap methods are studied: Layer-wise Relevance Propagation (LRP),\nIntegrated Gradients (IG), and Guided Grad-CAM (GGC). Contrary to prior studies\nwhere the quality of heatmaps was visually or qualitatively assessed, we\nobtained precise quantitative measures by computing overlap with a ground-truth\nmap from a large meta-analysis that combined 77 voxel-based morphometry (VBM)\nstudies independently from ADNI. Our results indicate that all three heatmap\nmethods were able to capture brain regions covering the meta-analysis map and\nachieved better results than SVM coefficients. Among them, IG produced the\nheatmaps with the best overlap with the independent meta-analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honnorat_N/0/1/0/all/0/1\">Nicolas Honnorat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fox_P/0/1/0/all/0/1\">Peter T. Fox</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ritter_K/0/1/0/all/0/1\">Kerstin Ritter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eickhoff_S/0/1/0/all/0/1\">Simon B. Eickhoff</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seshadri_S/0/1/0/all/0/1\">Sudha Seshadri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habes_M/0/1/0/all/0/1\">Mohamad Habes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric scene context for human-centric environment understanding from video. (arXiv:2207.11365v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11365","description":"<p>First-person video highlights a camera-wearer's activities in the context of\ntheir persistent environment. However, current video understanding approaches\nreason over visual features from short video clips that are detached from the\nunderlying physical space and only capture what is directly seen. We present an\napproach that links egocentric video and camera pose over time by learning\nrepresentations that are predictive of the camera-wearer's (potentially unseen)\nlocal surroundings to facilitate human-centric environment understanding. We\ntrain such models using videos from agents in simulated 3D environments where\nthe environment is fully observable, and test them on real-world videos of\nhouse tours from unseen environments. We show that by grounding videos in their\nphysical environment, our models surpass traditional scene classification\nmodels at predicting which room a camera-wearer is in (where frame-level\ninformation is insufficient), and can leverage this grounding to localize video\nmoments corresponding to environment-centric queries, outperforming prior\nmethods. Project page: <a href=\"http://vision.cs.utexas.edu/projects/ego-scene-context/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1\">Tushar Nagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1\">Santhosh Kumar Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_R/0/1/0/all/0/1\">Ruta Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hillis_J/0/1/0/all/0/1\">James Hillis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural-Sim: Learning to Generate Training Data with NeRF. (arXiv:2207.11368v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11368","description":"<p>Training computer vision models usually requires collecting and labeling vast\namounts of imagery under a diverse set of scene configurations and properties.\nThis process is incredibly time-consuming, and it is challenging to ensure that\nthe captured data distribution maps well to the target domain of an application\nscenario. Recently, synthetic data has emerged as a way to address both of\nthese issues. However, existing approaches either require human experts to\nmanually tune each scene property or use automatic methods that provide little\nto no control; this requires rendering large amounts of random data variations,\nwhich is slow and is often suboptimal for the target domain. We present the\nfirst fully differentiable synthetic data pipeline that uses Neural Radiance\nFields (NeRFs) in a closed-loop with a target application's loss function. Our\napproach generates data on-demand, with no human labor, to maximize accuracy\nfor a target task. We illustrate the effectiveness of our method on synthetic\nand real-world object detection tasks. We also introduce a new\n\"YCB-in-the-Wild\" dataset and benchmark that provides a test scenario for\nobject detection with varied poses in real-world environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yunhao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behl_H/0/1/0/all/0/1\">Harkirat Behl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiashu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1\">Laurent Itti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Different Annotation Strategies for Deployment of Parking Spaces Classification Systems. (arXiv:2207.11372v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11372","description":"<p>When using vision-based approaches to classify individual parking spaces\nbetween occupied and empty, human experts often need to annotate the locations\nand label a training set containing images collected in the target parking lot\nto fine-tune the system. We propose investigating three annotation types\n(polygons, bounding boxes, and fixed-size squares), providing different data\nrepresentations of the parking spaces. The rationale is to elucidate the best\ntrade-off between handcraft annotation precision and model performance. We also\ninvestigate the number of annotated parking spaces necessary to fine-tune a\npre-trained model in the target parking lot. Experiments using the PKLot\ndataset show that it is possible to fine-tune a model to the target parking lot\nwith less than 1,000 labeled samples, using low precision annotations such as\nfixed-size squares.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hochuli_A/0/1/0/all/0/1\">Andre G. Hochuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1\">Alceu S. Britto Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almeida_P/0/1/0/all/0/1\">Paulo R. L. de Almeida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alves_W/0/1/0/all/0/1\">Williams B. S. Alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cagni_F/0/1/0/all/0/1\">Fabio M. C. Cagni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Perceptually Aligned Gradients Imply Adversarial Robustness?. (arXiv:2207.11378v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11378","description":"<p>In the past decade, deep learning-based networks have achieved unprecedented\nsuccess in numerous tasks, including image classification. Despite this\nremarkable achievement, recent studies have demonstrated that such networks are\neasily fooled by small malicious perturbations, also known as adversarial\nexamples. This security weakness led to extensive research aimed at obtaining\nrobust models. Beyond the clear robustness benefits of such models, it was also\nobserved that their gradients with respect to the input align with human\nperception. Several works have identified Perceptually Aligned Gradients (PAG)\nas a byproduct of robust training, but none have considered it as a standalone\nphenomenon nor studied its own implications. In this work, we focus on this\ntrait and test whether Perceptually Aligned Gradients imply Robustness. To this\nend, we develop a novel objective to directly promote PAG in training\nclassifiers and examine whether models with such gradients are more robust to\nadversarial attacks. Extensive experiments on CIFAR-10 and STL validate that\nsuch models have improved robust performance, exposing the surprising\nbidirectional connection between PAG and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1\">Roy Ganz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawar_B/0/1/0/all/0/1\">Bahjat Kawar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Aspect Information Fusion Model For ABAW4 Multi-task Challenge. (arXiv:2207.11389v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11389","description":"<p>In this paper, we propose the solution to the Multi-Task Learning (MTL)\nChallenge of the 4th Affective Behavior Analysis in-the-wild (ABAW)\ncompetition. The task of ABAW is to predict frame-level emotion descriptors\nfrom videos: discrete emotional state; valence and arousal; and action units.\nAlthough researchers have proposed several approaches and achieved promising\nresults in ABAW, current works in this task rarely consider interactions\nbetween different emotion descriptors. To this end, we propose a novel end to\nend architecture to achieve full integration of different types of information.\nExperimental results demonstrate the effectiveness of our proposed solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haiyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zheng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Licai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Cong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Pneumonia: Attention-Based Contrastive Learning for Class-Imbalanced Pneumonia Lesion Recognition in Chest X-rays. (arXiv:2207.11393v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11393","description":"<p>Computer-aided X-ray pneumonia lesion recognition is important for accurate\ndiagnosis of pneumonia. With the emergence of deep learning, the identification\naccuracy of pneumonia has been greatly improved, but there are still some\nchallenges due to the fuzzy appearance of chest X-rays. In this paper, we\npropose a deep learning framework named Attention-Based Contrastive Learning\nfor Class-Imbalanced X-Ray Pneumonia Lesion Recognition (denoted as Deep\nPneumonia). We adopt self-supervised contrastive learning strategy to pre-train\nthe model without using extra pneumonia data for fully mining the limited\navailable dataset. In order to leverage the location information of the lesion\narea that the doctor has painstakingly marked, we propose mask-guided hard\nattention strategy and feature learning with contrastive regulation strategy\nwhich are applied on the attention map and the extracted features respectively\nto guide the model to focus more attention on the lesion area where contains\nmore discriminative features for improving the recognition performance. In\naddition, we adopt Class-Balanced Loss instead of traditional Cross-Entropy as\nthe loss function of classification to tackle the problem of serious class\nimbalance between different classes of pneumonia in the dataset. The\nexperimental results show that our proposed framework can be used as a reliable\ncomputer-aided pneumonia diagnosis system to assist doctors to better diagnose\npneumonia cases accurately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">Haohan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xianshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orientation and Context Entangled Network for Retinal Vessel Segmentation. (arXiv:2207.11396v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11396","description":"<p>Most of the existing deep learning based methods for vessel segmentation\nneglect two important aspects of retinal vessels, one is the orientation\ninformation of vessels, and the other is the contextual information of the\nwhole fundus region. In this paper, we propose a robust Orientation and Context\nEntangled Network (denoted as OCE-Net), which has the capability of extracting\ncomplex orientation and context information of the blood vessels. To achieve\ncomplex orientation aware, a Dynamic Complex Orientation Aware Convolution\n(DCOA Conv) is proposed to extract complex vessels with multiple orientations\nfor improving the vessel continuity. To simultaneously capture the global\ncontext information and emphasize the important local information, a Global and\nLocal Fusion Module (GLFM) is developed to simultaneously model the long-range\ndependency of vessels and focus sufficient attention on local thin vessels. A\nnovel Orientation and Context Entangled Non-local (OCE-NL) module is proposed\nto entangle the orientation and context information together. In addition, an\nUnbalanced Attention Refining Module (UARM) is proposed to deal with the\nunbalanced pixel numbers of background, thick and thin vessels. Extensive\nexperiments were performed on several commonly used datasets (DRIVE, STARE and\nCHASEDB1) and some more challenging datasets (AV-WIDE, UoA-DR, RFMiD and UK\nBiobank). The ablation study shows that the proposed method achieves promising\nperformance on maintaining the continuity of thin vessels and the comparative\nexperiments demonstrate that our OCE-Net can achieve state-of-the-art\nperformance on retinal vessel segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinxu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaifu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bzdok_D/0/1/0/all/0/1\">Danilo Bzdok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chunk-aware Alignment and Lexical Constraint for Visual Entailment with Natural Language Explanations. (arXiv:2207.11401v1 [cs.CL])","link":"http://arxiv.org/abs/2207.11401","description":"<p>Visual Entailment with natural language explanations aims to infer the\nrelationship between a text-image pair and generate a sentence to explain the\ndecision-making process. Previous methods rely mainly on a pre-trained\nvision-language model to perform the relation inference and a language model to\ngenerate the corresponding explanation. However, the pre-trained\nvision-language models mainly build token-level alignment between text and\nimage yet ignore the high-level semantic alignment between the phrases (chunks)\nand visual contents, which is critical for vision-language reasoning. Moreover,\nthe explanation generator based only on the encoded joint representation does\nnot explicitly consider the critical decision-making points of relation\ninference. Thus the generated explanations are less faithful to visual-language\nreasoning. To mitigate these problems, we propose a unified Chunk-aware\nAlignment and Lexical Constraint based method, dubbed as CALeC. It contains a\nChunk-aware Semantic Interactor (arr. CSI), a relation inferrer, and a Lexical\nConstraint-aware Generator (arr. LeCG). Specifically, CSI exploits the sentence\nstructure inherent in language and various image regions to build chunk-aware\nsemantic alignment. Relation inferrer uses an attention-based reasoning network\nto incorporate the token-level and chunk-level vision-language representations.\nLeCG utilizes lexical constraints to expressly incorporate the words or chunks\nfocused by the relation inferrer into explanation generation, improving the\nfaithfulness and informativeness of the explanations. We conduct extensive\nexperiments on three datasets, and experimental results indicate that CALeC\nsignificantly outperforms other competitor models on inference accuracy and\nquality of generated explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PS-NeRF: Neural Inverse Rendering for Multi-view Photometric Stereo. (arXiv:2207.11406v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11406","description":"<p>Traditional multi-view photometric stereo (MVPS) methods are often composed\nof multiple disjoint stages, resulting in noticeable accumulated errors. In\nthis paper, we present a neural inverse rendering method for MVPS based on\nimplicit representation. Given multi-view images of a non-Lambertian object\nilluminated by multiple unknown directional lights, our method jointly\nestimates the geometry, materials, and lights. Our method first employs\nmulti-light images to estimate per-view surface normal maps, which are used to\nregularize the normals derived from the neural radiance field. It then jointly\noptimizes the surface normals, spatially-varying BRDFs, and lights based on a\nshadow-aware differentiable rendering layer. After optimization, the\nreconstructed object can be used for novel-view rendering, relighting, and\nmaterial editing. Experiments on both synthetic and real datasets demonstrate\nthat our method achieves far more accurate shape reconstruction than existing\nMVPS and neural rendering methods. Our code and model can be found at\nhttps://ywq.github.io/psnerf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenfang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kwan-Yee K. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Halftoning with Multi-Agent Deep Reinforcement Learning. (arXiv:2207.11408v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11408","description":"<p>Deep neural networks have recently succeeded in digital halftoning using\nvanilla convolutional layers with high parallelism. However, existing deep\nmethods fail to generate halftones with a satisfying blue-noise property and\nrequire complex training schemes. In this paper, we propose a halftoning method\nbased on multi-agent deep reinforcement learning, called HALFTONERS, which\nlearns a shared policy to generate high-quality halftone images. Specifically,\nwe view the decision of each binary pixel value as an action of a virtual\nagent, whose policy is trained by a low-variance policy gradient. Moreover, the\nblue-noise property is achieved by a novel anisotropy suppressing loss\nfunction. Experiments show that our halftoning method produces high-quality\nhalftones while staying relatively fast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haitian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Dongliang Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaowen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1\">Aiguo Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Li Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satellite Detection in Unresolved Space Imagery for Space Domain Awareness Using Neural Networks. (arXiv:2207.11412v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11412","description":"<p>This work utilizes a MobileNetV2 Convolutional Neural Network (CNN) for fast,\nmobile detection of satellites, and rejection of stars, in cluttered unresolved\nspace imagery. First, a custom database is created using imagery from a\nsynthetic satellite image program and labeled with bounding boxes over\nsatellites for \"satellite-positive\" images. The CNN is then trained on this\ndatabase and the inference is validated by checking the accuracy of the model\non an external dataset constructed of real telescope imagery. In doing so, the\ntrained CNN provides a method of rapid satellite identification for subsequent\nutilization in ground-based orbit estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jordan_J/0/1/0/all/0/1\">Jarred Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posada_D/0/1/0/all/0/1\">Daniel Posada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuehlke_D/0/1/0/all/0/1\">David Zuehlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radulovic_A/0/1/0/all/0/1\">Angelica Radulovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1\">Aryslan Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_T/0/1/0/all/0/1\">Troy Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection and Initial Assessment of Lunar Landing Sites Using Neural Networks. (arXiv:2207.11413v1 [cs.RO])","link":"http://arxiv.org/abs/2207.11413","description":"<p>Robotic and human lunar landings are a focus of future NASA missions.\nPrecision landing capabilities are vital to guarantee the success of the\nmission, and the safety of the lander and crew. During the approach to the\nsurface there are multiple challenges associated with Hazard Relative\nNavigation to ensure safe landings. This paper will focus on a passive\nautonomous hazard detection and avoidance sub-system to generate an initial\nassessment of possible landing regions for the guidance system. The system uses\na single camera and the MobileNetV2 neural network architecture to detect and\ndiscern between safe landing sites and hazards such as rocks, shadows, and\ncraters. Then a monocular structure from motion will recreate the surface to\nprovide slope and roughness analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Posada_D/0/1/0/all/0/1\">Daniel Posada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_J/0/1/0/all/0/1\">Jarred Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radulovic_A/0/1/0/all/0/1\">Angelica Radulovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lillian Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1\">Aryslan Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_T/0/1/0/all/0/1\">Troy Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arbitrary Style Transfer with Structure Enhancement by Combining the Global and Local Loss. (arXiv:2207.11438v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11438","description":"<p>Arbitrary style transfer generates an artistic image which combines the\nstructure of a content image and the artistic style of the artwork by using\nonly one trained network. The image representation used in this method contains\ncontent structure representation and the style patterns representation, which\nis usually the features representation of high-level in the pre-trained\nclassification networks. However, the traditional classification networks were\ndesigned for classification which usually focus on high-level features and\nignore other features. As the result, the stylized images distribute style\nelements evenly throughout the image and make the overall image structure\nunrecognizable. To solve this problem, we introduce a novel arbitrary style\ntransfer method with structure enhancement by combining the global and local\nloss. The local structure details are represented by Lapstyle and the global\nstructure is controlled by the image depth. Experimental results demonstrate\nthat our method can generate higher-quality images with impressive visual\neffects on several common datasets, comparing with other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_L/0/1/0/all/0/1\">Lizhen Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pun_C/0/1/0/all/0/1\">Chi-Man Pun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Spatio-Temporal Debiasing for Video Scene Graph Generation. (arXiv:2207.11441v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11441","description":"<p>Video scene graph generation (VidSGG) aims to parse the video content into\nscene graphs, which involves modeling the spatio-temporal contextual\ninformation in the video. However, due to the long-tailed training data in\ndatasets, the generalization performance of existing VidSGG models can be\naffected by the spatio-temporal conditional bias problem. In this work, from\nthe perspective of meta-learning, we propose a novel Meta Video Scene Graph\nGeneration (MVSGG) framework to address such a bias problem. Specifically, to\nhandle various types of spatio-temporal conditional biases, our framework first\nconstructs a support set and a group of query sets from the training data,\nwhere the data distribution of each query set is different from that of the\nsupport set w.r.t. a type of conditional bias. Then, by performing a novel meta\ntraining and testing process to optimize the model to obtain good testing\nperformance on these query sets after training on the support set, our\nframework can effectively guide the model to learn to well generalize against\nbiases. Extensive experiments demonstrate the efficacy of our proposed\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Li Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Haoxuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BuyTheDips: PathLoss for improved topology-preserving deep learning-based image segmentation. (arXiv:2207.11446v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11446","description":"<p>Capturing the global topology of an image is essential for proposing an\naccurate segmentation of its domain. However, most of existing segmentation\nmethods do not preserve the initial topology of the given input, which is\ndetrimental for numerous downstream object-based tasks. This is all the more\ntrue for deep learning models which most work at local scales. In this paper,\nwe propose a new topology-preserving deep image segmentation method which\nrelies on a new leakage loss: the Pathloss. Our method is an extension of the\nBALoss [1], in which we want to improve the leakage detection for better\nrecovering the closeness property of the image segmentation. This loss allows\nus to correctly localize and fix the critical points (a leakage in the\nboundaries) that could occur in the predictions, and is based on a\nshortest-path search algorithm. This way, loss minimization enforces\nconnectivity only where it is necessary and finally provides a good\nlocalization of the boundaries of the objects in the image. Moreover, according\nto our research, our Pathloss learns to preserve stronger elongated structure\ncompared to methods without using topology-preserving loss. Training with our\ntopological loss function, our method outperforms state-of-the-art\ntopology-aware methods on two representative datasets of different natures:\nElectron Microscopy and Historical Map.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngoc_M/0/1/0/all/0/1\">Minh On Vu Ngoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yizi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boutry_N/0/1/0/all/0/1\">Nicolas Boutry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabrizio_J/0/1/0/all/0/1\">Jonathan Fabrizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallet_C/0/1/0/all/0/1\">Clement Mallet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UC-OWOD: Unknown-Classified Open World Object Detection. (arXiv:2207.11455v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11455","description":"<p>Open World Object Detection (OWOD) is a challenging computer vision problem\nthat requires detecting unknown objects and gradually learning the identified\nunknown classes. However, it cannot distinguish unknown instances as multiple\nunknown classes. In this work, we propose a novel OWOD problem called\nUnknown-Classified Open World Object Detection (UC-OWOD). UC-OWOD aims to\ndetect unknown instances and classify them into different unknown classes.\nBesides, we formulate the problem and devise a two-stage object detector to\nsolve UC-OWOD. First, unknown label-aware proposal and unknown-discriminative\nclassification head are used to detect known and unknown objects. Then,\nsimilarity-based unknown classification and unknown clustering refinement\nmodules are constructed to distinguish multiple unknown classes. Moreover, two\nnovel evaluation protocols are designed to evaluate unknown-class detection.\nAbundant experiments and visualizations prove the effectiveness of the proposed\nmethod. Code is available at https://github.com/JohnWuzh/UC-OWOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yue Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhengxing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Liwen Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Junzhi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Counting Meets HMER: Counting-Aware Network for Handwritten Mathematical Expression Recognition. (arXiv:2207.11463v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11463","description":"<p>Recently, most handwritten mathematical expression recognition (HMER) methods\nadopt the encoder-decoder networks, which directly predict the markup sequences\nfrom formula images with the attention mechanism. However, such methods may\nfail to accurately read formulas with complicated structure or generate long\nmarkup sequences, as the attention results are often inaccurate due to the\nlarge variance of writing styles or spatial layouts. To alleviate this problem,\nwe propose an unconventional network for HMER named Counting-Aware Network\n(CAN), which jointly optimizes two tasks: HMER and symbol counting.\nSpecifically, we design a weakly-supervised counting module that can predict\nthe number of each symbol class without the symbol-level position annotations,\nand then plug it into a typical attention-based encoder-decoder model for HMER.\nExperiments on the benchmark datasets for HMER validate that both joint\noptimization and counting results are beneficial for correcting the prediction\nerrors of encoder-decoder models, and CAN consistently outperforms the\nstate-of-the-art methods. In particular, compared with an encoder-decoder model\nfor HMER, the extra time cost caused by the proposed counting module is\nmarginal. The source code is available at https://github.com/LBH1024/CAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Ye Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Dingkang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinfeng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Object Placement via Dual-path Graph Completion. (arXiv:2207.11464v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11464","description":"<p>Object placement aims to place a foreground object over a background image\nwith a suitable location and size. In this work, we treat object placement as a\ngraph completion problem and propose a novel graph completion module (GCM). The\nbackground scene is represented by a graph with multiple nodes at different\nspatial locations with various receptive fields. The foreground object is\nencoded as a special node that should be inserted at a reasonable place in this\ngraph. We also design a dual-path framework upon the structure of GCM to fully\nexploit annotated composite images. With extensive experiments on OPA dataset,\nour method proves to significantly outperform existing methods in generating\nplausible object placement without loss of diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Siyuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CompNVS: Novel View Synthesis with Scene Completion. (arXiv:2207.11467v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11467","description":"<p>We introduce a scalable framework for novel view synthesis from RGB-D images\nwith largely incomplete scene coverage. While generative neural approaches have\ndemonstrated spectacular results on 2D images, they have not yet achieved\nsimilar photorealistic results in combination with scene completion where a\nspatial 3D scene understanding is essential. To this end, we propose a\ngenerative pipeline performing on a sparse grid-based neural scene\nrepresentation to complete unobserved scene parts via a learned distribution of\nscenes in a 2.5D-3D-2.5D manner. We process encoded image features in 3D space\nwith a geometry completion network and a subsequent texture inpainting network\nto extrapolate the missing area. Photorealistic image sequences can be finally\nobtained via consistency-relevant differentiable rendering. Comprehensive\nexperiments show that the graphical outputs of our method outperform the state\nof the art, especially within unobserved scene parts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuoyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1\">Tianxing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Scene Text Erasing with Self-Supervision. (arXiv:2207.11469v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11469","description":"<p>Scene text erasing seeks to erase text contents from scene images and current\nstate-of-the-art text erasing models are trained on large-scale synthetic data.\nAlthough data synthetic engines can provide vast amounts of annotated training\nsamples, there are differences between synthetic and real-world data. In this\npaper, we employ self-supervision for feature representation on unlabeled\nreal-world scene text images. A novel pretext task is designed to keep\nconsistent among text stroke masks of image variants. We design the Progressive\nErasing Network in order to remove residual texts. The scene text is erased\nprogressively by leveraging the intermediate generated results which provide\nthe foundation for subsequent higher quality results. Experiments show that our\nmethod significantly improves the generalization of the text erasing task and\nachieves state-of-the-art performance on public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiangcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yingbin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xingjiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tianlong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Cheng Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Labeling Tool. (arXiv:2207.11479v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11479","description":"<p>Training and testing supervised object detection models require a large\ncollection of images with ground truth labels. Labels define object classes in\nthe image, as well as their locations, shape, and possibly other information\nsuch as pose. The labeling process has proven extremely time consuming, even\nwith the presence of manpower. We introduce a novel labeling tool for 2D images\nas well as 3D triangular meshes: 3D Labeling Tool (3DLT). This is a standalone,\nfeature-heavy and cross-platform software that does not require installation\nand can run on Windows, macOS and Linux-based distributions. Instead of\nlabeling the same object on every image separately like current tools, we use\ndepth information to reconstruct a triangular mesh from said images and label\nthe object only once on the aforementioned mesh. We use registration to\nsimplify 3D labeling, outlier detection to improve 2D bounding box calculation\nand surface reconstruction to expand labeling possibility to large point\nclouds. Our tool is tested against state of the art methods and it greatly\nsurpasses them in terms of speed while preserving accuracy and ease of use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rachwan_J/0/1/0/all/0/1\">John Rachwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalaket_C/0/1/0/all/0/1\">Charbel Zalaket</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Emotion Recognition with Modality-Pairwise Unsupervised Contrastive Loss. (arXiv:2207.11482v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11482","description":"<p>Emotion recognition is involved in several real-world applications. With an\nincrease in available modalities, automatic understanding of emotions is being\nperformed more accurately. The success in Multimodal Emotion Recognition (MER),\nprimarily relies on the supervised learning paradigm. However, data annotation\nis expensive, time-consuming, and as emotion expression and perception depends\non several factors (e.g., age, gender, culture) obtaining labels with a high\nreliability is hard. Motivated by these, we focus on unsupervised feature\nlearning for MER. We consider discrete emotions, and as modalities text, audio\nand vision are used. Our method, as being based on contrastive loss between\npairwise modalities, is the first attempt in MER literature. Our end-to-end\nfeature learning approach has several differences (and advantages) compared to\nexisting MER methods: i) it is unsupervised, so the learning is lack of data\nlabelling cost; ii) it does not require data spatial augmentation, modality\nalignment, large number of batch size or epochs; iii) it applies data fusion\nonly at inference; and iv) it does not require backbones pre-trained on emotion\nrecognition task. The experiments on benchmark datasets show that our method\noutperforms several baseline approaches and unsupervised learning methods\napplied in MER. Particularly, it even surpasses a few supervised MER\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franceschini_R/0/1/0/all/0/1\">Riccardo Franceschini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1\">Cigdem Beyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_A/0/1/0/all/0/1\">Alessandro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arrigoni_F/0/1/0/all/0/1\">Federica Arrigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphFit: Learning Multi-scale Graph-Convolutional Representation for Point Cloud Normal Estimation. (arXiv:2207.11484v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11484","description":"<p>We propose a precise and efficient normal estimation method that can deal\nwith noise and nonuniform density for unstructured 3D point clouds. Unlike\nexisting approaches that directly take patches and ignore the local\nneighborhood relationships, which make them susceptible to challenging regions\nsuch as sharp edges, we propose to learn graph convolutional feature\nrepresentation for normal estimation, which emphasizes more local neighborhood\ngeometry and effectively encodes intrinsic relationships. Additionally, we\ndesign a novel adaptive module based on the attention mechanism to integrate\npoint features with their neighboring features, hence further enhancing the\nrobustness of the proposed normal estimator against point density variations.\nTo make it more distinguishable, we introduce a multi-scale architecture in the\ngraph block to learn richer geometric features. Our method outperforms\ncompetitors with the state-of-the-art accuracy on various benchmark datasets,\nand is quite robust against noise, outliers, as well as the density variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Keqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mingyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huaiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">Dong-Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei-Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1\">Gang Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Pointly-Supervised Instance Segmentation. (arXiv:2207.11493v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11493","description":"<p>The requirement of expensive annotations is a major burden for training a\nwell-performed instance segmentation model. In this paper, we present an\neconomic active learning setting, named active pointly-supervised instance\nsegmentation (APIS), which starts with box-level annotations and iteratively\nsamples a point within the box and asks if it falls on the object. The key of\nAPIS is to find the most desirable points to maximize the segmentation accuracy\nwith limited annotation budgets. We formulate this setting and propose several\nuncertainty-based sampling strategies. The model developed with these\nstrategies yields consistent performance gain on the challenging MS-COCO\ndataset, compared against other learning strategies. The results suggest that\nAPIS, integrating the advantages of active learning and point-based\nsupervision, is an effective learning paradigm for label-efficient instance\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chufeng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent 3D Network Protocol for Multimedia Data Classification using Deep Learning. (arXiv:2207.11504v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11504","description":"<p>In videos, the human's actions are of three-dimensional (3D) signals. These\nvideos investigate the spatiotemporal knowledge of human behavior. The\npromising ability is investigated using 3D convolution neural networks (CNNs).\nThe 3D CNNs have not yet achieved high output for their well-established\ntwo-dimensional (2D) equivalents in still photographs. Board 3D Convolutional\nMemory and Spatiotemporal fusion face training difficulty preventing 3D CNN\nfrom accomplishing remarkable evaluation. In this paper, we implement Hybrid\nDeep Learning Architecture that combines STIP and 3D CNN features to enhance\nthe performance of 3D videos effectively. After implementation, the more\ndetailed and deeper charting for training in each circle of space-time fusion.\nThe training model further enhances the results after handling complicated\nevaluations of models. The video classification model is used in this\nimplemented model. Intelligent 3D Network Protocol for Multimedia Data\nClassification using Deep Learning is introduced to further understand\nspacetime association in human endeavors. In the implementation of the result,\nthe well-known dataset, i.e., UCF101 to, evaluates the performance of the\nproposed hybrid technique. The results beat the proposed hybrid technique that\nsubstantially beats the initial 3D CNNs. The results are compared with\nstate-of-the-art frameworks from literature for action recognition on UCF101\nwith an accuracy of 95%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Syed_A/0/1/0/all/0/1\">Arslan Syed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aldhahri_E/0/1/0/all/0/1\">Eman A. Aldhahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_M/0/1/0/all/0/1\">Muhammad Munawar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Abid Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthanna_A/0/1/0/all/0/1\">Ammar Muthanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamil_H/0/1/0/all/0/1\">Harun Jamil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamil_F/0/1/0/all/0/1\">Faisal Jamil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSBNet: Improving Visual Recognition Efficiency by Adaptive Sampling. (arXiv:2207.11511v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11511","description":"<p>Downsampling is widely adopted to achieve a good trade-off between accuracy\nand latency for visual recognition. Unfortunately, the commonly used pooling\nlayers are not learned, and thus cannot preserve important information. As\nanother dimension reduction method, adaptive sampling weights and processes\nregions that are relevant to the task, and is thus able to better preserve\nuseful information. However, the use of adaptive sampling has been limited to\ncertain layers. In this paper, we show that using adaptive sampling in the\nbuilding blocks of a deep neural network can improve its efficiency. In\nparticular, we propose SSBNet which is built by inserting sampling layers\nrepeatedly into existing networks like ResNet. Experiment results show that the\nproposed SSBNet can achieve competitive image classification and object\ndetection performance on ImageNet and COCO datasets. For example, the\nSSB-ResNet-RS-200 achieved 82.6% accuracy on ImageNet dataset, which is 0.6%\nhigher than the baseline ResNet-RS-152 with a similar complexity. Visualization\nshows the advantage of SSBNet in allowing different layers to focus on\ndifferent positions, and ablation studies further validate the advantage of\nadaptive sampling over uniform methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwan_H/0/1/0/all/0/1\">Ho Man Kwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shenghui Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Hybrid Architecture and Pseudo-label for Semi-supervised Abdominal Organ Segmentation. (arXiv:2207.11512v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11512","description":"<p>Abdominal organ segmentation has many important clinical applications, such\nas organ quantification, surgical planning, and disease diagnosis. However,\nmanually annotating organs from CT scans is time-consuming and labor-intensive.\nSemi-supervised learning has shown the potential to alleviate this challenge by\nlearning from a large set of unlabeled images and limited labeled samples. In\nthis work, we follow the self-training strategy and employ a hybrid\narchitecture (PHTrans) with CNN and Transformer for both teacher and student\nmodels to generate precise pseudo-labels. Afterward, we introduce them with\nlabel data together into a two-stage segmentation framework with lightweight\nPHTrans for training to improve the performance and generalization ability of\nthe model while remaining efficient. Experiments on the validation set of\nFLARE2022 demonstrate that our method achieves excellent segmentation\nperformance as well as fast and low-resource model inference. The average DSC\nand HSD are 0.8956 and 0.9316, respectively. Under our development\nenvironments, the average inference time is 18.62 s, the average maximum GPU\nmemory is 1995.04 MB, and the area under the GPU memory-time curve and the\naverage area under the CPU utilization-time curve are 23196.84 and 319.67.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Songlin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lemeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huihua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-Language Models. (arXiv:2207.11514v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11514","description":"<p>We study open-world 3D scene understanding, a family of tasks that require\nagents to reason about their 3D environment with an open-set vocabulary and\nout-of-domain visual inputs - a critical skill for robots to operate in the\nunstructured 3D world. Towards this end, we propose Semantic Abstraction\n(SemAbs), a framework that equips 2D Vision-Language Models (VLMs) with new 3D\nspatial capabilities, while maintaining their zero-shot robustness. We achieve\nthis abstraction using relevancy maps extracted from CLIP, and learn 3D spatial\nand geometric reasoning skills on top of those abstractions in a\nsemantic-agnostic manner. We demonstrate the usefulness of SemAbs on two\nopen-world 3D scene understanding tasks: 1) completing partially observed\nobjects and 2) localizing hidden objects from language descriptions.\nExperiments show that SemAbs can generalize to novel vocabulary,\nmaterials/lighting, classes, and domains (i.e., real-world scans) from training\non limited 3D synthetic data. Code and data will be available at\nhttps://semantic-abstraction.cs.columbia.edu/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ha_H/0/1/0/all/0/1\">Huy Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Marior: Margin Removal and Iterative Content Rectification for Document Dewarping in the Wild. (arXiv:2207.11515v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11515","description":"<p>Camera-captured document images usually suffer from perspective and geometric\ndeformations. It is of great value to rectify them when considering poor visual\naesthetics and the deteriorated performance of OCR systems. Recent\nlearning-based methods intensively focus on the accurately cropped document\nimage. However, this might not be sufficient for overcoming practical\nchallenges, including document images either with large marginal regions or\nwithout margins. Due to this impracticality, users struggle to crop documents\nprecisely when they encounter large marginal regions. Simultaneously, dewarping\nimages without margins is still an insurmountable problem. To the best of our\nknowledge, there is still no complete and effective pipeline for rectifying\ndocument images in the wild. To address this issue, we propose a novel approach\ncalled Marior (Margin Removal and \\Iterative Content Rectification). Marior\nfollows a progressive strategy to iteratively improve the dewarping quality and\nreadability in a coarse-to-fine manner. Specifically, we divide the pipeline\ninto two modules: margin removal module (MRM) and iterative content\nrectification module (ICRM). First, we predict the segmentation mask of the\ninput image to remove the margin, thereby obtaining a preliminary result. Then\nwe refine the image further by producing dense displacement flows to achieve\ncontent-aware rectification. We determine the number of refinement iterations\nadaptively. Experiments demonstrate the state-of-the-art performance of our\nmethod on public benchmarks. The resources are available at\nhttps://github.com/ZZZHANG-jx/Marior for further comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Canjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fengjun Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Kai Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Monotonic Pixel-Level Modulation. (arXiv:2207.11517v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11517","description":"<p>Continuous one-to-many mapping is a less investigated yet important task in\nboth low-level visions and neural image translation. In this paper, we present\na new formulation called MonoPix, an unsupervised and contrastive continuous\nmodulation model, and take a step further to enable a pixel-level spatial\ncontrol which is critical but can not be properly handled previously. The key\nfeature of this work is to model the monotonicity between controlling signals\nand the domain discriminator with a novel contrastive modulation framework and\ncorresponding monotonicity constraints. We have also introduced a selective\ninference strategy with logarithmic approximation complexity and support fast\ndomain adaptations. The state-of-the-art performance is validated on a variety\nof continuous mapping tasks, including AFHQ cat-dog and Yosemite summer-winter\ntranslation. The introduced approach also helps to provide a new solution for\nmany low-level tasks like low-light enhancement and natural noise generation,\nwhich is beyond the long-established practice of one-to-one training and\ninference. Code is available at https://github.com/lukun199/MonoPix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition. (arXiv:2207.11518v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11518","description":"<p>The teacher-free online Knowledge Distillation (KD) aims to train an ensemble\nof multiple student models collaboratively and distill knowledge from each\nother. Although existing online KD methods achieve desirable performance, they\noften focus on class probabilities as the core knowledge type, ignoring the\nvaluable feature representational information. We present a Mutual Contrastive\nLearning (MCL) framework for online KD. The core idea of MCL is to perform\nmutual interaction and transfer of contrastive distributions among a cohort of\nnetworks in an online manner. Our MCL can aggregate cross-network embedding\ninformation and maximize the lower bound to the mutual information between two\nnetworks. This enables each network to learn extra contrastive knowledge from\nothers, leading to better feature representations, thus improving the\nperformance of visual recognition tasks. Beyond the final layer, we extend MCL\nto several intermediate layers assisted by auxiliary feature refinement\nmodules. This further enhances the ability of representation learning for\nonline KD. Experiments on image classification and transfer learning to visual\nrecognition tasks show that MCL can lead to consistent performance gains\nagainst state-of-the-art online KD approaches. The superiority demonstrates\nthat MCL can guide the network to generate better feature representations. Our\ncode is publicly available at https://github.com/winycg/MCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Helong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Q/0/1/0/all/0/1\">Qian Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unstructured Road Segmentation using Hypercolumn based Random Forests of Local experts. (arXiv:2207.11523v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11523","description":"<p>Monocular vision based road detection methods are mostly based on machine\nlearning methods, relying on classification and feature extraction accuracy,\nand suffer from appearance, illumination and weather changes. Traditional\nmethods introduce the predictions into conditional random fields or markov\nrandom fields models to improve the intermediate predictions based on\nstructure. These methods are optimization based and therefore resource heavy\nand slow, making it unsuitable for real time applications. We propose a method\nto detect and segment roads with a random forest classifier of local experts\nwith superpixel based machine-learned features. The random forest takes in\nmachine learnt descriptors from a pre-trained convolutional neural network -\nVGG-16. The features are also pooled into their respective superpixels,\nallowing for local structure to be continuous. We compare our algorithm against\nNueral Network based methods and Traditional approaches (based on Hand-crafted\nfeatures), on both Structured Road (CamVid and Kitti) and Unstructured Road\nDatasets. Finally, we introduce a Road Scene Dataset with 1000 annotated\nimages, and verify that our algorithm works well in non-urban and rural road\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_P/0/1/0/all/0/1\">Prassanna Ganesh Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1\">Antonio M. Lopez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_G/0/1/0/all/0/1\">Gemma M. Sanchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-driven Neural Gesture Reenactment with Video Motion Graphs. (arXiv:2207.11524v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11524","description":"<p>Human speech is often accompanied by body gestures including arm and hand\ngestures. We present a method that reenacts a high-quality video with gestures\nmatching a target speech audio. The key idea of our method is to split and\nre-assemble clips from a reference video through a novel video motion graph\nencoding valid transitions between clips. To seamlessly connect different clips\nin the reenactment, we propose a pose-aware video blending network which\nsynthesizes video frames around the stitched frames between two clips.\nMoreover, we developed an audio-based gesture searching algorithm to find the\noptimal order of the reenacted frames. Our system generates reenactments that\nare consistent with both the audio rhythms and the speech content. We evaluate\nour synthesized video quality quantitatively, qualitatively, and with user\nstudies, demonstrating that our method produces videos of much higher quality\nand consistency with the target audio compared to previous work and baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jimei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingzeyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_J/0/1/0/all/0/1\">Jun Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_D/0/1/0/all/0/1\">Deepali Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalogerakis_E/0/1/0/all/0/1\">Evangelos Kalogerakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Validation of AI and non-AI Methods in MRI Volumetry to Diagnose Parkinsonian Syndromes. (arXiv:2207.11534v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11534","description":"<p>Automated segmentation and volumetry of brain magnetic resonance imaging\n(MRI) scans are essential for the diagnosis of Parkinson's disease (PD) and\nParkinson's plus syndromes (P-plus). To enhance the diagnostic performance, we\nadopt deep learning (DL) models in brain segmentation and compared their\nperformance with the gold-standard non-DL method. We collected brain MRI scans\nof healthy controls (n=105) and patients with PD (n=105), multiple systemic\natrophy (n=132), and progressive supranuclear palsy (n=69) at Samsung Medical\nCenter from January 2017 to December 2020. Using the gold-standard non-DL\nmodel, FreeSurfer (FS), we segmented six brain structures: midbrain, pons,\ncaudate, putamen, pallidum, and third ventricle, and considered them as\nannotating data for DL models, the representative V-Net and UNETR. The Dice\nscores and area under the curve (AUC) for differentiating normal, PD, and\nP-plus cases were calculated. The segmentation times of V-Net and UNETR for the\nsix brain structures per patient were 3.48 +- 0.17 and 48.14 +- 0.97 s,\nrespectively, being at least 300 times faster than FS (15,735 +- 1.07 s). Dice\nscores of both DL models were sufficiently high (&gt;0.85), and their AUCs for\ndisease classification were superior to that of FS. For classification of\nnormal vs. P-plus and PD vs. multiple systemic atrophy (cerebellar type), the\nDL models and FS showed AUCs above 0.8. DL significantly reduces the analysis\ntime without compromising the performance of brain segmentation and\ndifferential diagnosis. Our findings may contribute to the adoption of DL brain\nMRI segmentation in clinical settings and advance brain research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Joomee Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hahm_J/0/1/0/all/0/1\">Juyoung Hahm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Jisoo Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lim_C/0/1/0/all/0/1\">Chae Yeon Lim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_M/0/1/0/all/0/1\">Myung Jin Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Youn_J/0/1/0/all/0/1\">Jinyoung Youn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_J/0/1/0/all/0/1\">Jin Whan Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_J/0/1/0/all/0/1\">Jong Hyeon Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HPS-Det: Dynamic Sample Assignment with Hyper-Parameter Search for Object Detection. (arXiv:2207.11539v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11539","description":"<p>Sample assignment plays a prominent part in modern object detection\napproaches. However, most existing methods rely on manual design to assign\npositive / negative samples, which do not explicitly establish the\nrelationships between sample assignment and object detection performance. In\nthis work, we propose a novel dynamic sample assignment scheme based on\nhyper-parameter search. We first define the number of positive samples assigned\nto each ground truth as the hyper-parameters and employ a surrogate\noptimization algorithm to derive the optimal choices. Then, we design a dynamic\nsample assignment procedure to dynamically select the optimal number of\npositives at each training iteration. Experiments demonstrate that the\nresulting HPS-Det brings improved performance over different object detection\nbaselines. Moreover, We analyze the hyper-parameter reusability when\ntransferring between different datasets and between different backbones for\nobject detection, which exhibits the superiority and versatility of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_W/0/1/0/all/0/1\">Wenjing Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Lu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Yi Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Support Few-Shot Semantic Segmentation. (arXiv:2207.11549v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11549","description":"<p>Existing few-shot segmentation methods have achieved great progress based on\nthe support-query matching framework. But they still heavily suffer from the\nlimited coverage of intra-class variations from the few-shot supports provided.\nMotivated by the simple Gestalt principle that pixels belonging to the same\nobject are more similar than those to different objects of same class, we\npropose a novel self-support matching strategy to alleviate this problem, which\nuses query prototypes to match query features, where the query prototypes are\ncollected from high-confidence query predictions. This strategy can effectively\ncapture the consistent underlying characteristics of the query objects, and\nthus fittingly match query features. We also propose an adaptive self-support\nbackground prototype generation module and self-support loss to further\nfacilitate the self-support matching procedure. Our self-support network\nsubstantially improves the prototype quality, benefits more improvement from\nstronger backbones and more supports, and achieves SOTA on multiple datasets.\nCodes are at \\url{https://github.com/fanq15/SSP}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1\">Wenjie Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi-Keung Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Resolution Swin Transformer for Automatic Medical Image Segmentation. (arXiv:2207.11553v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11553","description":"<p>The Resolution of feature maps is critical for medical image segmentation.\nMost of the existing Transformer-based networks for medical image segmentation\nare U-Net-like architecture that contains an encoder that utilizes a sequence\nof Transformer blocks to convert the input medical image from high-resolution\nrepresentation into low-resolution feature maps and a decoder that gradually\nrecovers the high-resolution representation from low-resolution feature maps.\nUnlike previous studies, in this paper, we utilize the network design style\nfrom the High-Resolution Network (HRNet), replace the convolutional layers with\nTransformer blocks, and continuously exchange information from the different\nresolution feature maps that are generated by Transformer blocks. The newly\nTransformer-based network presented in this paper is denoted as High-Resolution\nSwin Transformer Network (HRSTNet). Extensive experiments illustrate that\nHRSTNet can achieve comparable performance with the state-of-the-art\nTransformer-based U-Net-like architecture on Brain Tumor Segmentation(BraTS)\n2021 and the liver dataset from Medical Segmentation Decathlon. The code of\nHRSTNet will be publicly available at https://github.com/auroua/HRSTNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1\">Chen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shenghan Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_K/0/1/0/all/0/1\">Kaitai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Haihong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jimin Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Open Set 3D Learning: A Benchmark on Object Point Clouds. (arXiv:2207.11554v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11554","description":"<p>In the last years, there has been significant progress in the field of 3D\nlearning on classification, detection and segmentation problems. The vast\nmajority of the existing studies focus on canonical closed-set conditions,\nneglecting the intrinsic open nature of the real-world. This limits the\nabilities of autonomous systems involved in safety-critical applications that\nrequire managing novel and unknown signals. In this context exploiting 3D data\ncan be a valuable asset since it conveys rich information about the geometry of\nsensed objects and scenes. This paper provides the first broad study on Open\nSet 3D learning. We introduce a novel testbed with settings of increasing\ndifficulty in terms of category semantic shift and cover both in-domain\n(synthetic-to-synthetic) and cross-domain (synthetic-to-real) scenarios.\nMoreover, we investigate the related out-of-distribution and Open Set 2D\nliterature to understand if and how their most recent approaches are effective\non 3D data. Our extensive benchmark positions several algorithms in the same\ncoherent picture, revealing their strengths and limitations. The results of our\nanalysis may serve as a reliable foothold for future tailored Open Set 3D\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alliegro_A/0/1/0/all/0/1\">Antonio Alliegro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borlino_F/0/1/0/all/0/1\">Francesco Cappio Borlino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tommasi_T/0/1/0/all/0/1\">Tatiana Tommasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robots Enact Malignant Stereotypes. (arXiv:2207.11569v1 [cs.RO])","link":"http://arxiv.org/abs/2207.11569","description":"<p>Stereotypes, bias, and discrimination have been extensively documented in\nMachine Learning (ML) methods such as Computer Vision (CV) [18, 80], Natural\nLanguage Processing (NLP) [6], or both, in the case of large image and caption\nmodels such as OpenAI CLIP [14]. In this paper, we evaluate how ML bias\nmanifests in robots that physically and autonomously act within the world. We\naudit one of several recently published CLIP-powered robotic manipulation\nmethods, presenting it with objects that have pictures of human faces on the\nsurface which vary across race and gender, alongside task descriptions that\ncontain terms associated with common stereotypes. Our experiments definitively\nshow robots acting out toxic stereotypes with respect to gender, race, and\nscientifically-discredited physiognomy, at scale. Furthermore, the audited\nmethods are less likely to recognize Women and People of Color. Our\ninterdisciplinary sociotechnical analysis synthesizes across fields and\napplications such as Science Technology and Society (STS), Critical Studies,\nHistory, Safety, Robotics, and AI. We find that robots powered by large\ndatasets and Dissolution Models (sometimes called \"foundation models\", e.g.\nCLIP) that contain humans risk physically amplifying malignant stereotypes in\ngeneral; and that merely correcting disparities will be insufficient for the\ncomplexity and scale of the problem. Instead, we recommend that robot learning\nmethods that physically manifest stereotypes or other harmful outcomes be\npaused, reworked, or even wound down when appropriate, until outcomes can be\nproven safe, effective, and just. Finally, we discuss comprehensive policy\nchanges and the potential of new interdisciplinary research on topics like\nIdentity Safety Assessment Frameworks and Design Justice to better understand\nand address these harms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hundt_A/0/1/0/all/0/1\">Andrew Hundt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1\">William Agnew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_V/0/1/0/all/0/1\">Vicky Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kacianka_S/0/1/0/all/0/1\">Severin Kacianka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gombolay_M/0/1/0/all/0/1\">Matthew Gombolay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Echocardiogram Videos Enables Data-Efficient Clinical Diagnosis. (arXiv:2207.11581v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11581","description":"<p>Given the difficulty of obtaining high-quality labels for medical image\nrecognition tasks, there is a need for deep learning techniques that can be\nadequately fine-tuned on small labeled data sets. Recent advances in\nself-supervised learning techniques have shown that such an in-domain\nrepresentation learning approach can provide a strong initialization for\nsupervised fine-tuning, proving much more data-efficient than standard transfer\nlearning from a supervised pretraining task. However, these applications are\nnot adapted to applications to medical diagnostics captured in a video format.\nWith this progress in mind, we developed a self-supervised learning approach\ncatered to echocardiogram videos with the goal of learning strong\nrepresentations for downstream fine-tuning on the task of diagnosing aortic\nstenosis (AS), a common and dangerous disease of the aortic valve. When\nfine-tuned on 1% of the training data, our best self-supervised learning model\nachieves 0.818 AUC (95% CI: 0.794, 0.840), while the standard transfer learning\napproach reaches 0.644 AUC (95% CI: 0.610, 0.677). We also find that our\nself-supervised model attends more closely to the aortic valve when predicting\nsevere AS as demonstrated by saliency map visualizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holste_G/0/1/0/all/0/1\">Gregory Holste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oikonomou_E/0/1/0/all/0/1\">Evangelos K. Oikonomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1\">Bobak Mortazavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khera_R/0/1/0/all/0/1\">Rohan Khera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defining an action of SO(d)-rotations on images generated by projections of d-dimensional objects: Applications to pose inference with Geometric VAEs. (arXiv:2207.11582v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11582","description":"<p>Recent advances in variational autoencoders (VAEs) have enabled learning\nlatent manifolds as compact Lie groups, such as $SO(d)$. Since this approach\nassumes that data lies on a subspace that is homeomorphic to the Lie group\nitself, we here investigate how this assumption holds in the context of images\nthat are generated by projecting a $d$-dimensional volume with unknown pose in\n$SO(d)$. Upon examining different theoretical candidates for the group and\nimage space, we show that the attempt to define a group action on the data\nspace generally fails, as it requires more specific geometric constraints on\nthe volume. Using geometric VAEs, our experiments confirm that this constraint\nis key to proper pose inference, and we discuss the potential of these results\nfor applications and future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Legendre_N/0/1/0/all/0/1\">Nicolas Legendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duc_K/0/1/0/all/0/1\">Khanh Dao Duc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miolane_N/0/1/0/all/0/1\">Nina Miolane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Artisan: A Semantic-Aware and Controllable CLIPstyler. (arXiv:2207.11598v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11598","description":"<p>Recall that most of the current image style transfer methods require the user\nto give an image of a particular style and then extract that styling feature\nand texture to generate the style of an image, but there are still some\nproblems: the user may not have a reference style image, or it may be difficult\nto summarise the desired style in mind with just one image. The recently\nproposed CLIPstyler has solved this problem, which is able to perform style\ntransfer based only on the provided description of the style image. Although\nCLIPstyler can achieve good performance when landscapes or portraits appear\nalone, it can blur the people and lose the original semantics when people and\nlandscapes coexist. Based on these issues, we demonstrate a novel framework\nthat uses a pre-trained CLIP text-image embedding model and guides image style\ntransfer through an FCN semantic segmentation network. Specifically, we solve\nthe portrait over-styling problem for both selfies and real-world landscape\nwith human subjects photos, enhance the contrast between the effect of style\ntransfer in portrait and landscape, and make the degree of image style transfer\nin different semantic parts fully controllable. Our Generative Artisan resolve\nthe failure case of CLIPstyler and yield both qualitative and quantitative\nmethods to prove ours have much better results than CLIPstyler in both selfies\nand real-world landscape with human subjects photos. This improvement makes it\npossible to commercialize our framework for business scenarios such as\nretouching graphics software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenling Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Huacheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiunan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Deblurring using Dual Camera Fusion on Mobile Phones. (arXiv:2207.11617v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11617","description":"<p>Motion blur of fast-moving subjects is a longstanding problem in photography\nand very common on mobile phones due to limited light collection efficiency,\nparticularly in low-light conditions. While we have witnessed great progress in\nimage deblurring in recent years, most methods require significant\ncomputational power and have limitations in processing high-resolution photos\nwith severe local motions. To this end, we develop a novel face deblurring\nsystem based on the dual camera fusion technique for mobile phones. The system\ndetects subject motion to dynamically enable a reference camera, e.g.,\nultrawide angle camera commonly available on recent premium phones, and\ncaptures an auxiliary photo with faster shutter settings. While the main shot\nis low noise but blurry, the reference shot is sharp but noisy. We learn ML\nmodels to align and fuse these two shots and output a clear photo without\nmotion blur. Our algorithm runs efficiently on Google Pixel 6, which takes 463\nms overhead per shot. Our experiments demonstrate the advantage and robustness\nof our system against alternative single-image, multi-frame, face-specific, and\nvideo deblurring algorithms as well as commercial products. To the best of our\nknowledge, our work is the first mobile solution for face motion deblurring\nthat works reliably and robustly over thousands of images in diverse motion and\nlighting conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1\">Wei-Sheng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shih_Y/0/1/0/all/0/1\">YiChang Shih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lun-Cheng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaotong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Sung-Fang Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krainin_M/0/1/0/all/0/1\">Michael Krainin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chia-Kai Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simplistic and Cost-Effective Design for Real-World Development of an Ambient Assisted Living System for Fall Detection and Indoor Localization: Proof of Concept. (arXiv:2207.11623v1 [cs.HC])","link":"http://arxiv.org/abs/2207.11623","description":"<p>Falls, highly common in the constantly increasing global aging population,\ncan have a variety of negative effects on their health, well-being, and quality\nof life, including restricting their capabilities to conduct Activities of\nDaily Living (ADLs), which are crucial for one's sustenance. Timely assistance\nduring falls is highly necessary, which involves tracking the indoor location\nof the elderly during their diverse navigational patterns associated with ADLs\nto detect the precise location of a fall. With the decreasing caregiver\npopulation on a global scale, it is important that the future of intelligent\nliving environments can detect falls during ADLs while being able to track the\nindoor location of the elderly in the real world. To address these challenges,\nthis work proposes a cost-effective and simplistic design paradigm for an\nAmbient Assisted Living system that can capture multimodal components of user\nbehaviors during ADLs that are necessary for performing fall detection and\nindoor localization in a simultaneous manner in the real world. Proof of\nconcept results from real-world experiments are presented to uphold the\neffective working of the system. The findings from two comparison studies with\nprior works in this field are also presented to uphold the novelty of this\nwork. The first comparison study shows how the proposed system outperforms\nprior works in the areas of indoor localization and fall detection in terms of\nthe effectiveness of its software design and hardware design. The second\ncomparison study shows that the cost for the development of this system is the\nleast as compared to prior works in these fields, which involved real-world\ndevelopment of the underlining systems, thereby upholding its cost-effective\nnature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-temporal Analysis for Automated Concrete Workability Estimation. (arXiv:2207.11635v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11635","description":"<p>Concrete workability measure is mostly determined based on subjective\nassessment of a certified assessor with visual inspections. The potential human\nerror in measuring the workability and the resulting unnecessary adjustments\nfor the workability is a major challenge faced by the construction industry,\nleading to significant costs, material waste and delay. In this paper, we try\nto apply computer vision techniques to observe the concrete mixing process and\nestimate the workability. Specifically, we collected the video data and then\nbuilt three different deep neural networks for spatial-temporal regression. The\npilot study demonstrates a practical application with computer vision\ntechniques to estimate the concrete workability during the mixing process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Litao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirivivatnanon_V/0/1/0/all/0/1\">Vute Sirivivatnanon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nezhad_A/0/1/0/all/0/1\">Ali Nezhad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explored An Effective Methodology for Fine-Grained Snake Recognition. (arXiv:2207.11637v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11637","description":"<p>Fine-Grained Visual Classification (FGVC) is a longstanding and fundamental\nproblem in computer vision and pattern recognition, and underpins a diverse set\nof real-world applications. This paper describes our contribution at\nSnakeCLEF2022 with FGVC. Firstly, we design a strong multimodal backbone to\nutilize various meta-information to assist in fine-grained identification.\nSecondly, we provide new loss functions to solve the long tail distribution\nwith dataset. Then, in order to take full advantage of unlabeled datasets, we\nuse self-supervised learning and supervised learning joint training to provide\npre-trained model. Moreover, some effective data process tricks also are\nconsidered in our experiments. Last but not least, fine-tuned in downstream\ntask with hard mining, ensambled kinds of model performance. Extensive\nexperiments demonstrate that our method can effectively improve the performance\nof fine-grained recognition. Our method can achieve a macro f1 score 92.7% and\n89.4% on private and public dataset, respectively, which is the 1st place among\nthe participators on private leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Aderon Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yanming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jinghua Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCT Approximations Based on Chen's Factorization. (arXiv:2207.11638v1 [eess.SP])","link":"http://arxiv.org/abs/2207.11638","description":"<p>In this paper, two 8-point multiplication-free DCT approximations based on\nthe Chen's factorization are proposed and their fast algorithms are also\nderived. Both transformations are assessed in terms of computational cost,\nerror energy, and coding gain. Experiments with a JPEG-like image compression\nscheme are performed and results are compared with competing methods. The\nproposed low-complexity transforms are scaled according to Jridi-Alfalou-Meher\nalgorithm to effect 16- and 32-point approximations. The new sets of\ntransformations are embedded into an HEVC reference software to provide a fully\nHEVC-compliant video coding scheme. We show that approximate transforms can\noutperform traditional transforms and state-of-the-art methods at a very low\ncomplexity cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tablada_C/0/1/0/all/0/1\">C. J. Tablada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silveira_T/0/1/0/all/0/1\">T. L. T. da Silveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cintra_R/0/1/0/all/0/1\">R. J. Cintra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayer_F/0/1/0/all/0/1\">F. M. Bayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Scene Inference under Noise-Blur Dual Corruptions. (arXiv:2207.11643v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11643","description":"<p>Scene inference under low-light is a challenging problem due to severe noise\nin the captured images. One way to reduce noise is to use longer exposure\nduring the capture. However, in the presence of motion (scene or camera\nmotion), longer exposures lead to motion blur, resulting in loss of image\ninformation. This creates a trade-off between these two kinds of image\ndegradations: motion blur (due to long exposure) vs. noise (due to short\nexposure), also referred as a dual image corruption pair in this paper. With\nthe rise of cameras capable of capturing multiple exposures of the same scene\nsimultaneously, it is possible to overcome this trade-off. Our key observation\nis that although the amount and nature of degradation varies for these\ndifferent image captures, the semantic content remains the same across all\nimages. To this end, we propose a method to leverage these multi exposure\ncaptures for robust inference under low-light and motion. Our method builds on\na feature consistency loss to encourage similar results from these individual\ncaptures, and uses the ensemble of their final predictions for robust visual\nrecognition. We demonstrate the effectiveness of our approach on simulated\nimages as well as real captures with multiple exposures, and across the tasks\nof object detection and image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_B/0/1/0/all/0/1\">Bhavya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lalonde_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Lalonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Regularization of Event-based Learning by Reversing and Drifting. (arXiv:2207.11659v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11659","description":"<p>Event camera has an enormous potential in challenging scenes for its\nadvantages of high temporal resolution, high dynamic range, low power\nconsumption, and no motion blur. However, event-based learning is hindered by\ninsufficient generalization ability. In this paper, we first analyze the\ninfluence of different brightness variations on event data. Then we propose two\nnovel augmentation methods: EventReverse and EventDrift. By reversing and\ndrifting events to their corresponding positions in the spatiotemporal or\npolarity domain, the proposed methods generate samples affected by different\nbrightness variations, which improves the robustness of event-based learning\nand results in a better generalization. Extensive experiments on N-CARS,\nN-Caltech101 and CIFAR10-DVS datasets demonstrate that our method is general\nand remarkably effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haibo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Juyu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianjiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAR: Masked Autoencoders for Efficient Action Recognition. (arXiv:2207.11660v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11660","description":"<p>Standard approaches for video recognition usually operate on the full input\nvideos, which is inefficient due to the widely present spatio-temporal\nredundancy in videos. Recent progress in masked video modelling, i.e.,\nVideoMAE, has shown the ability of vanilla Vision Transformers (ViT) to\ncomplement spatio-temporal contexts given only limited visual contents.\nInspired by this, we propose propose Masked Action Recognition (MAR), which\nreduces the redundant computation by discarding a proportion of patches and\noperating only on a part of the videos. MAR contains the following two\nindispensable components: cell running masking and bridging classifier.\nSpecifically, to enable the ViT to perceive the details beyond the visible\npatches easily, cell running masking is presented to preserve the\nspatio-temporal correlations in videos, which ensures the patches at the same\nspatial location can be observed in turn for easy reconstructions.\nAdditionally, we notice that, although the partially observed features can\nreconstruct semantically explicit invisible patches, they fail to achieve\naccurate classification. To address this, a bridging classifier is proposed to\nbridge the semantic gap between the ViT encoded features for reconstruction and\nthe features specialized for classification. Our proposed MAR reduces the\ncomputational cost of ViT by 53% and extensive experiments show that MAR\nconsistently outperforms existing ViT models with a notable margin. Especially,\nwe found a ViT-Large trained by MAR outperforms the ViT-Huge trained by a\nstandard training scheme by convincing margins on both Kinetics-400 and\nSomething-Something v2 datasets, while our computation overhead of ViT-Large is\nonly 14.5% of ViT-Huge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qing_Z/0/1/0/all/0/1\">Zhiwu Qing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuehuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yiliang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changxin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1\">Nong Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Associative Plasticity between Synapses to Enhance Learning of Spiking Neural Networks. (arXiv:2207.11670v1 [cs.NE])","link":"http://arxiv.org/abs/2207.11670","description":"<p>Spiking Neural Networks (SNNs) are the third generation of artificial neural\nnetworks that enable energy-efficient implementation on neuromorphic hardware.\nHowever, the discrete transmission of spikes brings significant challenges to\nthe robust and high-performance learning mechanism. Most existing works focus\nsolely on learning between neurons but ignore the influence between synapses,\nresulting in a loss of robustness and accuracy. To address this problem, we\npropose a robust and effective learning mechanism by modeling the associative\nplasticity between synapses (APBS) observed from the physiological phenomenon\nof associative long-term potentiation (ALTP). With the proposed APBS method,\nsynapses of the same neuron interact through a shared factor when concurrently\nstimulated by other neurons. In addition, we propose a spatiotemporal cropping\nand flipping (STCF) method to improve the generalization ability of our\nnetwork. Extensive experiments demonstrate that our approaches achieve superior\nperformance on static CIFAR-10 datasets and state-of-the-art performance on\nneuromorphic MNIST-DVS, CIFAR10-DVS datasets by a lightweight convolution\nnetwork. To our best knowledge, this is the first time to explore a learning\nmethod between synapses and an extended approach for neuromorphic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haibo Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Juyu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianjiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learnable Privacy-Preserving Anonymization for Pedestrian Images. (arXiv:2207.11677v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11677","description":"<p>This paper studies a novel privacy-preserving anonymization problem for\npedestrian images, which preserves personal identity information (PII) for\nauthorized models and prevents PII from being recognized by third parties.\nConventional anonymization methods unavoidably cause semantic information loss,\nleading to limited data utility. Besides, existing learned anonymization\ntechniques, while retaining various identity-irrelevant utilities, will change\nthe pedestrian identity, and thus are unsuitable for training robust\nre-identification models. To explore the privacy-utility trade-off for\npedestrian images, we propose a joint learning reversible anonymization\nframework, which can reversibly generate full-body anonymous images with little\nperformance drop on person re-identification tasks. The core idea is that we\nadopt desensitized images generated by conventional methods as the initial\nprivacy-preserving supervision and jointly train an anonymization encoder with\na recovery decoder and an identity-invariant model. We further propose a\nprogressive training strategy to improve the performance, which iteratively\nupgrades the initial anonymization supervision. Experiments further demonstrate\nthe effectiveness of our anonymized pedestrian images for privacy protection,\nwhich boosts the re-identification performance while preserving privacy. Code\nis available at \\url{https://github.com/whuzjw/privacy-reid}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Junwu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FD-MAR: Fourier Dual-domain Network for CT Metal Artifact Reduction. (arXiv:2207.11678v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11678","description":"<p>The presence of high-density objects such as metal implants and dental\nfillings can introduce severely streak-like artifacts in computed tomography\n(CT) images, greatly limiting subsequent diagnosis. Although various deep\nneural networks-based methods have been proposed for metal artifact reduction\n(MAR), they usually suffer from poor performance due to limited exploitation of\nglobal context in the sinogram domain, secondary artifacts introduced in the\nimage domain, and the requirement of precise metal masks. To address these\nissues, this paper explores fast Fourier convolution for MAR in both sinogram\nand image domains, and proposes a Fourier dual-domain network for MAR, termed\nFD-MAR. Specifically, we first propose a Fourier sinogram restoration network,\nwhich can leverage sinogram-wide receptive context to fill in the\nmetal-corrupted region from uncorrupted region and, hence, is robust to the\nmetal trace. Second, we propose a Fourier refinement network in the image\ndomain, which can refine the reconstructed images in a local-to-global manner\nby exploring image-wide context information. As a result, the proposed FD-MAR\ncan explore the sinogram- and image-wide receptive fields for MAR. By\noptimizing FD-MAR with a composite loss function, extensive experimental\nresults demonstrate the superiority of the proposed FD-MAR over the\nstate-of-the-art MAR methods in terms of quantitative metrics and visual\ncomparison. Notably, FD-MAR does not require precise metal masks, which is of\ngreat importance in clinical routine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zilong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Q/0/1/0/all/0/1\">Qi Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yaping Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Junping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Meiyun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affective Behaviour Analysis Using Pretrained Model with Facial Priori. (arXiv:2207.11679v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11679","description":"<p>Affective behaviour analysis has aroused researchers' attention due to its\nbroad applications. However, it is labor exhaustive to obtain accurate\nannotations for massive face images. Thus, we propose to utilize the prior\nfacial information via Masked Auto-Encoder (MAE) pretrained on unlabeled face\nimages. Furthermore, we combine MAE pretrained Vision Transformer (ViT) and\nAffectNet pretrained CNN to perform multi-task emotion recognition. We notice\nthat expression and action unit (AU) scores are pure and intact features for\nvalence-arousal (VA) regression. As a result, we utilize AffectNet pretrained\nCNN to extract expression scores concatenating with expression and AU scores\nfrom ViT to obtain the final VA features. Moreover, we also propose a\nco-training framework with two parallel MAE pretrained ViT for expression\nrecognition tasks. In order to make the two views independent, we random mask\nmost patches during the training process. Then, JS divergence is performed to\nmake the predictions of the two views as consistent as possible. The results on\nABAW4 show that our methods are effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haomiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaori Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Graph Neural Networks for Image Style Transfer. (arXiv:2207.11681v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11681","description":"<p>State-of-the-art parametric and non-parametric style transfer approaches are\nprone to either distorted local style patterns due to global statistics\nalignment, or unpleasing artifacts resulting from patch mismatching. In this\npaper, we study a novel semi-parametric neural style transfer framework that\nalleviates the deficiency of both parametric and non-parametric stylization.\nThe core idea of our approach is to establish accurate and fine-grained\ncontent-style correspondences using graph neural networks (GNNs). To this end,\nwe develop an elaborated GNN model with content and style local patches as the\ngraph vertices. The style transfer procedure is then modeled as the\nattention-based heterogeneous message passing between the style and content\nnodes in a learnable manner, leading to adaptive many-to-one style-content\ncorrelations at the local patch level. In addition, an elaborated deformable\ngraph convolutional operation is introduced for cross-scale style-content\nmatching. Experimental results demonstrate that the proposed semi-parametric\nimage stylization approach yields encouraging results on the challenging style\npatterns, preserving both global appearance and exquisite details. Furthermore,\nby controlling the number of edges at the inference stage, the proposed method\nalso triggers novel functionalities like diversified patch-based stylization\nwith a single model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_Y/0/1/0/all/0/1\">Yongcheng Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yining Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiding Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCA: Semi-supervised Segmentation with Patch Confidence Adversarial Training. (arXiv:2207.11683v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11683","description":"<p>Deep learning based semi-supervised learning (SSL) methods have achieved\nstrong performance in medical image segmentation, which can alleviate doctors'\nexpensive annotation by utilizing a large amount of unlabeled data. Unlike most\nexisting semi-supervised learning methods, adversarial training based methods\ndistinguish samples from different sources by learning the data distribution of\nthe segmentation map, leading the segmenter to generate more accurate\npredictions. We argue that the current performance restrictions for such\napproaches are the problems of feature extraction and learning preference. In\nthis paper, we propose a new semi-supervised adversarial method called Patch\nConfidence Adversarial Training (PCA) for medical image segmentation. Rather\nthan single scalar classification results or pixel-level confidence maps, our\nproposed discriminator creates patch confidence maps and classifies them at the\nscale of the patches. The prediction of unlabeled data learns the pixel\nstructure and context information in each patch to get enough gradient\nfeedback, which aids the discriminator in convergent to an optimal state and\nimproves semi-supervised segmentation performance. Furthermore, at the\ndiscriminator's input, we supplement semantic information constraints on\nimages, making it simpler for unlabeled data to fit the expected data\ndistribution. Extensive experiments on the Automated Cardiac Diagnosis\nChallenge (ACDC) 2017 dataset and the Brain Tumor Segmentation (BraTS) 2019\nchallenge dataset show that our method outperforms the state-of-the-art\nsemi-supervised methods, which demonstrates its effectiveness for medical image\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zihang Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenghua Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shuo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Relative-prototype Spectral Filtering for Few-shot Learning. (arXiv:2207.11685v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11685","description":"<p>Few-shot learning performs classification tasks and regression tasks on\nscarce samples. As one of the most representative few-shot learning models,\nPrototypical Network represents each class as sample average, or a prototype,\nand measures the similarity of samples and prototypes by Euclidean distance. In\nthis paper, we propose a framework of spectral filtering (shrinkage) for\nmeasuring the difference between query samples and prototypes, or namely the\nrelative prototypes, in a reproducing kernel Hilbert space (RKHS). In this\nframework, we further propose a method utilizing Tikhonov regularization as the\nfilter function for few-shot classification. We conduct several experiments to\nverify our method utilizing different kernels based on the miniImageNet\ndataset, tiered-ImageNet dataset and CIFAR-FS dataset. The experimental results\nshow that the proposed model can perform the state-of-the-art. In addition, the\nexperimental results show that the proposed shrinkage method can boost the\nperformance. Source code is available at https://github.com/zhangtao2022/DSFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proving Common Mechanisms Shared by Twelve Methods of Boosting Adversarial Transferability. (arXiv:2207.11694v1 [cs.LG])","link":"http://arxiv.org/abs/2207.11694","description":"<p>Although many methods have been proposed to enhance the transferability of\nadversarial perturbations, these methods are designed in a heuristic manner,\nand the essential mechanism for improving adversarial transferability is still\nunclear. This paper summarizes the common mechanism shared by twelve previous\ntransferability-boosting methods in a unified view, i.e., these methods all\nreduce game-theoretic interactions between regional adversarial perturbations.\nTo this end, we focus on the attacking utility of all interactions between\nregional adversarial perturbations, and we first discover and prove the\nnegative correlation between the adversarial transferability and the attacking\nutility of interactions. Based on this discovery, we theoretically prove and\nempirically verify that twelve previous transferability-boosting methods all\nreduce interactions between regional adversarial perturbations. More crucially,\nwe consider the reduction of interactions as the essential reason for the\nenhancement of adversarial transferability. Furthermore, we design the\ninteraction loss to directly penalize interactions between regional adversarial\nperturbations during attacking. Experimental results show that the interaction\nloss significantly improves the transferability of adversarial perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shuyun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangming Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Deep Multi-view Stereo. (arXiv:2207.11699v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11699","description":"<p>Significant progress has been witnessed in learning-based Multi-view Stereo\n(MVS) of supervised and unsupervised settings. To combine their respective\nmerits in accuracy and completeness, meantime reducing the demand for expensive\nlabeled data, this paper explores a novel semi-supervised setting of\nlearning-based MVS problem that only a tiny part of the MVS data is attached\nwith dense depth ground truth. However, due to huge variation of scenarios and\nflexible setting in views, semi-supervised MVS problem (Semi-MVS) may break the\nbasic assumption in classic semi-supervised learning, that unlabeled data and\nlabeled data share the same label space and data distribution. To handle these\nissues, we propose a novel semi-supervised MVS framework, namely SE-MVS. For\nthe simple case that the basic assumption works in MVS data, consistency\nregularization encourages the model predictions to be consistent between\noriginal sample and randomly augmented sample via constraints on KL divergence.\nFor further troublesome case that the basic assumption is conflicted in MVS\ndata, we propose a novel style consistency loss to alleviate the negative\neffect caused by the distribution gap. The visual style of unlabeled sample is\ntransferred to labeled sample to shrink the gap, and the model prediction of\ngenerated sample is further supervised with the label in original labeled\nsample. The experimental results on DTU, BlendedMVS, GTA-SFM, and\nTanks\\&amp;Temples datasets show the superior performance of the proposed method.\nWith the same settings in backbone network, our proposed SE-MVS outperforms its\nfully-supervised and unsupervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Weitao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1\">Wenxiong Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Test-Time Adaptation via Shift-agnostic Weight Regularization and Nearest Source Prototypes. (arXiv:2207.11707v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11707","description":"<p>This paper proposes a novel test-time adaptation strategy that adjusts the\nmodel pre-trained on the source domain using only unlabeled online data from\nthe target domain to alleviate the performance degradation due to the\ndistribution shift between the source and target domains. Adapting the entire\nmodel parameters using the unlabeled online data may be detrimental due to the\nerroneous signals from an unsupervised objective. To mitigate this problem, we\npropose a shift-agnostic weight regularization that encourages largely updating\nthe model parameters sensitive to distribution shift while slightly updating\nthose insensitive to the shift, during test-time adaptation. This\nregularization enables the model to quickly adapt to the target domain without\nperformance degradation by utilizing the benefit of a high learning rate. In\naddition, we present an auxiliary task based on nearest source prototypes to\nalign the source and target features, which helps reduce the distribution shift\nand leads to further performance improvement. We show that our method exhibits\nstate-of-the-art performance on various standard benchmarks and even\noutperforms its supervised counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sungha Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seunghan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seokeon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Sungrack Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoint-less Camera Calibration for Sports Field Registration in Soccer. (arXiv:2207.11709v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11709","description":"<p>Sports field registration in broadcast videos is typically interpreted as the\ntask of homography estimation, which provides a mapping between a planar field\nand the corresponding visible area of the image. In contrast to previous\napproaches, we consider the task as a camera calibration problem. First, we\nintroduce a differentiable objective function that is able to learn the camera\npose and focal length from segment correspondences (e.g., lines, point clouds),\nbased on pixel-level annotations for segments of a known calibration object,\ni.e., the sports field. The calibration module iteratively minimizes the\nsegment reprojection error induced by the estimated camera parameters. Second,\nwe propose a novel approach for 3D sports field registration from broadcast\nsoccer images. The calibration module does not require any training data and\ncompared to the typical solution, which subsequently refines an initial\nestimation, our solution does it in one step. The proposed method is evaluated\nfor sports field registration on two datasets and achieves superior results\ncompared to two state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theiner_J/0/1/0/all/0/1\">Jonas Theiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIPS: Text-Induced Pose Synthesis. (arXiv:2207.11718v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11718","description":"<p>In computer vision, human pose synthesis and transfer deal with probabilistic\nimage generation of a person in a previously unseen pose from an already\navailable observation of that person. Though researchers have recently proposed\nseveral methods to achieve this task, most of these techniques derive the\ntarget pose directly from the desired target image on a specific dataset,\nmaking the underlying process challenging to apply in real-world scenarios as\nthe generation of the target image is the actual aim. In this paper, we first\npresent the shortcomings of current pose transfer algorithms and then propose a\nnovel text-based pose transfer technique to address those issues. We divide the\nproblem into three independent stages: (a) text to pose representation, (b)\npose refinement, and (c) pose rendering. To the best of our knowledge, this is\none of the first attempts to develop a text-based pose transfer framework where\nwe also introduce a new dataset DF-PASS, by adding descriptive pose annotations\nfor the images of the DeepFashion dataset. The proposed method generates\npromising results with significant qualitative and quantitative scores in our\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Prasun Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhankar Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumenstein_M/0/1/0/all/0/1\">Michael Blumenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Feature Learning for Realistic Cloth-Changing Gait Recognition. (arXiv:2207.11720v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11720","description":"<p>Gait recognition is instrumental in crime prevention and social security, for\nit can be conducted at a long distance without the cooperation of subjects.\nHowever, existing datasets and methods cannot deal with the most challenging\nproblem in realistic gait recognition effectively: walking in different clothes\n(CL). In order to tackle this problem, we propose two benchmarks: CASIA-BN-RCC\nand OUMVLP-RCC, to simulate the cloth-changing condition in practice. The two\nbenchmarks can force the algorithm to realize cross-view and cross-cloth with\ntwo sub-datasets. Furthermore, we propose a new framework that can be applied\nwith off-the-shelf backbones to improve its performance in the Realistic\nCloth-Changing problem with Progressive Feature Learning. Specifically, in our\nframework, we design Progressive Mapping and Progressive Uncertainty to extract\nthe cross-view features and then extract cross-cloth features on the basis. In\nthis way, the features from the cross-view sub-dataset can first dominate the\nfeature space and relieve the uneven distribution caused by the adverse effect\nfrom the cross-cloth sub-dataset. The experiments on our benchmarks show that\nour framework can effectively improve the recognition performance in CL\nconditions. Our codes and datasets will be released after accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_S/0/1/0/all/0/1\">Saihui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Chunshui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongzhen Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-guided Multi-Mask Image Harmonization. (arXiv:2207.11722v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11722","description":"<p>Previous harmonization methods focus on adjusting one inharmonious region in\nan image based on an input mask. They may face problems when dealing with\ndifferent perturbations on different semantic regions without available input\nmasks. To deal with the problem that one image has been pasted with several\nforegrounds coming from different images and needs to harmonize them towards\ndifferent domain directions without any mask as input, we propose a new\nsemantic-guided multi-mask image harmonization task. Different from the\nprevious single-mask image harmonization task, each inharmonious image is\nperturbed with different methods according to the semantic segmentation masks.\nTwo challenging benchmarks, HScene and HLIP, are constructed based on $150$ and\n$19$ semantic classes, respectively. Furthermore, previous baselines focus on\nregressing the exact value for each pixel of the harmonized images. The\ngenerated results are in the `black box' and cannot be edited. In this work, we\npropose a novel way to edit the inharmonious images by predicting a series of\noperator masks. The masks indicate the level and the position to apply a\ncertain image editing operation, which could be the brightness, the saturation,\nand the color in a specific dimension. The operator masks provide more\nflexibility for users to edit the image further. Extensive experiments verify\nthat the operator mask-based network can further improve those state-of-the-art\nmethods which directly regress RGB images when the perturbations are\nstructural. Experiments have been conducted on our constructed benchmarks to\nverify that our proposed operator mask-based framework can locate and modify\nthe inharmonious regions in more complex scenes. Our code and models are\navailable at\nhttps://github.com/XuqianRen/Semantic-guided-Multi-mask-Image-Harmonization.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xuqian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Internal and External Constraints for Unrolling Shutter in Videos. (arXiv:2207.11725v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11725","description":"<p>Videos obtained by rolling-shutter (RS) cameras result in spatially-distorted\nframes. These distortions become significant under fast camera/scene motions.\nUndoing effects of RS is sometimes addressed as a spatial problem, where\nobjects need to be rectified/displaced in order to generate their correct\nglobal shutter (GS) frame. However, the cause of the RS effect is inherently\ntemporal, not spatial. In this paper we propose a space-time solution to the RS\nproblem. We observe that despite the severe differences between their xy\nframes, a RS video and its corresponding GS video tend to share the exact same\nxt slices -- up to a known sub-frame temporal shift. Moreover, they share the\nsame distribution of small 2D xt-patches, despite the strong temporal aliasing\nwithin each video. This allows to constrain the GS output video using\nvideo-specific constraints imposed by the RS input video. Our algorithm is\ncomposed of 3 main components: (i) Dense temporal upsampling between\nconsecutive RS frames using an off-the-shelf method, (which was trained on\nregular video sequences), from which we extract GS \"proposals\". (ii) Learning\nto correctly merge an ensemble of such GS \"proposals\" using a dedicated\nMergeNet. (iii) A video-specific zero-shot optimization which imposes the\nsimilarity of xt-patches between the GS output video and the RS input video.\nOur method obtains state-of-the-art results on benchmark datasets, both\nnumerically and visually, despite being trained on a small synthetic RS/GS\ndataset. Moreover, it generalizes well to new complex RS videos with motion\ntypes outside the distribution of the training set (e.g., complex non-rigid\nmotions) -- videos which competing methods trained on much more data cannot\nhandle well. We attribute these generalization capabilities to the combination\nof external and internal constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naor_E/0/1/0/all/0/1\">Eyal Naor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antebi_I/0/1/0/all/0/1\">Itai Antebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagon_S/0/1/0/all/0/1\">Shai Bagon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can we achieve robustness from data alone?. (arXiv:2207.11727v1 [cs.LG])","link":"http://arxiv.org/abs/2207.11727","description":"<p>Adversarial training and its variants have come to be the prevailing methods\nto achieve adversarially robust classification using neural networks. However,\nits increased computational cost together with the significant gap between\nstandard and robust performance hinder progress and beg the question of whether\nwe can do better. In this work, we take a step back and ask: Can models achieve\nrobustness via standard training on a suitably optimized set? To this end, we\ndevise a meta-learning method for robust classification, that optimizes the\ndataset prior to its deployment in a principled way, and aims to effectively\nremove the non-robust parts of the data. We cast our optimization method as a\nmulti-step PGD procedure on kernel regression, with a class of kernels that\ndescribe infinitely wide neural nets (Neural Tangent Kernels - NTKs).\nExperiments on MNIST and CIFAR-10 demonstrate that the datasets we produce\nenjoy very high robustness against PGD attacks, when deployed in both kernel\nregression classifiers and neural networks. However, this robustness is\nsomewhat fallacious, as alternative attacks manage to fool the models, which we\nfind to be the case for previous similar works in the literature as well. We\ndiscuss potential reasons for this and outline further avenues of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsilivis_N/0/1/0/all/0/1\">Nikolaos Tsilivis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jingtong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kempe_J/0/1/0/all/0/1\">Julia Kempe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Super Resolution of MR Images Using CNNs and Vision Transformers. (arXiv:2207.11748v1 [eess.IV])","link":"http://arxiv.org/abs/2207.11748","description":"<p>State of the art magnetic resonance (MR) image super-resolution methods (ISR)\nusing convolutional neural networks (CNNs) leverage limited contextual\ninformation due to the limited spatial coverage of CNNs. Vision transformers\n(ViT) learn better global context that is helpful in generating superior\nquality HR images. We combine local information of CNNs and global information\nfrom ViTs for image super resolution and output super resolved images that have\nsuperior quality than those produced by state of the art methods. We include\nextra constraints through multiple novel loss functions that preserve structure\nand texture information from the low resolution to high resolution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Guided Auxiliary Training Improves 3D Object Detector. (arXiv:2207.11753v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11753","description":"<p>Detecting 3D objects from point clouds is a practical yet challenging task\nthat has attracted increasing attention recently. In this paper, we propose a\nLabel-Guided auxiliary training method for 3D object detection (LG3D), which\nserves as an auxiliary network to enhance the feature learning of existing 3D\nobject detectors. Specifically, we propose two novel modules: a\nLabel-Annotation-Inducer that maps annotations and point clouds in bounding\nboxes to task-specific representations and a Label-Knowledge-Mapper that\nassists the original features to obtain detection-critical representations. The\nproposed auxiliary network is discarded in inference and thus has no extra\ncomputational cost at test time. We conduct extensive experiments on both\nindoor and outdoor datasets to verify the effectiveness of our approach. For\nexample, our proposed LG3D improves VoteNet by 2.5% and 3.1% mAP on the SUN\nRGB-D and ScanNetV2 datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yaomin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinmei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yichen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chaomin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guixu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yaxin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Feifei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generalizable Light Field Networks from Few Images. (arXiv:2207.11757v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11757","description":"<p>We explore a new strategy for few-shot novel view synthesis based on a neural\nlight field representation. Given a target camera pose, an implicit neural\nnetwork maps each ray to its target pixel's color directly. The network is\nconditioned on local ray features generated by coarse volumetric rendering from\nan explicit 3D feature volume. This volume is built from the input images using\na 3D ConvNet. Our method achieves competitive performances on synthetic and\nreal MVS data with respect to state-of-the-art neural radiance field based\ncompetition, while offering a 100 times faster rendering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Multon_F/0/1/0/all/0/1\">Franck Multon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1\">Adnane Boukhayma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges. (arXiv:2207.11759v1 [cs.LG])","link":"http://arxiv.org/abs/2207.11759","description":"<p>Data drift is a thorny challenge when deploying person re-identification\n(ReID) models into real-world devices, where the data distribution is\nsignificantly different from that of the training environment and keeps\nchanging. To tackle this issue, we propose a federated spatial-temporal\nincremental learning approach, named FedSTIL, which leverages both lifelong\nlearning and federated learning to continuously optimize models deployed on\nmany distributed edge clients. Unlike previous efforts, FedSTIL aims to mine\nspatial-temporal correlations among the knowledge learnt from different edge\nclients. Specifically, the edge clients first periodically extract general\nrepresentations of drifted data to optimize their local models. Then, the\nlearnt knowledge from edge clients will be aggregated by centralized parameter\nserver, where the knowledge will be selectively and attentively distilled from\nspatial- and temporal-dimension with carefully designed mechanisms. Finally,\nthe distilled informative spatial-temporal knowledge will be sent back to\ncorrelated edge clients to further improve the recognition accuracy of each\nedge client with a lifelong learning method. Extensive experiments on a mixture\nof five real-world datasets demonstrate that our method outperforms others by\nnearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All\nimplementation codes are publicly available on\nhttps://github.com/MSNLAB/Federated-Lifelong-Person-ReID\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guanyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huaizheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic Facial Radiance Fields for Few-Shot Talking Head Synthesis. (arXiv:2207.11770v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11770","description":"<p>Talking head synthesis is an emerging technology with wide applications in\nfilm dubbing, virtual avatars and online education. Recent NeRF-based methods\ngenerate more natural talking videos, as they better capture the 3D structural\ninformation of faces. However, a specific model needs to be trained for each\nidentity with a large dataset. In this paper, we propose Dynamic Facial\nRadiance Fields (DFRF) for few-shot talking head synthesis, which can rapidly\ngeneralize to an unseen identity with few training data. Different from the\nexisting NeRF-based methods which directly encode the 3D geometry and\nappearance of a specific person into the network, our DFRF conditions face\nradiance field on 2D appearance images to learn the face prior. Thus the facial\nradiance field can be flexibly adjusted to the new identity with few reference\nimages. Additionally, for better modeling of the facial deformations, we\npropose a differentiable face warping module conditioned on audio signals to\ndeform all reference images to the query space. Extensive experiments show that\nwith only tens of seconds of training clip available, our proposed DFRF can\nsynthesize natural and high-quality audio-driven talking head videos for novel\nidentities with only 40k iterations. We highly recommend readers view our\nsupplementary video for intuitive comparisons. Code is available in\nhttps://sstzal.github.io/DFRF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Shuai Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanhua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Denoising Using Convolutional Autoencoder. (arXiv:2207.11771v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11771","description":"<p>With the inexorable digitalisation of the modern world, every subset in the\nfield of technology goes through major advancements constantly. One such subset\nis digital images which are ever so popular. Images can not always be as\nvisually pleasing or clear as you would want them to be and are often distorted\nor obscured with noise. A number of techniques to enhance images have come up\nas the years passed, all with their own respective pros and cons. In this\npaper, we look at one such particular technique which accomplishes this task\nwith the help of a neural network model commonly known as an autoencoder. We\nconstruct different architectures for the model and compare results in order to\ndecide the one best suited for the task. The characteristics and working of the\nmodel are discussed briefly knowing which can help set a path for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_P/0/1/0/all/0/1\">Prashanth Venkataraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Semi-Supervised Contrastive Learning for Contamination-Resistant Anomaly Detection. (arXiv:2207.11789v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11789","description":"<p>Anomaly detection aims at identifying deviant samples from the normal data\ndistribution. Contrastive learning has provided a successful way to sample\nrepresentation that enables effective discrimination on anomalies. However,\nwhen contaminated with unlabeled abnormal samples in training set under\nsemi-supervised settings, current contrastive-based methods generally 1) ignore\nthe comprehensive relation between training data, leading to suboptimal\nperformance, and 2) require fine-tuning, resulting in low efficiency. To\naddress the above two issues, in this paper, we propose a novel hierarchical\nsemi-supervised contrastive learning (HSCL) framework, for\ncontamination-resistant anomaly detection. Specifically, HSCL hierarchically\nregulates three complementary relations: sample-to-sample, sample-to-prototype,\nand normal-to-abnormal relations, enlarging the discrimination between normal\nand abnormal samples with a comprehensive exploration of the contaminated data.\nBesides, HSCL is an end-to-end learning approach that can efficiently learn\ndiscriminative representations without fine-tuning. HSCL achieves\nstate-of-the-art performance in multiple scenarios, such as one-class\nclassification and cross-dataset detection. Extensive ablation studies further\nverify the effectiveness of each considered relation. The code is available at\nhttps://github.com/GaoangW/HSCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahrstedt_K/0/1/0/all/0/1\">Klara Nahrstedt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PatchRD: Detail-Preserving Shape Completion by Learning Patch Retrieval and Deformation. (arXiv:2207.11790v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11790","description":"<p>This paper introduces a data-driven shape completion approach that focuses on\ncompleting geometric details of missing regions of 3D shapes. We observe that\nexisting generative methods lack the training data and representation capacity\nto synthesize plausible, fine-grained details with complex geometry and\ntopology. Our key insight is to copy and deform patches from the partial input\nto complete missing regions. This enables us to preserve the style of local\ngeometric features, even if it drastically differs from the training data. Our\nfully automatic approach proceeds in two stages. First, we learn to retrieve\ncandidate patches from the input shape. Second, we select and deform some of\nthe retrieved candidates to seamlessly blend them into the complete shape. This\nmethod combines the advantages of the two most common completion methods:\nsimilarity-based single-instance completion, and completion by learning a shape\nspace. We leverage repeating patterns by retrieving patches from the partial\ninput, and learn global structural priors by using a neural network to guide\nthe retrieval and deformation steps. Experimental results show our approach\nconsiderably outperforms baselines across multiple datasets and shape\ncategories. Code and data are available at https://github.com/GitBoSun/PatchRD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir G. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Siddhartha Chaudhuri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal 3D Shape Generation and Manipulation. (arXiv:2207.11795v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11795","description":"<p>Creating and editing the shape and color of 3D objects require tremendous\nhuman effort and expertise. Compared to direct manipulation in 3D interfaces,\n2D interactions such as sketches and scribbles are usually much more natural\nand intuitive for the users. In this paper, we propose a generic multi-modal\ngenerative model that couples the 2D modalities and implicit 3D representations\nthrough shared latent spaces. With the proposed model, versatile 3D generation\nand manipulation are enabled by simply propagating the editing from a specific\n2D controlling modality through the latent spaces. For example, editing the 3D\nshape by drawing a sketch, re-colorizing the 3D surface via painting color\nscribbles on the 2D rendering, or generating 3D shapes of a certain category\ngiven one or a few reference images. Unlike prior works, our model does not\nrequire re-training or fine-tuning per editing task and is also conceptually\nsimple, easy to implement, robust to input domain shifts, and flexible to\ndiverse reconstruction on partial 2D inputs. We evaluate our framework on two\nrepresentative 2D modalities of grayscale line sketches and rendered color\nimages, and demonstrate that our method enables various shape manipulation and\ngeneration tasks with these 2D modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zezhou Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-Supervised Temporal Action Detection for Fine-Grained Videos with Hierarchical Atomic Actions. (arXiv:2207.11805v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11805","description":"<p>Action understanding has evolved into the era of fine granularity, as most\nhuman behaviors in real life have only minor differences. To detect these\nfine-grained actions accurately in a label-efficient way, we tackle the problem\nof weakly-supervised fine-grained temporal action detection in videos for the\nfirst time. Without the careful design to capture subtle differences between\nfine-grained actions, previous weakly-supervised models for general action\ndetection cannot perform well in the fine-grained setting. We propose to model\nactions as the combinations of reusable atomic actions which are automatically\ndiscovered from data through self-supervised clustering, in order to capture\nthe commonality and individuality of fine-grained actions. The learnt atomic\nactions, represented by visual concepts, are further mapped to fine and coarse\naction labels leveraging the semantic label hierarchy. Our approach constructs\na visual representation hierarchy of four levels: clip level, atomic action\nlevel, fine action class level and coarse action class level, with supervision\nat each level. Extensive experiments on two large-scale fine-grained video\ndatasets, FineAction and FineGym, show the benefit of our proposed\nweakly-supervised model for fine-grained action detection, and it achieves\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Huijuan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VizWiz-FewShot: Locating Objects in Images Taken by People With Visual Impairments. (arXiv:2207.11810v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11810","description":"<p>We introduce a few-shot localization dataset originating from photographers\nwho authentically were trying to learn about the visual content in the images\nthey took. It includes nearly 10,000 segmentations of 100 categories in over\n4,500 images that were taken by people with visual impairments. Compared to\nexisting few-shot object detection and instance segmentation datasets, our\ndataset is the first to locate holes in objects (e.g., found in 12.3\\% of our\nsegmentations), it shows objects that occupy a much larger range of sizes\nrelative to the images, and text is over five times more common in our objects\n(e.g., found in 22.4\\% of our segmentations). Analysis of three modern few-shot\nlocalization algorithms demonstrates that they generalize poorly to our new\ndataset. The algorithms commonly struggle to locate objects with holes, very\nsmall and very large objects, and objects lacking text. To encourage a larger\ncommunity to work on these unsolved challenges, we publicly share our annotated\nfew-shot dataset at https://vizwiz.org .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1\">Yu-Yun Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bell_A/0/1/0/all/0/1\">Alexander Bell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object State Change Classification in Egocentric Videos using the Divided Space-Time Attention Mechanism. (arXiv:2207.11814v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11814","description":"<p>This report describes our submission called \"TarHeels\" for the Ego4D: Object\nState Change Classification Challenge. We use a transformer-based video\nrecognition model and leverage the Divided Space-Time Attention mechanism for\nclassifying object state change in egocentric videos. Our submission achieves\nthe second-best performance in the challenge. Furthermore, we perform an\nablation study to show that identifying object state change in egocentric\nvideos requires temporal modeling ability. Lastly, we present several positive\nand negative examples to visualize our model's predictions. The code is\npublicly available at: https://github.com/md-mohaiminul/ObjectStateChange\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Mohaiminul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inter-model Interpretability: Self-supervised Models as a Case Study. (arXiv:2207.11837v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11837","description":"<p>Since early machine learning models, metrics such as accuracy and precision\nhave been the de facto way to evaluate and compare trained models. However, a\nsingle metric number doesn't fully capture the similarities and differences\nbetween models, especially in the computer vision domain. A model with high\naccuracy on a certain dataset might provide a lower accuracy on another\ndataset, without any further insights. To address this problem we build on a\nrecent interpretability technique called Dissect to introduce\n\\textit{inter-model interpretability}, which determines how models relate or\ncomplement each other based on the visual concepts they have learned (such as\nobjects and materials). Towards this goal, we project 13 top-performing\nself-supervised models into a Learned Concepts Embedding (LCE) space that\nreveals proximities among models from the perspective of learned concepts. We\nfurther crossed this information with the performance of these models on four\ncomputer vision tasks and 15 datasets. The experiment allowed us to categorize\nthe models into three categories and revealed for the first time the type of\nvisual concepts different tasks requires. This is a step forward for designing\ncross-task learning algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mustapha_A/0/1/0/all/0/1\">Ahmad Mustapha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khreich_W/0/1/0/all/0/1\">Wael Khreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masri_W/0/1/0/all/0/1\">Wassim Masri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAVCHOI: Detecting Suspicious Activities using Dense Video Captioning with Human Object Interactions. (arXiv:2207.11838v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11838","description":"<p>Detecting suspicious activities in surveillance videos has been a\nlongstanding problem, which can further lead to difficulties in detecting\ncrimes. The authors propose a novel approach for detecting and summarizing the\nsuspicious activities going on in the surveillance videos. They also create\nground truth summaries for the UCF-Crime video dataset. Further, the authors\ntest existing state-of-the-art algorithms for Dense Video Captioning for a\nsubset of this dataset and propose a model for this task by leveraging\nHuman-Object Interaction models for the Visual features. They observe that this\nformulation for Dense Captioning achieves large gains over earlier approaches\nby a significant margin. The authors also perform an ablative analysis of the\ndataset and the model and report their findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ansh Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_S/0/1/0/all/0/1\">Shuvam Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rishibha Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngyuyen_D/0/1/0/all/0/1\">Dat Ngyuyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Dive into Deep Cluster. (arXiv:2207.11839v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11839","description":"<p>Deep Learning has demonstrated a significant improvement against traditional\nmachine learning approaches in different domains such as image and speech\nrecognition. Their success on benchmark datasets is transferred to the\nreal-world through pretrained models by practitioners. Pretraining visual\nmodels using supervised learning requires a significant amount of expensive\ndata annotation. To tackle this limitation, DeepCluster - a simple and scalable\nunsupervised pretraining of visual representations - has been proposed.\nHowever, the underlying work of the model is not yet well understood. In this\npaper, we analyze DeepCluster internals and exhaustively evaluate the impact of\nvarious hyperparameters over a wide range of values on three different\ndatasets. Accordingly, we propose an explanation of why the algorithm works in\npractice. We also show that DeepCluster convergence and performance highly\ndepend on the interplay between the quality of the randomly initialized filters\nof the convolutional layer and the selected number of clusters. Furthermore, we\ndemonstrate that continuous clustering is not critical for DeepCluster\nconvergence. Therefore, early stopping of the clustering phase will reduce the\ntraining time and allow the algorithm to scale to large datasets. Finally, we\nderive plausible hyperparameter selection criteria in a semi-supervised\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mustapha_A/0/1/0/all/0/1\">Ahmad Mustapha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khreich_W/0/1/0/all/0/1\">Wael Khreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masr_W/0/1/0/all/0/1\">Wasim Masr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Image Rescaling using Dual Latent Variables in Invertible Neural Network. (arXiv:2207.11844v1 [cs.CV])","link":"http://arxiv.org/abs/2207.11844","description":"<p>Normalizing flow models have been used successfully for generative image\nsuper-resolution (SR) by approximating complex distribution of natural images\nto simple tractable distribution in latent space through Invertible Neural\nNetworks (INN). These models can generate multiple realistic SR images from one\nlow-resolution (LR) input using randomly sampled points in the latent space,\nsimulating the ill-posed nature of image upscaling where multiple\nhigh-resolution (HR) images correspond to the same LR. Lately, the invertible\nprocess in INN has also been used successfully by bidirectional image rescaling\nmodels like IRN and HCFlow for joint optimization of downscaling and inverse\nupscaling, resulting in significant improvements in upscaled image quality.\nWhile they are optimized for image downscaling too, the ill-posed nature of\nimage downscaling, where one HR image could be downsized to multiple LR images\ndepending on different interpolation kernels and resampling methods, is not\nconsidered. A new downscaling latent variable, in addition to the original one\nrepresenting uncertainties in image upscaling, is introduced to model\nvariations in the image downscaling process. This dual latent variable\nenhancement is applicable to different image rescaling models and it is shown\nin extensive experiments that it can improve image upscaling accuracy\nconsistently without sacrificing image quality in downscaled LR images. It is\nalso shown to be effective in enhancing other INN-based models for image\nrestoration applications like image hiding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhihong Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Gait Database for Normal Walk Collected by Smartphone Accelerometer. (arXiv:1905.03109v3 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/1905.03109","description":"<p>Gait recognition is the characterization of unique biometric patterns\nassociated with each individual which can be utilized to identify a person\nwithout direct contact. A public gait database with a relatively large number\nof subjects can provide a great opportunity for future studies to build and\nvalidate gait authentication models. The goal of this study is to introduce a\ncomprehensive gait database of 93 human subjects who walked between two\nendpoints (320 meters) during two different sessions and record their gait data\nusing two smartphones, one attached to the right thigh and another one on the\nleft side of the waist. This data is collected to be utilized by a deep\nlearning-based method that requires enough time points. The metadata including\nage, gender, smoking, daily exercise time, height, and weight of an individual\nis recorded. this data set is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vajdi_A/0/1/0/all/0/1\">Amir Vajdi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaghian_M/0/1/0/all/0/1\">Mohammad Reza Zaghian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehkordi2_N/0/1/0/all/0/1\">Nazli Rafei Dehkordi2</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastegari_E/0/1/0/all/0/1\">Elham Rastegari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maroofi_K/0/1/0/all/0/1\">Kian Maroofi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farahmand_S/0/1/0/all/0/1\">Saman Farahmand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Shaohua Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pomplun_M/0/1/0/all/0/1\">Marc Pomplun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haspel_N/0/1/0/all/0/1\">Nurit Haspel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayat_A/0/1/0/all/0/1\">Akram Bayat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Test-time Augmentation for Content-based Image Retrieval. (arXiv:2002.01642v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.01642","description":"<p>Off-the-shelf convolutional neural network features achieve outstanding\nresults in many image retrieval tasks. However, their invariance to target data\nis pre-defined by the network architecture and training data. Existing image\nretrieval approaches require fine-tuning or modification of pre-trained\nnetworks to adapt to variations unique to the target data. In contrast, our\nmethod enhances the invariance of off-the-shelf features by aggregating\nfeatures extracted from images augmented at test-time, with augmentations\nguided by a policy learned through reinforcement learning. The learned policy\nassigns different magnitudes and weights to the selected transformations, which\nare selected from a list of image transformations. Policies are evaluated using\na metric learning protocol to learn the optimal policy. The model converges\nquickly and the cost of each policy iteration is minimal as we propose an\noff-line caching technique to greatly reduce the computational cost of\nextracting features from augmented images. Experimental results on large\ntrademark retrieval (METU trademark dataset) and landmark retrieval (ROxford5k\nand RParis6k scene datasets) tasks show that the learned ensemble of\ntransformations is highly effective for improving performance, and is\npractical, and transferable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tursun_O/0/1/0/all/0/1\">Osman Tursun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining How Deep Neural Networks Forget by Deep Visualization. (arXiv:2005.01004v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2005.01004","description":"<p>Explaining the behaviors of deep neural networks, usually considered as black\nboxes, is critical especially when they are now being adopted over diverse\naspects of human life. Taking the advantages of interpretable machine learning\n(interpretable ML), this paper proposes a novel tool called Catastrophic\nForgetting Dissector (or CFD) to explain catastrophic forgetting in continual\nlearning settings. We also introduce a new method called Critical Freezing\nbased on the observations of our tool. Experiments on ResNet articulate how\ncatastrophic forgetting happens, particularly showing which components of this\nfamous network are forgetting. Our new continual learning algorithm defeats\nvarious recent techniques by a significant margin, proving the capability of\nthe investigation. Critical freezing not only attacks catastrophic forgetting\nbut also exposes explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1\">Giang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_T/0/1/0/all/0/1\">Tae Joon Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SegGroup: Seg-Level Supervision for 3D Instance and Semantic Segmentation. (arXiv:2012.10217v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.10217","description":"<p>Most existing point cloud instance and semantic segmentation methods rely\nheavily on strong supervision signals, which require point-level labels for\nevery point in the scene. However, such strong supervision suffers from large\nannotation costs, arousing the need to study efficient annotating. In this\npaper, we discover that the locations of instances matter for both instance and\nsemantic 3D scene segmentation. By fully taking advantage of locations, we\ndesign a weakly-supervised point cloud segmentation method that only requires\nclicking on one point per instance to indicate its location for annotation.\nWith over-segmentation for pre-processing, we extend these location annotations\ninto segments as seg-level labels. We further design a segment grouping network\n(SegGroup) to generate point-level pseudo labels under seg-level labels by\nhierarchically grouping the unlabeled segments into the relevant nearby labeled\nsegments, so that existing point-level supervised segmentation models can\ndirectly consume these pseudo labels for training. Experimental results show\nthat our seg-level supervised method (SegGroup) achieves comparable results\nwith the fully annotated point-level supervised methods. Moreover, it\noutperforms the recent weakly-supervised methods given a fixed annotation\nbudget. Code is available at https://github.com/AnTao97/SegGroup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">An Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLASTER: Clustering with Reinforcement Learning for Zero-Shot Action Recognition. (arXiv:2101.07042v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07042","description":"<p>Zero-shot action recognition is the task of recognizingaction classes without\nvisual examples, only with a seman-tic embedding which relates unseen to seen\nclasses. Theproblem can be seen as learning a function which general-izes well\nto instances of unseen classes without losing dis-crimination between classes.\nNeural networks can modelthe complex boundaries between visual classes, which\nex-plains their success as supervised models. However, inzero-shot learning,\nthese highly specialized class bound-aries may not transfer well from seen to\nunseen classes.In this paper we propose a centroid-based representation,which\nclusters visual and semantic representation, consid-ers all training samples at\nonce, and in this way generaliz-ing well to instances from unseen classes. We\noptimize theclustering using Reinforcement Learning which we show iscritical\nfor our approach to work. We call the proposedmethod CLASTER and observe that\nit consistently outper-forms the state-of-the-art in all standard datasets,\ninclud-ing UCF101, HMDB51 and Olympic Sports; both in thestandard zero-shot\nevaluation and the generalized zero-shotlearning. Further, we show that our\nmodel performs com-petitively in the image domain as well, outperforming\nthestate-of-the-art in many settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lidar Point Cloud Guided Monocular 3D Object Detection. (arXiv:2104.09035v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09035","description":"<p>Monocular 3D object detection is a challenging task in the self-driving and\ncomputer vision community. As a common practice, most previous works use\nmanually annotated 3D box labels, where the annotating process is expensive. In\nthis paper, we find that the precisely and carefully annotated labels may be\nunnecessary in monocular 3D detection, which is an interesting and\ncounterintuitive finding. Using rough labels that are randomly disturbed, the\ndetector can achieve very close accuracy compared to the one using the\nground-truth labels. We delve into this underlying mechanism and then\nempirically find that: concerning the label accuracy, the 3D location part in\nthe label is preferred compared to other parts of labels. Motivated by the\nconclusions above and considering the precise LiDAR 3D measurement, we propose\na simple and effective framework, dubbed LiDAR point cloud guided monocular 3D\nobject detection (LPCG). This framework is capable of either reducing the\nannotation costs or considerably boosting the detection accuracy without\nintroducing extra annotation costs. Specifically, It generates pseudo labels\nfrom unlabeled LiDAR point clouds. Thanks to accurate LiDAR 3D measurements in\n3D space, such pseudo labels can replace manually annotated labels in the\ntraining of monocular 3D detectors, since their 3D location information is\nprecise. LPCG can be applied into any monocular 3D detector to fully use\nmassive unlabeled data in a self-driving system. As a result, in KITTI\nbenchmark, we take the first place on both monocular 3D and BEV\n(bird's-eye-view) detection with a significant margin. In Waymo benchmark, our\nmethod using 10% labeled data achieves comparable accuracy to the baseline\ndetector using 100% labeled data. The codes are released at\nhttps://github.com/SPengLiang/LPCG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengxu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Senbo Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Dan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextAdaIN: Paying Attention to Shortcut Learning in Text Recognizers. (arXiv:2105.03906v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03906","description":"<p>Leveraging the characteristics of convolutional layers, neural networks are\nextremely effective for pattern recognition tasks. However in some cases, their\ndecisions are based on unintended information leading to high performance on\nstandard benchmarks but also to a lack of generalization to challenging testing\nconditions and unintuitive failures. Recent work has termed this \"shortcut\nlearning\" and addressed its presence in multiple domains. In text recognition,\nwe reveal another such shortcut, whereby recognizers overly depend on local\nimage statistics. Motivated by this, we suggest an approach to regulate the\nreliance on local statistics that improves text recognition performance.\n</p>\n<p>Our method, termed TextAdaIN, creates local distortions in the feature map\nwhich prevent the network from overfitting to local statistics. It does so by\nviewing each feature map as a sequence of elements and deliberately mismatching\nfine-grained feature statistics between elements in a mini-batch. Despite\nTextAdaIN's simplicity, extensive experiments show its effectiveness compared\nto other, more complicated methods. TextAdaIN achieves state-of-the-art results\non standard handwritten text recognition benchmarks. It generalizes to multiple\narchitectures and to the domain of scene text recognition. Furthermore, we\ndemonstrate that integrating TextAdaIN improves robustness towards more\nchallenging testing conditions. The official Pytorch implementation can be\nfound at https://github.com/amazon-research/textadain-robust-recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nuriel_O/0/1/0/all/0/1\">Oren Nuriel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogel_S/0/1/0/all/0/1\">Sharon Fogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_R/0/1/0/all/0/1\">Ron Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KVT: k-NN Attention for Boosting Vision Transformers. (arXiv:2106.00515v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00515","description":"<p>Convolutional Neural Networks (CNNs) have dominated computer vision for\nyears, due to its ability in capturing locality and translation invariance.\nRecently, many vision transformer architectures have been proposed and they\nshow promising performance. A key component in vision transformers is the\nfully-connected self-attention which is more powerful than CNNs in modelling\nlong range dependencies. However, since the current dense self-attention uses\nall image patches (tokens) to compute attention matrix, it may neglect locality\nof images patches and involve noisy tokens (e.g., clutter background and\nocclusion), leading to a slow training process and potential degradation of\nperformance. To address these problems, we propose the $k$-NN attention for\nboosting vision transformers. Specifically, instead of involving all the tokens\nfor attention matrix calculation, we only select the top-$k$ similar tokens\nfrom the keys for each query to compute the attention map. The proposed $k$-NN\nattention naturally inherits the local bias of CNNs without introducing\nconvolutional operations, as nearby tokens tend to be more similar than others.\nIn addition, the $k$-NN attention allows for the exploration of long range\ncorrelation and at the same time filters out irrelevant tokens by choosing the\nmost similar tokens from the entire image. Despite its simplicity, we verify,\nboth theoretically and empirically, that $k$-NN attention is powerful in\nspeeding up training and distilling noise from input tokens. Extensive\nexperiments are conducted by using 11 different vision transformer\narchitectures to verify that the proposed $k$-NN attention can work with any\nexisting transformer architectures to improve its prediction performance. The\ncodes are available at \\url{https://github.com/damo-cv/KVT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuning Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"2D vs. 3D LiDAR-based Person Detection on Mobile Robots. (arXiv:2106.11239v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2106.11239","description":"<p>Person detection is a crucial task for mobile robots navigating in\nhuman-populated environments. LiDAR sensors are promising for this task, thanks\nto their accurate depth measurements and large field of view. Two types of\nLiDAR sensors exist: the 2D LiDAR sensors, which scan a single plane, and the\n3D LiDAR sensors, which scan multiple planes, thus forming a volume. How do\nthey compare for the task of person detection? To answer this, we conduct a\nseries of experiments, using the public, large-scale JackRabbot dataset and the\nstate-of-the-art 2D and 3D LiDAR-based person detectors (DR-SPAAM and\nCenterPoint respectively). Our experiments include multiple aspects, ranging\nfrom the basic performance and speed comparison, to more detailed analysis on\nlocalization accuracy and robustness against distance and scene clutter. The\ninsights from these experiments highlight the strengths and weaknesses of 2D\nand 3D LiDAR sensors as sources for person detection, and are especially\nvaluable for designing mobile robots that will operate in close proximity to\nsurrounding humans (e.g. service or social robot).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Dan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermans_A/0/1/0/all/0/1\">Alexander Hermans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Self-supervised Augmented Knowledge Distillation. (arXiv:2107.13715v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13715","description":"<p>Knowledge distillation often involves how to define and transfer knowledge\nfrom teacher to student effectively. Although recent self-supervised\ncontrastive knowledge achieves the best performance, forcing the network to\nlearn such knowledge may damage the representation learning of the original\nclass recognition task. We therefore adopt an alternative self-supervised\naugmented task to guide the network to learn the joint distribution of the\noriginal recognition task and self-supervised auxiliary task. It is\ndemonstrated as a richer knowledge to improve the representation power without\nlosing the normal classification capability. Moreover, it is incomplete that\nprevious methods only transfer the probabilistic knowledge between the final\nlayers. We propose to append several auxiliary classifiers to hierarchical\nintermediate feature maps to generate diverse self-supervised knowledge and\nperform the one-to-one transfer to teach the student network thoroughly. Our\nmethod significantly surpasses the previous SOTA SSKD with an average\nimprovement of 2.56\\% on CIFAR-100 and an improvement of 0.77\\% on ImageNet\nacross widely used network pairs. Codes are available at\nhttps://github.com/winycg/HSAKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation. (arXiv:2107.13824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13824","description":"<p>In recent years, sparse voxel-based methods have become the state-of-the-arts\nfor 3D semantic segmentation of indoor scenes, thanks to the powerful 3D CNNs.\nNevertheless, being oblivious to the underlying geometry, voxel-based methods\nsuffer from ambiguous features on spatially close objects and struggle with\nhandling complex and irregular geometries due to the lack of geodesic\ninformation. In view of this, we present Voxel-Mesh Network (VMNet), a novel 3D\ndeep architecture that operates on the voxel and mesh representations\nleveraging both the Euclidean and geodesic information. Intuitively, the\nEuclidean information extracted from voxels can offer contextual cues\nrepresenting interactions between nearby objects, while the geodesic\ninformation extracted from meshes can help separate objects that are spatially\nclose but have disconnected surfaces. To incorporate such information from the\ntwo domains, we design an intra-domain attentive module for effective feature\naggregation and an inter-domain attentive module for adaptive feature fusion.\nExperimental results validate the effectiveness of VMNet: specifically, on the\nchallenging ScanNet dataset for large-scale segmentation of indoor scenes, it\noutperforms the state-of-the-art SparseConvNet and MinkowskiNet (74.6% vs 72.5%\nand 73.6% in mIoU) with a simpler network structure (17M vs 30M and 38M\nparameters). Code release: https://github.com/hzykent/VMNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zeyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xuyang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jiaxiang Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Runze Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiayu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hongbo Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_C/0/1/0/all/0/1\">Chiew-Lan Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Scene Decoration from a Single Photograph. (arXiv:2108.01806v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01806","description":"<p>Furnishing and rendering indoor scenes has been a long-standing task for\ninterior design, where artists create a conceptual design for the space, build\na 3D model of the space, decorate, and then perform rendering. Although the\ntask is important, it is tedious and requires tremendous effort. In this paper,\nwe introduce a new problem of domain-specific indoor scene image synthesis,\nnamely neural scene decoration. Given a photograph of an empty indoor space and\na list of decorations with layout determined by user, we aim to synthesize a\nnew image of the same space with desired furnishing and decorations. Neural\nscene decoration can be applied to create conceptual interior designs in a\nsimple yet effective manner. Our attempt to this research problem is a novel\nscene generation architecture that transforms an empty scene and an object\nlayout into a realistic furnished scene photograph. We demonstrate the\nperformance of our proposed method by comparing it with conditional image\nsynthesis baselines built upon prevailing image translation approaches both\nqualitatively and quantitatively. We conduct extensive experiments to further\nvalidate the plausibility and aesthetics of our generated scenes. Our\nimplementation is available at\n\\url{https://github.com/hkust-vgd/neural_scene_decoration}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_H/0/1/0/all/0/1\">Hong-Wing Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingshu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_P/0/1/0/all/0/1\">Phuoc-Hieu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1\">Sai-Kit Yeung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors. (arXiv:2109.00182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00182","description":"<p>In this paper, we propose a novel local descriptor-based framework, called\nYou Only Hypothesize Once (YOHO), for the registration of two unaligned point\nclouds. In contrast to most existing local descriptors which rely on a fragile\nlocal reference frame to gain rotation invariance, the proposed descriptor\nachieves the rotation invariance by recent technologies of group equivariant\nfeature learning, which brings more robustness to point density and noise.\nMeanwhile, the descriptor in YOHO also has a rotation equivariant part, which\nenables us to estimate the registration from just one correspondence\nhypothesis. Such property reduces the searching space for feasible\ntransformations, thus greatly improves both the accuracy and the efficiency of\nYOHO. Extensive experiments show that YOHO achieves superior performances with\nmuch fewer needed RANSAC iterations on four widely-used datasets, the\n3DMatch/3DLoMatch datasets, the ETH dataset and the WHU-TLS dataset. More\ndetails are shown in our project page: https://hpwang-whu.github.io/YOHO/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution. (arXiv:2109.03075v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03075","description":"<p>Knowledge distillation (KD) is an effective framework that aims to transfer\nmeaningful information from a large teacher to a smaller student. Generally, KD\noften involves how to define and transfer knowledge. Previous KD methods often\nfocus on mining various forms of knowledge, for example, feature maps and\nrefined information. However, the knowledge is derived from the primary\nsupervised task and thus is highly task-specific. Motivated by the recent\nsuccess of self-supervised representation learning, we propose an auxiliary\nself-supervision augmented task to guide networks to learn more meaningful\nfeatures. Therefore, we can derive soft self-supervision augmented\ndistributions as richer dark knowledge from this task for KD. Unlike previous\nknowledge, this distribution encodes joint knowledge from supervised and\nself-supervised feature learning. Beyond knowledge exploration, we propose to\nappend several auxiliary branches at various hidden layers, to fully take\nadvantage of hierarchical feature maps. Each auxiliary branch is guided to\nlearn self-supervision augmented task and distill this distribution from\nteacher to student. Overall, we call our KD method as Hierarchical\nSelf-Supervision Augmented Knowledge Distillation (HSSAKD). Experiments on\nstandard image classification show that both offline and online HSSAKD achieves\nstate-of-the-art performance in the field of KD. Further transfer experiments\non object detection further verify that HSSAKD can guide the network to learn\nbetter features. The code is available at https://github.com/winycg/HSAKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Linhang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering performance analysis using a new correlation-based cluster validity index. (arXiv:2109.11172v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2109.11172","description":"<p>There are various cluster validity indices used for evaluating clustering\nresults. One of the main objectives of using these indices is to seek the\noptimal unknown number of clusters. Some indices work well for clusters with\ndifferent densities, sizes, and shapes. Yet, one shared weakness of those\nvalidity indices is that they often provide only one optimal number of\nclusters. That number is unknown in real-world problems, and there might be\nmore than one possible option. We develop a new cluster validity index based on\na correlation between an actual distance between a pair of data points and a\ncentroid distance of clusters that the two points occupy. Our proposed index\nconstantly yields several local peaks and overcomes the previously stated\nweakness. Several experiments in different scenarios, including UCI real-world\ndata sets, have been conducted to compare the proposed validity index with\nseveral well-known ones. An R package related to this new index called NCvalid\nis available at https://github.com/nwiroonsri/NCvalid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Wiroonsri_N/0/1/0/all/0/1\">Nathakhun Wiroonsri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-Guided Quadrupedal Locomotion in the Wild with Multi-Modal Delay Randomization. (arXiv:2109.14549v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2109.14549","description":"<p>Developing robust vision-guided controllers for quadrupedal robots in complex\nenvironments, with various obstacles, dynamical surroundings and uneven\nterrains, is very challenging. While Reinforcement Learning (RL) provides a\npromising paradigm for agile locomotion skills with vision inputs in\nsimulation, it is still very challenging to deploy the RL policy in the real\nworld. Our key insight is that aside from the discrepancy in the domain gap, in\nvisual appearance between the simulation and the real world, the latency from\nthe control pipeline is also a major cause of difficulty. In this paper, we\npropose Multi-Modal Delay Randomization (MMDR) to address this issue when\ntraining RL agents. Specifically, we simulate the latency of real hardware by\nusing past observations, sampled with randomized periods, for both\nproprioception and vision. We train the RL policy for end-to-end control in a\nphysical simulator without any predefined controller or reference motion, and\ndirectly deploy it on the real A1 quadruped robot running in the wild. We\nevaluate our method in different outdoor environments with complex terrains and\nobstacles. We demonstrate the robot can smoothly maneuver at a high speed,\navoid the obstacles, and show significant improvement over the baselines. Our\nproject page with videos is at https://mehooz.github.io/mmdr-wild/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imai_C/0/1/0/all/0/1\">Chieko Sarah Imai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kierebinski_M/0/1/0/all/0/1\">Marcin Kierebinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Synthetic Anomalies for Self-Supervised Anomaly Detection and Localization. (arXiv:2109.15222v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15222","description":"<p>We introduce a simple and intuitive self-supervision task, Natural Synthetic\nAnomalies (NSA), for training an end-to-end model for anomaly detection and\nlocalization using only normal training data. NSA integrates Poisson image\nediting to seamlessly blend scaled patches of various sizes from separate\nimages. This creates a wide range of synthetic anomalies which are more similar\nto natural sub-image irregularities than previous data-augmentation strategies\nfor self-supervised anomaly detection. We evaluate the proposed method using\nnatural and medical images. Our experiments with the MVTec AD dataset show that\na model trained to localize NSA anomalies generalizes well to detecting\nreal-world a priori unknown types of manufacturing defects. Our method achieves\nan overall detection AUROC of 97.2 outperforming all previous methods that\nlearn without the use of additional datasets. Code available at\nhttps://github.com/hmsch/natural-synthetic-anomalies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schluter_H/0/1/0/all/0/1\">Hannah M. Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1\">Jeremy Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Construction Site Safety Monitoring and Excavator Activity Analysis System. (arXiv:2110.03083v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03083","description":"<p>With the recent advancements in deep learning and computer vision, the\nAI-powered construction machine such as autonomous excavator has made\nsignificant progress. Safety is the most important section in modern\nconstruction, where construction machines are more and more automated. In this\npaper, we propose a vision-based excavator perception, activity analysis, and\nsafety monitoring system. Our perception system could detect multi-class\nconstruction machines and humans in real-time while estimating the poses and\nactions of the excavator. Then, we present a novel safety monitoring and\nexcavator activity analysis system based on the perception result. To evaluate\nthe performance of our method, we collect a dataset using the Autonomous\nExcavator System (AES) including multi-class of objects in different lighting\nconditions with human annotations. We also evaluate our method on a benchmark\nconstruction dataset. The results showed our YOLO v5 multi-class objects\ndetection model improved inference speed by 8 times (YOLO v5 x-large) to 34\ntimes (YOLO v5 small) compared with Faster R-CNN/ YOLO v3 model. Furthermore,\nthe accuracy of YOLO v5 models is improved by 2.7% (YOLO v5 x-large) while\nmodel size is reduced by 63.9% (YOLO v5 x-large) to 93.9% (YOLO v5 small). The\nexperimental results show that the proposed action recognition approach\noutperforms the state-of-the-art approaches on top-1 accuracy by about 5.18%.\nThe proposed real-time safety monitoring system is not only designed for our\nAutonomous Excavator System (AES) in solid waste scenes, it can also be applied\nto general construction scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sibo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangjun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating generative networks using Gaussian mixtures of image features. (arXiv:2110.05240v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05240","description":"<p>We develop a measure for evaluating the performance of generative networks\ngiven two sets of images. A popular performance measure currently used to do\nthis is the Fr\\'echet Inception Distance (FID). FID assumes that images\nfeaturized using the penultimate layer of Inception-v3 follow a Gaussian\ndistribution, an assumption which cannot be violated if we wish to use FID as a\nmetric. However, we show that Inception-v3 features of the ImageNet dataset are\nnot Gaussian; in particular, every single marginal is not Gaussian. To remedy\nthis problem, we model the featurized images using Gaussian mixture models\n(GMMs) and compute the 2-Wasserstein distance restricted to GMMs. We define a\nperformance measure, which we call WaM, on two sets of images by using\nInception-v3 (or another classifier) to featurize the images, estimate two\nGMMs, and use the restricted $2$-Wasserstein distance to compare the GMMs. We\nexperimentally show the advantages of WaM over FID, including how FID is more\nsensitive than WaM to imperceptible image perturbations. By modelling the\nnon-Gaussian features obtained from Inception-v3 as GMMs and using a GMM\nmetric, we can more accurately evaluate generative network performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luzi_L/0/1/0/all/0/1\">Lorenzo Luzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrero_C/0/1/0/all/0/1\">Carlos Ortiz Marrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wynar_N/0/1/0/all/0/1\">Nile Wynar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henry_M/0/1/0/all/0/1\">Michael J. Henry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack across Datasets. (arXiv:2110.07718v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07718","description":"<p>Existing transfer attack methods commonly assume that the attacker knows the\ntraining set (e.g., the label set, the input size) of the black-box victim\nmodels, which is usually unrealistic because in some cases the attacker cannot\nknow this information. In this paper, we define a Generalized Transferable\nAttack (GTA) problem where the attacker doesn't know this information and is\nacquired to attack any randomly encountered images that may come from unknown\ndatasets. To solve the GTA problem, we propose a novel Image Classification\nEraser (ICE) that trains a particular attacker to erase classification\ninformation of any images from arbitrary datasets. Experiments on several\ndatasets demonstrate that ICE greatly outperforms existing transfer attacks on\nGTA, and show that ICE uses similar texture-like noises to perturb different\nimages from different datasets. Moreover, fast fourier transformation analysis\nindicates that the main components in each ICE noise are three sine waves for\nthe R, G, and B image channels. Inspired by this interesting finding, we then\ndesign a novel Sine Attack (SA) method to optimize the three sine waves.\nExperiments show that SA performs comparably to ICE, indicating that the three\nsine waves are effective and enough to break DNNs under the GTA setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanhao Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jinfeng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Generating Identifiable Virtual Faces. (arXiv:2110.07986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07986","description":"<p>Face anonymization with generative models have become increasingly prevalent\nsince they sanitize private information by generating virtual face images,\nensuring both privacy and image utility. Such virtual face images are usually\nnot identifiable after the removal or protection of the original identity. In\nthis paper, we formalize and tackle the problem of generating identifiable\nvirtual face images. Our virtual face images are visually different from the\noriginal ones for privacy protection. In addition, they are bound with new\nvirtual identities, which can be directly used for face recognition. We propose\nan Identifiable Virtual Face Generator (IVFG) to generate the virtual face\nimages. The IVFG projects the latent vectors of the original face images into\nvirtual ones according to a user specific key, based on which the virtual face\nimages are generated. To make the virtual face images identifiable, we propose\na multi-task learning objective as well as a triplet styled training strategy\nto learn the IVFG. We evaluate the performance of our virtual face images using\ndifferent face recognizers on diffident face image datasets, all of which\ndemonstrate the effectiveness of the IVFG for generate identifiable virtual\nface images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhuowen Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhengxin You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1\">Alex Kot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NYU-VPR: Long-Term Visual Place Recognition Benchmark with View Direction and Data Anonymization Influences. (arXiv:2110.09004v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09004","description":"<p>Visual place recognition (VPR) is critical in not only localization and\nmapping for autonomous driving vehicles, but also in assistive navigation for\nthe visually impaired population. To enable a long-term VPR system on a large\nscale, several challenges need to be addressed. First, different applications\ncould require different image view directions, such as front views for\nself-driving cars while side views for the low vision people. Second, VPR in\nmetropolitan scenes can often cause privacy concerns due to the imaging of\npedestrian and vehicle identity information, calling for the need for data\nanonymization before VPR queries and database construction. Both factors could\nlead to VPR performance variations that are not well understood yet. To study\ntheir influences, we present the NYU-VPR dataset that contains more than\n200,000 images over a 2km by 2km area near the New York University campus,\ntaken within the whole year of 2016. We present benchmark results on several\npopular VPR algorithms showing that side views are significantly more\nchallenging for current VPR methods while the influence of data anonymization\nis almost negligible, together with our hypothetical explanations and in-depth\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_D/0/1/0/all/0/1\">Diwei Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yuxiang Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Claudio Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizzo_J/0/1/0/all/0/1\">John-Ross Rizzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning. (arXiv:2110.13214v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.13214","description":"<p>Current visual question answering (VQA) tasks mainly consider answering\nhuman-annotated questions for natural images. However, aside from natural\nimages, abstract diagrams with semantic richness are still understudied in\nvisual understanding and reasoning research. In this work, we introduce a new\nchallenge of Icon Question Answering (IconQA) with the goal of answering a\nquestion in an icon image context. We release IconQA, a large-scale dataset\nthat consists of 107,439 questions and three sub-tasks: multi-image-choice,\nmulti-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by\nreal-world diagram word problems that highlight the importance of abstract\ndiagram understanding and comprehensive cognitive reasoning. Thus, IconQA\nrequires not only perception skills like object recognition and text\nunderstanding, but also diverse cognitive reasoning skills, such as geometric\nreasoning, commonsense reasoning, and arithmetic reasoning. To facilitate\npotential IconQA models to learn semantic representations for icon images, we\nfurther release an icon dataset Icon645 which contains 645,687 colored icons on\n377 classes. We conduct extensive user studies and blind experiments and\nreproduce a wide range of advanced VQA methods to benchmark the IconQA task.\nAlso, we develop a strong IconQA baseline Patch-TRM that applies a pyramid\ncross-modal Transformer with input diagram embeddings pre-trained on the icon\ndataset. IconQA and Icon645 are available at https://iconqa.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liang Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tony Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yizhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-Item Fashion Recommender: Towards Cross-Domain Recommendations. (arXiv:2111.00758v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2111.00758","description":"<p>Nowadays, recommender systems and search engines play an integral role in\nfashion e-commerce. Still, many challenges lie ahead, and this study tries to\ntackle some. This article first suggests a content-based fashion recommender\nsystem that uses a parallel neural network to take a single fashion item shop\nimage as input and make in-shop recommendations by listing similar items\navailable in the store. Next, the same structure is enhanced to personalize the\nresults based on user preferences. This work then introduces a background\naugmentation technique that makes the system more robust to out-of-domain\nqueries, enabling it to make street-to-shop recommendations using only a\ntraining set of catalog shop images. Moreover, the last contribution of this\npaper is a new evaluation metric for recommendation tasks called\nobjective-guided human score. This method is an entirely customizable framework\nthat produces interpretable, comparable scores from subjective evaluations of\nhuman scorers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammadi_S/0/1/0/all/0/1\">Seyed Omid Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodaghi_H/0/1/0/all/0/1\">Hossein Bodaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhor_A/0/1/0/all/0/1\">Ahmad Kalhor</a> (University of Tehran, College of Engineering, School of Electrical and Computer Engineering, Tehran, Iran)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Semi-supervised Video Object Segmentation Problem from a Cyclic Perspective. (arXiv:2111.01323v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.01323","description":"<p>Modern video object segmentation (VOS) algorithms have achieved remarkably\nhigh performance in a sequential processing order, while most of currently\nprevailing pipelines still show some obvious inadequacy like accumulative\nerror, unknown robustness or lack of proper interpretation tools. In this\npaper, we place the semi-supervised video object segmentation problem into a\ncyclic workflow and find the defects above can be collectively addressed via\nthe inherent cyclic property of semi-supervised VOS systems. Firstly, a cyclic\nmechanism incorporated to the standard sequential flow can produce more\nconsistent representations for pixel-wise correspondance. Relying on the\naccurate reference mask in the starting frame, we show that the error\npropagation problem can be mitigated. Next, a simple gradient correction\nmodule, which naturally extends the offline cyclic pipeline to an online\nmanner, can highlight the high-frequent and detailed part of results to further\nimprove the segmentation quality while keeping feasible computation cost.\nMeanwhile such correction can protect the network from severe performance\ndegration resulted from interference signals. Finally we develop cycle\neffective receptive field (cycle-ERF) based on gradient correction process to\nprovide a new perspective into analyzing object-specific regions of interests.\nWe conduct comprehensive comparison and detailed analysis on challenging\nbenchmarks of DAVIS16, DAVIS17 and Youtube-VOS, demonstrating that the cyclic\nmechanism is helpful to enhance segmentation quality, improve the robustness of\nVOS systems, and further provide qualitative comparison and interpretation on\nhow different VOS algorithms work. The code of this project can be found at\nhttps://github.com/lyxok1/STM-Training\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1\">John See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weiyao Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sliced Recursive Transformer. (arXiv:2111.05297v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.05297","description":"<p>We present a neat yet effective recursive operation on vision transformers\nthat can improve parameter utilization without involving additional parameters.\nThis is achieved by sharing weights across the depth of transformer networks.\nThe proposed method can obtain a substantial gain (~2%) simply using naive\nrecursive operation, requires no special or sophisticated knowledge for\ndesigning principles of networks, and introduces minimal computational overhead\nto the training procedure. To reduce the additional computation caused by\nrecursive operation while maintaining the superior accuracy, we propose an\napproximating method through multiple sliced group self-attentions across\nrecursive layers which can reduce the cost consumption by 10~30% with minimal\nperformance loss. We call our model Sliced Recursive Transformer (SReT), a\nnovel and parameter-efficient vision transformer design that is compatible with\na broad range of other designs for efficient ViT architectures. Our best model\nestablishes significant improvement on ImageNet-1K over state-of-the-art\nmethods while containing fewer parameters. The proposed weight sharing\nmechanism by sliced recursion structure allows us to build a transformer with\nmore than 100 or even 1000 shared layers with ease while keeping a compact size\n(13~15M), to avoid optimization difficulties when the model is too large. The\nflexible scalability has shown great potential for scaling up models and\nconstructing extremely deep vision transformers. Code is available at\nhttps://github.com/szq0214/SReT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Document Generator for Annotation-free Layout Recognition. (arXiv:2111.06016v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06016","description":"<p>Analyzing the layout of a document to identify headers, sections, tables,\nfigures etc. is critical to understanding its content. Deep learning based\napproaches for detecting the layout structure of document images have been\npromising. However, these methods require a large number of annotated examples\nduring training, which are both expensive and time consuming to obtain. We\ndescribe here a synthetic document generator that automatically produces\nrealistic documents with labels for spatial positions, extents and categories\nof the layout elements. The proposed generative process treats every physical\ncomponent of a document as a random variable and models their intrinsic\ndependencies using a Bayesian Network graph. Our hierarchical formulation using\nstochastic templates allow parameter sharing between documents for retaining\nbroad themes and yet the distributional characteristics produces visually\nunique samples, thereby capturing complex and diverse layouts. We empirically\nillustrate that a deep layout detection model trained purely on the synthetic\ndocuments can match the performance of a model that uses real documents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raman_N/0/1/0/all/0/1\">Natraj Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_M/0/1/0/all/0/1\">Manuela Veloso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SWAT: Spatial Structure Within and Among Tokens. (arXiv:2111.13677v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13677","description":"<p>Modeling visual data as tokens (i.e., image patches) using attention\nmechanisms, feed-forward networks or convolutions has been highly effective in\nrecent years. Such methods usually have a common pipeline: a tokenization\nmethod, followed by a set of layers/blocks for information mixing, both within\nand among tokens. When image patches are converted into tokens, they are often\nflattened, discarding the spatial structure within each patch. As a result, any\nprocessing that follows (eg: multi-head self-attention) may fail to recover\nand/or benefit from such information. In this paper, we argue that models can\nhave significant gains when spatial structure is preserved during tokenization,\nand is explicitly used during the mixing stage. We propose two key\ncontributions: (1) Structure-aware Tokenization and, (2) Structure-aware\nMixing, both of which can be combined with existing models with minimal effort.\nWe introduce a family of models (SWAT), showing improvements over the likes of\nDeiT, MLP-Mixer and Swin Transformer, across multiple benchmarks including\nImageNet classification and ADE20K segmentation. Our code and models will be\nreleased online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kahatapitiya_K/0/1/0/all/0/1\">Kumara Kahatapitiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OOD-CV: A Benchmark for Robustness to Out-of-Distribution Shifts of Individual Nuisances in Natural Images. (arXiv:2111.14341v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14341","description":"<p>Enhancing the robustness of vision algorithms in real-world scenarios is\nchallenging. One reason is that existing robustness benchmarks are limited, as\nthey either rely on synthetic data or ignore the effects of individual nuisance\nfactors. We introduce OOD-CV, a benchmark dataset that includes\nout-of-distribution examples of 10 object categories in terms of pose, shape,\ntexture, context and the weather conditions, and enables benchmarking models\nfor image classification, object detection, and 3D pose estimation. In addition\nto this novel dataset, we contribute extensive experiments using popular\nbaseline methods, which reveal that: 1. Some nuisance factors have a much\nstronger negative effect on the performance compared to others, also depending\non the vision task. 2. Current approaches to enhance robustness have only\nmarginal effects, and can even reduce robustness. 3. We do not observe\nsignificant differences between convolutional and transformer architectures. We\nbelieve our dataset provides a rich testbed to study robustness and will help\npush forward research in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shaozuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wufei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingxin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Shenxiao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Angtian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Ju He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Neural Networks. (arXiv:2112.00891v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00891","description":"<p>Video data is often repetitive; for example, the contents of adjacent frames\nare usually strongly correlated. Such redundancy occurs at multiple levels of\ncomplexity, from low-level pixel values to textures and high-level semantics.\nWe propose Event Neural Networks (EvNets), which leverage this redundancy to\nachieve considerable computation savings during video inference. A defining\ncharacteristic of EvNets is that each neuron has state variables that provide\nit with long-term memory, which allows low-cost, high-accuracy inference even\nin the presence of significant camera motion. We show that it is possible to\ntransform a wide range of neural networks into EvNets without re-training. We\ndemonstrate our method on state-of-the-art architectures for both high- and\nlow-level visual processing, including pose recognition, object detection,\noptical flow, and image enhancement. We observe roughly an order-of-magnitude\nreduction in computational costs compared to conventional networks, with\nminimal reductions in model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutson_M/0/1/0/all/0/1\">Matthew Dutson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransFGU: A Top-down Approach to Fine-Grained Unsupervised Semantic Segmentation. (arXiv:2112.01515v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01515","description":"<p>Unsupervised semantic segmentation aims to obtain high-level semantic\nrepresentation on low-level visual features without manual annotations. Most\nexisting methods are bottom-up approaches that try to group pixels into regions\nbased on their visual cues or certain predefined rules. As a result, it is\ndifficult for these bottom-up approaches to generate fine-grained semantic\nsegmentation when coming to complicated scenes with multiple objects and some\nobjects sharing similar visual appearance. In contrast, we propose the first\ntop-down unsupervised semantic segmentation framework for fine-grained\nsegmentation in extremely complicated scenarios. Specifically, we first obtain\nrich high-level structured semantic concept information from large-scale vision\ndata in a self-supervised learning manner, and use such information as a prior\nto discover potential semantic categories presented in target datasets.\nSecondly, the discovered high-level semantic categories are mapped to low-level\npixel features by calculating the class activate map (CAM) with respect to\ncertain discovered semantic representation. Lastly, the obtained CAMs serve as\npseudo labels to train the segmentation module and produce the final semantic\nsegmentation. Experimental results on multiple semantic segmentation benchmarks\nshow that our top-down unsupervised segmentation is robust to both\nobject-centric and scene-centric datasets under different semantic granularity\nlevels, and outperforms all the current state-of-the-art bottom-up methods. Our\ncode is available at \\url{https://github.com/damo-cv/TransFGU}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaoyuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xianzhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanling Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AirDet: Few-Shot Detection without Fine-tuning for Autonomous Exploration. (arXiv:2112.01740v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01740","description":"<p>Few-shot object detection has attracted increasing attention and rapidly\nprogressed in recent years. However, the requirement of an exhaustive offline\nfine-tuning stage in existing methods is time-consuming and significantly\nhinders their usage in online applications such as autonomous exploration of\nlow-power robots. We find that their major limitation is that the little but\nvaluable information from a few support images is not fully exploited. To solve\nthis problem, we propose a brand new architecture, AirDet, and surprisingly\nfind that, by learning class-agnostic relation with the support images in all\nmodules, including cross-scale object proposal network, shots aggregation\nmodule, and localization network, AirDet without fine-tuning achieves\ncomparable or even better results than many fine-tuned methods, reaching up to\n30-40% improvements. We also present solid results of onboard tests on\nreal-world exploration data from the DARPA Subterranean Challenge, which\nstrongly validate the feasibility of AirDet in robotics. To the best of our\nknowledge, AirDet is the first feasible few-shot detection method for\nautonomous exploration of low-power robots. The code and pre-trained models are\nreleased at https://github.com/Jaraxxus-Me/AirDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_P/0/1/0/all/0/1\">Pranay Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungchan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1\">Sebastian Scherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROCA: Robust CAD Model Retrieval and Alignment from a Single Image. (arXiv:2112.01988v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01988","description":"<p>We present ROCA, a novel end-to-end approach that retrieves and aligns 3D CAD\nmodels from a shape database to a single input image. This enables 3D\nperception of an observed scene from a 2D RGB observation, characterized as a\nlightweight, compact, clean CAD representation. Core to our approach is our\ndifferentiable alignment optimization based on dense 2D-3D object\ncorrespondences and Procrustes alignment. ROCA can thus provide a robust CAD\nalignment while simultaneously informing CAD retrieval by leveraging the 2D-3D\ncorrespondences to learn geometrically similar CAD models. Experiments on\nchallenging, real-world imagery from ScanNet show that ROCA significantly\nimproves on state of the art, from 9.5% to 17.6% in retrieval-aware CAD\nalignment accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gumeli_C/0/1/0/all/0/1\">Can G&#xfc;meli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coupling Vision and Proprioception for Navigation of Legged Robots. (arXiv:2112.02094v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.02094","description":"<p>We exploit the complementary strengths of vision and proprioception to\ndevelop a point-goal navigation system for legged robots, called VP-Nav. Legged\nsystems are capable of traversing more complex terrain than wheeled robots, but\nto fully utilize this capability, we need a high-level path planner in the\nnavigation system to be aware of the walking capabilities of the low-level\nlocomotion policy in varying environments. We achieve this by using\nproprioceptive feedback to ensure the safety of the planned path by sensing\nunexpected obstacles like glass walls, terrain properties like slipperiness or\nsoftness of the ground and robot properties like extra payload that are likely\nmissed by vision. The navigation system uses onboard cameras to generate an\noccupancy map and a corresponding cost map to reach the goal. A fast marching\nplanner then generates a target path. A velocity command generator takes this\nas input to generate the desired velocity for the walking policy. A safety\nadvisor module adds sensed unexpected obstacles to the occupancy map and\nenvironment-determined speed limits to the velocity command generator. We show\nsuperior performance compared to wheeled robot baselines, and ablation studies\nwhich have disjoint high-level planning and low-level control. We also show the\nreal-world deployment of VP-Nav on a quadruped robot with onboard sensors and\ncomputation. Videos at https://navigation-locomotion.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zipeng Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Ananye Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Haozhi Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLT: Bidirectional Layout Transformer for Controllable Layout Generation. (arXiv:2112.05112v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05112","description":"<p>Creating visual layouts is a critical step in graphic design. Automatic\ngeneration of such layouts is essential for scalable and diverse visual\ndesigns. To advance conditional layout generation, we introduce BLT, a\nbidirectional layout transformer. BLT differs from previous work on\ntransformers in adopting non-autoregressive transformers. In training, BLT\nlearns to predict the masked attributes by attending to surrounding attributes\nin two directions. During inference, BLT first generates a draft layout from\nthe input and then iteratively refines it into a high-quality layout by masking\nout low-confident attributes. The masks generated in both training and\ninference are controlled by a new hierarchical sampling policy. We verify the\nproposed model on six benchmarks of diverse design tasks. Experimental results\ndemonstrate two benefits compared to the state-of-the-art layout transformer\nmodels. First, our model empowers layout transformers to fulfill controllable\nlayout generation. Second, it achieves up to 10x speedup in generating a layout\nat inference time than the layout transformer baseline. Code is released at\nhttps://shawnkx.github.io/blt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiang Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yuan Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Haifeng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1\">Irfan Essa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering. (arXiv:2112.05504v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05504","description":"<p>Neural radiance fields (NeRF) has achieved outstanding performance in\nmodeling 3D objects and controlled scenes, usually under a single scale. In\nthis work, we focus on multi-scale cases where large changes in imagery are\nobserved at drastically different scales. This scenario vastly exists in\nreal-world 3D environments, such as city scenes, with views ranging from\nsatellite level that captures the overview of a city, to ground level imagery\nshowing complex details of an architecture; and can also be commonly identified\nin landscape and delicate minecraft 3D models. The wide span of viewing\npositions within these scenes yields multi-scale renderings with very different\nlevels of detail, which poses great challenges to neural radiance field and\nbiases it towards compromised results. To address these issues, we introduce\nBungeeNeRF, a progressive neural radiance field that achieves level-of-detail\nrendering across drastically varied scales. Starting from fitting distant views\nwith a shallow base block, as training progresses, new blocks are appended to\naccommodate the emerging details in the increasingly closer views. The strategy\nprogressively activates high-frequency channels in NeRF's positional encoding\ninputs and successively unfolds more complex details as the training proceeds.\nWe demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale\nscenes with drastically varying views on multiple data sources (city models,\nsynthetic, and drone captured data) and its support for high-quality rendering\nin different levels of detail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiangli_Y/0/1/0/all/0/1\">Yuanbo Xiangli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Linning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Nanxuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Anyi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMPOSER: Compositional Reasoning of Group Activity in Videos with Keypoint-Only Modality. (arXiv:2112.05892v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05892","description":"<p>Group Activity Recognition detects the activity collectively performed by a\ngroup of actors, which requires compositional reasoning of actors and objects.\nWe approach the task by modeling the video as tokens that represent the\nmulti-scale semantic concepts in the video. We propose COMPOSER, a Multiscale\nTransformer based architecture that performs attention-based reasoning over\ntokens at each scale and learns group activity compositionally. In addition,\nprior works suffer from scene biases with privacy and ethical concerns. We only\nuse the keypoint modality which reduces scene biases and prevents acquiring\ndetailed visual data that may contain private or biased information of users.\nWe improve the multiscale representations in COMPOSER by clustering the\nintermediate scale representations, while maintaining consistent cluster\nassignments between scales. Finally, we use techniques such as auxiliary\nprediction and data augmentations tailored to the keypoint signals to aid model\ntraining. We demonstrate the model's strength and interpretability on two\nwidely-used datasets (Volleyball and Collective Activity). COMPOSER achieves up\nto +5.4% improvement with just the keypoint modality. Code is available at\nhttps://github.com/hongluzhou/composer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honglu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadav_A/0/1/0/all/0/1\">Asim Kadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsian_A/0/1/0/all/0/1\">Aviv Shamsian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Shijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_F/0/1/0/all/0/1\">Farley Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1\">Mubbasir Kapadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graf_H/0/1/0/all/0/1\">Hans Peter Graf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices. (arXiv:2112.07642v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07642","description":"<p>Understanding social interactions from egocentric views is crucial for many\napplications, ranging from assistive robotics to AR/VR. Key to reasoning about\ninteractions is to understand the body pose and motion of the interaction\npartner from the egocentric view. However, research in this area is severely\nhindered by the lack of datasets. Existing datasets are limited in terms of\neither size, capture/annotation modalities, ground-truth quality, or\ninteraction diversity. We fill this gap by proposing EgoBody, a novel\nlarge-scale dataset for human pose, shape and motion estimation from egocentric\nviews, during interactions in complex 3D scenes. We employ Microsoft HoloLens2\nheadsets to record rich egocentric data streams (including RGB, depth, eye\ngaze, head and hand tracking). To obtain accurate 3D ground truth, we calibrate\nthe headset with a multi-Kinect rig and fit expressive SMPL-X body meshes to\nmulti-view RGB-D frames, reconstructing 3D human shapes and poses relative to\nthe scene, over time. We collect 125 sequences, spanning diverse interaction\nscenarios, and propose the first benchmark for 3D full-body pose and shape\nestimation of the social partner from egocentric views. We extensively evaluate\nstate-of-the-art methods, highlight their limitations in the egocentric\nscenario, and address such limitations leveraging our high-quality annotations.\nData and code are available at\nhttps://sanweiliti.github.io/egobody/egobody.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qianli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhiyin Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Taein Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogo_F/0/1/0/all/0/1\">Federica Bogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13592","description":"<p>As information exists in various modalities in real world, effective\ninteraction and fusion among multimodal information plays a key role for the\ncreation and perception of multimodal data in computer vision and deep learning\nresearch. With superb power in modelling the interaction among multimodal\ninformation, multimodal image synthesis and editing has become a hot research\ntopic in recent years. Instead of providing explicit guidance for network\ntraining, multimodal guidance offers intuitive and flexible means for image\nsynthesis and editing. On the other hand, this field is also facing several\nchallenges in alignment of features with inherent modality gaps, synthesis of\nhigh-resolution images, faithful evaluation metrics, etc. In this survey, we\ncomprehensively contextualize the advance of the recent multimodal image\nsynthesis and editing and formulate taxonomies according to data modality and\nmodel architectures. We start with an introduction to different types of\nguidance modalities in image synthesis and editing. We then describe multimodal\nimage synthesis and editing approaches extensively with detailed frameworks\nincluding Generative Adversarial Networks (GANs), Auto-regressive models,\nDiffusion models, Neural Radiance Fields (NeRF) and other methods. This is\nfollowed by a comprehensive description of benchmark datasets and corresponding\nevaluation metrics as widely adopted in multimodal image synthesis and editing,\nas well as detailed comparisons of various synthesis methods with analysis of\nrespective advantages and limitations. Finally, we provide insights about the\ncurrent research challenges and possible directions for future research. We\nhope this survey could lay a sound and valuable foundation for future\ndevelopment of multimodal image synthesis and editing. A project associated\nwith this survey is available at https://github.com/fnzhan/MISE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invariance encoding in sliced-Wasserstein space for image classification with limited training data. (arXiv:2201.02980v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02980","description":"<p>Deep convolutional neural networks (CNNs) are broadly considered to be\nstate-of-the-art generic end-to-end image classification systems. However, they\nare known to underperform when training data are limited and thus require data\naugmentation strategies that render the method computationally expensive and\nnot always effective. Rather than using a data augmentation strategy to encode\ninvariances as typically done in machine learning, here we propose to\nmathematically augment a nearest subspace classification model in\nsliced-Wasserstein space by exploiting certain mathematical properties of the\nRadon Cumulative Distribution Transform (R-CDT), a recently introduced image\ntransform. We demonstrate that for a particular type of learning problem, our\nmathematical solution has advantages over data augmentation with deep CNNs in\nterms of classification accuracy and computational complexity, and is\nparticularly effective under a limited training data setting. The method is\nsimple, effective, computationally efficient, non-iterative, and requires no\nparameters to be tuned. Python code implementing our method is available at\nhttps://github.com/rohdelab/mathematical_augmentation. Our method is integrated\nas a part of the software package PyTransKit, which is available at\nhttps://github.com/rohdelab/PyTransKit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rabbi_M/0/1/0/all/0/1\">Mohammad Shifat E Rabbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubaiyat_A/0/1/0/all/0/1\">Abu Hasnat Mohammad Rubaiyat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xuwang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1\">Gustavo K. Rohde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Frequency-domain-based Feature Disentanglement and Interaction. (arXiv:2201.08029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.08029","description":"<p>Adaptation to out-of-distribution data is a meta-challenge for all\nstatistical learning algorithms that strongly rely on the i.i.d. assumption. It\nleads to unavoidable labor costs and confidence crises in realistic\napplications. For that, domain generalization aims at mining domain-irrelevant\nknowledge from multiple source domains that can generalize to unseen target\ndomains. In this paper, by leveraging the frequency domain of an image, we\nuniquely work with two key observations: (i) the high-frequency information of\nan image depicts object edge structure, which preserves high-level semantic\ninformation of the object is naturally consistent across different domains, and\n(ii) the low-frequency component retains object smooth structure, while this\ninformation is susceptible to domain shifts. Motivated by the above\nobservations, we introduce (i) an encoder-decoder structure to disentangle\nhigh- and low-frequency feature of an image, (ii) an information interaction\nmechanism to ensure the helpful knowledge from both two parts can cooperate\neffectively, and (iii) a novel data augmentation technique that works on the\nfrequency domain to encourage the robustness of frequency-wise feature\ndisentangling. The proposed method obtains state-of-the-art performance on\nthree widely used domain generalization benchmarks (Digit-DG, Office-Home, and\nPACS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruoyi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kongming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Ignoring Backgrounds and Improving Generalization: a Costless DNN Visual Attention Mechanism. (arXiv:2202.00232v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.00232","description":"<p>This work introduces an attention mechanism for image classifiers and the\ncorresponding deep neural network (DNN) architecture, dubbed ISNet. During\ntraining, the ISNet uses segmentation targets to learn how to find the image's\nregion of interest and concentrate its attention on it. The proposal is based\non a novel concept, background relevance minimization in explanation heatmaps.\nIt can be applied to virtually any classification neural network architecture,\nwithout any extra computational cost at run-time. Capable of ignoring the\nbackground, the resulting single DNN can substitute the common pipeline of a\nsegmenter followed by a classifier, being faster and lighter. We tested the\nISNet with three applications: COVID-19 and tuberculosis detection in chest\nX-rays, and facial attribute estimation. The first two tasks employed mixed\ntraining databases, and fostered shortcut learning. By focusing on lungs and\nignoring sources of bias in the background, the ISNet reduced the problem.\nThus, it improved generalization to external (out-of-distribution) test\ndatasets in the biomedical classification problems, surpassing a standard\nclassifier, a multi-task DNN (performing classification and segmentation), an\nattention-gated neural network, and the standard segmentation-classification\npipeline. Facial attribute estimation demonstrated that ISNet could precisely\nfocus on faces, being also applicable to natural images. ISNet presents an\naccurate, fast, and light methodology to ignore backgrounds and improve\ngeneralization in diverse domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1\">Pedro R.A.S. Bassi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalli_A/0/1/0/all/0/1\">Andrea Cavalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactron: Embodied Adaptive Object Detection. (arXiv:2202.00660v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00660","description":"<p>Over the years various methods have been proposed for the problem of object\ndetection. Recently, we have witnessed great strides in this domain owing to\nthe emergence of powerful deep neural networks. However, there are typically\ntwo main assumptions common among these approaches. First, the model is trained\non a fixed training set and is evaluated on a pre-recorded test set. Second,\nthe model is kept frozen after the training phase, so no further updates are\nperformed after the training is finished. These two assumptions limit the\napplicability of these methods to real-world settings. In this paper, we\npropose Interactron, a method for adaptive object detection in an interactive\nsetting, where the goal is to perform object detection in images observed by an\nembodied agent navigating in different environments. Our idea is to continue\ntraining during inference and adapt the model at test time without any explicit\nsupervision via interacting with the environment. Our adaptive object detection\nmodel provides a 7.2 point improvement in AP (and 12.7 points in AP50) over\nDETR, a recent, high-performance object detector. Moreover, we show that our\nobject detection model adapts to environments with completely different\nappearance characteristics, and performs well in them. The code is available\nat: https://github.com/allenai/interactron .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Audio-Visual Separation of Dynamic Sound Sources. (arXiv:2202.00850v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00850","description":"<p>We explore active audio-visual separation for dynamic sound sources, where an\nembodied agent moves intelligently in a 3D environment to continuously isolate\nthe time-varying audio stream being emitted by an object of interest. The agent\nhears a mixed stream of multiple audio sources (e.g., multiple people\nconversing and a band playing music at a noisy party). Given a limited time\nbudget, it needs to extract the target sound accurately at every step using\negocentric audio-visual observations. We propose a reinforcement learning agent\nequipped with a novel transformer memory that learns motion policies to control\nits camera and microphone to recover the dynamic target audio, using\nself-attention to make high-quality estimates for current timesteps and also\nsimultaneously improve its past estimates. Using highly realistic acoustic\nSoundSpaces simulations in real-world scanned Matterport3D environments, we\nshow that our model is able to learn efficient behavior to carry out continuous\nseparation of a dynamic audio target. Project:\nhttps://vision.cs.utexas.edu/projects/active-av-dynamic-separation/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1\">Sagnik Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Interactive Vision-Language Navigation with Unknown Command Feasibility. (arXiv:2202.02312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02312","description":"<p>Vision-language navigation (VLN), in which an agent follows language\ninstruction in a visual environment, has been studied under the premise that\nthe input command is fully feasible in the environment. Yet in practice, a\nrequest may not be possible due to language ambiguity or environment changes.\nTo study VLN with unknown command feasibility, we introduce a new dataset\nMobile app Tasks with Iterative Feedback (MoTIF), where the goal is to complete\na natural language command in a mobile app. Mobile apps provide a scalable\ndomain to study real downstream uses of VLN methods. Moreover, mobile app\ncommands provide instruction for interactive navigation, as they result in\naction sequences with state changes via clicking, typing, or swiping. MoTIF is\nthe first to include feasibility annotations, containing both binary\nfeasibility labels and fine-grained labels for why tasks are unsatisfiable. We\nfurther collect follow-up questions for ambiguous queries to enable research on\ntask uncertainty resolution. Equipped with our dataset, we propose the new\nproblem of feasibility prediction, in which a natural language instruction and\nmultimodal app environment are used to predict command feasibility. MoTIF\nprovides a more realistic app dataset as it contains many diverse environments,\nhigh-level goals, and longer action sequences than prior work. We evaluate\ninteractive VLN methods using MoTIF, quantify the generalization ability of\ncurrent approaches to new app environments, and measure the effect of task\nfeasibility on navigation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burns_A/0/1/0/all/0/1\">Andrea Burns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arsan_D/0/1/0/all/0/1\">Deniz Arsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Sanjna Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ranjitha Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1\">Bryan A. Plummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Abduction of Sherlock Holmes: A Dataset for Visual Abductive Reasoning. (arXiv:2202.04800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04800","description":"<p>Humans have remarkable capacity to reason abductively and hypothesize about\nwhat lies beyond the literal content of an image. By identifying concrete\nvisual clues scattered throughout a scene, we almost can't help but draw\nprobable inferences beyond the literal scene based on our everyday experience\nand knowledge about the world. For example, if we see a \"20 mph\" sign alongside\na road, we might assume the street sits in a residential area (rather than on a\nhighway), even if no houses are pictured. Can machines perform similar visual\nreasoning?\n</p>\n<p>We present Sherlock, an annotated corpus of 103K images for testing machine\ncapacity for abductive reasoning beyond literal image contents. We adopt a\nfree-viewing paradigm: participants first observe and identify salient clues\nwithin images (e.g., objects, actions) and then provide a plausible inference\nabout the scene, given the clue. In total, we collect 363K (clue, inference)\npairs, which form a first-of-its-kind abductive visual reasoning dataset. Using\nour corpus, we test three complementary axes of abductive reasoning. We\nevaluate the capacity of models to: i) retrieve relevant inferences from a\nlarge candidate corpus; ii) localize evidence for inferences via bounding\nboxes, and iii) compare plausible inferences to match human judgments on a\nnewly-collected diagnostic corpus of 19K Likert-scale judgments. While we find\nthat fine-tuning CLIP-RN50x64 with a multitask objective outperforms strong\nbaselines, significant headroom exists between model performance and human\nagreement. Data, models, and leaderboard available at\n<a href=\"http://visualabduction.com/\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1\">Jack Hessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena D. Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Sung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1\">Rowan Zellers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic Telekinesis: Learning a Robotic Hand Imitator by Watching Humans on Youtube. (arXiv:2202.10448v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.10448","description":"<p>We build a system that enables any human to control a robot hand and arm,\nsimply by demonstrating motions with their own hand. The robot observes the\nhuman operator via a single RGB camera and imitates their actions in real-time.\nHuman hands and robot hands differ in shape, size, and joint structure, and\nperforming this translation from a single uncalibrated camera is a highly\nunderconstrained problem. Moreover, the retargeted trajectories must\neffectively execute tasks on a physical robot, which requires them to be\ntemporally smooth and free of self-collisions. Our key insight is that while\npaired human-robot correspondence data is expensive to collect, the internet\ncontains a massive corpus of rich and diverse human hand videos. We leverage\nthis data to train a system that understands human hands and retargets a human\nvideo stream into a robot hand-arm trajectory that is smooth, swift, safe, and\nsemantically similar to the guiding demonstration. We demonstrate that it\nenables previously untrained people to teleoperate a robot on various dexterous\nmanipulation tasks. Our low-cost, glove-free, marker-free remote teleoperation\nsystem makes robot teaching more accessible and we hope that it can aid robots\nin learning to act autonomously in the real world. Videos at\nhttps://robotic-telekinesis.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_A/0/1/0/all/0/1\">Aravind Sivakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_K/0/1/0/all/0/1\">Kenneth Shaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Learning Contrastive Representations for Learning with Noisy Labels. (arXiv:2203.01785v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01785","description":"<p>Deep neural networks are able to memorize noisy labels easily with a softmax\ncross-entropy (CE) loss. Previous studies attempted to address this issue focus\non incorporating a noise-robust loss function to the CE loss. However, the\nmemorization issue is alleviated but still remains due to the non-robust CE\nloss. To address this issue, we focus on learning robust contrastive\nrepresentations of data on which the classifier is hard to memorize the label\nnoise under the CE loss. We propose a novel contrastive regularization function\nto learn such representations over noisy data where label noise does not\ndominate the representation learning. By theoretically investigating the\nrepresentations induced by the proposed regularization function, we reveal that\nthe learned representations keep information related to true labels and discard\ninformation related to corrupted labels. Moreover, our theoretical results also\nindicate that the learned representations are robust to the label noise. The\neffectiveness of this method is demonstrated with experiments on benchmark\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLeod_A/0/1/0/all/0/1\">A. Ian McLeod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation. (arXiv:2203.04568v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04568","description":"<p>The success of Transformer in computer vision has attracted increasing\nattention in the medical imaging community. Especially for medical image\nsegmentation, many excellent hybrid architectures based on convolutional neural\nnetworks (CNNs) and Transformer have been presented and achieve impressive\nperformance. However, most of these methods, which embed modular Transformer\ninto CNNs, struggle to reach their full potential. In this paper, we propose a\nnovel hybrid architecture for medical image segmentation called PHTrans, which\nparallelly hybridizes Transformer and CNN in main building blocks to produce\nhierarchical representations from global and local features and adaptively\naggregate them, aiming to fully exploit their strengths to obtain better\nsegmentation performance. Specifically, PHTrans follows the U-shaped\nencoder-decoder design and introduces the parallel hybird module in deep\nstages, where convolution blocks and the modified 3D Swin Transformer learn\nlocal features and global dependencies separately, then a sequence-to-volume\noperation unifies the dimensions of the outputs to achieve feature aggregation.\nExtensive experimental results on both Multi-Atlas Labeling Beyond the Cranial\nVault and Automated Cardiac Diagnosis Challeng datasets corroborate its\neffectiveness, consistently outperforming state-of-the-art methods. The code is\navailable at: https://github.com/lseventeen/PHTrans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_T/0/1/0/all/0/1\">Tong Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1\">Weijin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huihua Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_S/0/1/0/all/0/1\">Songlin Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lemeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PD-Flow: A Point Cloud Denoising Framework with Normalizing Flows. (arXiv:2203.05940v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05940","description":"<p>Point cloud denoising aims to restore clean point clouds from raw\nobservations corrupted by noise and outliers while preserving the fine-grained\ndetails. We present a novel deep learning-based denoising model, that\nincorporates normalizing flows and noise disentanglement techniques to achieve\nhigh denoising accuracy. Unlike existing works that extract features of point\nclouds for point-wise correction, we formulate the denoising process from the\nperspective of distribution learning and feature disentanglement. By\nconsidering noisy point clouds as a joint distribution of clean points and\nnoise, the denoised results can be derived from disentangling the noise\ncounterpart from latent point representation, and the mapping between Euclidean\nand latent spaces is modeled by normalizing flows. We evaluate our method on\nsynthesized 3D models and real-world datasets with various noise settings.\nQualitative and quantitative results show that our method outperforms previous\nstate-of-the-art deep learning-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_A/0/1/0/all/0/1\">Aihua Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zihui Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yu-Hui Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_J/0/1/0/all/0/1\">Jun Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong-Jin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Worst Case Matters for Few-Shot Recognition. (arXiv:2203.06574v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06574","description":"<p>Few-shot recognition learns a recognition model with very few (e.g., 1 or 5)\nimages per category, and current few-shot learning methods focus on improving\nthe average accuracy over many episodes. We argue that in real-world\napplications we may often only try one episode instead of many, and hence\nmaximizing the worst-case accuracy is more important than maximizing the\naverage accuracy. We empirically show that a high average accuracy not\nnecessarily means a high worst-case accuracy. Since this objective is not\naccessible, we propose to reduce the standard deviation and increase the\naverage accuracy simultaneously. In turn, we devise two strategies from the\nbias-variance tradeoff perspective to implicitly reach this goal: a simple yet\neffective stability regularization (SR) loss together with model ensemble to\nreduce variance during fine-tuning, and an adaptability calibration mechanism\nto reduce the bias. Extensive experiments on benchmark datasets demonstrate the\neffectiveness of the proposed strategies, which outperforms current\nstate-of-the-art methods with a significant margin in terms of not only\naverage, but also worst-case accuracy. Our code is available at\nhttps://github.com/heekhero/ACSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_M/0/1/0/all/0/1\">Minghao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yun-Hao Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08207","description":"<p>Predicting pedestrian movement is critical for human behavior analysis and\nalso for safe and efficient human-agent interactions. However, despite\nsignificant advancements, it is still challenging for existing approaches to\ncapture the uncertainty and multimodality of human navigation decision making.\nIn this paper, we propose SocialVAE, a novel approach for human trajectory\nprediction. The core of SocialVAE is a timewise variational autoencoder\narchitecture that exploits stochastic recurrent neural networks to perform\nprediction, combined with a social attention mechanism and a backward posterior\napproximation to allow for better extraction of pedestrian navigation\nstrategies. We show that SocialVAE improves current state-of-the-art\nperformance on several pedestrian trajectory prediction benchmarks, including\nthe ETH/UCY benchmark, Stanford Drone Dataset, and SportVU NBA movement\ndataset. Code is available at: https://github.com/xupei0610/SocialVAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayet_J/0/1/0/all/0/1\">Jean-Bernard Hayet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1\">Ioannis Karamouzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Historical Document Image Datasets. (arXiv:2203.08504v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08504","description":"<p>This paper presents a systematic literature review of image datasets for\ndocument image analysis, focusing on historical documents, such as handwritten\nmanuscripts and early prints. Finding appropriate datasets for historical\ndocument analysis is a crucial prerequisite to facilitate research using\ndifferent machine learning algorithms. However, because of the very large\nvariety of the actual data (e.g., scripts, tasks, dates, support systems, and\namount of deterioration), the different formats for data and label\nrepresentation, and the different evaluation processes and benchmarks, finding\nappropriate datasets is a difficult task. This work fills this gap, presenting\na meta-study on existing datasets. After a systematic selection process\n(according to PRISMA guidelines), we select 56 studies that are chosen based on\ndifferent factors, such as the year of publication, number of methods\nimplemented in the article, reliability of the chosen algorithms, dataset size,\nand journal outlet. We summarize each study by assigning it to one of three\npre-defined tasks: document classification, layout structure, or semantic\nanalysis. We present the statistics, document type, language, tasks, input\nvisual aspects, and ground truth information for every dataset. In addition, we\nprovide the benchmark tasks and results from these papers or recent\ncompetitions. We further discuss gaps and challenges in this domain. We\nadvocate for providing conversion tools to common formats (e.g., COCO format\nfor computer vision tasks) and always providing a set of evaluation metrics,\ninstead of just one, to make results comparable across studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidou_K/0/1/0/all/0/1\">Konstantina Nikolaidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seuret_M/0/1/0/all/0/1\">Mathias Seuret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokayed_H/0/1/0/all/0/1\">Hamam Mokayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PressureVision: Estimating Hand Pressure from a Single RGB Image. (arXiv:2203.10385v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10385","description":"<p>People often interact with their surroundings by applying pressure with their\nhands. While hand pressure can be measured by placing pressure sensors between\nthe hand and the environment, doing so can alter contact mechanics, interfere\nwith human tactile perception, require costly sensors, and scale poorly to\nlarge environments. We explore the possibility of using a conventional RGB\ncamera to infer hand pressure, enabling machine perception of hand pressure\nfrom uninstrumented hands and surfaces. The central insight is that the\napplication of pressure by a hand results in informative appearance changes.\nHands share biomechanical properties that result in similar observable\nphenomena, such as soft-tissue deformation, blood distribution, hand pose, and\ncast shadows. We collected videos of 36 participants with diverse skin tone\napplying pressure to an instrumented planar surface. We then trained a deep\nmodel (PressureVisionNet) to infer a pressure image from a single RGB image.\nOur model infers pressure for participants outside of the training data and\noutperforms baselines. We also show that the output of our model depends on the\nappearance of the hand and cast shadows near contact regions. Overall, our\nresults suggest the appearance of a previously unobserved human hand can be\nused to accurately infer applied pressure. Data, code, and models are available\nonline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grady_P/0/1/0/all/0/1\">Patrick Grady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengcheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahmbhatt_S/0/1/0/all/0/1\">Samarth Brahmbhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twigg_C/0/1/0/all/0/1\">Christopher D. Twigg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Chengde Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1\">James Hays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemp_C/0/1/0/all/0/1\">Charles C. Kemp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V2X-ViT: Vehicle-to-Everything Cooperative Perception with Vision Transformer. (arXiv:2203.10638v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10638","description":"<p>In this paper, we investigate the application of Vehicle-to-Everything (V2X)\ncommunication to improve the perception performance of autonomous vehicles. We\npresent a robust cooperative perception framework with V2X communication using\na novel vision Transformer. Specifically, we build a holistic attention model,\nnamely V2X-ViT, to effectively fuse information across on-road agents (i.e.,\nvehicles and infrastructure). V2X-ViT consists of alternating layers of\nheterogeneous multi-agent self-attention and multi-scale window self-attention,\nwhich captures inter-agent interaction and per-agent spatial relationships.\nThese key modules are designed in a unified Transformer architecture to handle\ncommon V2X challenges, including asynchronous information sharing, pose errors,\nand heterogeneity of V2X components. To validate our approach, we create a\nlarge-scale V2X perception dataset using CARLA and OpenCDA. Extensive\nexperimental results demonstrate that V2X-ViT sets new state-of-the-art\nperformance for 3D object detection and achieves robust performance even under\nharsh, noisy environments. The code is available at\nhttps://github.com/DerrickXuNu/v2x-vit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_H/0/1/0/all/0/1\">Hao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HM: Hybrid Masking for Few-Shot Segmentation. (arXiv:2203.12826v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12826","description":"<p>We study few-shot semantic segmentation that aims to segment a target object\nfrom a query image when provided with a few annotated support images of the\ntarget class. Several recent methods resort to a feature masking (FM) technique\nto discard irrelevant feature activations which eventually facilitates the\nreliable prediction of segmentation mask. A fundamental limitation of FM is the\ninability to preserve the fine-grained spatial details that affect the accuracy\nof segmentation mask, especially for small target objects. In this paper, we\ndevelop a simple, effective, and efficient approach to enhance feature masking\n(FM). We dub the enhanced FM as hybrid masking (HM). Specifically, we\ncompensate for the loss of fine-grained spatial details in FM technique by\ninvestigating and leveraging a complementary basic input masking method.\nExperiments have been conducted on three publicly available benchmarks with\nstrong few-shot segmentation (FSS) baselines. We empirically show improved\nperformance against the current state-of-the-art methods by visible margins\nacross different benchmarks. Our code and trained models are available at:\nhttps://github.com/moonsh/HM-Hybrid-Masking\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seonghyeon Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1\">Samuel S. Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Honglu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sejong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Haris Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1\">Mubbasir Kapadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Visual Navigation Perspective for Category-Level Object Pose Estimation. (arXiv:2203.13572v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13572","description":"<p>This paper studies category-level object pose estimation based on a single\nmonocular image. Recent advances in pose-aware generative models have paved the\nway for addressing this challenging task using analysis-by-synthesis. The idea\nis to sequentially update a set of latent variables, e.g., pose, shape, and\nappearance, of the generative model until the generated image best agrees with\nthe observation. However, convergence and efficiency are two challenges of this\ninference procedure. In this paper, we take a deeper look at the inference of\nanalysis-by-synthesis from the perspective of visual navigation, and\ninvestigate what is a good navigation policy for this specific task. We\nevaluate three different strategies, including gradient descent, reinforcement\nlearning and imitation learning, via thorough comparisons in terms of\nconvergence, robustness and efficiency. Moreover, we show that a simple hybrid\napproach leads to an effective and efficient solution. We further compare these\nstrategies to state-of-the-art methods, and demonstrate superior performance on\nsynthetic and real-world datasets leveraging off-the-shelf pose-aware\ngenerative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxin Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1\">Fangxun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Active Speaker Detection. (arXiv:2203.14250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14250","description":"<p>Recent advances in the Active Speaker Detection (ASD) problem build upon a\ntwo-stage process: feature extraction and spatio-temporal context aggregation.\nIn this paper, we propose an end-to-end ASD workflow where feature learning and\ncontextual predictions are jointly learned. Our end-to-end trainable network\nsimultaneously learns multi-modal embeddings and aggregates spatio-temporal\ncontext. This results in more suitable feature representations and improved\nperformance in the ASD task. We also introduce interleaved graph neural network\n(iGNN) blocks, which split the message passing according to the main sources of\ncontext in the ASD problem. Experiments show that the aggregated features from\nthe iGNN blocks are more suitable for ASD, resulting in state-of-the art\nperformance. Finally, we design a weakly-supervised strategy, which\ndemonstrates that the ASD problem can also be approached by utilizing\naudiovisual data but relying exclusively on audio annotations. We achieve this\nby modelling the direct relationship between the audio signal and the possible\nsound sources (speakers), as well as introducing a contrastive loss. All the\nresources of this project will be made available at:\nhttps://github.com/fuankarion/end-to-end-asd.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1\">Juan Leon Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordes_M/0/1/0/all/0/1\">Moritz Cordes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14272","description":"<p>A comprehensive understanding of human-object interaction (HOI) requires\ndetecting not only a small portion of predefined HOI concepts (or categories)\nbut also other reasonable HOI concepts, while current approaches usually fail\nto explore a huge portion of unknown HOI concepts (i.e., unknown but reasonable\ncombinations of verbs and objects). In this paper, 1) we introduce a novel and\nchallenging task for a comprehensive HOI understanding, which is termed as HOI\nConcept Discovery; and 2) we devise a self-compositional learning framework (or\nSCL) for HOI concept discovery. Specifically, we maintain an online updated\nconcept confidence matrix during training: 1) we assign pseudo-labels for all\ncomposite HOI instances according to the concept confidence matrix for\nself-training; and 2) we update the concept confidence matrix using the\npredictions of all composite HOI instances. Therefore, the proposed method\nenables the learning on both known and unknown HOI concepts. We perform\nextensive experiments on several popular HOI datasets to demonstrate the\neffectiveness of the proposed method for HOI concept discovery, object\naffordance recognition and HOI detection. For example, the proposed\nself-compositional learning framework significantly improves the performance of\n1) HOI concept discovery by over 10% on HICO-DET and over 3% on V-COCO,\nrespectively; 2) object affordance recognition by over 9% mAP on MS-COCO and\nHICO-DET; and 3) rare-first and non-rare-first unknown HOI detection relatively\nover 30% and 20%, respectively. Code is publicly available at\nhttps://github.com/zhihou7/HOI-CL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15174","description":"<p>Conventional self-supervised monocular depth prediction methods are based on\na static environment assumption, which leads to accuracy degradation in dynamic\nscenes due to the mismatch and occlusion problems introduced by object motions.\nExisting dynamic-object-focused methods only partially solved the mismatch\nproblem at the training loss level. In this paper, we accordingly propose a\nnovel multi-frame monocular depth prediction method to solve these problems at\nboth the prediction and supervision loss levels. Our method, called\nDynamicDepth, is a new framework trained via a self-supervised cycle consistent\nlearning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is\nproposed to disentangle object motions to solve the mismatch problem. Moreover,\nnovel occlusion-aware Cost Volume and Re-projection Loss are designed to\nalleviate the occlusion effects of object motions. Extensive analyses and\nexperiments on the Cityscapes and KITTI datasets show that our method\nsignificantly outperforms the state-of-the-art monocular depth prediction\nmethods, especially in the areas of dynamic objects. Code is available at\nhttps://github.com/AutoAILab/DynamicDepth\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">YingLi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acknowledging the Unknown for Multi-label Learning with Single Positive Labels. (arXiv:2203.16219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16219","description":"<p>Due to the difficulty of collecting exhaustive multi-label annotations,\nmulti-label datasets often contain partial labels. We consider an extreme of\nthis weakly supervised learning problem, called single positive multi-label\nlearning (SPML), where each multi-label training image has only one positive\nlabel. Traditionally, all unannotated labels are assumed as negative labels in\nSPML, which introduces false negative labels and causes model training to be\ndominated by assumed negative labels. In this work, we choose to treat all\nunannotated labels from an alternative perspective, i.e. acknowledging they are\nunknown. Hence, we propose entropy-maximization (EM) loss to attain a special\ngradient regime for providing proper supervision signals. Moreover, we propose\nasymmetric pseudo-labeling (APL), which adopts asymmetric-tolerance strategies\nand a self-paced procedure, to cooperate with EM loss and then provide more\nprecise supervision. Experiments show that our method significantly improves\nperformance and achieves state-of-the-art results on all four benchmarks. Code\nis available at https://github.com/Correr-Zhou/SPML-AckTheUnknown.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Donghao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pengfei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guangyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqTR: A Simple yet Universal Network for Visual Grounding. (arXiv:2203.16265v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16265","description":"<p>In this paper, we propose a simple yet universal network termed SeqTR for\nvisual grounding tasks, e.g., phrase localization, referring expression\ncomprehension (REC) and segmentation (RES). The canonical paradigms for visual\ngrounding often require substantial expertise in designing network\narchitectures and loss functions, making them hard to generalize across tasks.\nTo simplify and unify the modeling, we cast visual grounding as a point\nprediction problem conditioned on image and text inputs, where either the\nbounding box or binary mask is represented as a sequence of discrete coordinate\ntokens. Under this paradigm, visual grounding tasks are unified in our SeqTR\nnetwork without task-specific branches or heads, e.g., the convolutional mask\ndecoder for RES, which greatly reduces the complexity of multi-task modeling.\nIn addition, SeqTR also shares the same optimization objective for all tasks\nwith a simple cross-entropy loss, further reducing the complexity of deploying\nhand-crafted loss functions. Experiments on five benchmark datasets demonstrate\nthat the proposed SeqTR outperforms (or is on par with) the existing\nstate-of-the-arts, proving that a simple yet universal approach for visual\ngrounding is indeed feasible. Source code is available at\nhttps://github.com/sean-zhuh/SeqTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chaoyang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Liujuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2L: Distilling Neural Radiance Field to Neural Light Field for Efficient Novel View Synthesis. (arXiv:2203.17261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.17261","description":"<p>Recent research explosion on Neural Radiance Field (NeRF) shows the\nencouraging potential to represent complex scenes with neural networks. One\nmajor drawback of NeRF is its prohibitive inference time: Rendering a single\npixel requires querying the NeRF network hundreds of times. To resolve it,\nexisting efforts mainly attempt to reduce the number of required sampled\npoints. However, the problem of iterative sampling still exists. On the other\nhand, Neural Light Field (NeLF) presents a more straightforward representation\nover NeRF in novel view synthesis -- the rendering of a pixel amounts to one\nsingle forward pass without ray-marching. In this work, we present a deep\nresidual MLP network (88 layers) to effectively learn the light field. We show\nthe key to successfully learning such a deep NeLF network is to have sufficient\ndata, for which we transfer the knowledge from a pre-trained NeRF model via\ndata distillation. Extensive experiments on both synthetic and real-world\nscenes show the merits of our method over other counterpart algorithms. On the\nsynthetic scenes, we achieve 26-35x FLOPs reduction (per camera ray) and 28-31x\nruntime speedup, meanwhile delivering significantly better (1.4-2.8 dB average\nPSNR improvement) rendering quality than NeRF without any customized\nparallelism requirement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_M/0/1/0/all/0/1\">Menglei Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digitizing Historical Balance Sheet Data: A Practitioner's Guide. (arXiv:2204.00052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00052","description":"<p>This paper discusses how to successfully digitize large-scale historical\nmicro-data by augmenting optical character recognition (OCR) engines with pre-\nand post-processing methods. Although OCR software has improved dramatically in\nrecent years due to improvements in machine learning, off-the-shelf OCR\napplications still present high error rates which limit their applications for\naccurate extraction of structured information. Complementing OCR with\nadditional methods can however dramatically increase its success rate, making\nit a powerful and cost-efficient tool for economic historians. This paper\nshowcases these methods and explains why they are useful. We apply them against\ntwo large balance sheet datasets and introduce quipucamayoc, a Python package\ncontaining these methods in a unified framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Correia_S/0/1/0/all/0/1\">Sergio Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luck_S/0/1/0/all/0/1\">Stephan Luck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00833","description":"<p>Pixel synthesis is a promising research paradigm for image generation, which\ncan well exploit pixel-wise prior knowledge for generation. However, existing\nmethods still suffer from excessive memory footprint and computation overhead.\nIn this paper, we propose a progressive pixel synthesis network towards\nefficient image generation, coined as PixelFolder. Specifically, PixelFolder\nformulates image generation as a progressive pixel regression problem and\nsynthesizes images via a multi-stage structure, which can greatly reduce the\noverhead caused by large tensor transformations. In addition, we introduce\nnovel pixel folding operations to further improve model efficiency while\nmaintaining pixel-wise prior knowledge for end-to-end regression. With these\ninnovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,\nreducing 89% computation and 53% parameters compared with the latest pixel\nsynthesis method CIPS. To validate our approach, we conduct extensive\nexperiments on two benchmark datasets, namely FFHQ and LSUN Church. The\nexperimental results show that with much less expenditure, PixelFolder obtains\nnew state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77\nFID and 2.45 FID on FFHQ and LSUN Church, respectively.Meanwhile, PixelFolder\nis also more efficient than the SOTA methods like StyleGAN2, reducing about 72%\ncomputation and 31% parameters, respectively. These results greatly validate\nthe effectiveness of the proposed PixelFolder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00993","description":"<p>The transformer models have shown promising effectiveness in dealing with\nvarious vision tasks. However, compared with training Convolutional Neural\nNetwork (CNN) models, training Vision Transformer (ViT) models is more\ndifficult and relies on the large-scale training set. To explain this\nobservation we make a hypothesis that \\textit{ViT models are less effective in\ncapturing the high-frequency components of images than CNN models}, and verify\nit by a frequency analysis. Inspired by this finding, we first investigate the\neffects of existing techniques for improving ViT models from a new frequency\nperspective, and find that the success of some techniques (e.g., RandAugment)\ncan be attributed to the better usage of the high-frequency components. Then,\nto compensate for this insufficient ability of ViT models, we propose HAT,\nwhich directly augments high-frequency components of images via adversarial\ntraining. We show that HAT can consistently boost the performance of various\nViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance\nthe advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the\nsuperiority can also be maintained on out-of-distribution data and transferred\nto downstream tasks. The code is available at:\nhttps://github.com/jiawangbai/HAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiawang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaxViT: Multi-Axis Vision Transformer. (arXiv:2204.01697v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01697","description":"<p>Transformers have recently gained significant attention in the computer\nvision community. However, the lack of scalability of self-attention mechanisms\nwith respect to image size has limited their wide adoption in state-of-the-art\nvision backbones. In this paper we introduce an efficient and scalable\nattention model we call multi-axis attention, which consists of two aspects:\nblocked local and dilated global attention. These design choices allow\nglobal-local spatial interactions on arbitrary input resolutions with only\nlinear complexity. We also present a new architectural element by effectively\nblending our proposed attention model with convolutions, and accordingly\npropose a simple hierarchical vision backbone, dubbed MaxViT, by simply\nrepeating the basic building block over multiple stages. Notably, MaxViT is\nable to ''see'' globally throughout the entire network, even in earlier,\nhigh-resolution stages. We demonstrate the effectiveness of our model on a\nbroad spectrum of vision tasks. On image classification, MaxViT achieves\nstate-of-the-art performance under various settings: without extra data, MaxViT\nattains 86.5% ImageNet-1K top-1 accuracy; with ImageNet-21K pre-training, our\nmodel achieves 88.7% top-1 accuracy. For downstream tasks, MaxViT as a backbone\ndelivers favorable performance on object detection as well as visual aesthetic\nassessment. We also show that our proposed model expresses strong generative\nmodeling capability on ImageNet, demonstrating the superior potential of MaxViT\nblocks as a universal vision module. The source code and trained models will be\navailable at https://github.com/google-research/maxvit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talebi_H/0/1/0/all/0/1\">Hossein Talebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan Bovik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinxiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer. (arXiv:2204.03638v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03638","description":"<p>Videos are created to express emotion, exchange information, and share\nexperiences. Video synthesis has intrigued researchers for a long time. Despite\nthe rapid progress driven by advances in visual synthesis, most existing\nstudies focus on improving the frames' quality and the transitions between\nthem, while little progress has been made in generating longer videos. In this\npaper, we present a method that builds on 3D-VQGAN and transformers to generate\nvideos with thousands of frames. Our evaluation shows that our model trained on\n16-frame video clips from standard benchmarks such as UCF-101, Sky Time-lapse,\nand Taichi-HD datasets can generate diverse, coherent, and high-quality long\nvideos. We also showcase conditional extensions of our approach for generating\nmeaningful long videos by incorporating temporal information with text and\naudio. Videos and code can be found at\nhttps://songweige.github.io/projects/tats/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Thomas Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Harry Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_D/0/1/0/all/0/1\">David Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Particle Video Revisited: Tracking Through Occlusions Using Point Trajectories. (arXiv:2204.04153v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04153","description":"<p>Tracking pixels in videos is typically studied as an optical flow estimation\nproblem, where every pixel is described with a displacement vector that locates\nit in the next frame. Even though wider temporal context is freely available,\nprior efforts to take this into account have yielded only small gains over\n2-frame methods. In this paper, we revisit Sand and Teller's \"particle video\"\napproach, and study pixel tracking as a long-range motion estimation problem,\nwhere every pixel is described with a trajectory that locates it in multiple\nfuture frames. We re-build this classic approach using components that drive\nthe current state-of-the-art in flow and object tracking, such as dense cost\nmaps, iterative optimization, and learned appearance updates. We train our\nmodels using long-range amodal point trajectories mined from existing optical\nflow data that we synthetically augment with multi-frame occlusions. We test\nour approach in trajectory estimation benchmarks and in keypoint label\npropagation tasks, and compare favorably against state-of-the-art optical flow\nand feature tracking methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1\">Adam W. Harley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhaoyuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind Image Deblurring. (arXiv:2204.07820v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07820","description":"<p>Blind image deblurring (BID) remains a challenging and significant task.\nBenefiting from the strong fitting ability of deep learning, paired data-driven\nsupervised BID method has obtained great progress. However, paired data are\nusually synthesized by hand, and the realistic blurs are more complex than\nsynthetic ones, which makes the supervised methods inept at modeling realistic\nblurs and hinders their real-world applications. As such, unsupervised deep BID\nmethod without paired data offers certain advantages, but current methods still\nsuffer from some drawbacks, e.g., bulky model size, long inference time, and\nstrict image resolution and domain requirements. In this paper, we propose a\nlightweight and real-time unsupervised BID baseline, termed Frequency-domain\nContrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with\nattractive properties, i.e., no image domain limitation, no image resolution\nlimitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the\nlightweight property and performance superiority, two new collaboration units\ncalled lightweight domain conversion unit(LDCU) and parameter-free\nfrequency-domain contrastive unit(PFCU) are designed. LDCU mainly implements\ninter-domain conversion in lightweight manner. PFCU further explores the\nsimilarity measure, external difference and internal connection between the\nblurred domain and sharp domain images in frequency domain, without involving\nextra parameters. Extensive experiments on several image datasets demonstrate\nthe effectiveness of our FCL-GAN in terms of performance, model size and\nreference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Suiyi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1\">Richang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Share With Thy Neighbors: Single-View Reconstruction by Cross-Instance Consistency. (arXiv:2204.10310v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10310","description":"<p>Approaches for single-view reconstruction typically rely on viewpoint\nannotations, silhouettes, the absence of background, multiple views of the same\ninstance, a template shape, or symmetry. We avoid all such supervision and\nassumptions by explicitly leveraging the consistency between images of\ndifferent object instances. As a result, our method can learn from large\ncollections of unlabelled images depicting the same object category. Our main\ncontributions are two ways for leveraging cross-instance consistency: (i)\nprogressive conditioning, a training strategy to gradually specialize the model\nfrom category to instances in a curriculum learning fashion; and (ii) neighbor\nreconstruction, a loss enforcing consistency between instances having similar\nshape or texture. Also critical to the success of our method are: our\nstructured autoencoding architecture decomposing an image into explicit shape,\ntexture, pose, and background; an adapted formulation of differential\nrendering; and a new optimization scheme alternating between 3D and pose\nlearning. We compare our approach, UNICORN, both on the diverse synthetic\nShapeNet dataset - the classical benchmark for methods requiring multiple views\nas supervision - and on standard real-image benchmarks (Pascal3D+ Car, CUB) for\nwhich most methods require known templates and silhouette annotations. We also\nshowcase applicability to more challenging real-world collections (CompCars,\nLSUN), where silhouettes are not available and images are not cropped around\nthe object.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monnier_T/0/1/0/all/0/1\">Tom Monnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1\">Matthew Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aubry_M/0/1/0/all/0/1\">Mathieu Aubry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where in the World is this Image? Transformer-based Geo-localization in the Wild. (arXiv:2204.13861v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13861","description":"<p>Predicting the geographic location (geo-localization) from a single\nground-level RGB image taken anywhere in the world is a very challenging\nproblem. The challenges include huge diversity of images due to different\nenvironmental scenarios, drastic variation in the appearance of the same\nlocation depending on the time of the day, weather, season, and more\nimportantly, the prediction is made from a single image possibly having only a\nfew geo-locating cues. For these reasons, most existing works are restricted to\nspecific cities, imagery, or worldwide landmarks. In this work, we focus on\ndeveloping an efficient solution to planet-scale single-image geo-localization.\nTo this end, we propose TransLocator, a unified dual-branch transformer network\nthat attends to tiny details over the entire image and produces robust feature\nrepresentation under extreme appearance variations. TransLocator takes an RGB\nimage and its semantic segmentation map as inputs, interacts between its two\nparallel branches after each transformer layer, and simultaneously performs\ngeo-localization and scene recognition in a multi-task fashion. We evaluate\nTransLocator on four benchmark datasets - Im2GPS, Im2GPS3k, YFCC4k, YFCC26k and\nobtain 5.5%, 14.1%, 4.9%, 9.9% continent-level accuracy improvement over the\nstate-of-the-art. TransLocator is also validated on real-world test images and\nfound to be more effective than previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowara_E/0/1/0/all/0/1\">Ewa M. Nowara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking. (arXiv:2205.02301v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02301","description":"<p>Estimating human motion from video is an active research area due to its many\npotential applications. Most state-of-the-art methods predict human shape and\nposture estimates for individual images and do not leverage the temporal\ninformation available in video. Many \"in the wild\" sequences of human motion\nare captured by a moving camera, which adds the complication of conflated\ncamera and human motion to the estimation. We therefore present BodySLAM, a\nmonocular SLAM system that jointly estimates the position, shape, and posture\nof human bodies, as well as the camera trajectory. We also introduce a novel\nhuman motion model to constrain sequential body postures and observe the scale\nof the scene. Through a series of experiments on video sequences of human\nmotion captured by a moving monocular camera, we demonstrate that BodySLAM\nimproves estimates of all human body parameters and camera poses when compared\nto estimating these separately.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henning_D/0/1/0/all/0/1\">Dorian F. Henning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1\">Stefan Leutenegger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-CLOP: CLIP-Guided Collage and Photomontage. (arXiv:2205.03146v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03146","description":"<p>The unabated mystique of large-scale neural networks, such as the CLIP dual\nimage-and-text encoder, popularized automatically generated art. Increasingly\nmore sophisticated generators enhanced the artworks' realism and visual\nappearance, and creative prompt engineering enabled stylistic expression.\nGuided by an artist-in-the-loop ideal, we design a gradient-based generator to\nproduce collages. It requires the human artist to curate libraries of image\npatches and to describe (with prompts) the whole image composition, with the\noption to manually adjust the patches' positions during generation, thereby\nallowing humans to reclaim some control of the process and achieve greater\ncreative freedom. We explore the aesthetic potentials of high-resolution\ncollages, and provide an open-source Google Colab as an artistic tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirowski_P/0/1/0/all/0/1\">Piotr Mirowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banarse_D/0/1/0/all/0/1\">Dylan Banarse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_C/0/1/0/all/0/1\">Chrisantha Fernando</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Racially Unbiased Skin Tone Estimation via Scene Disambiguation. (arXiv:2205.03962v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03962","description":"<p>Virtual facial avatars will play an increasingly important role in immersive\ncommunication, games and the metaverse, and it is therefore critical that they\nbe inclusive. This requires accurate recovery of the appearance, represented by\nalbedo, regardless of age, sex, or ethnicity. While significant progress has\nbeen made on estimating 3D facial geometry, albedo estimation has received less\nattention. The task is fundamentally ambiguous because the observed color is a\nfunction of albedo and lighting, both of which are unknown. We find that\ncurrent methods are biased towards light skin tones due to (1) strongly biased\npriors that prefer lighter pigmentation and (2) algorithmic solutions that\ndisregard the light/albedo ambiguity. To address this, we propose a new\nevaluation dataset (FAIR) and an algorithm (TRUST) to improve albedo estimation\nand, hence, fairness. Specifically, we create the first facial albedo\nevaluation benchmark where subjects are balanced in terms of skin color, and\nmeasure accuracy using the Individual Typology Angle (ITA) metric. We then\naddress the light/albedo ambiguity by building on a key observation: the image\nof the full scene -- as opposed to a cropped image of the face -- contains\nimportant information about lighting that can be used for disambiguation. TRUST\nregresses facial albedo by conditioning both on the face region and a global\nillumination signal obtained from the scene image. Our experimental results\nshow significant improvement compared to state-of-the-art methods on albedo\nestimation, both in terms of accuracy and fairness. The evaluation benchmark\nand code will be made available for research purposes at\nhttps://trust.is.tue.mpg.de.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1\">Haiwen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolkart_T/0/1/0/all/0/1\">Timo Bolkart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tesch_J/0/1/0/all/0/1\">Joachim Tesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1\">Victoria Abrevaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06803","description":"<p>Although generative facial prior and geometric prior have recently\ndemonstrated high-quality results for blind face restoration, producing\nfine-grained facial details faithful to inputs remains a challenging problem.\nMotivated by the classical dictionary-based methods and the recent vector\nquantization (VQ) technique, we propose a VQ-based face restoration method -\nVQFR. VQFR takes advantage of high-quality low-level feature banks extracted\nfrom high-quality faces and can thus help recover realistic facial details.\nHowever, the simple application of the VQ codebook cannot achieve good results\nwith faithful details and identity preservation. Therefore, we further\nintroduce two special network designs. 1). We first investigate the compression\npatch size in the VQ codebook and find that the VQ codebook designed with a\nproper compression patch size is crucial to balance the quality and fidelity.\n2). To further fuse low-level features from inputs while not \"contaminating\"\nthe realistic details generated from the VQ codebook, we proposed a parallel\ndecoder consisting of a texture decoder and a main decoder. Those two decoders\nthen interact with a texture warping module with deformable convolution.\nEquipped with the VQ codebook as a facial detail dictionary and the parallel\ndecoder design, the proposed VQFR can largely enhance the restored quality of\nfacial details while keeping the fidelity to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Segmentation in Real-World Images via Spelke Object Inference. (arXiv:2205.08515v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08515","description":"<p>Self-supervised, category-agnostic segmentation of real-world images is a\nchallenging open problem in computer vision. Here, we show how to learn static\ngrouping priors from motion self-supervision by building on the cognitive\nscience concept of a Spelke Object: a set of physical stuff that moves\ntogether. We introduce the Excitatory-Inhibitory Segment Extraction Network\n(EISEN), which learns to extract pairwise affinity graphs for static scenes\nfrom motion-based training signals. EISEN then produces segments from\naffinities using a novel graph propagation and competition network. During\ntraining, objects that undergo correlated motion (such as robot arms and the\nobjects they move) are decoupled by a bootstrapping process: EISEN explains\naway the motion of objects it has already learned to segment. We show that\nEISEN achieves a substantial improvement in the state of the art for\nself-supervised image segmentation on challenging synthetic and real-world\nrobotics datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honglin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_R/0/1/0/all/0/1\">Rahul Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedman_Y/0/1/0/all/0/1\">Yoni Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1\">Daniel L. K. Yamins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bear_D/0/1/0/all/0/1\">Daniel M. Bear</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically-Based Editing of Indoor Scene Lighting from a Single Image. (arXiv:2205.09343v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09343","description":"<p>We present a method to edit complex indoor lighting from a single image with\nits predicted depth and light source segmentation masks. This is an extremely\nchallenging problem that requires modeling complex light transport, and\ndisentangling HDR lighting from material and geometry with only a partial LDR\nobservation of the scene. We tackle this problem using two novel components: 1)\na holistic scene reconstruction method that estimates scene reflectance and\nparametric 3D lighting, and 2) a neural rendering framework that re-renders the\nscene from our predictions. We use physically-based indoor light\nrepresentations that allow for intuitive editing, and infer both visible and\ninvisible light sources. Our neural rendering framework combines\nphysically-based direct illumination and shadow rendering with deep networks to\napproximate global illumination. It can capture challenging lighting effects,\nsuch as soft shadows, directional lighting, specular materials, and\ninterreflections. Previous single image inverse rendering methods usually\nentangle scene lighting and geometry and only support applications like object\ninsertion. Instead, by combining parametric 3D lighting estimation with neural\nscene rendering, we demonstrate the first automatic method to achieve full\nscene relighting, including light source insertion, removal, and replacement,\nfrom a single image. All source code and data will be publicly released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Rui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Milo&#x161; Ha&#x161;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Gender Prediction Based on Deep Transfer Learning from Panoramic Radiograph Images. (arXiv:2205.09850v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.09850","description":"<p>Panoramic Dental Radiography (PDR) image processing is one of the most widely\nused methods for gender determination in forensic medicine. Deep learning\nmodels are widely used in automated analysis of radiological images today due\nto their high processing speed, accuracy and stability. A few approach using\ntransfer learning is proposed to gender-classify PDR images. In this study,\nDenseNet121 convolutional neural network (CNN) classifier, which is one of the\npre-trained Deep learning architectures, was used. The proposed DenseNet121\nnetwork has been expanded and fine-tuned with several additional layers before\nthe final layer to increase its ability to understand more complex patterns\nfrom data. At the end of this stage, it has been trained with the dental\ndataset containing PDR images and has become more experienced. K-fold cross\nvalidation method is adopted to increase the accuracy of the proposed\nDenseNet121 model.In this study the best performance was achieved for the 4,800\ntest datasets with a classification accuracy of 97.25%. The proposed model,\nalong with Grad-CAM based analysis also revealed that the mandible\ncircumference and teeth are the most significant areas to consider in gender\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Atas_I/0/1/0/all/0/1\">I. Atas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Skin Lesion Segmentation via Dilated Scale-Wise Feature Fusion Network. (arXiv:2205.10272v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10272","description":"<p>Skin lesion detection in dermoscopic images is essential in the accurate and\nearly diagnosis of skin cancer by a computerized apparatus. Current skin lesion\nsegmentation approaches show poor performance in challenging circumstances such\nas indistinct lesion boundaries, low contrast between the lesion and the\nsurrounding area, or heterogeneous background that causes over/under\nsegmentation of the skin lesion. To accurately recognize the lesion from the\nneighboring regions, we propose a dilated scale-wise feature fusion network\nbased on convolution factorization. Our network is designed to simultaneously\nextract features at different scales which are systematically fused for better\ndetection. The proposed model has satisfactory accuracy and efficiency. Various\nexperiments for lesion segmentation are performed along with comparisons with\nthe state-of-the-art models. Our proposed model consistently showcases\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1\">Pourya Shamsolmoali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1\">Masoumeh Zareapoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Individual Topology Structure of Eye Movement Trajectories. (arXiv:2205.10667v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10667","description":"<p>Traditionally, extracting patterns from eye movement data relies on\nstatistics of different macro-events such as fixations and saccades. This\nrequires an additional preprocessing step to separate the eye movement\nsubtypes, often with a number of parameters on which the classification results\ndepend. Besides that, definitions of such macro events are formulated in\ndifferent ways by different researchers.\n</p>\n<p>We propose an application of a new class of features to the quantitative\nanalysis of personal eye movement trajectories structure. This new class of\nfeatures based on algebraic topology allows extracting patterns from different\nmodalities of gaze such as time series of coordinates and amplitudes, heatmaps,\nand point clouds in a unified way at all scales from micro to macro. We\nexperimentally demonstrate the competitiveness of the new class of features\nwith the traditional ones and their significant synergy while being used\ntogether for the person authentication task on the recently published eye\nmovement trajectories dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Onuchin_A/0/1/0/all/0/1\">Arsenii A. Onuchin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachan_O/0/1/0/all/0/1\">Oleg N. Kachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiSPRNet: End-to-End Learning for Single-Shot Phase Retrieval. (arXiv:2205.11434v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11434","description":"<p>With the success of deep learning methods in many image processing tasks,\ndeep learning approaches have also been introduced to the phase retrieval\nproblem recently. These approaches are different from the traditional iterative\noptimization methods in that they usually require only one intensity\nmeasurement and can reconstruct phase images in real-time. However, because of\ntremendous domain discrepancy, the quality of the reconstructed images given by\nthese approaches still has much room to improve to meet the general application\nrequirements. In this paper, we design a novel deep neural network structure\nnamed SiSPRNet for phase retrieval based on a single Fourier intensity\nmeasurement. To effectively utilize the spectral information of the\nmeasurements, we propose a new feature extraction unit using the Multi-Layer\nPerceptron (MLP) as the front end. It allows all pixels of the input intensity\nimage to be considered together for exploring their global representation. The\nsize of the MLP is carefully designed to facilitate the extraction of the\nrepresentative features while reducing noises and outliers. A dropout layer is\nalso equipped to mitigate the possible overfitting problem in training the MLP.\nTo promote the global correlation in the reconstructed images, a self-attention\nmechanism is introduced to the Up-sampling and Reconstruction (UR) blocks of\nthe proposed SiSPRNet. These UR blocks are inserted into a residual learning\nstructure to prevent the weak information flow and vanishing gradient problems\ndue to their complex layer structure. Extensive evaluations of the proposed\nmodel are performed using different testing datasets of phase-only images and\nimages with linearly related magnitude and phase. Experiments were conducted on\nan optical experimentation platform to understand the performance of different\ndeep learning methods when working in a practical environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qiuliang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li-Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lun_D/0/1/0/all/0/1\">Daniel P.K. Lun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust 3D Object Detection in Cold Weather Conditions. (arXiv:2205.11925v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11925","description":"<p>Adverse weather conditions can negatively affect LiDAR-based object\ndetectors. In this work, we focus on the phenomenon of vehicle gas exhaust\ncondensation in cold weather conditions. This everyday effect can influence the\nestimation of object sizes, orientations and introduce ghost object detections,\ncompromising the reliability of the state of the art object detectors. We\npropose to solve this problem by using data augmentation and a novel training\nloss term. To effectively train deep neural networks, a large set of labeled\ndata is needed. In case of adverse weather conditions, this process can be\nextremely laborious and expensive. We address this issue in two steps: First,\nwe present a gas exhaust data generation method based on 3D surface\nreconstruction and sampling which allows us to generate large sets of gas\nexhaust clouds from a small pool of labeled data. Second, we introduce a point\ncloud augmentation process that can be used to add gas exhaust to datasets\nrecorded in good weather conditions. Finally, we formulate a new training loss\nterm that leverages the augmented point cloud to increase object detection\nrobustness by penalizing predictions that include noise. In contrast to other\nworks, our method can be used with both grid-based and point-based detectors.\nMoreover, since our approach does not require any network architecture changes,\ninference times remain unchanged. Experimental results on real data show that\nour proposed method greatly increases robustness to gas exhaust and noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piroli_A/0/1/0/all/0/1\">Aldi Piroli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dallabetta_V/0/1/0/all/0/1\">Vinzenz Dallabetta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walessa_M/0/1/0/all/0/1\">Marc Walessa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meissner_D/0/1/0/all/0/1\">Daniel Meissner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_J/0/1/0/all/0/1\">Johannes Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RADNet: Ensemble Model for Robust Glaucoma Classification in Color Fundus Images. (arXiv:2205.12902v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.12902","description":"<p>Glaucoma is one of the most severe eye diseases, characterized by rapid\nprogression and leading to irreversible blindness. It is often the case that\ndiagnostics is carried out when one's sight has already significantly degraded\ndue to the lack of noticeable symptoms at early stage of the disease. Regular\nglaucoma screenings of the population shall improve early-stage detection,\nhowever the desirable frequency of etymological checkups is often not feasible\ndue to the excessive load imposed by manual diagnostics on limited number of\nspecialists. Considering the basic methodology to detect glaucoma is to analyze\nfundus images for the optic-disc-to-optic-cup ratio, Machine Learning\nalgorithms can offer sophisticated methods for image processing and\nclassification. In our work, we propose an advanced image pre-processing\ntechnique combined with a multi-view network of deep classification models to\ncategorize glaucoma. Our Glaucoma Automated Retinal Detection Network (GARDNet)\nhas been successfully tested on Rotterdam EyePACS AIROGS dataset with an AUC of\n0.92, and then additionally fine-tuned and tested on RIM-ONE DL dataset with an\nAUC of 0.9308 outperforming the state-of-the-art of 0.9272. Our code will be\nmade available on GitHub upon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mahrooqi_A/0/1/0/all/0/1\">Ahmed Al Mahrooqi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Medvedev_D/0/1/0/all/0/1\">Dmitrii Medvedev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muhtaseb_R/0/1/0/all/0/1\">Rand Muhtaseb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EiX-GNN : Concept-level eigencentrality explainer for graph neural networks. (arXiv:2206.03491v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2206.03491","description":"<p>Explaining is a human knowledge transfer process regarding a phenomenon\nbetween an explainer and an explainee. Each word used to explain this\nphenomenon must be carefully selected by the explainer in accordance with the\ncurrent explainee phenomenon-related knowledge level and the phenomenon itself\nin order to have a high understanding from the explainee of the phenomenon.\nNowadays, deep models, especially graph neural networks, have a major place in\ndaily life even in critical applications. In such context, those models need to\nhave a human high interpretability also referred as being explainable, in order\nto improve usage trustability of them in sensitive cases. Explaining is also a\nhuman dependent task and methods that explain deep model behavior must include\nthese social-related concerns for providing profitable and quality\nexplanations. Current explaining methods often occlude such social aspect for\nproviding their explanations and only focus on the signal aspect of the\nquestion. In this contribution we propose a reliable social-aware explaining\nmethod suited for graph neural network that includes this social feature as a\nmodular concept generator and by both leveraging signal and graph domain aspect\nthanks to an eigencentrality concept ordering approach. Besides our method\ntakes into account the human-dependent aspect underlying any explanation\nprocess, we also reach high score regarding state-of-the-art objective metrics\nassessing explanation methods for graph neural networks models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raison_A/0/1/0/all/0/1\">Adrien Raison</a> (XLIM-ASALI), <a href=\"http://arxiv.org/find/cs/1/au:+Bourdon_P/0/1/0/all/0/1\">Pascal Bourdon</a> (XLIM-ASALI), <a href=\"http://arxiv.org/find/cs/1/au:+Helbert_D/0/1/0/all/0/1\">David Helbert</a> (XLIM-ASALI)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn2Augment: Learning to Composite Videos for Data Augmentation in Action Recognition. (arXiv:2206.04790v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04790","description":"<p>We address the problem of data augmentation for video action recognition.\nStandard augmentation strategies in video are hand-designed and sample the\nspace of possible augmented data points either at random, without knowing which\naugmented points will be better, or through heuristics. We propose to learn\nwhat makes a good video for action recognition and select only high-quality\nsamples for augmentation. In particular, we choose video compositing of a\nforeground and a background video as the data augmentation process, which\nresults in diverse and realistic new samples. We learn which pairs of videos to\naugment without having to actually composite them. This reduces the space of\npossible augmentations, which has two advantages: it saves computational cost\nand increases the accuracy of the final trained classifier, as the augmented\npairs are of higher quality than average. We present experimental results on\nthe entire spectrum of training settings: few-shot, semi-supervised and fully\nsupervised. We observe consistent improvements across all of them over prior\nwork and baselines on Kinetics, UCF101, HMDB51, and achieve a new\nstate-of-the-art on settings with limited data. We see improvements of up to\n8.6% in the semi-supervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shreyank N Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_M/0/1/0/all/0/1\">Marcus Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sevilla_Lara_L/0/1/0/all/0/1\">Laura Sevilla-Lara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects. (arXiv:2206.07219v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.07219","description":"<p>The recent development of deep learning combined with compressed sensing\nenables fast reconstruction of undersampled MR images and has achieved\nstate-of-the-art performance for Cartesian k-space trajectories. However,\nnon-Cartesian trajectories such as the radial trajectory need to be transformed\nonto a Cartesian grid in each iteration of the network training, slowing down\nthe training process and posing inconvenience and delay during training.\nMultiple iterations of nonuniform Fourier transform in the networks offset the\ndeep learning advantage of fast inference. Current approaches typically either\nwork on image-to-image networks or grid the non-Cartesian trajectories before\nthe network training to avoid the repeated gridding process. However, the\nimage-to-image networks cannot ensure the k-space data consistency in the\nreconstructed images and the pre-processing of non-Cartesian k-space leads to\ngridding errors which cannot be compensated by the network training. Inspired\nby the Transformer network to handle long-range dependencies in sequence\ntransduction tasks, we propose to rearrange the radial spokes to sequential\ndata based on the chronological order of acquisition and use the Transformer to\npredict unacquired radial spokes from acquired ones. We propose novel data\naugmentation methods to generate a large amount of training data from a limited\nnumber of subjects. The network can be generated to different anatomical\nstructures. Experimental results show superior performance of the proposed\nframework compared to state-of-the-art deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shih_S/0/1/0/all/0/1\">Shu-Fu Shih</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Finn_J/0/1/0/all/0/1\">J. Paul Finn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_X/0/1/0/all/0/1\">Xiaodong Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Captioning based on Feature Refinement and Reflective Decoding. (arXiv:2206.07986v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07986","description":"<p>Image captioning is the process of automatically generating a description of\nan image in natural language. Image captioning is one of the significant\nchallenges in image understanding since it requires not only recognizing\nsalient objects in the image but also their attributes and the way they\ninteract. The system must then generate a syntactically and semantically\ncorrect caption that describes the image content in natural language. With the\nsignificant progress in deep learning models and their ability to effectively\nencode large sets of images and generate correct sentences, several\nneural-based captioning approaches have been proposed recently, each trying to\nachieve better accuracy and caption quality. This paper introduces an\nencoder-decoder-based image captioning system in which the encoder extracts\nspatial features from the image using ResNet-101. This stage is followed by a\nrefining model, which uses an attention-on-attention mechanism to extract the\nvisual features of the target image objects, then determine their interactions.\nThe decoder consists of an attention-based recurrent module and a reflective\nattention module, which collaboratively apply attention to the visual and\ntextual features to enhance the decoder's ability to model long-term sequential\ndependencies. Extensive experiments performed on Flickr30K, show the\neffectiveness of the proposed approach and the high quality of the generated\ncaptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alabduljabbar_G/0/1/0/all/0/1\">Ghadah Alabduljabbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhidour_H/0/1/0/all/0/1\">Hafida Benhidour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerrache_S/0/1/0/all/0/1\">Said Kerrache</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Annotation Need in Self-Explanatory Models for Lung Nodule Diagnosis. (arXiv:2206.13608v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13608","description":"<p>Feature-based self-explanatory methods explain their classification in terms\nof human-understandable features. In the medical imaging community, this\nsemantic matching of clinical knowledge adds significantly to the\ntrustworthiness of the AI. However, the cost of additional annotation of\nfeatures remains a pressing issue. We address this problem by proposing\ncRedAnno, a data-/annotation-efficient self-explanatory approach for lung\nnodule diagnosis. cRedAnno considerably reduces the annotation need by\nintroducing self-supervised contrastive learning to alleviate the burden of\nlearning most parameters from annotation, replacing end-to-end training with\ntwo-stage training. When training with hundreds of nodule samples and only 1%\nof their annotations, cRedAnno achieves competitive accuracy in predicting\nmalignancy, meanwhile significantly surpassing most previous works in\npredicting nodule attributes. Visualisation of the learned space further\nindicates that the correlation between the clustering of malignancy and nodule\nattributes coincides with clinical knowledge. Our complete code is open-source\navailable: https://github.com/diku-dk/credanno.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiahao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Chong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krause_O/0/1/0/all/0/1\">Oswin Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erleben_K/0/1/0/all/0/1\">Kenny Erleben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_M/0/1/0/all/0/1\">Michael Bachmann Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Timestamp-Supervised Action Segmentation with Graph Convolutional Networks. (arXiv:2206.15031v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15031","description":"<p>We introduce a novel approach for temporal activity segmentation with\ntimestamp supervision. Our main contribution is a graph convolutional network,\nwhich is learned in an end-to-end manner to exploit both frame features and\nconnections between neighboring frames to generate dense framewise labels from\nsparse timestamp labels. The generated dense framewise labels can then be used\nto train the segmentation model. In addition, we propose a framework for\nalternating learning of both the segmentation model and the graph convolutional\nmodel, which first initializes and then iteratively refines the learned models.\nDetailed experiments on four public datasets, including 50 Salads, GTEA,\nBreakfast, and Desktop Assembly, show that our method is superior to the\nmulti-layer perceptron baseline, while performing on par with or better than\nthe state of the art in temporal activity segmentation with timestamp\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Hamza Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1\">Sanjay Haresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Awais Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_S/0/1/0/all/0/1\">Shakeeb Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1\">Andrey Konin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1\">M. Zeeshan Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-term Leap Attention, Short-term Periodic Shift for Video Classification. (arXiv:2207.05526v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05526","description":"<p>Video transformer naturally incurs a heavier computation burden than a static\nvision transformer, as the former processes $T$ times longer sequence than the\nlatter under the current attention of quadratic complexity $(T^2N^2)$. The\nexisting works treat the temporal axis as a simple extension of spatial axes,\nfocusing on shortening the spatio-temporal sequence by either generic pooling\nor local windowing without utilizing temporal redundancy.\n</p>\n<p>However, videos naturally contain redundant information between neighboring\nframes; thereby, we could potentially suppress attention on visually similar\nframes in a dilated manner. Based on this hypothesis, we propose the LAPS, a\nlong-term ``\\textbf{\\textit{Leap Attention}}'' (LA), short-term\n``\\textbf{\\textit{Periodic Shift}}'' (\\textit{P}-Shift) module for video\ntransformers, with $(2TN^2)$ complexity. Specifically, the ``LA'' groups\nlong-term frames into pairs, then refactors each discrete pair via attention.\nThe ``\\textit{P}-Shift'' exchanges features between temporal neighbors to\nconfront the loss of short-term dynamics. By replacing a vanilla 2D attention\nwith the LAPS, we could adapt a static transformer into a video one, with zero\nextra parameters and neglectable computation overhead ($\\sim$2.6\\%).\nExperiments on the standard Kinetics-400 benchmark demonstrate that our LAPS\ntransformer could achieve competitive performances in terms of accuracy, FLOPs,\nand Params among CNN and transformer SOTAs. We open-source our project in\n\\sloppy\n\\href{https://github.com/VideoNetworks/LAPS-transformer}{\\textit{\\color{magenta}{https://github.com/VideoNetworks/LAPS-transformer}}} .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransGrasp: Grasp Pose Estimation of a Category of Objects by Transferring Grasps from Only One Labeled Instance. (arXiv:2207.07861v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2207.07861","description":"<p>Grasp pose estimation is an important issue for robots to interact with the\nreal world. However, most of existing methods require exact 3D object models\navailable beforehand or a large amount of grasp annotations for training. To\navoid these problems, we propose TransGrasp, a category-level grasp pose\nestimation method that predicts grasp poses of a category of objects by\nlabeling only one object instance. Specifically, we perform grasp pose transfer\nacross a category of objects based on their shape correspondences and propose a\ngrasp pose refinement module to further fine-tune grasp pose of grippers so as\nto ensure successful grasps. Experiments demonstrate the effectiveness of our\nmethod on achieving high-quality grasps with the transferred grasp poses. Our\ncode is available at https://github.com/yanjh97/TransGrasp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hongtao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wanli Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDM:Visual Explanations for Neural Networks via Multiple Dynamic Mask. (arXiv:2207.08046v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08046","description":"<p>The Class Activation Maps(CAM) lookup of a neural network can tell us what\nregions the neural network is focusing on when making a decision.We propose an\nalgorithm Multiple Dynamic Mask (MDM), which is a general saliency graph query\nmethod with interpretability of inference process. The algorithm is based on an\nassumption: when a picture is input into a trained neural network, only the\nactivation features related to classification will affect the classification\nresults of the neural network, and the features unrelated to classification\nwill hardly affect the classification results of the network. MDM: A\nlearning-based end-to-end algorithm for finding regions of interest for neural\nnetwork classification.It has the following advantages: 1. It has the\ninterpretability of the reasoning process, and the reasoning process conforms\nto human cognition. 2. It is universal, it can be used for any neural network\nand does not depend on the internal structure of the neural network. 3. The\nsearch performance is better. The algorithm is based on learning and has the\nability to adapt to different data and networks. The performance is better than\nthe method proposed in the previous paper. For the MDM saliency map search\nalgorithm, we experimentally compared ResNet and DenseNet as the trained neural\nnetwork. The recent advanced saliency map search method and the results of MDM\non the performance indicators of each search effect item, the performance of\nMDM has reached the state of the art. We applied the MDM method to the\ninterpretable neural network ProtoPNet and XProtoNet, which improved the\nmodel's interpretability prototype search performance. And we visualize the\neffect of convolutional neural architecture and Transformer architecture in\nsaliency map search, illustrating the interpretability and generality of MDM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yitao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Longzhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lianghua He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latency-Aware Collaborative Perception. (arXiv:2207.08560v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08560","description":"<p>Collaborative perception has recently shown great potential to improve\nperception capabilities over single-agent perception. Existing collaborative\nperception methods usually consider an ideal communication environment.\nHowever, in practice, the communication system inevitably suffers from latency\nissues, causing potential performance degradation and high risks in\nsafety-critical applications, such as autonomous driving. To mitigate the\neffect caused by the inevitable latency, from a machine learning perspective,\nwe present the first latency-aware collaborative perception system, which\nactively adapts asynchronous perceptual features from multiple agents to the\nsame time stamp, promoting the robustness and effectiveness of collaboration.\nTo achieve such a feature-level synchronization, we propose a novel latency\ncompensation module, called SyncNet, which leverages feature-attention\nsymbiotic estimation and time modulation techniques. Experiments results show\nthat the proposed latency aware collaborative perception system with SyncNet\ncan outperforms the state-of-the-art collaborative perception method by 15.6%\nin the communication latency scenario and keep collaborative perception being\nsuperior to single agent perception under severe latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zixing Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shunli Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Partition Implicit with Surface Codes for 3D Representation. (arXiv:2207.08631v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.08631","description":"<p>Deep implicit functions have shown remarkable shape modeling ability in\nvarious 3D computer vision tasks. One drawback is that it is hard for them to\nrepresent a 3D shape as multiple parts. Current solutions learn various\nprimitives and blend the primitives directly in the spatial space, which still\nstruggle to approximate the 3D shape accurately. To resolve this problem, we\nintroduce a novel implicit representation to represent a single 3D shape as a\nset of parts in the latent space, towards both highly accurate and plausibly\ninterpretable shape modeling. Our insight here is that both the part learning\nand the part blending can be conducted much easier in the latent space than in\nthe spatial space. We name our method Latent Partition Implicit (LPI), because\nof its ability of casting the global shape modeling into multiple local part\nmodeling, which partitions the global shape unity. LPI represents a shape as\nSigned Distance Functions (SDFs) using surface codes. Each surface code is a\nlatent code representing a part whose center is on the surface, which enables\nus to flexibly employ intrinsic attributes of shapes or additional surface\nproperties. Eventually, LPI can reconstruct both the shape and the parts on the\nshape, both of which are plausible meshes. LPI is a multi-level representation,\nwhich can partition a shape into different numbers of parts after training. LPI\ncan be learned without ground truth signed distances, point normals or any\nsupervision for part partition. LPI outperforms the latest methods under the\nwidely used benchmarks in terms of reconstruction accuracy and modeling\ninterpretability. Our code, data and models are available at\nhttps://github.com/chenchao15/LPI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Recognition based on Multi-Task Learning Framework in the ABAW4 Challenge. (arXiv:2207.09373v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.09373","description":"<p>This paper presents our submission to the Multi-Task Learning (MTL) Challenge\nof the 4th Affective Behavior Analysis in-the-wild (ABAW) competition. Based on\nvisual feature representations, we utilize three types of temporal encoder to\ncapture the temporal context information in the video, including the\ntransformer based encoder, LSTM based encoder and GRU based encoder. With the\ntemporal context-aware representations, we employ multi-task framework to\npredict the valence, arousal, expression and AU values of the images. In\naddition, smoothing processing is applied to refine the initial valence and\narousal predictions, and a model ensemble strategy is used to combine multiple\nresults from different model setups. Our system achieves the performance of\n$1.742$ on MTL Challenge validation dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tenggan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chuanhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Liyu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fengyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single Frame Atmospheric Turbulence Mitigation: A Benchmark Study and A New Physics-Inspired Transformer Model. (arXiv:2207.10040v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.10040","description":"<p>Image restoration algorithms for atmospheric turbulence are known to be much\nmore challenging to design than traditional ones such as blur or noise because\nthe distortion caused by the turbulence is an entanglement of spatially varying\nblur, geometric distortion, and sensor noise. Existing CNN-based restoration\nmethods built upon convolutional kernels with static weights are insufficient\nto handle the spatially dynamical atmospheric turbulence effect. To address\nthis problem, in this paper, we propose a physics-inspired transformer model\nfor imaging through atmospheric turbulence. The proposed network utilizes the\npower of transformer blocks to jointly extract a dynamical turbulence\ndistortion map and restore a turbulence-free image. In addition, recognizing\nthe lack of a comprehensive dataset, we collect and present two new real-world\nturbulence datasets that allow for evaluation with both classical objective\nmetrics (e.g., PSNR and SSIM) and a new task-driven metric using text\nrecognition accuracy. Both real testing sets and all related code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiyuan Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ajay Jaiswal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SplitMixer: Fat Trimmed From MLP-like Models. (arXiv:2207.10255v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10255","description":"<p>We present SplitMixer, a simple and lightweight isotropic MLP-like\narchitecture, for visual recognition. It contains two types of interleaving\nconvolutional operations to mix information across spatial locations (spatial\nmixing) and channels (channel mixing). The first one includes sequentially\napplying two depthwise 1D kernels, instead of a 2D kernel, to mix spatial\ninformation. The second one is splitting the channels into overlapping or\nnon-overlapping segments, with or without shared parameters, and applying our\nproposed channel mixing approaches or 3D convolution to mix channel\ninformation. Depending on design choices, a number of SplitMixer variants can\nbe constructed to balance accuracy, the number of parameters, and speed. We\nshow, both theoretically and experimentally, that SplitMixer performs on par\nwith the state-of-the-art MLP-like models while having a significantly lower\nnumber of parameters and FLOPS. For example, without strong data augmentation\nand optimization, SplitMixer achieves around 94% accuracy on CIFAR-10 with only\n0.28M parameters, while ConvMixer achieves the same accuracy with about 0.6M\nparameters. The well-known MLP-Mixer achieves 85.45% with 17.1M parameters. On\nCIFAR-100 dataset, SplitMixer achieves around 73% accuracy, on par with\nConvMixer, but with about 52% fewer parameters and FLOPS. We hope that our\nresults spark further research towards finding more efficient vision\narchitectures and facilitate the development of MLP-like models. Code is\navailable at https://github.com/aliborji/splitmixer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sikun Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MeshLoc: Mesh-Based Visual Localization. (arXiv:2207.10762v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10762","description":"<p>Visual localization, i.e., the problem of camera pose estimation, is a\ncentral component of applications such as autonomous robots and augmented\nreality systems. A dominant approach in the literature, shown to scale to large\nscenes and to handle complex illumination and seasonal changes, is based on\nlocal features extracted from images. The scene representation is a sparse\nStructure-from-Motion point cloud that is tied to a specific local feature.\nSwitching to another feature type requires an expensive feature matching step\nbetween the database images used to construct the point cloud. In this work, we\nthus explore a more flexible alternative based on dense 3D meshes that does not\nrequire features matching between database images to build the scene\nrepresentation. We show that this approach can achieve state-of-the-art\nresults. We further show that surprisingly competitive results can be obtained\nwhen extracting features on renderings of these meshes, without any neural\nrendering stage, and even when rendering raw scene geometry without color or\ntexture. Our results show that dense 3D model-based representations are a\npromising alternative to existing representations and point to interesting and\nchallenging directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Panek_V/0/1/0/all/0/1\">Vojtech Panek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kukelova_Z/0/1/0/all/0/1\">Zuzana Kukelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_T/0/1/0/all/0/1\">Torsten Sattler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Random Occlusion and Multi-Layer Projection for Deep Multi-Camera Pedestrian Localization. (arXiv:2207.10895v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10895","description":"<p>Although deep-learning based methods for monocular pedestrian detection have\nmade great progress, they are still vulnerable to heavy occlusions. Using\nmulti-view information fusion is a potential solution but has limited\napplications, due to the lack of annotated training samples in existing\nmulti-view datasets, which increases the risk of overfitting. To address this\nproblem, a data augmentation method is proposed to randomly generate 3D\ncylinder occlusions, on the ground plane, which are of the average size of\npedestrians and projected to multiple views, to relieve the impact of\noverfitting in the training. Moreover, the feature map of each view is\nprojected to multiple parallel planes at different heights, by using\nhomographies, which allows the CNNs to fully utilize the features across the\nheight of each pedestrian to infer the locations of pedestrians on the ground\nplane. The proposed 3DROM method has a greatly improved performance in\ncomparison with the state-of-the-art deep-learning based methods for multi-view\npedestrian detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_R/0/1/0/all/0/1\">Rui Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuyao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_J/0/1/0/all/0/1\">Jeremy S. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-temporal speckle reduction with self-supervised deep neural networks. (arXiv:2207.11095v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.11095","description":"<p>Speckle filtering is generally a prerequisite to the analysis of synthetic\naperture radar (SAR) images. Tremendous progress has been achieved in the\ndomain of single-image despeckling. Latest techniques rely on deep neural\nnetworks to restore the various structures and textures peculiar to SAR images.\nThe availability of time series of SAR images offers the possibility of\nimproving speckle filtering by combining different speckle realizations over\nthe same area. The supervised training of deep neural networks requires\nground-truth speckle-free images. Such images can only be obtained indirectly\nthrough some form of averaging, by spatial or temporal integration, and are\nimperfect. Given the potential of very high quality restoration reachable by\nmulti-temporal speckle filtering, the limitations of ground-truth images need\nto be circumvented. We extend a recent self-supervised training strategy for\nsingle-look complex SAR images, called MERLIN, to the case of multi-temporal\nfiltering. This requires modeling the sources of statistical dependencies in\nthe spatial and temporal dimensions as well as between the real and imaginary\ncomponents of the complex amplitudes. Quantitative analysis on datasets with\nsimulated speckle indicates a clear improvement of speckle reduction when\nadditional SAR images are included. Our method is then applied to stacks of\nTerraSAR-X images and shown to outperform competing multi-temporal speckle\nfiltering approaches. The code of the trained models is made freely available\non the Gitlab of the IMAGES team of the LTCI Lab, T\\'el\\'ecom Paris Institut\nPolytechnique de Paris\n(https://gitlab.telecom-paris.fr/ring/multi-temporal-merlin/).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meraoumia_I/0/1/0/all/0/1\">In&#xe8;s Meraoumia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalsasso_E/0/1/0/all/0/1\">Emanuele Dalsasso</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denis_L/0/1/0/all/0/1\">Lo&#xef;c Denis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abergel_R/0/1/0/all/0/1\">R&#xe9;my Abergel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tupin_F/0/1/0/all/0/1\">Florence Tupin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEFANN: Scene Text Editor using Font Adaptive Neural Network. (arXiv:1903.01192v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/1903.01192","description":"<p>Textual information in a captured scene plays an important role in scene\ninterpretation and decision making. Though there exist methods that can\nsuccessfully detect and interpret complex text regions present in a scene, to\nthe best of our knowledge, there is no significant prior work that aims to\nmodify the textual information in an image. The ability to edit text directly\non images has several advantages including error correction, text restoration\nand image reusability. In this paper, we propose a method to modify text in an\nimage at character-level. We approach the problem in two stages. At first, the\nunobserved character (target) is generated from an observed character (source)\nbeing modified. We propose two different neural network architectures - (a)\nFANnet to achieve structural consistency with source font and (b) Colornet to\npreserve source color. Next, we replace the source character with the generated\ncharacter maintaining both geometric and visual consistency with neighboring\ncharacters. Our method works as a unified platform for modifying text in\nimages. We present the effectiveness of our method on COCO-Text and ICDAR\ndatasets both qualitatively and quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Prasun Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Saumik Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhankar Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_U/0/1/0/all/0/1\">Umapada Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}