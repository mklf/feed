{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"How can NLP Help Revitalize Endangered Languages? A Case Study and Roadmap for the Cherokee Language. (arXiv:2204.11909v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11909","description":"<p>More than 43% of the languages spoken in the world are endangered, and\nlanguage loss currently occurs at an accelerated rate because of globalization\nand neocolonialism. Saving and revitalizing endangered languages has become\nvery important for maintaining the cultural diversity on our planet. In this\nwork, we focus on discussing how NLP can help revitalize endangered languages.\nWe first suggest three principles that may help NLP practitioners to foster\nmutual understanding and collaboration with language communities, and we\ndiscuss three ways in which NLP can potentially assist in language education.\nWe then take Cherokee, a severely-endangered Native American language, as a\ncase study. After reviewing the language's history, linguistic features, and\nexisting resources, we (in collaboration with Cherokee community members)\narrive at a few meaningful ways NLP practitioners can collaborate with\ncommunity partners. We suggest two approaches to enrich the Cherokee language's\nresources with machine-in-the-loop processing, and discuss several NLP tools\nthat people from the Cherokee community have shown interest in. We hope that\nour work serves not only to inform the NLP community about Cherokee, but also\nto provide inspiration for future work on endangered languages in general. Our\ncode and data will be open-sourced at\nhttps://github.com/ZhangShiyue/RevitalizeCherokee\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frey_B/0/1/0/all/0/1\">Ben Frey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks. (arXiv:2204.11922v1 [cs.CL])","link":"http://arxiv.org/abs/2204.11922","description":"<p>Pre-trained language models have shown excellent results in few-shot learning\nscenarios using in-context learning. Although it is impressive, the size of\nlanguage models can be prohibitive to make them usable in on-device\napplications, such as sensors or smartphones. With smaller language models,\ntask-specific data annotation is needed to fine-tune the language model for a\nspecific purpose. However, data annotation can have a substantial financial and\ntime burden for small research groups, startups, and even companies. In this\npaper, we analyze different prompt-based fine-tuning techniques to improve\nresults on both language and multimodal causal transformer models. To evaluate\nour results, we use a dataset focusing on visual commonsense reasoning in time.\nOur results show that by simple model-agnostic prompt-based fine-tuning,\ncomparable results can be reached by only using 35%-40% of the fine-tuning\ntraining dataset. The proposed approaches result in significant time and\nfinancial savings. As the proposed methods make minimal architectural\nassumptions, other researchers can use the results in their transformer models\nwith minimal adaptations. We plan to release the source code freely to make it\neasier for the community to use and contribute to our work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_N/0/1/0/all/0/1\">Navid Rezaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reformat_M/0/1/0/all/0/1\">Marek Z. Reformat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"C3: Continued Pretraining with Contrastive Weak Supervision for Cross Language Ad-Hoc Retrieval. (arXiv:2204.11989v1 [cs.IR])","link":"http://arxiv.org/abs/2204.11989","description":"<p>Pretrained language models have improved effectiveness on numerous tasks,\nincluding ad-hoc retrieval. Recent work has shown that continuing to pretrain a\nlanguage model with auxiliary objectives before fine-tuning on the retrieval\ntask can further improve retrieval effectiveness. Unlike monolingual retrieval,\ndesigning an appropriate auxiliary task for cross-language mappings is\nchallenging. To address this challenge, we use comparable Wikipedia articles in\ndifferent languages to further pretrain off-the-shelf multilingual pretrained\nmodels before fine-tuning on the retrieval task. We show that our approach\nyields improvements in retrieval effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eugene Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandradevan_R/0/1/0/all/0/1\">Ramraj Chandradevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iglesias_Flores_R/0/1/0/all/0/1\">Rebecca Iglesias-Flores</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1\">Douglas W. Oard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Personification: Estimating the Personality of Language Models. (arXiv:2204.12000v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12000","description":"<p>Technology for open-ended language generation, a key application of\nartificial intelligence, has advanced to a great extent in recent years.\nLarge-scale language models, which are trained on large corpora of text, are\nbeing used in a wide range of applications everywhere, from virtual assistants\nto conversational bots. While these language models output fluent text,\nexisting research shows that these models can and do capture human biases. Many\nof these biases, especially those that could potentially cause harm, are being\nwell investigated. On the other hand, studies that infer and change personality\ntraits inherited by these models have been scarce or non-existent. In this\nwork, we explore the personality traits of several large-scale language models\ndesigned for open-ended text generation and the datasets used for training\nthem. Our work builds on the popular Big Five factors and develops robust\nmethods that quantify the personality traits of these models and their\nunderlying datasets. In particular, we trigger the models with a questionnaire\ndesigned for personality assessment and subsequently classify the text\nresponses into quantifiable traits using a Zero-shot classifier. Our\nclassification sheds light on an important anthropomorphic element found in\nsuch AI models and can help stakeholders decide how they should be applied and\nhow society could perceive them. We augment our analysis by studying approaches\nthat can alter these personalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karra_S/0/1/0/all/0/1\">Saketh Reddy Karra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1\">Son Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulabandhula_T/0/1/0/all/0/1\">Theja Tulabandhula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reprint: a randomized extrapolation based on principal components for data augmentation. (arXiv:2204.12024v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12024","description":"<p>Data scarcity and data imbalance have attracted a lot of attention in many\nfields. Data augmentation, explored as an effective approach to tackle them,\ncan improve the robustness and efficiency of classification models by\ngenerating new samples. This paper presents REPRINT, a simple and effective\nhidden-space data augmentation method for imbalanced data classification. Given\nhidden-space representations of samples in each class, REPRINT extrapolates, in\na randomized fashion, augmented examples for target class by using subspaces\nspanned by principal components to summarize distribution structure of both\nsource and target class. Consequently, the examples generated would diversify\nthe target while maintaining the original geometry of target distribution.\nBesides, this method involves a label refinement component which allows to\nsynthesize new soft labels for augmented examples. Compared with different NLP\ndata augmentation approaches under a range of data imbalanced scenarios on four\ntext classification benchmark, REPRINT shows prominent improvements. Moreover,\nthrough comprehensive ablation studies, we show that label refinement is better\nthan label-preserving for augmented examples, and that our method suggests\nstable and consistent improvements in terms of suitable choices of principal\ncomponents. Moreover, REPRINT is appealing for its easy-to-use since it\ncontains only one hyperparameter determining the dimension of subspace and\nrequires low computational resource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiale Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guedj_B/0/1/0/all/0/1\">Benjamin Guedj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Le Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boundary Smoothing for Named Entity Recognition. (arXiv:2204.12031v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12031","description":"<p>Neural named entity recognition (NER) models may easily encounter the\nover-confidence issue, which degrades the performance and calibration. Inspired\nby label smoothing and driven by the ambiguity of boundary annotation in NER\nengineering, we propose boundary smoothing as a regularization technique for\nspan-based neural NER models. It re-assigns entity probabilities from annotated\nspans to the surrounding ones. Built on a simple but strong baseline, our model\nachieves results better than or competitive with previous state-of-the-art\nsystems on eight well-known NER benchmarks. Further empirical analysis suggests\nthat boundary smoothing effectively mitigates over-confidence, improves model\ncalibration, and brings flatter neural minima and more smoothed loss\nlandscapes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1\">Enwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining Chinese BERT for Detecting Word Insertion and Deletion Errors. (arXiv:2204.12052v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12052","description":"<p>Chinese BERT models achieve remarkable progress in dealing with grammatical\nerrors of word substitution. However, they fail to handle word insertion and\ndeletion because BERT assumes the existence of a word at each position. To\naddress this, we present a simple and effective Chinese pretrained model. The\nbasic idea is to enable the model to determine whether a word exists at a\nparticular position. We achieve this by introducing a special token\n\\texttt{[null]}, the prediction of which stands for the non-existence of a\nword. In the training stage, we design pretraining tasks such that the model\nlearns to predict \\texttt{[null]} and real words jointly given the surrounding\ncontext. In the inference stage, the model readily detects whether a word\nshould be inserted or deleted with the standard masked language modeling\nfunction. We further create an evaluation dataset to foster research on word\ninsertion and deletion. It includes human-annotated corrections for 7,726\nerroneous sentences. Results show that existing Chinese BERT performs poorly on\ndetecting insertion and deletion errors. Our approach significantly improves\nthe F1 scores from 24.1\\% to 78.1\\% for word insertion and from 26.5\\% to\n68.5\\% for word deletion, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_E/0/1/0/all/0/1\">Enbo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_L/0/1/0/all/0/1\">Li Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PLOD: An Abbreviation Detection Dataset for Scientific Documents. (arXiv:2204.12061v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12061","description":"<p>The detection and extraction of abbreviations from unstructured texts can\nhelp to improve the performance of Natural Language Processing tasks, such as\nmachine translation and information retrieval. However, in terms of publicly\navailable datasets, there is not enough data for training\ndeep-neural-networks-based models to the point of generalising well over data.\nThis paper presents PLOD, a large-scale dataset for abbreviation detection and\nextraction that contains 160k+ segments automatically annotated with\nabbreviations and their long forms. We performed manual validation over a set\nof instances and a complete automatic validation for this dataset. We then used\nit to generate several baseline models for detecting abbreviations and long\nforms. The best models achieved an F1-score of 0.92 for abbreviations and 0.89\nfor detecting their corresponding long forms. We release this dataset along\nwith our code and all the models publicly in\nhttps://github.com/surrey-nlp/AbbreviationDetRepo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zilio_L/0/1/0/all/0/1\">Leonardo Zilio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadany_H/0/1/0/all/0/1\">Hadeel Saadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Prashant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1\">Constantin Orasan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suggesting Relevant Questions for a Query Using Statistical Natural Language Processing Technique. (arXiv:2204.12069v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12069","description":"<p>Suggesting similar questions for a user query has many applications ranging\nfrom reducing search time of users on e-commerce websites, training of\nemployees in companies to holistic learning for students. The use of Natural\nLanguage Processing techniques for suggesting similar questions is prevalent\nover the existing architecture. Mainly two approaches are studied for finding\ntext similarity namely syntactic and semantic, however each has its draw-backs\nand fail to provide the desired outcome. In this article, a self-learning\ncombined approach is proposed for determining textual similarity that\nintroduces a robust weighted syntactic and semantic similarity index for\ndetermining similar questions from a predetermined database, this approach\nlearns the optimal combination of the mentioned approaches for a database under\nconsideration. Comprehensive analysis has been carried out to justify the\nefficiency and efficacy of the proposed approach over the existing literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayak_S/0/1/0/all/0/1\">Shriniwas Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanetkar_A/0/1/0/all/0/1\">Anuj Kanetkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirudkar_H/0/1/0/all/0/1\">Hrushabh Hirudkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghotkar_A/0/1/0/all/0/1\">Archana Ghotkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonawane_S/0/1/0/all/0/1\">Sheetal Sonawane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litake_O/0/1/0/all/0/1\">Onkar Litake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Symlink: A New Dataset for Scientific Symbol-Description Linking. (arXiv:2204.12070v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12070","description":"<p>Mathematical symbols and descriptions appear in various forms across document\nsection boundaries without explicit markup. In this paper, we present a new\nlarge-scale dataset that emphasizes extracting symbols and descriptions in\nscientific documents. Symlink annotates scientific papers of 5 different\ndomains (i.e., computer science, biology, physics, mathematics, and economics).\nOur experiments on Symlink demonstrate the challenges of the symbol-description\nlinking task for existing models and call for further research effort in this\narea. We will publicly release Symlink to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_V/0/1/0/all/0/1\">Viet Dac Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veyseh_A/0/1/0/all/0/1\">Amir Pouran Ben Veyseh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Approach to Predicting News -- A Precise Multi-LSTM Network With BERT. (arXiv:2204.12093v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12093","description":"<p>Varieties of Democracy (V-Dem) is a new approach to conceptualizing and\nmeasuring democracy and politics. It has information for 200 countries and is\none of the biggest databases for political science. According to the V-Dem\nannual democracy report 2019, Taiwan is one of the two countries that got\ndisseminated false information from foreign governments the most. It also shows\nthat the \"made-up news\" has caused a great deal of confusion in Taiwanese\nsociety and has serious impacts on global stability. Although there are several\napplications helping distinguish the false information, we found out that the\npre-processing of categorizing the news is still done by human labor. However,\nhuman labor may cause mistakes and cannot work for a long time. The growing\ndemands for automatic machines in the near decades show that while the machine\ncan do as good as humans or even better, using machines can reduce humans'\nburden and cut down costs. Therefore, in this work, we build a predictive model\nto classify the category of news. The corpora we used contains 28358 news and\n200 news scraped from the online newspaper Liberty Times Net (LTN) website and\nincludes 8 categories: Technology, Entertainment, Fashion, Politics, Sports,\nInternational, Finance, and Health. At first, we use Bidirectional Encoder\nRepresentations from Transformers (BERT) for word embeddings which transform\neach Chinese character into a (1,768) vector. Then, we use a Long Short-Term\nMemory (LSTM) layer to transform word embeddings into sentence embeddings and\nadd another LSTM layer to transform them into document embeddings. Each\ndocument embedding is an input for the final predicting model, which contains\ntwo Dense layers and one Activation layer. And each document embedding is\ntransformed into 1 vector with 8 real numbers, then the highest one will\ncorrespond to the 8 news categories with up to 99% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chia-Lin Chen</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Pei-Yu Huang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi-Ting Huang</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chun Lin</a> (3) ((1) Computer Science and Engineering, National Sun Yat-sen University, Kaohsiung, Taiwan, (2) Management and Digital Innovation, University of London, Singapore, (3) Institute of Information Science, Academia Sinica, Taipei, Taiwan)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Function-words Enhanced Attention Networks for Few-Shot Inverse Relation Classification. (arXiv:2204.12111v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12111","description":"<p>The relation classification is to identify semantic relations between two\nentities in a given text. While existing models perform well for classifying\ninverse relations with large datasets, their performance is significantly\nreduced for few-shot learning. In this paper, we propose a function words\nadaptively enhanced attention framework (FAEA) for few-shot inverse relation\nclassification, in which a hybrid attention model is designed to attend\nclass-related function words based on meta-learning. As the involvement of\nfunction words brings in significant intra-class redundancy, an adaptive\nmessage passing mechanism is introduced to capture and transfer inter-class\ndifferences.We mathematically analyze the negative impact of function words\nfrom dot-product measurement, which explains why message passing mechanism\neffectively reduces the impact. Our experimental results show that FAEA\noutperforms strong baselines, especially the inverse relation accuracy is\nimproved by 14.33% under 1-shot setting in FewRel1.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_C/0/1/0/all/0/1\">Chunliu Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shaojuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhiyong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kewen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Contrastive Alignment Method For Multi-Domain Text Classification. (arXiv:2204.12125v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12125","description":"<p>Multi-domain text classification can automatically classify texts in various\nscenarios. Due to the diversity of human languages, texts with the same label\nin different domains may differ greatly, which brings challenges to the\nmulti-domain text classification. Current advanced methods use the\nprivate-shared paradigm, capturing domain-shared features by a shared encoder,\nand training a private encoder for each domain to extract domain-specific\nfeatures. However, in realistic scenarios, these methods suffer from\ninefficiency as new domains are constantly emerging. In this paper, we propose\na robust contrastive alignment method to align text classification features of\nvarious domains in the same feature space by supervised contrastive learning.\nBy this means, we only need two universal feature extractors to achieve\nmulti-domain text classification. Extensive experimental results show that our\nmethod performs on par with or sometimes better than the state-of-the-art\nmethod, which uses the complex multi-classifier in a private-shared framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_H/0/1/0/all/0/1\">Hao Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Guanting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jinzheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chunyun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models. (arXiv:2204.12130v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12130","description":"<p>The opaque nature and unexplained behavior of transformer-based language\nmodels (LMs) have spurred a wide interest in interpreting their predictions.\nHowever, current interpretation methods mostly focus on probing models from\noutside, executing behavioral tests, and analyzing salience input features,\nwhile the internal prediction construction process is largely not understood.\nIn this work, we introduce LM-Debugger, an interactive debugger tool for\ntransformer-based LMs, which provides a fine-grained interpretation of the\nmodel's internal prediction process, as well as a powerful framework for\nintervening in LM behavior. For its backbone, LM-Debugger relies on a recent\nmethod that interprets the inner token representations and their updates by the\nfeed-forward layers in the vocabulary space. We demonstrate the utility of\nLM-Debugger for single-prediction debugging, by inspecting the internal\ndisambiguation process done by GPT2. Moreover, we show how easily LM-Debugger\nallows to shift model behavior in a direction of the user's choice, by\nidentifying a few vectors in the network and inducing effective interventions\nto the prediction process. We release LM-Debugger as an open-source tool and a\ndemo over GPT2 models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1\">Mor Geva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_G/0/1/0/all/0/1\">Guy Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadde_S/0/1/0/all/0/1\">Shoval Sadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shlain_M/0/1/0/all/0/1\">Micah Shlain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamir_B/0/1/0/all/0/1\">Bar Tamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-determinsitic algebraic rewriting as adjunction. (arXiv:2204.12133v1 [math.LO])","link":"http://arxiv.org/abs/2204.12133","description":"<p>We develop a general model theoretic semantics to rewriting beyond the usual\nconfluence and termination assumptions. This is based on preordered algebra\nwhich is a model theory that extends many sorted algebra. In this framework we\ncharacterise rewriting in arbitrary algebras rather than term algebras (called\nalgebraic rewriting) as a persistent adjunction and use this result, on the one\nhand for proving the soundness and the completeness of an abstract\ncomputational model of rewriting that underlies the non-deterministic\nprogramming with Maude and CafeOBJ, and on the other hand for developing a\ncompositionality result for algebraic rewriting in the context of the\npushout-based modularisation technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Diaconescu_R/0/1/0/all/0/1\">R&#x103;zvan Diaconescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoVERT: A Corpus of Fact-checked Biomedical COVID-19 Tweets. (arXiv:2204.12164v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12164","description":"<p>Over the course of the COVID-19 pandemic, large volumes of biomedical\ninformation concerning this new disease have been published on social media.\nSome of this information can pose a real danger to people's health,\nparticularly when false information is shared, for instance recommendations on\nhow to treat diseases without professional medical advice. Therefore, automatic\nfact-checking resources and systems developed specifically for the medical\ndomain are crucial. While existing fact-checking resources cover\nCOVID-19-related information in news or quantify the amount of misinformation\nin tweets, there is no dataset providing fact-checked COVID-19-related Twitter\nposts with detailed annotations for biomedical entities, relations and relevant\nevidence. We contribute CoVERT, a fact-checked corpus of tweets with a focus on\nthe domain of biomedicine and COVID-19-related (mis)information. The corpus\nconsists of 300 tweets, each annotated with medical named entities and\nrelations. We employ a novel crowdsourcing methodology to annotate all tweets\nwith fact-checking labels and supporting evidence, which crowdworkers search\nfor online. This methodology results in moderate inter-annotator agreement.\nFurthermore, we use the retrieved evidence extracts as part of a fact-checking\npipeline, finding that the real-world evidence is more useful than the\nknowledge indirectly available in pretrained language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohr_I/0/1/0/all/0/1\">Isabelle Mohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wuhrl_A/0/1/0/all/0/1\">Amelie W&#xfc;hrl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When do Contrastive Word Alignments Improve Many-to-many Neural Machine Translation?. (arXiv:2204.12165v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12165","description":"<p>Word alignment has proven to benefit many-to-many neural machine translation\n(NMT). However, high-quality ground-truth bilingual dictionaries were used for\npre-editing in previous methods, which are unavailable for most language pairs.\nMeanwhile, the contrastive objective can implicitly utilize automatically\nlearned word alignment, which has not been explored in many-to-many NMT. This\nwork proposes a word-level contrastive objective to leverage word alignments\nfor many-to-many NMT. Empirical results show that this leads to 0.8 BLEU gains\nfor several language pairs. Analyses reveal that in many-to-many NMT, the\nencoder's sentence retrieval performance highly correlates with the translation\nquality, which explains when the proposed method impacts translation. This\nmotivates future exploration for many-to-many NMT to improve the encoder's\nsentence retrieval performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_C/0/1/0/all/0/1\">Chenhui Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Machine Learning to Fuse Verbal Autopsy Narratives and Binary Features in the Analysis of Deaths from Hyperglycaemia. (arXiv:2204.12169v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12169","description":"<p>Lower-and-middle income countries are faced with challenges arising from a\nlack of data on cause of death (COD), which can limit decisions on population\nhealth and disease management. A verbal autopsy(VA) can provide information\nabout a COD in areas without robust death registration systems. A VA consists\nof structured data, combining numeric and binary features, and unstructured\ndata as part of an open-ended narrative text. This study assesses the\nperformance of various machine learning approaches when analyzing both the\nstructured and unstructured components of the VA report. The algorithms were\ntrained and tested via cross-validation in the three settings of binary\nfeatures, text features and a combination of binary and text features derived\nfrom VA reports from rural South Africa. The results obtained indicate\nnarrative text features contain valuable information for determining COD and\nthat a combination of binary and text features improves the automated COD\nclassification task.\n</p>\n<p>Keywords: Diabetes Mellitus, Verbal Autopsy, Cause of Death, Machine\nLearning, Natural Language Processing\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manaka_T/0/1/0/all/0/1\">Thokozile Manaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zyl_T/0/1/0/all/0/1\">Terence Van Zyl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_A/0/1/0/all/0/1\">Alisha N Wade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kar_D/0/1/0/all/0/1\">Deepak Kar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillNet-NLG: General-Purpose Natural Language Generation with a Sparsely Activated Approach. (arXiv:2204.12184v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12184","description":"<p>We present SkillNet-NLG, a sparsely activated approach that handles many\nnatural language generation tasks with one model. Different from traditional\ndense models that always activate all the parameters, SkillNet-NLG selectively\nactivates relevant parts of the parameters to accomplish a task, where the\nrelevance is controlled by a set of predefined skills. The strength of such\nmodel design is that it provides an opportunity to precisely adapt relevant\nskills to learn new tasks effectively. We evaluate on Chinese natural language\ngeneration tasks. Results show that, with only one model file, SkillNet-NLG\noutperforms previous best performance methods on four of five tasks.\nSkillNet-NLG performs better than two multi-task learning baselines (a dense\nmodel and a Mixture-of-Expert model) and achieves comparable performance to\ntask-specific models. Lastly, SkillNet-NLG surpasses baseline systems when\nbeing adapted to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Junwei Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faster and Better Grammar-based Text-to-SQL Parsing via Clause-level Parallel Decoding and Alignment Loss. (arXiv:2204.12186v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12186","description":"<p>Grammar-based parsers have achieved high performance in the cross-domain\ntext-to-SQL parsing task, but suffer from low decoding efficiency due to the\nmuch larger number of actions for grammar selection than that of tokens in SQL\nqueries. Meanwhile, how to better align SQL clauses and question segments has\nbeen a key challenge for parsing performance. Therefore, this paper proposes\nclause-level parallel decoding and alignment loss to enhance two\nhigh-performance grammar-based parsers, i.e., RATSQL and LGESQL. Experimental\nresults of two parsers show that our method obtains consistent improvements\nboth in accuracy and decoding speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmpHi: Generating Empathetic Responses with Human-like Intents. (arXiv:2204.12191v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12191","description":"<p>In empathetic conversations, humans express their empathy to others with\nempathetic intents. However, most existing empathetic conversational methods\nsuffer from a lack of empathetic intents, which leads to monotonous empathy. To\naddress the bias of the empathetic intents distribution between empathetic\ndialogue models and humans, we propose a novel model to generate empathetic\nresponses with human-consistent empathetic intents, EmpHi for short. Precisely,\nEmpHi learns the distribution of potential empathetic intents with a discrete\nlatent variable, then combines both implicit and explicit intent representation\nto generate responses with various empathetic intents. Experiments show that\nEmpHi outperforms state-of-the-art models in terms of empathy, relevance, and\ndiversity on both automatic and human evaluation. Moreover, the case studies\ndemonstrate the high interpretability and outstanding performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mao Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flow-Adapter Architecture for Unsupervised Machine Translation. (arXiv:2204.12225v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12225","description":"<p>In this work, we propose a flow-adapter architecture for unsupervised NMT. It\nleverages normalizing flows to explicitly model the distributions of\nsentence-level latent representations, which are subsequently used in\nconjunction with the attention mechanism for the translation task. The primary\nnovelties of our model are: (a) capturing language-specific sentence\nrepresentations separately for each language using normalizing flows and (b)\nusing a simple transformation of these latent representations for translating\nfrom one language to another. This architecture allows for unsupervised\ntraining of each language independently. While there is prior work on latent\nvariables for supervised MT, to the best of our knowledge, this is the first\nwork that uses latent variables and normalizing flows for unsupervised MT. We\nobtain competitive results on several unsupervised MT benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabbar_H/0/1/0/all/0/1\">Haris Jabbar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Science Checker: Extractive-Boolean Question Answering For Scientific Fact Checking. (arXiv:2204.12263v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12263","description":"<p>With the explosive growth of scientific publications, making the synthesis of\nscientific knowledge and fact checking becomes an increasingly complex task. In\nthis paper, we propose a multi-task approach for verifying the scientific\nquestions based on a joint reasoning from facts and evidence in research\narticles. We propose an intelligent combination of (1) an automatic information\nsummarization and (2) a Boolean Question Answering which allows to generate an\nanswer to a scientific question from only extracts obtained after\nsummarization. Thus on a given topic, our proposed approach conducts structured\ncontent modeling based on paper abstracts to answer a scientific question while\nhighlighting texts from paper that discuss the topic. We based our final system\non an end-to-end Extractive Question Answering (EQA) combined with a three\noutputs classification model to perform in-depth semantic understanding of a\nquestion to illustrate the aggregation of multiple responses. With our light\nand fast proposed architecture, we achieved an average error rate of 4% and a\nF1-score of 95.6%. Our results are supported via experiments with two QA models\n(BERT, RoBERTa) over 3 Million Open Access (OA) articles in the medical and\nhealth domains on Europe PMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rakotoson_L/0/1/0/all/0/1\">Lo&#xef;c Rakotoson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Letaillieur_C/0/1/0/all/0/1\">Charles Letaillieur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massip_S/0/1/0/all/0/1\">Sylvain Massip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1\">Fr&#xe9;jus Laleye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis of Cybersecurity Content on Twitter and Reddit. (arXiv:2204.12267v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12267","description":"<p>Sentiment Analysis provides an opportunity to understand the subject(s),\nespecially in the digital age, due to an abundance of public data and effective\nalgorithms. Cybersecurity is a subject where opinions are plentiful and\ndiffering in the public domain. This descriptive research analyzed\ncybersecurity content on Twitter and Reddit to measure its sentiment, positive\nor negative, or neutral. The data from Twitter and Reddit was amassed via\ntechnology-specific APIs during a selected timeframe to create datasets, which\nwere then analyzed individually for their sentiment by VADER, an NLP (Natural\nLanguage Processing) algorithm. A random sample of cybersecurity content (ten\ntweets and posts) was also classified for sentiments by twenty human annotators\nto evaluate the performance of VADER. Cybersecurity content on Twitter was at\nleast 48% positive, and Reddit was at least 26.5% positive. The positive or\nneutral content far outweighed negative sentiments across both platforms. When\ncompared to human classification, which was considered the standard or source\nof truth, VADER produced 60% accuracy for Twitter and 70% for Reddit in\nassessing the sentiment; in other words, some agreement between algorithm and\nhuman classifiers. Overall, the goal was to explore an uninhibited research\ntopic about cybersecurity sentiment\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thapa_B/0/1/0/all/0/1\">Bipun Thapa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-dimensional representation of infant and adult vocalization acoustics. (arXiv:2204.12279v1 [eess.AS])","link":"http://arxiv.org/abs/2204.12279","description":"<p>During the first years of life, infant vocalizations change considerably, as\ninfants develop the vocalization skills that enable them to produce speech\nsounds. Characterizations based on specific acoustic features, protophone\ncategories, or phonetic transcription are able to provide a representation of\nthe sounds infants make at different ages and in different contexts but do not\nfully describe how sounds are perceived by listeners, can be inefficient to\nobtain at large scales, and are difficult to visualize in two dimensions\nwithout additional statistical processing. Machine-learning-based approaches\nprovide the opportunity to complement these characterizations with purely\ndata-driven representations of infant sounds. Here, we use spectral features\nextraction and unsupervised machine learning, specifically Uniform Manifold\nApproximation (UMAP), to obtain a novel 2-dimensional spatial representation of\ninfant and caregiver vocalizations extracted from day-long home recordings.\nUMAP yields a continuous and well-distributed space conducive to certain\nanalyses of infant vocal development. For instance, we found that the\ndispersion of infant vocalization acoustics within the 2-D space over a day\nincreased from 3 to 9 months, and then decreased from 9 to 18 months. The\nmethod also permits analysis of similarity between infant and adult\nvocalizations, which also shows changes with infant age.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pagliarini_S/0/1/0/all/0/1\">Silvia Pagliarini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schneider_S/0/1/0/all/0/1\">Sara Schneider</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kello_C/0/1/0/all/0/1\">Christopher T. Kello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Warlaumont_A/0/1/0/all/0/1\">Anne S. Warlaumont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monant Medical Misinformation Dataset: Mapping Articles to Fact-Checked Claims. (arXiv:2204.12294v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12294","description":"<p>False information has a significant negative influence on individuals as well\nas on the whole society. Especially in the current COVID-19 era, we witness an\nunprecedented growth of medical misinformation. To help tackle this problem\nwith machine learning approaches, we are publishing a feature-rich dataset of\napprox. 317k medical news articles/blogs and 3.5k fact-checked claims. It also\ncontains 573 manually and more than 51k automatically labelled mappings between\nclaims and articles. Mappings consist of claim presence, i.e., whether a claim\nis contained in a given article, and article stance towards the claim. We\nprovide several baselines for these two tasks and evaluate them on the manually\nlabelled part of the dataset. The dataset enables a number of additional tasks\nrelated to medical misinformation, such as misinformation characterisation\nstudies or studies of misinformation diffusion between sources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srba_I/0/1/0/all/0/1\">Ivan Srba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pecher_B/0/1/0/all/0/1\">Branislav Pecher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomlein_M/0/1/0/all/0/1\">Matus Tomlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moro_R/0/1/0/all/0/1\">Robert Moro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefancova_E/0/1/0/all/0/1\">Elena Stefancova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simko_J/0/1/0/all/0/1\">Jakub Simko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielikova_M/0/1/0/all/0/1\">Maria Bielikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Attention in Sequence-to-Sequence Models for Speech Recognition. (arXiv:2204.12308v1 [eess.AS])","link":"http://arxiv.org/abs/2204.12308","description":"<p>Attention mechanism in sequence-to-sequence models is designed to model the\nalignments between acoustic features and output tokens in speech recognition.\nHowever, attention weights produced by models trained end to end do not always\ncorrespond well with actual alignments, and several studies have further argued\nthat attention weights might not even correspond well with the relevance\nattribution of frames. Regardless, visual similarity between attention weights\nand alignments is widely used during training as an indicator of the models\nquality. In this paper, we treat the correspondence between attention weights\nand alignments as a learning problem by imposing a supervised attention loss.\nExperiments have shown significant improved performance, suggesting that\nlearning the alignments well during training critically determines the\nperformance of sequence-to-sequence models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Gene-Ping Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Systematicity, Compositionality and Transitivity of Deep NLP Models: a Metamorphic Testing Perspective. (arXiv:2204.12316v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12316","description":"<p>Metamorphic testing has recently been used to check the safety of neural NLP\nmodels. Its main advantage is that it does not rely on a ground truth to\ngenerate test cases. However, existing studies are mostly concerned with\nrobustness-like metamorphic relations, limiting the scope of linguistic\nproperties they can test. We propose three new classes of metamorphic\nrelations, which address the properties of systematicity, compositionality and\ntransitivity. Unlike robustness, our relations are defined over multiple source\ninputs, thus increasing the number of test cases that we can produce by a\npolynomial factor. With them, we test the internal consistency of\nstate-of-the-art NLP models, and show that they do not always behave according\nto their expected linguistic properties. Lastly, we introduce a novel graphical\nnotation that efficiently summarises the inner structure of metamorphic\nrelations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manino_E/0/1/0/all/0/1\">Edoardo Manino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_D/0/1/0/all/0/1\">Danilo Carvalho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1\">Lucas Cordeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Meta Word Embeddings by Unsupervised Weighted Concatenation of Source Embeddings. (arXiv:2204.12386v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12386","description":"<p>Given multiple source word embeddings learnt using diverse algorithms and\nlexical resources, meta word embedding learning methods attempt to learn more\naccurate and wide-coverage word embeddings.\n</p>\n<p>Prior work on meta-embedding has repeatedly discovered that simple vector\nconcatenation of the source embeddings to be a competitive baseline.\n</p>\n<p>However, it remains unclear as to why and when simple vector concatenation\ncan produce accurate meta-embeddings.\n</p>\n<p>We show that weighted concatenation can be seen as a spectrum matching\noperation between each source embedding and the meta-embedding, minimising the\npairwise inner-product loss.\n</p>\n<p>Following this theoretical analysis, we propose two \\emph{unsupervised}\nmethods to learn the optimal concatenation weights for creating meta-embeddings\nfrom a given set of source embeddings.\n</p>\n<p>Experimental results on multiple benchmark datasets show that the proposed\nweighted concatenated meta-embedding methods outperform previously proposed\nmeta-embedding learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disambiguation of morpho-syntactic features of African American English -- the case of habitual be. (arXiv:2204.12421v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12421","description":"<p>Recent research has highlighted that natural language processing (NLP)\nsystems exhibit a bias against African American speakers. The bias errors are\noften caused by poor representation of linguistic features unique to African\nAmerican English (AAE), due to the relatively low probability of occurrence of\nmany such features in training data. We present a workflow to overcome such\nbias in the case of habitual \"be\". Habitual \"be\" is isomorphic, and therefore\nambiguous, with other forms of \"be\" found in both AAE and other varieties of\nEnglish. This creates a clear challenge for bias in NLP technologies. To\novercome the scarcity, we employ a combination of rule-based filters and data\naugmentation that generate a corpus balanced between habitual and non-habitual\ninstances. With this balanced corpus, we train unbiased machine learning\nclassifiers, as demonstrated on a corpus of AAE transcribed texts, achieving\n.65 F$_1$ score disambiguating habitual \"be\".\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santiago_H/0/1/0/all/0/1\">Harrison Santiago</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_J/0/1/0/all/0/1\">Joshua Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeller_S/0/1/0/all/0/1\">Sarah Moeller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kevin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Detection Explorer: An Interactive Tool for Event Detection Exploration. (arXiv:2204.12456v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12456","description":"<p>Event Detection (ED) is an important task in natural language processing. In\nthe past few years, many datasets have been introduced for advancing ED machine\nlearning models. However, most of these datasets are under-explored because not\nmany tools are available for people to study events, trigger words, and event\nmention instances systematically and efficiently. In this paper, we present an\ninteractive and easy-to-use tool, namely ED Explorer, for ED dataset and model\nexploration. ED Explorer consists of an interactive web application, an API,\nand an NLP toolkit, which can help both domain experts and non-experts to\nbetter understand the ED task. We use ED Explorer to analyze a recent proposed\nlarge-scale ED datasets (referred to as MAVEN), and discover several underlying\nproblems, including sparsity, label bias, label imbalance, and debatable\nannotations, which provide us with directions to improve the MAVEN dataset. The\nED Explorer can be publicly accessed through <a href=\"http://edx.leafnlp.org/.\">this http URL</a> The\ndemonstration video is available here\nhttps://www.youtube.com/watch?v=6QPnxPwxg50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingale_B/0/1/0/all/0/1\">Bhagyashree Ingale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabir_H/0/1/0/all/0/1\">Hamza Shabir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1\">Tian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Ping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Hyperbolic Geometry Back to Word Embeddings. (arXiv:2204.12481v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12481","description":"<p>We choose random points in the hyperbolic disc and claim that these points\nare already word representations. However, it is yet to be uncovered which\npoint corresponds to which word of the human language of interest. This\ncorrespondence can be approximately established using a pointwise mutual\ninformation between words and recent alignment techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nurmukhamedov_S/0/1/0/all/0/1\">Sultan Nurmukhamedov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mach_T/0/1/0/all/0/1\">Thomas Mach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheverdin_A/0/1/0/all/0/1\">Arsen Sheverdin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assylbekov_Z/0/1/0/all/0/1\">Zhenisbek Assylbekov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-Scale Chinese Short-Text Conversation Dataset. (arXiv:2008.03946v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.03946","description":"<p>The advancements of neural dialogue generation models show promising results\non modeling short-text conversations. However, training such models usually\nneeds a large-scale high-quality dialogue corpus, which is hard to access. In\nthis paper, we present a large-scale cleaned Chinese conversation dataset,\nLCCC, which contains a base version (6.8million dialogues) and a large version\n(12.0 million dialogues). The quality of our dataset is ensured by a rigorous\ndata cleaning pipeline, which is built based on a set of rules and a classifier\nthat is trained on manually annotated 110K dialogue pairs. We also release\npre-training dialogue models which are trained on LCCC-base and LCCC-large\nrespectively. The cleaned dataset and the pre-training models will facilitate\nthe research of short-text conversation modeling. All the models and datasets\nare available at https://github.com/thu-coai/CDial-GPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yida Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1\">Pei Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoyan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Inheritance for Pre-trained Language Models. (arXiv:2105.13880v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.13880","description":"<p>Recent explorations of large-scale pre-trained language models (PLMs) have\nrevealed the power of PLMs with huge amounts of parameters, setting off a wave\nof training ever-larger PLMs. However, it requires tremendous computational\nresources to train a large-scale PLM, which may be practically unaffordable. In\naddition, existing large-scale PLMs are mainly trained from scratch\nindividually, ignoring that many well-trained PLMs are available. To this end,\nwe explore the question how could existing PLMs benefit training large-scale\nPLMs in future. Specifically, we introduce a pre-training framework named\n\"knowledge inheritance\" (KI) and explore how could knowledge distillation serve\nas auxiliary supervision during pre-training to efficiently learn larger PLMs.\nExperimental results demonstrate the superiority of KI in training efficiency.\nWe also conduct empirical analyses to explore the effects of teacher PLMs'\npre-training settings, including model architecture, pre-training data, etc.\nFinally, we show that KI could be applied to domain adaptation and knowledge\ntransfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jing Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation. (arXiv:2106.00903v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00903","description":"<p>Knowledge distillation (KD) is commonly used to construct synthetic data for\ntraining non-autoregressive translation (NAT) models. However, there exists a\ndiscrepancy on low-frequency words between the distilled and the original data,\nleading to more errors on predicting low-frequency words. To alleviate the\nproblem, we directly expose the raw data into NAT by leveraging pretraining. By\nanalyzing directed alignments, we found that KD makes low-frequency source\nwords aligned with targets more deterministically but fails to align sufficient\nlow-frequency words from target to source. Accordingly, we propose reverse KD\nto rejuvenate more alignments for low-frequency target words. To make the most\nof authentic and synthetic data, we combine these complementary approaches as a\nnew training strategy for further boosting NAT performance. We conduct\nexperiments on five translation benchmarks over two advanced architectures.\nResults demonstrate that the proposed approach can significantly and\nuniversally improve translation quality by reducing translation errors on\nlow-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU\npoints on the WMT14 English-German and WMT16 Romanian-English datasets,\nrespectively. Our code, data, and trained models are available at\n\\url{https://github.com/alphadl/RLFW-NAT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuebo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhaopeng Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell Me How to Survey: Literature Review Made Simple with Automatic Reading Path Generation. (arXiv:2110.06354v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06354","description":"<p>Recent years have witnessed the dramatic growth of paper volumes with plenty\nof new research papers published every day, especially in the area of computer\nscience. How to glean papers worth reading from the massive literature to do a\nquick survey or keep up with the latest advancement about a specific research\ntopic has become a challenging task. Existing academic search engines such as\nGoogle Scholar return relevant papers by individually calculating the relevance\nbetween each paper and query. However, such systems usually omit the\nprerequisite chains of a research topic and cannot form a meaningful reading\npath. In this paper, we introduce a new task named Reading Path Generation\n(RPG) which aims at automatically producing a path of papers to read for a\ngiven query. To serve as a research benchmark, we further propose SurveyBank, a\ndataset consisting of large quantities of survey papers in the field of\ncomputer science as well as their citation relationships. Each survey paper\ncontains key phrases extracted from its title and multi-level reading lists\ninferred from its references. Furthermore, we propose a\ngraph-optimization-based approach for reading path generation which takes the\nrelationship between papers into account. Extensive evaluations demonstrate\nthat our approach outperforms other baselines. A Real-time Reading Path\nGeneration System (RePaGer) has been also implemented with our designed model.\nTo the best of our knowledge, we are the first to target this important\nresearch problem. Our source code of RePaGer system and SurveyBank dataset can\nbe found on here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jiayuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zijing Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangyang Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07855","description":"<p>Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't speak too fast: The impact of data bias on self-supervised speech models. (arXiv:2110.07957v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.07957","description":"<p>Self-supervised Speech Models (S3Ms) have been proven successful in many\nspeech downstream tasks, like ASR. However, how pre-training data affects S3Ms'\ndownstream behavior remains an unexplored issue. In this paper, we study how\npre-training data affects S3Ms by pre-training models on biased datasets\ntargeting different factors of speech, including gender, content, and prosody,\nand evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB\nBenchmark. Our experiments show that S3Ms have tolerance toward gender bias.\nMoreover, we find that the content of speech has little impact on the\nperformance of S3Ms across downstream tasks, but S3Ms do show a preference\ntoward a slower speech rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Self-Rationalization with Natural Language Prompts. (arXiv:2111.08284v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08284","description":"<p>Self-rationalization models that predict task labels and generate free-text\nelaborations for their predictions could enable more intuitive interaction with\nNLP systems. These models are, however, currently trained with a large amount\nof human-written free-text explanations for each task which hinders their\nbroader usage. We propose to study a more realistic setting of\nself-rationalization using few training examples. We present FEB -- a\nstandardized collection of four existing English-language datasets and\nassociated metrics. We identify the right prompting approach by extensively\nexploring natural language prompts on FEB. Then, by using this prompt and\nscaling the model size, we demonstrate that making progress on few-shot\nself-rationalization is possible. We show there is still ample room for\nimprovement in this task: the average plausibility of generated explanations\nassessed by human annotators is at most 51% (with GPT-3), while plausibility of\nhuman explanations is 76%. We hope that FEB and our proposed approach will spur\nthe community to take on the few-shot self-rationalization challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Quality Rather than High Model Probability: Minimum Bayes Risk Decoding with Neural Metrics. (arXiv:2111.09388v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.09388","description":"<p>In Neural Machine Translation, it is typically assumed that the sentence with\nthe highest estimated probability should also be the translation with the\nhighest quality as measured by humans. In this work, we question this\nassumption and show that model estimates and translation quality only vaguely\ncorrelate. We apply Minimum Bayes Risk (MBR) decoding on unbiased samples to\noptimize diverse automated metrics of translation quality as an alternative\ninference strategy to beam search. Instead of targeting the hypotheses with the\nhighest model probability, MBR decoding extracts the hypotheses with the\nhighest estimated quality. Our experiments show that the combination of a\nneural translation model with a neural reference-based metric, BLEURT, results\nin significant improvement in human evaluations. This improvement is obtained\nwith translations different from classical beam-search output: these\ntranslations have much lower model likelihood and are less favored by surface\nmetrics like BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1\">Qijun Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Bowen Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLUE: New Benchmark Tasks for Spoken Language Understanding Evaluation on Natural Speech. (arXiv:2111.10367v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.10367","description":"<p>Progress in speech processing has been facilitated by shared datasets and\nbenchmarks. Historically these have focused on automatic speech recognition\n(ASR), speaker identification, or other lower-level tasks. Interest has been\ngrowing in higher-level spoken language understanding tasks, including using\nend-to-end models, but there are fewer annotated datasets for such tasks. At\nthe same time, recent work shows the possibility of pre-training generic\nrepresentations and then fine-tuning for several tasks using relatively little\nlabeled data. We propose to create a suite of benchmark tasks for Spoken\nLanguage Understanding Evaluation (SLUE) consisting of limited-size labeled\ntraining sets and corresponding evaluation sets. This resource would allow the\nresearch community to track progress, evaluate pre-trained representations for\nhigher-level tasks, and study open questions such as the utility of pipeline\nversus end-to-end approaches. We present the first phase of the SLUE benchmark\nsuite, consisting of named entity recognition, sentiment analysis, and ASR on\nthe corresponding datasets. We focus on naturally produced (not read or\nsynthesized) speech, and freely available datasets. We provide new\ntranscriptions and annotations on subsets of the VoxCeleb and VoxPopuli\ndatasets, evaluation metrics and results for baseline models, and an\nopen-source toolkit to reproduce the baselines and evaluate new models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shon_S/0/1/0/all/0/1\">Suwon Shon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasad_A/0/1/0/all/0/1\">Ankita Pasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brusco_P/0/1/0/all/0/1\">Pablo Brusco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livescu_K/0/1/0/all/0/1\">Karen Livescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyu J. Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surfer100: Generating Surveys From Web Resources, Wikipedia-style. (arXiv:2112.06377v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06377","description":"<p>Fast-developing fields such as Artificial Intelligence (AI) often outpace the\nefforts of encyclopedic sources such as Wikipedia, which either do not\ncompletely cover recently-introduced topics or lack such content entirely. As a\nresult, methods for automatically producing content are valuable tools to\naddress this information overload. We show that recent advances in pretrained\nlanguage modeling can be combined for a two-stage extractive and abstractive\napproach for Wikipedia lead paragraph generation. We extend this approach to\ngenerate longer Wikipedia-style summaries with sections and examine how such\nmethods struggle in this application through detailed studies with 100\nreference human-collected surveys. This is the first study on utilizing web\nresources for long Wikipedia-style summaries to the best of our knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawamura_R/0/1/0/all/0/1\">Rina Kawamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tae_J/0/1/0/all/0/1\">Jaesung Tae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Sally Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizutani_T/0/1/0/all/0/1\">Tomoe Mizutani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Approach to Entity-Centric Context Tracking in Social Conversations. (arXiv:2201.12409v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12409","description":"<p>In human-human conversations, Context Tracking deals with identifying\nimportant entities and keeping track of their properties and relationships.\nThis is a challenging problem that encompasses several subtasks such as slot\ntagging, coreference resolution, resolving plural mentions and entity linking.\nWe approach this problem as an end-to-end modeling task where the\nconversational context is represented by an entity repository containing the\nentity references mentioned so far, their properties and the relationships\nbetween them. The repository is updated turn-by-turn, thus making training and\ninference computationally efficient even for long conversations. This paper\nlays the groundwork for an investigation of this framework in two ways. First,\nwe release Contrack, a large scale human-human conversation corpus for context\ntracking with people and location annotations. It contains over 7000\nconversations with an average of 11.8 turns, 5.8 entities and 15.2 references\nper conversation. Second, we open-source a neural network architecture for\ncontext tracking. Finally we compare this network to state-of-the-art\napproaches for the subtasks it subsumes and report results on the involved\ntradeoffs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruckert_U/0/1/0/all/0/1\">Ulrich R&#xfc;ckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkara_S/0/1/0/all/0/1\">Srinivas Sunkara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Sushant Prakash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaitan_P/0/1/0/all/0/1\">Pranav Khaitan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct parsing to sentiment graphs. (arXiv:2203.13209v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13209","description":"<p>This paper demonstrates how a graph-based semantic parser can be applied to\nthe task of structured sentiment analysis, directly predicting sentiment graphs\nfrom text. We advance the state of the art on 4 out of 5 standard benchmark\nsets. We release the source code, models and predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1\">David Samuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_J/0/1/0/all/0/1\">Jeremy Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_R/0/1/0/all/0/1\">Robin Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oepen_S/0/1/0/all/0/1\">Stephan Oepen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velldal_E/0/1/0/all/0/1\">Erik Velldal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CharacterBERT and Self-Teaching for Improving the Robustness of Dense Retrievers on Queries with Typos. (arXiv:2204.00716v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.00716","description":"<p>Current dense retrievers are not robust to out-of-domain and outlier queries,\ni.e. their effectiveness on these queries is much poorer than what one would\nexpect. In this paper, we consider a specific instance of such queries: queries\nthat contain typos. We show that a small character level perturbation in\nqueries (as caused by typos) highly impacts the effectiveness of dense\nretrievers. We then demonstrate that the root cause of this resides in the\ninput tokenization strategy employed by BERT. In BERT, tokenization is\nperformed using the BERT's WordPiece tokenizer and we show that a token with a\ntypo will significantly change the token distributions obtained after\ntokenization. This distribution change translates to changes in the input\nembeddings passed to the BERT-based query encoder of dense retrievers. We then\nturn our attention to devising dense retriever methods that are robust to such\nqueries with typos, while still being as performant as previous methods on\nqueries without typos. For this, we use CharacterBERT as the backbone encoder\nand an efficient yet effective training method, called Self-Teaching (ST), that\ndistills knowledge from queries without typos into the queries with typos.\nExperimental results show that CharacterBERT in combination with ST achieves\nsignificantly higher effectiveness on queries with typos compared to previous\nmethods. Along with these results and the open-sourced implementation of the\nmethods, we also provide a new passage retrieval dataset consisting of\nreal-world queries with typos and associated relevance assessments on the MS\nMARCO corpus, thus supporting the research community in the investigation of\neffective and robust dense retrievers. Code, experimental results and dataset\nare made available at https://github.com/ielab/CharacterBERT-DR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PERFECT: Prompt-free and Efficient Few-shot Learning with Language Models. (arXiv:2204.01172v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.01172","description":"<p>Current methods for few-shot fine-tuning of pretrained masked language models\n(PLMs) require carefully engineered prompts and verbalizers for each new task\nto convert examples into a cloze-format that the PLM can score. In this work,\nwe propose PERFECT, a simple and efficient method for few-shot fine-tuning of\nPLMs without relying on any such handcrafting, which is highly effective given\nas few as 32 data points. PERFECT makes two key design choices: First, we show\nthat manually engineered task prompts can be replaced with task-specific\nadapters that enable sample-efficient fine-tuning and reduce memory and storage\ncosts by roughly factors of 5 and 100, respectively. Second, instead of using\nhandcrafted verbalizers, we learn new multi-token label embeddings during\nfine-tuning, which are not tied to the model vocabulary and which allow us to\navoid complex auto-regressive decoding. These embeddings are not only learnable\nfrom limited data but also enable nearly 100x faster training and inference.\nExperiments on a wide range of few-shot NLP tasks demonstrate that PERFECT,\nwhile being simple and efficient, also outperforms existing state-of-the-art\nfew-shot learning methods. Our code is publicly available at\nhttps://github.com/facebookresearch/perfect.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahabadi_R/0/1/0/all/0/1\">Rabeeh Karimi Mahabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_L/0/1/0/all/0/1\">Lambert Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1\">Veselin Stoyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaMemo: Language Modeling with Look-Ahead Memory. (arXiv:2204.07341v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.07341","description":"<p>Although Transformers with fully connected self-attentions are powerful to\nmodel long-term dependencies, they are struggling to scale to long texts with\nthousands of words in language modeling. One of the solutions is to equip the\nmodel with a recurrence memory. However, existing approaches directly reuse\nhidden states from the previous segment that encodes contexts in a\nuni-directional way. As a result, this prohibits the memory to dynamically\ninteract with the current context that provides up-to-date information for\ntoken prediction. To remedy this issue, we propose Look-Ahead Memory (LaMemo)\nthat enhances the recurrence memory by incrementally attending to the\nright-side tokens, and interpolating with the old memory states to maintain\nlong-term information in the history. LaMemo embraces bi-directional attention\nand segment recurrence with an additional computation overhead only linearly\nproportional to the memory length. Experiments on widely used language modeling\nbenchmarks demonstrate its superiority over the baselines equipped with\ndifferent types of memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Haozhe Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhipeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doctor XAvIer: Explainable Diagnosis on Physician-Patient Dialogues and XAI Evaluation. (arXiv:2204.10178v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10178","description":"<p>We introduce Doctor XAvIer, a BERT-based diagnostic system that extracts\nrelevant clinical data from transcribed patient-doctor dialogues and explains\npredictions using feature attribution methods. We present a novel performance\nplot and evaluation metric for feature attribution methods: Feature Attribution\nDropping (FAD) curve and its Normalized Area Under the Curve (N-AUC). FAD curve\nanalysis shows that integrated gradients outperforms Shapley values in\nexplaining diagnosis classification. Doctor XAvIer outperforms the baseline\nwith 0.97 F1-score in named entity recognition and symptom pertinence\nclassification and 0.91 F1-score in diagnosis classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngai_H/0/1/0/all/0/1\">Hillary Ngai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locally Aggregated Feature Attribution on Natural Language Model Understanding. (arXiv:2204.10893v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10893","description":"<p>With the growing popularity of deep-learning models, model understanding\nbecomes more important. Much effort has been devoted to demystify deep neural\nnetworks for better interpretability. Some feature attribution methods have\nshown promising results in computer vision, especially the gradient-based\nmethods where effectively smoothing the gradients with reference data is key to\na robust and faithful result. However, direct application of these\ngradient-based methods to NLP tasks is not trivial due to the fact that the\ninput consists of discrete tokens and the \"reference\" tokens are not explicitly\ndefined. In this work, we propose Locally Aggregated Feature Attribution\n(LAFA), a novel gradient-based feature attribution method for NLP models.\nInstead of relying on obscure reference tokens, it smooths gradients by\naggregating similar reference texts derived from language model embeddings. For\nevaluation purpose, we also design experiments on different NLP tasks including\nEntity Recognition and Sentiment Analysis on public datasets as well as key\nfeature detection on a constructed Amazon catalogue dataset. The superior\nperformance of the proposed method is demonstrated through experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haitao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Rui Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-level Alignment Training Scheme for Video-and-Language Grounding. (arXiv:2204.10938v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10938","description":"<p>To solve video-and-language grounding tasks, the key is for the network to\nunderstand the connection between the two modalities. For a pair of video and\nlanguage description, their semantic relation is reflected by their encodings'\nsimilarity. A good multi-modality encoder should be able to well capture both\ninputs' semantics and encode them in the shared feature space where embedding\ndistance gets properly translated into their semantic similarity. In this work,\nwe focused on this semantic connection between video and language, and\ndeveloped a multi-level alignment training scheme to directly shape the\nencoding process. Global and segment levels of video-language alignment pairs\nwere designed, based on the information similarity ranging from high-level\ncontext to fine-grained semantics. The contrastive loss was used to contrast\nthe encodings' similarities between the positive and negative alignment pairs,\nand to ensure the network is trained in such a way that similar information is\nencoded closely in the shared feature space while information of different\nsemantics is kept apart. Our multi-level alignment training can be applied to\nvarious video-and-language grounding tasks. Together with the task-specific\ntraining loss, our framework achieved comparable performance to previous\nstate-of-the-arts on multiple video QA and retrieval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_F/0/1/0/all/0/1\">Feiyang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translation between Molecules and Natural Language. (arXiv:2204.11817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11817","description":"<p>Joint representations between images and text have been deeply investigated\nin the literature. In computer vision, the benefits of incorporating natural\nlanguage have become clear for enabling semantic-level control of images. In\nthis work, we present $\\textbf{MolT5}-$a self-supervised learning framework for\npretraining models on a vast amount of unlabeled natural language text and\nmolecule strings. $\\textbf{MolT5}$ allows for new, useful, and challenging\nanalogs of traditional vision-language tasks, such as molecule captioning and\ntext-based de novo molecule generation (altogether: translation between\nmolecules and language), which we explore for the first time. Furthermore,\nsince $\\textbf{MolT5}$ pretrains models on single-modal data, it helps overcome\nthe chemistry domain shortcoming of data scarcity. Additionally, we consider\nseveral metrics, including a new cross-modal embedding-based metric, to\nevaluate the tasks of molecule captioning and text-based molecule generation.\nBy interfacing molecules with natural language, we enable a higher semantic\nlevel of control over molecule discovery and understanding--a critical task for\nscientific domains such as drug discovery and material design. Our results show\nthat $\\textbf{MolT5}$-based models are able to generate outputs, both molecule\nand text, which in many cases are high quality and match the input modality. On\nmolecule generation, our best model achieves 30% exact matching test accuracy\n(i.e., it generates the correct structure for about one-third of the captions\nin our held-out test set).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edwards_C/0/1/0/all/0/1\">Carl Edwards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_T/0/1/0/all/0/1\">Tuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ros_K/0/1/0/all/0/1\">Kevin Ros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honke_G/0/1/0/all/0/1\">Garrett Honke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Closer Look at Personalization in Federated Image Classification. (arXiv:2204.11841v1 [cs.LG])","link":"http://arxiv.org/abs/2204.11841","description":"<p>Federated Learning (FL) is developed to learn a single global model across\nthe decentralized data, while is susceptible when realizing client-specific\npersonalization in the presence of statistical heterogeneity. However, studies\nfocus on learning a robust global model or personalized classifiers, which\nyield divergence due to inconsistent objectives. This paper shows that it is\npossible to achieve flexible personalization after the convergence of the\nglobal model by introducing representation learning. In this paper, we first\nanalyze and determine that non-IID data harms representation learning of the\nglobal model. Existing FL methods adhere to the scheme of jointly learning\nrepresentations and classifiers, where the global model is an average of\nclassification-based local models that are consistently subject to\nheterogeneity from non-IID data. As a solution, we separate representation\nlearning from classification learning in FL and propose RepPer, an independent\ntwo-stage personalized FL framework.We first learn the client-side feature\nrepresentation models that are robust to non-IID data and aggregate them into a\nglobal common representation model. After that, we achieve personalization by\nlearning a classifier head for each client, based on the common representation\nobtained at the former stage. Notably, the proposed two-stage learning scheme\nof RepPer can be potentially used for lightweight edge computing that involves\ndevices with constrained computation power.Experiments on various datasets\n(CIFAR-10/100, CINIC-10) and heterogeneous data setup show that RepPer\noutperforms alternatives in flexibility and personalization on non-IID data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jing_C/0/1/0/all/0/1\">Changxing Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yihong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Liyan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zhenlong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Leveraging Variational Graph Embeddings for Open World Compositional Zero-Shot Learning. (arXiv:2204.11848v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11848","description":"<p>Humans are able to identify and categorize novel compositions of known\nconcepts. The task in Compositional Zero-Shot learning (CZSL) is to learn\ncomposition of primitive concepts, i.e. objects and states, in such a way that\neven their novel compositions can be zero-shot classified. In this work, we do\nnot assume any prior knowledge on the feasibility of novel compositions\ni.e.open-world setting, where infeasible compositions dominate the search\nspace. We propose a Compositional Variational Graph Autoencoder (CVGAE)\napproach for learning the variational embeddings of the primitive concepts\n(nodes) as well as feasibility of their compositions (via edges). Such\nmodelling makes CVGAE scalable to real-world application scenarios. This is in\ncontrast to SOTA method, CGE, which is computationally very expensive. e.g.for\nbenchmark C-GQA dataset, CGE requires 3.94 x 10^5 nodes, whereas CVGAE requires\nonly 1323 nodes. We learn a mapping of the graph and image embeddings onto a\ncommon embedding space. CVGAE adopts a deep metric learning approach and learns\na similarity metric in this space via bi-directional contrastive loss between\nprojected graph and image embeddings. We validate the effectiveness of our\napproach on three benchmark datasets.We also demonstrate via an image retrieval\ntask that the representations learnt by CVGAE are better suited for\ncompositional generalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anwaar_M/0/1/0/all/0/1\">Muhammad Umer Anwaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhihui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinsteuber_M/0/1/0/all/0/1\">Martin Kleinsteuber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real or Virtual: A Video Conferencing Background Manipulation-Detection System. (arXiv:2204.11853v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11853","description":"<p>Recently, the popularity and wide use of the last-generation video\nconferencing technologies created an exponential growth in its market size.\nSuch technology allows participants in different geographic regions to have a\nvirtual face-to-face meeting. Additionally, it enables users to employ a\nvirtual background to conceal their own environment due to privacy concerns or\nto reduce distractions, particularly in professional settings. Nevertheless, in\nscenarios where the users should not hide their actual locations, they may\nmislead other participants by claiming their virtual background as a real one.\nTherefore, it is crucial to develop tools and strategies to detect the\nauthenticity of the considered virtual background. In this paper, we present a\ndetection strategy to distinguish between real and virtual video conferencing\nuser backgrounds. We demonstrate that our detector is robust against two attack\nscenarios. The first scenario considers the case where the detector is unaware\nabout the attacks and inn the second scenario, we make the detector aware of\nthe adversarial attacks, which we refer to Adversarial Multimedia Forensics\n(i.e, the forensically-edited frames are included in the training set). Given\nthe lack of publicly available dataset of virtual and real backgrounds for\nvideo conferencing, we created our own dataset and made them publicly available\n[1]. Then, we demonstrate the robustness of our detector against different\nadversarial attacks that the adversary considers. Ultimately, our detector's\nperformance is significant against the CRSPAM1372 [2] features, and\npost-processing operations such as geometric transformations with different\nquality factors that the attacker may choose. Moreover, our performance results\nshows that we can perfectly identify a real from a virtual background with an\naccuracy of 99.80%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekdad_Y/0/1/0/all/0/1\">Yassine Mekdad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uluagac_S/0/1/0/all/0/1\">Selcuk Uluagac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanikoglu_B/0/1/0/all/0/1\">Berrin Yanikoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolutionary latent space search for driving human portrait generation. (arXiv:2204.11887v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11887","description":"<p>This article presents an evolutionary approach for synthetic human portraits\ngeneration based on the latent space exploration of a generative adversarial\nnetwork. The idea is to produce different human face images very similar to a\ngiven target portrait. The approach applies StyleGAN2 for portrait generation\nand FaceNet for face similarity evaluation. The evolutionary search is based on\nexploring the real-coded latent space of StyleGAN2. The main results over both\nsynthetic and real images indicate that the proposed approach generates\naccurate and diverse solutions, which represent realistic human portraits. The\nproposed research can contribute to improving the security of face recognition\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machin_B/0/1/0/all/0/1\">Benjam&#xed;n Mach&#xed;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nesmachnow_S/0/1/0/all/0/1\">Sergio Nesmachnow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1\">Jamal Toutouh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProCST: Boosting Semantic Segmentation using Progressive Cyclic Style-Transfer. (arXiv:2204.11891v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11891","description":"<p>Using synthetic data for training neural networks that achieve good\nperformance on real-world data is an important task as it has the potential to\nreduce the need for costly data annotation. Yet, a network that is trained on\nsynthetic data alone does not perform well on real data due to the domain gap\nbetween the two. Reducing this gap, also known as domain adaptation, has been\nwidely studied in recent years. In the unsupervised domain adaptation (UDA)\nframework, unlabeled real data is used during training with labeled synthetic\ndata to obtain a neural network that performs well on real data. In this work,\nwe focus on image data. For the semantic segmentation task, it has been shown\nthat performing image-to-image translation from source to target, and then\ntraining a network for segmentation on source annotations - leads to poor\nresults. Therefore a joint training of both is essential, which has been a\ncommon practice in many techniques. Yet, closing the large domain gap between\nthe source and the target by directly performing the adaptation between the two\nis challenging. In this work, we propose a novel two-stage framework for\nimproving domain adaptation techniques. In the first step, we progressively\ntrain a multi-scale neural network to perform an initial transfer between the\nsource data to the target data. We denote the new transformed data as \"Source\nin Target\" (SiT). Then, we use the generated SiT data as the input to any\nstandard UDA approach. This new data has a reduced domain gap from the desired\ntarget domain, and the applied UDA approach further closes the gap. We\ndemonstrate the improvement achieved by our framework with two state-of-the-art\nmethods for semantic segmentation, DAFormer and ProDA, on two UDA tasks, GTA5\nto Cityscapes and Synthia to Cityscapes. Code and state-of-the-art checkpoints\nof ProCST+DAFormer are provided.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ettedgui_S/0/1/0/all/0/1\">Shahaf Ettedgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abu_Hussein_S/0/1/0/all/0/1\">Shady Abu-Hussein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DArch: Dental Arch Prior-assisted 3D Tooth Instance Segmentation. (arXiv:2204.11911v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11911","description":"<p>Automatic tooth instance segmentation on 3D dental models is a fundamental\ntask for computer-aided orthodontic treatments. Existing learning-based methods\nrely heavily on expensive point-wise annotations. To alleviate this problem, we\nare the first to explore a low-cost annotation way for 3D tooth instance\nsegmentation, i.e., labeling all tooth centroids and only a few teeth for each\ndental model. Regarding the challenge when only weak annotation is provided, we\npresent a dental arch prior-assisted 3D tooth segmentation method, namely\nDArch. Our DArch consists of two stages, including tooth centroid detection and\ntooth instance segmentation. Accurately detecting the tooth centroids can help\nlocate the individual tooth, thus benefiting the segmentation. Thus, our DArch\nproposes to leverage the dental arch prior to assist the detection.\nSpecifically, we firstly propose a coarse-to-fine method to estimate the dental\narch, in which the dental arch is initially generated by Bezier curve\nregression, and then a graph-based convolutional network (GCN) is trained to\nrefine it. With the estimated dental arch, we then propose a novel Arch-aware\nPoint Sampling (APS) method to assist the tooth centroid proposal generation.\nMeantime, a segmentor is independently trained using a patch-based training\nstrategy, aiming to segment a tooth instance from a 3D patch centered at the\ntooth centroid. Experimental results on $4,773$ dental models have shown our\nDArch can accurately segment each tooth of a dental model, and its performance\nis superior to the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Liangdong Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chongjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunbi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Relevance Analysis for Video Action Models. (arXiv:2204.11929v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11929","description":"<p>In this paper, we provide a deep analysis of temporal modeling for action\nrecognition, an important but underexplored problem in the literature. We first\npropose a new approach to quantify the temporal relationships between frames\ncaptured by CNN-based action models based on layer-wise relevance propagation.\nWe then conduct comprehensive experiments and in-depth analysis to provide a\nbetter understanding of how temporal modeling is affected by various factors\nsuch as dataset, network architecture, and input frames. With this, we further\nstudy some important questions for action recognition that lead to interesting\nfindings. Our analysis shows that there is no strong correlation between\ntemporal relevance and model performance; and action models tend to capture\nlocal temporal information, but less long-range dependencies. Our codes and\nmodels will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun-Fu/0/1/0/all/0/1\">Chun-Fu</a> (Richard) <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1\">Sarah Adel Bargal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Dual-Graph Regularized Moving Object Detection. (arXiv:2204.11939v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11939","description":"<p>Moving object detection and its associated background-foreground separation\nhave been widely used in a lot of applications, including computer vision,\ntransportation and surveillance. Due to the presence of the static background,\na video can be naturally decomposed into a low-rank background and a sparse\nforeground. Many regularization techniques, such as matrix nuclear norm, have\nbeen imposed on the background. In the meanwhile, sparsity or smoothness based\nregularizations, such as total variation and $\\ell_1$, can be imposed on the\nforeground. Moreover, graph Laplacians are further imposed to capture the\ncomplicated geometry of background images. Recently, weighted regularization\ntechniques including the weighted nuclear norm regularization have been\nproposed in the image processing community to promote adaptive sparsity while\nachieving efficient performance. In this paper, we propose a robust dual-graph\nregularized moving object detection model based on the weighted nuclear norm\nregularization, which is solved by the alternating direction method of\nmultipliers (ADMM). Numerical experiments on body movement data sets have\ndemonstrated the effectiveness of this method in separating moving objects from\nbackground, and the great potential in robotic applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1\">Ruilong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruihan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Biyun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SceneTrilogy: On Scene Sketches and its Relationship with Text and Photo. (arXiv:2204.11964v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11964","description":"<p>We for the first time extend multi-modal scene understanding to include that\nof free-hand scene sketches. This uniquely results in a trilogy of scene data\nmodalities (sketch, text, and photo), where each offers unique perspectives for\nscene understanding, and together enable a series of novel scene-specific\napplications across discriminative (retrieval) and generative (captioning)\ntasks. Our key objective is to learn a common three-way embedding space that\nenables many-to-many modality interactions (e.g, sketch+text $\\rightarrow$\nphoto retrieval). We importantly leverage the information bottleneck theory to\nachieve this goal, where we (i) decouple intra-modality information by\nminimising the mutual information between modality-specific and\nmodality-agnostic components via a conditional invertible neural network, and\n(ii) align \\textit{cross-modalities information} by maximising the mutual\ninformation between their modality-agnostic components using InfoNCE, with a\nspecific multihead attention mechanism to allow many-to-many modality\ninteractions. We spell out a few insights on the complementarity of each\nmodality for scene understanding, and study for the first time a series of\nscene-specific applications like joint sketch- and text-based image retrieval,\nsketch captioning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Acuity Prediction on Real-Life Patient Data Using a Machine Learning Based Multistage System. (arXiv:2204.11970v1 [eess.IV])","link":"http://arxiv.org/abs/2204.11970","description":"<p>In ophthalmology, intravitreal operative medication therapy (IVOM) is\nwidespread treatment for diseases such as the age-related macular degeneration\n(AMD), the diabetic macular edema (DME), as well as the retinal vein occlusion\n(RVO). However, in real-world settings, patients often suffer from loss of\nvision on time scales of years despite therapy, whereas the prediction of the\nvisual acuity (VA) and the earliest possible detection of deterioration under\nreal-life conditions is challenging due to heterogeneous and incomplete data.\nIn this contribution, we present a workflow for the development of a\nresearch-compatible data corpus fusing different IT systems of the department\nof ophthalmology of a German maximum care hospital. The extensive data corpus\nallows predictive statements of the expected progression of a patient and his\nor her VA in each of the three diseases. Within our proposed multistage system,\nwe classify the VA progression into the three groups of therapy \"winners\",\n\"stabilizers\", and \"losers\" (WSL scheme). Our OCT biomarker classification\nusing an ensemble of deep neural networks results in a classification accuracy\n(F1-score) of over 98 %, enabling us to complete incomplete OCT documentations\nwhile allowing us to exploit them for a more precise VA modelling process. Our\nVA prediction requires at least four VA examinations and optionally OCT\nbiomarkers from the same time period to predict the VA progression within a\nforecasted time frame. While achieving a prediction accuracy of up to 69 %\n(macro average F1-score) when considering all three WSL-based progression\ngroups, this corresponds to an improvement by 11 % in comparison to our\nophthalmic expertise (58 %).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schlosser_T/0/1/0/all/0/1\">Tobias Schlosser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beuth_F/0/1/0/all/0/1\">Frederik Beuth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meyer_T/0/1/0/all/0/1\">Trixy Meyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Arunodhayan Sampath Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stolze_G/0/1/0/all/0/1\">Gabriel Stolze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Furashova_O/0/1/0/all/0/1\">Olga Furashova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Engelmann_K/0/1/0/all/0/1\">Katrin Engelmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kowerko_D/0/1/0/all/0/1\">Danny Kowerko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BronchoPose: an analysis of data and model configuration for vision-based bronchoscopy pose estimation. (arXiv:2204.11982v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11982","description":"<p>Vision-based bronchoscopy (VB) models require the registration of the virtual\nlung model with the frames from the video bronchoscopy to provide effective\nguidance during the biopsy. The registration can be achieved by either tracking\nthe position and orientation of the bronchoscopy camera or by calibrating its\ndeviation from the pose (position and orientation) simulated in the virtual\nlung model. Recent advances in neural networks and temporal image processing\nhave provided new opportunities for guided bronchoscopy. However, such progress\nhas been hindered by the lack of comparative experimental conditions.\n</p>\n<p>In the present paper, we share a novel synthetic dataset allowing for a fair\ncomparison of methods. Moreover, this paper investigates several neural network\narchitectures for the learning of temporal information at different levels of\nsubject personalization. In order to improve orientation measurement, we also\npresent a standardized comparison framework and a novel metric for camera\norientation learning. Results on the dataset show that the proposed metric and\narchitectures, as well as the standardized conditions, provide notable\nimprovements to current state-of-the-art camera pose estimation in video\nbronchoscopy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borrego_Carazo_J/0/1/0/all/0/1\">Juan Borrego-Carazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_C/0/1/0/all/0/1\">Carles S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castells_Rufas_D/0/1/0/all/0/1\">David Castells-Rufas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrabina_J/0/1/0/all/0/1\">Jordi Carrabina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gil_D/0/1/0/all/0/1\">D&#xe9;bora Gil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive learning-based computational histopathology predict differential expression of cancer driver genes. (arXiv:2204.11994v1 [cs.CV])","link":"http://arxiv.org/abs/2204.11994","description":"<p>Digital pathological analysis is run as the main examination used for cancer\ndiagnosis. Recently, deep learning-driven feature extraction from pathology\nimages is able to detect genetic variations and tumor environment, but few\nstudies focus on differential gene expression in tumor cells. In this paper, we\npropose a self-supervised contrastive learning framework, HistCode, to infer\ndifferential gene expressions from whole slide images (WSIs). We leveraged\ncontrastive learning on large-scale unannotated WSIs to derive slide-level\nhistopathological feature in latent space, and then transfer it to tumor\ndiagnosis and prediction of differentially expressed cancer driver genes. Our\nextensive experiments showed that our method outperformed other\nstate-of-the-art models in tumor diagnosis tasks, and also effectively\npredicted differential gene expressions. Interestingly, we found the higher\nfold-changed genes can be more precisely predicted. To intuitively illustrate\nthe ability to extract informative features from pathological images, we\nspatially visualized the WSIs colored by the attentive scores of image tiles.\nWe found that the tumor and necrosis areas were highly consistent with the\nannotations of experienced pathologists. Moreover, the spatial heatmap\ngenerated by lymphocyte-specific gene expression patterns was also consistent\nwith the manually labeled WSI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gongming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuejun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dachuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the ability of generative adversarial networks to learn canonical medical image statistics. (arXiv:2204.12007v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12007","description":"<p>In recent years, generative adversarial networks (GANs) have gained\ntremendous popularity for potential applications in medical imaging, such as\nmedical image synthesis, restoration, reconstruction, translation, as well as\nobjective image quality assessment. Despite the impressive progress in\ngenerating high-resolution, perceptually realistic images, it is not clear if\nmodern GANs reliably learn the statistics that are meaningful to a downstream\nmedical imaging application. In this work, the ability of a state-of-the-art\nGAN to learn the statistics of canonical stochastic image models (SIMs) that\nare relevant to objective assessment of image quality is investigated. It is\nshown that although the employed GAN successfully learned several basic first-\nand second-order statistics of the specific medical SIMs under consideration\nand generated images with high perceptual quality, it failed to correctly learn\nseveral per-image statistics pertinent to the these SIMs, highlighting the\nurgent need to assess medical image GANs in terms of objective measures of\nimage quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotsis_D/0/1/0/all/0/1\">Dimitrios S. Gotsis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+KC_P/0/1/0/all/0/1\">Prabhat KC</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myers_K/0/1/0/all/0/1\">Kyle J. Myers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_R/0/1/0/all/0/1\">Rongping Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating the Resize Parameter in End-to-end Learned Image Compression. (arXiv:2204.12022v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12022","description":"<p>We describe a search-free resizing framework that can further improve the\nrate-distortion tradeoff of recent learned image compression models. Our\napproach is simple: compose a pair of differentiable downsampling/upsampling\nlayers that sandwich a neural compression model. To determine resize factors\nfor different inputs, we utilize another neural network jointly trained with\nthe compression model, with the end goal of minimizing the rate-distortion\nobjective. Our results suggest that \"compression friendly\" downsampled\nrepresentations can be quickly determined during encoding by using an auxiliary\nnetwork and differentiable image warping. By conducting extensive experimental\ntests on existing deep image compression models, we show results that our new\nresizing parameter estimation framework can provide Bj{\\o}ntegaard-Delta rate\n(BD-rate) improvement of about 10% against leading perceptual quality engines.\nWe also carried out a subjective quality study, the results of which show that\nour new approach yields favorable compressed images. To facilitate reproducible\nresearch in this direction, the implementation used in this paper is being made\nfreely available online at: https://github.com/treammm/ResizeCompression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Li-Heng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bampis_C/0/1/0/all/0/1\">Christos G. Bampis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krasula_L/0/1/0/all/0/1\">Luk&#xe1;&#x161; Krasula</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information Fusion: Scaling Subspace-Driven Approaches. (arXiv:2204.12035v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12035","description":"<p>In this work, we seek to exploit the deep structure of multi-modal data to\nrobustly exploit the group subspace distribution of the information using the\nConvolutional Neural Network (CNN) formalism. Upon unfolding the set of\nsubspaces constituting each data modality, and learning their corresponding\nencoders, an optimized integration of the generated inherent information is\ncarried out to yield a characterization of various classes. Referred to as deep\nMultimodal Robust Group Subspace Clustering (DRoGSuRe), this approach is\ncompared against the independently developed state-of-the-art approach named\nDeep Multimodal Subspace Clustering (DMSC). Experiments on different multimodal\ndatasets show that our approach is competitive and more robust in the presence\nof noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_S/0/1/0/all/0/1\">Sally Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krim_H/0/1/0/all/0/1\">Hamid Krim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Reasoning with Spatial-temporal Representation Learning: A Prospective Study. (arXiv:2204.12037v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12037","description":"<p>Spatial-temporal representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the existing visual methods rely heavily on large-scale\ndata annotations and supervised learning to learn a powerful big model.\nHowever, the lack of interpretability, robustness, and out-of-distribution\ngeneralization are becoming the bottleneck problems of these models, which\nhinders the progress of interpretable and reliable artificial intelligence. The\nmajority of the existing methods are based on correlation learning with the\nassumption that the data are independent and identically distributed, which\nlack an unified guidance and analysis about why modern spatial-temporal\nrepresentation learning methods have limited interpretability and easily\ncollapse into dataset bias. Inspired by the strong inference ability of\nhuman-level agents, recent years have therefore witnessed great effort in\ndeveloping causal reasoning paradigms to realize robust representation and\nmodel learning with good interpretability. In this paper, we conduct a\ncomprehensive review of existing causal reasoning methods for spatial-temporal\nrepresentation learning, covering fundamental theories, models, and datasets.\nThe limitations of current methods and datasets are also discussed. Moreover,\nwe propose some primary challenges, opportunities, and future research\ndirections for benchmarking causal reasoning algorithms in spatial-temporal\nrepresentation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yushen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Weighting Map for Bit-Depth Expansion within a Rational Range. (arXiv:2204.12039v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12039","description":"<p>Bit-depth expansion (BDE) is one of the emerging technologies to display high\nbit-depth (HBD) image from low bit-depth (LBD) source. Existing BDE methods\nhave no unified solution for various BDE situations, and directly learn a\nmapping for each pixel from LBD image to the desired value in HBD image, which\nmay change the given high-order bits and lead to a huge deviation from the\nground truth. In this paper, we design a bit restoration network (BRNet) to\nlearn a weight for each pixel, which indicates the ratio of the replenished\nvalue within a rational range, invoking an accurate solution without modifying\nthe given high-order bit information. To make the network adaptive for any\nbit-depth degradation, we investigate the issue in an optimization perspective\nand train the network under progressive training strategy for better\nperformance. Moreover, we employ Wasserstein distance as a visual quality\nindicator to evaluate the difference of color distribution between restored\nimage and the ground truth. Experimental results show our method can restore\ncolorful images with fewer artifacts and false contours, and outperforms\nstate-of-the-art methods with higher PSNR/SSIM results and lower Wasserstein\ndistance. The source code will be made available at\nhttps://github.com/yuqing-liu-dut/bit-depth-expansion\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-recoverable Adversarial Examples: A New Effective Protection Mechanism in Social Networks. (arXiv:2204.12050v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12050","description":"<p>Malicious intelligent algorithms greatly threaten the security of social\nusers' privacy by detecting and analyzing the uploaded photos to social network\nplatforms. The destruction to DNNs brought by the adversarial attack sparks the\npotential that adversarial examples serve as a new protection mechanism for\nprivacy security in social networks. However, the existing adversarial example\ndoes not have recoverability for serving as an effective protection mechanism.\nTo address this issue, we propose a recoverable generative adversarial network\nto generate self-recoverable adversarial examples. By modeling the adversarial\nattack and recovery as a united task, our method can minimize the error of the\nrecovered examples while maximizing the attack ability, resulting in better\nrecoverability of adversarial examples. To further boost the recoverability of\nthese examples, we exploit a dimension reducer to optimize the distribution of\nadversarial perturbation. The experimental results prove that the adversarial\nexamples generated by the proposed method present superior recoverability,\nattack ability, and robustness on different datasets and network architectures,\nwhich ensure its effectiveness as a protection mechanism in social networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiangyang Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Recent Work in Media Forensics: Methods and Threats. (arXiv:2204.12067v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12067","description":"<p>In this paper, we review recent work in media forensics for digital images,\nvideo, audio (specifically speech), and documents. For each data modality, we\ndiscuss synthesis and manipulation techniques that can be used to create and\nmodify digital media. We then review technological advancements for detecting\nand quantifying such manipulations. Finally, we consider open issues and\nsuggest directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhagtani_K/0/1/0/all/0/1\">Kratika Bhagtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_A/0/1/0/all/0/1\">Amit Kumar Singh Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartusiak_E/0/1/0/all/0/1\">Emily R. Bartusiak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1\">Ziyue Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1\">Ruiting Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baireddy_S/0/1/0/all/0/1\">Sriram Baireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images. (arXiv:2204.12077v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12077","description":"<p>Various deep learning methods have been proposed to segment breast lesion\nfrom ultrasound images. However, similar intensity distributions, variable\ntumor morphology and blurred boundaries present challenges for breast lesions\nsegmentation, especially for malignant tumors with irregular shapes.\nConsidering the complexity of ultrasound images, we develop an adaptive\nattention U-net (AAU-net) to segment breast lesions automatically and stably\nfrom ultrasound images. Specifically, we introduce a hybrid adaptive attention\nmodule, which mainly consists of a channel self-attention block and a spatial\nself-attention block, to replace the traditional convolution operation.\nCompared with the conventional convolution operation, the design of the hybrid\nadaptive attention module can help us capture more features under different\nreceptive fields. Different from existing attention mechanisms, the hybrid\nadaptive attention module can guide the network to adaptively select more\nrobust representation in channel and space dimensions to cope with more complex\nbreast lesions segmentation. Extensive experiments with several\nstate-of-the-art deep learning segmentation methods on three public breast\nultrasound datasets show that our method has better performance on breast\nlesion segmentation. Furthermore, robustness analysis and external experiments\ndemonstrate that our proposed AAU-net has better generalization performance on\nthe segmentation of breast lesions. Moreover, the hybrid adaptive attention\nmodule can be flexibly applied to existing network frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Gongping Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Y/0/1/0/all/0/1\">Yu Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jianxun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_M/0/1/0/all/0/1\">Moi Hoon Yap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-Net with ResNet Backbone for Garment Landmarking Purpose. (arXiv:2204.12084v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12084","description":"<p>We build a heatmap-based landmark detection model to locate important\nlandmarks on 2D RGB garment images. The main goal is to detect edges, corners\nand suitable interior region of the garments. This let us re-create 3D garments\nin modern 3D editing software by incorporate landmark detection model and\ntexture unwrapping. We use a U-net architecture with ResNet backbone to build\nthe model. With an appropriate loss function, we are able to train a moderately\nrobust model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1\">Khay Boon Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acquiring a Dynamic Light Field through a Single-Shot Coded Image. (arXiv:2204.12089v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12089","description":"<p>We propose a method for compressively acquiring a dynamic light field (a 5-D\nvolume) through a single-shot coded image (a 2-D measurement). We designed an\nimaging model that synchronously applies aperture coding and pixel-wise\nexposure coding within a single exposure time. This coding scheme enables us to\neffectively embed the original information into a single observed image. The\nobserved image is then fed to a convolutional neural network (CNN) for\nlight-field reconstruction, which is jointly trained with the camera-side\ncoding patterns. We also developed a hardware prototype to capture a real 3-D\nscene moving over time. We succeeded in acquiring a dynamic light field with\n5x5 viewpoints over 4 temporal sub-frames (100 views in total) from a single\nobserved image. Repeating capture and reconstruction processes over time, we\ncan acquire a dynamic light field at 4x the frame rate of the camera. To our\nknowledge, our method is the first to achieve a finer temporal resolution than\nthe camera itself in compressive light-field acquisition. Our software is\navailable from our project webpage\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mizuno_R/0/1/0/all/0/1\">Ryoya Mizuno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takahashi_K/0/1/0/all/0/1\">Keita Takahashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshida_M/0/1/0/all/0/1\">Michitaka Yoshida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsutake_C/0/1/0/all/0/1\">Chihiro Tsutake</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fujii_T/0/1/0/all/0/1\">Toshiaki Fujii</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nagahara_H/0/1/0/all/0/1\">Hajime Nagahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dual-Pixel Alignment for Defocus Deblurring. (arXiv:2204.12105v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12105","description":"<p>It is a challenging task to recover all-in-focus image from a single defocus\nblurry image in real-world applications. On many modern cameras, dual-pixel\n(DP) sensors create two-image views, based on which stereo information can be\nexploited to benefit defocus deblurring. Despite existing DP defocus deblurring\nmethods achieving impressive results, they directly take naive concatenation of\nDP views as input, while neglecting the disparity between left and right views\nin the regions out of camera's depth of field (DoF). In this work, we propose a\nDual-Pixel Alignment Network (DPANet) for defocus deblurring. Generally, DPANet\nis an encoder-decoder with skip-connections, where two branches with shared\nparameters in the encoder are employed to extract and align deep features from\nleft and right views, and one decoder is adopted to fuse aligned features for\npredicting the all-in-focus image. Due to that DP views suffer from different\nblur amounts, it is not trivial to align left and right views. To this end, we\npropose novel encoder alignment module (EAM) and decoder alignment module\n(DAM). In particular, a correlation layer is suggested in EAM to measure the\ndisparity between DP views, whose deep features can then be accordingly aligned\nusing deformable convolutions. And DAM can further enhance the alignment of\nskip-connected features from encoder and deep features in decoder. By\nintroducing several EAMs and DAMs, DP views in DPANet can be well aligned for\nbetter predicting latent all-in-focus image. Experimental results on real-world\ndatasets show that our DPANet is notably superior to state-of-the-art\ndeblurring methods in reducing defocus blur while recovering visually plausible\nsharp structures and textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1\">Yaling Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qince Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-Specific Feature Propagation for Referring Segmentation. (arXiv:2204.12109v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12109","description":"<p>Referring segmentation aims to generate a segmentation mask for the target\ninstance indicated by a natural language expression. There are typically two\nkinds of existing methods: one-stage methods that directly perform segmentation\non the fused vision and language features; and two-stage methods that first\nutilize an instance segmentation model for instance proposal and then select\none of these instances via matching them with language features. In this work,\nwe propose a novel framework that simultaneously detects the target-of-interest\nvia feature propagation and generates a fine-grained segmentation mask. In our\nframework, each instance is represented by an Instance-Specific Feature (ISF),\nand the target-of-referring is identified by exchanging information among all\nISFs using our proposed Feature Propagation Module (FPM). Our instance-aware\napproach learns the relationship among all objects, which helps to better\nlocate the target-of-interest than one-stage methods. Comparing to two-stage\nmethods, our approach collaboratively and interactively utilizes both vision\nand language information for synchronous identification and segmentation. In\nthe experimental tests, our method outperforms previous state-of-the-art\nmethods on all three RefCOCO series datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xudong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Maximum A Posteriori Estimation on Unpaired Data for Motion Deblurring. (arXiv:2204.12139v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12139","description":"<p>Real-world dynamic scene deblurring has long been a challenging task since\npaired blurry-sharp training data is unavailable. Conventional Maximum A\nPosteriori estimation and deep learning-based deblurring methods are restricted\nby handcrafted priors and synthetic blurry-sharp training pairs respectively,\nthereby failing to generalize to real dynamic blurriness. To this end, we\npropose a Neural Maximum A Posteriori (NeurMAP) estimation framework for\ntraining neural networks to recover blind motion information and sharp content\nfrom unpaired data. The proposed NeruMAP consists of a motion estimation\nnetwork and a deblurring network which are trained jointly to model the\n(re)blurring process (i.e. likelihood function). Meanwhile, the motion\nestimation network is trained to explore the motion information in images by\napplying implicit dynamic motion prior, and in return enforces the deblurring\nnetwork training (i.e. providing sharp image prior). The proposed NeurMAP is an\northogonal approach to existing deblurring neural networks, and is the first\nframework that enables training image deblurring networks on unpaired datasets.\nExperiments demonstrate our superiority on both quantitative metrics and visual\nquality over state-of-the-art methods. Codes are available on\nhttps://github.com/yjzhang96/NeurMAP-deblur.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youjian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper Insights into ViTs Robustness towards Common Corruptions. (arXiv:2204.12143v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12143","description":"<p>Recent literature have shown design strategies from Convolutions Neural\nNetworks (CNNs) benefit Vision Transformers (ViTs) in various vision tasks.\nHowever, it remains unclear how these design choices impact on robustness when\ntransferred to ViTs. In this paper, we make the first attempt to investigate\nhow CNN-like architectural designs and CNN-based data augmentation strategies\nimpact on ViTs' robustness towards common corruptions through an extensive and\nrigorous benchmarking. We demonstrate that overlapping patch embedding and\nconvolutional Feed-Forward Network (FFN) boost performance on robustness.\nFurthermore, adversarial noise training is powerful on ViTs while\nfourier-domain augmentation fails. Moreover, we introduce a novel conditional\nmethod enabling input-varied augmentations from two angles: (1) Generating\ndynamic augmentation parameters conditioned on input images. It conduces to\nstate-of-the-art performance on robustness through conditional convolutions;\n(2) Selecting most suitable augmentation strategy by an extra predictor helps\nto achieve the best trade-off between clean accuracy and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Rui Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yugang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where and What: Driver Attention-based Object Detection. (arXiv:2204.12150v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12150","description":"<p>Human drivers use their attentional mechanisms to focus on critical objects\nand make decisions while driving. As human attention can be revealed from gaze\ndata, capturing and analyzing gaze information has emerged in recent years to\nbenefit autonomous driving technology. Previous works in this context have\nprimarily aimed at predicting \"where\" human drivers look at and lack knowledge\nof \"what\" objects drivers focus on. Our work bridges the gap between\npixel-level and object-level attention prediction. Specifically, we propose to\nintegrate an attention prediction module into a pretrained object detection\nframework and predict the attention in a grid-based style. Furthermore,\ncritical objects are recognized based on predicted attended-to areas. We\nevaluate our proposed method on two driver attention datasets, BDD-A and\nDR(eye)VE. Our framework achieves competitive state-of-the-art performance in\nthe attention prediction on both pixel-level and object-level but is far more\nefficient (75.3 GFLOPs less) in computation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kassautzki_N/0/1/0/all/0/1\">Naemi-Rebecca Kassautzki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhl_W/0/1/0/all/0/1\">Wolfgang Fuhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClothFormer:Taming Video Virtual Try-on in All Module. (arXiv:2204.12151v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12151","description":"<p>The task of video virtual try-on aims to fit the target clothes to a person\nin the video with spatio-temporal consistency. Despite tremendous progress of\nimage virtual try-on, they lead to inconsistency between frames when applied to\nvideos. Limited work also explored the task of video-based virtual try-on but\nfailed to produce visually pleasing and temporally coherent results. Moreover,\nthere are two other key challenges: 1) how to generate accurate warping when\nocclusions appear in the clothing region; 2) how to generate clothes and\nnon-target body parts (e.g. arms, neck) in harmony with the complicated\nbackground; To address them, we propose a novel video virtual try-on framework,\nClothFormer, which successfully synthesizes realistic, harmonious, and\nspatio-temporal consistent results in complicated environment. In particular,\nClothFormer involves three major modules. First, a two-stage anti-occlusion\nwarping module that predicts an accurate dense flow mapping between the body\nregions and the clothing regions. Second, an appearance-flow tracking module\nutilizes ridge regression and optical flow correction to smooth the dense flow\nsequence and generate a temporally smooth warped clothing sequence. Third, a\ndual-stream transformer extracts and fuses clothing textures, person features,\nand environment information to generate realistic try-on videos. Through\nrigorous experiments, we demonstrate that our method highly surpasses the\nbaselines in terms of synthesized video quality both qualitatively and\nquantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jianbin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">He Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study on Approaches to Acoustic Scene Classification using CNNs. (arXiv:2204.12177v1 [cs.SD])","link":"http://arxiv.org/abs/2204.12177","description":"<p>Acoustic scene classification is a process of characterizing and classifying\nthe environments from sound recordings. The first step is to generate features\n(representations) from the recorded sound and then classify the background\nenvironments. However, different kinds of representations have dramatic effects\non the accuracy of the classification. In this paper, we explored the three\nsuch representations on classification accuracy using neural networks. We\ninvestigated the spectrograms, MFCCs, and embeddings representations using\ndifferent CNN networks and autoencoders. Our dataset consists of sounds from\nthree settings of indoors and outdoors environments - thus the dataset contains\nsound from six different kinds of environments. We found that the spectrogram\nrepresentation has the highest classification accuracy while MFCC has the\nlowest classification accuracy. We reported our findings, insights as well as\nsome guidelines to achieve better accuracy for environment classification using\nsounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ananya_I/0/1/0/all/0/1\">Ishrat Jahan Ananya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suad_S/0/1/0/all/0/1\">Sarah Suad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1\">Shadab Hafiz Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohammad Ashrafuzzaman Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TranSiam: Fusing Multimodal Visual Features Using Transformer for Medical Image Segmentation. (arXiv:2204.12185v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12185","description":"<p>Automatic segmentation of medical images based on multi-modality is an\nimportant topic for disease diagnosis. Although the convolutional neural\nnetwork (CNN) has been proven to have excellent performance in image\nsegmentation tasks, it is difficult to obtain global information. The lack of\nglobal information will seriously affect the accuracy of the segmentation\nresults of the lesion area. In addition, there are visual representation\ndifferences between multimodal data of the same patient. These differences will\naffect the results of the automatic segmentation methods. To solve these\nproblems, we propose a segmentation method suitable for multimodal medical\nimages that can capture global information, named TranSiam. TranSiam is a 2D\ndual path network that extracts features of different modalities. In each path,\nwe utilize convolution to extract detailed information in low level stage, and\ndesign a ICMT block to extract global information in high level stage. ICMT\nblock embeds convolution in the transformer, which can extract global\ninformation while retaining spatial and detailed information. Furthermore, we\ndesign a novel fusion mechanism based on cross attention and selfattention,\ncalled TMM block, which can effectively fuse features between different\nmodalities. On the BraTS 2019 and BraTS 2020 multimodal datasets, we have a\nsignificant improvement in accuracy over other popular methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuejian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shiqiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jijun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Coherence Over Attention Trajectory For Continuous Learning In Video Streams. (arXiv:2204.12193v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12193","description":"<p>Devising intelligent agents able to live in an environment and learn by\nobserving the surroundings is a longstanding goal of Artificial Intelligence.\nFrom a bare Machine Learning perspective, challenges arise when the agent is\nprevented from leveraging large fully-annotated dataset, but rather the\ninteractions with supervisory signals are sparsely distributed over space and\ntime. This paper proposes a novel neural-network-based approach to\nprogressively and autonomously develop pixel-wise representations in a video\nstream. The proposed method is based on a human-like attention mechanism that\nallows the agent to learn by observing what is moving in the attended\nlocations. Spatio-temporal stochastic coherence along the attention trajectory,\npaired with a contrastive term, leads to an unsupervised learning criterion\nthat naturally copes with the considered setting. Differently from most\nexisting works, the learned representations are used in open-set\nclass-incremental classification of each frame pixel, relying on few\nsupervisions. Our experiments leverage 3D virtual environments and they show\nthat the proposed agents can learn to distinguish objects just by observing the\nvideo stream. Inheriting features from state-of-the art models is not as\npowerful as one might expect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiezzi_M/0/1/0/all/0/1\">Matteo Tiezzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marullo_S/0/1/0/all/0/1\">Simone Marullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faggi_L/0/1/0/all/0/1\">Lapo Faggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meloni_E/0/1/0/all/0/1\">Enrico Meloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betti_A/0/1/0/all/0/1\">Alessandro Betti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melacci_S/0/1/0/all/0/1\">Stefano Melacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Split-Fusion Transformer. (arXiv:2204.12196v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12196","description":"<p>Neural networks for visual content understanding have recently evolved from\nconvolutional ones (CNNs) to transformers. The prior (CNN) relies on\nsmall-windowed kernels to capture the regional clues, demonstrating solid local\nexpressiveness. On the contrary, the latter (transformer) establishes\nlong-range global connections between localities for holistic learning.\nInspired by this complementary nature, there is a growing interest in designing\nhybrid models to best utilize each technique. Current hybrids merely replace\nconvolutions as simple approximations of linear projection or juxtapose a\nconvolution branch with attention, without concerning the importance of\nlocal/global modeling. To tackle this, we propose a new hybrid named Adaptive\nSplit-Fusion Transformer (ASF-former) to treat convolutional and attention\nbranches differently with adaptive weights. Specifically, an ASF-former encoder\nequally splits feature channels into half to fit dual-path inputs. Then, the\noutputs of dual-path are fused with weighting scalars calculated from visual\ncues. We also design the convolutional path compactly for efficiency concerns.\nExtensive experiments on standard benchmarks, such as ImageNet-1K, CIFAR-10,\nand CIFAR-100, show that our ASF-former outperforms its CNN, transformer\ncounterparts, and hybrid pilots in terms of accuracy (83.9% on ImageNet-1K),\nunder similar conditions (12.9G MACs/56.7M Params, without large-scale\npre-training). The code is available at:\nhttps://github.com/szx503045266/ASF-former.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Lei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Urban Change Detection Using a Dual-Task Siamese Network and Semi-Supervised Learning. (arXiv:2204.12202v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12202","description":"<p>In this study, a Semi-Supervised Learning (SSL) method for improving urban\nchange detection from bi-temporal image pairs was presented. The proposed\nmethod adapted a Dual-Task Siamese Difference network that not only predicts\nchanges with the difference decoder, but also segments buildings for both\nimages with a semantics decoder. First, the architecture was modified to\nproduce a second change prediction derived from the semantics predictions.\nSecond, SSL was adopted to improve supervised change detection. For unlabeled\ndata, we introduced a loss that encourages the network to predict consistent\nchanges across the two change outputs. The proposed method was tested on urban\nchange detection using the SpaceNet7 dataset. SSL achieved improved results\ncompared to three fully supervised benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hafner_S/0/1/0/all/0/1\">Sebastian Hafner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yifang Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascetti_A/0/1/0/all/0/1\">Andrea Nascetti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Adversarial Transferability of MLP-Mixer. (arXiv:2204.12204v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12204","description":"<p>The security of models based on new architectures such as MLP-Mixer and ViTs\nneeds to be studied urgently. However, most of the current researches are\nmainly aimed at the adversarial attack against ViTs, and there is still\nrelatively little adversarial work on MLP-mixer. We propose an adversarial\nattack method against MLP-Mixer called Maxwell's demon Attack (MA). MA breaks\nthe channel-mixing and token-mixing mechanism of MLP-Mixer by controlling the\npart input of MLP-Mixer's each Mixer layer, and disturbs MLP-Mixer to obtain\nthe main information of images. Our method can mask the part input of the Mixer\nlayer, avoid overfitting of the adversarial examples to the source model, and\nimprove the transferability of cross-architecture. Extensive experimental\nevaluation demonstrates the effectiveness and superior performance of the\nproposed MA. Our method can be easily combined with existing methods and can\nimprove the transferability by up to 38.0% on MLP-based ResMLP. Adversarial\nexamples produced by our method on MLP-Mixer are able to exceed the\ntransferability of adversarial examples produced using DenseNet against CNNs.\nTo the best of our knowledge, we are the first work to study adversarial\ntransferability of MLP-Mixer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Haoran Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yu-an Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuhang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanxin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Sequence Alignment using 4D Skeletal Augmentation. (arXiv:2204.12223v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12223","description":"<p>Temporal alignment of fine-grained human actions in videos is important for\nnumerous applications in computer vision, robotics, and mixed reality.\nState-of-the-art methods directly learn image-based embedding space by\nleveraging powerful deep convolutional neural networks. While being\nstraightforward, their results are far from satisfactory, the aligned videos\nexhibit severe temporal discontinuity without additional post-processing steps.\nThe recent advancements in human body and hand pose estimation in the wild\npromise new ways of addressing the task of human action alignment in videos. In\nthis work, based on off-the-shelf human pose estimators, we propose a novel\ncontext-aware self-supervised learning architecture to align sequences of\nactions. We name it CASA. Specifically, CASA employs self-attention and\ncross-attention mechanisms to incorporate the spatial and temporal context of\nhuman actions, which can solve the temporal discontinuity problem. Moreover, we\nintroduce a self-supervised learning scheme that is empowered by novel 4D\naugmentation techniques for 3D skeleton representations. We systematically\nevaluate the key components of our method. Our experiments on three public\ndatasets demonstrate CASA significantly improves phase progress and Kendall's\nTau scores over the previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Taein Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tekin_B/0/1/0/all/0/1\">Bugra Tekin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intercategorical Label Interpolation for Emotional Face Generation with Conditional Generative Adversarial Networks. (arXiv:2204.12237v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12237","description":"<p>Generative adversarial networks offer the possibility to generate deceptively\nreal images that are almost indistinguishable from actual photographs. Such\nsystems however rely on the presence of large datasets to realistically\nreplicate the corresponding domain. This is especially a problem if not only\nrandom new images are to be generated, but specific (continuous) features are\nto be co-modeled. A particularly important use case in \\emph{Human-Computer\nInteraction} (HCI) research is the generation of emotional images of human\nfaces, which can be used for various use cases, such as the automatic\ngeneration of avatars. The problem hereby lies in the availability of training\ndata. Most suitable datasets for this task rely on categorical emotion models\nand therefore feature only discrete annotation labels. This greatly hinders the\nlearning and modeling of smooth transitions between displayed affective states.\nTo overcome this challenge, we explore the potential of label interpolation to\nenhance networks trained on categorical datasets with the ability to generate\nimages conditioned on continuous features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1\">Silvan Mertes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiller_D/0/1/0/all/0/1\">Dominik Schiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lingenfelser_F/0/1/0/all/0/1\">Florian Lingenfelser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiderle_T/0/1/0/all/0/1\">Thomas Kiderle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroner_V/0/1/0/all/0/1\">Valentin Kroner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_L/0/1/0/all/0/1\">Lama Diab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Fine-Grained Structured Sparsity for Image Restoration. (arXiv:2204.12266v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12266","description":"<p>Image restoration tasks have witnessed great performance improvement in\nrecent years by developing large deep models. Despite the outstanding\nperformance, the heavy computation demanded by the deep models has restricted\nthe application of image restoration. To lift the restriction, it is required\nto reduce the size of the networks while maintaining accuracy. Recently, N:M\nstructured pruning has appeared as one of the effective and practical pruning\napproaches for making the model efficient with the accuracy constraint.\nHowever, it fails to account for different computational complexities and\nperformance requirements for different layers of an image restoration network.\nTo further optimize the trade-off between the efficiency and the restoration\naccuracy, we propose a novel pruning method that determines the pruning ratio\nfor N:M structured sparsity at each layer. Extensive experimental results on\nsuper-resolution and deblurring tasks demonstrate the efficacy of our method\nwhich outperforms previous pruning methods significantly. PyTorch\nimplementation for the proposed methods will be publicly available at\nhttps://github.com/JungHunOh/SLS_CVPR2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Junghun Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nah_S/0/1/0/all/0/1\">Seungjun Nah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1\">Cheeun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient Backdoor Attacks. (arXiv:2204.12281v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12281","description":"<p>Recent studies have proven that deep neural networks are vulnerable to\nbackdoor attacks. Specifically, by mixing a small number of poisoned samples\ninto the training set, the behavior of the trained model can be maliciously\ncontrolled. Existing attack methods construct such adversaries by randomly\nselecting some clean data from the benign set and then embedding a trigger into\nthem. However, this selection strategy ignores the fact that each poisoned\nsample contributes inequally to the backdoor injection, which reduces the\nefficiency of poisoning. In this paper, we formulate improving the poisoned\ndata efficiency by the selection as an optimization problem and propose a\nFiltering-and-Updating Strategy (FUS) to solve it. The experimental results on\nCIFAR-10 and ImageNet-10 indicate that the proposed method is effective: the\nsame attack success rate can be achieved with only 47% to 75% of the poisoned\nsample volume compared to the random selection strategy. More importantly, the\nadversaries selected according to one setting can generalize well to other\nsettings, exhibiting strong transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Pengfei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Framework for Quantification of Immune Cell Density and Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment. (arXiv:2204.12283v1 [q-bio.QM])","link":"http://arxiv.org/abs/2204.12283","description":"<p>Understanding the impact of tumor biology on the composition of nearby cells\noften requires characterizing the impact of biologically distinct tumor\nregions. Biomarkers have been developed to label biologically distinct tumor\nregions, but challenges arise because of differences in the spatial extent and\ndistribution of differentially labeled regions. In this work, we present a\nframework for systematically investigating the impact of distinct tumor regions\non cells near the tumor borders, accounting their cross spatial distributions.\nWe apply the framework to multiplex immunohistochemistry (mIHC) studies of\npancreatic cancer and show its efficacy in demonstrating how biologically\ndifferent tumor regions impact the immune response in the tumor\nmicroenvironment. Furthermore, we show that the proposed framework can be\nextended to largescale whole slide image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Hasan_M/0/1/0/all/0/1\">Mahmudul Hasan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kaczmarzyk_J/0/1/0/all/0/1\">Jakub R. Kaczmarzyk</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Paredes_D/0/1/0/all/0/1\">David Paredes</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Oblein_L/0/1/0/all/0/1\">Lyanne Oblein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Oentoro_J/0/1/0/all/0/1\">Jaymie Oentoro</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abousamra_S/0/1/0/all/0/1\">Shahira Abousamra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Horowitz_M/0/1/0/all/0/1\">Michael Horowitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shroyer_K/0/1/0/all/0/1\">Kenneth R. Shroyer</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saltz_J/0/1/0/all/0/1\">Joel Saltz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Language-Action Pre-training for Temporal Localization. (arXiv:2204.12293v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12293","description":"<p>Long-form video understanding requires designing approaches that are able to\ntemporally localize activities or language. End-to-end training for such tasks\nis limited by the compute device memory constraints and lack of temporal\nannotations at large-scale. These limitations can be addressed by pre-training\non large datasets of temporally trimmed videos supervised by class annotations.\nOnce the video encoder is pre-trained, it is common practice to freeze it\nduring fine-tuning. Therefore, the video encoder does not learn temporal\nboundaries and unseen classes, causing a domain gap with respect to the\ndownstream tasks. Moreover, using temporally trimmed videos prevents to capture\nthe relations between different action categories and the background context in\na video clip which results in limited generalization capacity. To address these\nlimitations, we propose a novel post-pre-training approach without freezing the\nvideo encoder which leverages language. We introduce a masked contrastive\nlearning loss to capture visio-linguistic relations between activities,\nbackground video clips and language in the form of captions. Our experiments\nshow that the proposed approach improves the state-of-the-art on temporal\naction localization, few-shot temporal action localization, and video language\ngrounding tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengmeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundogdu_E/0/1/0/all/0/1\">Erhan Gundogdu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapin_M/0/1/0/all/0/1\">Maksim Lapin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donoser_M/0/1/0/all/0/1\">Michael Donoser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzani_L/0/1/0/all/0/1\">Loris Bazzani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Segmentation of Hyperspectral Remote Sensing Images with Superpixels. (arXiv:2204.12296v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12296","description":"<p>In this paper, we propose an unsupervised method for hyperspectral remote\nsensing image segmentation. The method exploits the mean-shift clustering\nalgorithm that takes as input a preliminary hyperspectral superpixels\nsegmentation together with the spectral pixel information. The proposed method\ndoes not require the number of segmentation classes as input parameter, and it\ndoes not exploit any a-priori knowledge about the type of land-cover or\nland-use to be segmented (e.g. water, vegetation, building etc.). Experiments\non Salinas, SalinasA, Pavia Center and Pavia University datasets are carried\nout. Performance are measured in terms of normalized mutual information,\nadjusted Rand index and F1-score. Results demonstrate the validity of the\nproposed method in comparison with the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_M/0/1/0/all/0/1\">Mirko Paolo Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napoletano_P/0/1/0/all/0/1\">Paolo Napoletano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1\">Flavio Piccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schettini_R/0/1/0/all/0/1\">Raimondo Schettini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified GCNs: Towards Connecting GCNs with CNNs. (arXiv:2204.12300v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12300","description":"<p>Graph Convolutional Networks (GCNs) have been widely demonstrated their\npowerful ability in graph data representation and learning. Existing graph\nconvolution layers are mainly designed based on graph signal processing and\ntransform aspect which usually suffer from some limitations, such as\nover-smoothing, over-squashing and non-robustness, etc. As we all know that\nConvolution Neural Networks (CNNs) have received great success in many computer\nvision and machine learning. One main aspect is that CNNs leverage many\nlearnable convolution filters (kernels) to obtain rich feature descriptors and\nthus can have high capacity to encode complex patterns in visual data analysis.\nAlso, CNNs are flexible in designing their network architecture, such as\nMobileNet, ResNet, Xception, etc. Therefore, it is natural to arise a question:\ncan we design graph convolutional layer as flexibly as that in CNNs?\nInnovatively, in this paper, we consider connecting GCNs with CNNs deeply from\na general perspective of depthwise separable convolution operation.\nSpecifically, we show that GCN and GAT indeed perform some specific depthwise\nseparable convolution operations. This novel interpretation enables us to\nbetter understand the connections between GCNs (GCN, GAT) and CNNs and further\ninspires us to design more Unified GCNs (UGCNs). As two showcases, we implement\ntwo UGCNs, i.e., Separable UGCN (S-UGCN) and General UGCN (G-UGCN) for graph\ndata representation and learning. Promising experiments on several graph\nrepresentation benchmarks demonstrate the effectiveness and advantages of the\nproposed UGCNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bin Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Quality of a Synthesized Motion with the Fr\\'echet Motion Distance. (arXiv:2204.12318v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12318","description":"<p>Evaluating the Quality of a Synthesized Motion with the Fr\\'echet Motion\nDistance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_A/0/1/0/all/0/1\">Antoine Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1\">Youngwoo Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RAPQ: Rescuing Accuracy for Power-of-Two Low-bit Post-training Quantization. (arXiv:2204.12322v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12322","description":"<p>We introduce a Power-of-Two post-training quantization( PTQ) method for deep\nneural network that meets hardware requirements and does not call for long-time\nretraining. PTQ requires a small set of calibration data and is easier for\ndeployment, but results in lower accuracy than Quantization-Aware Training(\nQAT). Power-of-Two quantization can convert the multiplication introduced by\nquantization and dequantization to bit-shift that is adopted by many efficient\naccelerators. However, the Power-of-Two scale has fewer candidate values, which\nleads to more rounding or clipping errors. We propose a novel Power-of-Two PTQ\nframework, dubbed RAPQ, which dynamically adjusts the Power-of-Two scales of\nthe whole network instead of statically determining them layer by layer. It can\ntheoretically trade off the rounding error and clipping error of the whole\nnetwork. Meanwhile, the reconstruction method in RAPQ is based on the BN\ninformation of every unit. Extensive experiments on ImageNet prove the\nexcellent performance of our proposed method. Without bells and whistles, RAPQ\ncan reach accuracy of 65% and 48% on ResNet-18 and MobileNetV2 respectively\nwith weight INT2 activation INT4. We are the first to propose PTQ for the more\nconstrained but hardware-friendly Power-of-Two quantization and prove that it\ncan achieve nearly the same accuracy as SOTA PTQ method. The code will be\nreleased.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hongyi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jian Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangcheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chenying Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingzhang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algorithm for the Labeling and Interactive Visualization of the Cerebrovascular System of Ischemic Strokes. (arXiv:2204.12333v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12333","description":"<p>During the diagnosis of ischemic strokes, the Circle of Willis and its\nsurrounding vessels are the arteries of interest. Their visualization in case\nof an acute stroke is often enabled by Computed Tomography Angiography (CTA).\nStill, the identification and analysis of the cerebral arteries remain time\nconsuming in such scans due to a large number of peripheral vessels which may\ndisturb the visual impression. In previous work we proposed VirtualDSA++, an\nalgorithm designed to segment and label the cerebrovascular tree on CTA scans.\nEspecially with stroke patients, labeling is a delicate procedure, as in the\nworst case whole hemispheres may not be present due to impeded perfusion.\nHence, we extended the labeling mechanism for the cerebral arteries to identify\noccluded vessels. In the work at hand, we place the algorithm in a clinical\ncontext by evaluating the labeling and occlusion detection on stroke patients,\nwhere we have achieved labeling sensitivities comparable to other works between\n92\\,\\% and 95\\,\\%. To the best of our knowledge, ours is the first work to\naddress labeling and occlusion detection at once, whereby a sensitivity of\n67\\,\\% and a specificity of 81\\,\\% were obtained for the latter. VirtualDSA++\nalso automatically segments and models the intracranial system, which we\nfurther used in a deep learning driven follow up work. We present the generic\nconcept of iterative systematic search for pathways on all nodes of said model,\nwhich enables new interactive features. Exemplary, we derive in detail,\nfirstly, the interactive planning of vascular interventions like the mechanical\nthrombectomy and secondly, the interactive suppression of vessel structures\nthat are not of interest in diagnosing strokes (like veins). We discuss both\nfeatures as well as further possibilities emerging from the proposed concept.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thamm_F/0/1/0/all/0/1\">Florian Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jurgens_M/0/1/0/all/0/1\">Markus J&#xfc;rgens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taubmann_O/0/1/0/all/0/1\">Oliver Taubmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thamm_A/0/1/0/all/0/1\">Aleksandra Thamm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rist_L/0/1/0/all/0/1\">Leonhard Rist</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ditt_H/0/1/0/all/0/1\">Hendrik Ditt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Topological Structure of Floorplans from Room Attributes. (arXiv:2204.12338v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12338","description":"<p>Analysis of indoor spaces requires topological information. In this paper, we\npropose to extract topological information from room attributes using what we\ncall Iterative and adaptive graph Topology Learning (ITL). ITL progressively\npredicts multiple relations between rooms; at each iteration, it improves node\nembeddings, which in turn facilitates generation of a better topological graph\nstructure. This notion of iterative improvement of node embeddings and\ntopological graph structure is in the same spirit as \\cite{chen2020iterative}.\nHowever, while \\cite{chen2020iterative} computes the adjacency matrix based on\nnode similarity, we learn the graph metric using a relational decoder to\nextract room correlations. Experiments using a new challenging indoor dataset\nvalidate our proposed method. Qualitative and quantitative evaluation for\nlayout topology prediction and floorplan generation applications also\ndemonstrate the effectiveness of ITL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Will_H/0/1/0/all/0/1\">Hutchcroft Will</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naji_K/0/1/0/all/0/1\">Khosravan Naji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivaylo_B/0/1/0/all/0/1\">Boyadzhiev Ivaylo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_F/0/1/0/all/0/1\">Fu Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_K/0/1/0/all/0/1\">Kang Sing Bing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Restricted Black-box Adversarial Attack Against DeepFake Face Swapping. (arXiv:2204.12347v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12347","description":"<p>DeepFake face swapping presents a significant threat to online security and\nsocial media, which can replace the source face in an arbitrary photo/video\nwith the target face of an entirely different person. In order to prevent this\nfraud, some researchers have begun to study the adversarial methods against\nDeepFake or face manipulation. However, existing works focus on the white-box\nsetting or the black-box setting driven by abundant queries, which severely\nlimits the practical application of these methods. To tackle this problem, we\nintroduce a practical adversarial attack that does not require any queries to\nthe facial image forgery model. Our method is built on a substitute model\npersuing for face reconstruction and then transfers adversarial examples from\nthe substitute model directly to inaccessible black-box DeepFake models.\nSpecially, we propose the Transferable Cycle Adversary Generative Adversarial\nNetwork (TCA-GAN) to construct the adversarial perturbation for disrupting\nunknown DeepFake systems. We also present a novel post-regularization module\nfor enhancing the transferability of generated adversarial examples. To\ncomprehensively measure the effectiveness of our approaches, we construct a\nchallenging benchmark of DeepFake adversarial attacks for future development.\nExtensive experiments impressively show that the proposed adversarial attack\nmethod makes the visual quality of DeepFake face images plummet so that they\nare easier to be detected by humans and algorithms. Moreover, we demonstrate\nthat the proposed algorithm can be generalized to offer face image protection\nagainst various face translation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junhao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jianhuang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohua Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Transportability for Visual Recognition. (arXiv:2204.12363v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12363","description":"<p>Visual representations underlie object recognition tasks, but they often\ncontain both robust and non-robust features. Our main observation is that image\nclassifiers may perform poorly on out-of-distribution samples because spurious\ncorrelations between non-robust features and labels can be changed in a new\nenvironment. By analyzing procedures for out-of-distribution generalization\nwith a causal graph, we show that standard classifiers fail because the\nassociation between images and labels is not transportable across settings.\nHowever, we then show that the causal effect, which severs all sources of\nconfounding, remains invariant across domains. This motivates us to develop an\nalgorithm to estimate the causal effect for image classification, which is\ntransportable (i.e., invariant) across source and target environments. Without\nobserving additional variables, we show that we can derive an estimand for the\ncausal effect under empirical assumptions using representations in deep models\nas proxies. Theoretical analysis, empirical results, and visualizations show\nthat our approach captures causal invariances and improves overall\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_C/0/1/0/all/0/1\">Chengzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kevin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junfeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROMA: Cross-Domain Region Similarity Matching for Unpaired Nighttime Infrared to Daytime Visible Video Translation. (arXiv:2204.12367v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12367","description":"<p>Infrared cameras are often utilized to enhance the night vision since the\nvisible light cameras exhibit inferior efficacy without sufficient\nillumination. However, infrared data possesses inadequate color contrast and\nrepresentation ability attributed to its intrinsic heat-related imaging\nprinciple. This makes it arduous to capture and analyze information for human\nbeings, meanwhile hindering its application. Although, the domain gaps between\nunpaired nighttime infrared and daytime visible videos are even huger than\npaired ones that captured at the same time, establishing an effective\ntranslation mapping will greatly contribute to various fields. In this case,\nthe structural knowledge within nighttime infrared videos and semantic\ninformation contained in the translated daytime visible pairs could be utilized\nsimultaneously. To this end, we propose a tailored framework ROMA that couples\nwith our introduced cRoss-domain regiOn siMilarity mAtching technique for\nbridging the huge gaps. To be specific, ROMA could efficiently translate the\nunpaired nighttime infrared videos into fine-grained daytime visible ones,\nmeanwhile maintain the spatiotemporal consistency via matching the cross-domain\nregion similarity. Furthermore, we design a multiscale region-wise\ndiscriminator to distinguish the details from synthesized visible results and\nreal references. Extensive experiments and evaluations for specific\napplications indicate ROMA outperforms the state-of-the-art methods. Moreover,\nwe provide a new and challenging dataset encouraging further research for\nunpaired nighttime infrared and daytime visible video translation, named\nInfraredCity. In particular, it consists of 9 long video clips including City,\nHighway and Monitor scenarios. All clips could be split into 603,142 frames in\ntotal, which are 20 times larger than the recently released daytime\ninfrared-to-visible dataset IRVI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhenjie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bingfeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuigen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Fragile Features and Batch Normalization in Adversarial Training. (arXiv:2204.12393v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12393","description":"<p>Modern deep learning architecture utilize batch normalization (BN) to\nstabilize training and improve accuracy. It has been shown that the BN layers\nalone are surprisingly expressive. In the context of robustness against\nadversarial examples, however, BN is argued to increase vulnerability. That is,\nBN helps to learn fragile features. Nevertheless, BN is still used in\nadversarial training, which is the de-facto standard to learn robust features.\nIn order to shed light on the role of BN in adversarial training, we\ninvestigate to what extent the expressiveness of BN can be used to robustify\nfragile features in comparison to random features. On CIFAR10, we find that\nadversarially fine-tuning just the BN layers can result in non-trivial\nadversarial robustness. Adversarially training only the BN layers from scratch,\nin contrast, is not able to convey meaningful adversarial robustness. Our\nresults indicate that fragile features can be used to learn models with\nmoderate adversarial robustness, while random features cannot\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walter_N/0/1/0/all/0/1\">Nils Philipp Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stutz_D/0/1/0/all/0/1\">David Stutz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1\">Bernt Schiele</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Impact of Edge Cases from Occluded Pedestrians for ML Systems. (arXiv:2204.12402v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12402","description":"<p>Machine learning (ML)-enabled approaches are considered a substantial support\ntechnique of detection and classification of obstacles of traffic participants\nin self-driving vehicles. Major breakthroughs have been demonstrated the past\nfew years, even covering complete end-to-end data processing chain from sensory\ninputs through perception and planning to vehicle control of acceleration,\nbreaking and steering. YOLO (you-only-look-once) is a state-of-the-art\nperception neural network (NN) architecture providing object detection and\nclassification through bounding box estimations on camera images. As the NN is\ntrained on well annotated images, in this paper we study the variations of\nconfidence levels from the NN when tested on hand-crafted occlusion added to a\ntest set. We compare regular pedestrian detection to upper and lower body\ndetection. Our findings show that the two NN using only partial information\nperform similarly well like the NN for the full body when the full body NN's\nperformance is 0.75 or better. Furthermore and as expected, the network, which\nis only trained on the lower half body is least prone to disturbances from\nocclusions of the upper half and vice versa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henriksson_J/0/1/0/all/0/1\">Jens Henriksson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ursing_S/0/1/0/all/0/1\">Stig Ursing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on attention mechanisms for medical applications: are we moving towards better algorithms?. (arXiv:2204.12406v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12406","description":"<p>The increasing popularity of attention mechanisms in deep learning algorithms\nfor computer vision and natural language processing made these models\nattractive to other research domains. In healthcare, there is a strong need for\ntools that may improve the routines of the clinicians and the patients.\nNaturally, the use of attention-based algorithms for medical applications\noccurred smoothly. However, being healthcare a domain that depends on\nhigh-stake decisions, the scientific community must ponder if these\nhigh-performing algorithms fit the needs of medical applications. With this\nmotto, this paper extensively reviews the use of attention mechanisms in\nmachine learning (including Transformers) for several medical applications.\nThis work distinguishes itself from its predecessors by proposing a critical\nanalysis of the claims and potentialities of attention mechanisms presented in\nthe literature through an experimental case study on medical image\nclassification with three different use cases. These experiments focus on the\nintegrating process of attention mechanisms into established deep learning\narchitectures, the analysis of their predictive power, and a visual assessment\nof their saliency maps generated by post-hoc explanation methods. This paper\nconcludes with a critical analysis of the claims and potentialities presented\nin the literature about attention mechanisms and proposes future research lines\nin medical applications that may benefit from these frameworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_T/0/1/0/all/0/1\">Tiago Gon&#xe7;alves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rio_Torto_I/0/1/0/all/0/1\">Isabel Rio-Torto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teixeira_L/0/1/0/all/0/1\">Lu&#xed;s F. Teixeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MILES: Visual BERT Pre-training with Injected Language Semantics for Video-text Retrieval. (arXiv:2204.12408v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12408","description":"<p>Dominant pre-training work for video-text retrieval mainly adopt the\n\"dual-encoder\" architectures to enable efficient retrieval, where two separate\nencoders are used to contrast global video and text representations, but ignore\ndetailed local semantics. The recent success of image BERT pre-training with\nmasked visual modeling that promotes the learning of local visual context,\nmotivates a possible solution to address the above limitation. In this work, we\nfor the first time investigate masked visual modeling in video-text\npre-training with the \"dual-encoder\" architecture. We perform Masked visual\nmodeling with Injected LanguagE Semantics (MILES) by employing an extra\nsnapshot video encoder as an evolving \"tokenizer\" to produce reconstruction\ntargets for masked video patch prediction. Given the corrupted video, the video\nencoder is trained to recover text-aligned features of the masked patches via\nreasoning with the visible regions along the spatial and temporal dimensions,\nwhich enhances the discriminativeness of local visual features and the\nfine-grained cross-modality alignment. Our method outperforms state-of-the-art\nmethods for text-to-video retrieval on four datasets with both zero-shot and\nfine-tune evaluation protocols. Our approach also surpasses the baseline models\nsignificantly on zero-shot action recognition, which can be cast as\nvideo-to-text retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yuying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xihui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianping Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RadioPathomics: Multimodal Learning in Non-Small Cell Lung Cancer for Adaptive Radiotherapy. (arXiv:2204.12423v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12423","description":"<p>The current cancer treatment practice collects multimodal data, such as\nradiology images, histopathology slides, genomics and clinical data. The\nimportance of these data sources taken individually has fostered the recent\nraise of radiomics and pathomics, i.e. the extraction of quantitative features\nfrom radiology and histopathology images routinely collected to predict\nclinical outcomes or to guide clinical decisions using artificial intelligence\nalgorithms. Nevertheless, how to combine them into a single multimodal\nframework is still an open issue. In this work we therefore develop a\nmultimodal late fusion approach that combines hand-crafted features computed\nfrom radiomics, pathomics and clinical data to predict radiation therapy\ntreatment outcomes for non-small-cell lung cancer patients. Within this\ncontext, we investigate eight different late fusion rules (i.e. product,\nmaximum, minimum, mean, decision template, Dempster-Shafer, majority voting,\nand confidence rule) and two patient-wise aggregation rules leveraging the\nrichness of information given by computer tomography images and whole-slide\nscans. The experiments in leave-one-patient-out cross-validation on an in-house\ncohort of 33 patients show that the proposed multimodal paradigm with an AUC\nequal to $90.9\\%$ outperforms each unimodal approach, suggesting that data\nintegration can advance precision medicine. As a further contribution, we also\ncompare the hand-crafted representations with features automatically computed\nby deep networks, and the late fusion paradigm with early fusion, another\npopular multimodal approach. In both cases, the experiments show that the\nproposed multimodal approach provides the best results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tortora_M/0/1/0/all/0/1\">Matteo Tortora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordelli_E/0/1/0/all/0/1\">Ermanno Cordelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_R/0/1/0/all/0/1\">Rosa Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nibid_L/0/1/0/all/0/1\">Lorenzo Nibid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_E/0/1/0/all/0/1\">Edy Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrone_G/0/1/0/all/0/1\">Giuseppe Perrone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramella_S/0/1/0/all/0/1\">Sara Ramella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1\">Paolo Soda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding The Robustness in Vision Transformers. (arXiv:2204.12451v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12451","description":"<p>Recent studies show that Vision Transformers(ViTs) exhibit strong robustness\nagainst various corruptions. Although this property is partly attributed to the\nself-attention mechanism, there is still a lack of systematic understanding. In\nthis paper, we examine the role of self-attention in learning robust\nrepresentations. Our study is motivated by the intriguing properties of the\nemerging visual grouping in Vision Transformers, which indicates that\nself-attention may promote robustness through improved mid-level\nrepresentations. We further propose a family of fully attentional networks\n(FANs) that strengthen this capability by incorporating an attentional channel\nprocessing design. We validate the design comprehensively on various\nhierarchical backbones. Our model achieves a state of-the-art 87.1% accuracy\nand 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also\ndemonstrate state-of-the-art accuracy and robustness in two downstream tasks:\nsemantic segmentation and object detection. Code will be available at\nhttps://github.com/NVlabs/FAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Daquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Zooming for Multiple Instance Learning on Whole-Slide Images. (arXiv:2204.12454v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12454","description":"<p>Multiple Instance Learning (MIL) methods have become increasingly popular for\nclassifying giga-pixel sized Whole-Slide Images (WSIs) in digital pathology.\nMost MIL methods operate at a single WSI magnification, by processing all the\ntissue patches. Such a formulation induces high computational requirements, and\nconstrains the contextualization of the WSI-level representation to a single\nscale. A few MIL methods extend to multiple scales, but are computationally\nmore demanding. In this paper, inspired by the pathological diagnostic process,\nwe propose ZoomMIL, a method that learns to perform multi-level zooming in an\nend-to-end manner. ZoomMIL builds WSI representations by aggregating\ntissue-context information from multiple magnifications. The proposed method\noutperforms the state-of-the-art MIL methods in WSI classification on two large\ndatasets, while significantly reducing the computational demands with regard to\nFloating-Point Operations (FLOPs) and processing time by up to 40x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thandiackal_K/0/1/0/all/0/1\">Kevin Thandiackal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pati_P/0/1/0/all/0/1\">Pushpak Pati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaume_G/0/1/0/all/0/1\">Guillaume Jaume</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1\">Drew F. K. Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrani_M/0/1/0/all/0/1\">Maria Gabrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goksel_O/0/1/0/all/0/1\">Orcun Goksel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Focal Sparse Convolutional Networks for 3D Object Detection. (arXiv:2204.12463v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12463","description":"<p>Non-uniformed 3D sparse data, e.g., point clouds or voxels in different\nspatial positions, make contribution to the task of 3D object detection in\ndifferent ways. Existing basic components in sparse convolutional networks\n(Sparse CNNs) process all sparse data, regardless of regular or submanifold\nsparse convolution. In this paper, we introduce two new modules to enhance the\ncapability of Sparse CNNs, both are based on making feature sparsity learnable\nwith position-wise importance prediction. They are focal sparse convolution\n(Focals Conv) and its multi-modal variant of focal sparse convolution with\nfusion, or Focals Conv-F for short. The new modules can readily substitute\ntheir plain counterparts in existing Sparse CNNs and be jointly trained in an\nend-to-end fashion. For the first time, we show that spatially learnable\nsparsity in sparse convolution is essential for sophisticated 3D object\ndetection. Extensive experiments on the KITTI, nuScenes and Waymo benchmarks\nvalidate the effectiveness of our approach. Without bells and whistles, our\nresults outperform all existing single-model entries on the nuScenes test\nbenchmark at the paper submission time. Code and models are at\nhttps://github.com/dvlab-research/FocalsConv.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-free representation learning for few-shot learning via stochastic weight averaging. (arXiv:2204.12466v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12466","description":"<p>Recent studies on few-shot classification using transfer learning pose\nchallenges to the effectiveness and efficiency of episodic meta-learning\nalgorithms. Transfer learning approaches are a natural alternative, but they\nare restricted to few-shot classification. Moreover, little attention has been\non the development of probabilistic models with well-calibrated uncertainty\nfrom few-shot samples, except for some Bayesian episodic learning algorithms.\nTo tackle the aforementioned issues, we propose a new transfer learning method\nto obtain accurate and reliable models for few-shot regression and\nclassification. The resulting method does not require episodic meta-learning\nand is called meta-free representation learning (MFRL). MFRL first finds\nlow-rank representation generalizing well on meta-test tasks. Given the learned\nrepresentation, probabilistic linear models are fine-tuned with few-shot\nsamples to obtain models with well-calibrated uncertainty. The proposed method\nnot only achieves the highest accuracy on a wide range of few-shot learning\nbenchmark datasets but also correctly quantifies the prediction uncertainty. In\naddition, weight averaging and temperature scaling are effective in improving\nthe accuracy and reliability of few-shot learning in existing meta-learning\nalgorithms with a wide range of learning paradigms and model architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuilin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chi-Guhn Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-fine Q-attention with Tree Expansion. (arXiv:2204.12471v1 [cs.RO])","link":"http://arxiv.org/abs/2204.12471","description":"<p>Coarse-to-fine Q-attention enables sample-efficient robot manipulation by\ndiscretizing the translation space in a coarse-to-fine manner, where the\nresolution gradually increases at each layer in the hierarchy. Although\neffective, Q-attention suffers from \"coarse ambiguity\" - when voxelization is\nsignificantly coarse, it is not feasible to distinguish similar-looking objects\nwithout first inspecting at a finer resolution. To combat this, we propose to\nenvision Q-attention as a tree that can be expanded and used to accumulate\nvalue estimates across the top-k voxels at each Q-attention depth. When our\nextension, Q-attention with Tree Expansion (QTE), replaces standard Q-attention\nin the Attention-driven Robot Manipulation (ARM) system, we are able to\naccomplish a larger set of tasks; especially on those that suffer from \"coarse\nambiguity\". In addition to evaluating our approach across 12 RLBench tasks, we\nalso show that the improved performance is visible in a real-world task\ninvolving small objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViTPose: Simple Vision Transformer Baselines for Human Pose Estimation. (arXiv:2204.12484v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12484","description":"<p>Recently, customized vision transformers have been adapted for human pose\nestimation and have achieved superior performance with elaborate structures.\nHowever, it is still unclear whether plain vision transformers can facilitate\npose estimation. In this paper, we take the first step toward answering the\nquestion by employing a plain and non-hierarchical vision transformer together\nwith simple deconvolution decoders termed ViTPose for human pose estimation. We\ndemonstrate that a plain vision transformer with MAE pretraining can obtain\nsuperior performance after finetuning on human pose estimation datasets.\nViTPose has good scalability with respect to model size and flexibility\nregarding input resolution and token number. Moreover, it can be easily\npretrained using the unlabeled pose data without the need for large-scale\nupstream ImageNet data. Our biggest ViTPose model based on the ViTAE-G backbone\nwith 1 billion parameters obtains the best 80.9 mAP on the MS COCO test-dev\nset, while the ensemble models further set a new state-of-the-art for human\npose estimation, i.e., 81.1 mAP. The source code and models will be released at\nhttps://github.com/ViTAE-Transformer/ViTPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sound Localization by Self-Supervised Time Delay Estimation. (arXiv:2204.12489v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12489","description":"<p>Sounds reach one microphone in a stereo pair sooner than the other, resulting\nin an interaural time delay that conveys their directions. Estimating a sound's\ntime delay requires finding correspondences between the signals recorded by\neach microphone. We propose to learn these correspondences through\nself-supervision, drawing on recent techniques from visual tracking. We adapt\nthe contrastive random walk of Jabri et al. to learn a cycle-consistent\nrepresentation from unlabeled stereo sounds, resulting in a model that performs\non par with supervised methods on \"in the wild\" internet recordings. We also\npropose a multimodal contrastive learning model that solves a visually-guided\nlocalization task: estimating the time delay for a particular person in a\nmulti-speaker mixture, given a visual representation of their face. Project\nsite: https://ificl.github.io/stereocrw/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fouhey_D/0/1/0/all/0/1\">David F. Fouhey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owens_A/0/1/0/all/0/1\">Andrew Owens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From One Hand to Multiple Hands: Imitation Learning for Dexterous Manipulation from Single-Camera Teleoperation. (arXiv:2204.12490v1 [cs.RO])","link":"http://arxiv.org/abs/2204.12490","description":"<p>We propose to perform imitation learning for dexterous manipulation with\nmulti-finger robot hand from human demonstrations, and transfer the policy to\nthe real robot hand. We introduce a novel single-camera teleoperation system to\ncollect the 3D demonstrations efficiently with only an iPad and a computer. One\nkey contribution of our system is that we construct a customized robot hand for\neach user in the physical simulator, which is a manipulator resembling the same\nkinematics structure and shape of the operator's hand. This provides an\nintuitive interface and avoid unstable human-robot hand retargeting for data\ncollection, leading to large-scale and high quality data. Once the data is\ncollected, the customized robot hand trajectories can be converted to different\nspecified robot hands (models that are manufactured) to generate training\ndemonstrations. With imitation learning using our data, we show large\nimprovement over baselines with multiple complex manipulation tasks.\nImportantly, we show our learned policy is significantly more robust when\ntransferring to the real robot. More videos can be found in the\nhttps://yzqin.github.io/dex-teleop-imitation .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAMMA: A General Agent Motion Model for Autonomous Driving. (arXiv:1906.01566v6 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/1906.01566","description":"<p>This paper presents GAMMA, a general motion prediction model that enables\nlarge-scale real-time simulation and planning for autonomous driving. GAMMA\nmodels heterogeneous, interactive traffic agents. They operate under diverse\nroad conditions, with various geometric and kinematic constraints. GAMMA treats\nthe prediction task as constrained optimization in traffic agents' velocity\nspace. The objective is to optimize an agent's driving performance, while\nobeying all the constraints resulting from the agent's kinematics, collision\navoidance with other agents, and the environmental context. Further, GAMMA\nexplicitly conditions the prediction on human behavioral states as parameters\nof the optimization model, in order to account for versatile human behaviors.\nWe evaluated GAMMA on a set of real-world benchmark datasets. The results show\nthat GAMMA achieves high prediction accuracy on both homogeneous and\nheterogeneous traffic datasets, with sub-millisecond execution time. Further,\nthe computational efficiency and the flexibility of GAMMA enable (i) simulation\nof mixed urban traffic at many locations worldwide and (ii) planning for\nautonomous driving in dense traffic with uncertain driver behaviors, both in\nreal-time. The open-source code of GAMMA is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuanfu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Panpan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yiyuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Weakly-Supervised Learning Methods for Classification and Localization in Histology Images: A Comparative Study. (arXiv:1909.03354v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.03354","description":"<p>Using deep learning models to diagnose cancer from histology data presents\nseveral challenges. Cancer grading and localization of regions of interest\n(ROIs) in these images normally relies on both image- and pixel-level labels,\nthe latter requiring a costly annotation process. Deep weakly-supervised object\nlocalization (WSOL) methods provide different strategies for low-cost training\nof deep learning models. Using only image-class annotations, these methods can\nbe trained to classify an image, and yield class activation maps (CAMs) for ROI\nlocalization. This paper provides a review of state-of-art DL methods for WSOL.\nWe propose a taxonomy where these methods are divided into bottom-up and\ntop-down methods according to the information flow in models. Although the\nlatter have seen limited progress, recent bottom-up methods are currently\ndriving much progress with deep WSOL methods. Early works focused on designing\ndifferent spatial pooling functions. However, these methods reached limited\nlocalization accuracy, and unveiled a major limitation -- the under-activation\nof CAMs which leads to high false negative localization. Subsequent works aimed\nto alleviate this issue and recover complete object. Representative methods\nfrom our taxonomy are evaluated and compared in terms of classification and\nlocalization accuracy on two challenging histology datasets. Overall, the\nresults indicate poor localization performance, particularly for generic\nmethods that were initially designed to process natural images. Methods\ndesigned to address the challenges of histology data yielded good results.\nHowever, all methods suffer from high false positive/negative localization.\nFour key challenges are identified for the application of deep WSOL methods in\nhistology -- under/over activation of CAMs, sensitivity to thresholding, and\nmodel selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rony_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Rony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belharbi_S/0/1/0/all/0/1\">Soufiane Belharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCaffrey_L/0/1/0/all/0/1\">Luke McCaffrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1\">Eric Granger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PDE-based Group Equivariant Convolutional Neural Networks. (arXiv:2001.09046v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2001.09046","description":"<p>We present a PDE-based framework that generalizes Group equivariant\nConvolutional Neural Networks (G-CNNs). In this framework, a network layer is\nseen as a set of PDE-solvers where geometrically meaningful PDE-coefficients\nbecome the layer's trainable weights. Formulating our PDEs on homogeneous\nspaces allows these networks to be designed with built-in symmetries such as\nrotation in addition to the standard translation equivariance of CNNs.\n</p>\n<p>Having all the desired symmetries included in the design obviates the need to\ninclude them by means of costly techniques such as data augmentation. We will\ndiscuss our PDE-based G-CNNs (PDE-G-CNNs) in a general homogeneous space\nsetting while also going into the specifics of our primary case of interest:\nroto-translation equivariance.\n</p>\n<p>We solve the PDE of interest by a combination of linear group convolutions\nand non-linear morphological group convolutions with analytic kernel\napproximations that we underpin with formal theorems. Our kernel approximations\nallow for fast GPU-implementation of the PDE-solvers, we release our\nimplementation with this article in the form of the LieTorch extension to\nPyTorch, available at https://gitlab.com/bsmetsjr/lietorch . Just like for\nlinear convolution a morphological convolution is specified by a kernel that we\ntrain in our PDE-G-CNNs. In PDE-G-CNNs we do not use non-linearities such as\nmax/min-pooling and ReLUs as they are already subsumed by morphological\nconvolutions.\n</p>\n<p>We present a set of experiments to demonstrate the strength of the proposed\nPDE-G-CNNs in increasing the performance of deep learning based imaging\napplications with far fewer parameters than traditional CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smets_B/0/1/0/all/0/1\">Bart Smets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portegies_J/0/1/0/all/0/1\">Jim Portegies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekkers_E/0/1/0/all/0/1\">Erik Bekkers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duits_R/0/1/0/all/0/1\">Remco Duits</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark for Point Clouds Registration Algorithms. (arXiv:2003.12841v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2003.12841","description":"<p>Point clouds registration is a fundamental step of many point clouds\nprocessing pipelines; however, most algorithms are tested on data that are\ncollected ad-hoc and not shared with the research community. These data often\ncover only a very limited set of use cases; therefore, the results cannot be\ngeneralised. Public datasets proposed until now, taken individually, cover only\na few kinds of environment and mostly a single sensor. For these reasons, we\ndeveloped a benchmark, for localization and mapping applications, using\nmultiple publicly available datasets. In this way, we are able to cover many\nkinds of environment and many kinds of sensor that can produce point clouds.\nFurthermore, the ground truth has been thoroughly inspected and evaluated to\nensure its quality. For some of the datasets, the accuracy of the ground truth\nmeasuring system was not reported by the original authors, therefore we\nestimated it with our own novel method, based on an iterative registration\nalgorithm. Along with the data, we provide a broad set of registration\nproblems, chosen to cover different types of initial misalignment, various\ndegrees of overlap, and different kinds of registration problems. Lastly, we\npropose a metric to measure the performances of registration algorithms: it\ncombines the commonly used rotation and translation errors together, to allow\nan objective comparison of the alignments. This work aims at encouraging\nauthors to use a public and shared benchmark, instead of data collected ad-hoc,\nto ensure objectivity and repeatability, two fundamental characteristics in any\nscientific field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fontana_S/0/1/0/all/0/1\">Simone Fontana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattaneo_D/0/1/0/all/0/1\">Daniele Cattaneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballardini_A/0/1/0/all/0/1\">Augusto Luis Ballardini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaghi_M/0/1/0/all/0/1\">Matteo Vaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorrenti_D/0/1/0/all/0/1\">Domenico Giorgio Sorrenti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Riemannian Gradient-Based Methods for Minimax Problems. (arXiv:2010.06097v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.06097","description":"<p>In the paper, we study a class of useful minimax optimization problems on\nRiemanian manifolds and propose a class of Riemanian gradient-based methods to\nsolve these minimax problems. Specifically, we propose a Riemannian gradient\ndescent ascent (RGDA) algorithm for the deterministic minimax optimization.\nMoreover, we prove that our RGDA has a sample complexity of\n$O(\\kappa^2\\epsilon^{-2})$ for finding an $\\epsilon$-stationary point of the\nnonconvex strongly-concave minimax problems, where $\\kappa$ denotes the\ncondition number. At the same time, we introduce a Riemannian stochastic\ngradient descent ascent (RSGDA) algorithm for the stochastic minimax\noptimization. In the theoretical analysis, we prove that our RSGDA can achieve\na sample complexity of $O(\\kappa^4\\epsilon^{-4})$. To further reduce the sample\ncomplexity, we propose an accelerated Riemannian stochastic gradient descent\nascent (Acc-RSGDA) algorithm based on the variance-reduced technique. We prove\nthat our Acc-RSGDA algorithm achieves a lower sample complexity of\n$\\tilde{O}(\\kappa^{4}\\epsilon^{-3})$. Extensive experimental results on the\nrobust distributional optimization and Deep Neural Networks (DNNs) training\nover Stiefel manifold demonstrate efficiency of our algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shangqian Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Action Concept Grounding in Semantic Video Prediction. (arXiv:2011.11201v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11201","description":"<p>Recent works in video prediction have mainly focused on passive forecasting\nand low-level action-conditional prediction, which sidesteps the learning of\ninteraction between agents and objects. We introduce the task of semantic\naction-conditional video prediction, which uses semantic action labels to\ndescribe those interactions and can be regarded as an inverse problem of action\nrecognition. The challenge of this new task primarily lies in how to\neffectively inform the model of semantic action information. Inspired by the\nidea of Mixture of Experts, we embody each abstract label by a structured\ncombination of various visual concept learners and propose a novel video\nprediction model, Modular Action Concept Network (MAC). Our method is evaluated\non two newly designed synthetic datasets, CLEVR-Building-Blocks and\nSapien-Kitchen, and one real-world dataset called Tower-Creation. Extensive\nexperiments demonstrate that MAC can correctly condition on given instructions\nand generate corresponding future frames without need of bounding boxes. We\nfurther show that the trained model can make out-of-distribution\ngeneralization, be quickly adapted to new object categories and exploit its\nlearnt features for object detection, showing the progression towards\nhigher-level cognitive abilities. More visualizations can be found at\n<a href=\"http://www.pair.toronto.edu/mac/.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Songhenh Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Easterbrook_S/0/1/0/all/0/1\">Steve Easterbrook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Framework for Fluid Motion Estimation using a Constraint-Based Refinement Approach. (arXiv:2011.12267v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12267","description":"<p>The goal of this paper is to formulate a general framework for fluid motion\nestimation using a constraint-based refinement approach. We demonstrate that\nfor a particular choice of the constraint, our results closely approximate the\ncontinuity equation based fluid flow. This closeness is theoretically justified\nthrough a modified augmented Lagrangian method and validated numerically.\nFurther, along with the continuity constraint, our model can include other\ngeometric constraints as demonstrated. The mathematical well-posedness is\nstudied in the Hilbert space setting. Moreover, a special feature of our system\nis the possibility of a diagonalization by the Cauchy-Riemann operator and\ntransforming it to a diffusion process on the curl and the divergence of the\nflow. Using the theory of semigroups on the decoupled system, we show that our\napproach preserves the spatial characteristics of the divergence and the\nvorticities. We perform several numerical experiments and show the results on\ndifferent datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doshi_H/0/1/0/all/0/1\">Hirak Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiran_N/0/1/0/all/0/1\">N. Uday Kiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple multi-dataset detection. (arXiv:2102.13086v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.13086","description":"<p>How do we build a general and broad object detection system? We use all\nlabels of all concepts ever annotated. These labels span diverse datasets with\npotentially inconsistent taxonomies. In this paper, we present a simple method\nfor training a unified detector on multiple large-scale datasets. We use\ndataset-specific training protocols and losses, but share a common detection\narchitecture with dataset-specific outputs. We show how to automatically\nintegrate these dataset-specific outputs into a common semantic taxonomy. In\ncontrast to prior work, our approach does not require manual taxonomy\nreconciliation. Experiments show our learned taxonomy outperforms a\nexpert-designed taxonomy in all datasets. Our multi-dataset detector performs\nas well as dataset-specific models on each training domain, and can generalize\nto new unseen dataset without fine-tuning on them. Code is available at\nhttps://github.com/xingyizhou/UniDet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Inverse Problems by Joint Posterior Maximization with Autoencoding Prior. (arXiv:2103.01648v4 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2103.01648","description":"<p>In this work we address the problem of solving ill-posed inverse problems in\nimaging where the prior is a variational autoencoder (VAE). Specifically we\nconsider the decoupled case where the prior is trained once and can be reused\nfor many different log-concave degradation models without retraining. Whereas\nprevious MAP-based approaches to this problem lead to highly non-convex\noptimization algorithms, our approach computes the joint (space-latent) MAP\nthat naturally leads to alternate optimization algorithms and to the use of a\nstochastic encoder to accelerate computations. The resulting technique (JPMAP)\nperforms Joint Posterior Maximization using an Autoencoding Prior. We show\ntheoretical and experimental evidence that the proposed objective function is\nquite close to bi-convex. Indeed it satisfies a weak bi-convexity property\nwhich is sufficient to guarantee that our optimization scheme converges to a\nstationary point. We also highlight the importance of correctly training the\nVAE using a denoising criterion, in order to ensure that the encoder\ngeneralizes well to out-of-distribution images, without affecting the quality\nof the generative model. This simple modification is key to providing\nrobustness to the whole procedure. Finally we show how our joint MAP\nmethodology relates to more common MAP approaches, and we propose a\ncontinuation scheme that makes use of our JPMAP algorithm to provide more\nrobust MAP estimates. Experimental results also show the higher quality of the\nsolutions obtained by our JPMAP approach with respect to other non-convex MAP\napproaches which more often get stuck in spurious local optima.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mario Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Almansa_A/0/1/0/all/0/1\">Andr&#xe9;s Almansa</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tan_P/0/1/0/all/0/1\">Pauline Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Material Measurement Units for a Circular Economy: Foundations through a Review. (arXiv:2103.01997v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01997","description":"<p>Long-term availability of minerals and industrial materials is a necessary\ncondition for sustainable development as they are the constituents of any\nmanufacturing product. To enhance the efficiency of material management, we\ndefine a computer-vision-enabled material measurement system and provide a\nreview of works relevant to its development with particular emphasis on the\nfoundations. A network of such systems for wide-area material stock monitoring\nis also covered. Finally, challenges and future research directions are\ndiscussed. As the first article bridging industrial ecology and advanced\ncomputer vision, this review is intended to support both research communities\ntowards more sustainable manufacturing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zocco_F/0/1/0/all/0/1\">Federico Zocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLoone_S/0/1/0/all/0/1\">Se&#xe1;n McLoone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smyth_B/0/1/0/all/0/1\">Beatrice Smyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Multitask Neural Network for Face Alignment, Head Pose Estimation and Face Tracking. (arXiv:2103.07615v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.07615","description":"<p>While Convolutional Neural Networks (CNNs) have significantly boosted the\nperformance of face related algorithms, maintaining accuracy and efficiency\nsimultaneously in practical use remains challenging. The state-of-the-art\nmethods employ deeper networks for better performance, which makes it less\npractical for mobile applications because of more parameters and higher\ncomputational complexity. Therefore, we propose an efficient multitask neural\nnetwork, Alignment &amp; Tracking &amp; Pose Network (ATPN) for face alignment, face\ntracking and head pose estimation. Specifically, to achieve better performance\nwith fewer layers for face alignment, we introduce a shortcut connection\nbetween shallow-layer and deep-layer features. We find the shallow-layer\nfeatures are highly correspond to facial boundaries that can provide the\nstructural information of face and it is crucial for face alignment. Moreover,\nwe generate a cheap heatmap based on the face alignment result and fuse it with\nfeatures to improve the performance of the other two tasks. Based on the\nheatmap, the network can utilize both geometric information of landmarks and\nappearance information for head pose estimation. The heatmap also provides\nattention clues for face tracking. The face tracking task also saves us the\nface detection procedure for each frame, which also significantly boost the\nreal-time capability for video-based tasks. We experimentally validate ATPN on\nfour benchmark datasets, WFLW, 300VW, WIDER Face and 300W-LP. The experimental\nresults demonstrate that it achieves better performance with much less\nparameters and lower computational complexity compared to other light models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jiahao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haimin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shiping Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCALoss: Side and Corner Aligned Loss for Bounding Box Regression. (arXiv:2104.00462v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00462","description":"<p>Bounding box regression is an important component in object detection. Recent\nwork achieves promising performance by optimizing the Intersection over\nUnion~(IoU). However, IoU-based loss has the gradient vanish problem in the\ncase of low overlapping bounding boxes, and the model could easily ignore these\nsimple cases. In this paper, we propose Side Overlap~(SO) loss by maximizing\nthe side overlap of two bounding boxes, which puts more penalty for low\noverlapping bounding box cases. Besides, to speed up the convergence, the\nCorner Distance~(CD) is added into the objective function. Combining the Side\nOverlap and Corner Distance, we get a new regression objective function,\n\\textit{Side and Corner Align Loss~(SCALoss)}. The SCALoss is well-correlated\nwith IoU loss, which also benefits the evaluation metric but produces more\npenalty for low-overlapping cases. It can serve as a comprehensive similarity\nmeasure, leading to better localization performance and faster convergence\nspeed. Experiments on COCO, PASCAL VOC, and LVIS benchmarks show that SCALoss\ncan bring consistent improvement and outperform $\\ell_n$ loss and IoU based\nloss with popular object detectors such as YOLOV3, SSD, Faster-RCNN. Code is\navailable at: \\url{https://github.com/Turoad/SCALoss}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zili Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand Gesture Recognition Based on a Nonconvex Regularization. (arXiv:2104.14349v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14349","description":"<p>Recognition of hand gestures is one of the most fundamental tasks in\nhuman-robot interaction. Sparse representation based methods have been widely\nused due to their efficiency and low demands on the training data. Recently,\nnonconvex regularization techniques including the $\\ell_{1-2}$ regularization\nhave been proposed in the image processing community to promote sparsity while\nachieving efficient performance. In this paper, we propose a vision-based hand\ngesture recognition model based on the $\\ell_{1-2}$ regularization, which is\nsolved by the alternating direction method of multipliers (ADMM). Numerical\nexperiments on binary and gray-scale data sets have demonstrated the\neffectiveness of this method in identifying hand gestures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ashley_J/0/1/0/all/0/1\">Joshua Ashley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Biyun Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Refined Inertial DC Algorithm for DC Programming. (arXiv:2104.14750v2 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2104.14750","description":"<p>In this paper we consider the difference-of-convex (DC) programming problems,\nwhose objective function is the difference of two convex functions. The\nclassical DC Algorithm (DCA) is well-known for solving this kind of problems,\nwhich generally returns a critical point. Recently, an inertial DC algorithm\n(InDCA) equipped with heavy-ball inertial-force procedure was proposed in de\nOliveira et al. (Set-Valued and Variational Analysis 27(4):895--919, 2019),\nwhich potentially helps to improve both the convergence speed and the solution\nquality. Based on InDCA, we propose a refined inertial DC algorithm (RInDCA)\nequipped with enlarged inertial step-size compared with InDCA. Empirically,\nlarger step-size accelerates the convergence. We demonstrate the subsequential\nconvergence of our refined version to a critical point. In addition, by\nassuming the Kurdyka-{\\L}ojasiewicz (KL) property of the objective function, we\nestablish the sequential convergence of RInDCA. Numerical simulations on\nchecking copositivity of matrices and image denoising problem show the benefit\nof larger step-size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+You_Y/0/1/0/all/0/1\">Yu You</a>, <a href=\"http://arxiv.org/find/math/1/au:+Niu_Y/0/1/0/all/0/1\">Yi-Shuai Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Light-weight and Real-time Line Segment Detection. (arXiv:2106.00186v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00186","description":"<p>Previous deep learning-based line segment detection (LSD) suffers from the\nimmense model size and high computational cost for line prediction. This\nconstrains them from real-time inference on computationally restricted\nenvironments. In this paper, we propose a real-time and light-weight line\nsegment detector for resource-constrained environments named Mobile LSD\n(M-LSD). We design an extremely efficient LSD architecture by minimizing the\nbackbone network and removing the typical multi-module process for line\nprediction found in previous methods. To maintain competitive performance with\na light-weight network, we present novel training schemes: Segments of Line\nsegment (SoL) augmentation, matching and geometric loss. SoL augmentation\nsplits a line segment into multiple subparts, which are used to provide\nauxiliary line data during the training process. Moreover, the matching and\ngeometric loss allow a model to capture additional geometric cues. Compared\nwith TP-LSD-Lite, previously the best real-time LSD method, our model\n(M-LSD-tiny) achieves competitive performance with 2.5% of model size and an\nincrease of 130.5% in inference speed on GPU. Furthermore, our model runs at\n56.8 FPS and 48.6 FPS on the latest Android and iPhone mobile devices,\nrespectively. To the best of our knowledge, this is the first real-time deep\nLSD available on mobile devices. Our code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1\">Geonmo Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Byungsoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Go_S/0/1/0/all/0/1\">SeoungHyun Go</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sung-Hyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jingeun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1\">Minchul Shin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can An Image Classifier Suffice For Action Recognition?. (arXiv:2106.14104v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14104","description":"<p>We explore a new perspective on video understanding by casting the video\nrecognition problem as an image recognition task. Our approach rearranges input\nvideo frames into super images, which allow for training an image classifier\ndirectly to fulfill the task of action recognition, in exactly the same way as\nimage classification. With such a simple idea, we show that transformer-based\nimage classifiers alone can suffice for action recognition. In particular, our\napproach demonstrates strong and promising performance against SOTA methods on\nseveral public datasets including Kinetics400, Moments In Time,\nSomething-Something V2 (SSV2), Jester and Diving48. We also experiment with the\nprevalent ResNet image classifiers in computer vision to further validate our\nidea. The results on both Kinetics400 and SSV2 are comparable to some of the\nbest-performed CNN approaches based on spatio-temporal modeling. Our source\ncodes and models are available at https://github.com/IBM/sifar-pytorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun-Fu/0/1/0/all/0/1\">Chun-Fu</a> (Richard) <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Progressive and Coarse-to-fine Registration of Brain MRI via Deformation Field Integration and Non-Rigid Feature Fusion. (arXiv:2109.12384v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.12384","description":"<p>Registration of brain MRI images requires to solve a deformation field, which\nis extremely difficult in aligning intricate brain tissues, e.g., subcortical\nnuclei, etc. Existing efforts resort to decomposing the target deformation\nfield into intermediate sub-fields with either tiny motions, i.e., progressive\nregistration stage by stage, or lower resolutions, i.e., coarse-to-fine\nestimation of the full-size deformation field. In this paper, we argue that\nthose efforts are not mutually exclusive, and propose a unified framework for\nrobust brain MRI registration in both progressive and coarse-to-fine manners\nsimultaneously. Specifically, building on a dual-encoder U-Net, the\nfixed-moving MRI pair is encoded and decoded into multi-scale deformation\nsub-fields from coarse to fine. Each decoding block contains two proposed novel\nmodules: i) in Deformation Field Integration (DFI), a single integrated\nsub-field is calculated, warping by which is equivalent to warping\nprogressively by sub-fields from all previous decoding blocks, and ii) in\nNon-rigid Feature Fusion (NFF), features of the fixed-moving pair are aligned\nby DFI-integrated sub-field, and then fused to predict a finer sub-field.\nLeveraging both DFI and NFF, the target deformation field is factorized into\nmulti-scale sub-fields, where the coarser fields alleviate the estimate of a\nfiner one and the finer field learns to make up those misalignments insolvable\nby previous coarser ones. The extensive and comprehensive experimental results\non both private and public datasets demonstrate a superior registration\nperformance of brain MRI images over progressive registration only and\ncoarse-to-fine estimation only, with an increase by at most 8% in the average\nDice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jinxin Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiwei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_H/0/1/0/all/0/1\">Hongkuan Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haobo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yilang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anchor-free Oriented Proposal Generator for Object Detection. (arXiv:2110.01931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.01931","description":"<p>Oriented object detection is a practical and challenging task in remote\nsensing image interpretation. Nowadays, oriented detectors mostly use\nhorizontal boxes as intermedium to derive oriented boxes from them. However,\nthe horizontal boxes are inclined to get small Intersection-over-Unions (IoUs)\nwith ground truths, which may have some undesirable effects, such as\nintroducing redundant noise, mismatching with ground truths, detracting from\nthe robustness of detectors, etc. In this paper, we propose a novel Anchor-free\nOriented Proposal Generator (AOPG) that abandons horizontal box-related\noperations from the network architecture. AOPG first produces coarse oriented\nboxes by a Coarse Location Module (CLM) in an anchor-free manner and then\nrefines them into high-quality oriented proposals. After AOPG, we apply a Fast\nR-CNN head to produce the final detection results. Furthermore, the shortage of\nlarge-scale datasets is also a hindrance to the development of oriented object\ndetection. To alleviate the data insufficiency, we release a new dataset on the\nbasis of our DIOR dataset and name it DIOR-R. Massive experiments demonstrate\nthe effectiveness of AOPG. Particularly, without bells and whistles, we achieve\nthe accuracy of 64.41%, 75.24% and 96.22% mAP on the DIOR-R, DOTA and HRSC2016\ndatasets respectively. Code and models are available at\nhttps://github.com/jbwang1997/AOPG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiabao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xingxing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Chunbo Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yanqing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPT++: Customized Feature Representation for Siamese Visual Tracking. (arXiv:2110.12194v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12194","description":"<p>While recent years have witnessed remarkable progress in the feature\nrepresentation of visual tracking, the problem of feature misalignment between\nthe classification and regression tasks is largely overlooked. The approaches\nof feature extraction make no difference for these two tasks in most of\nadvanced trackers. We argue that the performance gain of visual tracking is\nlimited since features extracted from the salient area provide more\nrecognizable visual patterns for classification, while these around the\nboundaries contribute to accurately estimating the target state.\n</p>\n<p>We address this problem by proposing two customized feature extractors, named\npolar pooling and extreme pooling to capture task-specific visual patterns.\nPolar pooling plays the role of enriching information collected from the\nsemantic keypoints for stronger classification, while extreme pooling\nfacilitates explicit visual patterns of the object boundary for accurate target\nstate estimation. We demonstrate the effectiveness of the task-specific feature\nrepresentation by integrating it into the recent and advanced tracker RPT.\nExtensive experiments on several benchmarks show that our Customized Features\nbased RPT (RPT++) achieves new state-of-the-art performances on OTB-100,\nVOT2018, VOT2019, GOT-10k, TrackingNet and LaSOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LOLNeRF: Learn from One Look. (arXiv:2111.09996v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09996","description":"<p>We present a method for learning a generative 3D model based on neural\nradiance fields, trained solely from data with only single views of each\nobject. While generating realistic images is no longer a difficult task,\nproducing the corresponding 3D structure such that they can be rendered from\ndifferent views is non-trivial. We show that, unlike existing methods, one does\nnot need multi-view data to achieve this goal. Specifically, we show that by\nreconstructing many images aligned to an approximate canonical pose with a\nsingle network conditioned on a shared latent space, you can learn a space of\nradiance fields that models shape and appearance for a class of objects. We\ndemonstrate this by training models to reconstruct object categories using\ndatasets that contain only one view of each subject without depth or geometry\ninformation. Our experiments show that we achieve state-of-the-art results in\nnovel view synthesis and high-quality results for monocular depth prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebain_D/0/1/0/all/0/1\">Daniel Rebain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthews_M/0/1/0/all/0/1\">Mark Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kwang Moo Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lagun_D/0/1/0/all/0/1\">Dmitry Lagun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESGN: Efficient Stereo Geometry Network for Fast 3D Object Detection. (arXiv:2111.14055v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14055","description":"<p>Fast stereo based 3D object detectors have made great progress recently.\nHowever, they lag far behind high-precision stereo based methods in accuracy.\nWe argue that the main reason is due to the poor geometry-aware feature\nrepresentation in 3D space. To solve this problem, we propose an efficient\nstereo geometry network (ESGN). The key in our ESGN is an efficient\ngeometry-aware feature generation (EGFG) module. Our EGFG module first uses a\nstereo correlation and reprojection module to construct multi-scale stereo\nvolumes in camera frustum space, second employs a multi-scale BEV projection\nand fusion module to generate multiple geometry-aware features. In these two\nsteps, we adopt deep multi-scale information fusion for discriminative\ngeometry-aware feature generation, without any complex aggregation networks. In\naddition, we introduce a deep geometry-aware feature distillation scheme to\nguide stereo feature learning with a LiDAR-based detector. The experiments are\nperformed on the classical KITTI dataset. On KITTI test set, our ESGN\noutperforms the fast state-of-art-art detector YOLOStereo3D by 5.14\\% on\nmAP$_{3d}$ at 62$ms$. To the best of our knowledge, our ESGN achieves a best\ntrade-off between accuracy and speed. We hope that our efficient stereo\ngeometry network can provide more possible directions for fast 3D object\ndetection. Our source code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1\">Aqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1\">Yanwei Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiale Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yishun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CDLNet: Noise-Adaptive Convolutional Dictionary Learning Network for Blind Denoising and Demosaicing. (arXiv:2112.00913v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.00913","description":"<p>Deep learning based methods hold state-of-the-art results in low-level image\nprocessing tasks, but remain difficult to interpret due to their black-box\nconstruction. Unrolled optimization networks present an interpretable\nalternative to constructing deep neural networks by deriving their architecture\nfrom classical iterative optimization methods without use of tricks from the\nstandard deep learning tool-box. So far, such methods have demonstrated\nperformance close to that of state-of-the-art models while using their\ninterpretable construction to achieve a comparably low learned parameter count.\nIn this work, we propose an unrolled convolutional dictionary learning network\n(CDLNet) and demonstrate its competitive denoising and joint denoising and\ndemosaicing (JDD) performance both in low and high parameter count regimes.\nSpecifically, we show that the proposed model outperforms state-of-the-art\nfully convolutional denoising and JDD models when scaled to a similar parameter\ncount. In addition, we leverage the model's interpretable construction to\npropose a noise-adaptive parameterization of thresholds in the network that\nenables state-of-the-art blind denoising performance, and near perfect\ngeneralization on noise-levels unseen during training. Furthermore, we show\nthat such performance extends to the JDD task and unsupervised learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Janjusevic_N/0/1/0/all/0/1\">Nikola Janju&#x161;evi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalilian_Gourtani_A/0/1/0/all/0/1\">Amirhossein Khalilian-Gourtani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Modality-Aware Multiple Granularity Pre-Training for RGB-Infrared Person Re-Identification. (arXiv:2112.06147v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06147","description":"<p>RGB-Infrared person re-identification (RGB-IR ReID) aims to associate people\nacross disjoint RGB and IR camera views. Currently, state-of-the-art\nperformance of RGB-IR ReID is not as impressive as that of conventional ReID.\nMuch of that is due to the notorious modality bias training issue brought by\nthe single-modality ImageNet pre-training, which might yield RGB-biased\nrepresentations that severely hinder the cross-modality image retrieval. This\npaper makes first attempt to tackle the task from a pre-training perspective.\nWe propose a self-supervised pre-training solution, named Modality-Aware\nMultiple Granularity Learning (MMGL), which directly trains models from scratch\nonly on multi-modal ReID datasets, but achieving competitive results against\nImageNet pre-training, without using any external data or sophisticated tuning\ntricks. First, we develop a simple-but-effective 'permutation recovery' pretext\ntask that globally maps shuffled RGB-IR images into a shared latent permutation\nspace, providing modality-invariant global representations for downstream ReID\ntasks. Second, we present a part-aware cycle-contrastive (PCC) learning\nstrategy that utilizes cross-modality cycle-consistency to maximize agreement\nbetween semantically similar RGB-IR image patches. This enables contrastive\nlearning for the unpaired multi-modal scenarios, further improving the\ndiscriminability of local features without laborious instance augmentation.\nBased on these designs, MMGL effectively alleviates the modality bias training\nproblem. Extensive experiments demonstrate that it learns better\nrepresentations (+8.03% Rank-1 accuracy) with faster training speed (converge\nonly in few hours) and higher data efficiency (&lt;5% data size) than ImageNet\npre-training. The results also suggest it generalizes well to various existing\nmodels, losses and has promising transferability across datasets. The code will\nbe released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1\">Lin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Q/0/1/0/all/0/1\">Qianyan Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zongyuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yehansen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06455","description":"<p>Deep discriminative models (DDMs), such as deep regression forests, deep\nneural decision forests, have been extensively studied recently to solve\nproblems like facial age estimation, head pose estimation, gaze estimation and\nso forth. Such problems are challenging in part because a large amount of\neffective training data without noise and bias is often not available. While\nsome progress has been achieved through learning more discriminative features,\nor reweighting samples, we argue what is more desirable is to learn gradually\nto discriminate like human beings. Then, we resort to self-paced learning\n(SPL). But a natural question arises: can self-paced regime lead DDMs to\nachieve more robust and less biased solutions? A serious problem with SPL,\nwhich is firstly discussed by this work, is it tends to aggravate the bias of\nsolutions, especially for obvious imbalanced data. To this end, this paper\nproposes a new self-paced paradigm for deep discriminative model, which\ndistinguishes noisy and underrepresented examples according to the output\nlikelihood and entropy associated with each example, and tackle the fundamental\nranking problem in SPL from a new perspective: fairness. This paradigm is\nfundamental, and could be easily combined with a variety of DDMs. Extensive\nexperiments on three computer vision tasks, such as facial age estimation, head\npose estimation and gaze estimation, demonstrate the efficacy of our paradigm.\nTo the best of our knowledge, our work is the first paper in the literature of\nSPL that considers ranking fairness for self-paced regime construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingming Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yali Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Community Detection in Medical Image Datasets: Using Wavelets and Spectral Methods. (arXiv:2112.12021v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.12021","description":"<p>Medical image datasets can have large number of images representing patients\nwith different health conditions and various disease severity. When dealing\nwith raw unlabeled image datasets, the large number of samples often makes it\nhard for experts and non-experts to understand the variety of images present in\na dataset. Supervised learning methods rely on labeled images which requires a\nconsiderable effort by medical experts to first understand the communities of\nimages present in the data and then labeling the images. Here, we propose an\nalgorithm to facilitate the automatic identification of communities in medical\nimage datasets. We further demonstrate that such analysis can be insightful in\na supervised setting when the images are already labeled. Such insights are\nuseful because, health and disease severity can be considered a continuous\nspectrum, and within each class, there usually are finer communities worthy of\ninvestigation, especially when they have similarities to communities in other\nclasses. In our approach, we use wavelet decomposition of images in tandem with\nspectral methods. We show that the eigenvalues of a graph Laplacian can reveal\nthe number of notable communities in an image dataset. Moreover, analyzing the\nsimilarities may be used to infer a spectrum representing the severity of the\ndisease. In our experiments, we use a dataset of images labeled with different\nconditions for COVID patients. We detect 25 communities in the dataset and then\nobserve that only 6 of those communities contain patients with pneumonia. We\nalso investigate the contents of a colorectal cancer histology dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yousefzadeh_R/0/1/0/all/0/1\">Roozbeh Yousefzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v4 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2201.09367","description":"<p>The freeform architectural modeling process often involves two important\nstages: concept design and digital modeling. In the first stage, architects\nusually sketch the overall 3D shape and the panel layout on a physical or\ndigital paper briefly. In the second stage, a digital 3D model is created using\nthe sketch as a reference. The digital model needs to incorporate geometric\nrequirements for its components, such as the planarity of panels due to\nconsideration of construction costs, which can make the modeling process more\nchallenging. In this work, we present a novel sketch-based system to bridge the\nconcept design and digital modeling of freeform roof-like shapes represented as\nplanar quadrilateral (PQ) meshes. Our system allows the user to sketch the\nsurface boundary and contour lines under axonometric projection and supports\nthe sketching of occluded regions. In addition, the user can sketch feature\nlines to provide directional guidance to the PQ mesh layout. Given the 2D\nsketch input, we propose a deep neural network to infer in real-time the\nunderlying surface shape along with a dense conjugate direction field, both of\nwhich are used to extract the final PQ mesh. To train and validate our network,\nwe generate a large synthetic dataset that mimics architect sketching of\nfreeform quadrilateral patches. The effectiveness and usability of our system\nare demonstrated with quantitative and qualitative evaluation as well as user\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabi_W/0/1/0/all/0/1\">Wassim Jabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature based Cross-slide Registration. (arXiv:2202.09971v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.09971","description":"<p>Cross-slide image analysis provides additional information by analysing the\nexpression of different biomarkers as compared to a single slide analysis.\nThese biomarker stained slides are analysed side by side, revealing unknown\nrelations between them. During the slide preparation, a tissue section may be\nplaced at an arbitrary orientation as compared to other sections of the same\ntissue block. The problem is compounded by the fact that tissue contents are\nlikely to change from one section to the next and there may be unique artefacts\non some of the slides. This makes registration of each section to a reference\nsection of the same tissue block an important pre-requisite task before any\ncross-slide analysis. We propose a deep feature based registration (DFBR)\nmethod which utilises data-driven features to estimate the rigid\ntransformation. We adopted a multi-stage strategy for improving the quality of\nregistration. We also developed a visualisation tool to view registered pairs\nof WSIs at different magnifications. With the help of this tool, one can apply\na transformation on the fly without the need to generate transformed source WSI\nin a pyramidal form. We compared the performance of data-driven features with\nthat of hand-crafted features on the COMET dataset. Our approach can align the\nimages with low registration errors. Generally, the success of non-rigid\nregistration is dependent on the quality of rigid registration. To evaluate the\nefficacy of the DFBR method, the first two steps of the ANHIR winner's\nframework are replaced with our DFBR to register challenge provided image\npairs. The modified framework produces comparable results to that of challenge\nwinning team.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Awan_R/0/1/0/all/0/1\">Ruqayya Awan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weiss_N/0/1/0/all/0/1\">Nick Weiss</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning based Prediction of MSI using MMR Markers in Colorectal Cancer. (arXiv:2203.00449v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2203.00449","description":"<p>The accurate diagnosis and molecular profiling of colorectal cancers are\ncritical for planning the best treatment options for patients. Microsatellite\ninstability (MSI) or mismatch repair (MMR) status plays a vital role in\nappropriate treatment selection, has prognostic implications and is used to\ninvestigate the possibility of patients having underlying genetic disorders\n(Lynch syndrome). NICE recommends that all CRC patients should be offered\nMMR/MSI testing. Immunohistochemistry is commonly used to assess MMR status\nwith subsequent molecular testing performed as required. This incurs\nsignificant extra costs and requires additional resources. The introduction of\nautomated methods that can predict MSI or MMR status from a target image could\nsubstantially reduce the cost associated with MMR testing. Unlike previous\nstudies on MSI prediction involving training a CNN using coarse labels (MSI vs\nMicrosatellite Stable (MSS)), we have utilised fine-grain MMR labels for\ntraining purposes. In this paper, we present our work on predicting MSI status\nin a two-stage process using a single target slide either stained with CK8/18\nor H&amp;E. First, we trained a multi-headed convolutional neural network model\nwhere each head was responsible for predicting one of the MMR protein\nexpressions. To this end, we performed the registration of MMR stained slides\nto the target slide as a pre-processing step. In the second stage, statistical\nfeatures computed from the MMR prediction maps were used for the final MSI\nprediction. Our results demonstrated that MSI classification can be improved by\nincorporating fine-grained MMR labels in comparison to the previous approaches\nin which only coarse labels were utilised.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Awan_R/0/1/0/all/0/1\">Ruqayya Awan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Nimir_M/0/1/0/all/0/1\">Mohammed Nimir</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Raza_S/0/1/0/all/0/1\">Shan E Ahmed Raza</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bilal_M/0/1/0/all/0/1\">Mohsin Bilal</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lotz_J/0/1/0/all/0/1\">Johannes Lotz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Snead_D/0/1/0/all/0/1\">David Snead</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Robinson_A/0/1/0/all/0/1\">Andrew Robinson</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BPM-Net: non-contact blood pressure measuring network based on face videos. (arXiv:2203.03634v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03634","description":"<p>Blood pressure indicates cardiac function and peripheral vascular resistance\nand is critical for disease diagnosis. Traditionally, blood pressure data are\nmainly acquired through contact sensors, which require high maintenance and may\nbe inconvenient and unfriendly to some people (e.g., burn patients). In this\npaper, we proposed an efficient non-contact blood pressure measurement network\nbased on face videos. First, an innovative oversampling training strategy is\nproposed to handle the unbalanced data distribution. The input video sequences\nare first normalized and converted to our proposed YUVT color space. Then the\nspatio-temporal slicer encodes it into a multi-domain spatio-temporal mapping.\nFinally, the feature extractor composed of a series backbone network and LSTM\nfits the high-dimensional feature, which is fed into blood pressure classifier\nto locates the blood pressure interval. The blood pressure calculator combines\nthe results of the feature extractor and the blood pressure classifier to\noutput the final blood pressure value. We tested BPM-Net on MMSE-HR dataset,\nthe MAE of systolic blood pressure reached 12.35 mmHg and that of diastolic\nblood pressure reached 9.5 mmHg. Experimental results on MMSE-HR show that the\nnetwork outperforms existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jialiang Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yun Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_X/0/1/0/all/0/1\">Xiujuan Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tuning-free multi-coil compressed sensing MRI with Parallel Variable Density Approximate Message Passing (P-VDAMP). (arXiv:2203.04180v2 [math.NA] UPDATED)","link":"http://arxiv.org/abs/2203.04180","description":"<p>Magnetic Resonance Imaging (MRI) has excellent soft tissue contrast but is\nhindered by an inherently slow data acquisition process. Compressed sensing,\nwhich reconstructs sparse signals from incoherently sampled data, has been\nwidely applied to accelerate MRI acquisitions. Compressed sensing MRI requires\none or more model parameters to be tuned, which is usually done by hand, giving\nsub-optimal tuning in general. To address this issue, we build on previous work\nby the authors on the single-coil Variable Density Approximate Message Passing\n(VDAMP) algorithm, extending the framework to multiple receiver coils to\npropose the Parallel VDAMP (P-VDAMP) algorithm. For Bernoulli random variable\ndensity sampling, P-VDAMP obeys a \"state evolution\", where the intermediate\nper-iteration image estimate is distributed according to the ground truth\ncorrupted by a zero-mean Gaussian vector with approximately known covariance.\nTo our knowledge, P-VDAMP is the first algorithm for multi-coil MRI data that\nobeys a state evolution with accurately tracked parameters. We leverage state\nevolution to automatically tune sparse parameters on-the-fly with Stein's\nUnbiased Risk Estimate (SURE). P-VDAMP is evaluated on brain, knee and\nangiogram datasets and compared with four variants of the Fast Iterative\nShrinkage-Thresholding algorithm (FISTA), including two tuning-free variants\nfrom the literature. The proposed method is found to have a similar\nreconstruction quality and time to convergence as FISTA with an optimally tuned\nsparse weighting and offers substantial robustness and reconstruction quality\nimprovements over competing tuning-free methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Millard_C/0/1/0/all/0/1\">Charles Millard</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chiew_M/0/1/0/all/0/1\">Mark Chiew</a>, <a href=\"http://arxiv.org/find/math/1/au:+Tanner_J/0/1/0/all/0/1\">Jared Tanner</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hess_A/0/1/0/all/0/1\">Aaron T. Hess</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mailhe_B/0/1/0/all/0/1\">Boris Mailhe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Tracking Transformers. (arXiv:2203.13250v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13250","description":"<p>We present a novel transformer-based architecture for global multi-object\ntracking. Our network takes a short sequence of frames as input and produces\nglobal trajectories for all objects. The core component is a global tracking\ntransformer that operates on objects from all frames in the sequence. The\ntransformer encodes object features from all frames, and uses trajectory\nqueries to group them into trajectories. The trajectory queries are object\nfeatures from a single frame and naturally produce unique trajectories. Our\nglobal tracking transformer does not require intermediate pairwise grouping or\ncombinatorial association, and can be jointly trained with an object detector.\nIt achieves competitive performance on the popular MOT17 benchmark, with 75.3\nMOTA and 59.1 HOTA. More importantly, our framework seamlessly integrates into\nstate-of-the-art large-vocabulary detectors to track any objects. Experiments\non the challenging TAO dataset show that our framework consistently improves\nupon baselines that are based on pairwise association, outperforming published\nworks by a significant 7.7 tracking mAP. Code is available at\nhttps://github.com/xingyizhou/GTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_T/0/1/0/all/0/1\">Tianwei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krahenbuhl_P/0/1/0/all/0/1\">Philipp Kr&#xe4;henb&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition of polar lows in Sentinel-1 SAR images with deep learning. (arXiv:2203.16401v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16401","description":"<p>In this paper, we explore the possibility of detecting polar lows in C-band\nSAR images by means of deep learning. Specifically, we introduce a novel\ndataset consisting of Sentinel-1 images labeled as positive; representing a\nmaritime mesocyclone, or negative; representing a normal sea state. The dataset\nis constructed using the ERA5 dataset as baseline and it consists of 2004\nannotated images. To our knowledge, this is the first dataset of its kind to be\npublicly released. The dataset is used to train a deep learning model to\nclassify the labeled images. Evaluated on an independent test set, the model\nyields an F-1 score of 0.95, indicating that polar lows can be consistently\ndetected from SAR images. Interpretability techniques applied to the deep\nlearning model reveal that atmospheric fronts and cyclonic eyes are key\nfeatures in the classification. Moreover, experimental results show that the\nmodel is accurate even if: (i) such features are significantly cropped due to\nthe limited swath width of the SAR, (ii) the features are partly covered by sea\nice and (iii) land is covering significant parts of the images. By evaluating\nthe model performance on multiple input image resolutions (pixel sizes of 500m,\n1km and 2km), it is found that higher resolution yield the best performance.\nThis emphasises the potential of using high resolution sensors like SAR for\ndetecting polar lows, as compared to conventionally used sensors such as\nscatterometers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grahn_J/0/1/0/all/0/1\">Jakob Grahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchi_F/0/1/0/all/0/1\">Filippo Maria Bianchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive Biases for Object-Centric Representations in the Presence of Complex Textures. (arXiv:2204.08479v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08479","description":"<p>Understanding which inductive biases could be helpful for the unsupervised\nlearning of object-centric representations of natural scenes is challenging. We\nuse neural style transfer to generate datasets where objects have complex\ntextures while still retaining ground-truth annotations. We find that methods\nthat use a single module to reconstruct both the shape and visual appearance of\neach object learn more useful representations and achieve better object\nseparation. In addition, we observe that adjusting the latent space size is not\nsufficient to improve segmentation performance. Finally, the downstream\nusefulness of the representations is significantly more strongly correlated\nwith segmentation quality than with reconstruction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAFSSR: Stereo Image Super-Resolution Using NAFNet. (arXiv:2204.08714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08714","description":"<p>Stereo image super-resolution aims at enhancing the quality of\nsuper-resolution results by utilizing the complementary information provided by\nbinocular systems. To obtain reasonable performance, most methods focus on\nfinely designing modules, loss functions, and etc. to exploit information from\nanother viewpoint. This has the side effect of increasing system complexity,\nmaking it difficult for researchers to evaluate new ideas and compare methods.\nThis paper inherits a strong and simple image restoration model, NAFNet, for\nsingle-view feature extraction and extends it by adding cross attention modules\nto fuse features between views to adapt to binocular scenarios. The proposed\nbaseline for stereo image super-resolution is noted as NAFSSR. Furthermore,\ntraining/testing strategies are proposed to fully exploit the performance of\nNAFSSR. Extensive experiments demonstrate the effectiveness of our method. In\nparticular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012,\nKITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place\nin the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models\nwill be released at https://github.com/megvii-research/NAFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenqing Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Moment Retrieval from Text Queries via Single Frame Annotation. (arXiv:2204.09409v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09409","description":"<p>Video moment retrieval aims at finding the start and end timestamps of a\nmoment (part of a video) described by a given natural language query. Fully\nsupervised methods need complete temporal boundary annotations to achieve\npromising results, which is costly since the annotator needs to watch the whole\nmoment. Weakly supervised methods only rely on the paired video and query, but\nthe performance is relatively poor. In this paper, we look closer into the\nannotation process and propose a new paradigm called \"glance annotation\". This\nparadigm requires the timestamp of only one single random frame, which we refer\nto as a \"glance\", within the temporal boundary of the fully supervised\ncounterpart. We argue this is beneficial because comparing to weak supervision,\ntrivial cost is added yet more potential in performance is provided. Under the\nglance annotation setting, we propose a method named as Video moment retrieval\nvia Glance Annotation (ViGA) based on contrastive learning. ViGA cuts the input\nvideo into clips and contrasts between clips and queries, in which glance\nguided Gaussian distributed weights are assigned to all clips. Our extensive\nexperiments indicate that ViGA achieves better results than the\nstate-of-the-art weakly supervised methods by a large margin, even comparable\nto fully supervised methods in some cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tianwen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Pai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daskalaki_E/0/1/0/all/0/1\">Elena Daskalaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">De Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Person Video Dataset Annotation Method of Spatio-Temporally Actions. (arXiv:2204.10160v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10160","description":"<p>Spatio-temporal action detection is an important and challenging problem in\nvideo understanding. However, the application of the existing large-scale\nspatio-temporal action datasets in specific fields is limited, and there is\ncurrently no public tool for making spatio-temporal action datasets, it takes a\nlot of time and effort for researchers to customize the spatio-temporal action\ndatasets, so we propose a multi-Person video dataset Annotation Method of\nspatio-temporally actions.First, we use ffmpeg to crop the videos and frame the\nvideos; then use yolov5 to detect human in the video frame, and then use deep\nsort to detect the ID of the human in the video frame. By processing the\ndetection results of yolov5 and deep sort, we can get the annotation file of\nthe spatio-temporal action dataset to complete the work of customizing the\nspatio-temporal action dataset.\nhttps://github.com/Whiffe/Custom-ava-dataset_Custom-Spatio-Temporally-Action-Video-Dataset\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity Preserving Loss for Learned Image Compression. (arXiv:2204.10869v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10869","description":"<p>Deep learning model inference on embedded devices is challenging due to the\nlimited availability of computation resources. A popular alternative is to\nperform model inference on the cloud, which requires transmitting images from\nthe embedded device to the cloud. Image compression techniques are commonly\nemployed in such cloud-based architectures to reduce transmission latency over\nlow bandwidth networks. This work proposes an end-to-end image compression\nframework that learns domain-specific features to achieve higher compression\nratios than standard HEVC/JPEG compression techniques while maintaining\naccuracy on downstream tasks (e.g., recognition). Our framework does not\nrequire fine-tuning of the downstream task, which allows us to drop-in any\noff-the-shelf downstream task model without retraining. We choose faces as an\napplication domain due to the ready availability of datasets and off-the-shelf\nrecognition models as representative downstream tasks. We present a novel\nIdentity Preserving Reconstruction (IPR) loss function which achieves\nBits-Per-Pixel (BPP) values that are ~38% and ~42% of CRF-23 HEVC compression\nfor LFW (low-resolution) and CelebA-HQ (high-resolution) datasets,\nrespectively, while maintaining parity in recognition accuracy. The superior\ncompression ratio is achieved as the model learns to retain the domain-specific\nfeatures (e.g., facial features) while sacrificing details in the background.\nFurthermore, images reconstructed by our proposed compression model are robust\nto changes in downstream model architectures. We show at-par recognition\nperformance on the LFW dataset with an unseen recognition model while retaining\na lower BPP value of ~38% of CRF-23 HEVC compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiuhong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_L/0/1/0/all/0/1\">Lavisha Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Prithviraj Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Manoj Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-level Alignment Training Scheme for Video-and-Language Grounding. (arXiv:2204.10938v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10938","description":"<p>To solve video-and-language grounding tasks, the key is for the network to\nunderstand the connection between the two modalities. For a pair of video and\nlanguage description, their semantic relation is reflected by their encodings'\nsimilarity. A good multi-modality encoder should be able to well capture both\ninputs' semantics and encode them in the shared feature space where embedding\ndistance gets properly translated into their semantic similarity. In this work,\nwe focused on this semantic connection between video and language, and\ndeveloped a multi-level alignment training scheme to directly shape the\nencoding process. Global and segment levels of video-language alignment pairs\nwere designed, based on the information similarity ranging from high-level\ncontext to fine-grained semantics. The contrastive loss was used to contrast\nthe encodings' similarities between the positive and negative alignment pairs,\nand to ensure the network is trained in such a way that similar information is\nencoded closely in the shared feature space while information of different\nsemantics is kept apart. Our multi-level alignment training can be applied to\nvarious video-and-language grounding tasks. Together with the task-specific\ntraining loss, our framework achieved comparable performance to previous\nstate-of-the-arts on multiple video QA and retrieval datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_F/0/1/0/all/0/1\">Feiyang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_Q/0/1/0/all/0/1\">Qing Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thattai_G/0/1/0/all/0/1\">Govind Thattai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-DETR3D: Rethinking Overlapping Regions for Multi-View 3D Object Detection. (arXiv:2204.11582v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11582","description":"<p>3D object detection from multiple image views is a fundamental and\nchallenging task for visual scene understanding. Due to its low cost and high\nefficiency, multi-view 3D object detection has demonstrated promising\napplication prospects. However, accurately detecting objects through\nperspective views in the 3D space is extremely difficult due to the lack of\ndepth information. Recently, DETR3D introduces a novel 3D-2D query paradigm in\naggregating multi-view images for 3D object detection and achieves\nstate-of-the-art performance. In this paper, with intensive pilot experiments,\nwe quantify the objects located at different regions and find that the\n\"truncated instances\" (i.e., at the border regions of each image) are the main\nbottleneck hindering the performance of DETR3D. Although it merges multiple\nfeatures from two adjacent views in the overlapping regions, DETR3D still\nsuffers from insufficient feature aggregation, thus missing the chance to fully\nboost the detection performance. In an effort to tackle the problem, we propose\nGraph-DETR3D to automatically aggregate multi-view imagery information through\ngraph structure learning (GSL). It constructs a dynamic 3D graph between each\nobject query and 2D feature maps to enhance the object representations,\nespecially at the border regions. Besides, Graph-DETR3D benefits from a novel\ndepth-invariant multi-scale training strategy, which maintains the visual depth\nconsistency by simultaneously scaling the image size and the object depth.\nExtensive experiments on the nuScenes dataset demonstrate the effectiveness and\nefficiency of our Graph-DETR3D. Notably, our best model achieves 49.5 NDS on\nthe nuScenes test leaderboard, achieving new state-of-the-art in comparison\nwith various published image-view 3D object detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiquan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PVNAS: 3D Neural Architecture Search with Point-Voxel Convolution. (arXiv:2204.11797v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11797","description":"<p>3D neural networks are widely used in real-world applications (e.g., AR/VR\nheadsets, self-driving cars). They are required to be fast and accurate;\nhowever, limited hardware resources on edge devices make these requirements\nrather challenging. Previous work processes 3D data using either voxel-based or\npoint-based neural networks, but both types of 3D models are not\nhardware-efficient due to the large memory footprint and random memory access.\nIn this paper, we study 3D deep learning from the efficiency perspective. We\nfirst systematically analyze the bottlenecks of previous 3D methods. We then\ncombine the best from point-based and voxel-based models together and propose a\nnovel hardware-efficient 3D primitive, Point-Voxel Convolution (PVConv). We\nfurther enhance this primitive with the sparse convolution to make it more\neffective in processing large (outdoor) scenes. Based on our designed 3D\nprimitive, we introduce 3D Neural Architecture Search (3D-NAS) to explore the\nbest 3D network architecture given a resource constraint. We evaluate our\nproposed method on six representative benchmark datasets, achieving\nstate-of-the-art performance with 1.8-23.7x measured speedup. Furthermore, our\nmethod has been deployed to the autonomous racing vehicle of MIT Driverless,\nachieving larger detection range, higher accuracy and lower latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haotian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shengyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_K/0/1/0/all/0/1\">Kevin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Diffusion Models. (arXiv:2204.11824v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11824","description":"<p>Generative image synthesis with diffusion models has recently achieved\nexcellent visual quality in several tasks such as text-based or\nclass-conditional image synthesis. Much of this success is due to a dramatic\nincrease in the computational capacity invested in training these models. This\nwork presents an alternative approach: inspired by its successful application\nin natural language processing, we propose to complement the diffusion model\nwith a retrieval-based approach and to introduce an explicit memory in the form\nof an external database. During training, our diffusion model is trained with\nsimilar visual features retrieved via CLIP and from the neighborhood of each\ntraining instance. By leveraging CLIP's joint image-text embedding space, our\nmodel achieves highly competitive performance on tasks for which it has not\nbeen explicitly trained, such as class-conditional or text-image synthesis, and\ncan be conditioned on both text and image embeddings. Moreover, we can apply\nour approach to unconditional generation, where it achieves state-of-the-art\nperformance. Our approach incurs low computational and memory overheads and is\neasy to implement. We discuss its relationship to concurrent work and will\npublish code and pretrained models soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1\">Andreas Blattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1\">Robin Rombach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktay_K/0/1/0/all/0/1\">Kaan Oktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1\">Bj&#xf6;rn Ommer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Cross-Domain Content-Based Image Retrieval for E-commerce Snap and Search Application. (arXiv:2204.11593v1 [cs.IR] CROSS LISTED)","link":"http://arxiv.org/abs/2204.11593","description":"<p>In this industry talk at ECIR 2022, we illustrate how we approach the main\nchallenges from large scale cross-domain content-based image retrieval using a\ncascade method and a combination of our visual search and classification\ncapabilities. Specifically, we present a system that is able to handle the\nscale of the data for e-commerce usage and the cross-domain nature of the query\nand gallery image pools. We showcase the approach applied in real-world\ne-commerce snap and search use case and its impact on ranking and latency\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1\">Isaac Kwan Yin Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nussinovitch_E/0/1/0/all/0/1\">Eran Nussinovitch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}