{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"What Changed? Investigating Debiasing Methods using Causal Mediation Analysis. (arXiv:2206.00701v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00701","description":"<p>Previous work has examined how debiasing language models affect downstream\ntasks, specifically, how debiasing techniques influence task performance and\nwhether debiased models also make impartial predictions in downstream tasks or\nnot. However, what we don't understand well yet is why debiasing methods have\nvarying impacts on downstream tasks and how debiasing techniques affect\ninternal components of language models, i.e., neurons, layers, and attentions.\nIn this paper, we decompose the internal mechanisms of debiasing language\nmodels with respect to gender by applying causal mediation analysis to\nunderstand the influence of debiasing methods on toxicity detection as a\ndownstream task. Our findings suggest a need to test the effectiveness of\ndebiasing methods with different bias metrics, and to focus on changes in the\nbehavior of certain components of the models, e.g.,first two layers of language\nmodels, and attention heads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeoung_S/0/1/0/all/0/1\">Sullam Jeoung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diesner_J/0/1/0/all/0/1\">Jana Diesner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Reinforcement Learning and Distribution Matching for Fine-Tuning Language Models with no Catastrophic Forgetting. (arXiv:2206.00761v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00761","description":"<p>The availability of large pre-trained models is changing the landscape of\nMachine Learning research and practice, moving from a training-from-scratch to\na fine-tuning paradigm. While in some applications the goal is to \"nudge\" the\npre-trained distribution towards preferred outputs, in others it is to steer it\ntowards a different distribution over the sample space. Two main paradigms have\nemerged to tackle this challenge: Reward Maximization (RM) and, more recently,\nDistribution Matching (DM). RM applies standard Reinforcement Learning (RL)\ntechniques, such as Policy Gradients, to gradually increase the reward signal.\nDM prescribes to first make explicit the target distribution that the model is\nfine-tuned to approximate. Here we explore the theoretical connections between\nthe two paradigms, and show that methods such as KL-control developed for RM\ncan also be construed as belonging to DM. We further observe that while DM\ndiffers from RM, it can suffer from similar training difficulties, such as high\ngradient variance. We leverage connections between the two paradigms to import\nthe concept of baseline into DM methods. We empirically validate the benefits\nof adding a baseline on an array of controllable language generation tasks such\nas constraining topic, sentiment, and gender distributions in texts sampled\nfrom a language model. We observe superior performance in terms of constraint\nsatisfaction, stability and sample efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korbak_T/0/1/0/all/0/1\">Tomasz Korbak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsahar_H/0/1/0/all/0/1\">Hady Elsahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruszewski_G/0/1/0/all/0/1\">Germ&#xe1;n Kruszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dymetman_M/0/1/0/all/0/1\">Marc Dymetman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the trade-off between prediction accuracy and interpretability for topic modeling on energetic materials corpora. (arXiv:2206.00773v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00773","description":"<p>As the amount and variety of energetics research increases, machine aware\ntopic identification is necessary to streamline future research pipelines. The\nmakeup of an automatic topic identification process consists of creating\ndocument representations and performing classification. However, the\nimplementation of these processes on energetics research imposes new\nchallenges. Energetics datasets contain many scientific terms that are\nnecessary to understand the context of a document but may require more complex\ndocument representations. Secondly, the predictions from classification must be\nunderstandable and trusted by the chemists within the pipeline. In this work,\nwe study the trade-off between prediction accuracy and interpretability by\nimplementing three document embedding methods that vary in computational\ncomplexity. With our accuracy results, we also introduce local interpretability\nmodel-agnostic explanations (LIME) of each prediction to provide a localized\nunderstanding of each prediction and to validate classifier decisions with our\nteam of energetics experts. This study was carried out on a novel labeled\nenergetics dataset created and validated by our team of energetics experts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puerto_M/0/1/0/all/0/1\">Monica Puerto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellett_M/0/1/0/all/0/1\">Mason Kellett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikopoulou_R/0/1/0/all/0/1\">Rodanthi Nikopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuge_M/0/1/0/all/0/1\">Mark D. Fuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doherty_R/0/1/0/all/0/1\">Ruth Doherty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_P/0/1/0/all/0/1\">Peter W. Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukouvalas_Z/0/1/0/all/0/1\">Zois Boukouvalas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BayesFormer: Transformer with Uncertainty Estimation. (arXiv:2206.00826v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00826","description":"<p>Transformer has become ubiquitous due to its dominant performance in various\nNLP and image processing tasks. However, it lacks understanding of how to\ngenerate mathematically grounded uncertainty estimates for transformer\narchitectures. Models equipped with such uncertainty estimates can typically\nimprove predictive performance, make networks robust, avoid over-fitting and\nused as acquisition function in active learning. In this paper, we introduce\nBayesFormer, a Transformer model with dropouts designed by Bayesian theory. We\nproposed a new theoretical framework to extend the approximate variational\ninference-based dropout to Transformer-based architectures. Through extensive\nexperiments, we validate the proposed architecture in four paradigms and show\nimprovements across the board: language modeling and classification,\nlong-sequence understanding, machine translation and acquisition function for\nactive learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sankararaman_K/0/1/0/all/0/1\">Karthik Abinav Sankararaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sinong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Han Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSTR: Too Short to Represent, Summarize with Details! Intro-Guided Extended Summary Generation. (arXiv:2206.00847v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00847","description":"<p>Many scientific papers such as those in arXiv and PubMed data collections\nhave abstracts with varying lengths of 50-1000 words and average length of\napproximately 200 words, where longer abstracts typically convey more\ninformation about the source paper. Up to recently, scientific summarization\nresearch has typically focused on generating short, abstract-like summaries\nfollowing the existing datasets used for scientific summarization. In domains\nwhere the source text is relatively long-form, such as in scientific documents,\nsuch summary is not able to go beyond the general and coarse overview and\nprovide salient information from the source document. The recent interest to\ntackle this problem motivated curation of scientific datasets, arXiv-Long and\nPubMed-Long, containing human-written summaries of 400-600 words, hence,\nproviding a venue for research in generating long/extended summaries. Extended\nsummaries facilitate a faster read while providing details beyond coarse\ninformation. In this paper, we propose TSTR, an extractive summarizer that\nutilizes the introductory information of documents as pointers to their salient\ninformation. The evaluations on two existing large-scale extended summarization\ndatasets indicate statistically significant improvement in terms of Rouge and\naverage Rouge (F1) scores (except in one case) as compared to strong baselines\nand state-of-the-art. Comprehensive human evaluations favor our generated\nextended summaries in terms of cohesion and completeness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MentSum: A Resource for Exploring Summarization of Mental Health Online Posts. (arXiv:2206.00856v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00856","description":"<p>Mental health remains a significant challenge of public health worldwide.\nWith increasing popularity of online platforms, many use the platforms to share\ntheir mental health conditions, express their feelings, and seek help from the\ncommunity and counselors. Some of these platforms, such as Reachout, are\ndedicated forums where the users register to seek help. Others such as Reddit\nprovide subreddits where the users publicly but anonymously post their mental\nhealth distress. Although posts are of varying length, it is beneficial to\nprovide a short, but informative summary for fast processing by the counselors.\nTo facilitate research in summarization of mental health online posts, we\nintroduce Mental Health Summarization dataset, MentSum, containing over 24k\ncarefully selected user posts from Reddit, along with their short user-written\nsummary (called TLDR) in English from 43 mental health subreddits. This\ndomain-specific dataset could be of interest not only for generating short\nsummaries on Reddit, but also for generating summaries of posts on the\ndedicated mental health forums such as Reachout. We further evaluate both\nextractive and abstractive state-of-the-art summarization baselines in terms of\nRouge scores, and finally conduct an in-depth human evaluation study of both\nuser-written and system-generated summaries, highlighting challenges in this\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1\">Sajad Sotudeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1\">Nazli Goharian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_Z/0/1/0/all/0/1\">Zachary Young</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Squeezeformer: An Efficient Transformer for Automatic Speech Recognition. (arXiv:2206.00888v1 [eess.AS])","link":"http://arxiv.org/abs/2206.00888","description":"<p>The recently proposed Conformer model has become the de facto backbone model\nfor various downstream speech tasks based on its hybrid attention-convolution\narchitecture that captures both local and global features. However, through a\nseries of systematic studies, we find that the Conformer architecture's design\nchoices are not optimal. After reexamining the design choices for both the\nmacro and micro-architecture of Conformer, we propose the Squeezeformer model,\nwhich consistently outperforms the state-of-the-art ASR models under the same\ntraining schemes. In particular, for the macro-architecture, Squeezeformer\nincorporates (i) the Temporal U-Net structure, which reduces the cost of the\nmulti-head attention modules on long sequences, and (ii) a simpler block\nstructure of feed-forward module, followed up by multi-head attention or\nconvolution modules, instead of the Macaron structure proposed in Conformer.\nFurthermore, for the micro-architecture, Squeezeformer (i) simplifies the\nactivations in the convolutional block, (ii) removes redundant Layer\nNormalization operations, and (iii) incorporates an efficient depth-wise\ndownsampling layer to efficiently sub-sample the input signal. Squeezeformer\nachieves state-of-the-art results of 7.5%, 6.5%, and 6.0% word-error-rate on\nLibrispeech test-other without external language models. This is 3.1%, 1.4%,\nand 0.6% better than Conformer-CTC with the same number of FLOPs. Our code is\nopen-sourced and available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shaw_A/0/1/0/all/0/1\">Albert Shaw</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_N/0/1/0/all/0/1\">Nicholas Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mangalam_K/0/1/0/all/0/1\">Karttikeya Mangalam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahoney_M/0/1/0/all/0/1\">Michael W. Mahoney</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralSympCheck: A Symptom Checking and Disease Diagnostic Neural Model with Logic Regularization. (arXiv:2206.00906v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00906","description":"<p>The symptom checking systems inquire users for their symptoms and perform a\nrapid and affordable medical assessment of their condition. The basic symptom\nchecking systems based on Bayesian methods, decision trees, or information gain\nmethods are easy to train and do not require significant computational\nresources. However, their drawbacks are low relevance of proposed symptoms and\ninsufficient quality of diagnostics. The best results on these tasks are\nachieved by reinforcement learning models. Their weaknesses are the difficulty\nof developing and training such systems and limited applicability to cases with\nlarge and sparse decision spaces. We propose a new approach based on the\nsupervised learning of neural models with logic regularization that combines\nthe advantages of the different methods. Our experiments on real and synthetic\ndata show that the proposed approach outperforms the best existing methods in\nthe accuracy of diagnosis when the number of diagnoses and symptoms is large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nesterov_A/0/1/0/all/0/1\">Aleksandr Nesterov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibragimov_B/0/1/0/all/0/1\">Bulat Ibragimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umerenkov_D/0/1/0/all/0/1\">Dmitriy Umerenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelmanov_A/0/1/0/all/0/1\">Artem Shelmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubkova_G/0/1/0/all/0/1\">Galina Zubkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokh_V/0/1/0/all/0/1\">Vladimir Kokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The ParlaSent-BCS dataset of sentiment-annotated parliamentary debates from Bosnia-Herzegovina, Croatia, and Serbia. (arXiv:2206.00929v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00929","description":"<p>Expression of sentiment in parliamentary debates is deemed to be\nsignificantly different from that on social media or in product reviews. This\npaper adds to an emerging body of research on parliamentary debates with a\ndataset of sentences annotated for detection sentiment polarity in political\ndiscourse. We sample the sentences for annotation from the proceedings of three\nSoutheast European parliaments: Croatia, Bosnia-Herzegovina, and Serbia. A\nsix-level schema is applied to the data with the aim of training a\nclassification model for the detection of sentiment in parliamentary\nproceedings. Krippendorff's alpha measuring the inter-annotator agreement\nranges from 0.6 for the six-level annotation schema to 0.75 for the three-level\nschema and 0.83 for the two-level schema. Our initial experiments on the\ndataset show that transformer models perform significantly better than those\nusing a simpler architecture. Furthermore, regardless of the similarity of the\nthree languages, we observe differences in performance across different\nlanguages. Performing parliament-specific training and evaluation shows that\nthe main reason for the differing performance between parliaments seems to be\nthe different complexity of the automatic classification task, which is not\nobservable in annotator performance. Language distance does not seem to play\nany role neither in annotator nor in automatic classification performance. We\nrelease the dataset and the best-performing model under permissive licences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mochtak_M/0/1/0/all/0/1\">Michal Mochtak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rupnik_P/0/1/0/all/0/1\">Peter Rupnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ljubesic_N/0/1/0/all/0/1\">Nikola Ljube&#x161;i&#x10d;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Language Selection for Zero-Shot Cross-Lingual Abusive Language Detection. (arXiv:2206.00962v1 [cs.CL])","link":"http://arxiv.org/abs/2206.00962","description":"<p>We study the selection of transfer languages for automatic abusive language\ndetection. Instead of preparing a dataset for every language, we demonstrate\nthe effectiveness of cross-lingual transfer learning for zero-shot abusive\nlanguage detection. This way we can use existing data from higher-resource\nlanguages to build better detection systems for low-resource languages. Our\ndatasets are from seven different languages from three language families. We\nmeasure the distance between the languages using several language similarity\nmeasures, especially by quantifying the World Atlas of Language Structures. We\nshow that there is a correlation between linguistic similarity and classifier\nperformance. This discovery allows us to choose an optimal transfer language\nfor zero shot abusive language detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eronen_J/0/1/0/all/0/1\">Juuso Eronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ptaszynski_M/0/1/0/all/0/1\">Michal Ptaszynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masui_F/0/1/0/all/0/1\">Fumito Masui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arata_M/0/1/0/all/0/1\">Masaki Arata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leliwa_G/0/1/0/all/0/1\">Gniewosz Leliwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroczynski_M/0/1/0/all/0/1\">Michal Wroczynski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-BEiT: Generative Vision-Language Pretraining. (arXiv:2206.01127v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01127","description":"<p>We introduce a vision-language foundation model called VL-BEiT, which is a\nbidirectional multimodal Transformer learned by generative pretraining. Our\nminimalist solution conducts masked prediction on both monomodal and multimodal\ndata with a shared Transformer. Specifically, we perform masked vision-language\nmodeling on image-text pairs, masked language modeling on texts, and masked\nimage modeling on images. VL-BEiT is learned from scratch with one unified\npretraining task, one shared backbone, and one-stage training. Our method is\nconceptually simple and empirically effective. Experimental results show that\nVL-BEiT obtains strong results on various vision-language benchmarks, such as\nvisual question answering, visual reasoning, and image-text retrieval.\nMoreover, our method learns transferable visual features, achieving competitive\nperformance on image classification, and semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vygotskian Autotelic Artificial Intelligence: Language and Culture Internalization for Human-Like AI. (arXiv:2206.01134v1 [cs.AI])","link":"http://arxiv.org/abs/2206.01134","description":"<p>Building autonomous artificial agents able to grow open-ended repertoires of\nskills is one of the fundamental goals of AI. To that end, a promising\ndevelopmental approach recommends the design of intrinsically motivated agents\nthat learn new skills by generating and pursuing their own goals - autotelic\nagents. However, existing algorithms still show serious limitations in terms of\ngoal diversity, exploration, generalization or skill composition. This\nperspective calls for the immersion of autotelic agents into rich\nsocio-cultural worlds. We focus on language especially, and how its structure\nand content may support the development of new cognitive functions in\nartificial agents, just like it does in humans. Indeed, most of our skills\ncould not be learned in isolation. Formal education teaches us to reason\nsystematically, books teach us history, and YouTube might teach us how to cook.\nCrucially, our values, traditions, norms and most of our goals are cultural in\nessence. This knowledge, and some argue, some of our cognitive functions such\nas abstraction, compositional imagination or relational thinking, are formed\nthrough linguistic and cultural interactions. Inspired by the work of Vygotsky,\nwe suggest the design of Vygotskian autotelic agents able to interact with\nothers and, more importantly, able to internalize these interactions to\ntransform them into cognitive tools supporting the development of new cognitive\nfunctions. This perspective paper proposes a new AI paradigm in the quest for\nartificial lifelong skill discovery. It justifies the approach by uncovering\nexamples of new artificial cognitive functions emerging from interactions\nbetween language and embodiment in recent works at the intersection of deep\nreinforcement learning and natural language processing. Looking forward, it\nhighlights future opportunities and challenges for Vygotskian Autotelic AI\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1\">C&#xe9;dric Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karch_T/0/1/0/all/0/1\">Tristan Karch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moulin_Frier_C/0/1/0/all/0/1\">Cl&#xe9;ment Moulin-Frier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation. (arXiv:2206.01137v1 [cs.CL])","link":"http://arxiv.org/abs/2206.01137","description":"<p>General translation models often still struggle to generate accurate\ntranslations in specialized domains. To guide machine translation practitioners\nand characterize the effectiveness of domain adaptation methods under different\ndata availability scenarios, we conduct an in-depth empirical exploration of\nmonolingual and parallel data approaches to domain adaptation of pre-trained,\nthird-party, NMT models in settings where architecture change is impractical.\nWe compare data centric adaptation methods in isolation and combination. We\nstudy method effectiveness in very low resource (8k parallel examples) and\nmoderately low resource (46k parallel examples) conditions and propose an\nensemble approach to alleviate reductions in original domain translation\nquality. Our work includes three domains: consumer electronic, clinical, and\nbiomedical and spans four language pairs - Zh-En, Ja-En, Es-En, and Ru-En. We\nalso make concrete recommendations for achieving high in-domain performance and\nrelease our consumer electronic and medical domain datasets for all languages\nand make our code publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adams_V/0/1/0/all/0/1\">Virginia Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1\">Sandeep Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrzanowski_M/0/1/0/all/0/1\">Mike Chrzanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hrinchuk_O/0/1/0/all/0/1\">Oleksii Hrinchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuchaiev_O/0/1/0/all/0/1\">Oleksii Kuchaiev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01201","description":"<p>This paper revisits visual representation in knowledge-based visual question\nanswering (VQA) and demonstrates that using regional information in a better\nway can significantly improve the performance. While visual representation is\nextensively studied in traditional VQA, it is under-explored in knowledge-based\nVQA even though these two tasks share the common spirit, i.e., rely on visual\ninput to answer the question. Specifically, we observe that in most\nstate-of-the-art knowledge-based VQA methods: 1) visual features are extracted\neither from the whole image or in a sliding window manner for retrieving\nknowledge, and the important relationship within/among object regions is\nneglected; 2) visual features are not well utilized in the final answering\nmodel, which is counter-intuitive to some extent. Based on these observations,\nwe propose a new knowledge-based VQA method REVIVE, which tries to utilize the\nexplicit information of object regions not only in the knowledge retrieval\nstage but also in the answering model. The key motivation is that object\nregions and inherent relationships are important for knowledge-based VQA. We\nperform extensive experiments on the standard OK-VQA dataset and achieve new\nstate-of-the-art performance, i.e., 58.0% accuracy, surpassing previous\nstate-of-the-art method by a large margin (+3.6%). We also conduct detailed\nanalysis and show the necessity of regional information in different framework\ncomponents for knowledge-based VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillBot: Identifying Risky Content for Children in Alexa Skills. (arXiv:2102.03382v2 [cs.MA] UPDATED)","link":"http://arxiv.org/abs/2102.03382","description":"<p>Many households include children who use voice personal assistants (VPA) such\nas Amazon Alexa. Children benefit from the rich functionalities of VPAs and\nthird-party apps but are also exposed to new risks in the VPA ecosystem. In\nthis paper, we first investigate \"risky\" child-directed voice apps that contain\ninappropriate content or ask for personal information through voice\ninteractions. We build SkillBot - a natural language processing (NLP)-based\nsystem to automatically interact with VPA apps and analyze the resulting\nconversations. We find 28 risky child-directed apps and maintain a growing\ndataset of 31,966 non-overlapping app behaviors collected from 3,434 Alexa\napps. Our findings suggest that although child-directed VPA apps are subject to\nstricter policy requirements and more intensive vetting, children remain\nvulnerable to inappropriate content and privacy violations. We then conduct a\nuser study showing that parents are concerned about the identified risky apps.\nMany parents do not believe that these apps are available and designed for\nfamilies/kids, although these apps are actually published in Amazon's \"Kids\"\nproduct category. We also find that parents often neglect basic precautions\nsuch as enabling parental controls on Alexa devices. Finally, we identify a\nnovel risk in the VPA ecosystem: confounding utterances, or voice commands\nshared by multiple apps that may cause a user to interact with a different app\nthan intended. We identify 4,487 confounding utterances, including 581 shared\nby child-directed and non-child-directed apps. We find that 27% of these\nconfounding utterances prioritize invoking a non-child-directed app over a\nchild-directed app. This indicates that children are at real risk of\naccidentally invoking non-child-directed apps due to confounding utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Danny Yuxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apthorpe_N/0/1/0/all/0/1\">Noah Apthorpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ABC: Attention with Bounded-memory Control. (arXiv:2110.02488v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02488","description":"<p>Transformer architectures have achieved state-of-the-art results on a variety\nof sequence modeling tasks. However, their attention mechanism comes with a\nquadratic complexity in sequence lengths, making the computational overhead\nprohibitive, especially for long sequences. Attention context can be seen as a\nrandom-access memory with each token taking a slot. Under this perspective, the\nmemory size grows linearly with the sequence length, and so does the overhead\nof reading from it. One way to improve the efficiency is to bound the memory\nsize. We show that disparate approaches can be subsumed into one abstraction,\nattention with bounded-memory control (ABC), and they vary in their\norganization of the memory. ABC reveals new, unexplored possibilities. First,\nit connects several efficient attention variants that would otherwise seem\napart. Second, this abstraction gives new insights--an established approach\n(Wang et al., 2020b) previously thought to be not applicable in causal\nattention, actually is. Last, we present a new instance of ABC, which draws\ninspiration from existing ABC approaches, but replaces their heuristic\nmemory-organizing functions with a learned, contextualized one. Our experiments\non language modeling, machine translation, and masked language model finetuning\nshow that our approach outperforms previous efficient attention models;\ncompared to the strong transformer baselines, it significantly improves the\ninference time and space efficiency with no or negligible accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1\">Dani Yogatama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaofeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis and Effect of COVID-19 Pandemic using College SubReddit Data. (arXiv:2112.04351v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04351","description":"<p>Background: The COVID-19 pandemic has affected our society and human\nwell-being in various ways. In this study, we investigate how the pandemic has\ninfluenced people's emotions and psychological states compared to a\npre-pandemic period using real-world data from social media.\n</p>\n<p>Method: We collected Reddit social media data from 2019 (pre-pandemic) and\n2020 (pandemic) from the subreddits communities associated with eight\nuniversities. We applied the pre-trained Robustly Optimized BERT pre-training\napproach (RoBERTa) to learn text embedding from the Reddit messages, and\nleveraged the relational information among posted messages to train a graph\nattention network (GAT) for sentiment classification. Finally, we applied model\nstacking to combine the prediction probabilities from RoBERTa and GAT to yield\nthe final classification on sentiment. With the model-predicted sentiment\nlabels on the collected data, we used a generalized linear mixed-effects model\nto estimate the effects of pandemic and in-person teaching during the pandemic\non sentiment.\n</p>\n<p>Results: The results suggest that the odds of negative sentiments in 2020\n(pandemic) were 25.7% higher than the odds in 2019 (pre-pandemic) with a\n$p$-value $&lt;0.001$; and the odds of negative sentiments associated in-person\nlearning were 48.3% higher than with remote learning in 2020 with a $p$-value\nof 0.029.\n</p>\n<p>Conclusions: Our study results are consistent with the findings in the\nliterature on the negative impacts of the pandemic on people's emotions and\npsychological states. Our study contributes to the growing real-world evidence\non the various negative impacts of the pandemic on our society; it also\nprovides a good example of using both ML techniques and statistical modeling\nand inference to make better use of real-world data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Augmented Language Models for Cause-Effect Relation Classification. (arXiv:2112.08615v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08615","description":"<p>Previous studies have shown the efficacy of knowledge augmentation methods in\npretrained language models. However, these methods behave differently across\ndomains and downstream tasks. In this work, we investigate the augmentation of\npretrained language models with commonsense knowledge in the cause-effect\nrelation classification and commonsense causal reasoning tasks. After\nautomatically verbalizing ATOMIC2020, a wide coverage commonsense reasoning\nknowledge graph, and GLUCOSE, a dataset of implicit commonsense causal\nknowledge, we continually pretrain BERT and RoBERTa with the verbalized data.\nThen we evaluate the resulting models on cause-effect pair classification and\nanswering commonsense causal reasoning questions. Our results show that\ncontinually pretrained language models augmented with commonsense knowledge\noutperform our baselines on two commonsense causal reasoning benchmarks, COPA\nand BCOPA-CE, and the Temporal and Causal Reasoning (TCR) dataset, without\nadditional improvement in model architecture or using quality-enhanced data for\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_P/0/1/0/all/0/1\">Pedram Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broniatowski_D/0/1/0/all/0/1\">David A. Broniatowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona Diab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inherently Explainable Reinforcement Learning in Natural Language. (arXiv:2112.08907v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2112.08907","description":"<p>We focus on the task of creating a reinforcement learning agent that is\ninherently explainable -- with the ability to produce immediate local\nexplanations by thinking out loud while performing a task and analyzing entire\ntrajectories post-hoc to produce causal explanations. This Hierarchically\nExplainable Reinforcement Learning agent (HEX-RL), operates in Interactive\nFictions, text-based game environments in which an agent perceives and acts\nupon the world using textual natural language. These games are usually\nstructured as puzzles or quests with long-term dependencies in which an agent\nmust complete a sequence of actions to succeed -- providing ideal environments\nin which to test an agent's ability to explain its actions. Our agent is\ndesigned to treat explainability as a first-class citizen, using an extracted\nsymbolic knowledge graph-based state representation coupled with a Hierarchical\nGraph Attention mechanism that points to the facts in the internal graph\nrepresentation that most influenced the choice of actions. Experiments show\nthat this agent provides significantly improved explanations over strong\nbaselines, as rated by human participants generally unfamiliar with the\nenvironment, while also matching state-of-the-art task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiangyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark O. Riedl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1\">Prithviraj Ammanabrolu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebGPT: Browser-assisted question-answering with human feedback. (arXiv:2112.09332v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.09332","description":"<p>We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. By setting up the task so that it can be performed by humans, we are able\nto train models on the task using imitation learning, and then optimize answer\nquality with human feedback. To make human evaluation of factual accuracy\neasier, models must collect references while browsing in support of their\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\ncloning, and then performing rejection sampling against a reward model trained\nto predict human preferences. This model's answers are preferred by humans 56%\nof the time to those of our human demonstrators, and 69% of the time to the\nhighest-voted answer from Reddit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakano_R/0/1/0/all/0/1\">Reiichiro Nakano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balaji_S/0/1/0/all/0/1\">Suchir Balaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Christina Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hesse_C/0/1/0/all/0/1\">Christopher Hesse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Shantanu Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosaraju_V/0/1/0/all/0/1\">Vineet Kosaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cobbe_K/0/1/0/all/0/1\">Karl Cobbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eloundou_T/0/1/0/all/0/1\">Tyna Eloundou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1\">Gretchen Krueger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Button_K/0/1/0/all/0/1\">Kevin Button</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knight_M/0/1/0/all/0/1\">Matthew Knight</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chess_B/0/1/0/all/0/1\">Benjamin Chess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Trained Language Models for Interactive Decision-Making. (arXiv:2202.01771v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01771","description":"<p>Language model (LM) pre-training is useful in many language processing tasks.\nBut can pre-trained LMs be further leveraged for more general machine learning\nproblems? We propose an approach for using LMs to scaffold learning and\ngeneralization in general sequential decision-making problems. In this\napproach, goals and observations are represented as a sequence of embeddings,\nand a policy network initialized with a pre-trained LM predicts the next\naction. We demonstrate that this framework enables effective combinatorial\ngeneralization across different environments and supervisory modalities. We\nbegin by assuming access to a set of expert demonstrations, and show that\ninitializing policies with LMs and fine-tuning them via behavior cloning\nimproves task completion rates by 43.6% in the VirtualHome environment. We then\nexamine how our framework may be used in environments without pre-collected\nexpert data. To do this, we integrate an active data gathering procedure into\npre-trained LMs. The agent iteratively learns by interacting with the\nenvironment, relabeling the language goal of past 'failed' experiences, and\nupdating the policy in a self-supervised loop. The active data gathering\nprocedure also enables effective combinatorial generalization, outperforming\nthe best baseline by 25.1%. Finally, we explain these results by investigating\nthree possible factors underlying the effectiveness of the LM-based policy. We\nfind that sequential input representations (vs. fixed-dimensional feature\nvectors) and favorable weight initialization are both important for\ngeneralization. Surprisingly, however, the format of the policy inputs encoding\n(e.g. as a natural language string vs. an arbitrary sequential encoding) has\nlittle influence. Together, these results suggest that language modeling\ninduces representations that are useful for modeling not just language, but\nalso goals and plans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puig_X/0/1/0/all/0/1\">Xavier Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1\">Chris Paxton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clinton Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Linxi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">De-An Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1\">Ekin Aky&#xfc;rek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What are the best systems? New perspectives on NLP Benchmarking. (arXiv:2202.03799v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.03799","description":"<p>In Machine Learning, a benchmark refers to an ensemble of datasets associated\nwith one or multiple metrics together with a way to aggregate different systems\nperformances. They are instrumental in (i) assessing the progress of new\nmethods along different axes and (ii) selecting the best systems for practical\nuse. This is particularly the case for NLP with the development of large\npre-trained models (e.g. GPT, BERT) that are expected to generalize well on a\nvariety of tasks. While the community mainly focused on developing new datasets\nand metrics, there has been little interest in the aggregation procedure, which\nis often reduced to a simple average over various performance measures.\nHowever, this procedure can be problematic when the metrics are on a different\nscale, which may lead to spurious conclusions. This paper proposes a new\nprocedure to rank systems based on their performance across different tasks.\nMotivated by the social choice theory, the final system ordering is obtained\nthrough aggregating the rankings induced by each task and is theoretically\ngrounded. We conduct extensive numerical experiments (on over 270k scores) to\nassess the soundness of our approach both on synthetic and real scores (e.g.\nGLUE, EXTREM, SEVAL, TAC, FLICKR). In particular, we show that our method\nyields different conclusions on state-of-the-art systems than the\nmean-aggregation procedure while being both more reliable and robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noiry_N/0/1/0/all/0/1\">Nathan Noiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irurozki_E/0/1/0/all/0/1\">Ekhine Irurozki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clemencon_S/0/1/0/all/0/1\">Stephan Clemencon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locating and Editing Factual Associations in GPT. (arXiv:2202.05262v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.05262","description":"<p>We analyze the storage and recall of factual associations in autoregressive\ntransformer language models, finding evidence that these associations\ncorrespond to localized, directly-editable computations. We first develop a\ncausal intervention for identifying neuron activations that are decisive in a\nmodel's factual predictions. This reveals a distinct set of steps in\nmiddle-layer feed-forward modules that mediate factual predictions while\nprocessing subject tokens. To test our hypothesis that these computations\ncorrespond to factual association recall, we modify feed-forward weights to\nupdate specific factual associations using Rank-One Model Editing (ROME). We\nfind that ROME is effective on a standard zero-shot relation extraction (zsRE)\nmodel-editing task, comparable to existing methods. To perform a more sensitive\nevaluation, we also evaluate ROME on a new dataset of counterfactual\nassertions, on which it simultaneously maintains both specificity and\ngeneralization, whereas other methods sacrifice one or another. Our results\nconfirm an important role for mid-layer feed-forward modules in storing factual\nassociations and suggest that direct manipulation of computational mechanisms\nmay be a feasible approach for model editing. The code, dataset,\nvisualizations, and an interactive demo notebook are available at\nhttps://rome.baulab.info/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_K/0/1/0/all/0/1\">Kevin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Conditional Masked Language Pre-training for Neural Machine Translation. (arXiv:2203.09210v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09210","description":"<p>Pre-trained sequence-to-sequence models have significantly improved Neural\nMachine Translation (NMT). Different from prior works where pre-trained models\nusually adopt an unidirectional decoder, this paper demonstrates that\npre-training a sequence-to-sequence model but with a bidirectional decoder can\nproduce notable performance gains for both Autoregressive and\nNon-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked\nlanguage model pre-trained on large-scale bilingual and monolingual corpora in\nmany languages. We also introduce two simple but effective methods to enhance\nthe CeMAT, aligned code-switching &amp; masking and dynamic dual-masking. We\nconduct extensive experiments and show that our CeMAT can achieve significant\nperformance improvement for all scenarios from low- to extremely high-resource\nlanguages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on\naverage for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it\ncan also produce consistent performance gains, i.e., up to +5.3 BLEU. To the\nbest of our knowledge, this is the first work to pre-train a unified model for\nfine-tuning on both NMT tasks. Code, data, and pre-trained models are available\nat https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pengfei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minghao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Modeling of Multi-Domain Multi-Device ASR Systems. (arXiv:2205.06655v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06655","description":"<p>Modern Automatic Speech Recognition (ASR) systems often use a portfolio of\ndomain-specific models in order to get high accuracy for distinct user\nutterance types across different devices. In this paper, we propose an\ninnovative approach that integrates the different per-domain per-device models\ninto a unified model, using a combination of domain embedding, domain experts,\nmixture of experts and adversarial training. We run careful ablation studies to\nshow the benefit of each of these innovations in contributing to the accuracy\nof the overall unified model. Experiments show that our proposed unified\nmodeling approach actually outperforms the carefully tuned per-domain models,\ngiving relative gains of up to 10% over a baseline model with negligible\nincrease in the number of parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1\">Soumyajit Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1\">Swayambhu Nath Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padi_B/0/1/0/all/0/1\">Bharat Padi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1\">Arunasish Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilgi_R/0/1/0/all/0/1\">Raghavendra Bilgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arsikere_H/0/1/0/all/0/1\">Harish Arsikere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shalini Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasamurthy_A/0/1/0/all/0/1\">Ajay Srinivasamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garimella_S/0/1/0/all/0/1\">Sri Garimella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RigoBERTa: A State-of-the-Art Language Model For Spanish. (arXiv:2205.10233v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.10233","description":"<p>This paper presents RigoBERTa, a State-of-the-Art Language Model for Spanish.\nRigoBERTa is trained over a well-curated corpus formed up from different\nsubcorpora with key features. It follows the DeBERTa architecture, which has\nseveral advantages over other architectures of similar size as BERT or RoBERTa.\nRigoBERTa performance is assessed over 13 NLU tasks in comparison with other\navailable Spanish language models, namely, MarIA, BERTIN and BETO. RigoBERTa\noutperformed the three models in 10 out of the 13 tasks, achieving new\n\"State-of-the-Art\" results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Serrano_A/0/1/0/all/0/1\">Alejandro Vaca Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subies_G/0/1/0/all/0/1\">Guillem Garcia Subies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamorano_H/0/1/0/all/0/1\">Helena Montoro Zamorano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Nuria Aldama Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samy_D/0/1/0/all/0/1\">Doaa Samy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David Betancur Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandoval_A/0/1/0/all/0/1\">Antonio Moreno Sandoval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marta Guerrero Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jimenez_A/0/1/0/all/0/1\">Alvaro Barbero Jimenez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"THE-X: Privacy-Preserving Transformer Inference with Homomorphic Encryption. (arXiv:2206.00216v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2206.00216","description":"<p>As more and more pre-trained language models adopt on-cloud deployment, the\nprivacy issues grow quickly, mainly for the exposure of plain-text user data\n(e.g., search history, medical record, bank account). Privacy-preserving\ninference of transformer models is on the demand of cloud service users. To\nprotect privacy, it is an attractive choice to compute only with ciphertext in\nhomomorphic encryption (HE). However, enabling pre-trained models inference on\nciphertext data is difficult due to the complex computations in transformer\nblocks, which are not supported by current HE tools yet. In this work, we\nintroduce $\\textit{THE-X}$, an approximation approach for transformers, which\nenables privacy-preserving inference of pre-trained models developed by popular\nframeworks. $\\textit{THE-X}$ proposes a workflow to deal with complex\ncomputation in transformer networks, including all the non-polynomial functions\nlike GELU, softmax, and LayerNorm. Experiments reveal our proposed\n$\\textit{THE-X}$ can enable transformer inference on encrypted data for\ndifferent downstream tasks, all with negligible performance drop but enjoying\nthe theory-guaranteed privacy-preserving advantage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Context-Driven Detection of Invertebrate Species in Deep-Sea Video. (arXiv:2206.00718v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00718","description":"<p>Each year, underwater remotely operated vehicles (ROVs) collect thousands of\nhours of video of unexplored ocean habitats revealing a plethora of information\nregarding biodiversity on Earth. However, fully utilizing this information\nremains a challenge as proper annotations and analysis require trained\nscientists time, which is both limited and costly. To this end, we present a\nDataset for Underwater Substrate and Invertebrate Analysis (DUSIA), a benchmark\nsuite and growing large-scale dataset to train, validate, and test methods for\ntemporally localizing four underwater substrates as well as temporally and\nspatially localizing 59 underwater invertebrate species. DUSIA currently\nincludes over ten hours of footage across 25 videos captured in 1080p at 30 fps\nby an ROV following pre planned transects across the ocean floor near the\nChannel Islands of California. Each video includes annotations indicating the\nstart and end times of substrates across the video in addition to counts of\nspecies of interest. Some frames are annotated with precise bounding box\nlocations for invertebrate species of interest, as seen in Figure 1. To our\nknowledge, DUSIA is the first dataset of its kind for deep sea exploration,\nwith video from a moving camera, that includes substrate annotations and\ninvertebrate species that are present at significant depths where sunlight does\nnot penetrate. Additionally, we present the novel context-driven object\ndetector (CDD) where we use explicit substrate classification to influence an\nobject detection network to simultaneously predict a substrate and species\nclass influenced by that substrate. We also present a method for improving\ntraining on partially annotated bounding box frames. Finally, we offer a\nbaseline method for automating the counting of invertebrate species of\ninterest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McEver_R/0/1/0/all/0/1\">R. Austin McEver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levenson_C/0/1/0/all/0/1\">Connor Levenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1\">A S M Iftekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1\">B.S. Manjunath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Distillation using Neural Feature Regression. (arXiv:2206.00719v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00719","description":"<p>Dataset distillation aims to learn a small synthetic dataset that preserves\nmost of the information from the original dataset. Dataset distillation can be\nformulated as a bi-level meta-learning problem where the outer loop optimizes\nthe meta-dataset and the inner loop trains a model on the distilled data.\nMeta-gradient computation is one of the key challenges in this formulation, as\ndifferentiating through the inner loop learning procedure introduces\nsignificant computation and memory costs. In this paper, we address these\nchallenges using neural Feature Regression with Pooling (FRePo), achieving the\nstate-of-the-art performance with an order of magnitude less memory requirement\nand two orders of magnitude faster training than previous methods. The proposed\nalgorithm is analogous to truncated backpropagation through time with a pool of\nmodels to alleviate various types of overfitting in dataset distillation. FRePo\nsignificantly outperforms the previous methods on CIFAR100, Tiny ImageNet, and\nImageNet-1K. Furthermore, we show that high-quality distilled data can greatly\nimprove various downstream applications, such as continual learning and\nmembership inference defense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongchao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nezhadarya_E/0/1/0/all/0/1\">Ehsan Nezhadarya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1\">Jimmy Ba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascaded Video Generation for Videos In-the-Wild. (arXiv:2206.00735v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00735","description":"<p>Videos can be created by first outlining a global view of the scene and then\nadding local details. Inspired by this idea we propose a cascaded model for\nvideo generation which follows a coarse to fine approach. First our model\ngenerates a low resolution video, establishing the global scene structure,\nwhich is then refined by subsequent cascade levels operating at larger\nresolutions. We train each cascade level sequentially on partial views of the\nvideos, which reduces the computational complexity of our model and makes it\nscalable to high-resolution videos with many frames. We empirically validate\nour approach on UCF101 and Kinetics-600, for which our model is competitive\nwith the state-of-the-art. We further demonstrate the scaling capabilities of\nour model and train a three-level model on the BDD100K dataset which generates\n256x256 pixels videos with 48 frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Castrejon_L/0/1/0/all/0/1\">Lluis Castrejon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Multiplicative Filter Networks for Multiscale Reconstruction. (arXiv:2206.00746v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00746","description":"<p>Coordinate networks like Multiplicative Filter Networks (MFNs) and BACON\noffer some control over the frequency spectrum used to represent continuous\nsignals such as images or 3D volumes. Yet, they are not readily applicable to\nproblems for which coarse-to-fine estimation is required, including various\ninverse problems in which coarse-to-fine optimization plays a key role in\navoiding poor local minima. We introduce a new coordinate network architecture\nand training scheme that enables coarse-to-fine optimization with fine-grained\ncontrol over the frequency support of learned reconstructions. This is achieved\nwith two key innovations. First, we incorporate skip connections so that\nstructure at one scale is preserved when fitting finer-scale structure. Second,\nwe propose a novel initialization scheme to provide control over the model\nfrequency spectrum at each stage of optimization. We demonstrate how these\nmodifications enable multiscale optimization for coarse-to-fine fitting to\nnatural images. We then evaluate our model on synthetically generated datasets\nfor the the problem of single-particle cryo-EM reconstruction. We learn high\nresolution multiscale structures, on par with the state-of-the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekarforoush_S/0/1/0/all/0/1\">Shayan Shekarforoush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1\">David B. Lindell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Linear Transformer for 3D Biomedical Image Segmentation. (arXiv:2206.00771v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00771","description":"<p>Transformer-based neural networks have surpassed promising performance on\nmany biomedical image segmentation tasks due to a better global information\nmodeling from the self-attention mechanism. However, most methods are still\ndesigned for 2D medical images while ignoring the essential 3D volume\ninformation. The main challenge for 3D transformer-based segmentation methods\nis the quadratic complexity introduced by the self-attention mechanism\n\\cite{vaswani2017attention}. In this paper, we propose a novel transformer\narchitecture for 3D medical image segmentation using an encoder-decoder style\narchitecture with linear complexity. Furthermore, we newly introduce a dynamic\ntoken concept to further reduce the token numbers for self-attention\ncalculation. Taking advantage of the global information modeling, we provide\nuncertainty maps from different hierarchy stages. We evaluate this method on\nmultiple challenging CT pancreas segmentation datasets. Our promising results\nshow that our novel 3D Transformer-based segmentor could provide promising\nhighly feasible segmentation performance and accurate uncertainty\nquantification using single annotation. Code is available\nhttps://github.com/freshman97/LinTransUNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delivering Document Conversion as a Cloud Service with High Throughput and Responsiveness. (arXiv:2206.00785v1 [cs.DL])","link":"http://arxiv.org/abs/2206.00785","description":"<p>Document understanding is a key business process in the data-driven economy\nsince documents are central to knowledge discovery and business insights.\nConverting documents into a machine-processable format is a particular\nchallenge here due to their huge variability in formats and complex structure.\nAccordingly, many algorithms and machine-learning methods emerged to solve\nparticular tasks such as Optical Character Recognition (OCR), layout analysis,\ntable-structure recovery, figure understanding, etc. We observe the adoption of\nsuch methods in document understanding solutions offered by all major cloud\nproviders. Yet, publications outlining how such services are designed and\noptimized to scale in the cloud are scarce. In this paper, we focus on the case\nof document conversion to illustrate the particular challenges of scaling a\ncomplex data processing pipeline with a strong reliance on machine-learning\nmethods on cloud infrastructure. Our key objective is to achieve high\nscalability and responsiveness for different workload profiles in a\nwell-defined resource budget. We outline the requirements, design, and\nimplementation choices of our document conversion service and reflect on the\nchallenges we faced. Evidence for the scaling behavior and resource efficiency\nis provided for two alternative workload distribution strategies and deployment\nconfigurations. Our best-performing method achieves sustained throughput of\nover one million PDF pages per hour on 3072 CPU cores across 192 nodes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Auer_C/0/1/0/all/0/1\">Christoph Auer</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Dolfi_M/0/1/0/all/0/1\">Michele Dolfi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Carvalho_A/0/1/0/all/0/1\">Andr&#xe9; Carvalho</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Ramis_C/0/1/0/all/0/1\">Cesar Berrospi Ramis</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1\">Peter W. J. Staar</a> (1) ((1) IBM Research, (2) SoftINSA Lda.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Self-supervised Vision Pretraining with Local Masked Reconstruction. (arXiv:2206.00790v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00790","description":"<p>Self-supervised learning for computer vision has achieved tremendous progress\nand improved many downstream vision tasks such as image classification,\nsemantic segmentation, and object detection. Among these, generative\nself-supervised vision learning approaches such as MAE and BEiT show promising\nperformance. However, their global masked reconstruction mechanism is\ncomputationally demanding. To address this issue, we propose local masked\nreconstruction (LoMaR), a simple yet effective approach that performs masked\nreconstruction within a small window of 7$\\times$7 patches on a simple\nTransformer encoder, improving the trade-off between efficiency and accuracy\ncompared to global masked reconstruction over the entire image. Extensive\nexperiments show that LoMaR reaches 84.1% top-1 accuracy on ImageNet-1K\nclassification, outperforming MAE by 0.5%. After finetuning the pretrained\nLoMaR on 384$\\times$384 images, it can reach 85.4% top-1 accuracy, surpassing\nMAE by 0.6%. On MS COCO, LoMaR outperforms MAE by 0.5 $\\text{AP}^\\text{box}$ on\nobject detection and 0.5 $\\text{AP}^\\text{mask}$ on instance segmentation.\nLoMaR is especially more computation-efficient on pretraining high-resolution\nimages, e.g., it is 3.1$\\times$ faster than MAE with 0.2% higher classification\naccuracy on pretraining 448$\\times$448 images. This local masked reconstruction\nlearning mechanism can be easily integrated into any other generative\nself-supervised learning approach. Our code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Ming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-scale frequency separation network for image deblurring. (arXiv:2206.00798v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00798","description":"<p>Image deblurring aims to restore the detailed texture information or\nstructures from the blurry images, which has become an indispensable step in\nmany computer-vision tasks. Although various methods have been proposed to deal\nwith the image deblurring problem, most of them treated the blurry image as a\nwhole and neglected the characteristics of different image frequencies. In this\npaper, we present a new method called multi-scale frequency separation network\n(MSFS-Net) for image deblurring. MSFS-Net introduces the frequency separation\nmodule (FSM) into an encoder-decoder network architecture to capture the low\nand high-frequency information of image at multiple scales. Then, a simple\ncycle-consistency strategy and a sophisticated contrastive learning module\n(CLM) are respectively designed to retain the low-frequency information and\nrecover the high-frequency information during deblurring. At last, the features\nof different scales are fused by a cross-scale feature fusion module (CSFFM).\nExtensive experiments on benchmark datasets show that the proposed network\nachieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanni Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Miao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Di Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_J/0/1/0/all/0/1\">Jun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianzhong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CcHarmony: Color-checker based Image Harmonization Dataset. (arXiv:2206.00800v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00800","description":"<p>Image harmonization targets at adjusting the foreground in a composite image\nto make it compatible with the background, producing a more realistic and\nharmonious image. Training deep image harmonization network requires abundant\ntraining data, but it is extremely difficult to acquire training pairs of\ncomposite images and ground-truth harmonious images. Therefore, existing works\nturn to adjust the foreground appearance in a real image to create a synthetic\ncomposite image. However, such adjustment may not faithfully reflect the\nnatural illumination change of foreground. In this work, we explore a novel\ntransitive way to construct image harmonization dataset. Specifically, based on\nthe existing datasets with recorded illumination information, we first convert\nthe foreground in a real image to the standard illumination condition, and then\nconvert it to another illumination condition, which is combined with the\noriginal background to form a synthetic composite image. In this manner, we\nconstruct an image harmonization dataset called ccHarmony, which is named after\ncolor checker (cc). The dataset is available at\nhttps://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoxu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XBound-Former: Toward Cross-scale Boundary Modeling in Transformers. (arXiv:2206.00806v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00806","description":"<p>Skin lesion segmentation from dermoscopy images is of great significance in\nthe quantitative analysis of skin cancers, which is yet challenging even for\ndermatologists due to the inherent issues, i.e., considerable size, shape and\ncolor variation, and ambiguous boundaries. Recent vision transformers have\nshown promising performance in handling the variation through global context\nmodeling. Still, they have not thoroughly solved the problem of ambiguous\nboundaries as they ignore the complementary usage of the boundary knowledge and\nglobal contexts. In this paper, we propose a novel cross-scale boundary-aware\ntransformer, \\textbf{XBound-Former}, to simultaneously address the variation\nand boundary problems of skin lesion segmentation. XBound-Former is a purely\nattention-based network and catches boundary knowledge via three specially\ndesigned learners. We evaluate the model on two skin lesion datasets,\nISIC-2016\\&amp;PH$^2$ and ISIC-2018, where our model consistently outperforms other\nconvolution- and transformer-based models, especially on the boundary-wise\nmetrics. We extensively verify the generalization ability of polyp lesion\nsegmentation that has similar characteristics, and our model can also yield\nsignificant improvement compared to the latest models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yuxi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1\">Zhaodong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_J/0/1/0/all/0/1\">Jianwei Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangdong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qichao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Knowledge from Object Classification to Aesthetics Assessment. (arXiv:2206.00809v1 [cs.MM])","link":"http://arxiv.org/abs/2206.00809","description":"<p>In this work, we point out that the major dilemma of image aesthetics\nassessment (IAA) comes from the abstract nature of aesthetic labels. That is, a\nvast variety of distinct contents can correspond to the same aesthetic label.\nOn the one hand, during inference, the IAA model is required to relate various\ndistinct contents to the same aesthetic label. On the other hand, when\ntraining, it would be hard for the IAA model to learn to distinguish different\ncontents merely with the supervision from aesthetic labels, since aesthetic\nlabels are not directly related to any specific content. To deal with this\ndilemma, we propose to distill knowledge on semantic patterns for a vast\nvariety of image contents from multiple pre-trained object classification (POC)\nmodels to an IAA model. Expecting the combination of multiple POC models can\nprovide sufficient knowledge on various image contents, the IAA model can\neasier learn to relate various distinct contents to a limited number of\naesthetic labels. By supervising an end-to-end single-backbone IAA model with\nthe distilled knowledge, the performance of the IAA model is significantly\nimproved by 4.8% in SRCC compared to the version trained only with ground-truth\naesthetic labels. On specific categories of images, the SRCC improvement\nbrought by the proposed method can achieve up to 7.2%. Peer comparison also\nshows that our method outperforms 10 previous IAA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jingwen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling sRGB Camera Noise with Normalizing Flows. (arXiv:2206.00812v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00812","description":"<p>Noise modeling and reduction are fundamental tasks in low-level computer\nvision. They are particularly important for smartphone cameras relying on small\nsensors that exhibit visually noticeable noise. There has recently been renewed\ninterest in using data-driven approaches to improve camera noise models via\nneural networks. These data-driven approaches target noise present in the\nraw-sensor image before it has been processed by the camera's image signal\nprocessor (ISP). Modeling noise in the RAW-rgb domain is useful for improving\nand testing the in-camera denoising algorithm; however, there are situations\nwhere the camera's ISP does not apply denoising or additional denoising is\ndesired when the RAW-rgb domain image is no longer available. In such cases,\nthe sensor noise propagates through the ISP to the final rendered image encoded\nin standard RGB (sRGB). The nonlinear steps on the ISP culminate in a\nsignificantly more complex noise distribution in the sRGB domain and existing\nraw-domain noise models are unable to capture the sRGB noise distribution. We\npropose a new sRGB-domain noise model based on normalizing flows that is\ncapable of learning the complex noise distribution found in sRGB images under\nvarious ISO levels. Our normalizing flows-based approach outperforms other\nmodels by a large margin in noise modeling and synthesis tasks. We also show\nthat image denoisers trained on noisy images synthesized with our noise model\noutperforms those trained with noise from baselines models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kousha_S/0/1/0/all/0/1\">Shayan Kousha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maleky_A/0/1/0/all/0/1\">Ali Maleky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Cardiac MRI Reconstruction Using Combined Tensor Nuclear Norm and Casorati Matrix Nuclear Norm Regularizations. (arXiv:2206.00831v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00831","description":"<p>Low-rank tensor models have been applied in accelerating dynamic magnetic\nresonance imaging (dMRI). Recently, a new tensor nuclear norm based on t-SVD\nhas been proposed and applied to tensor completion. Inspired by the different\nproperties of the tensor nuclear norm (TNN) and the Casorati matrix nuclear\nnorm (MNN), we introduce a combined TNN and Casorati MNN regularizations\nframework to reconstruct dMRI, which we term as TMNN. The proposed method\nsimultaneously exploits the spatial structure and the temporal correlation of\nthe dynamic MR data. The optimization problem can be efficiently solved by the\nalternating direction method of multipliers (ADMM). In order to further improve\nthe computational efficiency, we develop a fast algorithm under the Cartesian\nsampling scenario. Numerical experiments based on cardiac cine MRI and\nperfusion MRI data demonstrate the performance improvement over the traditional\nCasorati nuclear norm regularization method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks. (arXiv:2206.00843v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00843","description":"<p>Efficient deep neural network (DNN) models equipped with compact operators\n(e.g., depthwise convolutions) have shown great potential in reducing DNNs'\ntheoretical complexity (e.g., the total number of weights/operations) while\nmaintaining a decent model accuracy. However, existing efficient DNNs are still\nlimited in fulfilling their promise in boosting real-hardware efficiency, due\nto their commonly adopted compact operators' low hardware utilization. In this\nwork, we open up a new compression paradigm for developing real-hardware\nefficient DNNs, leading to boosted hardware efficiency while maintaining model\naccuracy. Interestingly, we observe that while some DNN layers' activation\nfunctions help DNNs' training optimization and achievable accuracy, they can be\nproperly removed after training without compromising the model accuracy.\nInspired by this observation, we propose a framework dubbed DepthShrinker,\nwhich develops hardware-friendly compact networks via shrinking the basic\nbuilding blocks of existing efficient DNNs that feature irregular computation\npatterns into dense ones with much improved hardware utilization and thus\nreal-hardware efficiency. Excitingly, our DepthShrinker framework delivers\nhardware-friendly compact networks that outperform both state-of-the-art\nefficient DNNs and compression techniques, e.g., a 3.06\\% higher accuracy and\n1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method\nMetaPruning. Our codes are available at:\nhttps://github.com/RICE-EIC/DepthShrinker.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yonggan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiayi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Meng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1\">Cheng Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamoorthi_R/0/1/0/all/0/1\">Raghuraman Krishnamoorthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_V/0/1/0/all/0/1\">Vikas Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspherical Consistency Regularization. (arXiv:2206.00845v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00845","description":"<p>Recent advances in contrastive learning have enlightened diverse applications\nacross various semi-supervised fields. Jointly training supervised learning and\nunsupervised learning with a shared feature encoder becomes a common scheme.\nThough it benefits from taking advantage of both feature-dependent information\nfrom self-supervised learning and label-dependent information from supervised\nlearning, this scheme remains suffering from bias of the classifier. In this\nwork, we systematically explore the relationship between self-supervised\nlearning and supervised learning, and study how self-supervised learning helps\nrobust data-efficient deep learning. We propose hyperspherical consistency\nregularization (HCR), a simple yet effective plug-and-play method, to\nregularize the classifier using feature-dependent information and thus avoid\nbias from labels. Specifically, HCR first projects logits from the classifier\nand feature projections from the projection head on the respective hypersphere,\nthen it enforces data points on hyperspheres to have similar structures by\nminimizing binary cross entropy of pairwise distances' similarity metrics.\nExtensive experiments on semi-supervised and weakly-supervised learning\ndemonstrate the effectiveness of our method, by showing superior performance\nwith HCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Cheng Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhangyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic MRI using Learned Transform-based Deep Tensor Low-Rank Network (DTLR-Net). (arXiv:2206.00850v1 [eess.IV])","link":"http://arxiv.org/abs/2206.00850","description":"<p>While low-rank matrix prior has been exploited in dynamic MR image\nreconstruction and has obtained satisfying performance, low-rank tensors models\nhave recently emerged as powerful alternative representations for\nthree-dimensional dynamic MR datasets. In this paper, we introduce a\nmodel-based deep learning network by learning the tensor low-rank prior of the\ncardiac dynamic MR images. Instead of representing the dynamic dataset as a\nlow-rank tensor directly, we propose a learned transformation operator to\nexploit the tensor low-rank property in a transform domain. In particular, by\ngeneralizing the t-SVD tensor decomposition into a unitary transformed t-SVD,\nwe define a transformed tensor nuclear norm (TTNN) to enforce the tensor\nlow-rankness. The dynamic MRI reconstruction problem is thus formulated using a\nTTNN regularized optimization problem. An iterative algorithm based on ADMM\nused to minimize the cost is unrolled into a deep network, where the transform\nis learned using convolutional neural networks (CNNs) to promote the\nreconstruction quality in the feature domain. Experimental results on cardiac\ncine MRI reconstruction demonstrate that the proposed framework is able to\nprovide improved recovery results compared with the state-of-the-art\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinghao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled Generation Network for Enlarged License Plate Recognition and A Unified Dataset. (arXiv:2206.00859v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00859","description":"<p>License plate recognition plays a critical role in many practical\napplications, but license plates of large vehicles are difficult to be\nrecognized due to the factors of low resolution, contamination, low\nillumination, and occlusion, to name a few. To overcome the above factors, the\ntransportation management department generally introduces the enlarged license\nplate behind the rear of a vehicle. However, enlarged license plates have high\ndiversity as they are non-standard in position, size, and style. Furthermore,\nthe background regions contain a variety of noisy information which greatly\ndisturbs the recognition of license plate characters. Existing works have not\nstudied this challenging problem. In this work, we first address the enlarged\nlicense plate recognition problem and contribute a dataset containing 9342\nimages, which cover most of the challenges of real scenes. However, the created\ndata are still insufficient to train deep methods of enlarged license plate\nrecognition, and building large-scale training data is very time-consuming and\nhigh labor cost. To handle this problem, we propose a novel task-level\ndisentanglement generation framework based on the Disentangled Generation\nNetwork (DGNet), which disentangles the generation into the text generation and\nbackground generation in an end-to-end manner to effectively ensure diversity\nand integrity, for robust enlarged license plate recognition. Extensive\nexperiments on the created dataset are conducted, and we demonstrate the\neffectiveness of the proposed approach in three representative text recognition\nframeworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaobin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guohao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1\">Aihua Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Ruoran Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jin Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientNeRF: Efficient Neural Radiance Fields. (arXiv:2206.00878v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00878","description":"<p>Neural Radiance Fields (NeRF) has been wildly applied to various tasks for\nits high-quality representation of 3D scenes. It takes long per-scene training\ntime and per-image testing time. In this paper, we present EfficientNeRF as an\nefficient NeRF-based method to represent 3D scene and synthesize novel-view\nimages. Although several ways exist to accelerate the training or testing\nprocess, it is still difficult to much reduce time for both phases\nsimultaneously. We analyze the density and weight distribution of the sampled\npoints then propose valid and pivotal sampling at the coarse and fine stage,\nrespectively, to significantly improve sampling efficiency. In addition, we\ndesign a novel data structure to cache the whole scene during testing to\naccelerate the rendering speed. Overall, our method can reduce over 88\\% of\ntraining time, reach rendering speed of over 200 FPS, while still achieving\ncompetitive accuracy. Experiments prove that our method promotes the\npracticality of NeRF in the real world and enables many applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yilun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tiancheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Systematic Knowledge of 2D Transformations. (arXiv:2206.00893v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00893","description":"<p>The existing deep learning models suffer from out-of-distribution (o.o.d.)\nperformance drop in computer vision tasks. In comparison, humans have a\nremarkable ability to interpret images, even if the scenes in the images are\nrare, thanks to the systematicity of acquired knowledge. This work focuses on\n1) the acquisition of systematic knowledge of 2D transformations, and 2)\narchitectural components that can leverage the learned knowledge in image\nclassification tasks in an o.o.d. setting. With a new training methodology\nbased on synthetic datasets that are constructed under the causal framework,\nthe deep neural networks acquire knowledge from semantically different domains\n(e.g. even from noise), and exhibit certain level of systematicity in parameter\nestimation experiments. Based on this, a novel architecture is devised\nconsisting of a classifier, an estimator and an identifier (abbreviated as\n\"CED\"). By emulating the \"hypothesis-verification\" process in human visual\nperception, CED improves the classification accuracy significantly on test sets\nunder covariate shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jiachen Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenjing Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangjian He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture Imagery. (arXiv:2206.00897v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00897","description":"<p>Unsustainable fishing practices worldwide pose a major threat to marine\nresources and ecosystems. Identifying vessels that evade monitoring systems --\nknown as \"dark vessels\" -- is key to managing and securing the health of marine\nenvironments. With the rise of satellite-based synthetic aperture radar (SAR)\nimaging and modern machine learning (ML), it is now possible to automate\ndetection of dark vessels day or night, under all-weather conditions. SAR\nimages, however, require domain-specific treatment and is not widely accessible\nto the ML community. Moreover, the objects (vessels) are small and sparse,\nchallenging traditional computer vision approaches. We present the largest\nlabeled dataset for training ML models to detect and characterize vessels from\nSAR. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the\nSentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The\nimages are annotated using a combination of automated and manual analysis.\nCo-located bathymetry and wind state rasters accompany every SAR image. We\nprovide an overview of the results from the xView3 Computer Vision Challenge,\nan international competition using xView3-SAR for ship detection and\ncharacterization at large scale. We release the data (https://iuu.xview.us/)\nand code (https://github.com/DIUx-xView) to support ongoing development and\nevaluation of ML approaches for this important application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paolo_F/0/1/0/all/0/1\">Fernando Paolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsu-ting Tim Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Ritwik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_B/0/1/0/all/0/1\">Bryce Goodman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Nirav Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuster_D/0/1/0/all/0/1\">Daniel Kuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroodsma_D/0/1/0/all/0/1\">David Kroodsma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunnmon_J/0/1/0/all/0/1\">Jared Dunnmon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISSU: 3D Medical Image Segmentation via Self-distilling TransUNet. (arXiv:2206.00902v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00902","description":"<p>U-Nets have achieved tremendous success in medical image segmentation.\nNevertheless, it may suffer limitations in global (long-range) contextual\ninteractions and edge-detail preservation. In contrast, Transformer has an\nexcellent ability to capture long-range dependencies by leveraging the\nself-attention mechanism into the encoder. Although Transformer was born to\nmodel the long-range dependency on the extracted feature maps, it still suffers\nfrom extreme computational and spatial complexities in processing\nhigh-resolution 3D feature maps. This motivates us to design the efficiently\nTransformer-based UNet model and study the feasibility of Transformer-based\nnetwork architectures for medical image segmentation tasks. To this end, we\npropose to self-distill a Transformer-based UNet for medical image\nsegmentation, which simultaneously learns global semantic information and local\nspatial-detailed features. Meanwhile, a local multi-scale fusion block is first\nproposed to refine fine-grained details from the skipped connections in the\nencoder by the main CNN stem through self-distillation, only computed during\ntraining and removed at inference with minimal overhead. Extensive experiments\non BraTS 2019 and CHAOS datasets show that our MISSU achieves the best\nperformance over previous state-of-the-art methods. Code and models are\navailable at \\url{https://github.com/wangn123/MISSU.git}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaohui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask-Guided Divergence Loss Improves the Generalization and Robustness of Deep Neural Network. (arXiv:2206.00913v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00913","description":"<p>Deep neural network (DNN) with dropout can be regarded as an ensemble model\nconsisting of lots of sub-DNNs (i.e., an ensemble sub-DNN where the sub-DNN is\nthe remaining part of the DNN after dropout), and through increasing the\ndiversity of the ensemble sub-DNN, the generalization and robustness of the DNN\ncan be effectively improved. In this paper, a mask-guided divergence loss\nfunction (MDL), which consists of a cross-entropy loss term and an orthogonal\nterm, is proposed to increase the diversity of the ensemble sub-DNN by the\nadded orthogonal term. Particularly, the mask technique is introduced to assist\nin generating the orthogonal term for avoiding overfitting of the diversity\nlearning. The theoretical analysis and extensive experiments on 4 datasets\n(i.e., MNIST, FashionMNIST, CIFAR10, and CIFAR100) manifest that MDL can\nimprove the generalization and robustness of standard training and adversarial\ntraining. For CIFAR10 and CIFAR100, in standard training, the maximum\nimprovement of accuracy is $1.38\\%$ on natural data, $30.97\\%$ on FGSM (i.e.,\nFast Gradient Sign Method) attack, $38.18\\%$ on PGD (i.e., Projected Gradient\nDescent) attack. While in adversarial training, the maximum improvement is\n$1.68\\%$ on natural data, $4.03\\%$ on FGSM attack and $2.65\\%$ on PGD attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiangyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Image Composition for Complex Scene Generation. (arXiv:2206.00923v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00923","description":"<p>We present a method that achieves state-of-the-art results on challenging\n(few-shot) layout-to-image generation tasks by accurately modeling textures,\nstructures and relationships contained in a complex scene. After compressing\nRGB images into patch tokens, we propose the Transformer with Focal Attention\n(TwFA) for exploring dependencies of object-to-object, object-to-patch and\npatch-to-patch. Compared to existing CNN-based and Transformer-based generation\nmodels that entangled modeling on pixel-level&amp;patch-level and\nobject-level&amp;patch-level respectively, the proposed focal attention predicts\nthe current patch token by only focusing on its highly-related tokens that\nspecified by the spatial layout, thereby achieving disambiguation during\ntraining. Furthermore, the proposed TwFA largely increases the data efficiency\nduring training, therefore we propose the first few-shot complex scene\ngeneration strategy based on the well-trained TwFA. Comprehensive experiments\nshow the superiority of our method, which significantly increases both\nquantitative metrics and qualitative visual realism with respect to\nstate-of-the-art CNN-based and transformer-based methods. Code is available at\nhttps://github.com/JohnDreamer/TwFA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zuopeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FACM: Correct the Output of Deep Neural Network with Middle Layers Features against Adversarial Samples. (arXiv:2206.00924v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00924","description":"<p>In the strong adversarial attacks against deep neural network (DNN), the\noutput of DNN will be misclassified if and only if the last feature layer of\nthe DNN is completely destroyed by adversarial samples, while our studies found\nthat the middle feature layers of the DNN can still extract the effective\nfeatures of the original normal category in these adversarial attacks. To this\nend, in this paper, a middle $\\bold{F}$eature layer $\\bold{A}$nalysis and\n$\\bold{C}$onditional $\\bold{M}$atching prediction distribution (FACM) model is\nproposed to increase the robustness of the DNN against adversarial samples\nthrough correcting the output of DNN with the features extracted by the middle\nlayers of DNN. In particular, the middle $\\bold{F}$eature layer\n$\\bold{A}$nalysis (FA) module, the conditional matching prediction distribution\n(CMPD) module and the output decision module are included in our FACM model to\ncollaboratively correct the classification of adversarial samples. The\nexperiments results show that, our FACM model can significantly improve the\nrobustness of the naturally trained model against various attacks, and our FA\nmodel can significantly improve the robustness of the adversarially trained\nmodel against white-box attacks with weak transferability and black box attacks\nwhere FA model includes the FA module and the output decision module, not the\nCMPD module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiangyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Physical Object Properties from Video. (arXiv:2206.00930v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00930","description":"<p>We present a novel approach to estimating physical properties of objects from\nvideo. Our approach consists of a physics engine and a correction estimator.\nStarting from the initial observed state, object behavior is simulated forward\nin time. Based on the simulated and observed behavior, the correction estimator\nthen determines refined physical parameters for each object. The method can be\niterated for increased precision. Our approach is generic, as it allows for the\nuse of an arbitrary - not necessarily differentiable - physics engine and\ncorrection estimator. For the latter, we evaluate both gradient-free\nhyperparameter optimization and a deep convolutional neural network. We\ndemonstrate faster and more robust convergence of the learned method in several\nsimulated 2D scenarios focusing on bin situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Link_M/0/1/0/all/0/1\">Martin Link</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_M/0/1/0/all/0/1\">Max Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Diffusion Models for Inverse Problems using Manifold Constraints. (arXiv:2206.00941v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00941","description":"<p>Recently, diffusion models have been used to solve various inverse problems\nin an unsupervised manner with appropriate modifications to the sampling\nprocess. However, the current solvers, which recursively apply a reverse\ndiffusion step followed by a measurement consistency step, often produce\nsub-optimal results. By studying the generative sampling path, here we show\nthat current solvers throw the sample path off the data manifold, and hence the\nerror accumulates. To address this, we propose an additional correction term\ninspired by the manifold constraint, which can be used synergistically with the\nprevious solvers to make the iterations close to the manifold. The proposed\nmanifold constraint is straightforward to implement within a few lines of code,\nyet boosts the performance by a surprisingly large margin. With extensive\nexperiments, we show that our method is superior to the previous methods both\ntheoretically and empirically, producing promising results in many applications\nsuch as image inpainting, colorization, and sparse-view computed tomography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyungjin Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_B/0/1/0/all/0/1\">Byeongsu Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_D/0/1/0/all/0/1\">Dohoon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Space Particle Inference for Neural Network Ensembles. (arXiv:2206.00944v1 [cs.LG])","link":"http://arxiv.org/abs/2206.00944","description":"<p>Ensembles of deep neural networks demonstrate improved performance over\nsingle models. For enhancing the diversity of ensemble members while keeping\ntheir performance, particle-based inference methods offer a promising approach\nfrom a Bayesian perspective. However, the best way to apply these methods to\nneural networks is still unclear: seeking samples from the weight-space\nposterior suffers from inefficiency due to the over-parameterization issues,\nwhile seeking samples directly from the function-space posterior often results\nin serious underfitting. In this study, we propose optimizing particles in the\nfeature space where the activation of a specific intermediate layer lies to\naddress the above-mentioned difficulties. Our method encourages each member to\ncapture distinct features, which is expected to improve ensemble prediction\nrobustness. Extensive evaluation on real-world datasets shows that our model\nsignificantly outperforms the gold-standard Deep Ensembles on various metrics,\nincluding accuracy, calibration, and robustness. Code is available at\nhttps://github.com/DensoITLab/featurePI .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yashima_S/0/1/0/all/0/1\">Shingo Yashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Teppei Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_K/0/1/0/all/0/1\">Kohta Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Ikuro Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawakami_R/0/1/0/all/0/1\">Rei Kawakami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bhattacharyya Coefficient-Based Framework for Noise Model-Aware Random Walker Image Segmentation. (arXiv:2206.00947v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00947","description":"<p>One well established method of interactive image segmentation is the random\nwalker algorithm. Considerable research on this family of segmentation methods\nhas been continuously conducted in recent years with numerous applications.\nThese methods are common in using a simple Gaussian weight function which\ndepends on a parameter that strongly influences the segmentation performance.\nIn this work we propose a general framework of deriving weight functions based\non probabilistic modeling. This framework can be concretized to cope with\nvirtually any well-defined noise model. It eliminates the critical parameter\nand thus avoids time-consuming parameter search. We derive the specific weight\nfunctions for common noise types and show their superior performance on\nsynthetic data as well as different biomedical image data (MRI images from the\nNYU fastMRI dataset, larvae images acquired with the FIM technique). Our\nframework can also be used in multiple other applications, e.g., the graph cut\nalgorithm and its extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drees_D/0/1/0/all/0/1\">Dominik Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eilers_F/0/1/0/all/0/1\">Florian Eilers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_A/0/1/0/all/0/1\">Ang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseDet: Towards End-to-End 3D Object Detection. (arXiv:2206.00960v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00960","description":"<p>In this paper, we propose SparseDet for end-to-end 3D object detection from\npoint cloud. Existing works on 3D object detection rely on dense object\ncandidates over all locations in a 3D or 2D grid following the mainstream\nmethods for object detection in 2D images. However, this dense paradigm\nrequires expertise in data to fulfill the gap between label and detection. As a\nnew detection paradigm, SparseDet maintains a fixed set of learnable proposals\nto represent latent candidates and directly perform classification and\nlocalization for 3D objects through stacked transformers. It demonstrates that\neffective 3D object detection can be achieved with none of post-processing such\nas redundant removal and non-maximum suppression. With a properly designed\nnetwork, SparseDet achieves highly competitive detection accuracy while running\nwith a more efficient speed of 34.5 FPS. We believe this end-to-end paradigm of\nSparseDet will inspire new thinking on the sparsity of 3D object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhaoyi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bingfeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVM-Cervix: A Hybrid Cervical Pap-Smear Image Classification Framework Using CNN, Visual Transformer and Multilayer Perceptron. (arXiv:2206.00971v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00971","description":"<p>Cervical cancer is the seventh most common cancer among all the cancers\nworldwide and the fourth most common cancer among women. Cervical cytopathology\nimage classification is an important method to diagnose cervical cancer. Manual\nscreening of cytopathology images is time-consuming and error-prone. The\nemergence of the automatic computer-aided diagnosis system solves this problem.\nThis paper proposes a framework called CVM-Cervix based on deep learning to\nperform cervical cell classification tasks. It can analyze pap slides quickly\nand accurately. CVM-Cervix first proposes a Convolutional Neural Network module\nand a Visual Transformer module for local and global feature extraction\nrespectively, then a Multilayer Perceptron module is designed to fuse the local\nand global features for the final classification. Experimental results show the\neffectiveness and potential of the proposed CVM-Cervix in the field of cervical\nPap smear image classification. In addition, according to the practical needs\nof clinical work, we perform a lightweight post-processing to compress the\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiangchen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StopNet: Scalable Trajectory and Occupancy Prediction for Urban Autonomous Driving. (arXiv:2206.00991v1 [cs.RO])","link":"http://arxiv.org/abs/2206.00991","description":"<p>We introduce a motion forecasting (behavior prediction) method that meets the\nlatency requirements for autonomous driving in dense urban environments without\nsacrificing accuracy. A whole-scene sparse input representation allows StopNet\nto scale to predicting trajectories for hundreds of road agents with reliable\nlatency. In addition to predicting trajectories, our scene encoder lends itself\nto predicting whole-scene probabilistic occupancy grids, a complementary output\nrepresentation suitable for busy urban environments. Occupancy grids allow the\nAV to reason collectively about the behavior of groups of agents without\nprocessing their individual trajectories. We demonstrate the effectiveness of\nour sparse input representation and our model in terms of computation and\naccuracy over three datasets. We further show that co-training consistent\ntrajectory and occupancy predictions improves upon state-of-the-art performance\nunder standard metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinkyu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahjourian_R/0/1/0/all/0/1\">Reza Mahjourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ettinger_S/0/1/0/all/0/1\">Scott Ettinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mayank Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1\">Brandyn White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Ben Sapp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Mapping Necessary for Realistic PointGoal Navigation?. (arXiv:2206.00997v1 [cs.CV])","link":"http://arxiv.org/abs/2206.00997","description":"<p>Can an autonomous agent navigate in a new environment without building an\nexplicit map?\n</p>\n<p>For the task of PointGoal navigation ('Go to $\\Delta x$, $\\Delta y$') under\nidealized settings (no RGB-D and actuation noise, perfect GPS+Compass), the\nanswer is a clear 'yes' - map-less neural models composed of task-agnostic\ncomponents (CNNs and RNNs) trained with large-scale reinforcement learning\nachieve 100% Success on a standard dataset (Gibson). However, for PointNav in a\nrealistic setting (RGB-D and actuation noise, no GPS+Compass), this is an open\nquestion; one we tackle in this paper. The strongest published result for this\ntask is 71.7% Success.\n</p>\n<p>First, we identify the main (perhaps, only) cause of the drop in performance:\nthe absence of GPS+Compass. An agent with perfect GPS+Compass faced with RGB-D\nsensing and actuation noise achieves 99.8% Success (Gibson-v2 val). This\nsuggests that (to paraphrase a meme) robust visual odometry is all we need for\nrealistic PointNav; if we can achieve that, we can ignore the sensing and\nactuation noise.\n</p>\n<p>With that as our operating hypothesis, we scale the dataset and model size,\nand develop human-annotation-free data-augmentation techniques to train models\nfor visual odometry. We advance the state of art on the Habitat Realistic\nPointNav Challenge from 71% to 94% Success (+32, 4% relative) and 53% to 74%\nSPL (+39, 6% relative). While our approach does not saturate or 'solve' this\ndataset, this strong improvement combined with promising zero-shot sim2real\ntransfer (to a LoCoBot) provides evidence consistent with the hypothesis that\nexplicit mapping may not be necessary for navigation, even in a realistic\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Partsey_R/0/1/0/all/0/1\">Ruslan Partsey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoyama_N/0/1/0/all/0/1\">Naoki Yokoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobosevych_O/0/1/0/all/0/1\">Oles Dobosevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing One Sided Margin Loss for Solving Classification Problems in Deep Networks. (arXiv:2206.01002v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01002","description":"<p>This paper introduces a new loss function, OSM (One-Sided Margin), to solve\nmaximum-margin classification problems effectively. Unlike the hinge loss, in\nOSM the margin is explicitly determined with corresponding hyperparameters and\nthen the classification problem is solved. In experiments, we observe that\nusing OSM loss leads to faster training speeds and better accuracies than\nbinary and categorical cross-entropy in several commonly used deep models for\nclassification and optical character recognition problems.\n</p>\n<p>OSM has consistently shown better classification accuracies over\ncross-entropy and hinge losses for small to large neural networks. it has also\nled to a more efficient training procedure. We achieved state-of-the-art\naccuracies for small networks on several benchmark datasets of\nCIFAR10(98.82\\%), CIFAR100(91.56\\%), Flowers(98.04\\%), Stanford Cars(93.91\\%)\nwith considerable improvements over other loss functions. Moreover, the\naccuracies are rather better than cross-entropy and hinge loss for large\nnetworks. Therefore, we strongly believe that OSM is a powerful alternative to\nhinge and cross-entropy losses to train deep neural networks on classification\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1\">Ali Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kouzehkanan_Z/0/1/0/all/0/1\">Zahra Mousavi Kouzehkanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_R/0/1/0/all/0/1\">Reshad Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asheri_H/0/1/0/all/0/1\">Hadi Asheri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Recurrence Modeling for Video Action Anticipation. (arXiv:2206.01009v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01009","description":"<p>Forecasting future events based on evidence of current conditions is an\ninnate skill of human beings, and key for predicting the outcome of any\ndecision making. In artificial vision for example, we would like to predict the\nnext human action before it happens, without observing the future video frames\nassociated to it. Computer vision models for action anticipation are expected\nto collect the subtle evidence in the preamble of the target actions. In prior\nstudies recurrence modeling often leads to better performance, the strong\ntemporal inference is assumed to be a key element for reasonable prediction. To\nthis end, we propose a unified recurrence modeling for video action\nanticipation via message passing framework. The information flow in space-time\ncan be described by the interaction between vertices and edges, and the changes\nof vertices for each incoming frame reflects the underlying dynamics. Our model\nleverages self-attention as the building blocks for each of the message passing\nfunctions. In addition, we introduce different edge learning strategies that\ncan be end-to-end optimized to gain better flexibility for the connectivity\nbetween vertices. Our experimental results demonstrate that our proposed method\noutperforms previous works on the large-scale EPIC-Kitchen dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tai_T/0/1/0/all/0/1\">Tsung-Ming Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Cheng-Kuang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+See_S/0/1/0/all/0/1\">Simon See</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-tailed Recognition by Learning from Latent Categories. (arXiv:2206.01010v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01010","description":"<p>In this work, we address the challenging task of long-tailed image\nrecognition. Previous long-tailed recognition methods commonly focus on the\ndata augmentation or re-balancing strategy of the tail classes to give more\nattention to tail classes during the model training. However, due to the\nlimited training images for tail classes, the diversity of tail class images is\nstill restricted, which results in poor feature representations. In this work,\nwe hypothesize that common latent features among the head and tail classes can\nbe used to give better feature representation. Motivated by this, we introduce\na Latent Categories based long-tail Recognition (LCReg) method. Specifically,\nwe propose to learn a set of class-agnostic latent features shared among the\nhead and tail classes. Then, we implicitly enrich the training sample diversity\nvia applying semantic data augmentation to the latent features. Extensive\nexperiments on five long-tailed image recognition datasets demonstrate that our\nproposed LCReg is able to significantly outperform previous methods and achieve\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weide Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhonghua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Henghui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suggestive Annotation of Brain MR Images with Gradient-guided Sampling. (arXiv:2206.01014v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01014","description":"<p>Machine learning has been widely adopted for medical image analysis in recent\nyears given its promising performance in image segmentation and classification\ntasks. The success of machine learning, in particular supervised learning,\ndepends on the availability of manually annotated datasets. For medical imaging\napplications, such annotated datasets are not easy to acquire, it takes a\nsubstantial amount of time and resource to curate an annotated medical image\nset. In this paper, we propose an efficient annotation framework for brain MR\nimages that can suggest informative sample images for human experts to\nannotate. We evaluate the framework on two different brain image analysis\ntasks, namely brain tumour segmentation and whole brain segmentation.\nExperiments show that for brain tumour segmentation task on the BraTS 2019\ndataset, training a segmentation model with only 7% suggestively annotated\nimage samples can achieve a performance comparable to that of training on the\nfull dataset. For whole brain segmentation on the MALC dataset, training with\n42% suggestively annotated image samples can achieve a comparable performance\nto training on the full dataset. The proposed framework demonstrates a\npromising way to save manual annotation cost and improve data efficiency in\nmedical imaging applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_C/0/1/0/all/0/1\">Chengliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1\">Yuanhan Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_E/0/1/0/all/0/1\">Elsa Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Two-stream Attention Network for Video Question Answering. (arXiv:2206.01017v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01017","description":"<p>To date, visual question answering (VQA) (i.e., image QA and video QA) is\nstill a holy grail in vision and language understanding, especially for video\nQA. Compared with image QA that focuses primarily on understanding the\nassociations between image region-level details and corresponding questions,\nvideo QA requires a model to jointly reason across both spatial and long-range\ntemporal structures of a video as well as text to provide an accurate answer.\nIn this paper, we specifically tackle the problem of video QA by proposing a\nStructured Two-stream Attention network, namely STA, to answer a free-form or\nopen-ended natural language question about the content of a given video. First,\nwe infer rich long-range temporal structures in videos using our structured\nsegment component and encode text features. Then, our structured two-stream\nattention component simultaneously localizes important visual instance, reduces\nthe influence of background video and focuses on the relevant text. Finally,\nthe structured two-stream fusion component incorporates different segments of\nquery and video aware context representation and infers the answers.\nExperiments on the large-scale video QA dataset \\textit{TGIF-QA} show that our\nproposed method significantly surpasses the best counterpart (i.e., with one\nrepresentation for the video input) by 13.0%, 13.5%, 11.0% and 0.3 for Action,\nTrans., TrameQA and Count tasks. It also outperforms the best competitor (i.e.,\nwith two representations) on the Action, Trans., TrameQA tasks by 4.1%, 4.7%,\nand 5.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1\">Pengpeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Heng Tao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Laser Spot: Robust and Covert Physical Adversarial Attack to DNNs. (arXiv:2206.01034v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01034","description":"<p>Most existing deep neural networks (DNNs) are easily disturbed by slight\nnoise. As far as we know, there are few researches on physical adversarial\nattack technology by deploying lighting equipment. The light-based physical\nadversarial attack technology has excellent covertness, which brings great\nsecurity risks to many applications based on deep neural networks (such as\nautomatic driving technology). Therefore, we propose a robust physical\nadversarial attack technology with excellent covertness, called adversarial\nlaser point (AdvLS), which optimizes the physical parameters of laser point\nthrough genetic algorithm to perform physical adversarial attack. It realizes\nrobust and covert physical adversarial attack by using low-cost laser\nequipment. As far as we know, AdvLS is the first light-based adversarial attack\ntechnology that can perform physical adversarial attacks in the daytime. A\nlarge number of experiments in the digital and physical environments show that\nAdvLS has excellent robustness and concealment. In addition, through in-depth\nanalysis of the experimental data, we find that the adversarial perturbations\ngenerated by AdvLS have superior adversarial attack migration. The experimental\nresults show that AdvLS impose serious interference to the advanced deep neural\nnetworks, we call for the attention of the proposed physical adversarial attack\ntechnology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chengyin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Video Action Recognition in Sports: Datasets, Methods and Applications. (arXiv:2206.01038v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01038","description":"<p>To understand human behaviors, action recognition based on videos is a common\napproach. Compared with image-based action recognition, videos provide much\nmore information. Reducing the ambiguity of actions and in the last decade,\nmany works focused on datasets, novel models and learning approaches have\nimproved video action recognition to a higher level. However, there are\nchallenges and unsolved problems, in particular in sports analytics where data\ncollection and labeling are more sophisticated, requiring sport professionals\nto annotate data. In addition, the actions could be extremely fast and it\nbecomes difficult to recognize them. Moreover, in team sports like football and\nbasketball, one action could involve multiple players, and to correctly\nrecognize them, we need to analyse all players, which is relatively\ncomplicated. In this paper, we present a survey on video action recognition for\nsports analytics. We introduce more than ten types of sports, including team\nsports, such as football, basketball, volleyball, hockey and individual sports,\nsuch as figure skating, gymnastics, table tennis, tennis, diving and badminton.\nThen we compare numerous existing frameworks for sports analysis to present\nstatus quo of video action recognition in both team sports and individual\nsports. Finally, we discuss the challenges and unsolved problems in this area\nand to facilitate sports analytics, we develop a toolbox using PaddlePaddle,\nwhich supports football, basketball, table tennis and figure skating action\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qingzhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">Jian Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feixiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FV-UPatches: Enhancing Universality in Finger Vein Recognition. (arXiv:2206.01061v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01061","description":"<p>Many deep learning-based models have been introduced in finger vein\nrecognition in recent years. These solutions, however, suffer from data\ndependency and are difficult to achieve model generalization. To address this\nproblem, we are inspired by the idea of domain adaptation and propose a\nuniversal learning-based framework, which achieves generalization while\ntraining with limited data. To reduce differences between data distributions, a\ncompressed U-Net is introduced as a domain mapper to map the raw region of\ninterest image onto a target domain. The concentrated target domain is a\nunified feature space for the subsequent matching, in which a local descriptor\nmodel SOSNet is employed to embed patches into descriptors measuring the\nsimilarity of matching pairs. In the proposed framework, the domain mapper is\nan approximation to a specific extraction function thus the training is only a\none-time effort with limited data. Moreover, the local descriptor model can be\ntrained to be representative enough based on a public dataset of\nnon-finger-vein images. The whole pipeline enables the framework to be well\ngeneralized, making it possible to enhance universality and helps to reduce\ncosts of data collection, tuning and retraining. The comparable experimental\nresults to state-of-the-art (SOTA) performance in five public datasets prove\nthe effectiveness of the proposed framework. Furthermore, the framework shows\napplication potential in other vein-based biometric recognition as well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiazhen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Changwen Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Changlong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hakil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DocLayNet: A Large Human-Annotated Dataset for Document-Layout Analysis. (arXiv:2206.01062v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01062","description":"<p>Accurate document layout analysis is a key requirement for high-quality PDF\ndocument conversion. With the recent availability of public, large ground-truth\ndatasets such as PubLayNet and DocBank, deep-learning models have proven to be\nvery effective at layout detection and segmentation. While these datasets are\nof adequate size to train such models, they severely lack in layout variability\nsince they are sourced from scientific article repositories such as PubMed and\narXiv only. Consequently, the accuracy of the layout segmentation drops\nsignificantly when these models are applied on more challenging and diverse\nlayouts. In this paper, we present \\textit{DocLayNet}, a new, publicly\navailable, document-layout annotation dataset in COCO format. It contains 80863\nmanually annotated pages from diverse data sources to represent a wide\nvariability in layouts. For each PDF page, the layout annotations provide\nlabelled bounding-boxes with a choice of 11 distinct classes. DocLayNet also\nprovides a subset of double- and triple-annotated pages to determine the\ninter-annotator agreement. In multiple experiments, we provide baseline\naccuracy scores (in mAP) for a set of popular object detection models. We also\ndemonstrate that these models fall approximately 10\\% behind the\ninter-annotator agreement. Furthermore, we provide evidence that DocLayNet is\nof sufficient size. Lastly, we compare models trained on PubLayNet, DocBank and\nDocLayNet, showing that layout predictions of the DocLayNet-trained models are\nmore robust and thus the preferred choice for general-purpose document-layout\nanalysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfitzmann_B/0/1/0/all/0/1\">Birgit Pfitzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_C/0/1/0/all/0/1\">Christoph Auer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolfi_M/0/1/0/all/0/1\">Michele Dolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_A/0/1/0/all/0/1\">Ahmed S Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staar_P/0/1/0/all/0/1\">Peter W J Staar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning-based Lung and Colon Cancer Detection using Deep Feature Extraction and Ensemble Learning. (arXiv:2206.01088v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01088","description":"<p>Cancer is a fatal disease caused by a combination of genetic diseases and a\nvariety of biochemical abnormalities. Lung and colon cancer have emerged as two\nof the leading causes of death and disability in humans. The histopathological\ndetection of such malignancies is usually the most important component in\ndetermining the best course of action. Early detection of the ailment on either\nfront considerably decreases the likelihood of mortality. Machine learning and\ndeep learning techniques can be utilized to speed up such cancer detection,\nallowing researchers to study a large number of patients in a much shorter\namount of time and at a lower cost. In this research work, we introduced a\nhybrid ensemble feature extraction model to efficiently identify lung and colon\ncancer. It integrates deep feature extraction and ensemble learning with\nhigh-performance filtering for cancer image datasets. The model is evaluated on\nhistopathological (LC25000) lung and colon datasets. According to the study\nfindings, our hybrid model can detect lung, colon, and (lung and colon) cancer\nwith accuracy rates of 99.05%, 100%, and 99.30%, respectively. The study's\nfindings show that our proposed strategy outperforms existing models\nsignificantly. Thus, these models could be applicable in clinics to support the\ndoctor in the diagnosis of cancers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Talukder_M/0/1/0/all/0/1\">Md. Alamin Talukder</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1\">Md. Manowarul Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uddin_M/0/1/0/all/0/1\">Md Ashraf Uddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akhter_A/0/1/0/all/0/1\">Arnisha Akhter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_K/0/1/0/all/0/1\">Khondokar Fida Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moni_M/0/1/0/all/0/1\">Mohammad Ali Moni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A DTCWT-SVD Based Video Watermarking resistant to frame rate conversion. (arXiv:2206.01094v1 [cs.MM])","link":"http://arxiv.org/abs/2206.01094","description":"<p>Videos can be easily tampered, copied and redistributed by attackers for\nillegal and monetary usage. Such behaviors severely jeopardize the interest of\ncontent owners. Despite huge efforts made in digital video watermarking for\ncopyright protection, typical distortions in video transmission including\nsignal attacks, geometric attacks and temporal synchronization attacks can\nstill easily erase the embedded signal. Among them, temporal synchronization\nattacks which include frame dropping, frame insertion and frame rate conversion\nis one of the most prevalent attacks. To address this issue, we present a new\nvideo watermarking based on joint Dual-Tree Cosine Wavelet Transformation\n(DTCWT) and Singular Value Decomposition (SVD), which is resistant to frame\nrate conversion. We first extract a set of candidate coefficient by applying\nSVD decomposition after DTCWT transform. Then, we simulate the watermark\nembedding by adjusting the shape of candidate coefficient. Finally, we perform\ngroup-level watermarking that includes moderate temporal redundancy to resist\ntemporal desynchronization attacks. Extensive experimental results show that\nthe proposed scheme is more resilient to temporal desynchronization attacks and\nperforms better than the existing blind video watermarking schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinpeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-fusion Semantic Segmentation Framework With GAN For SAR Images. (arXiv:2206.01096v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01096","description":"<p>Deep learning based semantic segmentation is one of the popular methods in\nremote sensing image segmentation. In this paper, a network based on the widely\nused encoderdecoder architecture is proposed to accomplish the synthetic\naperture radar (SAR) images segmentation. With the better representation\ncapability of optical images, we propose to enrich SAR images with generated\noptical images via the generative adversative network (GAN) trained by numerous\nSAR and optical images. These optical images can be used as expansions of\noriginal SAR images, thus ensuring robust result of segmentation. Then the\noptical images generated by the GAN are stitched together with the\ncorresponding real images. An attention module following the stitched data is\nused to strengthen the representation of the objects. Experiments indicate that\nour method is efficient compared to other commonly used methods\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1\">Donghui Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jia Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1\">Fang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wenhua Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1\">Andi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wenfei Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jiao Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A temporal chrominance trigger for clean-label backdoor attack against anti-spoof rebroadcast detection. (arXiv:2206.01102v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01102","description":"<p>We propose a stealthy clean-label video backdoor attack against Deep Learning\n(DL)-based models aiming at detecting a particular class of spoofing attacks,\nnamely video rebroadcast attacks. The injected backdoor does not affect\nspoofing detection in normal conditions, but induces a misclassification in the\npresence of a specific triggering signal. The proposed backdoor relies on a\ntemporal trigger altering the average chrominance of the video sequence. The\nbackdoor signal is designed by taking into account the peculiarities of the\nHuman Visual System (HVS) to reduce the visibility of the trigger, thus\nincreasing the stealthiness of the backdoor. To force the network to look at\nthe presence of the trigger in the challenging clean-label scenario, we choose\nthe poisoned samples used for the injection of the backdoor following a\nso-called Outlier Poisoning Strategy (OPS). According to OPS, the triggering\nsignal is inserted in the training samples that the network finds more\ndifficult to classify. The effectiveness of the proposed backdoor attack and\nits generality are validated experimentally on different datasets and\nanti-spoofing rebroadcast detection architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1\">Benedetta Tondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1\">Mauro Barni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise2NoiseFlow: Realistic Camera Noise Modeling without Clean Images. (arXiv:2206.01103v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01103","description":"<p>Image noise modeling is a long-standing problem with many applications in\ncomputer vision. Early attempts that propose simple models, such as\nsignal-independent additive white Gaussian noise or the heteroscedastic\nGaussian noise model (a.k.a., camera noise level function) are not sufficient\nto learn the complex behavior of the camera sensor noise. Recently, more\ncomplex learning-based models have been proposed that yield better results in\nnoise synthesis and downstream tasks, such as denoising. However, their\ndependence on supervised data (i.e., paired clean images) is a limiting factor\ngiven the challenges in producing ground-truth images. This paper proposes a\nframework for training a noise model and a denoiser simultaneously while\nrelying only on pairs of noisy images rather than noisy/clean paired image\ndata. We apply this framework to the training of the Noise Flow architecture.\nThe noise synthesis and density estimation results show that our framework\noutperforms previous signal-processing-based noise models and is on par with\nits supervised counterpart. The trained denoiser is also shown to significantly\nimprove upon both supervised and weakly supervised baseline denoising\napproaches. The results indicate that the joint training of a denoiser and a\nnoise model yields significant improvements in the denoiser.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maleky_A/0/1/0/all/0/1\">Ali Maleky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kousha_S/0/1/0/all/0/1\">Shayan Kousha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Conventional and Deep Feature Models for Classifying Fundus Photography of Hemorrhages. (arXiv:2206.01118v1 [eess.IV])","link":"http://arxiv.org/abs/2206.01118","description":"<p>Diabetic retinopathy is an eye-related pathology creating abnormalities and\ncausing visual impairment, proper treatment of which requires identifying\nirregularities. This research uses a hemorrhage detection method and compares\nclassification of conventional and deep features. Especially, method identifies\nhemorrhage connected with blood vessels or reside at retinal border and\nreported challenging. Initially, adaptive brightness adjustment and contrast\nenhancement rectify degraded images. Prospective locations of hemorrhages are\nestimated by a Gaussian matched filter, entropy thresholding, and morphological\noperation. Hemorrhages are segmented by a novel technique based on regional\nvariance of intensities. Features are then extracted by conventional methods\nand deep models for training support vector machines, and results evaluated.\nEvaluation metrics for each model are promising, but findings suggest that\ncomparatively, deep models are more effective than conventional features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aziz_T/0/1/0/all/0/1\">Tamoor Aziz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Charoenlarpnopparut_C/0/1/0/all/0/1\">Chalie Charoenlarpnopparut</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahapakulchai_S/0/1/0/all/0/1\">Srijidtra Mahapakulchai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix Conditioning Unifies Language and Label Supervision. (arXiv:2206.01125v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01125","description":"<p>Vision-language contrastive learning suggests a new learning paradigm by\nleveraging a large amount of image-caption-pair data. The caption supervision\nexcels at providing wide coverage in vocabulary that enables strong zero-shot\nimage recognition performance. On the other hand, label supervision offers to\nlearn more targeted visual representations that are label-oriented and can\ncover rare categories. To gain the complementary advantages of both kinds of\nsupervision for contrastive image-caption pre-training, recent works have\nproposed to convert class labels into a sentence with pre-defined templates\ncalled prompts. However, a naive unification of the real caption and the prompt\nsentences could lead to a complication in learning, as the distribution shift\nin text may not be handled properly in the language encoder. In this work, we\npropose a simple yet effective approach to unify these two types of supervision\nusing prefix tokens that inform a language encoder of the type of the input\nsentence (e.g., caption or prompt) at training time. Our method is generic and\ncan be easily integrated into existing VL pre-training objectives such as CLIP\nor UniCL. In experiments, we show that this simple technique dramatically\nimproves the performance in zero-shot image recognition accuracy of the\npre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saito_K/0/1/0/all/0/1\">Kuniaki Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kihyuk Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chen-Yu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-BEiT: Generative Vision-Language Pretraining. (arXiv:2206.01127v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01127","description":"<p>We introduce a vision-language foundation model called VL-BEiT, which is a\nbidirectional multimodal Transformer learned by generative pretraining. Our\nminimalist solution conducts masked prediction on both monomodal and multimodal\ndata with a shared Transformer. Specifically, we perform masked vision-language\nmodeling on image-text pairs, masked language modeling on texts, and masked\nimage modeling on images. VL-BEiT is learned from scratch with one unified\npretraining task, one shared backbone, and one-stage training. Our method is\nconceptually simple and empirically effective. Experimental results show that\nVL-BEiT obtains strong results on various vision-language benchmarks, such as\nvisual question answering, visual reasoning, and image-text retrieval.\nMoreover, our method learns transferable visual features, achieving competitive\nperformance on image classification, and semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives. (arXiv:2206.01136v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01136","description":"<p>Transformer, the latest technological advance of deep learning, has gained\nprevalence in natural language processing or computer vision. Since medical\nimaging bear some resemblance to computer vision, it is natural to inquire\nabout the status quo of Transformers in medical imaging and ask the question:\ncan the Transformer models transform medical imaging? In this paper, we attempt\nto make a response to the inquiry. After a brief introduction of the\nfundamentals of Transformers, especially in comparison with convolutional\nneural networks (CNNs), and highlighting key defining properties that\ncharacterize the Transformers, we offer a comprehensive review of the\nstate-of-the-art Transformer-based approaches for medical imaging and exhibit\ncurrent research progresses made in the areas of medical image segmentation,\nrecognition, detection, registration, reconstruction, enhancement, etc. In\nparticular, what distinguishes our review lies in its organization based on the\nTransformer's key defining properties, which are mostly derived from comparing\nthe Transformer and CNN, and its type of architecture, which specifies the\nmanner in which the Transformer and CNN are combined, all helping the readers\nto best understand the rationale behind the reviewed approaches. We conclude\nwith discussions of future perspectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yucheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Active Fine-Grained Recognition. (arXiv:2206.01153v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01153","description":"<p>As fine-grained visual classification (FGVC) being developed for decades,\ngreat works related have exposed a key direction -- finding discriminative\nlocal regions and revealing subtle differences. However, unlike identifying\nvisual contents within static images, for recognizing objects in the real\nphysical world, discriminative information is not only present within seen\nlocal regions but also hides in other unseen perspectives. In other words, in\naddition to focusing on the distinguishable part from the whole, for efficient\nand accurate recognition, it is required to infer the key perspective with a\nfew glances, e.g., people may recognize a \"Benz AMG GT\" with a glance of its\nfront and then know that taking a look at its exhaust pipe can help to tell\nwhich year's model it is. In this paper, back to reality, we put forward the\nproblem of active fine-grained recognition (AFGR) and complete this study in\nthree steps: (i) a hierarchical, multi-view, fine-grained vehicle dataset is\ncollected as the testbed, (ii) a simple experiment is designed to verify that\ndifferent perspectives contribute differently for FGVC and different categories\nown different discriminative perspective, (iii) a policy-gradient-based\nframework is adopted to achieve efficient recognition with active view\nselection. Comprehensive experiments demonstrate that the proposed method\ndelivers a better performance-efficient trade-off than previous FGVC methods\nand advanced neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruoyi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenqing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongliang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DE-Net: Dynamic Text-guided Image Editing Adversarial Networks. (arXiv:2206.01160v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01160","description":"<p>Text-guided image editing models have shown remarkable results. However,\nthere remain two problems. First, they employ fixed manipulation modules for\nvarious editing requirements (e.g., color changing, texture changing, content\nadding and removing), which result in over-editing or insufficient editing.\nSecond, they do not clearly distinguish between text-required parts and\ntext-irrelevant parts, which leads to inaccurate editing. To solve these\nlimitations, we propose: (i) a Dynamic Editing Block (DEBlock) which combines\nspatial- and channel-wise manipulations dynamically for various editing\nrequirements. (ii) a Combination Weights Predictor (CWP) which predicts the\ncombination weights for DEBlock according to the inference on text and visual\nfeatures. (iii) a Dynamic text-adaptive Convolution Block (DCBlock) which\nqueries source image features to distinguish text-required parts and\ntext-irrelevant parts. Extensive experiments demonstrate that our DE-Net\nachieves excellent performance and manipulates source images more effectively\nand accurately. Code is available at \\url{https://github.com/tobran/DE-Net}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Ming Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1\">Bing-Kun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Relevance Maps of Vision Transformers Improves Robustness. (arXiv:2206.01161v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01161","description":"<p>It has been observed that visual classification models often rely mostly on\nthe image background, neglecting the foreground, which hurts their robustness\nto distribution changes. To alleviate this shortcoming, we propose to monitor\nthe model's relevancy signal and manipulate it such that the model is focused\non the foreground object. This is done as a finetuning step, involving\nrelatively few samples consisting of pairs of images and their associated\nforeground masks. Specifically, we encourage the model's relevancy map (i) to\nassign lower relevance to background regions, (ii) to consider as much\ninformation as possible from the foreground, and (iii) we encourage the\ndecisions to have high confidence. When applied to Vision Transformer (ViT)\nmodels, a marked improvement in robustness to domain shifts is observed.\nMoreover, the foreground masks can be obtained automatically, from a\nself-supervised variant of the ViT model itself; therefore no additional\nsupervision is required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1\">Hila Chefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1\">Idan Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning on Implicit Neural Datasets. (arXiv:2206.01178v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01178","description":"<p>Implicit neural representations (INRs) have become fast, lightweight tools\nfor storing continuous data, but to date there is no general method for\nlearning directly with INRs as a data representation. We introduce a principled\ndeep learning framework for learning and inference directly with INRs of any\ntype without reverting to grid-based features or operations. Our INR-Nets\nevaluate INRs on a low discrepancy sequence, enabling quasi-Monte Carlo (QMC)\nintegration throughout the network. We prove INR-Nets are universal\napproximators on a large class of maps between $L^2$ functions. Additionally,\nINR-Nets have convergent gradients under the empirical measure, enabling\nbackpropagation. We design INR-Nets as a continuous generalization of discrete\nnetworks, enabling them to be initialized with pre-trained models. We\ndemonstrate learning of INR-Nets on classification (INR$\\to$label) and\nsegmentation (INR$\\to$INR) tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Clinton J. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientFormer: Vision Transformers at MobileNet Speed. (arXiv:2206.01191v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01191","description":"<p>Vision Transformers (ViT) have shown rapid progress in computer vision tasks,\nachieving promising results on various benchmarks. However, due to the massive\nnumber of parameters and model design, e.g., attention mechanism, ViT-based\nmodels are generally times slower than lightweight convolutional networks.\nTherefore, the deployment of ViT for real-time applications is particularly\nchallenging, especially on resource-constrained hardware such as mobile\ndevices. Recent efforts try to reduce the computation complexity of ViT through\nnetwork architecture search or hybrid design with MobileNet block, yet the\ninference speed is still unsatisfactory. This leads to an important question:\ncan transformers run as fast as MobileNet while obtaining high performance? To\nanswer this, we first revisit the network architecture and operators used in\nViT-based models and identify inefficient designs. Then we introduce a\ndimension-consistent pure transformer (without MobileNet blocks) as design\nparadigm. Finally, we perform latency-driven slimming to get a series of final\nmodels dubbed EfficientFormer. Extensive experiments show the superiority of\nEfficientFormer in performance and speed on mobile devices. Our fastest model,\nEfficientFormer-L1, achieves 79.2% top-1 accuracy on ImageNet-1K with only 1.6\nms inference latency on iPhone 12 (compiled with CoreML), which is even a bit\nfaster than MobileNetV2 (1.7 ms, 71.8% top-1), and our largest model,\nEfficientFormer-L7, obtains 83.3% accuracy with only 7.0 ms latency. Our work\nproves that properly designed transformers can reach extremely low latency on\nmobile devices while maintaining high performance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1\">Eric Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evangelidis_G/0/1/0/all/0/1\">Georgios Evangelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jian Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard Negative Sampling Strategies for Contrastive Representation Learning. (arXiv:2206.01197v1 [cs.LG])","link":"http://arxiv.org/abs/2206.01197","description":"<p>One of the challenges in contrastive learning is the selection of appropriate\n\\textit{hard negative} examples, in the absence of label information. Random\nsampling or importance sampling methods based on feature similarity often lead\nto sub-optimal performance. In this work, we introduce UnReMix, a hard negative\nsampling strategy that takes into account anchor similarity, model uncertainty\nand representativeness. Experimental results on several benchmarks show that\nUnReMix improves negative sample selection, and subsequently downstream\nperformance when compared to state-of-the-art contrastive learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tabassum_A/0/1/0/all/0/1\">Afrina Tabassum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahed_M/0/1/0/all/0/1\">Muntasir Wahed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldardiry_H/0/1/0/all/0/1\">Hoda Eldardiry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pruning-as-Search: Efficient Neural Architecture Search via Channel Pruning and Structural Reparameterization. (arXiv:2206.01198v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01198","description":"<p>Neural architecture search (NAS) and network pruning are widely studied\nefficient AI techniques, but not yet perfect. NAS performs exhaustive candidate\narchitecture search, incurring tremendous search cost. Though (structured)\npruning can simply shrink model dimension, it remains unclear how to decide the\nper-layer sparsity automatically and optimally. In this work, we revisit the\nproblem of layer-width optimization and propose Pruning-as-Search (PaS), an\nend-to-end channel pruning method to search out desired sub-network\nautomatically and efficiently. Specifically, we add a depth-wise binary\nconvolution to learn pruning policies directly through gradient descent. By\ncombining the structural reparameterization and PaS, we successfully searched\nout a new family of VGG-like and lightweight networks, which enable the\nflexibility of arbitrary width with respect to each layer instead of each\nstage. Experimental results show that our proposed architecture outperforms\nprior arts by around $1.0\\%$ top-1 accuracy under similar inference speed on\nImageNet-1000 classification task. Furthermore, we demonstrate the\neffectiveness of our width search on complex tasks including instance\nsegmentation and image translation. Code and models are released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering. (arXiv:2206.01201v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01201","description":"<p>This paper revisits visual representation in knowledge-based visual question\nanswering (VQA) and demonstrates that using regional information in a better\nway can significantly improve the performance. While visual representation is\nextensively studied in traditional VQA, it is under-explored in knowledge-based\nVQA even though these two tasks share the common spirit, i.e., rely on visual\ninput to answer the question. Specifically, we observe that in most\nstate-of-the-art knowledge-based VQA methods: 1) visual features are extracted\neither from the whole image or in a sliding window manner for retrieving\nknowledge, and the important relationship within/among object regions is\nneglected; 2) visual features are not well utilized in the final answering\nmodel, which is counter-intuitive to some extent. Based on these observations,\nwe propose a new knowledge-based VQA method REVIVE, which tries to utilize the\nexplicit information of object regions not only in the knowledge retrieval\nstage but also in the answering model. The key motivation is that object\nregions and inherent relationships are important for knowledge-based VQA. We\nperform extensive experiments on the standard OK-VQA dataset and achieve new\nstate-of-the-art performance, i.e., 58.0% accuracy, surpassing previous\nstate-of-the-art method by a large margin (+3.6%). We also conduct detailed\nanalysis and show the necessity of regional information in different framework\ncomponents for knowledge-based VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuanze Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yujia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling The Mask of Position-Information Pattern Through the Mist of Image Features. (arXiv:2206.01202v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01202","description":"<p>Recent studies show that paddings in convolutional neural networks encode\nabsolute position information which can negatively affect the model performance\nfor certain tasks. However, existing metrics for quantifying the strength of\npositional information remain unreliable and frequently lead to erroneous\nresults. To address this issue, we propose novel metrics for measuring (and\nvisualizing) the encoded positional information. We formally define the encoded\ninformation as PPP (Position-information Pattern from Padding) and conduct a\nseries of experiments to study its properties as well as its formation. The\nproposed metrics measure the presence of positional information more reliably\nthan the existing metrics based on PosENet and a test in F-Conv. We also\ndemonstrate that for any extant (and proposed) padding schemes, PPP is\nprimarily a learning artifact and is less dependent on the characteristics of\nthe underlying padding schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chieh Hubert Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Instance Segmentation of 3D Scenes Through Weak Bounding Box Supervision. (arXiv:2206.01203v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01203","description":"<p>Current 3D segmentation methods heavily rely on large-scale point-cloud\ndatasets, which are notoriously laborious to annotate. Few attempts have been\nmade to circumvent the need for dense per-point annotations. In this work, we\nlook at weakly-supervised 3D instance semantic segmentation. The key idea is to\nleverage 3D bounding box labels which are easier and faster to annotate.\nIndeed, we show that it is possible to train dense segmentation models using\nonly weak bounding box labels. At the core of our method, Box2Mask, lies a deep\nmodel, inspired by classical Hough voting, that directly votes for bounding box\nparameters, and a clustering method specifically tailored to bounding box\nvotes. This goes beyond commonly used center votes, which would not fully\nexploit the bounding box annotations. On ScanNet test, our weakly supervised\nmodel attains leading performance among other weakly supervised approaches (+18\nmAP50). Remarkably, it also achieves 97% of the performance of fully supervised\nmodels. To prove the practicality of our approach, we show segmentation results\non the recently released ARKitScenes dataset which is annotated with 3D\nbounding boxes only, and obtain, for the first time, compelling 3D instance\nsegmentation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chibane_J/0/1/0/all/0/1\">Julian Chibane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1\">Francis Engelmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Tuan Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Siamese Image Modeling for Self-Supervised Vision Representation Learning. (arXiv:2206.01204v1 [cs.CV])","link":"http://arxiv.org/abs/2206.01204","description":"<p>Self-supervised learning (SSL) has delivered superior performance on a\nvariety of downstream vision tasks. Two main-stream SSL frameworks have been\nproposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM).\nID pulls together the representations of different views from the same image,\nwhile avoiding feature collapse. It does well on linear probing but is inferior\nin detection performance. On the other hand, MIM reconstructs the original\ncontent given a masked image. It excels at dense prediction but fails to\nperform well on linear probing. Their distinctions are caused by neglecting the\nrepresentation requirements of either semantic alignment or spatial\nsensitivity. Specifically, we observe that (1) semantic alignment demands\nsemantically similar views to be projected into nearby representation, which\ncan be achieved by contrasting different views with strong augmentations; (2)\nspatial sensitivity requires to model the local structure within an image.\nPredicting dense representations with masked image is therefore beneficial\nbecause it models the conditional distribution of image content. Driven by\nthese analysis, we propose Siamese Image Modeling (SIM), which predicts the\ndense representations of an augmented view, based on another masked view from\nthe same image but with different augmentations. Our method uses a Siamese\nnetwork with two branches. The online branch encodes the first view, and\npredicts the second view's representation according to the relative positions\nbetween these two views. The target branch produces the target by encoding the\nsecond view. In this way, we are able to achieve comparable linear probing and\ndense prediction performances with ID and MIM, respectively. We also\ndemonstrate that decent linear probing result can be obtained without a global\nloss. Code shall be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with Attentive Feature Alignment. (arXiv:2104.07719v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07719","description":"<p>Few-shot object detection (FSOD) aims to detect objects using only a few\nexamples. How to adapt state-of-the-art object detectors to the few-shot domain\nremains challenging. Object proposal is a key ingredient in modern object\ndetectors. However, the quality of proposals generated for few-shot classes\nusing existing methods is far worse than that of many-shot classes, e.g.,\nmissing boxes for few-shot classes due to misclassification or inaccurate\nspatial locations with respect to true objects. To address the noisy proposal\nproblem, we propose a novel meta-learning based FSOD model by jointly\noptimizing the few-shot proposal generation and fine-grained few-shot proposal\nclassification. To improve proposal generation for few-shot classes, we propose\nto learn a lightweight metric-learning based prototype matching network,\ninstead of the conventional simple linear object/nonobject classifier, e.g.,\nused in RPN. Our non-linear classifier with the feature fusion network could\nimprove the discriminative prototype matching and the proposal recall for\nfew-shot classes. To improve the fine-grained few-shot proposal classification,\nwe propose a novel attentive feature alignment method to address the spatial\nmisalignment between the noisy proposals and few-shot classes, thus improving\nthe performance of few-shot object detection. Meanwhile we learn a separate\nFaster R-CNN detection head for many-shot base classes and show strong\nperformance of maintaining base-classes knowledge. Our model achieves\nstate-of-the-art performance on multiple FSOD benchmarks over most of the shots\nand metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DASO: Distribution-Aware Semantics-Oriented Pseudo-label for Imbalanced Semi-Supervised Learning. (arXiv:2106.05682v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05682","description":"<p>The capability of the traditional semi-supervised learning (SSL) methods is\nfar from real-world application due to severely biased pseudo-labels caused by\n(1) class imbalance and (2) class distribution mismatch between labeled and\nunlabeled data. This paper addresses such a relatively under-explored problem.\nFirst, we propose a general pseudo-labeling framework that class-adaptively\nblends the semantic pseudo-label from a similarity-based classifier to the\nlinear one from the linear classifier, after making the observation that both\ntypes of pseudo-labels have complementary properties in terms of bias. We\nfurther introduce a novel semantic alignment loss to establish balanced feature\nrepresentation to reduce the biased predictions from the classifier. We term\nthe whole framework as Distribution-Aware Semantics-Oriented (DASO)\nPseudo-label. We conduct extensive experiments in a wide range of imbalanced\nbenchmarks: CIFAR10/100-LT, STL10-LT, and large-scale long-tailed Semi-Aves\nwith open-set class, and demonstrate that, the proposed DASO framework reliably\nimproves SSL learners with unlabeled data especially when both (1) class\nimbalance and (2) distribution mismatch dominate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngtaek Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Knowledge Distillation With Noise Elimination for RGB-D Salient Object Detection. (arXiv:2106.09517v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.09517","description":"<p>RGB-D salient object detection (SOD) demonstrates its superiority on\ndetecting in complex environments due to the additional depth information\nintroduced in the data. Inevitably, an independent stream is introduced to\nextract features from depth images, leading to extra computation and\nparameters. This methodology sacrifices the model size to improve the detection\naccuracy which may impede the practical application of SOD problems. To tackle\nthis dilemma, we propose a dynamic distillation method along with a lightweight\nstructure, which significantly reduces the computational burden while\nmaintaining validity. This method considers the factors of both teacher and\nstudent performance within the training stage and dynamically assigns the\ndistillation weight instead of applying a fixed weight on the student model. We\nalso investigate the issue of RGB-D early fusion strategy in distillation and\npropose a simple noise elimination method to mitigate the impact of distorted\ntraining data caused by low quality depth maps. Extensive experiments are\nconducted on five public datasets to demonstrate that our method can achieve\ncompetitive performance with a fast inference speed (136FPS) compared to 10\nprior methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guangyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yinxiao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hengyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1\">Tania Stathaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Efficient Vision Transformers via Fine-Grained Manifold Distillation. (arXiv:2107.01378v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01378","description":"<p>In the past few years, transformers have achieved promising performances on\nvarious computer vision tasks. Unfortunately, the immense inference overhead of\nmost existing vision transformers withholds their from being deployed on edge\ndevices such as cell phones and smart watches. Knowledge distillation is a\nwidely used paradigm for compressing cumbersome architectures via transferring\ninformation to a compact student. However, most of them are designed for\nconvolutional neural networks (CNNs), which do not fully investigate the\ncharacter of vision transformer (ViT). In this paper, we utilize the\npatch-level information and propose a fine-grained manifold distillation\nmethod. Specifically, we train a tiny student model to match a pre-trained\nteacher model in the patch-level manifold space. Then, we decouple the manifold\nmatching loss into three terms with careful design to further reduce the\ncomputational costs for the patch relationship. Equipped with the proposed\nmethod, a DeiT-Tiny model containing 5M parameters achieves 76.5% top-1\naccuracy on ImageNet-1k, which is +2.0% higher than previous distillation\napproaches. Transfer learning results on other classification benchmarks and\ndownstream vision tasks also demonstrate the superiority of our method over the\nstate-of-the-art algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhiwei Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1\">Ding Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards real-world navigation with deep differentiable planners. (arXiv:2108.05713v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.05713","description":"<p>We train embodied neural networks to plan and navigate unseen complex 3D\nenvironments, emphasising real-world deployment. Rather than requiring prior\nknowledge of the agent or environment, the planner learns to model the state\ntransitions and rewards. To avoid the potentially hazardous trial-and-error of\nreinforcement learning, we focus on differentiable planners such as Value\nIteration Networks (VIN), which are trained offline from safe expert\ndemonstrations. Although they work well in small simulations, we address two\nmajor limitations that hinder their deployment. First, we observed that current\ndifferentiable planners struggle to plan long-term in environments with a high\nbranching complexity. While they should ideally learn to assign low rewards to\nobstacles to avoid collisions, we posit that the constraints imposed on the\nnetwork are not strong enough to guarantee the network to learn sufficiently\nlarge penalties for every possible collision. We thus impose a structural\nconstraint on the value iteration, which explicitly learns to model any\nimpossible actions. Secondly, we extend the model to work with a limited\nperspective camera under translation and rotation, which is crucial for real\nrobot deployment. Many VIN-like planners assume a 360 degrees or overhead view\nwithout rotation. In contrast, our method uses a memory-efficient lattice map\nto aggregate CNN embeddings of partial observations, and models the rotational\ndynamics explicitly using a 3D state-space grid (translation and rotation). Our\nproposals significantly improve semantic navigation and exploration on several\n2D and 3D environments, succeeding in settings that are otherwise challenging\nfor this class of methods. As far as we know, we are the first to successfully\nperform differentiable planning on the difficult Active Vision Dataset,\nconsisting of real images captured from a robot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ishida_S/0/1/0/all/0/1\">Shu Ishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Mesh Deep Learning for Automated Tooth Segmentation and Landmark Localization on 3D Intraoral Scans. (arXiv:2109.11941v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11941","description":"<p>Accurately segmenting teeth and identifying the corresponding anatomical\nlandmarks on dental mesh models are essential in computer-aided orthodontic\ntreatment. Manually performing these two tasks is time-consuming, tedious, and,\nmore importantly, highly dependent on orthodontists' experiences due to the\nabnormality and large-scale variance of patients' teeth. Some machine\nlearning-based methods have been designed and applied in the orthodontic field\nto automatically segment dental meshes (e.g., intraoral scans). In contrast,\nthe number of studies on tooth landmark localization is still limited. This\npaper proposes a two-stage framework based on mesh deep learning (called\nTS-MDL) for joint tooth labeling and landmark identification on raw intraoral\nscans. Our TS-MDL first adopts an end-to-end \\emph{i}MeshSegNet method (i.e., a\nvariant of the existing MeshSegNet with both improved accuracy and efficiency)\nto label each tooth on the downsampled scan. Guided by the segmentation\noutputs, our TS-MDL further selects each tooth's region of interest (ROI) on\nthe original mesh to construct a light-weight variant of the pioneering\nPointNet (i.e., PointNet-Reg) for regressing the corresponding landmark\nheatmaps. Our TS-MDL was evaluated on a real-clinical dataset, showing\npromising segmentation and localization performance. Specifically,\n\\emph{i}MeshSegNet in the first stage of TS-MDL reached an averaged Dice\nsimilarity coefficient (DSC) at \\textcolor[rgb]{0,0,0}{$0.964\\pm0.054$},\nsignificantly outperforming the original MeshSegNet. In the second stage,\nPointNet-Reg achieved a mean absolute error (MAE) of $0.597\\pm0.761 \\, mm$ in\ndistances between the prediction and ground truth for $66$ landmarks, which is\nsuperior compared with other networks for landmark detection. All these results\nsuggest the potential usage of our TS-MDL in orthodontics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tai-Hsien Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sanghee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pastewait_M/0/1/0/all/0/1\">Matthew Pastewait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piers_C/0/1/0/all/0/1\">Christian Piers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chiung-Ying Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenchi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_C/0/1/0/all/0/1\">Christina Jackson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1\">Ching-Chang Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffusionCLIP: Text-Guided Diffusion Models for Robust Image Manipulation. (arXiv:2110.02711v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02711","description":"<p>Recently, GAN inversion methods combined with Contrastive Language-Image\nPretraining (CLIP) enables zero-shot image manipulation guided by text prompts.\nHowever, their applications to diverse real images are still difficult due to\nthe limited GAN inversion capability. Specifically, these approaches often have\ndifficulties in reconstructing images with novel poses, views, and highly\nvariable contents compared to the training data, altering object identity, or\nproducing unwanted image artifacts. To mitigate these problems and enable\nfaithful manipulation of real images, we propose a novel method, dubbed\nDiffusionCLIP, that performs text-driven image manipulation using diffusion\nmodels. Based on full inversion capability and high-quality image generation\npower of recent diffusion models, our method performs zero-shot image\nmanipulation successfully even between unseen domains and takes another step\ntowards general application by manipulating images from a widely varying\nImageNet dataset. Furthermore, we propose a novel noise combination method that\nallows straightforward multi-attribute manipulation. Extensive experiments and\nhuman evaluation confirmed robust and superior manipulation performance of our\nmethods compared to the existing baselines. Code is available at\nhttps://github.com/gwang-kim/DiffusionCLIP.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gwanghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_T/0/1/0/all/0/1\">Taesung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Feature-Level Adversaries are Interpretability Tools. (arXiv:2110.03605v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03605","description":"<p>The literature on adversarial attacks in computer vision typically focuses on\npixel-level perturbations. These tend to be very difficult to interpret. Recent\nwork that manipulates the latent representations of image generators to create\n\"feature-level\" adversarial perturbations gives us an opportunity to explore\ninterpretable adversarial attacks. We make three contributions. First, we\nobserve that feature-level attacks provide useful classes of inputs for\nstudying the representations in models. Second, we show that these adversaries\nare versatile and highly robust. We demonstrate that they can be used to\nproduce targeted, universal, disguised, physically-realizable, and black-box\nattacks at the ImageNet scale. Third, we show how these adversarial images can\nbe used as a practical interpretability tool for identifying bugs in networks.\nWe use these adversaries to make predictions about spurious associations\nbetween features and classes which we then test by designing \"copy/paste\"\nattacks in which one natural image is pasted into another to cause a targeted\nmisclassification. Our results indicate that feature-level attacks are a\npromising approach for rigorous interpretability research. They support the\ndesign of tools to better understand what a model has learned and diagnose\nbrittle feature associations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1\">Max Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hybrid Spatial-temporal Deep Learning Architecture for Lane Detection. (arXiv:2110.04079v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04079","description":"<p>Accurate and reliable lane detection is vital for the safe performance of\nlane-keeping assistance and lane departure warning systems. However, under\ncertain challenging circumstances, it is difficult to get satisfactory\nperformance in accurately detecting the lanes from one single image as mostly\ndone in current literature. Since lane markings are continuous lines, the lanes\nthat are difficult to be accurately detected in the current single image can\npotentially be better deduced if information from previous frames is\nincorporated. This study proposes a novel hybrid spatial-temporal (ST)\nsequence-to-one deep learning architecture. This architecture makes full use of\nthe ST information in multiple continuous image frames to detect the lane\nmarkings in the very last frame. Specifically, the hybrid model integrates the\nfollowing aspects: (a) the single image feature extraction module equipped with\nthe spatial convolutional neural network; (b) the ST feature integration module\nconstructed by ST recurrent neural network; (c) the encoder-decoder structure,\nwhich makes this image segmentation problem work in an end-to-end supervised\nlearning format. Extensive experiments reveal that the proposed model\narchitecture can effectively handle challenging driving scenes and outperforms\navailable state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yongqi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_S/0/1/0/all/0/1\">Sandeep Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arem_B/0/1/0/all/0/1\">Bart van Arem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farah_H/0/1/0/all/0/1\">Haneen Farah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Voting with Relational Box Fields for Active Object Detection. (arXiv:2110.11524v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11524","description":"<p>A key component of understanding hand-object interactions is the ability to\nidentify the active object -- the object that is being manipulated by the human\nhand. In order to accurately localize the active object, any method must reason\nusing information encoded by each image pixel, such as whether it belongs to\nthe hand, the object, or the background. To leverage each pixel as evidence to\ndetermine the bounding box of the active object, we propose a pixel-wise voting\nfunction. Our pixel-wise voting function takes an initial bounding box as input\nand produces an improved bounding box of the active object as output. The\nvoting function is designed so that each pixel inside of the input bounding box\nvotes for an improved bounding box, and the box with the majority vote is\nselected as the output. We call the collection of bounding boxes generated\ninside of the voting function, the Relational Box Field, as it characterizes a\nfield of bounding boxes defined in relationship to the current bounding box.\nWhile our voting function is able to improve the bounding box of the active\nobject, one round of voting is typically not enough to accurately localize the\nactive object. Therefore, we repeatedly apply the voting function to\nsequentially improve the location of the bounding box. However, since it is\nknown that repeatedly applying a one-step predictor (i.e., auto-regressive\nprocessing with our voting function) can cause a data distribution shift, we\nmitigate this issue using reinforcement learning (RL). We adopt standard RL to\nlearn the voting function parameters and show that it provides a meaningful\nimprovement over a standard supervised learning approach. We perform\nexperiments on two large-scale datasets: 100DOH and MECCANO, improving AP50\nperformance by 8% and 30%, respectively, over the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xingyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically Explainable CNN for SAR Image Classification. (arXiv:2110.14144v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.14144","description":"<p>Integrating the special electromagnetic characteristics of Synthetic Aperture\nRadar (SAR) in deep neural networks is essential in order to enhance the\nexplainability and physics awareness of deep learning. In this paper, we first\npropose a novel physically explainable convolutional neural network for SAR\nimage classification, namely physics guided and injected learning (PGIL). It\ncomprises three parts: (1) explainable models (XM) to provide prior physics\nknowledge, (2) physics guided network (PGN) to encode the knowledge into\nphysics-aware features, and (3) physics injected network (PIN) to adaptively\nintroduce the physics-aware features into classification pipeline for label\nprediction. A hybrid Image-Physics SAR dataset format is proposed for\nevaluation, with both Sentinel-1 and Gaofen-3 SAR data being experimented. The\nresults show that the proposed PGIL substantially improve the classification\nperformance in case of limited labeled data compared with the counterpart\ndata-driven CNN and other pre-training methods. Additionally, the physics\nexplanations are discussed to indicate the interpretability and the physical\nconsistency preserved in the predictions. We deem the proposed method would\npromote the development of physically explainable deep learning in SAR image\ninterpretation field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongling Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_X/0/1/0/all/0/1\">Xiwen Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dumitru_C/0/1/0/all/0/1\">Corneliu Octavian Dumitru</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Datcu_M/0/1/0/all/0/1\">Mihai Datcu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining machine learning with physics: A framework for tracking and sorting multiple dark solitons. (arXiv:2111.04881v2 [cond-mat.quant-gas] UPDATED)","link":"http://arxiv.org/abs/2111.04881","description":"<p>In ultracold-atom experiments, data often comes in the form of images which\nsuffer information loss inherent in the techniques used to prepare and measure\nthe system. This is particularly problematic when the processes of interest are\ncomplicated, such as interactions among excitations in Bose-Einstein\ncondensates (BECs). In this paper, we describe a framework combining machine\nlearning (ML) models with physics-based traditional analyses to identify and\ntrack multiple solitonic excitations in images of BECs. We use an ML-based\nobject detector to locate the solitonic excitations and develop a\nphysics-informed classifier to sort solitonic excitations into physically\nmotivated subcategories. Lastly, we introduce a quality metric quantifying the\nlikelihood that a specific feature is a longitudinal soliton. Our trained\nimplementation of this framework, SolDet, is publicly available as an\nopen-source python package. SolDet is broadly applicable to feature\nidentification in cold-atom images when trained on a suitable user-provided\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Guo_S/0/1/0/all/0/1\">Shangjie Guo</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Koh_S/0/1/0/all/0/1\">Sophia M. Koh</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Fritsch_A/0/1/0/all/0/1\">Amilson R. Fritsch</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Spielman_I/0/1/0/all/0/1\">I. B. Spielman</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zwolak_J/0/1/0/all/0/1\">Justyna P. Zwolak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Equivalence of Siamese Self-Supervised Learning via A Unified Gradient Framework. (arXiv:2112.05141v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05141","description":"<p>Self-supervised learning has shown its great potential to extract powerful\nvisual representations without human annotations. Various works are proposed to\ndeal with self-supervised learning from different perspectives: (1) contrastive\nlearning methods (e.g., MoCo, SimCLR) utilize both positive and negative\nsamples to guide the training direction; (2) asymmetric network methods (e.g.,\nBYOL, SimSiam) get rid of negative samples via the introduction of a predictor\nnetwork and the stop-gradient operation; (3) feature decorrelation methods\n(e.g., Barlow Twins, VICReg) instead aim to reduce the redundancy between\nfeature dimensions. These methods appear to be quite different in the designed\nloss functions from various motivations. The final accuracy numbers also vary,\nwhere different networks and tricks are utilized in different works. In this\nwork, we demonstrate that these methods can be unified into the same form.\nInstead of comparing their loss functions, we derive a unified formula through\ngradient analysis. Furthermore, we conduct fair and detailed experiments to\ncompare their performances. It turns out that there is little gap between these\nmethods, and the use of momentum encoder is the key factor to boost\nperformance. From this unified framework, we propose UniGrad, a simple but\neffective gradient form for self-supervised learning. It does not require a\nmemory bank or a predictor network, but can still achieve state-of-the-art\nperformance and easily adopt other training strategies. Extensive experiments\non linear evaluation and many downstream tasks also show its effectiveness.\nCode shall be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chenxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Honghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiahua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shiji Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatteGAN: Visually Guided Language Attention for Multi-Turn Text-Conditioned Image Manipulation. (arXiv:2112.13985v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13985","description":"<p>Text-guided image manipulation tasks have recently gained attention in the\nvision-and-language community. While most of the prior studies focused on\nsingle-turn manipulation, our goal in this paper is to address the more\nchallenging multi-turn image manipulation (MTIM) task. Previous models for this\ntask successfully generate images iteratively, given a sequence of instructions\nand a previously generated image. However, this approach suffers from\nunder-generation and a lack of generated quality of the objects that are\ndescribed in the instructions, which consequently degrades the overall\nperformance. To overcome these problems, we present a novel architecture called\na Visually Guided Language Attention GAN (LatteGAN). Here, we address the\nlimitations of the previous approaches by introducing a Visually Guided\nLanguage Attention (Latte) module, which extracts fine-grained text\nrepresentations for the generator, and a Text-Conditioned U-Net discriminator\narchitecture, which discriminates both the global and local representations of\nfake or real images. Extensive experiments on two distinct MTIM datasets,\nCoDraw and i-CLEVR, demonstrate the state-of-the-art performance of the\nproposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsumori_S/0/1/0/all/0/1\">Shoya Matsumori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_Y/0/1/0/all/0/1\">Yuki Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shingyouchi_K/0/1/0/all/0/1\">Kosuke Shingyouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imai_M/0/1/0/all/0/1\">Michita Imai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoftDropConnect (SDC) -- Effective and Efficient Quantification of the Network Uncertainty in Deep MR Image Analysis. (arXiv:2201.08418v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.08418","description":"<p>Recently, deep learning has achieved remarkable successes in medical image\nanalysis. Although deep neural networks generate clinically important\npredictions, they have inherent uncertainty. Such uncertainty is a major\nbarrier to report these predictions with confidence. In this paper, we propose\na novel yet simple Bayesian inference approach called SoftDropConnect (SDC) to\nquantify the network uncertainty in medical imaging tasks with gliomas\nsegmentation and metastases classification as initial examples. Our key idea is\nthat during training and testing SDC modulates network parameters continuously\nso as to allow affected information processing channels still in operation,\ninstead of disabling them as Dropout or DropConnet does. When compared with\nthree popular Bayesian inference methods including Bayes By Backprop, Dropout,\nand DropConnect, our SDC method (SDC-W after optimization) outperforms the\nthree competing methods with a substantial margin. Quantitatively, our proposed\nmethod generates substantial improvements in prediction accuracy (by 3.4%,\n2.5%, and 6.7% respectively for whole tumor segmentation in terms of dice\nscore; and by 11.7%, 3.9%, and 8.7% respectively for brain metastases\nclassification) and greatly reduced epistemic and aleatoric uncertainties. Our\napproach promises to deliver better diagnostic performance and make medical AI\nimaging more explainable and trustworthy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whitlow_C/0/1/0/all/0/1\">Christopher T. Whitlow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-Lambda: Disentangling Dynamic Task Relationships. (arXiv:2202.03091v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.03091","description":"<p>Understanding the structure of multiple related tasks allows for multi-task\nlearning to improve the generalisation ability of one or all of them. However,\nit usually requires training each pairwise combination of tasks together in\norder to capture task relationships, at an extremely high computational cost.\nIn this work, we learn task relationships via an automated weighting framework,\nnamed Auto-Lambda. Unlike previous methods where task relationships are assumed\nto be fixed, Auto-Lambda is a gradient-based meta learning framework which\nexplores continuous, dynamic task relationships via task-specific weightings,\nand can optimise any choice of combination of tasks through the formulation of\na meta-loss; where the validation loss automatically influences task weightings\nthroughout training. We apply the proposed framework to both multi-task and\nauxiliary learning problems in computer vision and robotics, and show that\nAuto-Lambda achieves state-of-the-art performance, even when compared to\noptimisation strategies designed specifically for each problem and data domain.\nFinally, we observe that Auto-Lambda can discover interesting learning\nbehaviors, leading to new insights in multi-task learning. Code is available at\nhttps://github.com/lorenmt/auto-lambda.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shikun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davison_A/0/1/0/all/0/1\">Andrew J. Davison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1\">Edward Johns</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncalibrated Models Can Improve Human-AI Collaboration. (arXiv:2202.05983v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2202.05983","description":"<p>In many practical applications of AI, an AI model is used as a decision aid\nfor human users. The AI provides advice that a human (sometimes) incorporates\ninto their decision-making process. The AI advice is often presented with some\nmeasure of \"confidence\" that the human can use to calibrate how much they\ndepend on or trust the advice. In this paper, we demonstrate that human-AI\nperformance can be improved by calibrating this confidence to the humans using\nthe advice. In practice, this means presenting calibrated AI models as more or\nless confident than they actually are. We show empirically that this can\nimprove human-AI performance (measured as the accuracy and confidence of the\nhuman's final prediction after seeing the AI advice). We first train a model to\npredict human incorporation of AI advice using data from thousands of human\ninteractions. This enables us to explicitly estimate how to transform the AI's\nprediction confidence, making the AI uncalibrated, in order to improve the\nfinal human prediction. We empirically validate our results across four\ndifferent tasks--dealing with images, text and tabular data--involving hundreds\nof human participants. We further support our findings with simulation\nanalysis. Our findings suggest the importance of and a framework for jointly\noptimizing the human-AI system in contrast to the standard paradigm of\noptimizing the AI model alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1\">Kailas Vodrahalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstenberg_T/0/1/0/all/0/1\">Tobias Gerstenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">James Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rare Gems: Finding Lottery Tickets at Initialization. (arXiv:2202.12002v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.12002","description":"<p>Large neural networks can be pruned to a small fraction of their original\nsize, with little loss in accuracy, by following a time-consuming \"train,\nprune, re-train\" approach. Frankle &amp; Carbin conjecture that we can avoid this\nby training \"lottery tickets\", i.e., special sparse subnetworks found at\ninitialization, that can be trained to high accuracy. However, a subsequent\nline of work by Frankle et al. and Su et al. presents concrete evidence that\ncurrent algorithms for finding trainable networks at initialization, fail\nsimple baseline comparisons, e.g., against training random sparse subnetworks.\nFinding lottery tickets that train to better accuracy compared to simple\nbaselines remains an open problem. In this work, we resolve this open problem\nby proposing Gem-Miner which finds lottery tickets at initialization that beat\ncurrent baselines. Gem-Miner finds lottery tickets trainable to accuracy\ncompetitive or better than Iterative Magnitude Pruning (IMP), and does so up to\n$19\\times$ faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sreenivasan_K/0/1/0/all/0/1\">Kartik Sreenivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_J/0/1/0/all/0/1\">Jy-yong Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grinde_M/0/1/0/all/0/1\">Matthew Grinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagle_A/0/1/0/all/0/1\">Alliot Nagle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangwook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training privacy-preserving video analytics pipelines by suppressing features that reveal information about private attributes. (arXiv:2203.02635v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02635","description":"<p>Deep neural networks are increasingly deployed for scene analytics, including\nto evaluate the attention and reaction of people exposed to out-of-home\nadvertisements. However, the features extracted by a deep neural network that\nwas trained to predict a specific, consensual attribute (e.g. emotion) may also\nencode and thus reveal information about private, protected attributes (e.g.\nage or gender). In this work, we focus on such leakage of private information\nat inference time. We consider an adversary with access to the features\nextracted by the layers of a deployed neural network and use these features to\npredict private attributes. To prevent the success of such an attack, we modify\nthe training of the network using a confusion loss that encourages the\nextraction of features that make it difficult for the adversary to accurately\npredict private attributes. We validate this training approach on image-based\ntasks using a publicly available dataset. Results show that, compared to the\noriginal network, the proposed PrivateNet can reduce the leakage of private\ninformation of a state-of-the-art emotion recognition classifier by 2.88% for\ngender and by 13.06% for age group, with a minimal effect on task accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chau Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallaro_A/0/1/0/all/0/1\">Andrea Cavallaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04203","description":"<p>A long-standing goal of intelligent assistants such as AR glasses/robots has\nbeen to assist users in affordance-centric real-world scenarios, such as \"how\ncan I run the microwave for 1 minute?\". However, there is still no clear task\ndefinition and suitable benchmarks. In this paper, we define a new task called\nAffordance-centric Question-driven Task Completion, where the AI assistant\nshould learn from instructional videos and scripts to guide the user\nstep-by-step. To support the task, we constructed AssistQ, a new dataset\ncomprising 531 question-answer samples derived from 100 newly filmed\nfirst-person videos. Each question should be completed with multi-step\nguidances by inferring from visual details (e.g., buttons' position) and\ntextural details (e.g., actions like press/turn). To address this unique task,\nwe developed a Question-to-Actions (Q2A) model that significantly outperforms\nseveral baseline methods while still having large room for improvement. We\nexpect our task and dataset to advance Egocentric AI Assistant's development.\nOur project page is available at: https://showlab.github.io/assistq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_B/0/1/0/all/0/1\">Benita Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Joya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1\">Dongxing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Efficient Person Re-Identification with Diverse Cluster-Based Pair Selection. (arXiv:2203.05395v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05395","description":"<p>Person Re-identification (Re-ID) has attracted great attention due to its\npromising real-world applications. However, in practice, it is always costly to\nannotate the training data to train a Re-ID model, and it still remains\nchallenging to reduce the annotation cost while maintaining the performance for\nthe Re-ID task. To solve this problem, we propose the Annotation Efficient\nPerson Re-Identification method to select image pairs from an alternative pair\nset according to the fallibility and diversity of pairs, and train the Re-ID\nmodel based on the annotation. Specifically, we design an annotation and\ntraining framework to firstly reduce the size of the alternative pair set by\nclustering all images considering the locality of features, secondly select\nimages pairs from intra-/inter-cluster samples for human to annotate, thirdly\nre-assign clusters according to the annotation, and finally train the model\nwith the re-assigned clusters. During the pair selection, we seek for valuable\npairs according to pairs' fallibility and diversity, which includes an\nintra-cluster criterion to construct image pairs with the most chaotic samples\nand the representative samples within clusters, an inter-cluster criterion to\nconstruct image pairs between clusters based on the second-order Wasserstein\ndistance, and a diversity criterion for clusterbased pair selection. Combining\nall criteria above, a greedy strategy is developed to solve the pair selection\nproblem. Finally, the above\nclustering-selecting-annotating-reassigning-training procedure will be repeated\nuntil the annotation budget is reached. Extensive experiments on three widely\nadopted Re-ID datasets show that we can greatly reduce the annotation cost\nwhile achieving better performance compared with state-of-the-art works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lantian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yixiong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Font Generation with Missing Impression Labels. (arXiv:2203.10348v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10348","description":"<p>Our goal is to generate fonts with specific impressions, by training a\ngenerative adversarial network with a font dataset with impression labels. The\nmain difficulty is that font impression is ambiguous and the absence of an\nimpression label does not always mean that the font does not have the\nimpression. This paper proposes a font generation model that is robust against\nmissing impression labels. The key ideas of the proposed method are (1)a\nco-occurrence-based missing label estimator and (2)an impression label space\ncompressor. The first is to interpolate missing impression labels based on the\nco-occurrence of labels in the dataset and use them for training the model as\ncompleted label conditions. The second is an encoder-decoder module to compress\nthe high-dimensional impression space into low-dimensional. We proved that the\nproposed model generates high-quality font images using multi-label data with\nmissing labels through qualitative and quantitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsuda_S/0/1/0/all/0/1\">Seiya Matsuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_A/0/1/0/all/0/1\">Akisato Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APP: Anytime Progressive Pruning. (arXiv:2204.01640v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.01640","description":"<p>With the latest advances in deep learning, there has been a lot of focus on\nthe online learning paradigm due to its relevance in practical settings.\nAlthough many methods have been investigated for optimal learning settings in\nscenarios where the data stream is continuous over time, sparse networks\ntraining in such settings have often been overlooked. In this paper, we explore\nthe problem of training a neural network with a target sparsity in a particular\ncase of online learning: the anytime learning at macroscale paradigm (ALMA). We\npropose a novel way of progressive pruning, referred to as \\textit{Anytime\nProgressive Pruning} (APP); the proposed approach significantly outperforms the\nbaseline dense and Anytime OSP models across multiple architectures and\ndatasets under short, moderate, and long-sequence training. Our method, for\nexample, shows an improvement in accuracy of $\\approx 7\\%$ and a reduction in\nthe generalization gap by $\\approx 22\\%$, while being $\\approx 1/3$ rd the size\nof the dense baseline model in few-shot restricted imagenet training. We\nfurther observe interesting nonmonotonic transitions in the generalization gap\nin the high number of megabatches-based ALMA. The code and experiment\ndashboards can be accessed at\n\\url{https://github.com/landskape-ai/Progressive-Pruning} and\n\\url{https://wandb.ai/landskape/APP}, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1\">Diganta Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runwal_B/0/1/0/all/0/1\">Bharat Runwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning with Boosted Memorization. (arXiv:2205.12693v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.12693","description":"<p>Self-supervised learning has achieved a great success in the representation\nlearning of visual and textual data. However, the current methods are mainly\nvalidated on the well-curated datasets, which do not exhibit the real-world\nlong-tailed distribution. Recent attempts to consider self-supervised\nlong-tailed learning are made by rebalancing in the loss perspective or the\nmodel perspective, resembling the paradigms in the supervised long-tailed\nlearning. Nevertheless, without the aid of labels, these explorations have not\nshown the expected significant promise due to the limitation in tail sample\ndiscovery or the heuristic structure design. Different from previous works, we\nexplore this direction from an alternative perspective, i.e., the data\nperspective, and propose a novel Boosted Contrastive Learning (BCL) method.\nSpecifically, BCL leverages the memorization effect of deep neural networks to\nautomatically drive the information discrepancy of the sample views in\ncontrastive learning, which is more efficient to enhance the long-tailed\nlearning in the label-unaware context. Extensive experiments on a range of\nbenchmark datasets demonstrate the effectiveness of BCL over several\nstate-of-the-art methods. Our code is available at\nhttps://github.com/MediaBrain-SJTU/BCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Penalizing Proposals using Classifiers for Semi-Supervised Object Detection. (arXiv:2205.13219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13219","description":"<p>Obtaining gold standard annotated data for object detection is often costly,\ninvolving human-level effort. Semi-supervised object detection algorithms solve\nthe problem with a small amount of gold-standard labels and a large unlabelled\ndataset used to generate silver-standard labels. But training on the silver\nstandard labels does not produce good results, because they are\nmachine-generated annotations. In this work, we design a modified loss function\nto train on large silver standard annotated sets generated by a weak annotator.\nWe include a confidence metric associated with the annotation as an additional\nterm in the loss function, signifying the quality of the annotation. We test\nthe effectiveness of our approach on various test sets and use numerous\nvariations to compare the results with some of the current approaches to object\ndetection. In comparison with the baseline where no confidence metric is used,\nwe achieved a 4% gain in mAP with 25% labeled data and 10% gain in mAP with 50%\nlabeled data by using the proposed confidence metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hazra_S/0/1/0/all/0/1\">Somnath Hazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_P/0/1/0/all/0/1\">Pallab Dasgupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient textual explanations for complex road and traffic scenarios based on semantic segmentation. (arXiv:2205.14118v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14118","description":"<p>The complex driving environment brings great challenges to the visual\nperception of autonomous vehicles. It's essential to extract clear and\nexplainable information from the complex road and traffic scenarios and offer\nclues to decision and control. However, the previous scene explanation had been\nimplemented as a separate model. The black box model makes it difficult to\ninterpret the driving environment. It cannot detect comprehensive textual\ninformation and requires a high computational load and time consumption. Thus,\nthis study proposed a comprehensive and efficient textual explanation model.\nFrom 336k video frames of the driving environment, critical images of complex\nroad and traffic scenarios were selected into a dataset. Through transfer\nlearning, this study established an accurate and efficient segmentation model\nto obtain the critical traffic elements in the environment. Based on the\nXGBoost algorithm, a comprehensive model was developed. The model provided\ntextual information about states of traffic elements, the motion of conflict\nobjects, and scenario complexity. The approach was verified on the real-world\nroad. It improved the perception accuracy of critical traffic elements to\n78.8%. The time consumption reached 13 minutes for each epoch, which was 11.5\ntimes more efficient than the pre-trained network. The textual information\nanalyzed from the model was also accordant with reality. The findings offer\nclear and explainable information about the complex driving environment, which\nlays a foundation for subsequent decision and control. It can improve the\nvisual perception ability and enrich the prior knowledge and judgments of\ncomplex traffic situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiyue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_X/0/1/0/all/0/1\">Xinyu Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_C/0/1/0/all/0/1\">Chen Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenxuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Lip Region-of-Interest Sufficient for Lipreading?. (arXiv:2205.14295v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14295","description":"<p>Lip region-of-interest (ROI) is conventionally used for visual input in the\nlipreading task. Few works have adopted the entire face as visual input because\nlip-excluded parts of the face are usually considered to be redundant and\nirrelevant to visual speech recognition. However, faces contain much more\ndetailed information than lips, such as speakers' head pose, emotion, identity\netc. We argue that such information might benefit visual speech recognition if\na powerful feature extractor employing the entire face is trained. In this\nwork, we propose to adopt the entire face for lipreading with self-supervised\nlearning. AV-HuBERT, an audio-visual multi-modal self-supervised learning\nframework, was adopted in our experiments. Our experimental results showed that\nadopting the entire face achieved 16% relative word error rate (WER) reduction\non the lipreading task, compared with the baseline method using lip as visual\ninput. Without self-supervised pretraining, the model with face input achieved\na higher WER than that using lip input in the case of limited training data (30\nhours), while a slightly lower WER when using large amount of training data\n(433 hours).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing-Xuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_G/0/1/0/all/0/1\">Gen-Shun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jia Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepRM: Deep Recurrent Matching for 6D Pose Refinement. (arXiv:2205.14474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14474","description":"<p>Precise 6D pose estimation of rigid objects from RGB images is a critical but\nchallenging task in robotics and augmented reality. To address this problem, we\npropose DeepRM, a novel recurrent network architecture for 6D pose refinement.\nDeepRM leverages initial coarse pose estimates to render synthetic images of\ntarget objects. The rendered images are then matched with the observed images\nto predict a rigid transform for updating the previous pose estimate. This\nprocess is repeated to incrementally refine the estimate at each iteration.\nLSTM units are used to propagate information through each refinement step,\nsignificantly improving overall performance. In contrast to many 2-stage\nPerspective-n-Point based solutions, DeepRM is trained end-to-end, and uses a\nscalable backbone that can be tuned via a single parameter for accuracy and\nefficiency. During training, a multi-scale optical flow head is added to\npredict the optical flow between the observed and synthetic images. Optical\nflow prediction stabilizes the training process, and enforces the learning of\nfeatures that are relevant to the task of pose estimation. Our results\ndemonstrate that DeepRM achieves state-of-the-art performance on two widely\naccepted challenging datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avery_A/0/1/0/all/0/1\">Alexander Avery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savakis_A/0/1/0/all/0/1\">Andreas Savakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Batch Normalization Is Blind to the First and Second Derivatives of the Loss. (arXiv:2205.15146v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.15146","description":"<p>In this paper, we prove the effects of the BN operation on the\nback-propagation of the first and second derivatives of the loss. When we do\nthe Taylor series expansion of the loss function, we prove that the BN\noperation will block the influence of the first-order term and most influence\nof the second-order term of the loss. We also find that such a problem is\ncaused by the standardization phase of the BN operation. Experimental results\nhave verified our theoretical conclusions, and we have found that the BN\noperation significantly affects feature representations in specific tasks,\nwhere losses of different samples share similar analytic formulas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhanpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Ling Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial synthesis based data-augmentation for code-switched spoken language identification. (arXiv:2205.15747v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2205.15747","description":"<p>Spoken Language Identification (LID) is an important sub-task of Automatic\nSpeech Recognition(ASR) that is used to classify the language(s) in an audio\nsegment. Automatic LID plays an useful role in multilingual countries. In\nvarious countries, identifying a language becomes hard, due to the multilingual\nscenario where two or more than two languages are mixed together during\nconversation. Such phenomenon of speech is called as code-mixing or\ncode-switching. This nature is followed not only in India but also in many\nAsian countries. Such code-mixed data is hard to find, which further reduces\nthe capabilities of the spoken LID. Hence, this work primarily addresses this\nproblem using data augmentation as a solution on the on the data scarcity of\nthe code-switched class. This study focuses on Indic language code-mixed with\nEnglish. Spoken LID is performed on Hindi, code-mixed with English. This\nresearch proposes Generative Adversarial Network (GAN) based data augmentation\ntechnique performed using Mel spectrograms for audio data. GANs have already\nbeen proven to be accurate in representing the real data distribution in the\nimage domain. Proposed research exploits these capabilities of GANs in speech\ndomains such as speech classification, automatic speech recognition, etc. GANs\nare trained to generate Mel spectrograms of the minority code-mixed class which\nare then used to augment data for the classifier. Utilizing GANs give an\noverall improvement on Unweighted Average Recall by an amount of 3.5% as\ncompared to a Convolutional Recurrent Neural Network (CRNN) classifier used as\nthe baseline reference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shastri_P/0/1/0/all/0/1\">Parth Shastri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patil_C/0/1/0/all/0/1\">Chirag Patil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wanere_P/0/1/0/all/0/1\">Poorval Wanere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahajan_D/0/1/0/all/0/1\">Dr. Shrinivas Mahajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhatt_D/0/1/0/all/0/1\">Dr. Abhishek Bhatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sailor_D/0/1/0/all/0/1\">Dr. Hardik Sailor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Deep Learning Classifier by Detection of Prototypical Parts on Kidney Stones Images. (arXiv:2206.00252v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00252","description":"<p>Identifying the type of kidney stones can allow urologists to determine their\nformation cause, improving the early prescription of appropriate treatments to\ndiminish future relapses. However, currently, the associated ex-vivo diagnosis\n(known as morpho-constitutional analysis, MCA) is time-consuming, expensive,\nand requires a great deal of experience, as it requires a visual analysis\ncomponent that is highly operator dependant. Recently, machine learning methods\nhave been developed for in-vivo endoscopic stone recognition. Shallow methods\nhave been demonstrated to be reliable and interpretable but exhibit low\naccuracy, while deep learning-based methods yield high accuracy but are not\nexplainable. However, high stake decisions require understandable\ncomputer-aided diagnosis (CAD) to suggest a course of action based on\nreasonable evidence, rather than merely prescribe one. Herein, we investigate\nmeans for learning part-prototypes (PPs) that enable interpretable models. Our\nproposal suggests a classification for a kidney stone patch image and provides\nexplanations in a similar way as those used on the MCA method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Flores_Araiza_D/0/1/0/all/0/1\">Daniel Flores-Araiza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Tiro_F/0/1/0/all/0/1\">Francisco Lopez-Tiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villalvazo_Avila_E/0/1/0/all/0/1\">Elias Villalvazo-Avila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Beze_J/0/1/0/all/0/1\">Jonathan El-Beze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubert_J/0/1/0/all/0/1\">Jacques Hubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daul_C/0/1/0/all/0/1\">Christian Daul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Invariant Visual Representations for Compositional Zero-Shot Learning. (arXiv:2206.00415v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00415","description":"<p>Compositional Zero-Shot Learning (CZSL) aims to recognize novel compositions\nusing knowledge learned from seen attribute-object compositions in the training\nset. Previous works mainly project an image and a composition into a common\nembedding space to measure their compatibility score. However, both attributes\nand objects share the visual representations learned above, leading the model\nto exploit spurious correlations and bias towards seen pairs. Instead, we\nreconsider CZSL as an out-of-distribution generalization problem. If an object\nis treated as a domain, we can learn object-invariant features to recognize the\nattributes attached to any object reliably. Similarly, attribute-invariant\nfeatures can also be learned when recognizing the objects with attributes as\ndomains. Specifically, we propose an invariant feature learning framework to\nalign different domains at the representation and gradient levels to capture\nthe intrinsic characteristics associated with the tasks. Experiments on two\nCZSL benchmarks demonstrate that the proposed method significantly outperforms\nthe previous state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1\">Kongming Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruoyi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhanyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jun Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Caricatures: Amplifying attention to artifacts increases deepfake detection by humans and machines. (arXiv:2206.00535v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00535","description":"<p>Deepfakes pose a serious threat to our digital society by fueling the spread\nof misinformation. It is essential to develop techniques that both detect them,\nand effectively alert the human user to their presence. Here, we introduce a\nnovel deepfake detection framework that meets both of these needs. Our approach\nlearns to generate attention maps of video artifacts, semi-supervised on human\nannotations. These maps make two contributions. First, they improve the\naccuracy and generalizability of a deepfake classifier, demonstrated across\nseveral deepfake detection datasets. Second, they allow us to generate an\nintuitive signal for the human user, in the form of \"Deepfake Caricatures\":\ntransformations of the original deepfake video where attended artifacts are\nexacerbated to improve human recognition. Our approach, based on a mixture of\nhuman and artificial supervision, aims to further the development of\ncountermeasures against fake visual content, and grants humans the ability to\nmake their own judgment when presented with dubious visual media.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fosco_C/0/1/0/all/0/1\">Camilo Fosco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Josephs_E/0/1/0/all/0/1\">Emilie Josephs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andonian_A/0/1/0/all/0/1\">Alex Andonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Allen Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1\">Aude Oliva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}