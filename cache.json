{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-12-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Improving scripts with a memory of natural feedback. (arXiv:2112.09737v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09737","description":"<p>How can an end-user provide feedback if a deployed structured prediction\nmodel generates incorrect output? Our goal is to allow users to correct errors\ndirectly through interaction, without retraining, by giving feedback on the\nmodel's output. We create a dynamic memory architecture with a growing memory\nof feedbacks about errors in the output. Given a new, unseen input, our model\ncan use feedback from a similar, past erroneous state. On a script generation\ntask, we show empirically that the model learns to apply feedback effectively\n(up to 30 points improvement), while avoiding similar past mistakes after\ndeployment (up to 10 points improvement on an unseen set). This is a first step\ntowards strengthening deployed models, potentially broadening their utility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Ethical Outcomes with Machine-in-the-Loop: Broadening Human Understanding of Data Annotations. (arXiv:2112.09738v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09738","description":"<p>We introduce a machine-in-the-loop pipeline that aims to address root causes\nof unwanted bias in natural language based supervised machine learning tasks in\nthe education domain. Learning from the experiences of students is foundational\nfor education researchers, and academic administrators. 21st-century skills\nlearned from experience are becoming a core part of college and career\nreadiness as well as the hiring process in the new knowledge economy.\nMinoritized students demonstrate these skills in their daily lives, but\ndocumenting, assessing, and validating these skills is a huge problem for\neducational institutions. As an equity focused online platform, LivedX\ntranslates minoritized students' lived experiences into the 21st century\nskills, issues micro-credentials, and creates personal 21st century skills\nportfolio. To automate the micro credential mining from the natural language\ntexts received from the students' submitted essays, we employed a bag-of-word\nmodel to construct a multi-output classifier. Despite our goal, our model\ninitially exacerbated disparate impact on minoritized students. We used a\nmachine-in-the-loop model development pipeline to address the problem and\nrefine the aforementioned model to ensure fairness in its prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biswas_A/0/1/0/all/0/1\">Ashis Kumer Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1\">Geeta Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barber_J/0/1/0/all/0/1\">Justin Otto Barber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can we Fix the Scope for Coreference? Problems and Solutions for Benchmarks beyond OntoNotes. (arXiv:2112.09742v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09742","description":"<p>Current work on automatic coreference resolution has focused on the OntoNotes\nbenchmark dataset, due to both its size and consistency. However many aspects\nof the OntoNotes annotation scheme are not well understood by NLP\npractitioners, including the treatment of generic NPs, noun modifiers,\nindefinite anaphora, predication and more. These often lead to counterintuitive\nclaims, results and system behaviors. This opinion piece aims to highlight some\nof the problems with the OntoNotes rendition of coreference, and to propose a\nway forward relying on three principles: 1. a focus on semantics, not\nmorphosyntax; 2. cross-linguistic generalizability; and 3. a separation of\nidentity and scope, which can resolve old problems involving temporal and modal\ndomain consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Post-editing Effort in the English-Hindi Direction. (arXiv:2112.09841v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09841","description":"<p>We present findings from a first in-depth post-editing effort estimation\nstudy in the English-Hindi direction along multiple effort indicators. We\nconduct a controlled experiment involving professional translators, who\ncomplete assigned tasks alternately, in a translation from scratch and a\npost-edit condition. We find that post-editing reduces translation time (by\n63%), utilizes fewer keystrokes (by 59%), and decreases the number of pauses\n(by 63%) when compared to translating from scratch. We further verify the\nquality of translations thus produced via a human evaluation task in which we\ndo not detect any discernible quality differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_A/0/1/0/all/0/1\">Arafat Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujadia_V/0/1/0/all/0/1\">Vandan Mujadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dipti Misra Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morpheme Boundary Detection & Grammatical Feature Prediction for Gujarati : Dataset & Model. (arXiv:2112.09860v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09860","description":"<p>Developing Natural Language Processing resources for a low resource language\nis a challenging but essential task. In this paper, we present a Morphological\nAnalyzer for Gujarati. We have used a Bi-Directional LSTM based approach to\nperform morpheme boundary detection and grammatical feature tagging. We have\ncreated a data set of Gujarati words with lemma and grammatical features. The\nBi-LSTM based model of Morph Analyzer discussed in the paper handles the\nlanguage morphology effectively without the knowledge of any hand-crafted\nsuffix rules. To the best of our knowledge, this is the first dataset and morph\nanalyzer model for the Gujarati language which performs both grammatical\nfeature tagging and morpheme boundary detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baxi_J/0/1/0/all/0/1\">Jatayu Baxi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_D/0/1/0/all/0/1\">Dr. Brijesh Bhatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascading Adaptors to Leverage English Data to Improve Performance of Question Answering for Low-Resource Languages. (arXiv:2112.09866v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09866","description":"<p>Transformer based architectures have shown notable results on many down\nstreaming tasks including question answering. The availability of data, on the\nother hand, impedes obtaining legitimate performance for low-resource\nlanguages. In this paper, we investigate the applicability of pre-trained\nmultilingual models to improve the performance of question answering in\nlow-resource languages. We tested four combinations of language and task\nadapters using multilingual transformer architectures on seven languages\nsimilar to MLQA dataset. Additionally, we have also proposed zero-shot transfer\nlearning of low-resource question answering using language and task adapters.\nWe observed that stacking the language and the task adapters improves the\nmultilingual transformer models' performance significantly for low-resource\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandya_H/0/1/0/all/0/1\">Hariom A. Pandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardeshna_B/0/1/0/all/0/1\">Bhavik Ardeshna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_D/0/1/0/all/0/1\">Dr. Brijesh S. Bhatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Web Is Your Oyster -- Knowledge-Intensive NLP against a Very Large Web Corpus. (arXiv:2112.09924v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09924","description":"<p>In order to address the increasing demands of real-world applications, the\nresearch for knowledge-intensive NLP (KI-NLP) should advance by capturing the\nchallenges of a truly open-domain environment: web scale knowledge, lack of\nstructure, inconsistent quality, and noise. To this end, we propose a new setup\nfor evaluating existing KI-NLP tasks in which we generalize the background\ncorpus to a universal web snapshot. We repurpose KILT, a standard KI-NLP\nbenchmark initially developed for Wikipedia, and ask systems to use a subset of\nCCNet - the Sphere corpus - as a knowledge source. In contrast to Wikipedia,\nSphere is orders of magnitude larger and better reflects the full diversity of\nknowledge on the Internet. We find that despite potential gaps of coverage,\nchallenges of scale, lack of structure and lower quality, retrieval from Sphere\nenables a state-of-the-art retrieve-and-read system to match and even\noutperform Wikipedia-based models on several KILT tasks - even if we\naggressively filter content that looks like Wikipedia. We also observe that\nwhile a single dense passage index over Wikipedia can outperform a sparse BM25\nversion, on Sphere this is not yet possible. To facilitate further research\ninto this area, and minimise the community's reliance on proprietary black box\nsearch engines, we will share our indices, evaluation metrics and\ninfrastructure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piktus_A/0/1/0/all/0/1\">Aleksandra Piktus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karpukhin_V/0/1/0/all/0/1\">Vladimir Karpukhin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okhonko_D/0/1/0/all/0/1\">Dmytro Okhonko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Broscheit_S/0/1/0/all/0/1\">Samuel Broscheit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Izacard_G/0/1/0/all/0/1\">Gautier Izacard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_B/0/1/0/all/0/1\">Barlas O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grave_E/0/1/0/all/0/1\">Edouard Grave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Graph Guided Summarization for Radiology Findings. (arXiv:2112.09925v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09925","description":"<p>Radiology reports play a critical role in communicating medical findings to\nphysicians. In each report, the impression section summarizes essential\nradiology findings. In clinical practice, writing impression is highly demanded\nyet time-consuming and prone to errors for radiologists. Therefore, automatic\nimpression generation has emerged as an attractive research direction to\nfacilitate such clinical practice. Existing studies mainly focused on\nintroducing salient word information to the general text summarization\nframework to guide the selection of the key content in radiology findings.\nHowever, for this task, a model needs not only capture the important words in\nfindings but also accurately describe their relations so as to generate\nhigh-quality impressions. In this paper, we propose a novel method for\nautomatic impression generation, where a word graph is constructed from the\nfindings to record the critical words and their relations, then a Word Graph\nguided Summarization model (WGSum) is designed to generate impressions with the\nhelp of the word graph. Experimental results on two datasets, OpenI and\nMIMIC-CXR, confirm the validity and effectiveness of our proposed approach,\nwhere the state-of-the-art results are achieved on both datasets. Further\nexperiments are also conducted to analyze the impact of different graph designs\nto the performance of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinpeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhihong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaling Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tsung-Hui Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Syntactic-GCN Bert based Chinese Event Extraction. (arXiv:2112.09939v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09939","description":"<p>With the rapid development of information technology, online platforms (e.g.,\nnews portals and social media) generate enormous web information every moment.\nTherefore, it is crucial to extract structured representations of events from\nsocial streams. Generally, existing event extraction research utilizes pattern\nmatching, machine learning, or deep learning methods to perform event\nextraction tasks. However, the performance of Chinese event extraction is not\nas good as English due to the unique characteristics of the Chinese language.\nIn this paper, we propose an integrated framework to perform Chinese event\nextraction. The proposed approach is a multiple channel input neural framework\nthat integrates semantic features and syntactic features. The semantic features\nare captured by BERT architecture. The Part of Speech (POS) features and\nDependency Parsing (DP) features are captured by profiling embeddings and Graph\nConvolutional Network (GCN), respectively. We also evaluate our model on a\nreal-world dataset. Experimental results show that the proposed method\noutperforms the benchmark approaches significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingshu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_L/0/1/0/all/0/1\">Liangyu Min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Transformers for Hate Speech Detection in Conversational Code-Mixed Tweets. (arXiv:2112.09986v1 [cs.CL])","link":"http://arxiv.org/abs/2112.09986","description":"<p>In the current era of the internet, where social media platforms are easily\naccessible for everyone, people often have to deal with threats, identity\nattacks, hate, and bullying due to their association with a cast, creed,\ngender, religion, or even acceptance or rejection of a notion. Existing works\nin hate speech detection primarily focus on individual comment classification\nas a sequence labeling task and often fail to consider the context of the\nconversation. The context of a conversation often plays a substantial role when\ndetermining the author's intent and sentiment behind the tweet. This paper\ndescribes the system proposed by team MIDAS-IIITD for HASOC 2021 subtask 2, one\nof the first shared tasks focusing on detecting hate speech from Hindi-English\ncode-mixed conversations on Twitter. We approach this problem using neural\nnetworks, leveraging the transformer's cross-lingual embeddings and further\nfinetuning them for low-resource hate-speech classification in transliterated\nHindi text. Our best performing system, a hard voting ensemble of Indic-BERT,\nXLM-RoBERTa, and Multilingual BERT, achieved a macro F1 score of 0.7253,\nplacing us first on the overall leaderboard standings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooqi_Z/0/1/0/all/0/1\">Zaki Mustafa Farooqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning with Knowledge Transfer for Sentiment Classification. (arXiv:2112.10021v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10021","description":"<p>This paper studies continual learning (CL) for sentiment classification (SC).\nIn this setting, the CL system learns a sequence of SC tasks incrementally in a\nneural network, where each task builds a classifier to classify the sentiment\nof reviews of a particular product category or domain. Two natural questions\nare: Can the system transfer the knowledge learned in the past from the\nprevious tasks to the new task to help it learn a better model for the new\ntask? And, can old models for previous tasks be improved in the process as\nwell? This paper proposes a novel technique called KAN to achieve these\nobjectives. KAN can markedly improve the SC accuracy of both the new task and\nthe old tasks via forward and backward knowledge transfer. The effectiveness of\nKAN is demonstrated through extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Mental Health Classification on Social Media. (arXiv:2112.10064v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10064","description":"<p>The mental disorder of online users is determined using social media posts.\nThe major challenge in this domain is to avail the ethical clearance for using\nthe user generated text on social media platforms. Academic re searchers\nidentified the problem of insufficient and unlabeled data for mental health\nclassification. To handle this issue, we have studied the effect of data\naugmentation techniques on domain specific user generated text for mental\nhealth classification. Among the existing well established data augmentation\ntechniques, we have identified Easy Data Augmentation (EDA), conditional BERT,\nand Back Translation (BT) as the potential techniques for generating additional\ntext to improve the performance of classifiers. Further, three different\nclassifiers Random Forest (RF), Support Vector Machine (SVM) and Logistic\nRegression (LR) are employed for analyzing the impact of data augmentation on\ntwo publicly available social media datasets. The experiments mental results\nshow significant improvements in classifiers performance when trained on the\naugmented data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_G/0/1/0/all/0/1\">Gunjan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1\">Muskan Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_C/0/1/0/all/0/1\">Chandni Saxena</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Named Entity Recognition as Word-Word Relation Classification. (arXiv:2112.10070v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10070","description":"<p>So far, named entity recognition (NER) has been involved with three major\ntypes, including flat, overlapped (aka. nested), and discontinuous NER, which\nhave mostly been studied individually. Recently, a growing interest has been\nbuilt for unified NER, tackling the above three jobs concurrently with one\nsingle model. Current best-performing methods mainly include span-based and\nsequence-to-sequence models, where unfortunately the former merely focus on\nboundary identification and the latter may suffer from exposure bias. In this\nwork, we present a novel alternative by modeling the unified NER as word-word\nrelation classification, namely W^2NER. The architecture resolves the kernel\nbottleneck of unified NER by effectively modeling the neighboring relations\nbetween entity words with Next-Neighboring-Word (NNW) and Tail-Head-Word-*\n(THW-*) relations. Based on the W^2NER scheme we develop a neural framework, in\nwhich the unified NER is modeled as a 2D grid of word pairs. We then propose\nmulti-granularity 2D convolutions for better refining the grid representations.\nFinally, a co-predictor is used to sufficiently reason the word-word relations.\nWe perform extensive experiments on 14 widely-used benchmark datasets for flat,\noverlapped, and discontinuous NER (8 English and 6 Chinese datasets), where our\nmodel beats all the current top-performing baselines, pushing the\nstate-of-the-art performances of unified NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingye Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hao Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengqiong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_C/0/1/0/all/0/1\">Chong Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigation of Densely Connected Convolutional Networks with Domain Adversarial Learning for Noise Robust Speech Recognition. (arXiv:2112.10108v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10108","description":"<p>We investigate densely connected convolutional networks (DenseNets) and their\nextension with domain adversarial training for noise robust speech recognition.\nDenseNets are very deep, compact convolutional neural networks which have\ndemonstrated incredible improvements over the state-of-the-art results in\ncomputer vision. Our experimental results reveal that DenseNets are more robust\nagainst noise than other neural network based models such as deep feed forward\nneural networks and convolutional neural networks. Moreover, domain adversarial\nlearning can further improve the robustness of DenseNets against both, known\nand unknown noise conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chia Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Detection of Security-Relevant Bug Reports using Machine Learning: How Far Are We?. (arXiv:2112.10123v1 [cs.SE])","link":"http://arxiv.org/abs/2112.10123","description":"<p>Bug reports are common artefacts in software development. They serve as the\nmain channel for users to communicate to developers information about the\nissues that they encounter when using released versions of software programs.\nIn the descriptions of issues, however, a user may, intentionally or not,\nexpose a vulnerability. In a typical maintenance scenario, such\nsecurity-relevant bug reports are prioritised by the development team when\npreparing corrective patches. Nevertheless, when security relevance is not\nimmediately expressed (e.g., via a tag) or rapidly identified by triaging\nteams, the open security-relevant bug report can become a critical leak of\nsensitive information that attackers can leverage to perform zero-day attacks.\nTo support practitioners in triaging bug reports, the research community has\nproposed a number of approaches for the detection of security-relevant bug\nreports. In recent years, approaches in this respect based on machine learning\nhave been reported with promising performance. Our work focuses on such\napproaches, and revisits their building blocks to provide a comprehensive view\non the current achievements. To that end, we built a large experimental dataset\nand performed extensive experiments with variations in feature sets and\nlearning algorithms. Eventually, our study highlights different approach\nconfigurations that yield best performing classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sawadogo_A/0/1/0/all/0/1\">Arthur D. Sawadogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guimard_Q/0/1/0/all/0/1\">Quentin Guimard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissyande_T/0/1/0/all/0/1\">Tegawend&#xe9; F. Bissyand&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabore_A/0/1/0/all/0/1\">Abdoul Kader Kabor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_J/0/1/0/all/0/1\">Jacques Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moha_N/0/1/0/all/0/1\">Naouel Moha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LUC at ComMA-2021 Shared Task: Multilingual Gender Biased and Communal Language Identification without using linguistic features. (arXiv:2112.10189v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10189","description":"<p>This work aims to evaluate the ability that both probabilistic and\nstate-of-the-art vector space modeling (VSM) methods provide to well known\nmachine learning algorithms to identify social network documents to be\nclassified as aggressive, gender biased or communally charged. To this end, an\nexploratory stage was performed first in order to find relevant settings to\ntest, i.e. by using training and development samples, we trained multiple\nalgorithms using multiple vector space modeling and probabilistic methods and\ndiscarded the less informative configurations. These systems were submitted to\nthe competition of the ComMA@ICON'21 Workshop on Multilingual Gender Biased and\nCommunal Language Identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cuellar_Hidalgo_R/0/1/0/all/0/1\">Rodrigo Cu&#xe9;llar-Hidalgo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_Zambrano_J/0/1/0/all/0/1\">Julio de Jes&#xfa;s Guerrero-Zambrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forest_D/0/1/0/all/0/1\">Dominic Forest</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Salgado_G/0/1/0/all/0/1\">Gerardo Reyes-Salgado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_Moreno_J/0/1/0/all/0/1\">Juan-Manuel Torres-Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-turn RNN-T for streaming recognition of multi-party speech. (arXiv:2112.10200v1 [eess.AS])","link":"http://arxiv.org/abs/2112.10200","description":"<p>Automatic speech recognition (ASR) of single channel far-field recordings\nwith an unknown number of speakers is traditionally tackled by cascaded\nmodules. Recent research shows that end-to-end (E2E) multi-speaker ASR models\ncan achieve superior recognition accuracy compared to modular systems. However,\nthese models do not ensure real-time applicability due to their dependency on\nfull audio context. This work takes real-time applicability as the first\npriority in model design and addresses a few challenges in previous work on\nmulti-speaker recurrent neural network transducer (MS-RNN-T). First, we\nintroduce on-the-fly overlapping speech simulation during training, yielding\n14% relative word error rate (WER) improvement on LibriSpeechMix test set.\nSecond, we propose a novel multi-turn RNN-T (MT-RNN-T) model with an\noverlap-based target arrangement strategy that generalizes to an arbitrary\nnumber of speakers without changes in the model architecture. We investigate\nthe impact of the maximum number of speakers seen during training on MT-RNN-T\nperformance on LibriCSS test set, and report 28% relative WER improvement over\nthe two-speaker MS-RNN-T. Third, we experiment with a rich transcription\nstrategy for joint recognition and segmentation of multi-party speech. Through\nan in-depth analysis, we discuss potential pitfalls of the proposed system as\nwell as promising future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sklyar_I/0/1/0/all/0/1\">Ilya Sklyar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Piunova_A/0/1/0/all/0/1\">Anna Piunova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_X/0/1/0/all/0/1\">Xianrui Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yulan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Knowledge in End-to-End Automatic Speech Recognition for Mandarin-English Code-Switching. (arXiv:2112.10202v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10202","description":"<p>Code-Switching (CS) is a common linguistic phenomenon in multilingual\ncommunities that consists of switching between languages while speaking. This\npaper presents our investigations on end-to-end speech recognition for\nMandarin-English CS speech. We analyse different CS specific issues such as the\nproperties mismatches between languages in a CS language pair, the\nunpredictable nature of switching points, and the data scarcity problem. We\nexploit and improve the state-of-the-art end-to-end system by merging\nnonlinguistic symbols, by integrating language identification using\nhierarchical softmax, by modeling sub-word units, by artificially lowering the\nspeaking rate, and by augmenting data using speed perturbed technique and\nseveral monolingual datasets to improve the final performance not only on CS\nspeech but also on monolingual benchmarks in order to make the system more\napplicable on real life settings. Finally, we explore the effect of different\nlanguage model integration methods on the performance of the proposed model.\nOur experimental results reveal that all the proposed techniques improve the\nrecognition performance. The best combined system improves the baseline system\nby up to 35% relatively in terms of mixed error rate and delivers acceptable\nperformance on monolingual benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chia-Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"English-to-Chinese Transliteration with Phonetic Back-transliteration. (arXiv:2112.10321v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10321","description":"<p>Transliteration is a task of translating named entities from a language to\nanother, based on phonetic similarity. The task has embraced deep learning\napproaches in recent years, yet, most ignore the phonetic features of the\ninvolved languages. In this work, we incorporate phonetic information into\nneural networks in two ways: we synthesize extra data using forward and\nback-translation but in a phonetic manner; and we pre-train models on a\nphonetic task before learning transliteration. Our experiments include three\nlanguage pairs and six directions, namely English to and from Chinese, Hebrew\nand Thai. Results indicate that our proposed approach brings benefits to the\nmodel and achieves better or similar performance when compared to state of the\nart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuofei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Songpeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Article Reranking by Memory-Enhanced Key Sentence Matching for Detecting Previously Fact-Checked Claims. (arXiv:2112.10322v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10322","description":"<p>False claims that have been previously fact-checked can still spread on\nsocial media. To mitigate their continual spread, detecting previously\nfact-checked claims is indispensable. Given a claim, existing works focus on\nproviding evidence for detection by reranking candidate fact-checking articles\n(FC-articles) retrieved by BM25. However, these performances may be limited\nbecause they ignore the following characteristics of FC-articles: (1) claims\nare often quoted to describe the checked events, providing lexical information\nbesides semantics; (2) sentence templates to introduce or debunk claims are\ncommon across articles, providing pattern information. Models that ignore the\ntwo aspects only leverage semantic relevance and may be misled by sentences\nthat describe similar but irrelevant events. In this paper, we propose a novel\nreranker, MTM (Memory-enhanced Transformers for Matching) to rank FC-articles\nusing key sentences selected with event (lexical and semantic) and pattern\ninformation. For event information, we propose a ROUGE-guided Transformer which\nis finetuned with regression of ROUGE. For pattern information, we generate\npattern vectors for matching with sentences. By fusing event and pattern\ninformation, we select key sentences to represent an article and then predict\nif the article fact-checks the given claim using the claim, key sentences, and\npatterns. Experiments on two real-world datasets show that MTM outperforms\nexisting methods. Human evaluation proves that MTM can capture key sentences\nfor explanations. The code and the dataset are at\nhttps://github.com/ICTMCG/MTM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Lei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"May the Force Be with Your Copy Mechanism: Enhanced Supervised-Copy Method for Natural Language Generation. (arXiv:2112.10360v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10360","description":"<p>Recent neural sequence-to-sequence models with a copy mechanism have achieved\nremarkable progress in various text generation tasks. These models addressed\nout-of-vocabulary problems and facilitated the generation of rare words.\nHowever, the identification of the word which needs to be copied is difficult,\nas observed by prior copy models, which suffer from incorrect generation and\nlacking abstractness. In this paper, we propose a novel supervised approach of\na copy network that helps the model decide which words need to be copied and\nwhich need to be generated. Specifically, we re-define the objective function,\nwhich leverages source sequences and target vocabularies as guidance for\ncopying. The experimental results on data-to-text generation and abstractive\nsummarization tasks verify that our approach enhances the copying quality and\nimproves the degree of abstractness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Sanghyuk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jeong-in Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_H/0/1/0/all/0/1\">Hyungjong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonsoo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction. (arXiv:2112.10424v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10424","description":"<p>Recent works have shown explainability and robustness are two crucial\ningredients of trustworthy and reliable text classification. However, previous\nworks usually address one of two aspects: i) how to extract accurate rationales\nfor explainability while being beneficial to prediction; ii) how to make the\npredictive model robust to different types of adversarial attacks. Intuitively,\na model that produces helpful explanations should be more robust against\nadversarial attacks, because we cannot trust the model that outputs\nexplanations but changes its prediction under small perturbations. To this end,\nwe propose a joint classification and rationale extraction model named AT-BMC.\nIt includes two key mechanisms: mixed Adversarial Training (AT) is designed to\nuse various perturbations in discrete and embedding space to improve the\nmodel's robustness, and Boundary Match Constraint (BMC) helps to locate\nrationales more precisely with the guidance of boundary information.\nPerformances on benchmark datasets demonstrate that the proposed AT-BMC\noutperforms baselines on both classification and rationale extraction by a\nlarge margin. Robustness analysis shows that the proposed AT-BMC decreases the\nattack success rate effectively by up to 69%. The empirical results indicate\nthat there are connections between robust models and better explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongfang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tujie Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jingcong Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP. (arXiv:2112.10508v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10508","description":"<p>What are the units of text that we want to model? From bytes to multi-word\nexpressions, text can be analyzed and generated at many granularities. Until\nrecently, most natural language processing (NLP) models operated over words,\ntreating those as discrete and atomic tokens, but starting with byte-pair\nencoding (BPE), subword-based approaches have become dominant in many areas,\nenabling small vocabularies while still allowing for fast inference. Is the end\nof the road character-level model or byte-level processing? In this survey, we\nconnect several lines of work from the pre-neural and neural era, by showing\nhow hybrid approaches of words and characters as well as subword-based\napproaches based on learned segmentation have been proposed and evaluated. We\nconclude that there is and likely will never be a silver bullet singular\nsolution for all applications and that thinking seriously about tokenization\nremains important for many applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mielke_S/0/1/0/all/0/1\">Sabrina J. Mielke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salesky_E/0/1/0/all/0/1\">Elizabeth Salesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galle_M/0/1/0/all/0/1\">Matthias Gall&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_A/0/1/0/all/0/1\">Arun Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wilson Y. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagot_B/0/1/0/all/0/1\">Beno&#xee;t Sagot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spiral Language Modeling. (arXiv:2112.10543v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10543","description":"<p>In almost all text generation applications, word sequences are constructed in\na left-to-right (L2R) or right-to-left (R2L) manner, as natural language\nsentences are written either L2R or R2L. However, we find that the natural\nlanguage written order is not essential for text generation. In this paper, we\npropose Spiral Language Modeling (SLM), a general approach that enables one to\nconstruct natural language sentences beyond the L2R and R2L order. SLM allows\none to form natural language text by starting from an arbitrary token inside\nthe result text and expanding the rest tokens around the selected ones. It\nmakes the decoding order a new optimization objective besides the language\nmodel perplexity, which further improves the diversity and quality of the\ngenerated text. Furthermore, SLM makes it possible to manipulate the text\nconstruction process by selecting a proper starting token. SLM also introduces\ngeneration orderings as additional regularization to improve model robustness\nin low-resource scenarios. Experiments on 8 widely studied Neural Machine\nTranslation (NMT) tasks show that SLM is constantly effective with up to 4.7\nBLEU increase comparing to the conventional L2R decoding approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yukun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_S/0/1/0/all/0/1\">Shaohui Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Gu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training dataset and dictionary sizes matter in BERT models: the case of Baltic languages. (arXiv:2112.10553v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10553","description":"<p>Large pretrained masked language models have become state-of-the-art\nsolutions for many NLP problems. While studies have shown that monolingual\nmodels produce better results than multilingual models, the training datasets\nmust be sufficiently large. We trained a trilingual LitLat BERT-like model for\nLithuanian, Latvian, and English, and a monolingual Est-RoBERTa model for\nEstonian. We evaluate their performance on four downstream tasks: named entity\nrecognition, dependency parsing, part-of-speech tagging, and word analogy. To\nanalyze the importance of focusing on a single language and the importance of a\nlarge training set, we compare created models with existing monolingual and\nmultilingual BERT models for Estonian, Latvian, and Lithuanian. The results\nshow that the newly created LitLat BERT and Est-RoBERTa models improve the\nresults of existing models on all tested tasks in most situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An ensemble deep learning technique for detecting suicidal ideation from posts in social media platforms. (arXiv:2112.10609v1 [cs.IR])","link":"http://arxiv.org/abs/2112.10609","description":"<p>Suicidal ideation detection from social media is an evolving research with\ngreat challenges. Many of the people who have the tendency to suicide share\ntheir thoughts and opinions through social media platforms. As part of many\nresearches it is observed that the publicly available posts from social media\ncontain valuable criteria to effectively detect individuals with suicidal\nthoughts. The most difficult part to prevent suicide is to detect and\nunderstand the complex risk factors and warning signs that may lead to suicide.\nThis can be achieved by identifying the sudden changes in a user behavior\nautomatically. Natural language processing techniques can be used to collect\nbehavioral and textual features from social media interactions and these\nfeatures can be passed to a specially designed framework to detect anomalies in\nhuman interactions that are indicators of suicidal intentions. We can achieve\nfast detection of suicidal ideation using deep learning and/or machine learning\nbased classification approaches. For such a purpose, we can employ the\ncombination of LSTM and CNN models to detect such emotions from posts of the\nusers. In order to improve the accuracy, some approaches like using more data\nfor training, using attention model to improve the efficiency of existing\nmodels etc. could be done. This paper proposes a LSTM-Attention-CNN combined\nmodel to analyze social media submissions to detect any underlying suicidal\nintentions. During evaluations, the proposed model demonstrated an accuracy of\n90.3 percent and an F1-score of 92.6 percent, which is greater than the\nbaseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Renjith_S/0/1/0/all/0/1\">Shini Renjith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abraham_A/0/1/0/all/0/1\">Annie Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jyothi_S/0/1/0/all/0/1\">Surya B.Jyothi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandran_L/0/1/0/all/0/1\">Lekshmi Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomson_J/0/1/0/all/0/1\">Jincy Thomson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intelligent Online Selling Point Extraction for E-Commerce Recommendation. (arXiv:2112.10613v1 [cs.IR])","link":"http://arxiv.org/abs/2112.10613","description":"<p>In the past decade, automatic product description generation for e-commerce\nhave witnessed significant advancement. As the services provided by e-commerce\nplatforms become diverse, it is necessary to dynamically adapt the patterns of\ndescriptions generated. The selling point of products is an important type of\nproduct description for which the length should be as short as possible while\nstill conveying key information. In addition, this kind of product description\nshould be eye-catching to the readers. Currently, product selling points are\nnormally written by human experts. Thus, the creation and maintenance of these\ncontents incur high costs. These costs can be significantly reduced if product\nselling points can be automatically generated by machines. In this paper, we\nreport our experience developing and deploying the Intelligent Online Selling\nPoint Extraction (IOSPE) system to serve the recommendation system in the\nJD.com e-commerce platform. Since July 2020, IOSPE has become a core service\nfor 62 key categories of products (covering more than 4 million products). So\nfar, it has generated more than 0.1 billion selling points, thereby\nsignificantly scaling up the selling point creation operation and saving human\nlabour. These IOSPE generated selling points have increased the click-through\nrate (CTR) by 1.89\\% and the average duration the customers spent on the\nproducts by more than 2.03\\% compared to the previous practice, which are\nsignificant improvements for such a large-scale e-commerce platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaojie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shugen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanqing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_S/0/1/0/all/0/1\">Shiliang Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Han Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Language Model with Entanglement Embedding for Question Answering. (arXiv:2008.09943v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.09943","description":"<p>Quantum Language Models (QLMs) in which words are modelled as quantum\nsuperposition of sememes have demonstrated a high level of model transparency\nand good post-hoc interpretability. Nevertheless, in the current literature\nword sequences are basically modelled as a classical mixture of word states,\nwhich cannot fully exploit the potential of a quantum probabilistic\ndescription. A full quantum model is yet to be developed to explicitly capture\nthe non-classical correlations within the word sequences. We propose a neural\nnetwork model with a novel Entanglement Embedding (EE) module, whose function\nis to transform the word sequences into entangled pure states of many-body\nquantum systems. Strong quantum entanglement, which is the central concept of\nquantum information and an indication of parallelized correlations among the\nwords, is observed within the word sequences. Numerical experiments show that\nthe proposed QLM with EE (QLM-EE) achieves superior performance compared with\nthe classical deep neural network models and other QLMs on Question Answering\n(QA) datasets. In addition, the post-hoc interpretability of the model can be\nimproved by quantizing the degree of entanglement among the words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1\">Daoyi Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A learning perspective on the emergence of abstractions: the curious case of phonemes. (arXiv:2012.07499v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.07499","description":"<p>In the present paper we use a range of modeling techniques to investigate\nwhether an abstract phone could emerge from exposure to speech sounds. In\neffect, the study represents an attempt for operationalize a theoretical device\nof Usage-based Linguistics of emergence of an abstraction from language use.\nOur quest focuses on the simplest of such hypothesized abstractions. We test\ntwo opposing principles regarding the development of language knowledge in\nlinguistically untrained language users: Memory-Based Learning (MBL) and\nError-Correction Learning (ECL). A process of generalization underlies the\nabstractions linguists operate with, and we probed whether MBL and ECL could\ngive rise to a type of language knowledge that resembles linguistic\nabstractions. Each model was presented with a significant amount of\npre-processed speech produced by one speaker. We assessed the consistency or\nstability of what these simple models have learned and their ability to give\nrise to abstract categories. Both types of models fare differently with regard\nto these tests. We show that ECL models can learn abstractions and that at\nleast part of the phone inventory and grouping into traditional types can be\nreliably identified from the input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milin_P/0/1/0/all/0/1\">Petar Milin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_B/0/1/0/all/0/1\">Benjamin V. Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divjak_D/0/1/0/all/0/1\">Dagmar Divjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Adapters for Cross-lingual Low-resource Speech Recognition. (arXiv:2105.11905v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.11905","description":"<p>Cross-lingual speech adaptation aims to solve the problem of leveraging\nmultiple rich-resource languages to build models for a low-resource target\nlanguage. Since the low-resource language has limited training data, speech\nrecognition models can easily overfit. In this paper, we propose to use\nadapters to investigate the performance of multiple adapters for\nparameter-efficient cross-lingual speech adaptation. Based on our previous\nMetaAdapter that implicitly leverages adapters, we propose a novel algorithms\ncalled SimAdapter for explicitly learning knowledge from adapters. Our\nalgorithm leverages adapters which can be easily integrated into the\nTransformer structure.MetaAdapter leverages meta-learning to transfer the\ngeneral knowledge from training data to the test language. SimAdapter aims to\nlearn the similarities between the source and target languages during\nfine-tuning using the adapters. We conduct extensive experiments on\nfive-low-resource languages in Common Voice dataset. Results demonstrate that\nour MetaAdapter and SimAdapter methods can reduce WER by 2.98% and 2.55% with\nonly 2.5% and 15.5% of trainable parameters compared to the strong full-model\nfine-tuning baseline. Moreover, we also show that these two novel algorithms\ncan be integrated for better performance with up to 3.55% relative WER\nreduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1\">Wenxin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Han Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jindong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Renjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinozaki_T/0/1/0/all/0/1\">Takahiro Shinozaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAS: Self-Augmentation Strategy for Language Model Pre-training. (arXiv:2106.07176v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07176","description":"<p>The core of self-supervised learning for pre-training language models\nincludes pre-training task design as well as appropriate data augmentation.\nMost data augmentations in language model pre-training are context-independent.\nA seminal contextualized augmentation was recently proposed in ELECTRA and\nachieved state-of-the-art performance by introducing an auxiliary generation\nnetwork (generator) to produce contextualized data augmentation for the\ntraining of a main discrimination network (discriminator). This design,\nhowever, introduces extra computation cost of the generator and a need to\nadjust the relative capability between the generator and the discriminator. In\nthis paper, we propose a self-augmentation strategy (SAS) where a single\nnetwork is utilized for both regular pre-training and contextualized data\naugmentation for the training in later epochs. Essentially, this strategy\neliminates a separate generator and uses the single network to jointly conduct\ntwo pre-training tasks with MLM (Masked Language Modeling) and RTD (Replaced\nToken Detection) heads. It avoids the challenge to search for an appropriate\nsize of the generator, which is critical to the performance as evidenced in\nELECTRA and its subsequent variant models. In addition, SAS is a general\nstrategy that can be seamlessly combined with many new techniques emerging\nrecently or in the future, such as the disentangled attention mechanism from\nDeBERTa. Our experiments show that SAS is able to outperform ELECTRA and other\nstate-of-the-art models in the GLUE tasks with similar or less computation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ru He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Liangzhu Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Universality of Deep Contextual Language Models. (arXiv:2109.07140v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07140","description":"<p>Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors\ndominate the landscape of Natural Language Processing due to their ability to\nscale across multiple tasks rapidly by pre-training a single model, followed by\ntask-specific fine-tuning. Furthermore, multilingual versions of such models\nlike XLM-R and mBERT have given promising results in zero-shot cross-lingual\ntransfer, potentially enabling NLP applications in many under-served and\nunder-resourced languages. Due to this initial success, pre-trained models are\nbeing used as `Universal Language Models' as the starting point across diverse\ntasks, domains, and languages. This work explores the notion of `Universality'\nby identifying seven dimensions across which a universal model should be able\nto scale, that is, perform equally well or reasonably well, to be useful across\ndiverse settings. We outline the current theoretical and empirical results that\nsupport model performance across these dimensions, along with extensions that\nmay help address some of their current limitations. Through this survey, we lay\nthe foundation for understanding the capabilities and limitations of massive\ncontextual language models and help discern research gaps and directions for\nfuture work to make these LMs inclusive and fair to diverse applications,\nusers, and linguistic phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1\">Shaily Bhatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Poonam Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dandapat_S/0/1/0/all/0/1\">Sandipan Dandapat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1\">Sunayana Sitaram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoxCeleb Enrichment for Age and Gender Recognition. (arXiv:2109.13510v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13510","description":"<p>VoxCeleb datasets are widely used in speaker recognition studies. Our work\nserves two purposes. First, we provide speaker age labels and (an alternative)\nannotation of speaker gender. Second, we demonstrate the use of this metadata\nby constructing age and gender recognition models with different features and\nclassifiers. We query different celebrity databases and apply consensus rules\nto derive age and gender labels. We also compare the original VoxCeleb gender\nlabels with our labels to identify records that might be mislabeled in the\noriginal VoxCeleb data. On modeling side, we design a comprehensive study of\nmultiple features and models for recognizing gender and age. Our best system,\nusing i-vector features, achieved an F1-score of 0.9829 for gender recognition\ntask using logistic regression, and the lowest mean absolute error (MAE) in age\nregression, 9.443 years, is obtained with ridge regression. This indicates\nchallenge in age estimation from in-the-wild style speech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hechmi_K/0/1/0/all/0/1\">Khaled Hechmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trong_T/0/1/0/all/0/1\">Trung Ngo Trong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautamaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Knowledge Assimilation for Expert-Layman Text Style Transfer. (arXiv:2110.02950v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02950","description":"<p>Expert-layman text style transfer technologies have the potential to improve\ncommunication between members of scientific communities and the general public.\nHigh-quality information produced by experts is often filled with difficult\njargon laypeople struggle to understand. This is a particularly notable issue\nin the medical domain, where layman are often confused by medical text online.\nAt present, two bottlenecks interfere with the goal of building high-quality\nmedical expert-layman style transfer systems: a dearth of pretrained\nmedical-domain language models spanning both expert and layman terminologies\nand a lack of parallel corpora for training the transfer task itself. To\nmitigate the first issue, we propose a novel language model (LM) pretraining\ntask, Knowledge Base Assimilation, to synthesize pretraining data from the\nedges of a graph of expert- and layman-style medical terminology terms into an\nLM during self-supervised learning. To mitigate the second issue, we build a\nlarge-scale parallel corpus in the medical expert-layman domain using a\nmargin-based criterion. Our experiments show that transformer-based models\npretrained on knowledge base assimilation and other well-established\npretraining tasks fine-tuning on our new parallel corpus leads to considerable\nimprovement against expert-layman transfer benchmarks, gaining an average\nrelative improvement of our human evaluation, the Overall Success Rate (OSR),\nby 106%. We release our code and parallel corpus for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenda Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Explanation of In-context Learning as Implicit Bayesian Inference. (arXiv:2111.02080v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02080","description":"<p>Large pretrained language models such as GPT-3 have the surprising ability to\ndo in-context learning, where the model learns to do a downstream task simply\nby conditioning on a prompt consisting of input-output examples. Without being\nexplicitly pretrained to do so, the language model learns from these examples\nduring its forward pass without parameter updates on \"out-of-distribution\"\nprompts. Thus, it is unclear what mechanism enables in-context learning. In\nthis paper, we study the role of the pretraining distribution on the emergence\nof in-context learning under a mathematical setting where the pretraining texts\nhave long-range coherence. Here, language model pretraining requires inferring\na latent document-level concept from the conditioning text to generate coherent\nnext tokens. At test time, this mechanism enables in-context learning by\ninferring the shared latent concept between prompt examples and applying it to\nmake a prediction on the test example. Concretely, we prove that in-context\nlearning occurs implicitly via Bayesian inference of the latent concept when\nthe pretraining distribution is a mixture of HMMs. This can occur despite the\ndistribution mismatch between prompts and pretraining data. In contrast to\nmessy large-scale pretraining datasets for in-context learning in natural\nlanguage, we generate a family of small-scale synthetic datasets (GINC) where\nTransformer and LSTM language models both exhibit in-context learning. Beyond\nthe theory which focuses on the effect of the pretraining distribution, we\nempirically find that scaling model size improves in-context accuracy even when\nthe pretraining loss is the same.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sang Michael Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghunathan_A/0/1/0/all/0/1\">Aditi Raghunathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEM: Unsupervised STructural EMbedding for Stance Detection. (arXiv:2112.00712v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.00712","description":"<p>Stance detection is an important task, supporting many downstream tasks such\nas discourse parsing and modeling the propagation of fake news, rumors, and\nscience denial. In this paper, we propose a novel framework for stance\ndetection. Our framework is unsupervised and domain-independent. Given a claim\nand a multi-participant discussion - we construct the interaction network from\nwhich we derive topological embedding for each speaker. These speaker embedding\nenjoy the following property: speakers with the same stance tend to be\nrepresented by similar vectors, while antipodal vectors represent speakers with\nopposing stances. These embedding are then used to divide the speakers into\nstance-partitions. We evaluate our method on three different datasets from\ndifferent platforms. Our method outperforms or is comparable with supervised\nmodels while providing confidence levels for its output. Furthermore, we\ndemonstrate how the structural embedding relate to the valence expressed by the\nspeakers. Finally, we discuss some limitations inherent to the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pick_R/0/1/0/all/0/1\">Ron Korenblum Pick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kozhukhov_V/0/1/0/all/0/1\">Vladyslav Kozhukhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilenchik_D/0/1/0/all/0/1\">Dan Vilenchik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1\">Oren Tsur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Interactions Using Pretrained Unimodal Models for SIMMC 2.0. (arXiv:2112.05328v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.05328","description":"<p>This paper presents our work on the Situated Interactive MultiModal\nConversations 2.0 challenge held at Dialog State Tracking Challenge 10. SIMMC\n2.0 includes 4 subtasks, and we introduce our multimodal approaches for the\nsubtask \\#1, \\#2 and the generation of subtask \\#4. SIMMC 2.0 dataset is a\nmultimodal dataset containing image and text information, which is more\nchallenging than the problem of only text-based conversations because it must\nbe solved by understanding the relationship between image and text. Therefore,\nsince there is a limit to solving only text models such as BERT or GPT2, we\npropose a multimodal model combining image and text. We first pretrain the\nmultimodal model to understand the relationship between image and text, then\nfinetune our model for each task. We achieve the 3rd best performance in\nsubtask \\#1, \\#2 and a runner-up in the generation of subtask \\#4. The source\ncode is available at https://github.com/rungjoo/simmc2.0.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kijong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Existing language grounding models often use object proposal bottlenecks: a\npre-trained detector proposes objects in the scene and the model learns to\nselect the answer from these box proposals, without attending to the original\nimage or 3D point cloud. Object detectors are typically trained on a fixed\nvocabulary of objects and attributes that is often too restrictive for\nopen-domain language grounding, where an utterance may refer to visual entities\nat various levels of abstraction, such as a chair, the leg of a chair, or the\ntip of the front leg of a chair. We propose a model for grounding language in\n3D scenes that bypasses box proposal bottlenecks with three main innovations:\ni) Iterative attention across the language stream, the point cloud feature\nstream and 3D box proposals. ii) Transformer decoders with non-parametric\nentity queries that decode 3D boxes for object and part referentials. iii)\nJoint supervision from 3D object annotations and language grounding\nannotations, by treating object detection as grounding of referential\nutterances comprised of a list of candidate category labels. These innovations\nresult in significant quantitative gains (up to +9% absolute improvement on the\nSR3D benchmark) over previous approaches on popular 3D language grounding\nbenchmarks. We ablate each of our innovations to show its contribution to the\nperformance of the model. When applied on language grounding on 2D images with\nminor changes, it performs on par with the state-of-the-art while converges in\nhalf of the GPU time. The code and checkpoints will be made available at\nhttps://github.com/nickgkan/beauty_detr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Achieving Forgetting Prevention and Knowledge Transfer in Continual Learning. (arXiv:2112.02706v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2112.02706","description":"<p>Continual learning (CL) learns a sequence of tasks incrementally with the\ngoal of achieving two main objectives: overcoming catastrophic forgetting (CF)\nand encouraging knowledge transfer (KT) across tasks. However, most existing\ntechniques focus only on overcoming CF and have no mechanism to encourage KT,\nand thus do not do well in KT. Although several papers have tried to deal with\nboth CF and KT, our experiments show that they suffer from serious CF when the\ntasks do not have much shared knowledge. Another observation is that most\ncurrent CL methods do not use pre-trained models, but it has been shown that\nsuch models can significantly improve the end task performance. For example, in\nnatural language processing, fine-tuning a BERT-like pre-trained language model\nis one of the most effective approaches. However, for CL, this approach suffers\nfrom serious CF. An interesting question is how to make the best use of\npre-trained models for CL. This paper proposes a novel model called CTR to\nsolve these problems. Our experimental results demonstrate the effectiveness of\nCTR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Nianzu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLASSIC: Continual and Contrastive Learning of Aspect Sentiment Classification Tasks. (arXiv:2112.02714v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2112.02714","description":"<p>This paper studies continual learning (CL) of a sequence of aspect sentiment\nclassification(ASC) tasks in a particular CL setting called domain incremental\nlearning (DIL). Each task is from a different domain or product. The DIL\nsetting is particularly suited to ASC because in testing the system needs not\nknow the task/domain to which the test data belongs. To our knowledge, this\nsetting has not been studied before for ASC. This paper proposes a novel model\ncalled CLASSIC. The key novelty is a contrastive continual learning method that\nenables both knowledge transfer across tasks and knowledge distillation from\nold tasks to the new task, which eliminates the need for task ids in testing.\nExperimental results show the high effectiveness of CLASSIC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1\">Lei Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting BERT for Continual Learning of a Sequence of Aspect Sentiment Classification Tasks. (arXiv:2112.03271v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2112.03271","description":"<p>This paper studies continual learning (CL) of a sequence of aspect sentiment\nclassification (ASC) tasks. Although some CL techniques have been proposed for\ndocument sentiment classification, we are not aware of any CL work on ASC. A CL\nsystem that incrementally learns a sequence of ASC tasks should address the\nfollowing two issues: (1) transfer knowledge learned from previous tasks to the\nnew task to help it learn a better model, and (2) maintain the performance of\nthe models for previous tasks so that they are not forgotten. This paper\nproposes a novel capsule network based model called B-CL to address these\nissues. B-CL markedly improves the ASC performance on both the new task and the\nold tasks via forward and backward knowledge transfer. The effectiveness of\nB-CL is demonstrated through extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Can uncertainty boost the reliability of AI-based diagnostic methods in digital pathology?. (arXiv:2112.09693v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09693","description":"<p>Deep learning (DL) has shown great potential in digital pathology\napplications. The robustness of a diagnostic DL-based solution is essential for\nsafe clinical deployment. In this work we evaluate if adding uncertainty\nestimates for DL predictions in digital pathology could result in increased\nvalue for the clinical applications, by boosting the general predictive\nperformance or by detecting mispredictions. We compare the effectiveness of\nmodel-integrated methods (MC dropout and Deep ensembles) with a model-agnostic\napproach (Test time augmentation, TTA). Moreover, four uncertainty metrics are\ncompared. Our experiments focus on two domain shift scenarios: a shift to a\ndifferent medical center and to an underrepresented subtype of cancer. Our\nresults show that uncertainty estimates can add some reliability and reduce\nsensitivity to classification threshold selection. While advanced metrics and\ndeep ensembles perform best in our comparison, the added value over simpler\nmetrics and TTA is small. Importantly, the benefit of all evaluated uncertainty\nestimation methods is diminished by domain shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poceviciute_M/0/1/0/all/0/1\">Milda Pocevi&#x10d;i&#x16b;t&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eilertsen_G/0/1/0/all/0/1\">Gabriel Eilertsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jarkman_S/0/1/0/all/0/1\">Sofia Jarkman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundstrom_C/0/1/0/all/0/1\">Claes Lundstr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable and Interactive Deep Multiple Instance Learning for Dental Caries Classification in Bitewing X-rays. (arXiv:2112.09694v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09694","description":"<p>We propose a simple and efficient image classification architecture based on\ndeep multiple instance learning, and apply it to the challenging task of caries\ndetection in dental radiographs. Technically, our approach contributes in two\nways: First, it outputs a heatmap of local patch classification probabilities\ndespite being trained with weak image-level labels. Second, it is amenable to\nlearning from segmentation labels to guide training. In contrast to existing\nmethods, the human user can faithfully interpret predictions and interact with\nthe model to decide which regions to attend to. Experiments are conducted on a\nlarge clinical dataset of $\\sim$38k bitewings ($\\sim$316k teeth), where we\nachieve competitive performance compared to various baselines. When guided by\nan external caries segmentation model, a significant improvement in\nclassification and localization performance is observed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bergner_B/0/1/0/all/0/1\">Benjamin Bergner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rohrer_C/0/1/0/all/0/1\">Csaba Rohrer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taleb_A/0/1/0/all/0/1\">Aiham Taleb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duchrau_M/0/1/0/all/0/1\">Martha Duchrau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leon_G/0/1/0/all/0/1\">Guilherme De Leon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_J/0/1/0/all/0/1\">Jonas Almeida Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schwendicke_F/0/1/0/all/0/1\">Falk Schwendicke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krois_J/0/1/0/all/0/1\">Joachim Krois</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lippert_C/0/1/0/all/0/1\">Christoph Lippert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Soundify: Matching Sound Effects to Video. (arXiv:2112.09726v1 [cs.SD])","link":"http://arxiv.org/abs/2112.09726","description":"<p>In the art of video editing, sound is really half the story. A skilled video\neditor overlays sounds, such as effects and ambients, over footage to add\ncharacter to an object or immerse the viewer within a space. However, through\nformative interviews with professional video editors, we found that this\nprocess can be extremely tedious and time-consuming. We introduce Soundify, a\nsystem that matches sound effects to video. By leveraging labeled,\nstudio-quality sound effects libraries and extending CLIP, a neural network\nwith impressive zero-shot image classification capabilities, into a \"zero-shot\ndetector\", we are able to produce high-quality results without\nresource-intensive correspondence learning or audio generation. We encourage\nyou to have a look at, or better yet, have a listen to the results at\nhttps://chuanenlin.com/soundify.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">David Chuan-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Germanidis_A/0/1/0/all/0/1\">Anastasis Germanidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenzuela_C/0/1/0/all/0/1\">Crist&#xf3;bal Valenzuela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yining Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martelaro_N/0/1/0/all/0/1\">Nikolas Martelaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neurashed: A Phenomenological Model for Imitating Deep Learning Training. (arXiv:2112.09741v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09741","description":"<p>To advance deep learning methodologies in the next decade, a theoretical\nframework for reasoning about modern neural networks is needed. While efforts\nare increasing toward demystifying why deep learning is so effective, a\ncomprehensive picture remains lacking, suggesting that a better theory is\npossible. We argue that a future deep learning theory should inherit three\ncharacteristics: a \\textit{hierarchically} structured network architecture,\nparameters \\textit{iteratively} optimized using stochastic gradient-based\nmethods, and information from the data that evolves \\textit{compressively}. As\nan instantiation, we integrate these characteristics into a graphical model\ncalled \\textit{neurashed}. This model effectively explains some common\nempirical patterns in deep learning. In particular, neurashed enables insights\ninto implicit regularization, information bottleneck, and local elasticity.\nFinally, we discuss how neurashed can guide the development of deep learning\ntheories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Single-Scale Vision Transformer for Object Localization and Instance Segmentation. (arXiv:2112.09747v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09747","description":"<p>This work presents a simple vision transformer design as a strong baseline\nfor object localization and instance segmentation tasks. Transformers recently\ndemonstrate competitive performance in image classification tasks. To adopt ViT\nto object detection and dense prediction tasks, many works inherit the\nmultistage design from convolutional networks and highly customized ViT\narchitectures. Behind this design, the goal is to pursue a better trade-off\nbetween computational cost and effective aggregation of multiscale global\ncontexts. However, existing works adopt the multistage architectural design as\na black-box solution without a clear understanding of its true benefits. In\nthis paper, we comprehensively study three architecture design choices on ViT\n-- spatial reduction, doubled channels, and multiscale features -- and\ndemonstrate that a vanilla ViT architecture can fulfill this goal without\nhandcrafting multiscale features, maintaining the original ViT design\nphilosophy. We further complete a scaling rule to optimize our model's\ntrade-off on accuracy and computation cost / model size. By leveraging a\nconstant feature resolution and hidden size throughout the encoder blocks, we\npropose a simple and compact ViT architecture called Universal Vision\nTransformer (UViT) that achieves strong performance on COCO object detection\nand instance segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1\">Lucas Beyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaohua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huizhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xiaodan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Half-Quadratic Splitting Network for Magnetic Resonance Image Reconstruction. (arXiv:2112.09760v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09760","description":"<p>Magnetic Resonance (MR) image reconstruction from highly undersampled\n$k$-space data is critical in accelerated MR imaging (MRI) techniques. In\nrecent years, deep learning-based methods have shown great potential in this\ntask. This paper proposes a learned half-quadratic splitting algorithm for MR\nimage reconstruction and implements the algorithm in an unrolled deep learning\nnetwork architecture. We compare the performance of our proposed method on a\npublic cardiac MR dataset against DC-CNN and LPDNet, and our method outperforms\nother methods in both quantitative results and qualitative results with fewer\nmodel parameters and faster reconstruction speed. Finally, we enlarge our model\nto achieve superior reconstruction quality, and the improvement is $1.76$ dB\nand $2.74$ dB over LPDNet in peak signal-to-noise ratio on $5\\times$ and\n$10\\times$ acceleration, respectively. Code for our method is publicly\navailable at https://github.com/hellopipu/HQS-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xin_B/0/1/0/all/0/1\">Bingyu Xin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_T/0/1/0/all/0/1\">Timothy S. Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Axel_L/0/1/0/all/0/1\">Leon Axel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Subsampling for ROI-based Visual Tracking: Algorithms and FPGA Implementation. (arXiv:2112.09775v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09775","description":"<p>There is tremendous scope for improving the energy efficiency of embedded\nvision systems by incorporating programmable region-of-interest (ROI) readout\nin the image sensor design. In this work, we study how ROI programmability can\nbe leveraged for tracking applications by anticipating where the ROI will be\nlocated in future frames and switching pixels off outside of this region. We\nrefer to this process of ROI prediction and corresponding sensor configuration\nas adaptive subsampling. Our adaptive subsampling algorithms comprise an object\ndetector and an ROI predictor (Kalman filter) which operate in conjunction to\noptimize the energy efficiency of the vision pipeline with the end task being\nobject tracking. To further facilitate the implementation of our adaptive\nalgorithms in real life, we select a candidate algorithm and map it onto an\nFPGA. Leveraging Xilinx Vitis AI tools, we designed and accelerated a YOLO\nobject detector-based adaptive subsampling algorithm. In order to further\nimprove the algorithm post-deployment, we evaluated several competing baselines\non the OTB100 and LaSOT datasets. We found that coupling the ECO tracker with\nthe Kalman filter has a competitive AUC score of 0.4568 and 0.3471 on the\nOTB100 and LaSOT datasets respectively. Further, the power efficiency of this\nalgorithm is on par with, and in a couple of instances superior to, the other\nbaselines. The ECO-based algorithm incurs a power consumption of approximately\n4 W averaged across both datasets while the YOLO-based approach requires power\nconsumption of approximately 6 W (as per our power consumption model). In terms\nof accuracy-latency tradeoff, the ECO-based algorithm provides near-real-time\nperformance (19.23 FPS) while managing to attain competitive tracking\nprecision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_O/0/1/0/all/0/1\">Odrika Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muro_V/0/1/0/all/0/1\">Victor Isaac Torres Muro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katoch_S/0/1/0/all/0/1\">Sameeksha Katoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanias_A/0/1/0/all/0/1\">Andreas Spanias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasuriya_S/0/1/0/all/0/1\">Suren Jayasuriya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distill and De-bias: Mitigating Bias in Face Recognition using Knowledge Distillation. (arXiv:2112.09786v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09786","description":"<p>Face recognition networks generally demonstrate bias with respect to\nsensitive attributes like gender, skintone etc. For gender and skintone, we\nobserve that the regions of the face that a network attends to vary by the\ncategory of an attribute. This might contribute to bias. Building on this\nintuition, we propose a novel distillation-based approach called Distill and\nDe-bias (D&amp;D) to enforce a network to attend to similar face regions,\nirrespective of the attribute category. In D&amp;D, we train a teacher network on\nimages from one category of an attribute; e.g. light skintone. Then distilling\ninformation from the teacher, we train a student network on images of the\nremaining category; e.g., dark skintone. A feature-level distillation loss\nconstrains the student network to generate teacher-like representations. This\nallows the student network to attend to similar face regions for all attribute\ncategories and enables it to reduce bias. We also propose a second distillation\nstep on top of D&amp;D, called D&amp;D++. For the D&amp;D++ network, we distill the\n`un-biasedness' of the D&amp;D network into a new student network, the D&amp;D++\nnetwork. We train the new network on all attribute categories; e.g., both light\nand dark skintones. This helps us train a network that is less biased for an\nattribute, while obtaining higher face verification performance than D&amp;D. We\nshow that D&amp;D++ outperforms existing baselines in reducing gender and skintone\nbias on the IJB-C dataset, while obtaining higher face verification performance\nthan existing adversarial de-biasing methods. We evaluate the effectiveness of\nour proposed methods on two state-of-the-art face recognition networks:\nCrystalface and ArcFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1\">Prithviraj Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Aniket Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_P/0/1/0/all/0/1\">P. Jonathon Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Adaptive Few-Shot Object Detection with Heterogeneous Graph Convolutional Networks. (arXiv:2112.09791v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09791","description":"<p>Few-shot object detection (FSOD) aims to detect never-seen objects using few\nexamples. This field sees recent improvement owing to the meta-learning\ntechniques by learning how to match between the query image and few-shot class\nexamples, such that the learned model can generalize to few-shot novel classes.\nHowever, currently, most of the meta-learning-based methods perform pairwise\nmatching between query image regions (usually proposals) and novel classes\nseparately, therefore failing to take into account multiple relationships among\nthem. In this paper, we propose a novel FSOD model using heterogeneous graph\nconvolutional networks. Through efficient message passing among all the\nproposal and class nodes with three different types of edges, we could obtain\ncontext-aware proposal features and query-adaptive, multiclass-enhanced\nprototype representations for each class, which could help promote the pairwise\nmatching and improve final FSOD accuracy. Extensive experimental results show\nthat our proposed model, denoted as QA-FewDet, outperforms the current\nstate-of-the-art approaches on the PASCAL VOC and MSCOCO FSOD benchmarks under\ndifferent shots and evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yicheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multi-Domain Generalization through Domain Re-labeling. (arXiv:2112.09802v1 [cs.LG])","link":"http://arxiv.org/abs/2112.09802","description":"<p>Domain generalization (DG) methods aim to develop models that generalize to\nsettings where the test distribution is different from the training data. In\nthis paper, we focus on the challenging problem of multi-source zero-shot DG,\nwhere labeled training data from multiple source domains is available but with\nno access to data from the target domain. Though this problem has become an\nimportant topic of research, surprisingly, the simple solution of pooling all\nsource data together and training a single classifier is highly competitive on\nstandard benchmarks. More importantly, even sophisticated approaches that\nexplicitly optimize for invariance across different domains do not necessarily\nprovide non-trivial gains over ERM. In this paper, for the first time, we study\nthe important link between pre-specified domain labels and the generalization\nperformance. Using a motivating case-study and a new variant of a\ndistributional robust optimization algorithm, GroupDRO++, we first demonstrate\nhow inferring custom domain groups can lead to consistent improvements over the\noriginal domain labels that come with the dataset. Subsequently, we introduce a\ngeneral approach for multi-domain generalization, MulDEns, that uses an\nERM-based deep ensembling backbone and performs implicit domain re-labeling\nthrough a meta-optimization algorithm. Using empirical studies on multiple\nstandard benchmarks, we show that MulDEns does not require tailoring the\naugmentation strategy or the training process specific to a dataset,\nconsistently outperforms ERM by significant margins, and produces\nstate-of-the-art generalization performance, even when compared to existing\nmethods that exploit the domain labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thopalli_K/0/1/0/all/0/1\">Kowshik Thopalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katoch_S/0/1/0/all/0/1\">Sameeksha Katoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanias_A/0/1/0/all/0/1\">Andreas Spanias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman J. Thiagarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Direct simple computation of middle surface between 3D point clouds and/or discrete surfaces by tracking sources in distance function calculation algorithms. (arXiv:2112.09808v1 [math.NA])","link":"http://arxiv.org/abs/2112.09808","description":"<p>In this paper, we introduce novel methods for computing middle surfaces\nbetween various 3D data sets such as point clouds and/or discrete surfaces.\nTraditionally the middle surface is obtained by detecting singularities in\ncomputed distance function such as ridges, triple junctions, etc. It requires\nto compute second order differential characteristics and also some kinds of\nheuristics must be applied. Opposite to that, we determine the middle surface\njust from computing the distance function itself which is a fast and simple\napproach. We present and compare the results of the fast sweeping method, the\nvector distance transform algorithm, the fast marching method, and the\nDijkstra-Pythagoras method in finding the middle surface between 3D data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Kosa_B/0/1/0/all/0/1\">Balazs Kosa</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mikula_K/0/1/0/all/0/1\">Karol Mikula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Streaming Volumetric Image Generation Framework for Development and Evaluation of Out-of-Core Methods. (arXiv:2112.09809v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09809","description":"<p>Advances in 3D imaging technology in recent years have allowed for\nincreasingly high resolution volumetric images of large specimen. The resulting\ndatasets of hundreds of Gigabytes in size call for new scalable and memory\nefficient approaches in the field of image processing, where some progress has\nbeen made already. At the same time, quantitative evaluation of these new\nmethods is difficult both in terms of the availability of specific data sizes\nand in the generation of associated ground truth data. In this paper we present\nan algorithmic framework that can be used to efficiently generate test (and\nground truth) volume data, optionally even in a streaming fashion. As the\nproposed nested sweeps algorithm is fast, it can be used to generate test data\non demand. We analyze the asymptotic run time of the presented algorithm and\ncompare it experimentally to alternative approaches as well as a hypothetical\nbest-case baseline method. In a case study, the framework is applied to the\npopular VascuSynth software for vascular image generation, making it capable of\nefficiently producing larger-than-main memory volumes which is demonstrated by\ngenerating a trillion voxel (1TB) image. Implementations of the presented\nframework are available online in the form of the modified version of\nVascusynth and the code used for the experimental evaluation. In addition, the\ntest data generation procedure has been integrated into the popular volume\nrendering and processing framework Voreen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drees_D/0/1/0/all/0/1\">Dominik Drees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyi Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Long-Term Dependencies for Generating Dynamic Scene Graphs. (arXiv:2112.09828v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09828","description":"<p>Structured video representation in the form of dynamic scene graphs is an\neffective tool for several video understanding tasks. Compared to the task of\nscene graph generation from images, dynamic scene graph generation is more\nchallenging due to the temporal dynamics of the scene and the inherent temporal\nfluctuations of predictions. We show that capturing long-term dependencies is\nthe key to effective generation of dynamic scene graphs. We present the\ndetect-track-recognize paradigm by constructing consistent long-term object\ntracklets from a video, followed by transformers to capture the dynamics of\nobjects and visual relations. Experimental results demonstrate that our Dynamic\nScene Graph Detection Transformer (DSG-DETR) outperforms state-of-the-art\nmethods by a significant margin on the benchmark dataset Action Genome. We also\nperform ablation studies and validate the effectiveness of each component of\nthe proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shengyu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1\">Subarna Tripathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mostafa_H/0/1/0/all/0/1\">Hesham Mostafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassar_M/0/1/0/all/0/1\">Marcel Nassar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1\">Somdeb Majumdar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Deblurring Based on Separable Normalization and Adaptive Denormalization. (arXiv:2112.09833v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09833","description":"<p>Face deblurring aims to restore a clear face image from a blurred input image\nwith more explicit structure and facial details. However, most conventional\nimage and face deblurring methods focus on the whole generated image resolution\nwithout consideration of special face part texture and generally produce\nunsufficient details. Considering that faces and backgrounds have different\ndistribution information, in this study, we designed an effective face\ndeblurring network based on separable normalization and adaptive\ndenormalization (SNADNet). First, We fine-tuned the face parsing network to\nobtain an accurate face structure. Then, we divided the face parsing feature\ninto face foreground and background. Moreover, we constructed a new feature\nadaptive denormalization to regularize fafcial structures as a condition of the\nauxiliary to generate more harmonious and undistorted face structure. In\naddition, we proposed a texture extractor and multi-patch discriminator to\nenhance the generated facial texture information. Experimental results on both\nCelebA and CelebA-HQ datasets demonstrate that the proposed face deblurring\nnetwork restores face structure with more facial details and performs favorably\nagainst state-of-the-art methods in terms of structured similarity indexing\nmethod (SSIM), peak signal-to-noise ratio (PSNR), Frechet inception distance\n(FID) and L1, and qualitative comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1\">Jiancheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaojie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calorie Aware Automatic Meal Kit Generation from an Image. (arXiv:2112.09839v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09839","description":"<p>Calorie and nutrition research has attained increased interest in recent\nyears. But, due to the complexity of the problem, literature in this area\nfocuses on a limited subset of ingredients or dish types and simple\nconvolutional neural networks or traditional machine learning. Simultaneously,\nestimation of ingredient portions can help improve calorie estimation and meal\nre-production from a given image. In this paper, given a single cooking image,\na pipeline for calorie estimation and meal re-production for different servings\nof the meal is proposed. The pipeline contains two stages. In the first stage,\na set of ingredients associated with the meal in the given image are predicted.\nIn the second stage, given image features and ingredients, portions of the\ningredients and finally the total meal calorie are simultaneously estimated\nusing a deep transformer-based model. Portion estimation introduced in the\nmodel helps improve calorie estimation and is also beneficial for meal\nre-production in different serving sizes. To demonstrate the benefits of the\npipeline, the model can be used for meal kits generation. To evaluate the\npipeline, the large scale dataset Recipe1M is used. Prior to experiments, the\nRecipe1M dataset is parsed and explicitly annotated with portions of\ningredients. Experiments show that using ingredients and their portions\nsignificantly improves calorie estimation. Also, a visual interface is created\nin which a user can interact with the pipeline to reach accurate calorie\nestimations and generate a meal kit for cooking purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jelodar_A/0/1/0/all/0/1\">Ahmad Babaeian Jelodar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Object Detection in Floor-plan through Super Resolution. (arXiv:2112.09844v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09844","description":"<p>Building Information Modelling (BIM) software use scalable vector formats to\nenable flexible designing of floor plans in the industry. Floor plans in the\narchitectural domain can come from many sources that may or may not be in\nscalable vector format. The conversion of floor plan images to fully annotated\nvector images is a process that can now be realized by computer vision. Novel\ndatasets in this field have been used to train Convolutional Neural Network\n(CNN) architectures for object detection. Image enhancement through\nSuper-Resolution (SR) is also an established CNN based network in computer\nvision that is used for converting low resolution images to high resolution\nones. This work focuses on creating a multi-component module that stacks a SR\nmodel on a floor plan object detection model. The proposed stacked model shows\ngreater performance than the corresponding vanilla object detection model. For\nthe best case, the the inclusion of SR showed an improvement of 39.47% in\nobject detection over the vanilla network. Data and code are made publicly\navailable at https://github.com/rbg-research/Floor-Plan-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khare_D/0/1/0/all/0/1\">Dev Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_N/0/1/0/all/0/1\">N S Kamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HB_B/0/1/0/all/0/1\">Barathi Ganesh HB</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sowmya_V/0/1/0/all/0/1\">V Sowmya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Variyar_V/0/1/0/all/0/1\">V V Sajith Variyar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegoDNN: Block-grained Scaling of Deep Neural Networks for Mobile Vision. (arXiv:2112.09852v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09852","description":"<p>Deep neural networks (DNNs) have become ubiquitous techniques in mobile and\nembedded systems for applications such as image/object recognition and\nclassification. The trend of executing multiple DNNs simultaneously exacerbate\nthe existing limitations of meeting stringent latency/accuracy requirements on\nresource constrained mobile devices. The prior art sheds light on exploring the\naccuracy-resource tradeoff by scaling the model sizes in accordance to resource\ndynamics. However, such model scaling approaches face to imminent challenges:\n(i) large space exploration of model sizes, and (ii) prohibitively long\ntraining time for different model combinations. In this paper, we present\nLegoDNN, a lightweight, block-grained scaling solution for running multi-DNN\nworkloads in mobile vision systems. LegoDNN guarantees short model training\ntimes by only extracting and training a small number of common blocks (e.g. 5\nin VGG and 8 in ResNet) in a DNN. At run-time, LegoDNN optimally combines the\ndescendant models of these blocks to maximize accuracy under specific resources\nand latency constraints, while reducing switching overhead via smart\nblock-level scaling of the DNN. We implement LegoDNN in TensorFlow Lite and\nextensively evaluate it against state-of-the-art techniques (FLOP scaling,\nknowledge distillation and model compression) using a set of 12 popular DNN\nmodels. Evaluation results show that LegoDNN provides 1,296x to 279,936x more\noptions in model sizes without increasing training time, thus achieving as much\nas 31.74% improvement in inference accuracy and 71.07% reduction in scaling\nenergy consumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rui Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lydia Y. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Space Non-cooperative Object Active Tracking with Deep Reinforcement Learning. (arXiv:2112.09854v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09854","description":"<p>Active visual tracking of space non-cooperative object is significant for\nfuture intelligent spacecraft to realise space debris removal, asteroid\nexploration, autonomous rendezvous and docking. However, existing works often\nconsider this task into different subproblems (e.g. image preprocessing,\nfeature extraction and matching, position and pose estimation, control law\ndesign) and optimize each module alone, which are trivial and sub-optimal. To\nthis end, we propose an end-to-end active visual tracking method based on DQN\nalgorithm, named as DRLAVT. It can guide the chasing spacecraft approach to\narbitrary space non-cooperative target merely relied on color or RGBD images,\nwhich significantly outperforms position-based visual servoing baseline\nalgorithm that adopts state-of-the-art 2D monocular tracker, SiamRPN. Extensive\nexperiments implemented with diverse network architectures, different\nperturbations and multiple targets demonstrate the advancement and robustness\nof DRLAVT. In addition, We further prove our method indeed learnt the motion\npatterns of target with deep reinforcement learning through hundreds of\ntrial-and-errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guanghui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenxiao Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An effective coaxiality error measurement for twist drill based on line structured light sensor. (arXiv:2112.09873v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09873","description":"<p>Since the structure of twist drill is complex, it is hard and challenging for\nits coaxiality error measurement. In this paper, a novel mechanism, framework\nand method of coaxiality error measurement for twist drill is proposed. The\nmechanism includes encoder, PLC controller, line structured sensor and high\nprecision turntable. First, profile point cloud data of the twist drill is\ncollected through the line structured light sensor when the drill turns around\nin the controlling of PLC. Second, a GMM-based point cloud segmentation\nalgorithm based on local depth features is investigated to extract blade back\ndata. To improve the measurement accuracy, a statistical filter is designed to\nremove outliers during the target region extraction. Then, according to two\ncharacteristics of coaxiality error, an axis reconstruction method based on\northogonal synthesis of axisymmetric contour differences is presented, which is\nfacilitated to pre-position the maximum deviation cross sections of the drill\naxis. Finally, the coaxiality error is measured through fitting the benchmark\naxis and the axis at the pre-positioned maximum deviation position. At the end,\na large number of experiments are carried out, and it shows that our method is\naccuracy and robust.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1\">Ailing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiaojiao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shufang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Fei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Memory Networks for Action Prediction. (arXiv:2112.09875v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09875","description":"<p>Action prediction aims to infer the forthcoming human action with\npartially-observed videos, which is a challenging task due to the limited\ninformation underlying early observations. Existing methods mainly adopt a\nreconstruction strategy to handle this task, expecting to learn a single\nmapping function from partial observations to full videos to facilitate the\nprediction process. In this study, we propose adversarial memory networks\n(AMemNet) to generate the \"full video\" feature conditioning on a partial video\nquery from two new aspects. Firstly, a key-value structured memory generator is\ndesigned to memorize different partial videos as key memories and dynamically\nwrite full videos in value memories with gating mechanism and querying\nattention. Secondly, we develop a class-aware discriminator to guide the memory\ngenerator to deliver not only realistic but also discriminative full video\nfeatures upon adversarial training. The final prediction result of AMemNet is\ngiven by late fusion over RGB and optical flow streams. Extensive experimental\nresults on two benchmark video datasets, UCF-101 and HMDB51, are provided to\ndemonstrate the effectiveness of the proposed AMemNet model over\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1\">Zhiqiang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yue Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Handong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Explainable Machine Learning Uncover the Black Box in Vision Applications?. (arXiv:2112.09898v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09898","description":"<p>Machine learning (ML) in general and deep learning (DL) in particular has\nbecome an extremely popular tool in several vision applications (like object\ndetection, super resolution, segmentation, object tracking etc.). Almost in\nparallel, the issue of explainability in ML (i.e. the ability to\nexplain/elaborate the way a trained ML model arrived at its decision) in vision\nhas also received fairly significant attention from various quarters. However,\nwe argue that the current philosophy behind explainable ML suffers from certain\nlimitations, and the resulting explanations may not meaningfully uncover black\nbox ML models. To elaborate our assertion, we first raise a few fundamental\nquestions which have not been adequately discussed in the corresponding\nliterature. We also provide perspectives on how explainablity in ML can benefit\nby relying on more rigorous principles in the related areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narwaria_M/0/1/0/all/0/1\">Manish Narwaria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Instance Segmentation of MVS Buildings. (arXiv:2112.09902v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09902","description":"<p>We present a novel framework for instance segmentation of 3D buildings from\nMulti-view Stereo (MVS) urban scenes. Unlike existing works focusing on\nsemantic segmentation of an urban scene, the emphasis of this work lies in\ndetecting and segmenting 3D building instances even if they are attached and\nembedded in a large and imprecise 3D surface model. Multi-view RGB images are\nfirst enhanced to RGBH images by adding a heightmap and are segmented to obtain\nall roof instances using a fine-tuned 2D instance segmentation neural network.\nRoof instance masks from different multi-view images are then clustered into\nglobal masks. Our mask clustering accounts for spatial occlusion and\noverlapping, which can eliminate segmentation ambiguities among multi-view\nimages. Based on these global masks, 3D roof instances are segmented out by\nmask back-projections and extended to the entire building instances through a\nMarkov random field (MRF) optimization. Quantitative evaluations and ablation\nstudies have shown the effectiveness of all major steps of the method. A\ndataset for the evaluation of instance segmentation of 3D building models is\nprovided as well. To the best of our knowledge, it is the first dataset for 3D\nurban buildings on the instance segmentation level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiazhou Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shufang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ronghua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly Discovery in Semantic Segmentation via Distillation Comparison Networks. (arXiv:2112.09908v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09908","description":"<p>This paper aims to address the problem of anomaly discovery in semantic\nsegmentation. Our key observation is that semantic classification plays a\ncritical role in existing approaches, while the incorrectly classified pixels\nare easily regarded as anomalies. Such a phenomenon frequently appears and is\nrarely discussed, which significantly reduces the performance of anomaly\ndiscovery. To this end, we propose a novel Distillation Comparison Network\n(DiCNet). It comprises of a teacher branch which is a semantic segmentation\nnetwork that removed the semantic classification head, and a student branch\nthat is distilled from the teacher branch through a distribution distillation.\nWe show that the distillation guarantees the semantic features of the two\nbranches hold consistency in the known classes, while reflect inconsistency in\nthe unknown class. Therefore, we leverage the semantic feature discrepancy\nbetween the two branches to discover the anomalies. DiCNet abandons the\nsemantic classification head in the inference process, and hence significantly\nalleviates the issue caused by incorrect semantic classification. Extensive\nexperimental results on StreetHazards dataset and BDD-Anomaly dataset are\nconducted to verify the superior performance of DiCNet. In particular, DiCNet\nobtains a 6.3% improvement in AUPR and a 5.2% improvement in FPR95 on\nStreetHazards dataset, achieves a 4.2% improvement in AUPR and a 6.8%\nimprovement in FPR95 on BDD-Anomaly dataset. Codes are available at\nhttps://github.com/zhouhuan-hust/DiCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shi Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zengqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ronghua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Robust Registration of Partially Overlapping Point Clouds. (arXiv:2112.09922v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09922","description":"<p>Real-time registration of partially overlapping point clouds has emerging\napplications in cooperative perception for autonomous vehicles and multi-agent\nSLAM. The relative translation between point clouds in these applications is\nhigher than in traditional SLAM and odometry applications, which challenges the\nidentification of correspondences and a successful registration. In this paper,\nwe propose a novel registration method for partially overlapping point clouds\nwhere correspondences are learned using an efficient point-wise feature\nencoder, and refined using a graph-based attention network. This attention\nnetwork exploits geometrical relationships between key points to improve the\nmatching in point clouds with low overlap. At inference time, the relative pose\ntransformation is obtained by robustly fitting the correspondences through\nsample consensus. The evaluation is performed on the KITTI dataset and a novel\nsynthetic dataset including low-overlapping point clouds with displacements of\nup to 30m. The proposed method achieves on-par performance with\nstate-of-the-art methods on the KITTI dataset, and outperforms existing methods\nfor low overlapping point clouds. Additionally, the proposed method achieves\nsignificantly faster inference times, as low as 410ms, between 5 and 35 times\nfaster than competing methods. Our code and dataset are available at\nhttps://github.com/eduardohenriquearnold/fastreg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arnold_E/0/1/0/all/0/1\">Eduardo Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozaffari_S/0/1/0/all/0/1\">Sajjad Mozaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dianati_M/0/1/0/all/0/1\">Mehrdad Dianati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepUME: Learning the Universal Manifold Embedding for Robust Point Cloud Registration. (arXiv:2112.09938v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09938","description":"<p>Registration of point clouds related by rigid transformations is one of the\nfundamental problems in computer vision. However, a solution to the practical\nscenario of aligning sparsely and differently sampled observations in the\npresence of noise is still lacking. We approach registration in this scenario\nwith a fusion of the closed-form Universal Mani-fold Embedding (UME) method and\na deep neural network. The two are combined into a single unified framework,\nnamed DeepUME, trained end-to-end and in an unsupervised manner. To\nsuccessfully provide a global solution in the presence of large\ntransformations, we employ an SO(3)-invariant coordinate system to learn both a\njoint-resampling strategy of the point clouds and SO(3)-invariant features.\nThese features are then utilized by the geometric UME method for transformation\nestimation. The parameters of DeepUME are optimized using a metric designed to\novercome an ambiguity problem emerging in the registration of symmetric shapes,\nwhen noisy scenarios are considered. We show that our hybrid method outperforms\nstate-of-the-art registration methods in various scenarios, and generalizes\nwell to unseen data sets. Our code is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_N/0/1/0/all/0/1\">Natalie Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Francos_J/0/1/0/all/0/1\">Joseph M. Francos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid Face Mask Detection and Person Identification Model based on Deep Neural Networks. (arXiv:2112.09951v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09951","description":"<p>As Covid-19 has been constantly getting mutated and in three or four months a\nnew variant gets introduced to us and it comes with more deadly problems. The\nthings that prevent us from getting Covid is getting vaccinated and wearing a\nface mask. In this paper, we have implemented a new Face Mask Detection and\nPerson Recognition model named Insight face which is based on SoftMax loss\nclassification algorithm Arc Face loss and names it as RFMPI-DNN(Rapid Face\nDetection and Peron Identification Model based on Deep Neural Networks) to\ndetect face mask and person identity rapidly as compared to other models\navailable. To compare our new model, we have used previous MobileNet_V2 model\nand face recognition module for effective comparison on the basis of time. The\nproposed model implemented in the system has outperformed the model compared in\nthis paper in every aspect\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Abdullah Ahmad Khan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Belal_M/0/1/0/all/0/1\">Mohd. Belal</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+GhufranUllah/0/1/0/all/0/1\">GhufranUllah</a> (3) ((1,2 and 3) Aligarh Muslim University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Training Transformers for Domain Adaptation. (arXiv:2112.09965v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09965","description":"<p>The Visual Domain Adaptation Challenge 2021 called for unsupervised domain\nadaptation methods that could improve the performance of models by transferring\nthe knowledge obtained from source datasets to out-of-distribution target\ndatasets. In this paper, we utilize BeiT [1] and demonstrate its capability of\ncapturing key attributes from source datasets and apply it to target datasets\nin a semi-supervised manner. Our method was able to outperform current\nstate-of-the-art (SoTA) techniques and was able to achieve 1st place on the\nViSDA Domain Adaptation Challenge with ACC of 56.29% and AUROC of 69.79%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tayyab_B/0/1/0/all/0/1\">Burhan Ul Tayyab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_N/0/1/0/all/0/1\">Nicholas Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Structural Analysis of the Optic Nerve Head to Robustly Discriminate Between Papilledema and Optic Disc Drusen. (arXiv:2112.09970v1 [eess.IV])","link":"http://arxiv.org/abs/2112.09970","description":"<p>Purpose: (1) To develop a deep learning algorithm to identify major tissue\nstructures of the optic nerve head (ONH) in 3D optical coherence tomography\n(OCT) scans; (2) to exploit such information to robustly differentiate among\nhealthy, optic disc drusen (ODD), and papilledema ONHs.\n</p>\n<p>It was a cross-sectional comparative study with confirmed ODD (105 eyes),\npapilledema due to high intracranial pressure (51 eyes), and healthy controls\n(100 eyes). 3D scans of the ONHs were acquired using OCT, then processed to\nimprove deep-tissue visibility. At first, a deep learning algorithm was\ndeveloped using 984 B-scans (from 130 eyes) in order to identify: major\nneural/connective tissues, and ODD regions. The performance of our algorithm\nwas assessed using the Dice coefficient (DC). In a 2nd step, a classification\nalgorithm (random forest) was designed using 150 OCT volumes to perform 3-class\nclassifications (1: ODD, 2: papilledema, 3: healthy) strictly from their drusen\nand prelamina swelling scores (derived from the segmentations). To assess\nperformance, we reported the area under the receiver operating characteristic\ncurves (AUCs) for each class.\n</p>\n<p>Our segmentation algorithm was able to isolate neural and connective tissues,\nand ODD regions whenever present. This was confirmed by an average DC of\n0.93$\\pm$0.03 on the test set, corresponding to good performance.\nClassification was achieved with high AUCs, i.e. 0.99$\\pm$0.01 for the\ndetection of ODD, 0.99 $\\pm$ 0.01 for the detection of papilledema, and\n0.98$\\pm$0.02 for the detection of healthy ONHs.\n</p>\n<p>Our AI approach accurately discriminated ODD from papilledema, using a single\nOCT scan. Our classification performance was excellent, with the caveat that\nvalidation in a much larger population is warranted. Our approach may have the\npotential to establish OCT as the mainstay of diagnostic imaging in\nneuro-ophthalmology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Girard_M/0/1/0/all/0/1\">Micha&#xeb;l J.A. Girard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panda_S/0/1/0/all/0/1\">Satish K. Panda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tun_T/0/1/0/all/0/1\">Tin Aung Tun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wibroe_E/0/1/0/all/0/1\">Elisabeth A. Wibroe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Najjar_R/0/1/0/all/0/1\">Raymond P. Najjar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tin_A/0/1/0/all/0/1\">Aung Tin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiery_A/0/1/0/all/0/1\">Alexandre H. Thi&#xe9;ry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamann_S/0/1/0/all/0/1\">Steffen Hamann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fraser_C/0/1/0/all/0/1\">Clare Fraser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milea_D/0/1/0/all/0/1\">Dan Milea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tell me what you see: A zero-shot action recognition method based on natural language descriptions. (arXiv:2112.09976v1 [cs.CV])","link":"http://arxiv.org/abs/2112.09976","description":"<p>Recently, several approaches have explored the detection and classification\nof objects in videos to perform Zero-Shot Action Recognition with remarkable\nresults. In these methods, class-object relationships are used to associate\nvisual patterns with the semantic side information because these relationships\nalso tend to appear in texts. Therefore, word vector methods would reflect them\nin their latent representations. Inspired by these methods and by video\ncaptioning's ability to describe events not only with a set of objects but with\ncontextual information, we propose a method in which video captioning models,\ncalled observers, provide different and complementary descriptive sentences. We\ndemonstrate that representing videos with descriptive sentences instead of deep\nfeatures, in ZSAR, is viable and naturally alleviates the domain adaptation\nproblem, as we reached state-of-the-art (SOTA) performance on the UCF101\ndataset and competitive performance on HMDB51 without their training sets. We\nalso demonstrate that word vectors are unsuitable for building the semantic\nembedding space of our descriptions. Thus, we propose to represent the classes\nwith sentences extracted from documents acquired with search engines on the\nInternet, without any human evaluation on the quality of descriptions. Lastly,\nwe build a shared semantic space employing BERT-based embedders pre-trained in\nthe paraphrasing task on multiple text datasets. We show that this pre-training\nis essential for bridging the semantic gap. The projection onto this space is\nstraightforward for both types of information, visual and semantic, because\nthey are sentences, enabling the classification with nearest neighbour rule in\nthis shared space. Our code is available at\nhttps://github.com/valterlej/zsarcap.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Estevam_V/0/1/0/all/0/1\">Valter Estevam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroca_R/0/1/0/all/0/1\">Rayson Laroca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menotti_D/0/1/0/all/0/1\">David Menotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrini_H/0/1/0/all/0/1\">Helio Pedrini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Federated Learning in Medical Imaging. (arXiv:2112.10001v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10001","description":"<p>Federated learning is increasingly being explored in the field of medical\nimaging to train deep learning models on large scale datasets distributed\nacross different data centers while preserving privacy by avoiding the need to\ntransfer sensitive patient information. In this manuscript, we explore\nfederated learning in a multi-domain, multi-task setting wherein different\nparticipating nodes may contain datasets sourced from different domains and are\ntrained to solve different tasks. We evaluated cross-domain federated learning\nfor the tasks of object detection and segmentation across two different\nexperimental settings: multi-modal and multi-organ. The result from our\nexperiments on cross-domain federated learning framework were very encouraging\nwith an overlap similarity of 0.79 for organ localization and 0.65 for lesion\nsegmentation. Our results demonstrate the potential of federated learning in\ndeveloping multi-domain, multi-task deep learning models without sharing data\nfrom different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Parekh_V/0/1/0/all/0/1\">Vishwa S Parekh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_S/0/1/0/all/0/1\">Shuhao Lai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leal_J/0/1/0/all/0/1\">Jeff Leal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rowe_S/0/1/0/all/0/1\">Steven Rowe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pillai_J/0/1/0/all/0/1\">Jay J Pillai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jacobs_M/0/1/0/all/0/1\">Michael A Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-Based Multi-Modal Image Segmentation. (arXiv:2112.10003v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10003","description":"<p>Image segmentation is usually addressed by training a model for a fixed set\nof object classes. Incorporating additional classes or more complex queries\nlater is expensive as it requires re-training the model on a dataset that\nencompasses these expressions. Here we propose a system that can generate image\nsegmentations based on arbitrary prompts at test time. A prompt can be either a\ntext or an image. This approach enables us to create a unified model (trained\nonce) for three common segmentation tasks, which come with distinct challenges:\nreferring expression segmentation, zero-shot segmentation and one-shot\nsegmentation. We build upon the CLIP model as a backbone which we extend with a\ntransformer-based decoder that enables dense prediction. After training on an\nextended version of the PhraseCut dataset, our system generates a binary\nsegmentation map for an image based on a free-text prompt or on an additional\nimage expressing the query. Different variants of the latter image-based\nprompts are analyzed in detail. This novel hybrid input allows for dynamic\nadaptation not only to the three segmentation tasks mentioned above, but to any\nbinary segmentation task where a text or image query can be formulated.\nFinally, we find our system to adapt well to generalized queries involving\naffordances or properties. Source code: https://eckerlab.org/code/clipseg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luddecke_T/0/1/0/all/0/1\">Timo L&#xfc;ddecke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ecker_A/0/1/0/all/0/1\">Alexander S. Ecker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks. (arXiv:2112.10017v1 [cs.LG])","link":"http://arxiv.org/abs/2112.10017","description":"<p>Existing research on continual learning of a sequence of tasks focused on\ndealing with catastrophic forgetting, where the tasks are assumed to be\ndissimilar and have little shared knowledge. Some work has also been done to\ntransfer previously learned knowledge to the new task when the tasks are\nsimilar and have shared knowledge. To the best of our knowledge, no technique\nhas been proposed to learn a sequence of mixed similar and dissimilar tasks\nthat can deal with forgetting and also transfer knowledge forward and backward.\nThis paper proposes such a technique to learn both types of tasks in the same\nnetwork. For dissimilar tasks, the algorithm focuses on dealing with\nforgetting, and for similar tasks, the algorithm focuses on selectively\ntransferring the knowledge learned from some similar previous tasks to improve\nthe new task learning. Additionally, the algorithm automatically detects\nwhether a new task is similar to any previous tasks. Empirical evaluation using\nsequences of mixed tasks demonstrates the effectiveness of the proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zixuan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingchang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised laser-speckle image sampling of skin tissue to detect very early stage of diabetes by its effects on skin subcellular properties. (arXiv:2112.10024v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10024","description":"<p>This paper investigates the effectiveness of an expert system based on\nK-nearest neighbors algorithm for laser speckle image sampling applied to the\nearly detection of diabetes. With the latest developments in artificial\nintelligent guided laser speckle imaging technologies, it may be possible to\noptimise laser parameters, such as wavelength, energy level and image texture\nmeasures in association with a suitable AI technique to interact effectively\nwith the subcellular properties of a skin tissue to detect early signs of\ndiabetes. The new approach is potentially more effective than the classical\nskin glucose level observation because of its optimised combination of laser\nphysics and AI techniques, and additionally, it allows non-expert individuals\nto perform more frequent skin tissue tests for an early detection of diabetes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Orun_A/0/1/0/all/0/1\">Ahmet Orun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Critien_L/0/1/0/all/0/1\">Luke Vella Critien</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Carter_J/0/1/0/all/0/1\">Jennifer Carter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stacey_M/0/1/0/all/0/1\">Martin Stacey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A-ESRGAN: Training Real-World Blind Super-Resolution with Attention U-Net Discriminators. (arXiv:2112.10046v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10046","description":"<p>Blind image super-resolution(SR) is a long-standing task in CV that aims to\nrestore low-resolution images suffering from unknown and complex distortions.\nRecent work has largely focused on adopting more complicated degradation models\nto emulate real-world degradations. The resulting models have made\nbreakthroughs in perceptual loss and yield perceptually convincing results.\nHowever, the limitation brought by current generative adversarial network\nstructures is still significant: treating pixels equally leads to the ignorance\nof the image's structural features, and results in performance drawbacks such\nas twisted lines and background over-sharpening or blurring. In this paper, we\npresent A-ESRGAN, a GAN model for blind SR tasks featuring an attention U-Net\nbased, multi-scale discriminator that can be seamlessly integrated with other\ngenerators. To our knowledge, this is the first work to introduce attention\nU-Net structure as the discriminator of GAN to solve blind SR problems. And the\npaper also gives an interpretation for the mechanism behind multi-scale\nattention U-Net that brings performance breakthrough to the model. Through\ncomparison experiments with prior works, our model presents state-of-the-art\nlevel performance on the non-reference natural image quality evaluator metric.\nAnd our ablation studies have shown that with our discriminator, the RRDB based\ngenerator can leverage the structural features of an image in multiple scales,\nand consequently yields more perceptually realistic high-resolution images\ncompared to prior works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_Z/0/1/0/all/0/1\">Zihao Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yidong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yuang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_C/0/1/0/all/0/1\">Chenhao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1\">Jinnan Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlling the Quality of Distillation in Response-Based Network Compression. (arXiv:2112.10047v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10047","description":"<p>The performance of a distillation-based compressed network is governed by the\nquality of distillation. The reason for the suboptimal distillation of a large\nnetwork (teacher) to a smaller network (student) is largely attributed to the\ngap in the learning capacities of given teacher-student pair. While it is hard\nto distill all the knowledge of a teacher, the quality of distillation can be\ncontrolled to a large extent to achieve better performance. Our experiments\nshow that the quality of distillation is largely governed by the quality of\nteacher's response, which in turn is heavily affected by the presence of\nsimilarity information in its response. A well-trained large capacity teacher\nloses similarity information between classes in the process of learning\nfine-grained discriminative properties for classification. The absence of\nsimilarity information causes the distillation process to be reduced from one\nexample-many class learning to one example-one class learning, thereby\nthrottling the flow of diverse knowledge from the teacher. With the implicit\nassumption that only the instilled knowledge can be distilled, instead of\nfocusing only on the knowledge distilling process, we scrutinize the knowledge\ninculcation process. We argue that for a given teacher-student pair, the\nquality of distillation can be improved by finding the sweet spot between batch\nsize and number of epochs while training the teacher. We discuss the steps to\nfind this sweet spot for better distillation. We also propose the distillation\nhypothesis to differentiate the behavior of the distillation process between\nknowledge distillation and regularization effect. We conduct all our\nexperiments on three different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vats_V/0/1/0/all/0/1\">Vibhas Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Precondition and Effect Reasoning for Action Recognition. (arXiv:2112.10057v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10057","description":"<p>Human action recognition has drawn a lot of attention in the recent years due\nto the research and application significance. Most existing works on action\nrecognition focus on learning effective spatial-temporal features from videos,\nbut neglect the strong causal relationship among the precondition, action and\neffect. Such relationships are also crucial to the accuracy of action\nrecognition. In this paper, we propose to model the causal relationships based\non the precondition and effect to improve the performance of action\nrecognition. Specifically, a Cycle-Reasoning model is proposed to capture the\ncausal relationships for action recognition. To this end, we annotate\nprecondition and effect for a large-scale action dataset. Experimental results\nshow that the proposed Cycle-Reasoning model can effectively reason about the\nprecondition and effect and can enhance action recognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hongsang_Y/0/1/0/all/0/1\">Yoo Hongsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liangchen_L/0/1/0/all/0/1\">Liu Liangchen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Z/0/1/0/all/0/1\">Zhang Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Graph-level Anomaly Detection by Glocal Knowledge Distillation. (arXiv:2112.10063v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10063","description":"<p>Graph-level anomaly detection (GAD) describes the problem of detecting graphs\nthat are abnormal in their structure and/or the features of their nodes, as\ncompared to other graphs. One of the challenges in GAD is to devise graph\nrepresentations that enable the detection of both locally- and\nglobally-anomalous graphs, i.e., graphs that are abnormal in their fine-grained\n(node-level) or holistic (graph-level) properties, respectively. To tackle this\nchallenge we introduce a novel deep anomaly detection approach for GAD that\nlearns rich global and local normal pattern information by joint random\ndistillation of graph and node representations. The random distillation is\nachieved by training one GNN to predict another GNN with randomly initialized\nnetwork weights. Extensive experiments on 16 real-world graph datasets from\ndiverse domains show that our model significantly outperforms seven\nstate-of-the-art models. Code and datasets are available at\nhttps://git.io/GLocalKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rongrong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Ling Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Strong Scaling Through Burst Parallel Training. (arXiv:2112.10065v1 [cs.DC])","link":"http://arxiv.org/abs/2112.10065","description":"<p>As emerging deep neural network (DNN) models continue to grow in size, using\nlarge GPU clusters to train DNNs is becoming an essential requirement to\nachieving acceptable training times. In this paper, we consider the case where\nfuture increases in cluster size will cause the global batch size that can be\nused to train models to reach a fundamental limit: beyond a certain point,\nlarger global batch sizes cause sample efficiency to degrade, increasing\noverall time to accuracy. As a result, to achieve further improvements in\ntraining performance, we must instead consider \"strong scaling\" strategies that\nhold the global batch size constant and allocate smaller batches to each GPU.\nUnfortunately, this makes it significantly more difficult to use cluster\nresources efficiently. We present DeepPool, a system that addresses this\nefficiency challenge through two key ideas. First, burst parallelism allocates\nlarge numbers of GPUs to foreground jobs in bursts to exploit the unevenness in\nparallelism across layers. Second, GPU multiplexing prioritizes throughput for\nforeground training jobs, while packing in background training jobs to reclaim\nunderutilized GPU resources, thereby improving cluster-wide utilization.\nTogether, these two ideas enable DeepPool to deliver a 2.2 - 2.4x improvement\nin total cluster throughput over standard data parallelism with a single task\nwhen the cluster scale is large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seo Jin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_J/0/1/0/all/0/1\">Joshua Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mohammad Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belay_A/0/1/0/all/0/1\">Adam Belay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LocFormer: Enabling Transformers to Perform Temporal Moment Localization on Long Untrimmed Videos With a Feature Sampling Approach. (arXiv:2112.10066v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10066","description":"<p>We propose LocFormer, a Transformer-based model for video grounding which\noperates at a constant memory footprint regardless of the video length, i.e.\nnumber of frames. LocFormer is designed for tasks where it is necessary to\nprocess the entire long video and at its core lie two main contributions.\nFirst, our model incorporates a new sampling technique that splits the input\nfeature sequence into a fixed number of sections and selects a single feature\nper section using a stochastic approach, which allows us to obtain a feature\nsample set that is representative of the video content for the task at hand\nwhile keeping the memory footprint constant. Second, we propose a modular\ndesign that separates functionality, enabling us to learn an inductive bias via\nsupervising the self-attention heads, while also effectively leveraging\npre-trained text and video encoders. We test our proposals on relevant\nbenchmark datasets for video grounding, showing that not only LocFormer can\nachieve excellent results including state-of-the-art performance on YouCookII,\nbut also that our sampling technique is more effective than competing\ncounterparts and that it consistently improves the performance of prior work,\nby up to 3.13\\% in the mean temporal IoU, ultimately leading to a new\nstate-of-the-art performance on Charades-STA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Opazo_C/0/1/0/all/0/1\">Cristian Rodriguez-Opazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1\">Edison Marrese-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1\">Basura Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takamura_H/0/1/0/all/0/1\">Hiroya Takamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Image Codec Paradigm for Human and Machine Uses. (arXiv:2112.10071v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10071","description":"<p>With the AI of Things (AIoT) development, a huge amount of visual data, e.g.,\nimages and videos, are produced in our daily work and life. These visual data\nare not only used for human viewing or understanding but also for machine\nanalysis or decision-making, e.g., intelligent surveillance, automated\nvehicles, and many other smart city applications. To this end, a new image\ncodec paradigm for both human and machine uses is proposed in this work.\nFirstly, the high-level instance segmentation map and the low-level signal\nfeatures are extracted with neural networks. Then, the instance segmentation\nmap is further represented as a profile with the proposed 16-bit gray-scale\nrepresentation. After that, both 16-bit gray-scale profile and signal features\nare encoded with a lossless codec. Meanwhile, an image predictor is designed\nand trained to achieve the general-quality image reconstruction with the 16-bit\ngray-scale profile and signal features. Finally, the residual map between the\noriginal image and the predicted one is compressed with a lossy codec, used for\nhigh-quality image reconstruction. With such designs, on the one hand, we can\nachieve scalable image compression to meet the requirements of different human\nconsumption; on the other hand, we can directly achieve several machine vision\ntasks at the decoder side with the decoded 16-bit gray-scale profile, e.g.,\nobject classification, detection, and segmentation. Experimental results show\nthat the proposed codec achieves comparable results as most learning-based\ncodecs and outperforms the traditional codecs (e.g., BPG and JPEG2000) in terms\nof PSNR and MS-SSIM for image reconstruction. At the same time, it outperforms\nthe existing codecs in terms of the mAP for object detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sien Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_J/0/1/0/all/0/1\">Jian Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_L/0/1/0/all/0/1\">Lili Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_T/0/1/0/all/0/1\">Tsui-Shan Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Huaxiang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QU-BraTS: MICCAI BraTS 2020 Challenge on Quantifying Uncertainty in Brain Tumor Segmentation -- Analysis of Ranking Metrics and Benchmarking Results. (arXiv:2112.10074v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10074","description":"<p>Deep learning (DL) models have provided the state-of-the-art performance in a\nwide variety of medical imaging benchmarking challenges, including the Brain\nTumor Segmentation (BraTS) challenges. However, the task of focal pathology\nmulti-compartment segmentation (e.g., tumor and lesion sub-regions) is\nparticularly challenging, and potential errors hinder the translation of DL\nmodels into clinical workflows. Quantifying the reliability of DL model\npredictions in the form of uncertainties, could enable clinical review of the\nmost uncertain regions, thereby building trust and paving the way towards\nclinical translation. Recently, a number of uncertainty estimation methods have\nbeen introduced for DL medical image segmentation tasks. Developing metrics to\nevaluate and compare the performance of uncertainty measures will assist the\nend-user in making more informed decisions. In this study, we explore and\nevaluate a metric developed during the BraTS 2019-2020 task on uncertainty\nquantification (QU-BraTS), and designed to assess and rank uncertainty\nestimates for brain tumor multi-compartment segmentation. This metric (1)\nrewards uncertainty estimates that produce high confidence in correct\nassertions, and those that assign low confidence levels at incorrect\nassertions, and (2) penalizes uncertainty measures that lead to a higher\npercentages of under-confident correct assertions. We further benchmark the\nsegmentation uncertainties generated by 14 independent participating teams of\nQU-BraTS 2020, all of which also participated in the main BraTS segmentation\ntask. Overall, our findings confirm the importance and complementary value that\nuncertainty estimates provide to segmentation algorithms, and hence highlight\nthe need for uncertainty quantification in medical image analyses. Our\nevaluation code is made publicly available at\nhttps://github.com/RagMeh11/QU-BraTS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1\">Raghav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Filos_A/0/1/0/all/0/1\">Angelos Filos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sako_C/0/1/0/all/0/1\">Chiharu Sako</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McKinley_R/0/1/0/all/0/1\">Richard McKinley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rebsamen_M/0/1/0/all/0/1\">Michael Rebsamen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Datwyler_K/0/1/0/all/0/1\">Katrin D&#xe4;twyler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meier_R/0/1/0/all/0/1\">Raphael Meier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Radojewski_P/0/1/0/all/0/1\">Piotr Radojewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murugesan_G/0/1/0/all/0/1\">Gowtham Krishnan Murugesan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nalawade_S/0/1/0/all/0/1\">Sahil Nalawade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ganesh_C/0/1/0/all/0/1\">Chandan Ganesh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wagner_B/0/1/0/all/0/1\">Ben Wagner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_F/0/1/0/all/0/1\">Fang F. Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fei_B/0/1/0/all/0/1\">Baowei Fei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madhuranthakam_A/0/1/0/all/0/1\">Ananth J. Madhuranthakam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maldjian_J/0/1/0/all/0/1\">Joseph A. Maldjian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daza_L/0/1/0/all/0/1\">Laura Daza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gomez_C/0/1/0/all/0/1\">Catalina G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbelaez_P/0/1/0/all/0/1\">Pablo Arbel&#xe1;ez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_C/0/1/0/all/0/1\">Chengliang Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raynaud_H/0/1/0/all/0/1\">Hadrien Raynaud</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1\">Yuanhan Mo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angelini_E/0/1/0/all/0/1\">Elsa Angelini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Banerjee_S/0/1/0/all/0/1\">Subhashis Banerjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pei_L/0/1/0/all/0/1\">Linmin Pei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+AK_M/0/1/0/all/0/1\">Murat AK</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosas_Gonzalez_S/0/1/0/all/0/1\">Sarahi Rosas-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zemmoura_I/0/1/0/all/0/1\">Illyess Zemmoura</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tauber_C/0/1/0/all/0/1\">Clovis Tauber</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vu_M/0/1/0/all/0/1\">Minh H. Vu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nyholm_T/0/1/0/all/0/1\">Tufve Nyholm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lofstedt_T/0/1/0/all/0/1\">Tommy L&#xf6;fstedt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ballestar_L/0/1/0/all/0/1\">Laura Mora Ballestar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vilaplana_V/0/1/0/all/0/1\">Veronica Vilaplana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McHugh_H/0/1/0/all/0/1\">Hugh McHugh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talou_G/0/1/0/all/0/1\">Gonzalo Maso Talou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_A/0/1/0/all/0/1\">Alan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_J/0/1/0/all/0/1\">Jay Patel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_K/0/1/0/all/0/1\">Ken Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoebel_K/0/1/0/all/0/1\">Katharina Hoebel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gidwani_M/0/1/0/all/0/1\">Mishka Gidwani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arun_N/0/1/0/all/0/1\">Nishanth Arun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_S/0/1/0/all/0/1\">Sharut Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aggarwal_M/0/1/0/all/0/1\">Mehak Aggarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_P/0/1/0/all/0/1\">Praveer Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerstner_E/0/1/0/all/0/1\">Elizabeth R. Gerstner</a>, et al. (42 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks. (arXiv:2112.10082v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10082","description":"<p>We present a novel framework that brings the 3D motion retargeting task from\ncontrolled environments to in-the-wild scenarios. In particular, our method is\ncapable of retargeting body motion from a character in a 2D monocular video to\na 3D character without using any motion capture system or 3D reconstruction\nprocedure. It is designed to leverage massive online videos for unsupervised\ntraining, needless of 3D annotations or motion-body pairing information. The\nproposed method is built upon two novel canonicalization operations, structure\ncanonicalization and view canonicalization. Trained with the canonicalization\noperations and the derived regularizations, our method learns to factorize a\nskeleton sequence into three independent semantic subspaces, i.e., motion,\nstructure, and view angle. The disentangled representation enables motion\nretargeting from 2D to 3D with high precision. Our method achieves superior\nperformance on motion transfer benchmarks with large body variations and\nchallenging actions. Notably, the canonicalized skeleton sequence could serve\nas a disentangled and interpretable representation of human motion that\nbenefits action analysis and motion retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1\">Ziang Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Structural Relation for Occlusion-Robust Facial Landmark Localization. (arXiv:2112.10087v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10087","description":"<p>In facial landmark localization tasks, various occlusions heavily degrade the\nlocalization accuracy due to the partial observability of facial features. This\npaper proposes a structural relation network (SRN) for occlusion-robust\nlandmark localization. Unlike most existing methods that simply exploit the\nshape constraint, the proposed SRN aims to capture the structural relations\namong different facial components. These relations can be considered a more\npowerful shape constraint against occlusion. To achieve this, a hierarchical\nstructural relation module (HSRM) is designed to hierarchically reason the\nstructural relations that represent both long- and short-distance spatial\ndependencies. Compared with existing network architectures, HSRM can\nefficiently model the spatial relations by leveraging its geometry-aware\nnetwork architecture, which reduces the semantic ambiguity caused by occlusion.\nMoreover, the SRN augments the training data by synthesizing occluded faces. To\nfurther extend our SRN for occluded video data, we formulate the occluded face\nsynthesis as a Markov decision process (MDP). Specifically, it plans the\nmovement of the dynamic occlusion based on an accumulated reward associated\nwith the performance degradation of the pre-trained SRN. This procedure\naugments hard samples for robust facial landmark tracking. Extensive\nexperimental results indicate that the proposed method achieves outstanding\nperformance on occluded and masked faces. Code is available at\nhttps://github.com/zhuccly/SRN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Congcong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jide Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Songmin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_W/0/1/0/all/0/1\">Weiqin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera-aware Style Separation and Contrastive Learning for Unsupervised Person Re-identification. (arXiv:2112.10089v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10089","description":"<p>Unsupervised person re-identification (ReID) is a challenging task without\ndata annotation to guide discriminative learning. Existing methods attempt to\nsolve this problem by clustering extracted embeddings to generate pseudo\nlabels. However, most methods ignore the intra-class gap caused by camera style\nvariance, and some methods are relatively complex and indirect although they\ntry to solve the negative impact of the camera style on feature distribution.\nTo solve this problem, we propose a camera-aware style separation and\ncontrastive learning method (CA-UReID), which directly separates camera styles\nin the feature space with the designed camera-aware attention module. It can\nexplicitly divide the learnable feature into camera-specific and\ncamera-agnostic parts, reducing the influence of different cameras. Moreover,\nto further narrow the gap across cameras, we design a camera-aware contrastive\ncenter loss to learn more discriminative embedding for each identity. Extensive\nexperiments demonstrate the superiority of our method over the state-of-the-art\nmethods on the unsupervised person ReID task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tengfei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Initiative Defense against Facial Manipulation. (arXiv:2112.10098v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10098","description":"<p>Benefiting from the development of generative adversarial networks (GAN),\nfacial manipulation has achieved significant progress in both academia and\nindustry recently. It inspires an increasing number of entertainment\napplications but also incurs severe threats to individual privacy and even\npolitical security meanwhile. To mitigate such risks, many countermeasures have\nbeen proposed. However, the great majority methods are designed in a passive\nmanner, which is to detect whether the facial images or videos are tampered\nafter their wide propagation. These detection-based methods have a fatal\nlimitation, that is, they only work for ex-post forensics but can not prevent\nthe engendering of malicious behavior. To address the limitation, in this\npaper, we propose a novel framework of initiative defense to degrade the\nperformance of facial manipulation models controlled by malicious users. The\nbasic idea is to actively inject imperceptible venom into target facial data\nbefore manipulation. To this end, we first imitate the target manipulation\nmodel with a surrogate model, and then devise a poison perturbation generator\nto obtain the desired venom. An alternating training strategy are further\nleveraged to train both the surrogate model and the perturbation generator. Two\ntypical facial manipulation tasks: face attribute editing and face reenactment,\nare considered in our initiative defense framework. Extensive experiments\ndemonstrate the effectiveness and robustness of our framework in different\nsettings. Finally, we hope this work can shed some light on initiative\ncountermeasures against more adversarial scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qidong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+WeimingZhang/0/1/0/all/0/1\">WeimingZhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArcFace Knows the Gender, Too!. (arXiv:2112.10101v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10101","description":"<p>The main idea of this paper is that if a model can recognize a person, of\ncourse, it must be able to know the gender of that person, too. Therefore,\ninstead of defining a new model for gender classification, this paper uses\nArcFace features to determine gender, based on the facial features. A face\nimage is given to ArcFace and 512 features are obtained for the face. Then,\nwith the help of traditional machine learning models, gender is determined.\nDiscriminative methods such as Support Vector Machine (SVM), Linear\nDiscriminant, and Logistic Regression well demonstrate that the features\nextracted from the ArcFace create a remarkable distinction between the gender\nclasses. Experiments on the Gender Classification Dataset show that SVM with\nGaussian kernel is able to classify gender with an accuracy of 96.4% using\nArcFace features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farzaneh_M/0/1/0/all/0/1\">Majid Farzaneh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAGA: Stochastic Whole-Body Grasping with Contact. (arXiv:2112.10103v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10103","description":"<p>Human grasping synthesis has numerous applications including AR/VR, video\ngames, and robotics. While some methods have been proposed to generate\nrealistic hand-object interaction for object grasping and manipulation, they\ntypically only consider the hand interacting with objects. In this work, our\ngoal is to synthesize whole-body grasping motion. Given a 3D object, we aim to\ngenerate diverse and natural whole-body human motions that approach and grasp\nthe object. This task is challenging as it requires modeling both whole-body\ndynamics and dexterous finger movements. To this end, we propose SAGA\n(StochAstic whole-body Grasping with contAct) which consists of two key\ncomponents: (a) Static whole-body grasping pose generation. Specifically, we\npropose a multi-task generative model, to jointly learn static whole-body\ngrasping poses and human-object contacts. (b) Grasping motion infilling. Given\nan initial pose and the generated whole-body grasping pose as the starting and\nending poses of the motion respectively, we design a novel contact-aware\ngenerative motion infilling module to generate a diverse set of grasp-oriented\nmotions. We demonstrate the effectiveness of our method being the first\ngenerative framework to synthesize realistic and expressive whole-body motions\nthat approach and grasp randomly placed unseen objects. The code and videos are\navailable at: https://jiahaoplus.github.io/SAGA/saga.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anisotropic mesh adaptation for region-based segmentation accounting for image spatial information. (arXiv:2112.10138v1 [math.NA])","link":"http://arxiv.org/abs/2112.10138","description":"<p>A finite element-based image segmentation strategy enhanced by an anisotropic\nmesh adaptation procedure is presented. The methodology relies on a split\nBregman algorithm for the minimisation of a region-based energy functional and\non an anisotropic recovery-based error estimate to drive mesh adaptation. More\nprecisely, a Bayesian energy functional is considered to account for image\nspatial information, ensuring that the methodology is able to identify\ninhomogeneous spatial patterns in complex images. In addition, the anisotropic\nmesh adaptation guarantees a sharp detection of the interface between\nbackground and foreground of the image, with a reduced number of degrees of\nfreedom. The resulting split-adapt Bregman algorithm is tested on a set of real\nimages showing the accuracy and robustness of the method, even in the presence\nof Gaussian, salt and pepper and speckle noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Giacomini_M/0/1/0/all/0/1\">Matteo Giacomini</a>, <a href=\"http://arxiv.org/find/math/1/au:+Perotto_S/0/1/0/all/0/1\">Simona Perotto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoised Labels for Financial Time-Series Data via Self-Supervised Learning. (arXiv:2112.10139v1 [cs.LG])","link":"http://arxiv.org/abs/2112.10139","description":"<p>The introduction of electronic trading platforms effectively changed the\norganisation of traditional systemic trading from quote-driven markets into\norder-driven markets. Its convenience led to an exponentially increasing amount\nof financial data, which is however hard to use for the prediction of future\nprices, due to the low signal-to-noise ratio and the non-stationarity of\nfinancial time series. Simpler classification tasks -- where the goal is to\npredict the directions of future price movement -- via supervised learning\nalgorithms, need sufficiently reliable labels to generalise well. Labelling\nfinancial data is however less well defined than other domains: did the price\ngo up because of noise or because of signal? The existing labelling methods\nhave limited countermeasures against noise and limited effects in improving\nlearning algorithms. This work takes inspiration from image classification in\ntrading and success in self-supervised learning. We investigate the idea of\napplying computer vision techniques to financial time-series to reduce the\nnoise exposure and hence generate correct labels. We look at the label\ngeneration as the pretext task of a self-supervised learning approach and\ncompare the naive (and noisy) labels, commonly used in the literature, with the\nlabels generated by a denoising autoencoder for the same downstream\nclassification task. Our results show that our denoised labels improve the\nperformances of the downstream learning algorithm, for both small and large\ndatasets. We further show that the signals we obtain can be used to effectively\ntrade with binary strategies. We suggest that with proposed techniques,\nself-supervised learning constitutes a powerful framework for generating\n\"better\" financial labels that are useful for studying the underlying patterns\nof the market.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanqing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ventre_C/0/1/0/all/0/1\">Carmine Ventre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polukarov_M/0/1/0/all/0/1\">Maria Polukarov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoboAssembly: Learning Generalizable Furniture Assembly Policy in a Novel Multi-robot Contact-rich Simulation Environment. (arXiv:2112.10143v1 [cs.RO])","link":"http://arxiv.org/abs/2112.10143","description":"<p>Part assembly is a typical but challenging task in robotics, where robots\nassemble a set of individual parts into a complete shape. In this paper, we\ndevelop a robotic assembly simulation environment for furniture assembly. We\nformulate the part assembly task as a concrete reinforcement learning problem\nand propose a pipeline for robots to learn to assemble a diverse set of chairs.\nExperiments show that when testing with unseen chairs, our approach achieves a\nsuccess rate of 74.5% under the object-centric setting and 50.0% under the full\nsetting. We adopt an RRT-Connect algorithm as the baseline, which only achieves\na success rate of 18.8% after a significantly longer computation time.\nSupplemental materials and videos are available on our project webpage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingxin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Lin Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elastic-Link for Binarized Neural Network. (arXiv:2112.10149v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10149","description":"<p>Recent work has shown that Binarized Neural Networks (BNNs) are able to\ngreatly reduce computational costs and memory footprints, facilitating model\ndeployment on resource-constrained devices. However, in comparison to their\nfull-precision counterparts, BNNs suffer from severe accuracy degradation.\nResearch aiming to reduce this accuracy gap has thus far largely focused on\nspecific network architectures with few or no 1x1 convolutional layers, for\nwhich standard binarization methods do not work well. Because 1x1 convolutions\nare common in the design of modern architectures (e.g. GoogleNet, ResNet,\nDenseNet), it is crucial to develop a method to binarize them effectively for\nBNNs to be more widely adopted. In this work, we propose an \"Elastic-Link\" (EL)\nmodule to enrich information flow within a BNN by adaptively adding real-valued\ninput features to the subsequent convolutional output features. The proposed EL\nmodule is easily implemented and can be used in conjunction with other methods\nfor BNNs. We demonstrate that adding EL to BNNs produces a significant\nimprovement on the challenging large-scale ImageNet dataset. For example, we\nraise the top-1 accuracy of binarized ResNet26 from 57.9% to 64.0%. EL also\naids convergence in the training of binarized MobileNet, for which a top-1\naccuracy of 56.4% is achieved. Finally, with the integration of ReActNet, it\nyields a new state-of-the-art result of 71.9% top-1 accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziheng_W/0/1/0/all/0/1\">Wu Ziheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1\">Vince Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhilin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Mengze Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology Preserving Local Road Network Estimation from Single Onboard Camera Image. (arXiv:2112.10155v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10155","description":"<p>Knowledge of the road network topology is crucial for autonomous planning and\nnavigation. Yet, recovering such topology from a single image has only been\nexplored in part. Furthermore, it needs to refer to the ground plane, where\nalso the driving actions are taken. This paper aims at extracting the local\nroad network topology, directly in the bird's-eye-view (BEV), all in a complex\nurban setting. The only input consists of a single onboard, forward looking\ncamera image. We represent the road topology using a set of directed lane\ncurves and their interactions, which are captured using their intersection\npoints. To better capture topology, we introduce the concept of \\emph{minimal\ncycles} and their covers. A minimal cycle is the smallest cycle formed by the\ndirected curve segments (between two intersections). The cover is a set of\ncurves whose segments are involved in forming a minimal cycle. We first show\nthat the covers suffice to uniquely represent the road topology. The covers are\nthen used to supervise deep neural networks, along with the lane curve\nsupervision. These learn to predict the road topology from a single input\nimage. The results on the NuScenes and Argoverse benchmarks are significantly\nbetter than those obtained with baselines. Our source code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Can_Y/0/1/0/all/0/1\">Yigit Baran Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Face-Based Age Estimation with Attention-Based Dynamic Patch Fusion. (arXiv:2112.10167v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10167","description":"<p>With the increasing popularity of convolutional neural networks (CNNs),\nrecent works on face-based age estimation employ these networks as the\nbackbone. However, state-of-the-art CNN-based methods treat each facial region\nequally, thus entirely ignoring the importance of some facial patches that may\ncontain rich age-specific information. In this paper, we propose a face-based\nage estimation framework, called Attention-based Dynamic Patch Fusion (ADPF).\nIn ADPF, two separate CNNs are implemented, namely the AttentionNet and the\nFusionNet. The AttentionNet dynamically locates and ranks age-specific patches\nby employing a novel Ranking-guided Multi-Head Hybrid Attention (RMHHA)\nmechanism. The FusionNet uses the discovered patches along with the facial\nimage to predict the age of the subject. Since the proposed RMHHA mechanism\nranks the discovered patches based on their importance, the length of the\nlearning path of each patch in the FusionNet is proportional to the amount of\ninformation it carries (the longer, the more important). ADPF also introduces a\nnovel diversity loss to guide the training of the AttentionNet and reduce the\noverlap among patches so that the diverse and important patches are discovered.\nThrough extensive experiments, we show that our proposed framework outperforms\nstate-of-the-art methods on several age estimation benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang-Tsun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Efficient Transformer and Image Pre-training for Low-level Vision. (arXiv:2112.10175v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10175","description":"<p>Pre-training has marked numerous state of the arts in high-level computer\nvision, but few attempts have ever been made to investigate how pre-training\nacts in image processing systems. In this paper, we present an in-depth study\nof image pre-training. To conduct this study on solid ground with practical\nvalue in mind, we first propose a generic, cost-effective Transformer-based\nframework for image processing. It yields highly competitive performance across\na range of low-level tasks, though under constrained parameters and\ncomputational complexity. Then, based on this framework, we design a whole set\nof principled evaluation tools to seriously and comprehensively diagnose image\npre-training in different tasks, and uncover its effects on internal network\nrepresentations. We find pre-training plays strikingly different roles in\nlow-level tasks. For example, pre-training introduces more local information to\nhigher layers in super-resolution (SR), yielding significant performance gains,\nwhile pre-training hardly affects internal feature representations in\ndenoising, resulting in a little gain. Further, we explore different methods of\npre-training, revealing that multi-task pre-training is more effective and\ndata-efficient. All codes and models will be released at\nhttps://github.com/fenglinglwb/EDT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiangbo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10184","description":"<p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and\nlocalize lung nodules with chest radiographs(CXRs). We expected it to enhance\nthe efficiency of interpreting CXRs and reduce the possibilities of delayed\ndiagnosis of lung cancer.\n</p>\n<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an\nopen-source medical image dataset, as our training and validation data. A\nnumber of CXRs from the Ministry of Health and Welfare(MOHW) database served as\nour test data. We built a segmentation model to identify lung areas from CXRs,\nand sliced them into 16 patches. Physicians labeled the CXRs by clicking the\npatches. These labeled patches were then used to train and fine-tune a deep\nneural network(DNN) model, classifying the patches as positive or negative.\nFinally, we test the DNN model with the lung patches of CXRs from MOHW.\n</p>\n<p>RESULTS: Our segmentation model identified the lung regions well from the\nwhole CXR. The Intersection over Union(IoU) between the ground truth and the\nsegmentation result was 0.9228. In addition, our DNN model achieved a\nsensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125\ncases. For the other 27 difficult cases, the sensitivity was 0.54, specificity\n0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity\nof 0.79, and AUROC 0.837.\n</p>\n<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art\nalgorithms in the sensitivity and specificity of localizing lung nodules from\nCXRs. Notably, our workflow provides an efficient way for specialists to label\nthe data, which is valuable for relevant researches because of the relative\nrarity of labeled medical image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tai_Y/0/1/0/all/0/1\">Yang Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnweaveNet: Unweaving Activity Stories. (arXiv:2112.10194v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10194","description":"<p>Our lives can be seen as a complex weaving of activities; we switch from one\nactivity to another, to maximise our achievements or in reaction to demands\nplaced upon us. Observing a video of unscripted daily activities, we parse the\nvideo into its constituent activity threads through a process we call\nunweaving. To accomplish this, we introduce a video representation explicitly\ncapturing activity threads called a thread bank, along with a neural controller\ncapable of detecting goal changes and resuming of past activities, together\nforming UnweaveNet. We train and evaluate UnweaveNet on sequences from the\nunscripted egocentric dataset EPIC-KITCHENS. We propose and showcase the\nefficacy of pretraining UnweaveNet in a self-supervised manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Price_W/0/1/0/all/0/1\">Will Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Learning of Multi-category 3D Pose and Shape Estimation. (arXiv:2112.10196v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10196","description":"<p>In this paper, we study the representation of the shape and pose of objects\nusing their keypoints. Therefore, we propose an end-to-end method that\nsimultaneously detects 2D keypoints from an image and lifts them to 3D. The\nproposed method learns both 2D detection and 3D lifting only from 2D keypoints\nannotations. In this regard, a novel method that explicitly disentangles the\npose and 3D shape by means of augmentation-based cyclic self-supervision is\nproposed, for the first time. In addition of being end-to-end in image to 3D\nlearning, our method also handles objects from multiple categories using a\nsingle neural network. We use a Transformer-based architecture to detect the\nkeypoints, as well as to summarize the visual context of the image. This visual\ncontext information is then used while lifting the keypoints to 3D, so as to\nallow the context-based reasoning for better performance. While lifting, our\nmethod learns a small set of basis shapes and their sparse non-negative\ncoefficients to represent the 3D shape in canonical frame. Our method can\nhandle occlusions as well as wide variety of object classes. Our experiments on\nthree benchmarks demonstrate that our method performs better than the\nstate-of-the-art. Our source code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Can_Y/0/1/0/all/0/1\">Yigit Baran Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1\">Danda Pani Paudel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HVTR: Hybrid Volumetric-Textural Rendering for Human Avatars. (arXiv:2112.10203v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10203","description":"<p>We propose a novel neural rendering pipeline, Hybrid Volumetric-Textural\nRendering (HVTR), which synthesizes virtual human avatars from arbitrary poses\nefficiently and at high quality. First, we learn to encode articulated human\nmotions on a dense UV manifold of the human body surface. To handle complicated\nmotions (e.g., self-occlusions), we then leverage the encoded information on\nthe UV manifold to construct a 3D volumetric representation based on a dynamic\npose-conditioned neural radiance field. While this allows us to represent 3D\ngeometry with changing topology, volumetric rendering is computationally heavy.\nHence we employ only a rough volumetric representation using a pose-conditioned\ndownsampled neural radiance field (PD-NeRF), which we can render efficiently at\nlow resolutions. In addition, we learn 2D textural features that are fused with\nrendered volumetric features in image space. The key advantage of our approach\nis that we can then convert the fused features into a high resolution,\nhigh-quality avatar by a fast GAN-based textural renderer. We demonstrate that\nhybrid rendering enables HVTR to handle complicated motions, render\nhigh-quality avatars under user-controlled poses/shapes and even loose\nclothing, and most importantly, be fast at inference time. Our experimental\nresults also demonstrate state-of-the-art quantitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zerong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1\">Matthias Zwicker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPU optimization of the 3D Scale-invariant Feature Transform Algorithm and a Novel BRIEF-inspired 3D Fast Descriptor. (arXiv:2112.10258v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10258","description":"<p>This work details a highly efficient implementation of the 3D scale-invariant\nfeature transform (SIFT) algorithm, for the purpose of machine learning from\nlarge sets of volumetric medical image data. The primary operations of the 3D\nSIFT code are implemented on a graphics processing unit (GPU), including\nconvolution, sub-sampling, and 4D peak detection from scale-space pyramids. The\nperformance improvements are quantified in keypoint detection and\nimage-to-image matching experiments, using 3D MRI human brain volumes of\ndifferent people. Computationally efficient 3D keypoint descriptors are\nproposed based on the Binary Robust Independent Elementary Feature (BRIEF)\ncode, including a novel descriptor we call Ranked Robust Independent Elementary\nFeatures (RRIEF), and compared to the original 3D SIFT-Rank\nmethod\\citep{toews2013efficient}. The GPU implementation affords a speedup of\napproximately 7X beyond an optimised CPU implementation, where computation time\nis reduced from 1.4 seconds to 0.2 seconds for 3D volumes of size (145, 174,\n145) voxels with approximately 3000 keypoints. Notable speedups include the\nconvolution operation (20X), 4D peak detection (3X), sub-sampling (3X), and\ndifference-of-Gaussian pyramid construction (2X). Efficient descriptors offer a\nspeedup of 2X and a memory savings of 6X compared to standard SIFT-Rank\ndescriptors, at a cost of reduced numbers of keypoint correspondences,\nrevealing a trade-off between computational efficiency and algorithmic\nperformance. The speedups gained by our implementation will allow for a more\nefficient analysis on larger data sets. Our optimized GPU implementation of the\n3D SIFT-Rank extractor is available at\nhttps://github.com/CarluerJB/3D_SIFT_CUDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carluer_J/0/1/0/all/0/1\">Jean-Baptiste Carluer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauvin_L/0/1/0/all/0/1\">Laurent Chauvin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William M. Wells III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_I/0/1/0/all/0/1\">Ines Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harmouche_R/0/1/0/all/0/1\">Rola Harmouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toews_M/0/1/0/all/0/1\">Matthew Toews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wiener Guided DIP for Unsupervised Blind Image Deconvolution. (arXiv:2112.10271v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10271","description":"<p>Blind deconvolution is an ill-posed problem arising in various fields ranging\nfrom microscopy to astronomy. The ill-posed nature of the problem requires\nadequate priors to arrive to a desirable solution. Recently, it has been shown\nthat deep learning architectures can serve as an image generation prior during\nunsupervised blind deconvolution optimization, however often exhibiting a\nperformance fluctuation even on a single image. We propose to use\nWiener-deconvolution to guide the image generator during optimization by\nproviding it a sharpened version of the blurry image using an auxiliary kernel\nestimate starting from a Gaussian. We observe that the high-frequency artifacts\nof deconvolution are reproduced with a delay compared to low-frequency\nfeatures. In addition, the image generator reproduces low-frequency features of\nthe deconvolved image faster than that of a blurry image. We embed the\ncomputational process in a constrained optimization framework and show that the\nproposed method yields higher stability and performance across multiple\ndatasets. In addition, we provide the code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bredell_G/0/1/0/all/0/1\">Gustav Bredell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1\">Ertunc Erdil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_B/0/1/0/all/0/1\">Bruno Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel Multi-Scale Networks with Deep Supervision for Hand Keypoint Detection. (arXiv:2112.10275v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10275","description":"<p>Keypoint detection plays an important role in a wide range of applications.\nHowever, predicting keypoints of small objects such as human hands is a\nchallenging problem. Recent works fuse feature maps of deep Convolutional\nNeural Networks (CNNs), either via multi-level feature integration or\nmulti-resolution aggregation. Despite achieving some success, the feature\nfusion approaches increase the complexity and the opacity of CNNs. To address\nthis issue, we propose a novel CNN model named Multi-Scale Deep Supervision\nNetwork (P-MSDSNet) that learns feature maps at different scales with deep\nsupervisions to produce attention maps for adaptive feature propagation from\nlayers to layers. P-MSDSNet has a multi-stage architecture which makes it\nscalable while its deep supervision with spatial attention improves\ntransparency to the feature learning at each stage. We show that P-MSDSNet\noutperforms the state-of-the-art approaches on benchmark datasets while\nrequiring fewer number of parameters. We also show the application of P-MSDSNet\nto quantify finger tapping hand movements in a neuroscience study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Renjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawler_K/0/1/0/all/0/1\">Katherine Lawler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alty_J/0/1/0/all/0/1\">Jane Alty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Driver Drowsiness Detection Using Ensemble Convolutional Neural Networks on YawDD. (arXiv:2112.10298v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10298","description":"<p>Driver drowsiness detection using videos/images is one of the most essential\nareas in today's time for driver safety. The development of deep learning\ntechniques, notably Convolutional Neural Networks (CNN), applied in computer\nvision applications such as drowsiness detection, has shown promising results\ndue to the tremendous increase in technology in the recent few decades. Eyes\nthat are closed or blinking excessively, yawning, nodding, and occlusion are\nall key aspects of drowsiness. In this work, we have applied four different\nConvolutional Neural Network (CNN) techniques on the YawDD dataset to detect\nand examine the extent of drowsiness depending on the yawning frequency with\nspecific pose and occlusion variation. Preliminary computational results show\nthat our proposed Ensemble Convolutional Neural Network (ECNN) outperformed the\ntraditional CNN-based approach by achieving an F1 score of 0.935, whereas the\nother three CNN, such as CNN1, CNN2, and CNN3 approaches gained 0.92, 0.90, and\n0.912 F1 scores, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salman_R/0/1/0/all/0/1\">Rais Mohammad Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1\">Mahbubur Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rupal Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahsan_M/0/1/0/all/0/1\">Md Manjurul Ahsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddique_Z/0/1/0/all/0/1\">Zahed Siddique</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-based gait recognition using graph network on very large population database. (arXiv:2112.10305v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10305","description":"<p>At present, the existing gait recognition systems are focusing on developing\nmethods to extract robust gait feature from silhouette images and they indeed\nachieved great success. However, gait can be sensitive to appearance features\nsuch as clothing and carried items. Compared with appearance-based method,\nmodel-based gait recognition is promising due to the robustness against these\nvariations. In recent years, with the development of human pose estimation, the\ndifficulty of model-based gait recognition methods has been mitigated. In this\npaper, to resist the increase of subjects and views variation, local features\nare built and a siamese network is proposed to maximize the distance of samples\nfrom the same subject. We leverage recent advances in action recognition to\nembed human pose sequence to a vector and introduce Spatial-Temporal Graph\nConvolution Blocks (STGCB) which has been commonly used in action recognition\nfor gait recognition. Experiments on the very large population dataset named\nOUMVLP-Pose and the popular dataset, CASIA-B, show that our method archives\nsome state-of-the-art (SOTA) performances in model-based gait recognition. The\ncode and models of our method are available at\nhttps://github.com/timelessnaive/Gait-for-Large-Dataset after being accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin lesion segmentation and classification using deep learning and handcrafted features. (arXiv:2112.10307v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10307","description":"<p>Accurate diagnostics of a skin lesion is a critical task in classification\ndermoscopic images. In this research, we form a new type of image features,\ncalled hybrid features, which has stronger discrimination ability than single\nmethod features. This study involves a new technique where we inject the\nhandcrafted features or feature transfer into the fully connected layer of\nConvolutional Neural Network (CNN) model during the training process. Based on\nour literature review until now, no study has examined or investigated the\nimpact on classification performance by injecting the handcrafted features into\nthe CNN model during the training process. In addition, we also investigated\nthe impact of segmentation mask and its effect on the overall classification\nperformance. Our model achieves an 92.3% balanced multiclass accuracy, which is\n6.8% better than the typical single method classifier architecture for deep\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ali_R/0/1/0/all/0/1\">Redha Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ragb_H/0/1/0/all/0/1\">Hussin K. Ragb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Attention Network with Dense Field Estimation for Face Completion. (arXiv:2112.10310v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10310","description":"<p>Most modern face completion approaches adopt an autoencoder or its variants\nto restore missing regions in face images. Encoders are often utilized to learn\npowerful representations that play an important role in meeting the challenges\nof sophisticated learning tasks. Specifically, various kinds of masks are often\npresented in face images in the wild, forming complex patterns, especially in\nthis hard period of COVID-19. It's difficult for encoders to capture such\npowerful representations under this complex situation. To address this\nchallenge, we propose a self-supervised Siamese inference network to improve\nthe generalization and robustness of encoders. It can encode contextual\nsemantics from full-resolution images and obtain more discriminative\nrepresentations. To deal with geometric variations of face images, a dense\ncorrespondence field is integrated into the network. We further propose a\nmulti-scale decoder with a novel dual attention fusion module (DAF), which can\ncombine the restored and known regions in an adaptive manner. This multi-scale\narchitecture is beneficial for the decoder to utilize discriminative\nrepresentations learned from encoders into images. Extensive experiments\nclearly demonstrate that the proposed approach not only achieves more appealing\nresults compared with state-of-the-art methods but also improves the\nperformance of masked face recognition dramatically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaoqiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Huaibo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Gengyun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zhenhua Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaolin Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Product Re-identification System in Fully Automated Defect Detection. (arXiv:2112.10324v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10324","description":"<p>In this work, we introduce a method and present an improved neural work to\nperform product re-identification, which is an essential core function of a\nfully automated product defect detection system. Our method is based on feature\ndistance. It is the combination of feature extraction neural networks, such as\nVGG16, AlexNet, with an image search engine - Vearch. The dataset that we used\nto develop product re-identification systems is a water-bottle dataset that\nconsists of 400 images of 18 types of water bottles. This is a small dataset,\nwhich was the biggest challenge of our work. However, the combination of neural\nnetworks with Vearch shows potential to tackle the product re-identification\nproblems. Especially, our new neural network - AlphaAlexNet that a neural\nnetwork was improved based on AlexNet could improve the production\nidentification accuracy by four percent. This indicates that an ideal\nproduction identification accuracy could be achieved when efficient feature\nextraction methods could be introduced and redesigned for image feature\nextractions of nearly identical products. In order to solve the biggest\nchallenges caused by the small size of the dataset and the difficult nature of\nidentifying productions that have little differences from each other. In our\nfuture work, we propose a new roadmap to tackle nearly-identical production\nidentifications: to introduce or develop new algorithms that need very few\nimages to train themselves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenggui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Li Bin Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Cross-view Mutual Distillation for Self-supervised Medical CT Synthesis. (arXiv:2112.10325v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10325","description":"<p>Due to the constraints of the imaging device and high cost in operation time,\ncomputer tomography (CT) scans are usually acquired with low intra-slice\nresolution. Improving the intra-slice resolution is beneficial to the disease\ndiagnosis for both human experts and computer-aided systems. To this end, this\npaper builds a novel medical slice synthesis to increase the between-slice\nresolution. Considering that the ground-truth intermediate medical slices are\nalways absent in clinical practice, we introduce the incremental cross-view\nmutual distillation strategy to accomplish this task in the self-supervised\nlearning manner. Specifically, we model this problem from three different\nviews: slice-wise interpolation from axial view and pixel-wise interpolation\nfrom coronal and sagittal views. Under this circumstance, the models learned\nfrom different views can distill valuable knowledge to guide the learning\nprocesses of each other. We can repeat this process to make the models\nsynthesize intermediate slice data with increasing inter-slice resolution. To\ndemonstrate the effectiveness of the proposed approach, we conduct\ncomprehensive experiments on a large-scale CT dataset. Quantitative and\nqualitative comparison results show that our method outperforms\nstate-of-the-art algorithms by clear margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DMS-GCN: Dynamic Mutiscale Spatiotemporal Graph Convolutional Networks for Human Motion Prediction. (arXiv:2112.10365v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10365","description":"<p>Human motion prediction is an important and challenging task in many computer\nvision application domains. Recent work concentrates on utilizing the timing\nprocessing ability of recurrent neural networks (RNNs) to achieve smooth and\nreliable results in short-term prediction. However, as evidenced by previous\nwork, RNNs suffer from errors accumulation, leading to unreliable results. In\nthis paper, we propose a simple feed-forward deep neural network for motion\nprediction, which takes into account temporal smoothness and spatial\ndependencies between human body joints. We design a Multi-scale Spatio-temporal\ngraph convolutional networks (GCNs) to implicitly establish the Spatio-temporal\ndependence in the process of human movement, where different scales fused\ndynamically during training. The entire model is suitable for all actions and\nfollows a framework of encoder-decoder. The encoder consists of temporal GCNs\nto capture motion features between frames and semi-autonomous learned spatial\nGCNs to extract spatial structure among joint trajectories. The decoder uses\ntemporal convolution networks (TCNs) to maintain its extensive ability.\nExtensive experiments show that our approach outperforms SOTA methods on the\ndatasets of Human3.6M and CMU Mocap while only requiring much lesser\nparameters. Code will be available at https://github.com/yzg9353/DMSGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zigeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1\">Di-Hua Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yuanqing Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Co-supervision and Attention Fusion Strategy for Automatic COVID-19 Lung Infection Segmentation on CT Images. (arXiv:2112.10368v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10368","description":"<p>Due to the irregular shapes,various sizes and indistinguishable boundaries\nbetween the normal and infected tissues, it is still a challenging task to\naccurately segment the infected lesions of COVID-19 on CT images. In this\npaper, a novel segmentation scheme is proposed for the infections of COVID-19\nby enhancing supervised information and fusing multi-scale feature maps of\ndifferent levels based on the encoder-decoder architecture. To this end, a deep\ncollaborative supervision (Co-supervision) scheme is proposed to guide the\nnetwork learning the features of edges and semantics. More specifically, an\nEdge Supervised Module (ESM) is firstly designed to highlight low-level\nboundary features by incorporating the edge supervised information into the\ninitial stage of down-sampling. Meanwhile, an Auxiliary Semantic Supervised\nModule (ASSM) is proposed to strengthen high-level semantic information by\nintegrating mask supervised information into the later stage. Then an Attention\nFusion Module (AFM) is developed to fuse multiple scale feature maps of\ndifferent levels by using an attention mechanism to reduce the semantic gaps\nbetween high-level and low-level feature maps. Finally, the effectiveness of\nthe proposed scheme is demonstrated on four various COVID-19 CT datasets. The\nresults show that the proposed three modules are all promising. Based on the\nbaseline (ResUnet), using ESM, ASSM, or AFM alone can respectively increase\nDice metric by 1.12\\%, 1.95\\%,1.63\\% in our dataset, while the integration by\nincorporating three models together can rise 3.97\\%. Compared with the existing\napproaches in various datasets, the proposed method can obtain better\nsegmentation performance in some main metrics, and can achieve the best\ngeneralization and comprehensive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haigen Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Leizhao Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guan_Q/0/1/0/all/0/1\">Qiu Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Q/0/1/0/all/0/1\">Qianwei Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ruan_S/0/1/0/all/0/1\">Su Ruan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Adversarially Learned Inference with Factorized Discriminators. (arXiv:2112.10384v1 [cs.LG])","link":"http://arxiv.org/abs/2112.10384","description":"<p>Learning from multimodal data is an important research topic in machine\nlearning, which has the potential to obtain better representations. In this\nwork, we propose a novel approach to generative modeling of multimodal data\nbased on generative adversarial networks. To learn a coherent multimodal\ngenerative model, we show that it is necessary to align different encoder\ndistributions with the joint decoder distribution simultaneously. To this end,\nwe construct a specific form of the discriminator to enable our model to\nutilize data efficiently, which can be trained constrastively. By taking\nadvantage of contrastive learning through factorizing the discriminator, we\ntrain our model on unimodal data. We have conducted experiments on the\nbenchmark datasets, whose promising results show that our proposed approach\noutperforms the-state-of-the-art methods on a variety of metrics. The source\ncode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenxue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation and Comparison of Deep Learning Methods for Pavement Crack Identification with Visual Images. (arXiv:2112.10390v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10390","description":"<p>Compared with contact detection techniques, pavement crack identification\nwith visual images via deep learning algorithms has the advantages of not being\nlimited by the material of object to be detected, fast speed and low cost. The\nfundamental frameworks and typical model architectures of transfer learning\n(TL), encoder-decoder (ED), generative adversarial networks (GAN), and their\ncommon modules were first reviewed, and then the evolution of convolutional\nneural network (CNN) backbone models and GAN models were summarized. The crack\nclassification, segmentation performance, and effect were tested on the\nSDNET2018 and CFD public data sets. In the aspect of patch sample\nclassification, the fine-tuned TL models can be equivalent to or even slightly\nbetter than the ED models in accuracy, and the predicting time is faster; In\nthe aspect of accurate crack location, both ED and GAN algorithms can achieve\npixel-level segmentation and is expected to be detected in real time on low\ncomputing power platform. Furthermore, a weakly supervised learning framework\nof combined TL-SSGAN and its performance enhancement measures are proposed,\nwhich can maintain comparable crack identification performance with that of the\nsupervised learning, while greatly reducing the number of labeled samples\nrequired.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kai-Liang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UFPMP-Det: Toward Accurate and Efficient Object Detection on Drone Imagery. (arXiv:2112.10415v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10415","description":"<p>This paper proposes a novel approach to object detection on drone imagery,\nnamely Multi-Proxy Detection Network with Unified Foreground Packing\n(UFPMP-Det). To deal with the numerous instances of very small scales,\ndifferent from the common solution that divides the high-resolution input image\ninto quite a number of chips with low foreground ratios to perform detection on\nthem each, the Unified Foreground Packing (UFP) module is designed, where the\nsub-regions given by a coarse detector are initially merged through clustering\nto suppress background and the resulting ones are subsequently packed into a\nmosaic for a single inference, thus significantly reducing overall time cost.\nFurthermore, to address the more serious confusion between inter-class\nsimilarities and intra-class variations of instances, which deteriorates\ndetection performance but is rarely discussed, the Multi-Proxy Detection\nNetwork (MP-Det) is presented to model object distributions in a fine-grained\nmanner by employing multiple proxy learning, and the proxies are enforced to be\ndiverse by minimizing a Bag-of-Instance-Words (BoIW) guided optimal transport\nloss. By such means, UFPMP-Det largely promotes both the detection accuracy and\nefficiency. Extensive experiments are carried out on the widely used VisDrone\nand UAVDT datasets, and UFPMP-Det reports new state-of-the-art scores at a much\nhigher speed, highlighting its advantages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yecheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Label Noise for Image Retrieval by Selecting Interactions. (arXiv:2112.10453v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10453","description":"<p>Learning with noisy labels is an active research area for image\nclassification. However, the effect of noisy labels on image retrieval has been\nless studied. In this work, we propose a noise-resistant method for image\nretrieval named Teacher-based Selection of Interactions, T-SINT, which\nidentifies noisy interactions, ie. elements in the distance matrix, and selects\ncorrect positive and negative interactions to be considered in the retrieval\nloss by using a teacher-based training setup which contributes to the\nstability. As a result, it consistently outperforms state-of-the-art methods on\nhigh noise rates across benchmark datasets with synthetic noise and more\nrealistic noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimi_S/0/1/0/all/0/1\">Sarah Ibrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sors_A/0/1/0/all/0/1\">Arnaud Sors</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1\">Rafael Sampaio de Rezende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Animation with Keypoint Mask. (arXiv:2112.10457v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10457","description":"<p>Motion transfer is the task of synthesizing future video frames of a single\nsource image according to the motion from a given driving video. This task is\nchallenging due to the complexity of motion representation and the unknown\nrelations between the driving video and the source image. Despite this\ndifficulty, this problem attracted great interests from researches at the\nrecent years, with gradual improvements. The problem can be thought as\ndecoupling of motion and appearance, which is often solved by extracting the\nmotion from keypoint movement. We chose to tackle the generic, unsupervised\nsetting, where we need to apply animation to any arbitrary object, without any\ndomain specific model for the structure of the input. In this work, we extract\nthe structure from a keypoint heatmap, without an explicit motion\nrepresentation. Then, the structures from the image and the video are extracted\nto warp the image according to the video, by a deep generator.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toledano_O/0/1/0/all/0/1\">Or Toledano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marmor_Y/0/1/0/all/0/1\">Yanir Marmor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gertz_D/0/1/0/all/0/1\">Dov Gertz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reciprocal Normalization for Domain Adaptation. (arXiv:2112.10474v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10474","description":"<p>Batch normalization (BN) is widely used in modern deep neural networks, which\nhas been shown to represent the domain-related knowledge, and thus is\nineffective for cross-domain tasks like unsupervised domain adaptation (UDA).\nExisting BN variant methods aggregate source and target domain knowledge in the\nsame channel in normalization module. However, the misalignment between the\nfeatures of corresponding channels across domains often leads to a sub-optimal\ntransferability. In this paper, we exploit the cross-domain relation and\npropose a novel normalization method, Reciprocal Normalization (RN).\nSpecifically, RN first presents a Reciprocal Compensation (RC) module to\nacquire the compensatory for each channel in both domains based on the\ncross-domain channel-wise correlation. Then RN develops a Reciprocal\nAggregation (RA) module to adaptively aggregate the feature with its\ncross-domain compensatory components. As an alternative to BN, RN is more\nsuitable for UDA problems and can be easily integrated into popular domain\nadaptation methods. Experiments show that the proposed RN outperforms existing\nnormalization counterparts by a large margin and helps state-of-the-art\nadaptation approaches achieve better results. The source code is available on\nhttps://github.com/Openning07/reciprocal-normalization-for-DA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiyong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Weiming Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dengwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xing Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"a novel attention-based network for fast salient object detection. (arXiv:2112.10481v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10481","description":"<p>In the current salient object detection network, the most popular method is\nusing U-shape structure. However, the massive number of parameters leads to\nmore consumption of computing and storage resources which are not feasible to\ndeploy on the limited memory device. Some others shallow layer network will not\nmaintain the same accuracy compared with U-shape structure and the deep network\nstructure with more parameters will not converge to a global minimum loss with\ngreat speed. To overcome all of these disadvantages, we proposed a new deep\nconvolution network architecture with three contributions: (1) using smaller\nconvolution neural networks (CNNs) to compress the model in our improved\nsalient object features compression and reinforcement extraction module\n(ISFCREM) to reduce parameters of the model. (2) introducing channel attention\nmechanism in ISFCREM to weigh different channels for improving the ability of\nfeature representation. (3) applying a new optimizer to accumulate the\nlong-term gradient information during training to adaptively tune the learning\nrate. The results demonstrate that the proposed method can compress the model\nto 1/3 of the original size nearly without losing the accuracy and converging\nfaster and more smoothly on six widely used datasets of salient object\ndetection compared with the others models. Our code is published in\nhttps://gitee.com/binzhangbinzhangbin/code-a-novel-attention-based-network-for-fast-salient-object-detection.git\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaojing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Ming Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScanQA: 3D Question Answering for Spatial Scene Understanding. (arXiv:2112.10482v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10482","description":"<p>We propose a new 3D spatial understanding task of 3D Question Answering\n(3D-QA). In the 3D-QA task, models receive visual information from the entire\n3D scene of the rich RGB-D indoor scan and answer the given textual questions\nabout the 3D scene. Unlike the 2D-question answering of VQA, the conventional\n2D-QA models suffer from problems with spatial understanding of object\nalignment and directions and fail the object localization from the textual\nquestions in 3D-QA. We propose a baseline model for 3D-QA, named ScanQA model,\nwhere the model learns a fused descriptor from 3D object proposals and encoded\nsentence embeddings. This learned descriptor correlates the language\nexpressions with the underlying geometric features of the 3D scan and\nfacilitates the regression of 3D bounding boxes to determine described objects\nin textual questions. We collected human-edited question-answer pairs with\nfree-form answers that are grounded to 3D objects in each 3D scene. Our new\nScanQA dataset contains over 41K question-answer pairs from the 800 indoor\nscenes drawn from the ScanNet dataset. To the best of our knowledge, ScanQA is\nthe first large-scale effort to perform object-grounded question-answering in\n3D environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azuma_D/0/1/0/all/0/1\">Daichi Azuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyanishi_T/0/1/0/all/0/1\">Taiki Miyanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurita_S/0/1/0/all/0/1\">Shuhei Kurita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanabe_M/0/1/0/all/0/1\">Motoki Kawanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusion and Orthogonal Projection for Improved Face-Voice Association. (arXiv:2112.10483v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10483","description":"<p>We study the problem of learning association between face and voice, which is\ngaining interest in the computer vision community lately. Prior works adopt\npairwise or triplet loss formulations to learn an embedding space amenable for\nassociated matching and verification tasks. Albeit showing some progress, such\nloss formulations are, however, restrictive due to dependency on\ndistance-dependent margin parameter, poor run-time training complexity, and\nreliance on carefully crafted negative mining procedures. In this work, we\nhypothesize that enriched feature representation coupled with an effective yet\nefficient supervision is necessary in realizing a discriminative joint\nembedding space for improved face-voice association. To this end, we propose a\nlight-weight, plug-and-play mechanism that exploits the complementary cues in\nboth modalities to form enriched fused embeddings and clusters them based on\ntheir identity labels via orthogonality constraints. We coin our proposed\nmechanism as fusion and orthogonal projection (FOP) and instantiate in a\ntwo-stream pipeline. The overall resulting framework is evaluated on a\nlarge-scale VoxCeleb dataset with a multitude of tasks, including cross-modal\nverification and matching. Results show that our method performs favourably\nagainst the current state-of-the-art methods and our proposed supervision\nformulation is more effective and efficient than the ones employed by the\ncontemporary methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_M/0/1/0/all/0/1\">Muhammad Saad Saeed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Muhammad Haris Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nawaz_S/0/1/0/all/0/1\">Shah Nawaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yousaf_M/0/1/0/all/0/1\">Muhammad Haroon Yousaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-Net: Learning to Reduce Scale Differences for Large-Scale Invariant Image Matching. (arXiv:2112.10485v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10485","description":"<p>Most image matching methods perform poorly when encountering large scale\nchanges in images. To solve this problem, firstly, we propose a\nscale-difference-aware image matching method (SDAIM) that reduces image scale\ndifferences before local feature extraction, via resizing both images of an\nimage pair according to an estimated scale ratio. Secondly, in order to\naccurately estimate the scale ratio, we propose a\ncovisibility-attention-reinforced matching module (CVARM) and then design a\nnovel neural network, termed as Scale-Net, based on CVARM. The proposed CVARM\ncan lay more stress on covisible areas within the image pair and suppress the\ndistraction from those areas visible in only one image. Quantitative and\nqualitative experiments confirm that the proposed Scale-Net has higher scale\nratio estimation accuracy and much better generalization ability compared with\nall the existing scale ratio estimation methods. Further experiments on image\nmatching and relative pose estimation tasks demonstrate that our SDAIM and\nScale-Net are able to greatly boost the performance of representative local\nfeatures and state-of-the-art local feature matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yujie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yihong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Recognition as Classification of Visual Properties. (arXiv:2112.10531v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10531","description":"<p>We base our work on the teleosemantic modelling of concepts as abilities\nimplementing the distinct functions of recognition and classification.\nAccordingly, we model two types of concepts - substance concepts suited for\nobject recognition exploiting visual properties, and classification concepts\nsuited for classification of substance concepts exploiting linguistically\ngrounded properties. The goal in this paper is to demonstrate that object\nrecognition can be construed as classification of visual properties, as\ndistinct from work in mainstream computer vision. Towards that, we present an\nobject recognition process based on Ranganathan's four-phased faceted knowledge\norganization process, grounded in the teleosemantic distinctions of substance\nconcept and classification concept. We also briefly introduce the ongoing\nproject MultiMedia UKC, whose aim is to build an object recognition resource\nfollowing our proposed process\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1\">Mayukh Bagchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Neural Representation Learning for Hyperspectral Image Super-Resolution. (arXiv:2112.10541v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10541","description":"<p>Hyperspectral image (HSI) super-resolution without additional auxiliary image\nremains a constant challenge due to its high-dimensional spectral patterns,\nwhere learning an effective spatial and spectral representation is a\nfundamental issue. Recently, Implicit Neural Representations (INRs) are making\nstrides as a novel and effective representation, especially in the\nreconstruction task. Therefore, in this work, we propose a novel HSI\nreconstruction model based on INR which represents HSI by a continuous function\nmapping a spatial coordinate to its corresponding spectral radiance values. In\nparticular, as a specific implementation of INR, the parameters of parametric\nmodel are predicted by a hypernetwork that operates on feature extraction using\nconvolution network. It makes the continuous functions map the spatial\ncoordinates to pixel values in a content-aware manner. Moreover, periodic\nspatial encoding are deeply integrated with the reconstruction procedure, which\nmakes our model capable of recovering more high frequency details. To verify\nthe efficacy of our model, we conduct experiments on three HSI datasets (CAVE,\nNUS, and NTIRE2018). Experimental results show that the proposed model can\nachieve competitive reconstruction performance in comparison with the\nstate-of-the-art methods. In addition, we provide an ablation study on the\neffect of individual components of our model. We hope this paper could server\nas a potent reference for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1\">Kaiwei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Hypergraph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2112.10570v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10570","description":"<p>Graph convolutional networks (GCNs) based methods have achieved advanced\nperformance on skeleton-based action recognition task. However, the skeleton\ngraph cannot fully represent the motion information contained in skeleton data.\nIn addition, the topology of the skeleton graph in the GCN-based methods is\nmanually set according to natural connections, and it is fixed for all samples,\nwhich cannot well adapt to different situations. In this work, we propose a\nnovel dynamic hypergraph convolutional networks (DHGCN) for skeleton-based\naction recognition. DHGCN uses hypergraph to represent the skeleton structure\nto effectively exploit the motion information contained in human joints. Each\njoint in the skeleton hypergraph is dynamically assigned the corresponding\nweight according to its moving, and the hypergraph topology in our model can be\ndynamically adjusted to different samples according to the relationship between\nthe joints. Experimental results demonstrate that the performance of our model\nachieves competitive performance on three datasets: Kinetics-Skeleton 400, NTU\nRGB+D 60, and NTU RGB+D 120.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jinfeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mengli Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1\">Pei Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingliang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Greedy De-bias Learning. (arXiv:2112.10572v1 [cs.LG])","link":"http://arxiv.org/abs/2112.10572","description":"<p>Neural networks often make predictions relying on the spurious correlations\nfrom the datasets rather than the intrinsic properties of the task of interest,\nfacing sharp degradation on out-of-distribution (OOD) test data. Existing\nde-bias learning frameworks try to capture specific dataset bias by bias\nannotations, they fail to handle complicated OOD scenarios. Others implicitly\nidentify the dataset bias by the special design on the low capability biased\nmodel or the loss, but they degrade when the training and testing data are from\nthe same distribution. In this paper, we propose a General Greedy De-bias\nlearning framework (GGD), which greedily trains the biased models and the base\nmodel like gradient descent in functional space. It encourages the base model\nto focus on examples that are hard to solve with biased models, thus remaining\nrobust against spurious correlations in the test stage. GGD largely improves\nmodels' OOD generalization ability on various tasks, but sometimes\nover-estimates the bias level and degrades on the in-distribution test. We\nfurther re-analyze the ensemble process of GGD and introduce the Curriculum\nRegularization into GGD inspired by curriculum learning, which achieves a good\ntrade-off between in-distribution and out-of-distribution performance.\nExtensive experiments on image classification, adversarial question answering,\nand visual question answering demonstrate the effectiveness of our method. GGD\ncan learn a more robust base model under the settings of both task-specific\nbiased models with prior knowledge and self-ensemble biased model without prior\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-free multi-character recognition. (arXiv:2112.10587v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10587","description":"<p>The recently developed image-free sensing technique maintains the advantages\nof both the light hardware and software, which has been applied in simple\ntarget classification and motion tracking. In practical applications, however,\nthere usually exist multiple targets in the field of view, where existing\ntrials fail to produce multi-semantic information. In this letter, we report a\nnovel image-free sensing technique to tackle the multi-target recognition\nchallenge for the first time. Different from the convolutional layer stack of\nimage-free single-pixel networks, the reported CRNN network utilities the\nbidirectional LSTM architecture to predict the distribution of multiple\ncharacters simultaneously. The framework enables to capture the long-range\ndependencies, providing a high recognition accuracy of multiple characters. We\ndemonstrated the technique's effectiveness in license plate detection, which\nachieved 87.60% recognition accuracy at a 5% sampling rate with a higher than\n100 FPS refresh rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chunli Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1\">Liheng Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Optical Flow for Vehicular Perception with Low- and High-Resolution Event Cameras. (arXiv:2112.10591v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10591","description":"<p>Event cameras capture changes of illumination in the observed scene rather\nthan accumulating light to create images. Thus, they allow for applications\nunder high-speed motion and complex lighting conditions, where traditional\nframebased sensors show their limits with blur and over- or underexposed\npixels. Thanks to these unique properties, they represent nowadays an highly\nattractive sensor for ITS-related applications. Event-based optical flow (EBOF)\nhas been studied following the rise in popularity of these neuromorphic\ncameras. The recent arrival of high-definition neuromorphic sensors, however,\nchallenges the existing approaches, because of the increased resolution of the\nevents pixel array and a much higher throughput. As an answer to these points,\nwe propose an optimized framework for computing optical flow in real-time with\nboth low- and high-resolution event cameras. We formulate a novel dense\nrepresentation for the sparse events flow, in the form of the \"inverse\nexponential distance surface\". It serves as an interim frame, designed for the\nuse of proven, state-of-the-art frame-based optical flow computation methods.\nWe evaluate our approach on both low- and high-resolution driving sequences,\nand show that it often achieves better results than the current state of the\nart, while also reaching higher frame rates, 250Hz at 346 x 260 pixels and 77Hz\nat 1280 x 720 pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brebion_V/0/1/0/all/0/1\">Vincent Brebion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreau_J/0/1/0/all/0/1\">Julien Moreau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davoine_F/0/1/0/all/0/1\">Franck Davoine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeePaste -- Inpainting for Pasting. (arXiv:2112.10600v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10600","description":"<p>One of the challenges of supervised learning training is the need to procure\nan substantial amount of tagged data. A well-known method of solving this\nproblem is to use synthetic data in a copy-paste fashion, so that we cut\nobjects and paste them onto relevant backgrounds. Pasting the objects naively\nresults in artifacts that cause models to give poor results on real data. We\npresent a new method for cleanly pasting objects on different backgrounds so\nthat the dataset created gives competitive performance on real data. The main\nemphasis is on the treatment of the border of the pasted object using\ninpainting. We show state-of-the-art results both on instance detection and\nforeground segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Werman_L/0/1/0/all/0/1\">Levi Kassel Michael Werman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-user Oriented Live Free-viewpoint Video Streaming System Based On View Interpolation. (arXiv:2112.10603v1 [cs.MM])","link":"http://arxiv.org/abs/2112.10603","description":"<p>As an important application form of immersive multimedia services,\nfree-viewpoint video(FVV) enables users with great immersive experience by\nstrong interaction. However, the computational complexity of virtual view\nsynthesis algorithms poses a significant challenge to the real-time performance\nof an FVV system. Furthermore, the individuality of user interaction makes it\ndifficult to serve multiple users simultaneously for a system with conventional\narchitecture. In this paper, we novelly introduce a CNN-based view\ninterpolation algorithm to synthesis dense virtual views in real time. Based on\nthis, we also build an end-to-end live free-viewpoint system with a multi-user\noriented streaming strategy. Our system can utilize a single edge server to\nserve multiple users at the same time without having to bring a large view\nsynthesis load on the client side. We analysis the whole system and show that\nour approaches give the user a pleasant immersive experience, in terms of both\nvisual quality and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jingchuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shuai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Li Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Rotation-Invariant Face Detection with Progressive Calibration Networks. (arXiv:1804.06039v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.06039","description":"<p>Rotation-invariant face detection, i.e. detecting faces with arbitrary\nrotation-in-plane (RIP) angles, is widely required in unconstrained\napplications but still remains as a challenging task, due to the large\nvariations of face appearances. Most existing methods compromise with speed or\naccuracy to handle the large RIP variations. To address this problem more\nefficiently, we propose Progressive Calibration Networks (PCN) to perform\nrotation-invariant face detection in a coarse-to-fine manner. PCN consists of\nthree stages, each of which not only distinguishes the faces from non-faces,\nbut also calibrates the RIP orientation of each face candidate to upright\nprogressively. By dividing the calibration process into several progressive\nsteps and only predicting coarse orientations in early stages, PCN can achieve\nprecise and fast calibration. By performing binary classification of face vs.\nnon-face with gradually decreasing RIP ranges, PCN can accurately detect faces\nwith full $360^{\\circ}$ RIP angles. Such designs lead to a real-time\nrotation-invariant face detector. The experiments on multi-oriented FDDB and a\nchallenging subset of WIDER FACE containing rotated faces in the wild show that\nour PCN achieves quite promising performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xuepeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Meina Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuzhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MW-GAN: Multi-Warping GAN for Caricature Generation with Multi-Style Geometric Exaggeration. (arXiv:2001.01870v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.01870","description":"<p>Given an input face photo, the goal of caricature generation is to produce\nstylized, exaggerated caricatures that share the same identity as the photo. It\nrequires simultaneous style transfer and shape exaggeration with rich\ndiversity, and meanwhile preserving the identity of the input. To address this\nchallenging problem, we propose a novel framework called Multi-Warping GAN\n(MW-GAN), including a style network and a geometric network that are designed\nto conduct style transfer and geometric exaggeration respectively. We bridge\nthe gap between the style and landmarks of an image with corresponding latent\ncode spaces by a dual way design, so as to generate caricatures with arbitrary\nstyles and geometric exaggeration, which can be specified either through random\nsampling of latent code or from a given caricature sample. Besides, we apply\nidentity preserving loss to both image space and landmark space, leading to a\ngreat improvement in quality of generated caricatures. Experiments show that\ncaricatures generated by MW-GAN have better quality than existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_H/0/1/0/all/0/1\">Haodi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jing Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-Kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Domain Adaptation with Prototype-Based Normalized Output Conditioner. (arXiv:2003.13274v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.13274","description":"<p>In this work, we attempt to address unsupervised domain adaptation by\ndevising simple and compact conditional domain adversarial training methods. We\nfirst revisit the simple concatenation conditioning strategy where features are\nconcatenated with output predictions as the input of the discriminator. We find\nthe concatenation strategy suffers from the weak conditioning strength. We\nfurther demonstrate that enlarging the norm of concatenated predictions can\neffectively energize the conditional domain alignment. Thus we improve\nconcatenation conditioning by normalizing the output predictions to have the\nsame norm of features, and term the derived method as Normalized OutpUt\ncoNditioner~(NOUN). However, conditioning on raw output predictions for domain\nalignment, NOUN suffers from inaccurate predictions of the target domain. To\nthis end, we propose to condition the cross-domain feature alignment in the\nprototype space rather than in the output space. Combining the novel\nprototype-based conditioning with NOUN, we term the enhanced method as\nPROtotype-based Normalized OutpUt coNditioner~(PRONOUN). Experiments on both\nobject recognition and semantic segmentation show that NOUN can effectively\nalign the multi-modal structures across domains and even outperform\nstate-of-the-art domain adversarial training methods. Together with\nprototype-based conditioning, PRONOUN further improves the adaptation\nperformance over NOUN on multiple object recognition benchmarks for UDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dapeng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jian Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hanshu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slimming Neural Networks using Adaptive Connectivity Scores. (arXiv:2006.12463v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.12463","description":"<p>In general, deep neural network (DNN) pruning methods fall into two\ncategories: 1) Weight-based deterministic constraints, and 2) Probabilistic\nframeworks. While each approach has its merits and limitations there are a set\nof common practical issues such as, trial-and-error to analyze sensitivity and\nhyper-parameters to prune DNNs, which plague them both. In this work, we\npropose a new single-shot, fully automated pruning algorithm called Slimming\nNeural networks using Adaptive Connectivity Scores (SNACS). Our proposed\napproach combines a probabilistic pruning framework with constraints on the\nunderlying weight matrices, via a novel connectivity measure, at multiple\nlevels to capitalize on the strengths of both approaches while solving their\ndeficiencies. In \\alg{}, we propose a fast hash-based estimator of Adaptive\nConditional Mutual Information (ACMI), that uses a weight-based scaling\ncriterion, to evaluate the connectivity between filters and prune unimportant\nones. To automatically determine the limit up to which a layer can be pruned,\nwe propose a set of operating constraints that jointly define the upper pruning\npercentage limits across all the layers in a deep network. Finally, we define a\nnovel sensitivity criterion for filters that measures the strength of their\ncontributions to the succeeding layer and highlights critical filters that need\nto be completely protected from pruning. Through our experimental validation we\nshow that SNACS is faster by over 17x the nearest comparable method and is the\nstate of the art single-shot pruning method across three standard Dataset-DNN\npruning benchmarks: CIFAR10-VGG16, CIFAR10-ResNet56 and ILSVRC2012-ResNet50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_M/0/1/0/all/0/1\">Madan Ravi Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blanchard_D/0/1/0/all/0/1\">Dawsin Blanchard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekeh_S/0/1/0/all/0/1\">Salimeh Yasaei Sekeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep primal-dual proximal network for image restoration. (arXiv:2007.00959v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.00959","description":"<p>Image restoration remains a challenging task in image processing. Numerous\nmethods tackle this problem, often solved by minimizing a non-smooth penalized\nco-log-likelihood function. Although the solution is easily interpretable with\ntheoretic guarantees, its estimation relies on an optimization process that can\ntake time. Considering the research effort in deep learning for image\nclassification and segmentation, this class of methods offers a serious\nalternative to perform image restoration but stays challenging to solve inverse\nproblems. In this work, we design a deep network, named DeepPDNet, built from\nprimal-dual proximal iterations associated with the minimization of a standard\npenalized likelihood with an analysis prior, allowing us to take advantage of\nboth worlds.\n</p>\n<p>We reformulate a specific instance of the Condat-Vu primal-dual hybrid\ngradient (PDHG) algorithm as a deep network with fixed layers. The learned\nparameters are both the PDHG algorithm step-sizes and the analysis linear\noperator involved in the penalization (including the regularization parameter).\nThese parameters are allowed to vary from a layer to another one. Two different\nlearning strategies: \"Full learning\" and \"Partial learning\" are proposed, the\nfirst one is the most efficient numerically while the second one relies on\nstandard constraints ensuring convergence in the standard PDHG iterations.\nMoreover, global and local sparse analysis prior are studied to seek a better\nfeature representation. We apply the proposed methods to image restoration on\nthe MNIST and BSD68 datasets and to single image super-resolution on the BSD100\nand SET14 datasets. Extensive results show that the proposed DeepPDNet\ndemonstrates excellent performance on the MNIST and the more complex BSD68,\nBSD100, and SET14 datasets for image restoration and single image\nsuper-resolution task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiu_M/0/1/0/all/0/1\">Mingyuan Jiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pustelnik_N/0/1/0/all/0/1\">Nelly Pustelnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoder-side Cross Resolution Synthesis for Video Compression Enhancement. (arXiv:2012.00650v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.00650","description":"<p>This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to\npursue better compression efficiency beyond the latest Versatile Video Coding\n(VVC), where we encode intra frames at original high resolution (HR), compress\ninter frames at a lower resolution (LR), and then super-resolve decoded LR\ninter frames with the help from preceding HR intra and neighboring LR inter\nframes.\n</p>\n<p>For a LR inter frame, a motion alignment and aggregation network (MAN) is\ndevised to produce temporally aggregated motion representation to best\nguarantee the temporal smoothness; Another texture compensation network (TCN)\nis utilized to generate texture representation from decoded HR intra frame for\nbetter augmenting spatial details; Finally, a similarity-driven fusion engine\nsynthesizes motion and texture representations to upscale LR inter frames for\nthe removal of compression and resolution re-sampling noises.\n</p>\n<p>We enhance the VVC using proposed CRS, showing averaged 8.76% and 11.93%\nBj{\\o}ntegaard Delta Rate (BD-Rate) gains against the latest VVC anchor in\nRandom Access (RA) and Low-delay P (LDP) settings respectively. In addition,\nexperimental comparisons to the state-of-the-art super-resolution (SR) based\nVVC enhancement methods, and ablation studies are conducted to further report\nsuperior efficiency and generalization of the proposed algorithm. All materials\nwill be made to public at https://njuvision.github.io/CRS for reproducible\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhenyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Dandan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Object Association and Pose Updating for Semantic SLAM. (arXiv:2012.11368v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.11368","description":"<p>Current pandemic has caused the medical system to operate under high load. To\nrelieve it, robots with high autonomy can be used to effectively execute\ncontactless operations in hospitals and reduce cross-infection between medical\nstaff and patients. Although semantic Simultaneous Localization and Mapping\n(SLAM) technology can improve the autonomy of robots, semantic object\nassociation is still a problem that is worthy of being studied. The key to\nsolving this problem is to correctly associate multiple object measurements of\none object landmark by using semantic information, and to refine the pose of\nobject landmark in real time. To this end, we propose a hierarchical object\nassociation strategy and a pose-refinement approach. The former one consists of\ntwo levels, i.e., a short-term object association and a global one. In the\nfirst level, we employ the multiple-object-tracking for short-term object\nassociation, through which the incorrect association among objects whose\nlocations are close and appearances are similar can be avoided. Moreover, the\nshort-term object association can provide more abundant object appearance and\nmore robust estimation of object pose for the global object association in the\nsecond level. To refine the object pose in the map, we develop an approach to\nchoose the optimal object pose from all object measurements associated with an\nobject landmark. The proposed method is comprehensively evaluated on seven\nsimulated hospital sequences1, a real hospital environment and the KITTI\ndataset. Experimental results show that our method has an obviously improvement\nin terms of robustness and accuracy for the object association and the\ntrajectory estimation in the semantic SLAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kaiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jialing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qinying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianhua Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Shape Learning for Building Extraction in VHR Remote Sensing Images. (arXiv:2102.11262v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.11262","description":"<p>Building extraction in VHR RSIs remains a challenging task due to occlusion\nand boundary ambiguity problems. Although conventional convolutional neural\nnetworks (CNNs) based methods are capable of exploiting local texture and\ncontext information, they fail to capture the shape patterns of buildings,\nwhich is a necessary constraint in the human recognition. To address this\nissue, we propose an adversarial shape learning network (ASLNet) to model the\nbuilding shape patterns that improve the accuracy of building segmentation. In\nthe proposed ASLNet, we introduce the adversarial learning strategy to\nexplicitly model the shape constraints, as well as a CNN shape regularizer to\nstrengthen the embedding of shape features. To assess the geometric accuracy of\nbuilding segmentation results, we introduced several object-based quality\nassessment metrics. Experiments on two open benchmark datasets show that the\nproposed ASLNet improves both the pixel-based accuracy and the object-based\nquality measurements by a large margin. The code is available at:\nhttps://github.com/ggsDing/ASLNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yilei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Landmarks Augmentation with Manifold-Barycentric Oversampling. (arXiv:2104.00925v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00925","description":"<p>The training of Generative Adversarial Networks (GANs) requires a large\namount of data, stimulating the development of new augmentation methods to\nalleviate the challenge. Oftentimes, these methods either fail to produce\nenough new data or expand the dataset beyond the original manifold. In this\npaper, we propose a new augmentation method that guarantees to keep the new\ndata within the original data manifold thanks to the optimal transport theory.\nThe proposed algorithm finds cliques in the nearest-neighbors graph and, at\neach sampling iteration, randomly draws one clique to compute the Wasserstein\nbarycenter with random uniform weights. These barycenters then become the new\nnatural-looking elements that one could add to the dataset. We apply this\napproach to the problem of landmarks detection and augment the available\nannotation in both unpaired and in semi-supervised scenarios. Additionally, the\nidea is validated on cardiac data for the task of medical segmentation. Our\napproach reduces the overfitting and improves the quality metrics beyond the\noriginal data outcome and beyond the result obtained with popular modern\naugmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bespalov_I/0/1/0/all/0/1\">Iaroslav Bespalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzun_N/0/1/0/all/0/1\">Nazar Buzun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kachan_O/0/1/0/all/0/1\">Oleg Kachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-self contrastive pretraining for crop type semantic segmentation. (arXiv:2104.04310v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04310","description":"<p>In this paper, we propose a fully supervised pre-training scheme based on\ncontrastive learning particularly tailored to dense classification tasks. The\nproposed Context-Self Contrastive Loss (CSCL) learns an embedding space that\nmakes semantic boundaries pop-up by use of a similarity metric between every\nlocation in a training sample and its local context. For crop type semantic\nsegmentation from Satellite Image Time Series (SITS) we find performance at\nparcel boundaries to be a critical bottleneck and explain how CSCL tackles the\nunderlying cause of that problem, improving the state-of-the-art performance in\nthis task. Additionally, using images from the Sentinel-2 (S2) satellite\nmissions we compile the largest, to our knowledge, SITS dataset densely\nannotated by crop type and parcel identities, which we make publicly available\ntogether with the data generation pipeline. Using that data we find CSCL, even\nwith minimal pre-training, to improve all respective baselines and present a\nprocess for semantic segmentation at super-resolution for obtaining crop\nclasses at a more granular level. The code and instructions to download the\ndata can be found in https://github.com/michaeltrs/DeepSatModels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tarasiou_M/0/1/0/all/0/1\">Michail Tarasiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guler_R/0/1/0/all/0/1\">Riza Alp Guler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoT: Delving Deeper into Classification Head for Transformer. (arXiv:2104.10935v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10935","description":"<p>Transformer models are not only successful in natural language processing\n(NLP) but also demonstrate high potential in computer vision (CV). Despite\ngreat advance, most of works only focus on improvement of architectures but pay\nlittle attention to the classification head. For years transformer models base\nexclusively on classification token to construct the final classifier, without\nexplicitly harnessing high-level word tokens. In this paper, we propose a novel\ntransformer model called second-order transformer (SoT), exploiting\nsimultaneously the classification token and word tokens for the classifier.\nSpecifically, we empirically disclose that high-level word tokens contain rich\ninformation, which per se are very competent with the classifier and moreover,\nare complementary to the classification token. To effectively harness such rich\ninformation, we propose multi-headed global cross-covariance pooling with\nsingular value power normalization, which shares similar philosophy and thus is\ncompatible with the transformer block, better than commonly used pooling\nmethods. Then, we study comprehensively how to explicitly combine word tokens\nwith classification token for building the final classification head. For CV\ntasks, our SoT significantly improves state-of-the-art vision transformers on\nchallenging benchmarks including ImageNet and ImageNet-A. For NLP tasks,\nthrough fine-tuning based on pretrained language transformers including GPT and\nBERT, our SoT greatly boosts the performance on widely used tasks such as CoLA\nand RTE. Code will be available at https://peihuali.org/SoT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1\">Ruiren Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peihua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pri3D: Can 3D Priors Help 2D Representation Learning?. (arXiv:2104.11225v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11225","description":"<p>Recent advances in 3D perception have shown impressive progress in\nunderstanding geometric structures of 3Dshapes and even scenes. Inspired by\nthese advances in geometric understanding, we aim to imbue image-based\nperception with representations learned under geometric constraints. We\nintroduce an approach to learn view-invariant,geometry-aware representations\nfor network pre-training, based on multi-view RGB-D data, that can then be\neffectively transferred to downstream 2D tasks. We propose to employ\ncontrastive learning under both multi-view im-age constraints and\nimage-geometry constraints to encode3D priors into learned 2D representations.\nThis results not only in improvement over 2D-only representation learning on\nthe image-based tasks of semantic segmentation, instance segmentation, and\nobject detection on real-world in-door datasets, but moreover, provides\nsignificant improvement in the low data regime. We show a significant\nimprovement of 6.0% on semantic segmentation on full data as well as 11.9% on\n20% data against baselines on ScanNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1\">Benjamin Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Angela Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visformer: The Vision-friendly Transformer. (arXiv:2104.12533v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12533","description":"<p>The past year has witnessed the rapid development of applying the Transformer\nmodule to vision problems. While some researchers have demonstrated that\nTransformer-based models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models suffer over-fitting\nespecially when the training data is limited. This paper offers an empirical\nstudy by performing step-by-step operations to gradually transit a\nTransformer-based model to a convolution-based model. The results we obtain\nduring the transition process deliver useful messages for improving visual\nrecognition. Based on these observations, we propose a new architecture named\nVisformer, which is abbreviated from the `Vision-friendly Transformer'. With\nthe same computational complexity, Visformer outperforms both the\nTransformer-based and convolution-based models in terms of ImageNet\nclassification accuracy, and the advantage becomes more significant when the\nmodel complexity is lower or the training set is smaller. The code is available\nat https://github.com/danczs/Visformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengsu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuefeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Multiple Graph Embedding for Multi-View Clustering. (arXiv:2105.04880v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04880","description":"<p>Graph-based multi-view clustering aiming to obtain a partition of data across\nmultiple views, has received considerable attention in recent years. Although\ngreat efforts have been made for graph-based multi-view clustering, it remains\na challenge to fuse characteristics from various views to learn a common\nrepresentation for clustering. In this paper, we propose a novel Consistent\nMultiple Graph Embedding Clustering framework(CMGEC). Specifically, a multiple\ngraph auto-encoder(M-GAE) is designed to flexibly encode the complementary\ninformation of multi-view data using a multi-graph attention fusion encoder. To\nguide the learned common representation maintaining the similarity of the\nneighboring characteristics in each view, a Multi-view Mutual Information\nMaximization module(MMIM) is introduced. Furthermore, a graph fusion\nnetwork(GFN) is devised to explore the relationship among graphs from different\nviews and provide a common consensus graph needed in M-GAE. By jointly training\nthese models, the common latent representation can be obtained which encodes\nmore complementary information from multiple views and depicts data more\ncomprehensively. Experiments on three types of multi-view datasets demonstrate\nCMGEC outperforms the state-of-the-art clustering methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dongxia Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhiqiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Optical physics inspired CNN approach for intrinsic image decomposition. (arXiv:2105.10076v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10076","description":"<p>Intrinsic Image Decomposition is an open problem of generating the\nconstituents of an image. Generating reflectance and shading from a single\nimage is a challenging task specifically when there is no ground truth. There\nis a lack of unsupervised learning approaches for decomposing an image into\nreflectance and shading using a single image. We propose a neural network\narchitecture capable of this decomposition using physics-based parameters\nderived from the image. Through experimental results, we show that (a) the\nproposed methodology outperforms the existing deep learning-based IID\ntechniques and (b) the derived parameters improve the efficacy significantly.\nWe conclude with a closer analysis of the results (numerical and example\nimages) showing several avenues for improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weligampola_H/0/1/0/all/0/1\">Harshana Weligampola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1\">Gihan Jayatilaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sritharan_S/0/1/0/all/0/1\">Suren Sritharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1\">Parakrama Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragel_R/0/1/0/all/0/1\">Roshan Ragel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1\">Vijitha Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1\">Roshan Godaliyadda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Unfolding of Iteratively Reweighted ADMM for Wireless RF Sensing. (arXiv:2106.03686v3 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2106.03686","description":"<p>We address the detection of material defects, which are inside a layered\nmaterial structure using compressive sensing based multiple-input and\nmultiple-output (MIMO) wireless radar. Here, the strong clutter due to the\nreflection of the layered structure's surface often makes the detection of the\ndefects challenging. Thus, sophisticated signal separation methods are required\nfor improved defect detection. In many scenarios, the number of defects that we\nare interested in is limited and the signaling response of the layered\nstructure can be modeled as a low-rank structure. Therefore, we propose joint\nrank and sparsity minimization for defect detection. In particular, we propose\na non-convex approach based on the iteratively reweighted nuclear and\n$\\ell_1-$norm (a double-reweighted approach) to obtain a higher accuracy\ncompared to the conventional nuclear norm and $\\ell_1-$norm minimization. To\nthis end, an iterative algorithm is designed to estimate the low-rank and\nsparse contributions. Further, we propose deep learning to learn the parameters\nof the algorithm (i.e., algorithm unfolding) to improve the accuracy and the\nspeed of convergence of the algorithm. Our numerical results show that the\nproposed approach outperforms the conventional approaches in terms of mean\nsquare errors of the recovered low-rank and sparse components and the speed of\nconvergence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thanthrige_U/0/1/0/all/0/1\">Udaya S.K.P. Miriya Thanthrige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_P/0/1/0/all/0/1\">Peter Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sezgin_A/0/1/0/all/0/1\">Aydin Sezgin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pedestrian Attribute Recognition in Video Surveillance Scenarios Based on View-attribute Attention Localization. (arXiv:2106.06485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06485","description":"<p>Pedestrian attribute recognition in surveillance scenarios is still a\nchallenging task due to the inaccurate localization of specific attributes. In\nthis paper, we propose a novel view-attribute localization method based on\nattention (VALA), which utilizes view information to guide the recognition\nprocess to focus on specific attributes and attention mechanism to localize\nspecific attribute-corresponding areas. Concretely, view information is\nleveraged by the view prediction branch to generate four view weights that\nrepresent the confidences for attributes from different views. View weights are\nthen delivered back to compose specific view-attributes, which will participate\nand supervise deep feature extraction. In order to explore the spatial location\nof a view-attribute, regional attention is introduced to aggregate spatial\ninformation and encode inter-channel dependencies of the view feature.\nSubsequently, a fine attentive attribute-specific region is localized, and\nregional weights for the view-attribute from different spatial locations are\ngained by the regional attention. The final view-attribute recognition outcome\nis obtained by combining the view weights with the regional weights.\nExperiments on three wide datasets (RAP, RAPv2, and PA-100K) demonstrate the\neffectiveness of our approach compared with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weichen Chen</a> (1) <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a> (1) <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a> (1) ((1) College of Information Engineering, Zhejiang University of Technology, Hangzhou, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles. (arXiv:2106.11810v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11810","description":"<p>In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kabzan_J/0/1/0/all/0/1\">Juraj Kabzan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kok Seang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_A/0/1/0/all/0/1\">Alex Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fletcher_L/0/1/0/all/0/1\">Luke Fletcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omari_S/0/1/0/all/0/1\">Sammy Omari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems. (arXiv:2106.14386v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2106.14386","description":"<p>This paper presents Kimera-Multi, the first multi-robot system that (i) is\nrobust and capable of identifying and rejecting incorrect inter and intra-robot\nloop closures resulting from perceptual aliasing, (ii) is fully distributed and\nonly relies on local (peer-to-peer) communication to achieve distributed\nlocalization and mapping, and (iii) builds a globally consistent\nmetric-semantic 3D mesh model of the environment in real-time, where faces of\nthe mesh are annotated with semantic labels. Kimera-Multi is implemented by a\nteam of robots equipped with visual-inertial sensors. Each robot builds a local\ntrajectory estimate and a local mesh using Kimera. When communication is\navailable, robots initiate a distributed place recognition and robust pose\ngraph optimization protocol based on a novel distributed graduated\nnon-convexity algorithm. The proposed protocol allows the robots to improve\ntheir local trajectory estimates by leveraging inter-robot loop closures while\nbeing robust to outliers. Finally, each robot uses its improved trajectory\nestimate to correct the local mesh using mesh deformation techniques.\n</p>\n<p>We demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking\ndatasets, and challenging outdoor datasets collected using ground robots. Both\nreal and simulated experiments involve long trajectories (e.g., up to 800\nmeters per robot). The experiments show that Kimera-Multi (i) outperforms the\nstate of the art in terms of robustness and accuracy, (ii) achieves estimation\nerrors comparable to a centralized SLAM system while being fully distributed,\n(iii) is parsimonious in terms of communication bandwidth, (iv) produces\naccurate metric-semantic 3D meshes, and (v) is modular and can be also used for\nstandard 3D reconstruction (i.e., without semantic labels) or for trajectory\nestimation (i.e., without reconstructing a 3D mesh).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yulun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_F/0/1/0/all/0/1\">Fernando Herrera Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_Granda_C/0/1/0/all/0/1\">Carlos Nieto-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformation-Compensated Learning for Image Reconstruction without Ground Truth. (arXiv:2107.05533v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.05533","description":"<p>Deep neural networks for medical image reconstruction are traditionally\ntrained using high-quality ground-truth images as training targets. Recent work\non Noise2Noise (N2N) has shown the potential of using multiple noisy\nmeasurements of the same object as an alternative to having a ground-truth.\nHowever, existing N2N-based methods are not suitable for learning from the\nmeasurements of an object undergoing nonrigid deformation. This paper addresses\nthis issue by proposing the deformation-compensated learning (DeCoLearn) method\nfor training deep reconstruction networks by compensating for object\ndeformations. A key component of DeCoLearn is a deep registration module, which\nis jointly trained with the deep reconstruction network without any\nground-truth supervision. We validate DeCoLearn on both simulated and\nexperimentally collected magnetic resonance imaging (MRI) data and show that it\nsignificantly improves imaging quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Weijie Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldeniz_C/0/1/0/all/0/1\">Cihat Eldeniz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_H/0/1/0/all/0/1\">Hongyu An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamilov_U/0/1/0/all/0/1\">Ulugbek S. Kamilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parallel and High-Fidelity Text-to-Lip Generation. (arXiv:2107.06831v2 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2107.06831","description":"<p>As a key component of talking face generation, lip movements generation\ndetermines the naturalness and coherence of the generated talking face video.\nPrior literature mainly focuses on speech-to-lip generation while there is a\npaucity in text-to-lip (T2L) generation. T2L is a challenging task and existing\nend-to-end works depend on the attention mechanism and autoregressive (AR)\ndecoding manner. However, the AR decoding manner generates current lip frame\nconditioned on frames generated previously, which inherently hinders the\ninference speed, and also has a detrimental effect on the quality of generated\nlip frames due to error propagation. This encourages the research of parallel\nT2L generation. In this work, we propose a parallel decoding model for fast and\nhigh-fidelity text-to-lip generation (ParaLip). Specifically, we predict the\nduration of the encoded linguistic features and model the target lip frames\nconditioned on the encoded linguistic features with their duration in a\nnon-autoregressive manner. Furthermore, we incorporate the structural\nsimilarity index loss and adversarial learning to improve perceptual quality of\ngenerated lip frames and alleviate the blurry prediction problem. Extensive\nexperiments conducted on GRID and TCD-TIMIT datasets demonstrate the\nsuperiority of proposed methods. Video samples are available via\n\\url{https://paralip.github.io/}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wencan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1\">Baoxing Huai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1\">Nicholas Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards robust vision by multi-task learning on monkey visual cortex. (arXiv:2107.14344v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14344","description":"<p>Deep neural networks set the state-of-the-art across many tasks in computer\nvision, but their generalization ability to image distortions is surprisingly\nfragile. In contrast, the mammalian visual system is robust to a wide range of\nperturbations. Recent work suggests that this generalization ability can be\nexplained by useful inductive biases encoded in the representations of visual\nstimuli throughout the visual cortex. Here, we successfully leveraged these\ninductive biases with a multi-task learning approach: we jointly trained a deep\nnetwork to perform image classification and to predict neural activity in\nmacaque primary visual cortex (V1). We measured the out-of-distribution\ngeneralization abilities of our network by testing its robustness to image\ndistortions. We found that co-training on monkey V1 data leads to increased\nrobustness despite the absence of those distortions during training.\nAdditionally, we showed that our network's robustness is very close to that of\nan Oracle network where parts of the architecture are directly trained on noisy\nimages. Our results also demonstrated that the network's representations become\nmore brain-like as their robustness improves. Using a novel constrained\nreconstruction analysis, we investigated what makes our brain-regularized\nnetwork more robust. We found that our co-trained network is more sensitive to\ncontent than noise when compared to a Baseline network that we trained for\nimage classification alone. Using DeepGaze-predicted saliency maps for ImageNet\nimages, we found that our monkey co-trained network tends to be more sensitive\nto salient regions in a scene, reminiscent of existing theories on the role of\nV1 in the detection of object borders and bottom-up saliency. Overall, our work\nexpands the promising research avenue of transferring inductive biases from the\nbrain, and provides a novel analysis of the effects of our transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safarani_S/0/1/0/all/0/1\">Shahd Safarani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nix_A/0/1/0/all/0/1\">Arne Nix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willeke_K/0/1/0/all/0/1\">Konstantin Willeke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadena_S/0/1/0/all/0/1\">Santiago A. Cadena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restivo_K/0/1/0/all/0/1\">Kelli Restivo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denfield_G/0/1/0/all/0/1\">George Denfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1\">Andreas S. Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinz_F/0/1/0/all/0/1\">Fabian H. Sinz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polyp-PVT: Polyp Segmentation with Pyramid Vision Transformers. (arXiv:2108.06932v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06932","description":"<p>Most polyp segmentation methods use CNNs as their backbone, leading to two\nkey issues when exchanging information between the encoder and decoder: 1)\ntaking into account the differences in contribution between different-level\nfeatures; and 2) designing an effective mechanism for fusing these features.\nDifferent from existing CNN-based methods, we adopt a transformer encoder,\nwhich learns more powerful and robust representations. In addition, considering\nthe image acquisition influence and elusive properties of polyps, we introduce\nthree novel modules, including a cascaded fusion module (CFM), a camouflage\nidentification module (CIM), a and similarity aggregation module (SAM). Among\nthese, the CFM is used to collect the semantic and location information of\npolyps from high-level features, while the CIM is applied to capture polyp\ninformation disguised in low-level features. With the help of the SAM, we\nextend the pixel features of the polyp area with high-level semantic position\ninformation to the entire polyp area, thereby effectively fusing cross-level\nfeatures. The proposed model, named Polyp-PVT, effectively suppresses noises in\nthe features and significantly improves their expressive capabilities.\nExtensive experiments on five widely adopted datasets show that the proposed\nmodel is more robust to various challenging situations (e.g., appearance\nchanges, small objects) than existing methods, and achieves the new\nstate-of-the-art performance. The proposed model is available at\nhttps://github.com/DengPingFan/Polyp-PVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MISSFormer: An Effective Medical Image Segmentation Transformer. (arXiv:2109.07162v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07162","description":"<p>The CNN-based methods have achieved impressive results in medical image\nsegmentation, but they failed to capture the long-range dependencies due to the\ninherent locality of the convolution operation. Transformer-based methods are\nrecently popular in vision tasks because of their capacity for long-range\ndependencies and promising performance. However, it lacks in modeling local\ncontext. In this paper, taking medical image segmentation as an example, we\npresent MISSFormer, an effective and powerful Medical Image Segmentation\ntranSFormer. MISSFormer is a hierarchical encoder-decoder network with two\nappealing designs: 1) A feed-forward network is redesigned with the proposed\nEnhanced Transformer Block, which enhances the long-range dependencies and\nsupplements the local context, making the feature more discriminative. 2) We\nproposed Enhanced Transformer Context Bridge, different from previous methods\nof modeling only global information, the proposed context bridge with the\nenhanced transformer block extracts the long-range dependencies and local\ncontext of multi-scale features generated by our hierarchical transformer\nencoder. Driven by these two designs, the MISSFormer shows a solid capacity to\ncapture more discriminative dependencies and context in medical image\nsegmentation. The experiments on multi-organ and cardiac segmentation tasks\ndemonstrate the superiority, effectiveness and robustness of our MISSFormer,\nthe experimental results of MISSFormer trained from scratch even outperform\nstate-of-the-art methods pre-trained on ImageNet. The core designs can be\ngeneralized to other visual segmentation tasks. The code has been released on\nGithub: https://github.com/ZhifangDeng/MISSFormer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaohong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhifang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dandan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xueguang Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the Timing of Camera Movements From the Kinematics of Instruments in Robotic-Assisted Surgery Using Artificial Neural Networks. (arXiv:2109.11192v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.11192","description":"<p>Robotic-assisted surgeries benefit both surgeons and patients, however,\nsurgeons frequently need to adjust the endoscopic camera to achieve good\nviewpoints. Simultaneously controlling the camera and the surgical instruments\nis impossible, and consequentially, these camera adjustments repeatedly\ninterrupt the surgery. Autonomous camera control could help overcome this\nchallenge, but most existing systems are reactive, e.g., by having the camera\nfollow the surgical instruments. We propose a predictive approach for\nanticipating when camera movements will occur using artificial neural networks.\nWe used the kinematic data of the surgical instruments, which were recorded\nduring robotic-assisted surgical training on porcine models. We split the data\ninto segments, and labeled each either as a segment that immediately precedes a\ncamera movement, or one that does not. Due to the large class imbalance, we\ntrained an ensemble of networks, each on a balanced sub-set of the training\ndata. We found that the instruments' kinematic data can be used to predict when\ncamera movements will occur, and evaluated the performance on different segment\ndurations and ensemble sizes. We also studied how much in advance an upcoming\ncamera movement can be predicted, and found that predicting a camera movement\n0.25, 0.5, and 1 second before they occurred achieved 98%, 94%, and 84%\naccuracy relative to the prediction of an imminent camera movement. This\nindicates that camera movement events can be predicted early enough to leave\ntime for computing and executing an autonomous camera movement and suggests\nthat an autonomous camera controller for RAMIS may one day be feasible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kossowsky_H/0/1/0/all/0/1\">Hanna Kossowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisky_I/0/1/0/all/0/1\">Ilana Nisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Pose Transfer with Correspondence Learning and Mesh Refinement. (arXiv:2109.15025v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15025","description":"<p>3D pose transfer is one of the most challenging 3D generation tasks. It aims\nto transfer the pose of a source mesh to a target mesh and keep the identity\n(e.g., body shape) of the target mesh. Some previous works require key point\nannotations to build reliable correspondence between the source and target\nmeshes, while other methods do not consider any shape correspondence between\nsources and targets, which leads to limited generation quality. In this work,\nwe propose a correspondence-refinement network to achieve the 3D pose transfer\nfor both human and animal meshes. The correspondence between source and target\nmeshes is first established by solving an optimal transport problem. Then, we\nwarp the source mesh according to the dense correspondence and obtain a coarse\nwarped mesh. The warped mesh will be better refined with our proposed Elastic\nInstance Normalization, which is a conditional normalization layer and can help\nto generate high-quality meshes. Extensive experimental results show that the\nproposed architecture can effectively transfer the poses from source to target\nmeshes and produce better results with satisfied visual performance than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chaoyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Kernel Representation for Image Reconstruction in PET. (arXiv:2110.01174v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.01174","description":"<p>Image reconstruction for positron emission tomography (PET) is challenging\nbecause of the ill-conditioned tomographic problem and low counting statistics.\nKernel methods address this challenge by using kernel representation to\nincorporate image prior information in the forward model of iterative PET image\nreconstruction. Existing kernel methods construct the kernels commonly using an\nempirical process, which may lead to suboptimal performance. In this paper, we\ndescribe the equivalence between the kernel representation and a trainable\nneural network model. A deep kernel method is proposed by exploiting deep\nneural networks to enable an automated learning of an optimized kernel model.\nThe proposed method is directly applicable to single subjects. The training\nprocess utilizes available image prior data to seek the best way to form a set\nof robust kernels optimally rather than empirically. The results from computer\nsimulations and a real patient dataset demonstrate that the proposed deep\nkernel method can outperform existing kernel method and neural network method\nfor dynamic PET image reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Siqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guobao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision. (arXiv:2110.02929v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02929","description":"<p>Event-based sensing using dynamic vision sensors is gaining traction in\nlow-power vision applications. Spiking neural networks work well with the\nsparse nature of event-based data and suit deployment on low-power neuromorphic\nhardware. Being a nascent field, the sensitivity of spiking neural networks to\npotentially malicious adversarial attacks has received very little attention so\nfar. In this work, we show how white-box adversarial attack algorithms can be\nadapted to the discrete and sparse nature of event-based visual data, and to\nthe continuous-time setting of spiking neural networks. We test our methods on\nthe N-MNIST and IBM Gestures neuromorphic vision datasets and show adversarial\nperturbations achieve a high success rate, by injecting a relatively small\nnumber of appropriately placed events. We also verify, for the first time, the\neffectiveness of these perturbations directly on neuromorphic hardware.\nFinally, we discuss the properties of the resulting perturbations and possible\nfuture directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchel_J/0/1/0/all/0/1\">Julian B&#xfc;chel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1\">Gregor Lenz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yalun Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheik_S/0/1/0/all/0/1\">Sadique Sheik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorbaro_M/0/1/0/all/0/1\">Martino Sorbaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03374","description":"<p>Unsupervised domain adaptation aims to align a labeled source domain and an\nunlabeled target domain, but it requires to access the source data which often\nraises concerns in data privacy, data portability and data transmission\nefficiency. We study unsupervised model adaptation (UMA), or called\nUnsupervised Domain Adaptation without Source Data, an alternative setting that\naims to adapt source-trained models towards target distributions without\naccessing source data. To this end, we design an innovative historical\ncontrastive learning (HCL) technique that exploits historical source hypothesis\nto make up for the absence of source data in UMA. HCL addresses the UMA\nchallenge from two perspectives. First, it introduces historical contrastive\ninstance discrimination (HCID) that learns from target samples by contrasting\ntheir embeddings which are generated by the currently adapted model and the\nhistorical models. With the historical models, HCID encourages UMA to learn\ninstance-discriminative target representations while preserving the source\nhypothesis. Second, it introduces historical contrastive category\ndiscrimination (HCCD) that pseudo-labels target samples to learn\ncategory-discriminative target representations. Specifically, HCCD re-weights\npseudo labels according to their prediction consistency across the current and\nhistorical models. Extensive experiments show that HCL outperforms and\nstate-of-the-art methods consistently across a variety of visual tasks and\nsetups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Proxy Anchor Loss and Effectiveness of Deep Metric Learning Performance Metrics. (arXiv:2110.03997v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03997","description":"<p>Deep metric learning (DML) learns the mapping, which maps into embedding\nspace in which similar data is near and dissimilar data is far. However,\nconventional proxy-based losses for DML have two problems: gradient problems\nand applying the real-world dataset with multiple local centers. Besides, DML\nperformance metrics also have some issues have stability and flexibility. This\npaper proposes multi-proxies anchor (MPA) loss and normalized discounted\ncumulative gain (nDCG@k) metric. This study contributes three following: (1)\nMPA loss is able to learn the real-world dataset with multi proxies. (2) MPA\nloss improves the training capacity of a neural network, which solves the\ngradient issues. (3) nDCG@k metric encourages full evaluation for various\ndatasets. Finally, we demonstrate MPA loss's effectiveness, and MPA loss\nachieves the highest accuracy on two datasets for fine-grained images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_S/0/1/0/all/0/1\">Shozo Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_M/0/1/0/all/0/1\">Minoru Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aman_H/0/1/0/all/0/1\">Hirohisa Aman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based person re-identification methods: A survey and outlook of recent works. (arXiv:2110.04764v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04764","description":"<p>In recent years, with the increasing demand for public safety and the rapid\ndevelopment of intelligent surveillance networks, person re-identification\n(Re-ID) has become one of the hot research topics in the computer vision field.\nThe main research goal of person Re-ID is to retrieve persons with the same\nidentity from different cameras. However, traditional person Re-ID methods\nrequire manual marking of person targets, which consumes a lot of labor cost.\nWith the widespread application of deep neural networks, many deep\nlearning-based person Re-ID methods have emerged. Therefore, this paper is to\nfacilitate researchers to understand the latest research results and the future\ntrends in the field. Firstly, we summarize the studies of several recently\npublished person Re-ID surveys and complement the latest research methods to\nsystematically classify deep learning-based person Re-ID methods. Secondly, we\npropose a multi-dimensional taxonomy that classifies current deep\nlearning-based person Re-ID methods into four categories according to metric\nand representation learning, including methods for deep metric learning, local\nfeature learning, generative adversarial learning and sequence feature\nlearning. Furthermore, we subdivide the above four categories according to\ntheir methodologies and motivations, discussing the advantages and limitations\nof part subcategories. Finally, we discuss some challenges and possible\nresearch directions for person Re-ID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhangqiang Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Min Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiangkun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiamin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Junlong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chengrui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules. (arXiv:2110.07720v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.07720","description":"<p>Training from scratch is the most common way to build a Convolutional Neural\nNetwork (CNN) based model. What if we can build new CNN models by reusing parts\nfrom previously build CNN models? What if we can improve a CNN model by\nreplacing (possibly faulty) parts with other parts? In both cases, instead of\ntraining, can we identify the part responsible for each output class (module)\nin the model(s) and reuse or replace only the desired output classes to build a\nmodel? Prior work has proposed decomposing dense-based networks into modules\n(one for each output class) to enable reusability and replaceability in various\nscenarios. However, this work is limited to the dense layers and based on the\none-to-one relationship between the nodes in consecutive layers. Due to the\nshared architecture in the CNN model, prior work cannot be adapted directly. In\nthis paper, we propose to decompose a CNN model used for image classification\nproblems into modules for each output class. These modules can further be\nreused or replaced to build a new model. We have evaluated our approach with\nCIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet\nmodels and found that enabling decomposition comes with a small cost (1.77% and\n0.85% for top-1 and top-5 accuracy, respectively). Also, building a model by\nreusing or replacing modules can be done with a 2.3% and 0.5% average loss of\naccuracy. Furthermore, reusing and replacing these modules reduces CO2e\nemission by ~37 times compared to training the model from scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_R/0/1/0/all/0/1\">Rangeet Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_H/0/1/0/all/0/1\">Hridesh Rajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event Guided Depth Sensing. (arXiv:2110.10505v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10505","description":"<p>Active depth sensors like structured light, lidar, and time-of-flight systems\nsample the depth of the entire scene uniformly at a fixed scan rate. This leads\nto limited spatio-temporal resolution where redundant static information is\nover-sampled and precious motion information might be under-sampled. In this\npaper, we present an efficient bio-inspired event-camera-driven depth\nestimation algorithm. In our approach, we dynamically illuminate areas of\ninterest densely, depending on the scene activity detected by the event camera,\nand sparsely illuminate areas in the field of view with no motion. The depth\nestimation is achieved by an event-based structured light system consisting of\na laser point projector coupled with a second event-based sensor tuned to\ndetect the reflection of the laser from the scene. We show the feasibility of\nour approach in a simulated autonomous driving scenario and real indoor\nsequences using our prototype. We show that, in natural scenes like autonomous\ndriving and indoor environments, moving edges correspond to less than 10% of\nthe scene on average. Thus our setup requires the sensor to scan only 10% of\nthe scene, which could lead to almost 90% less power consumption by the\nillumination source. While we present the evaluation and proof-of-concept for\nan event-based structured-light system, the ideas presented here are applicable\nfor a wide range of depth-sensing modalities like LIDAR, time-of-flight, and\nstandard stereo. Video is available at \\url{https://youtu.be/Rvv9IQLYjCQ}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muglikar_M/0/1/0/all/0/1\">Manasi Muglikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moeys_D/0/1/0/all/0/1\">Diederik Paul Moeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influential Prototypical Networks for Few Shot Learning: A Dermatological Case Study. (arXiv:2111.00698v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.00698","description":"<p>Prototypical network (PN) is a simple yet effective few shot learning\nstrategy. It is a metric-based meta-learning technique where classification is\nperformed by computing Euclidean distances to prototypical representations of\neach class. Conventional PN attributes equal importance to all samples and\ngenerates prototypes by simply averaging the support sample embeddings\nbelonging to each class. In this work, we propose a novel version of PN that\nattributes weights to support samples corresponding to their influence on the\nsupport sample distribution. Influence weights of samples are calculated based\non maximum mean discrepancy (MMD) between the mean embeddings of sample\ndistributions including and excluding the sample. Comprehensive evaluation of\nour proposed influential PN (IPNet) is performed by comparing its performance\nwith other baseline PNs on three different benchmark dermatological datasets.\nIPNet outperforms all baseline models with compelling results across all three\ndatasets and various N-way, K-shot classification tasks. Findings from\ncross-domain adaptation experiments further establish the robustness and\ngeneralizability of IPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_R/0/1/0/all/0/1\">Ranjana Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Visual Quality of Image Synthesis by A Token-based Generator with Transformers. (arXiv:2111.03481v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03481","description":"<p>We present a new perspective of achieving image synthesis by viewing this\ntask as a visual token generation problem. Different from existing paradigms\nthat directly synthesize a full image from a single input (e.g., a latent\ncode), the new formulation enables a flexible local manipulation for different\nimage regions, which makes it possible to learn content-aware and fine-grained\nstyle control for image synthesis. Specifically, it takes as input a sequence\nof latent tokens to predict the visual tokens for synthesizing an image. Under\nthis perspective, we propose a token-based generator (i.e.,TokenGAN).\nParticularly, the TokenGAN inputs two semantically different visual tokens,\ni.e., the learned constant content tokens and the style tokens from the latent\nspace. Given a sequence of style tokens, the TokenGAN is able to control the\nimage synthesis by assigning the styles to the content tokens by attention\nmechanism with a Transformer. We conduct extensive experiments and show that\nthe proposed TokenGAN has achieved state-of-the-art results on several\nwidely-used image synthesis benchmarks, including FFHQ and LSUN CHURCH with\ndifferent resolutions. In particular, the generator is able to synthesize\nhigh-fidelity images with 1024x1024 size, dispensing with convolutions\nentirely.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yanhong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1\">Hongyang Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Autoencoders Are Scalable Vision Learners. (arXiv:2111.06377v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06377","description":"<p>This paper shows that masked autoencoders (MAE) are scalable self-supervised\nlearners for computer vision. Our MAE approach is simple: we mask random\npatches of the input image and reconstruct the missing pixels. It is based on\ntwo core designs. First, we develop an asymmetric encoder-decoder architecture,\nwith an encoder that operates only on the visible subset of patches (without\nmask tokens), along with a lightweight decoder that reconstructs the original\nimage from the latent representation and mask tokens. Second, we find that\nmasking a high proportion of the input image, e.g., 75%, yields a nontrivial\nand meaningful self-supervisory task. Coupling these two designs enables us to\ntrain large models efficiently and effectively: we accelerate training (by 3x\nor more) and improve accuracy. Our scalable approach allows for learning\nhigh-capacity models that generalize well: e.g., a vanilla ViT-Huge model\nachieves the best accuracy (87.8%) among methods that use only ImageNet-1K\ndata. Transfer performance in downstream tasks outperforms supervised\npre-training and shows promising scaling behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_P/0/1/0/all/0/1\">Piotr Doll&#xe1;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Girshick_R/0/1/0/all/0/1\">Ross Girshick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action2video: Generating Videos of Human 3D Actions. (arXiv:2111.06925v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.06925","description":"<p>We aim to tackle the interesting yet challenging problem of generating videos\nof diverse and natural human motions from prescribed action categories. The key\nissue lies in the ability to synthesize multiple distinct motion sequences that\nare realistic in their visual appearances. It is achieved in this paper by a\ntwo-step process that maintains internal 3D pose and shape representations,\naction2motion and motion2video. Action2motion stochastically generates\nplausible 3D pose sequences of a prescribed action category, which are\nprocessed and rendered by motion2video to form 2D videos. Specifically, the Lie\nalgebraic theory is engaged in representing natural human motions following the\nphysical law of human kinematics; a temporal variational auto-encoder (VAE) is\ndeveloped that encourages diversity of output motions. Moreover, given an\nadditional input image of a clothed human character, an entire pipeline is\nproposed to extract his/her 3D detailed shape, and to render in videos the\nplausible motions from different views. This is realized by improving existing\nmethods to extract 3D human shapes and textures from single 2D images, rigging,\nanimating, and rendering to form 2D videos of human motions. It also\nnecessitates the curation and reannotation of 3D human motion datasets for\ntraining purpose. Thorough empirical experiments including ablation study,\nqualitative and quantitative evaluations manifest the applicability of our\napproach, and demonstrate its competitiveness in addressing related tasks,\nwhere components of our approach are compared favorably to the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinshuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1\">Shihao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model-Based Single Image Deep Dehazing. (arXiv:2111.10943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10943","description":"<p>Model-based single image dehazing algorithms restore images with sharp edges\nand rich details at the expense of low PSNR values. Data-driven ones restore\nimages with high PSNR values but with low contrast, and even some remaining\nhaze. In this paper, a novel single image dehazing algorithm is introduced by\nfusing model-based and data-driven approaches. Both transmission map and\natmospheric light are initialized by the model-based methods, and refined by\ndeep learning approaches which form a neural augmentation. Haze-free images are\nrestored by using the transmission map and atmospheric light. Experimental\nresults indicate that the proposed algorithm can remove haze well from\nreal-world and synthetic hazy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chaobing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_H/0/1/0/all/0/1\">Haiyan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shiqian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Agnostic Clustering with Self-Distillation. (arXiv:2111.12170v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.12170","description":"<p>Recent advancements in self-supervised learning have reduced the gap between\nsupervised and unsupervised representation learning. However, most\nself-supervised and deep clustering techniques rely heavily on data\naugmentation, rendering them ineffective for many learning tasks where\ninsufficient domain knowledge exists for performing augmentation. We propose a\nnew self-distillation based algorithm for domain-agnostic clustering. Our\nmethod builds upon the existing deep clustering frameworks and requires no\nseparate student model. The proposed method outperforms existing domain\nagnostic (augmentation-free) algorithms on CIFAR-10. We empirically demonstrate\nthat knowledge distillation can improve unsupervised representation learning by\nextracting richer `dark knowledge' from the model than using predicted labels\nalone. Preliminary experiments also suggest that self-distillation improves the\nconvergence of DeepCluster-v2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1\">Mohammed Adnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ioannou_Y/0/1/0/all/0/1\">Yani A. Ioannou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1\">Chuan-Yung Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1\">Graham W. Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACPL: Anti-curriculum Pseudo-labelling for Semi-supervised Medical Image Classification. (arXiv:2111.12918v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12918","description":"<p>Effective semi-supervised learning (SSL) in medical im-age analysis (MIA)\nmust address two challenges: 1) workeffectively on both multi-class (e.g.,\nlesion classification)and multi-label (e.g., multiple-disease diagnosis)\nproblems,and 2) handle imbalanced learning (because of the highvariance in\ndisease prevalence). One strategy to explorein SSL MIA is based on the pseudo\nlabelling strategy, butit has a few shortcomings. Pseudo-labelling has in\ngenerallower accuracy than consistency learning, it is not specifi-cally design\nfor both multi-class and multi-label problems,and it can be challenged by\nimbalanced learning. In this paper, unlike traditional methods that select\nconfident pseudo label by threshold, we propose a new SSL algorithm, called\nanti-curriculum pseudo-labelling (ACPL), which introduces novel techniques to\nselect informative unlabelled samples, improving training balance and allowing\nthe model to work for both multi-label and multi-class problems, and to\nestimate pseudo labels by an accurate ensemble of classifiers(improving pseudo\nlabel accuracy). We run extensive experiments to evaluate ACPL on two public\nmedical image classification benchmarks: Chest X-Ray14 for thorax disease\nmulti-label classification and ISIC2018 for skin lesion multi-class\nclassification. Our method outperforms previous SOTA SSL methods on both\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector Quantized Diffusion Model for Text-to-Image Synthesis. (arXiv:2111.14822v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14822","description":"<p>We present the vector quantized diffusion (VQ-Diffusion) model for\ntext-to-image generation. This method is based on a vector quantized\nvariational autoencoder (VQ-VAE) whose latent space is modeled by a conditional\nvariant of the recently developed Denoising Diffusion Probabilistic Model\n(DDPM). We find that this latent-space method is well-suited for text-to-image\ngeneration tasks because it not only eliminates the unidirectional bias with\nexisting methods but also allows us to incorporate a mask-and-replace diffusion\nstrategy to avoid the accumulation of errors, which is a serious problem with\nexisting methods. Our experiments show that the VQ-Diffusion produces\nsignificantly better text-to-image generation results when compared with\nconventional autoregressive (AR) models with similar numbers of parameters.\nCompared with previous GAN-based text-to-image methods, our VQ-Diffusion can\nhandle more complex scenes and improve the synthesized image quality by a large\nmargin. Finally, we show that the image generation computation in our method\ncan be made highly efficient by reparameterization. With traditional AR\nmethods, the text-to-image generation time increases linearly with the output\nimage resolution and hence is quite time consuming even for normal size images.\nThe VQ-Diffusion allows us to achieve a better trade-off between quality and\nspeed. Our experiments indicate that the VQ-Diffusion model with the\nreparameterization is fifteen times faster than traditional AR methods while\nachieving a better image quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shuyang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Semantic Segmentation via Spatial and Multi-Scale Aware Visual Class Embedding. (arXiv:2111.15181v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15181","description":"<p>Fully supervised semantic segmentation technologies bring a paradigm shift in\nscene understanding. However, the burden of expensive labeling cost remains as\na challenge. To solve the cost problem, recent studies proposed language model\nbased zero-shot semantic segmentation (L-ZSSS) approaches. In this paper, we\naddress L-ZSSS has a limitation in generalization which is a virtue of\nzero-shot learning. Tackling the limitation, we propose a language-model-free\nzero-shot semantic segmentation framework, Spatial and Multi-scale aware Visual\nClass Embedding Network (SM-VCENet). Furthermore, leveraging vision-oriented\nclass embedding SM-VCENet enriches visual information of the class embedding by\nmulti-scale attention and spatial attention. We also propose a novel benchmark\n(PASCAL2COCO) for zero-shot semantic segmentation, which provides\ngeneralization evaluation by domain adaptation and contains visually\nchallenging samples. In experiments, our SM-VCENet outperforms zero-shot\nsemantic segmentation state-of-the-art by a relative margin in PASCAL-5i\nbenchmark and shows generalization-robustness in PASCAL2COCO benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungguk Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yooseung Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Deep Learning-Based Forensic Iris Segmentation and Recognition. (arXiv:2112.00849v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00849","description":"<p>Iris recognition of living individuals is a mature biometric modality that\nhas been adopted globally from governmental ID programs, border crossing, voter\nregistration and de-duplication, to unlocking mobile phones. On the other hand,\nthe possibility of recognizing deceased subjects with their iris patterns has\nemerged recently. In this paper, we present an end-to-end deep learning-based\nmethod for postmortem iris segmentation and recognition with a special\nvisualization technique intended to support forensic human examiners in their\nefforts. The proposed postmortem iris segmentation approach outperforms the\nstate of the art and in addition to iris annulus, as in case of classical iris\nsegmentation methods - detects abnormal regions caused by eye decomposition\nprocesses, such as furrows or irregular specular highlights present on the\ndrying and wrinkling cornea. The method was trained and validated with data\nacquired from 171 cadavers, kept in mortuary conditions, and tested on\nsubject-disjoint data acquired from 259 deceased subjects. To our knowledge,\nthis is the largest corpus of data used in postmortem iris recognition research\nto date. The source code of the proposed method are offered with the paper. The\ntest data will be available through the National Archive of Criminal Justice\nData (NACJD) archives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuehlkamp_A/0/1/0/all/0/1\">Andrey Kuehlkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_A/0/1/0/all/0/1\">Aidan Boyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czajka_A/0/1/0/all/0/1\">Adam Czajka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowyer_K/0/1/0/all/0/1\">Kevin Bowyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flynn_P/0/1/0/all/0/1\">Patrick Flynn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chute_D/0/1/0/all/0/1\">Dennis Chute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benjamin_E/0/1/0/all/0/1\">Eric Benjamin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Global Statistics Aggregation for Improving Image Restoration. (arXiv:2112.04491v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.04491","description":"<p>Global spatial statistics, which are aggregated along entire spatial\ndimensions, are widely used in top-performance image restorers. For example,\nmean, variance in Instance Normalization (IN) which is adopted by HINet, and\nglobal average pooling (i.e. mean) in Squeeze and Excitation (SE) which is\napplied to MPRNet. This paper first shows that statistics aggregated on the\npatches-based/entire-image-based feature in the training/testing phase\nrespectively may distribute very differently and lead to performance\ndegradation in image restorers. It has been widely overlooked by previous\nworks. To solve this issue, we propose a simple approach, Test-time Local\nStatistics Converter (TLSC), that replaces the region of statistics aggregation\noperation from global to local, only in the test time. Without retraining or\nfinetuning, our approach significantly improves the image restorer's\nperformance. In particular, by extending SE with TLSC to the state-of-the-art\nmodels, MPRNet boost by 0.65 dB in PSNR on GoPro dataset, achieves 33.31 dB,\nexceeds the previous best result 0.6 dB. In addition, we simply apply TLSC to\nthe high-level vision task, i.e. semantic segmentation, and achieves\ncompetitive results. Extensive quantity and quality experiments are conducted\nto demonstrate TLSC solves the issue with marginal costs while significant\ngain. The code is available at https://github.com/megvii-research/tlsc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chengpeng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_X/0/1/0/all/0/1\">Xin Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Cluster Contrastive learning for Person Re-Identification. (arXiv:2112.04662v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04662","description":"<p>Recently, cluster contrastive learning has been proven effective for person\nReID by computing the contrastive loss between the individual feature and the\ncluster memory. However, existing methods that use the individual feature to\nmomentum update the cluster memory are not robust to the noisy samples, such as\nthe samples with wrong annotated labels or the pseudo-labels. Unlike the\nindividual-based updating mechanism, the centroid-based updating mechanism that\napplies the mean feature of each cluster to update the cluster memory is robust\nagainst minority noisy samples. Therefore, we formulate the individual-based\nupdating and centroid-based updating mechanisms in a unified cluster\ncontrastive framework, named Dual Cluster Contrastive learning (DCC), which\nmaintains two types of memory banks: individual and centroid cluster memory\nbanks. Significantly, the individual cluster memory is momentum updated based\non the individual feature.The centroid cluster memory applies the mean feature\nof each cluter to update the corresponding cluster memory. Besides the vallina\ncontrastive loss for each memory, a consistency constraint is applied to\nguarantee the consistency of the output of two memories. Note that DCC can be\neasily applied for unsupervised or supervised person ReID by using ground-truth\nlabels or pseudo-labels generated with clustering method, respectively.\nExtensive experiments on two benchmarks under supervised person ReID and\nunsupervised person ReID demonstrate the superior of the proposed DCC. Code is\navailable at: https://github.com/htyao89/Dual-Cluster-Contrastive/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Hantao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IFR-Explore: Learning Inter-object Functional Relationships in 3D Indoor Scenes. (arXiv:2112.05298v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05298","description":"<p>Building embodied intelligent agents that can interact with 3D indoor\nenvironments has received increasing research attention in recent years. While\nmost works focus on single-object or agent-object visual functionality and\naffordances, our work proposes to study a new kind of visual relationship that\nis also important to perceive and model -- inter-object functional\nrelationships (e.g., a switch on the wall turns on or off the light, a remote\ncontrol operates the TV). Humans often spend little or no effort to infer these\nrelationships, even when entering a new room, by using our strong prior\nknowledge (e.g., we know that buttons control electrical devices) or using only\na few exploratory interactions in cases of uncertainty (e.g., multiple switches\nand lights in the same room). In this paper, we take the first step in building\nAI system learning inter-object functional relationships in 3D indoor\nenvironments with key technical contributions of modeling prior knowledge by\ntraining over large-scale scenes and designing interactive policies for\neffectively exploring the training scenes and quickly adapting to novel test\nscenes. We create a new benchmark based on the AI2Thor and PartNet datasets and\nperform extensive experiments that prove the effectiveness of our proposed\nmethod. Results show that our model successfully learns priors and\nfast-interactive-adaptation strategies for exploring inter-object functional\nrelationships in complex 3D scenes. Several ablation studies further validate\nthe usefulness of each proposed module.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attacking Point Cloud Segmentation with Color-only Perturbation. (arXiv:2112.05871v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05871","description":"<p>Recent research efforts on 3D point-cloud semantic segmentation have achieved\noutstanding performance by adopting deep CNN (convolutional neural networks)\nand GCN (graph convolutional networks). However, the robustness of these\ncomplex models has not been systematically analyzed. Given that semantic\nsegmentation has been applied in many safety-critical applications (e.g.,\nautonomous driving, geological sensing), it is important to fill this knowledge\ngap, in particular, how these models are affected under adversarial samples.\nWhile adversarial attacks against point cloud have been studied, we found all\nof them were targeting single-object recognition, and the perturbation is done\non the point coordinates. We argue that the coordinate-based perturbation is\nunlikely to realize under the physical-world constraints. Hence, we propose a\nnew color-only perturbation method named COLPER, and tailor it to semantic\nsegmentation. By evaluating COLPER on an indoor dataset (S3DIS) and an outdoor\ndataset (Semantic3D) against three point cloud segmentation models (PointNet++,\nDeepGCNs, and RandLA-Net), we found color-only perturbation is sufficient to\nsignificantly drop the segmentation accuracy and aIoU, under both targeted and\nnon-targeted attack settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiacen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhe Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_B/0/1/0/all/0/1\">Boyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yufei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Automatic Data Augmentation for 3D Point Cloud Classification. (arXiv:2112.06029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06029","description":"<p>Data augmentation is an important technique to reduce overfitting and improve\nlearning performance, but existing works on data augmentation for 3D point\ncloud data are based on heuristics. In this work, we instead propose to\nautomatically learn a data augmentation strategy using bilevel optimization. An\naugmentor is designed in a similar fashion to a conditional generator and is\noptimized by minimizing a base model's loss on a validation set when the\naugmented input is used for training the model. This formulation provides a\nmore principled way to learn data augmentation on 3D point clouds. We evaluate\nour approach on standard point cloud classification tasks and a more\nchallenging setting with pose misalignment between training and validation/test\nsets. The proposed strategy achieves competitive performance on both tasks and\nwe provide further insight into the augmentor's ability to learn the validation\nset distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wanyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan-Sheng Foo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tracking and Long-Term Identification Using Non-Visual Markers. (arXiv:2112.06809v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06809","description":"<p>Our objective is to track and identify mice in a cluttered home-cage\nenvironment, as a precursor to automated behaviour recognition for biological\nresearch. This is a very challenging problem due to (i) the lack of\ndistinguishing visual features for each mouse, and (ii) the close confines of\nthe scene with constant occlusion, making standard visual tracking approaches\nunusable. However, a coarse estimate of each mouse's location is available from\na unique RFID implant, so there is the potential to optimally combine\ninformation from (weak) tracking with coarse information on identity. To\nachieve our objective, we make the following key contributions: (a) the\nformulation of the identification problem as an assignment problem (solved\nusing Integer Linear Programming), and (b) a novel probabilistic model of the\naffinity between tracklets and RFID data. The latter is a crucial part of the\nmodel, as it provides a principled probabilistic treatment of object detections\ngiven coarse localisation. Our approach achieves 77% accuracy on this\nidentification problem, and is able to reject spurious detections when the\nanimals are hidden.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camilleri_M/0/1/0/all/0/1\">Michael P. J. Camilleri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1\">Rasneer S. Bains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1\">Christopher K. I. Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HVH: Learning a Hybrid Neural Volumetric Representation for Dynamic Hair Performance Capture. (arXiv:2112.06904v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06904","description":"<p>Capturing and rendering life-like hair is particularly challenging due to its\nfine geometric structure, the complex physical interaction and its non-trivial\nvisual appearance.Yet, hair is a critical component for believable avatars. In\nthis paper, we address the aforementioned problems: 1) we use a novel,\nvolumetric hair representation that is com-posed of thousands of primitives.\nEach primitive can be rendered efficiently, yet realistically, by building on\nthe latest advances in neural rendering. 2) To have a reliable control signal,\nwe present a novel way of tracking hair on the strand level. To keep the\ncomputational effort manageable, we use guide hairs and classic techniques to\nexpand those into a dense hood of hair. 3) To better enforce temporal\nconsistency and generalization ability of our model, we further optimize the 3D\nscene flow of our representation with multi-view optical flow, using volumetric\nray marching. Our method can not only create realistic renders of recorded\nmulti-view sequences, but also create renderings for new hair configurations by\nproviding new control signals. We compare our method with existing work on\nviewpoint synthesis and drivable animation and achieve state-of-the-art\nresults. Please check out our project website at\nhttps://ziyanw1.github.io/hvh/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1\">Giljoo Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuyck_T/0/1/0/all/0/1\">Tuur Stuyck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_S/0/1/0/all/0/1\">Stephen Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1\">Michael Zollhoefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1\">Jessica Hodgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1\">Christoph Lassner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Pursuit: Building a Space of Objects via Discriminative Weight Generation. (arXiv:2112.07954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07954","description":"<p>We propose a framework to continuously learn object-centric representations\nfor visual learning and understanding. Existing object-centric representations\neither rely on supervisions that individualize objects in the scene, or perform\nunsupervised disentanglement that can hardly deal with complex scenes in the\nreal world. To mitigate the annotation burden and relax the constraints on the\nstatistical complexity of the data, our method leverages interactions to\neffectively sample diverse variations of an object and the corresponding\ntraining signals while learning the object-centric representations. Throughout\nlearning, objects are streamed one by one in random order with unknown\nidentities, and are associated with latent codes that can synthesize\ndiscriminative weights for each object through a convolutional hypernetwork.\nMoreover, re-identification of learned objects and forgetting prevention are\nemployed to make the learning process efficient and robust. We perform an\nextensive study of the key features of the proposed framework and analyze the\ncharacteristics of the learned representations. Furthermore, we demonstrate the\ncapability of the proposed framework in learning representations that can\nimprove label efficiency in downstream tasks. Our code and trained models will\nbe made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chuanyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanchao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yueqi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Nearest Neighbors for Visual Classification. (arXiv:2112.08459v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08459","description":"<p>Neural network classifiers have become the de-facto choice for current\n\"pre-train then fine-tune\" paradigms of visual classification. In this paper,\nwe investigate k-Nearest-Neighbor (k-NN) classifiers, a classical model-free\nlearning method from the pre-deep learning era, as an augmentation to modern\nneural network based approaches. As a lazy learning method, k-NN simply\naggregates the distance between the test image and top-k neighbors in a\ntraining set. We adopt k-NN with pre-trained visual representations produced by\neither supervised or self-supervised methods in two steps: (1) Leverage k-NN\npredicted probabilities as indications for easy vs. hard examples during\ntraining. (2) Linearly interpolate the k-NN predicted distribution with that of\nthe augmented classifier. Via extensive experiments on a wide range of\nclassification tasks, our study reveals the generality and flexibility of k-NN\nintegration with additional insights: (1) k-NN achieves competitive results,\nsometimes even outperforming a standard linear classifier. (2) Incorporating\nk-NN is especially beneficial for tasks where parametric classifiers perform\npoorly and / or in low-data regimes. We hope these discoveries will encourage\npeople to rethink the role of pre-deep learning, classical methods in computer\nvision. Our code is available at: https://github.com/KMnP/nn-revisit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bor-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Existing language grounding models often use object proposal bottlenecks: a\npre-trained detector proposes objects in the scene and the model learns to\nselect the answer from these box proposals, without attending to the original\nimage or 3D point cloud. Object detectors are typically trained on a fixed\nvocabulary of objects and attributes that is often too restrictive for\nopen-domain language grounding, where an utterance may refer to visual entities\nat various levels of abstraction, such as a chair, the leg of a chair, or the\ntip of the front leg of a chair. We propose a model for grounding language in\n3D scenes that bypasses box proposal bottlenecks with three main innovations:\ni) Iterative attention across the language stream, the point cloud feature\nstream and 3D box proposals. ii) Transformer decoders with non-parametric\nentity queries that decode 3D boxes for object and part referentials. iii)\nJoint supervision from 3D object annotations and language grounding\nannotations, by treating object detection as grounding of referential\nutterances comprised of a list of candidate category labels. These innovations\nresult in significant quantitative gains (up to +9% absolute improvement on the\nSR3D benchmark) over previous approaches on popular 3D language grounding\nbenchmarks. We ablate each of our innovations to show its contribution to the\nperformance of the model. When applied on language grounding on 2D images with\nminor changes, it performs on par with the state-of-the-art while converges in\nhalf of the GPU time. The code and checkpoints will be made available at\nhttps://github.com/nickgkan/beauty_detr\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Spatio-Temporal Pretext Learning for Self-supervised Video Representation. (arXiv:2112.08913v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08913","description":"<p>Spatio-temporal representation learning is critical for video self-supervised\nrepresentation. Recent approaches mainly use contrastive learning and pretext\ntasks. However, these approaches learn representation by discriminating sampled\ninstances via feature similarity in the latent space while ignoring the\nintermediate state of the learned representations, which limits the overall\nperformance. In this work, taking into account the degree of similarity of\nsampled instances as the intermediate state, we propose a novel pretext task -\nspatio-temporal overlap rate (STOR) prediction. It stems from the observation\nthat humans are capable of discriminating the overlap rates of videos in space\nand time. This task encourages the model to discriminate the STOR of two\ngenerated samples to learn the representations. Moreover, we employ a joint\noptimization combining pretext tasks with contrastive learning to further\nenhance the spatio-temporal representation learning. We also study the mutual\ninfluence of each component in the proposed scheme. Extensive experiments\ndemonstrate that our proposed STOR task can favor both contrastive learning and\npretext tasks. The joint optimization scheme can significantly improve the\nspatio-temporal representation in video understanding. The code is available at\nhttps://github.com/Katou2/CSTP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1\">Lai-Man Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Weifeng Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuzhi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wing-Yin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Efficient Language-supervised Zero-shot Recognition with Optimal Transport Distillation. (arXiv:2112.09445v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09445","description":"<p>Traditional computer vision models are trained to predict a fixed set of\npredefined categories. Recently, natural language has been shown to be a\nbroader and richer source of supervision that provides finer descriptions to\nvisual concepts than supervised \"gold\" labels. Previous works, such as CLIP,\nuse InfoNCE loss to train a model to predict the pairing between images and\ntext captions. CLIP, however, is data hungry and requires more than 400M\nimage-text pairs for training. The inefficiency can be partially attributed to\nthe fact that the image-text pairs are noisy. To address this, we propose OTTER\n(Optimal TransporT distillation for Efficient zero-shot Recognition), which\nuses online entropic optimal transport to find a soft image-text match as\nlabels for contrastive learning. Based on pretrained image and text encoders,\nmodels trained with OTTER achieve strong performance with only 3M image text\npairs. Compared with InfoNCE loss, label smoothing, and knowledge distillation,\nOTTER consistently outperforms these baselines in zero shot evaluation on\nGoogle Open Images (19,958 classes) and multi-labeled ImageNet 10K (10032\nclasses) from Tencent ML-Images. Over 42 evaluations on 7 different\ndataset/architecture settings x 6 metrics, OTTER outperforms (32) or ties (2)\nall baselines in 34 of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bichen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1\">Ruizhe Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vajda_P/0/1/0/all/0/1\">Peter Vajda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1\">Joseph E. Gonzalez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Registration of Brain MR Images via a Hybrid Loss. (arXiv:2110.15027v2 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2110.15027","description":"<p>Unsupervised learning strategy is widely adopted by the deformable\nregistration models due to the lack of ground truth of deformation fields.\nThese models typically depend on the intensity-based similarity loss to obtain\nthe learning convergence. Despite the success, such dependence is insufficient.\nFor the deformable registration of mono-modality image, well-aligned two images\nnot only have indistinguishable intensity differences, but also are close in\nthe statistical distribution and the boundary areas. Considering that\nwell-designed loss functions can facilitate a learning model into a desirable\nconvergence, we learn a deformable registration model for T1-weighted MR images\nby integrating multiple image characteristics via a hybrid loss. Our method\nregisters the OASIS dataset with high accuracy while preserving deformation\nsmoothness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1\">Luyi Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yunzhi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}