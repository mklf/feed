{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model. (arXiv:2206.05281v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05281","description":"<p>Current architectures for multi-modality tasks such as visual question\nanswering suffer from their high complexity. As a result, these architectures\nare difficult to train and require high computational resources. To address\nthese problems we present a CLIP-based architecture that does not require any\nfine-tuning of the feature extractors. A simple linear classifier is used on\nthe concatenated features of the image and text encoder. During training an\nauxiliary loss is added which operates on the answer types. The resulting\nclassification is then used as an attention gate on the answer class selection.\nOn the VizWiz 2022 Visual Question Answering Challenge we achieve 60.15 %\naccuracy on Task 1: Predict Answer to a Visual Question and AP score of 83.78 %\non Task 2: Predict Answerability of a Visual Question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deuser_F/0/1/0/all/0/1\">Fabian Deuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habel_K/0/1/0/all/0/1\">Konrad Habel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosch_P/0/1/0/all/0/1\">Philipp J. R&#xf6;sch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_N/0/1/0/all/0/1\">Norbert Oswald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AHD ConvNet for Speech Emotion Classification. (arXiv:2206.05286v1 [cs.SD])","link":"http://arxiv.org/abs/2206.05286","description":"<p>Accomplishments in the field of artificial intelligence are utilized in the\nadvancement of computing and making of intelligent machines for facilitating\nmankind and improving user experience. Emotions are rudimentary for people,\naffecting thinking and ordinary exercises like correspondence, learning and\ndirection. Speech emotion recognition is domain of interest in this regard and\nin this work, we propose a novel mel spectrogram learning approach in which our\nmodel uses the datapoints to learn emotions from the given wav form voice notes\nin the popular CREMA-D dataset. Our model uses log mel-spectrogram as feature\nwith number of mels = 64. It took less training time compared to other\napproaches used to address the problem of emotion speech recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Asfand Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasir_D/0/1/0/all/0/1\">Danial Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawad_M/0/1/0/all/0/1\">Mohammad Hassan Jawad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-in-Graph Network for Automatic Gene Ontology Description Generation. (arXiv:2206.05311v1 [cs.AI])","link":"http://arxiv.org/abs/2206.05311","description":"<p>Gene Ontology (GO) is the primary gene function knowledge base that enables\ncomputational tasks in biomedicine. The basic element of GO is a term, which\nincludes a set of genes with the same function. Existing research efforts of GO\nmainly focus on predicting gene term associations. Other tasks, such as\ngenerating descriptions of new terms, are rarely pursued. In this paper, we\npropose a novel task: GO term description generation. This task aims to\nautomatically generate a sentence that describes the function of a GO term\nbelonging to one of the three categories, i.e., molecular function, biological\nprocess, and cellular component. To address this task, we propose a\nGraph-in-Graph network that can efficiently leverage the structural information\nof GO. The proposed network introduces a two-layer graph: the first layer is a\ngraph of GO terms where each node is also a graph (gene graph). Such a\nGraph-in-Graph network can derive the biological functions of GO terms and\ngenerate proper descriptions. To validate the effectiveness of the proposed\nnetwork, we build three large-scale benchmark datasets. By incorporating the\nproposed Graph-in-Graph network, the performances of seven different\nsequence-to-sequence models can be substantially boosted across all evaluation\nmetrics, with up to 34.7%, 14.5%, and 39.1% relative improvements in BLEU,\nROUGE-L, and METEOR, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woicik_A/0/1/0/all/0/1\">Adelaide Woicik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-TOP: Zero-Shot Cross-Schema Task-Oriented Parsing. (arXiv:2206.05352v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05352","description":"<p>Deep learning methods have enabled task-oriented semantic parsing of\nincreasingly complex utterances. However, a single model is still typically\ntrained and deployed for each task separately, requiring labeled training data\nfor each, which makes it challenging to support new tasks, even within a single\nbusiness vertical (e.g., food-ordering or travel booking). In this paper we\ndescribe Cross-TOP (Cross-Schema Task-Oriented Parsing), a zero-shot method for\ncomplex semantic parsing in a given vertical. By leveraging the fact that user\nrequests from the same vertical share lexical and semantic similarities, a\nsingle cross-schema parser is trained to service an arbitrary number of tasks,\nseen or unseen, within a vertical. We show that Cross-TOP can achieve high\naccuracy on a previously unseen task without requiring any additional training\ndata, thereby providing a scalable way to bootstrap semantic parsers for new\ntasks. As part of this work we release the FoodOrdering dataset, a\ntask-oriented parsing dataset in the food-ordering vertical, with utterances\nand annotations derived from five schemas, each from a different restaurant\nmenu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rubino_M/0/1/0/all/0/1\">Melanie Rubino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mesnards_N/0/1/0/all/0/1\">Nicolas Guenon des Mesnards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_U/0/1/0/all/0/1\">Uday Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkoudas_K/0/1/0/all/0/1\">Konstantine Arkoudas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why is constrained neural language generation particularly challenging?. (arXiv:2206.05395v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05395","description":"<p>Recent advances in deep neural language models combined with the capacity of\nlarge scale datasets have accelerated the development of natural language\ngeneration systems that produce fluent and coherent texts (to various degrees\nof success) in a multitude of tasks and application contexts. However,\ncontrolling the output of these models for desired user and task needs is still\nan open challenge. This is crucial not only to customizing the content and\nstyle of the generated language, but also to their safe and reliable deployment\nin the real world. We present an extensive survey on the emerging topic of\nconstrained neural language generation in which we formally define and\ncategorize the problems of natural language generation by distinguishing\nbetween conditions and constraints (the latter being testable conditions on the\noutput text instead of the input), present constrained text generation tasks,\nand review existing methods and evaluation metrics for constrained text\ngeneration. Our aim is to highlight recent progress and trends in this emerging\nfield, informing on the most promising directions and limitations towards\nadvancing the state-of-the-art of constrained neural language generation\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garbacea_C/0/1/0/all/0/1\">Cristina Garbacea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a Personalized Dialogue System with Prompt-Tuning. (arXiv:2206.05399v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05399","description":"<p>Dialogue systems without consistent responses are not fascinating. In this\nstudy, we build a dialogue system that can respond based on a given character\nsetting (persona) to bring consistency. Considering the trend of the rapidly\nincreasing scale of language models, we propose an approach that uses\nprompt-tuning, which has low learning costs, on pre-trained large-scale\nlanguage models. The results of automatic and manual evaluations in English and\nJapanese show that it is possible to build a dialogue system with more natural\nand personalized responses using less computational resources than fine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasahara_T/0/1/0/all/0/1\">Tomohito Kasahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1\">Daisuke Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_N/0/1/0/all/0/1\">Nguyen Tung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shinzato_K/0/1/0/all/0/1\">Kenta Shinzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_T/0/1/0/all/0/1\">Toshinori Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Decomposition-Based Approach for Evaluating Inter-Annotator Disagreement in Narrative Analysis. (arXiv:2206.05446v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05446","description":"<p>In this work, we explore sources of inter-annotator disagreement in narrative\nanalysis, in light of the question of whether or not a narrative plot exists in\nthe text. For this purpose, we present a method for a conceptual decomposition\nof an existing annotation into two separate levels: (1) \\textbf{whether} or not\na narrative plot exists in the text, and (2) \\textbf{which} plot elements exist\nin the text. We apply this method to an existing dataset of sentences annotated\nwith three different narrative plot elements: \\textit{Complication},\n\\textit{Resolution} and \\textit{Success}. We then employ statistical analysis\nin order to quantify how much of the inter-annotator disagreement can be\nexplained by each of the two levels. We further perform a qualitative analysis\nof disagreement cases in each level, observing several sources of disagreement,\nsuch as text ambiguity, scheme definition and personal differences between the\nannotators. The insights gathered on the dataset may serve to reduce\ninter-annotator disagreement in future annotation endeavors. We conclude with a\nbroader discussion on the potential implications of our approach in studying\nand evaluating inter-annotator disagreement in other settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levi_E/0/1/0/all/0/1\">Effi Levi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenhav_S/0/1/0/all/0/1\">Shaul R. Shenhav</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparative Snippet Generation. (arXiv:2206.05473v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05473","description":"<p>We model product reviews to generate comparative responses consisting of\npositive and negative experiences regarding the product. Specifically, we\ngenerate a single-sentence, comparative response from a given positive and a\nnegative opinion. We contribute the first dataset for this task of Comparative\nSnippet Generation from contrasting opinions regarding a product, and a\nperformance analysis of a pre-trained BERT model to generate such snippets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saurabh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yisong Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Adversarial Robustness of NLP Models by Information Bottleneck. (arXiv:2206.05511v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05511","description":"<p>Existing studies have demonstrated that adversarial examples can be directly\nattributed to the presence of non-robust features, which are highly predictive,\nbut can be easily manipulated by adversaries to fool NLP models. In this study,\nwe explore the feasibility of capturing task-specific robust features, while\neliminating the non-robust ones by using the information bottleneck theory.\nThrough extensive experiments, we show that the models trained with our\ninformation bottleneck-based method are able to achieve a significant\nimprovement in robust accuracy, exceeding performances of all the previously\nreported defense methods while suffering almost no performance drop in clean\naccuracy on SST-2, AGNEWS and IMDB datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cenyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yixin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaoqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigation of Ensemble features of Self-Supervised Pretrained Models for Automatic Speech Recognition. (arXiv:2206.05518v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05518","description":"<p>Self-supervised learning (SSL) based models have been shown to generate\npowerful representations that can be used to improve the performance of\ndownstream speech tasks. Several state-of-the-art SSL models are available, and\neach of these models optimizes a different loss which gives rise to the\npossibility of their features being complementary. This paper proposes using an\nensemble of such SSL representations and models, which exploits the\ncomplementary nature of the features extracted by the various pretrained\nmodels. We hypothesize that this results in a richer feature representation and\nshows results for the ASR downstream task. To this end, we use three SSL models\nthat have shown excellent results on ASR tasks, namely HuBERT, Wav2vec2.0, and\nWaveLM. We explore the ensemble of models fine-tuned for the ASR task and the\nensemble of features using the embeddings obtained from the pre-trained models\nfor a downstream ASR task. We get improved performance over individual models\nand pre-trained features using Librispeech(100h) and WSJ dataset for the\ndownstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arunkumar_A/0/1/0/all/0/1\">A Arunkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukhadia_V/0/1/0/all/0/1\">Vrunda N Sukhadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S. Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging the Gap Between Training and Inference of Bayesian Controllable Language Models. (arXiv:2206.05519v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05519","description":"<p>Large-scale pre-trained language models have achieved great success on\nnatural language generation tasks. However, it is difficult to control the\npre-trained language models to generate sentences with the desired attribute\nsuch as topic and sentiment, etc. Recently, Bayesian Controllable Language\nModels (BCLMs) have been shown to be efficient in controllable language\ngeneration. Rather than fine-tuning the parameters of pre-trained language\nmodels, BCLMs use external discriminators to guide the generation of\npre-trained language models. However, the mismatch between training and\ninference of BCLMs limits the performance of the models. To address the\nproblem, in this work we propose a \"Gemini Discriminator\" for controllable\nlanguage generation which alleviates the mismatch problem with a small\ncomputational cost. We tested our method on two controllable language\ngeneration tasks: sentiment control and topic control. On both tasks, our\nmethod reached achieved new state-of-the-art results in automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haijin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianjin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training. (arXiv:2206.05555v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05555","description":"<p>Multi-modal pre-training and knowledge discovery are two important research\ntopics in multi-modal machine learning. Nevertheless, none of existing works\nmake attempts to link knowledge discovery with knowledge guided multi-modal\npre-training. In this paper, we propose to unify them into a continuous\nlearning framework for mutual improvement. Taking the open-domain uni-modal\ndatasets of images and texts as input, we maintain a knowledge graph as the\nfoundation to support these two tasks. For knowledge discovery, a pre-trained\nmodel is used to identify cross-modal links on the graph. For model\npre-training, the knowledge graph is used as the external knowledge to guide\nthe model updating. These two steps are iteratively performed in our framework\nfor continuous learning. The experimental results on MS-COCO and Flickr30K with\nrespect to both knowledge discovery and the pre-trained model validate the\neffectiveness of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can the Language of the Collation be Translated into the Language of the Stemma? Using Machine Translation for Witness Localization. (arXiv:2206.05603v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05603","description":"<p>Stemmatology is a subfield of philology where one approach to understand the\ncopy-history of textual variants of a text (witnesses of a tradition) is to\ngenerate an evolutionary tree. Computational methods are partly shared between\nthe sister discipline of phylogenetics and stemmatology. In 2022, a surveypaper\nin nature communications found that Deep Learning (DL), which otherwise has\nbrought about major improvements in many fields (Krohn et al 2020) has had only\nminor successes in phylogenetics and that \"it is difficult to conceive of an\nend-to-end DL model to directly estimate phylogenetic trees from raw data in\nthe near future\"(Sapoval et al. 2022, p.8). In stemmatology, there is to date\nno known DL approach at all. In this paper, we present a new DL approach to\nplacement of manuscripts on a stemma and demonstrate its potential. This could\nbe extended to phylogenetics where the universal code of DNA might be an even\nbetter prerequisite for the method using sequence to sequence based neural\nnetworks in order to retrieve tree distances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoenen_A/0/1/0/all/0/1\">Armin Hoenen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Pre-trained Language Models with Noise Stability Regularization. (arXiv:2206.05658v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05658","description":"<p>The advent of large-scale pre-trained language models has contributed greatly\nto the recent progress in natural language processing. Many state-of-the-art\nlanguage models are first trained on a large text corpus and then fine-tuned on\ndownstream tasks. Despite its recent success and wide adoption, fine-tuning a\npre-trained language model often suffers from overfitting, which leads to poor\ngeneralizability due to the extremely high complexity of the model and the\nlimited training samples from downstream tasks. To address this problem, we\npropose a novel and effective fine-tuning framework, named Layerwise Noise\nStability Regularization (LNSR). Specifically, we propose to inject the\nstandard Gaussian noise or In-manifold noise and regularize hidden\nrepresentations of the fine-tuned model. We first provide theoretical analyses\nto support the efficacy of our method. We then demonstrate the advantages of\nthe proposed method over other state-of-the-art algorithms including L2-SP,\nMixout and SMART. While these previous works only verify the effectiveness of\ntheir methods on relatively simple text classification tasks, we also verify\nthe effectiveness of our method on question answering tasks, where the target\nproblem is much more difficult and more training examples are available.\nFurthermore, extensive experimental results indicate that the proposed\nalgorithm can not only enhance the in-domain performance of the language models\nbut also improve the domain generalization performance on out-of-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_H/0/1/0/all/0/1\">Hang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingjian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1\">Dejing Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng-Zhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding in social media: An approach to building a chit-chat dialogue model. (arXiv:2206.05696v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05696","description":"<p>Building open-domain dialogue systems capable of rich human-like\nconversational ability is one of the fundamental challenges in language\ngeneration. However, even with recent advancements in the field, existing\nopen-domain generative models fail to capture and utilize external knowledge,\nleading to repetitive or generic responses to unseen utterances. Current work\non knowledge-grounded dialogue generation primarily focuses on persona\nincorporation or searching a fact-based structured knowledge source such as\nWikipedia. Our method takes a broader and simpler approach, which aims to\nimprove the raw conversation ability of the system by mimicking the human\nresponse behavior through casual interactions found on social media. Utilizing\na joint retriever-generator setup, the model queries a large set of filtered\ncomment data from Reddit to act as additional context for the seq2seq\ngenerator. Automatic and human evaluations on open-domain dialogue datasets\ndemonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_R/0/1/0/all/0/1\">Ritvik Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_D/0/1/0/all/0/1\">Daisuke Kawahara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoSe-Co: Text Conditioned Generative CommonSense Contextualizer. (arXiv:2206.05706v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05706","description":"<p>Pre-trained Language Models (PTLMs) have been shown to perform well on\nnatural language tasks. Many prior works have leveraged structured commonsense\npresent in the form of entities linked through labeled relations in Knowledge\nGraphs (KGs) to assist PTLMs. Retrieval approaches use KG as a separate static\nmodule which limits coverage since KGs contain finite knowledge. Generative\nmethods train PTLMs on KG triples to improve the scale at which knowledge can\nbe obtained. However, training on symbolic KG entities limits their\napplicability in tasks involving natural language text where they ignore\noverall context. To mitigate this, we propose a CommonSense Contextualizer\n(CoSe-Co) conditioned on sentences as input to make it generically usable in\ntasks for generating knowledge relevant to the overall context of input text.\nTo train CoSe-Co, we propose a novel dataset comprising of sentence and\ncommonsense knowledge pairs. The knowledge inferred by CoSe-Co is diverse and\ncontain novel entities not present in the underlying KG. We augment generated\nknowledge in Multi-Choice QA and Open-ended CommonSense Reasoning tasks leading\nto improvements over current best methods on CSQA, ARC, QASC and OBQA datasets.\nWe also demonstrate its applicability in improving performance of a baseline\nmodel for paraphrase generation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bansal_R/0/1/0/all/0/1\">Rachit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Milan Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Sumit Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_J/0/1/0/all/0/1\">Jivat Neet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The YiTrans End-to-End Speech Translation System for IWSLT 2022 Offline Shared Task. (arXiv:2206.05777v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05777","description":"<p>This paper describes the submission of our end-to-end YiTrans speech\ntranslation system for the IWSLT 2022 offline task, which translates from\nEnglish audio to German, Chinese, and Japanese. The YiTrans system is built on\nlarge-scale pre-trained encoder-decoder models. More specifically, we first\ndesign a multi-stage pre-training strategy to build a multi-modality model with\na large amount of labeled and unlabeled data. We then fine-tune the\ncorresponding components of the model for the downstream speech translation\ntasks. Moreover, we make various efforts to improve performance, such as data\nfiltering, data augmentation, speech segmentation, model ensemble, and so on.\nExperimental results show that our YiTrans system obtains a significant\nimprovement than the strong baseline on three translation directions, and it\nachieves +5.2 BLEU improvements over last year's optimal end-to-end system on\ntst2021 English-German. Our final submissions rank first on English-German and\nEnglish-Chinese end-to-end systems in terms of the automatic evaluation metric.\nWe make our code and models publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_J/0/1/0/all/0/1\">Junyi Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Intent Classification. (arXiv:2206.05790v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05790","description":"<p>Training accurate intent classifiers requires labeled data, which can be\ncostly to obtain. Data augmentation methods may ameliorate this issue, but the\nquality of the generated data varies significantly across techniques. We study\nthe process of systematically producing pseudo-labeled data given a small seed\nset using a wide variety of data augmentation techniques, including mixing\nmethods together. We find that while certain methods dramatically improve\nqualitative and quantitative performance, other methods have minimal or even\nnegative impact. We also analyze key considerations when implementing data\naugmentation methods in production.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1\">Claire Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-critiquing models for assisting human evaluators. (arXiv:2206.05802v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05802","description":"<p>We fine-tune large language models to write natural language critiques\n(natural language critical comments) using behavioral cloning. On a topic-based\nsummarization task, critiques written by our models help humans find flaws in\nsummaries that they would have otherwise missed. Our models help find naturally\noccurring flaws in both model and human written summaries, and intentional\nflaws in summaries written by humans to be deliberately misleading. We study\nscaling properties of critiquing with both topic-based summarization and\nsynthetic tasks. Larger models write more helpful critiques, and on most tasks,\nare better at self-critiquing, despite having harder-to-critique outputs.\nLarger models can also integrate their own self-critiques as feedback, refining\ntheir own summaries into better ones. Finally, we motivate and introduce a\nframework for comparing critiquing ability to generation and discrimination\nability. Our measurements suggest that even large models may still have\nrelevant knowledge they cannot or do not articulate as critiques. These results\nare a proof of concept for using AI-assisted human feedback to scale the\nsupervision of machine learning systems to tasks that are difficult for humans\nto evaluate directly. We release our training datasets, as well as samples from\nour critique assistance experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_W/0/1/0/all/0/1\">William Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Catherine Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bills_S/0/1/0/all/0/1\">Steven Bills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ward_J/0/1/0/all/0/1\">Jonathan Ward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Over-Generation Cannot Be Rewarded: Length-Adaptive Average Lagging for Simultaneous Speech Translation. (arXiv:2206.05807v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05807","description":"<p>Simultaneous speech translation (SimulST) systems aim at generating their\noutput with the lowest possible latency, which is normally computed in terms of\nAverage Lagging (AL). In this paper we highlight that, despite its widespread\nadoption, AL provides underestimated scores for systems that generate longer\npredictions compared to the corresponding references. We also show that this\nproblem has practical relevance, as recent SimulST systems have indeed a\ntendency to over-generate. As a solution, we propose LAAL (Length-Adaptive\nAverage Lagging), a modified version of the metric that takes into account the\nover-generation phenomenon and allows for unbiased evaluation of both\nunder-/over-generating systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05836","description":"<p>We present GLIPv2, a grounded VL understanding model, that serves both\nlocalization tasks (e.g., object detection, instance segmentation) and\nVision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2\nelegantly unifies localization pre-training and Vision-Language Pre-training\n(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of\nthe detection task, region-word contrastive learning as a novel region-word\nlevel contrastive learning task, and the masked language modeling. This\nunification not only simplifies the previous multi-stage VLP procedure but also\nachieves mutual benefits between localization and understanding tasks.\nExperimental results show that a single GLIPv2 model (all model weights are\nshared) achieves near SoTA performance on various localization and\nunderstanding tasks. The model also shows (1) strong zero-shot and few-shot\nadaption performance on open-vocabulary object detection tasks and (2) superior\ngrounding capability on VL understanding tasks. Code will be released at\nhttps://github.com/microsoft/GLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing the diagrammatic semiotic mode. (arXiv:2001.11224v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2001.11224","description":"<p>As the use and diversity of diagrams across many disciplines grows, there is\nan increasing interest in the diagrams research community concerning how such\ndiversity might be documented and explained. In this article, we argue that one\nway of achieving increased reliability, coverage, and utility for a general\nclassification of diagrams is to draw on recently developed semiotic principles\ndeveloped within the field of multimodality. To this end, we sketch out the\ninternal details of what may tentatively be termed the diagrammatic semiotic\nmode. This provides a natural account of how diagrammatic representations may\nintegrate natural language, various forms of graphics, diagrammatic elements\nsuch as arrows, lines and other expressive resources into coherent\norganisations, while still respecting the crucial diagrammatic contributions of\nvisual organisation. We illustrate the proposed approach using two recent\ndiagram corpora and show how a multimodal approach supports the empirical\nanalysis of diagrammatic representations, especially in identifying\ndiagrammatic constituents and describing their interrelations in a manner that\nmay be generalised across diagram types and be used to characterise distinct\nkinds of functionality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hiippala_T/0/1/0/all/0/1\">Tuomo Hiippala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bateman_J/0/1/0/all/0/1\">John A. Bateman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Linearity of Cross-Lingual Word Embedding Mappings. (arXiv:2004.01079v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.01079","description":"<p>The technique of Cross-Lingual Word Embedding (CLWE) plays a fundamental role\nin tackling Natural Language Processing challenges for low-resource languages.\nIts dominant approaches assumed that the relationship between embeddings could\nbe represented by a linear mapping, but there has been no exploration of the\nconditions under which this assumption holds. Such a research gap becomes very\ncritical recently, as it has been evidenced that relaxing mappings to be\nnon-linear can lead to better performance in some cases. We, for the first\ntime, present a theoretical analysis that identifies the preservation of\nanalogies encoded in monolingual word embeddings as a necessary and sufficient\ncondition for the ground-truth CLWE mapping between those embeddings to be\nlinear. On a novel cross-lingual analogy dataset that covers five\nrepresentative analogy categories for twelve distinct languages, we carry out\nexperiments which provide direct empirical support for our theoretical claim.\nThese results offer additional insight into the observations of other\nresearchers and contribute inspiration for the development of more effective\ncross-lingual representation learning strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xutan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stevenson_M/0/1/0/all/0/1\">Mark Stevenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenghua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Embeddings Preserving the Semantic Relationships in WordNet. (arXiv:2004.10863v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.10863","description":"<p>In this paper, we provide a novel way to generate low dimensional vector\nembeddings for the noun and verb synsets in WordNet, where the hypernym-hyponym\nrelationship is preserved in the embeddings. We call this embedding the Sense\nSpectrum (and Sense Spectra for embeddings). In order to create suitable labels\nfor the training of sense spectra, we designed a new similarity measurement for\nnoun and verb synsets in WordNet. We call this similarity measurement the\nHypernym Intersection Similarity (HIS), since it compares the common and unique\nhypernyms between two synsets. Our experiments show that on the noun and verb\npairs of the SimLex-999 dataset, HIS outperforms the three similarity\nmeasurements in WordNet. Moreover, to the best of our knowledge, the sense\nspectra provide the first dense synset embeddings that preserve the semantic\nrelationships in WordNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Canlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuwen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Theoretical Rule-based Knowledge Graph Reasoning by Connectivity Dependency Discovery. (arXiv:2011.06174v7 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.06174","description":"<p>Discovering precise and interpretable rules from knowledge graphs is regarded\nas an essential challenge, which can improve the performances of many\ndownstream tasks and even provide new ways to approach some Natural Language\nProcessing research topics. In this paper, we present a fundamental theory for\nrule-based knowledge graph reasoning, based on which the connectivity\ndependencies in the graph are captured via multiple rule types. It is the first\ntime for some of these rule types in a knowledge graph to be considered. Based\non these rule types, our theory can provide precise interpretations to unknown\ntriples. Then, we implement our theory by what we call the RuleDict model.\nResults show that our RuleDict model not only provides precise rules to\ninterpret new triples, but also achieves state-of-the-art performances on one\nbenchmark knowledge graph completion task, and is competitive on other tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Canlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chun-Nan Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsis_Y/0/1/0/all/0/1\">Yannis Katsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Ho-Cheol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_Baeza_Y/0/1/0/all/0/1\">Yoshiki Vazquez-Baeza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VGNMN: Video-grounded Neural Module Network to Video-Grounded Language Tasks. (arXiv:2104.07921v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07921","description":"<p>Neural module networks (NMN) have achieved success in image-grounded tasks\nsuch as Visual Question Answering (VQA) on synthetic images. However, very\nlimited work on NMN has been studied in the video-grounded dialogue tasks.\nThese tasks extend the complexity of traditional visual tasks with the\nadditional visual temporal variance and language cross-turn dependencies.\nMotivated by recent NMN approaches on image-grounded tasks, we introduce\nVideo-grounded Neural Module Network (VGNMN) to model the information retrieval\nprocess in video-grounded language tasks as a pipeline of neural modules. VGNMN\nfirst decomposes all language components in dialogues to explicitly resolve any\nentity references and detect corresponding action-based inputs from the\nquestion. The detected entities and actions are used as parameters to\ninstantiate neural module networks and extract visual cues from the video. Our\nexperiments show that VGNMN can achieve promising performance on a challenging\nvideo-grounded dialogue benchmark as well as a video QA benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trakhtenbrot's Theorem in Coq: Finite Model Theory through the Constructive Lens. (arXiv:2104.14445v5 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2104.14445","description":"<p>We study finite first-order satisfiability (FSAT) in the constructive setting\nof dependent type theory. Employing synthetic accounts of enumerability and\ndecidability, we give a full classification of FSAT depending on the\nfirst-order signature of non-logical symbols. On the one hand, our development\nfocuses on Trakhtenbrot's theorem, stating that FSAT is undecidable as soon as\nthe signature contains an at least binary relation symbol. Our proof proceeds\nby a many-one reduction chain starting from the Post correspondence problem. On\nthe other hand, we establish the decidability of FSAT for monadic first-order\nlogic, i.e. where the signature only contains at most unary function and\nrelation symbols, as well as the enumerability of FSAT for arbitrary enumerable\nsignatures. To showcase an application of Trakhtenbrot's theorem, we continue\nour reduction chain with a many-one reduction from FSAT to separation logic.\nAll our results are mechanised in the framework of a growing Coq library of\nsynthetic undecidability proofs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirst_D/0/1/0/all/0/1\">Dominik Kirst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larchey_Wendling_D/0/1/0/all/0/1\">Dominique Larchey-Wendling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clean or Annotate: How to Spend a Limited Data Collection Budget. (arXiv:2110.08355v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08355","description":"<p>Crowdsourcing platforms are often used to collect datasets for training\nmachine learning models, despite higher levels of inaccurate labeling compared\nto expert labeling. There are two common strategies to manage the impact of\nsuch noise. The first involves aggregating redundant annotations, but comes at\nthe expense of labeling substantially fewer examples. Secondly, prior works\nhave also considered using the entire annotation budget to label as many\nexamples as possible and subsequently apply denoising algorithms to implicitly\nclean the dataset. We find a middle ground and propose an approach which\nreserves a fraction of annotations to explicitly clean up highly probable error\nsamples to optimize the annotation process. In particular, we allocate a large\nportion of the labeling budget to form an initial dataset used to train a\nmodel. This model is then used to identify specific examples that appear most\nlikely to be incorrect, which we spend the remaining budget to relabel.\nExperiments across three model variations and four natural language processing\ntasks show our approach outperforms or matches both label aggregation and\nadvanced denoising methods designed to handle noisy labels when allocated the\nsame finite annotation budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Derek Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Transformers with Probabilistic Attention Keys. (arXiv:2110.08678v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08678","description":"<p>Multi-head attention is a driving force behind state-of-the-art transformers,\nwhich achieve remarkable performance across a variety of natural language\nprocessing (NLP) and computer vision tasks. It has been observed that for many\napplications, those attention heads learn redundant embedding, and most of them\ncan be removed without degrading the performance of the model. Inspired by this\nobservation, we propose Transformer with a Mixture of Gaussian Keys\n(Transformer-MGK), a novel transformer architecture that replaces redundant\nheads in transformers with a mixture of keys at each head. These mixtures of\nkeys follow a Gaussian mixture model and allow each attention head to focus on\ndifferent parts of the input sequence efficiently. Compared to its conventional\ntransformer counterpart, Transformer-MGK accelerates training and inference,\nhas fewer parameters, and requires fewer FLOPs to compute while achieving\ncomparable or better accuracy across tasks. Transformer-MGK can also be easily\nextended to use with linear attention. We empirically demonstrate the advantage\nof Transformer-MGK in a range of practical applications, including language\nmodeling and tasks that involve very long sequences. On the Wikitext-103 and\nLong Range Arena benchmark, Transformer-MGKs with 4 heads attain comparable or\nbetter performance to the baseline transformers with 8 heads.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tan M. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Dung D. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy Khuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet-Anh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1\">Nhat Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1\">Stanley J. Osher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoLLIE: Continual Learning of Language Grounding from Language-Image Embeddings. (arXiv:2111.07993v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07993","description":"<p>This paper presents CoLLIE: a simple, yet effective model for continual\nlearning of how language is grounded in vision. Given a pre-trained multimodal\nembedding model, where language and images are projected in the same semantic\nspace (in this case CLIP by OpenAI), CoLLIE learns a transformation function\nthat adjusts the language embeddings when needed to accommodate new language\nuse. This is done by predicting the difference vector that needs to be applied,\nas well as a scaling factor for this vector, so that the adjustment is only\napplied when needed. Unlike traditional few-shot learning, the model does not\njust learn new classes and labels, but can also generalize to similar language\nuse and leverage semantic compositionality. We verify the model's performance\non two different tasks of identifying the targets of referring expressions,\nwhere it has to learn new language use. The results show that the model can\nefficiently learn and generalize from only a few examples, with little\ninterference with the model's original zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1\">Gabriel Skantze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willemsen_B/0/1/0/all/0/1\">Bram Willemsen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmTract: Investor Emotions and Market Behavior. (arXiv:2112.03868v2 [q-fin.PR] UPDATED)","link":"http://arxiv.org/abs/2112.03868","description":"<p>We develop a tool that extracts emotions from social media text data. Our\nmethodology has three main advantages. First, it is tailored for financial\ncontext; second, it incorporates key aspects of social media data, such as\nnon-standard phrases, emojis and emoticons; and third, it operates by\nsequentially learning a latent representation that includes features such as\nword order, word usage, and local context. This tool, along with a user guide\nis available at: https://github.com/dvamossy/EmTract. Using EmTract, we explore\nthe relationship between investor emotions expressed on social media and asset\nprices. We document a number of interesting insights. First, we confirm some of\nthe findings of controlled laboratory experiments relating investor emotions to\nasset price movements. Second, we show that investor emotions are predictive of\ndaily price movements. These impacts are larger when volatility or short\ninterest are higher, and when institutional ownership or liquidity are lower.\nThird, increased investor enthusiasm prior to the IPO contributes to the large\nfirst-day return and long-run underperformance of IPO stocks. To corroborate\nour results, we provide a number of robustness checks, including using an\nalternative emotion model. Our findings reinforce the intuition that emotions\nand market dynamics are closely related, and highlight the importance of\nconsidering investor emotions when assessing a stock's short-term value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Vamossy_D/0/1/0/all/0/1\">Domonkos Vamossy</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Skog_R/0/1/0/all/0/1\">Rolf Skog</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Design Challenges for a Multi-Perspective Search Engine. (arXiv:2112.08357v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08357","description":"<p>Many users turn to document retrieval systems (e.g. search engines) to seek\nanswers to controversial questions. Answering such user queries usually require\nidentifying responses within web documents, and aggregating the responses based\non their different perspectives.\n</p>\n<p>Classical document retrieval systems fall short at delivering a set of direct\nand diverse responses to the users. Naturally, identifying such responses\nwithin a document is a natural language understanding task. In this paper, we\nexamine the challenges of synthesizing such language understanding objectives\nwith document retrieval, and study a new perspective-oriented document\nretrieval paradigm. We discuss and assess the inherent natural language\nunderstanding challenges in order to achieve the goal. Following the design\nchallenges and principles, we demonstrate and evaluate a practical prototype\npipeline system. We use the prototype system to conduct a user survey in order\nto assess the utility of our paradigm, as well as understanding the user\ninformation needs for controversial queries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Siyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyttendaele_X/0/1/0/all/0/1\">Xander Uyttendaele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_W/0/1/0/all/0/1\">William Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Most language grounding models learn to select the referred object from a\npool of object proposals provided by a pre-trained detector. This object\nproposal bottleneck is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of a\nchair, or the tip of the front leg of a chair, which may be missed by the\ndetector. Recently, MDETR introduced a language grounding model for 2D images\nthat do not have such a box proposal bottleneck; instead of selecting objects\nfrom a proposal pool, it instead decodes the referenced object boxes directly\nfrom image and language features and achieves big leaps in performance. We\npropose a language grounding model for 3D scenes built on MDETR, which we call\nBEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an\nadditional object proposal pool computed bottom-up from a pre-trained detector.\nYet it decodes referenced objects without selecting them from the pool. In this\nway, it uses powerful object detectors to help ground language without being\nrestricted by their misses. Second, BEAUTY-DETR augments supervision from\nlanguage grounding annotations by configuring object detection annotations as\nlanguage prompts to be grounded in images. The proposed model sets a new\nstate-of-the-art across popular 3D language grounding benchmarks with\nsignificant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%\non NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the\n3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on\nthe ScanRefer benchmark. When applied to language grounding in 2D images, it\nperforms on par with MDETR. We ablate each of the design choices of the model\nand quantify their contribution to performance. Code and checkpoints are\navailable at https://github.com/nickgkan/beauty_detr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What is Event Knowledge Graph: A Survey. (arXiv:2112.15280v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.15280","description":"<p>Besides entity-centric knowledge, usually organized as Knowledge Graph (KG),\nevents are also an essential kind of knowledge in the world, which trigger the\nspring up of event-centric knowledge representation form like Event KG (EKG).\nIt plays an increasingly important role in many downstream applications, such\nas search, question-answering, recommendation, financial quantitative\ninvestments, and text generation. This paper provides a comprehensive survey of\nEKG from history, ontology, instance, and application views. Specifically, to\ncharacterize EKG thoroughly, we focus on its history, definition, schema\ninduction, acquisition, related representative graphs/systems, and\napplications. The development processes and trends are studied therein. We\nfurther summarize prospective directions to facilitate future research on EKG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1\">Saiping Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Long Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fujun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yutao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaolong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solvability of orbit-finite systems of linear equations. (arXiv:2201.09060v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09060","description":"<p>We study orbit-finite systems of linear equations, in the setting of sets\nwith atoms. Our principal contribution is a decision procedure for solvability\nof such systems. The procedure works for every field (and even commutative\nring) under mild effectiveness assumptions, and reduces a given orbit-finite\nsystem to a number of finite ones: exponentially many in general, but\npolynomially many when atom dimension of input systems is fixed. Towards\nobtaining the procedure we push further the theory of vector spaces generated\nby orbit-finite sets, and show that each such vector space admits an\norbit-finite basis. This fundamental property is a key tool in our development,\nbut should be also of wider interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Arka Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofman_P/0/1/0/all/0/1\">Piotr Hofman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lasota_S/0/1/0/all/0/1\">S&#x142;awomir Lasota</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing language context confusion for end-to-end code-switching automatic speech recognition. (arXiv:2201.12155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12155","description":"<p>Code-switching is about dealing with alternative languages in the\ncommunication process. Training end-to-end (E2E) automatic speech recognition\n(ASR) systems for code-switching is known to be a challenging problem because\nof the lack of data compounded by the increased language context confusion due\nto the presence of more than one language. In this paper, we propose a\nlanguage-related attention mechanism to reduce multilingual context confusion\nfor the E2E code-switching ASR model based on the Equivalence Constraint Theory\n(EC). The linguistic theory requires that any monolingual fragment that occurs\nin the code-switching sentence must occur in one of the monolingual sentences.\nIt establishes a bridge between monolingual data and code-switching data. By\ncalculating the respective attention of multiple languages, our method can\nefficiently transfer language knowledge from rich monolingual data. We evaluate\nour method on ASRU 2019 Mandarin-English code-switching challenge dataset.\nCompared with the baseline model, the proposed method achieves 11.37% relative\nmix error rate reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuraHealth: An Automated Screening Pipeline to Detect Undiagnosed Cognitive Impairment in Electronic Health Records with Deep Learning and Natural Language Processing. (arXiv:2202.00478v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00478","description":"<p>Dementia related cognitive impairment (CI) affects over 55 million people\nworldwide and is growing rapidly at the rate of one new case every 3 seconds.\nWith a recurring failure of clinical trials, early diagnosis is crucial, but\n75% of dementia cases go undiagnosed globally with up to 90% in\nlow-and-middle-income countries. Current diagnostic methods are notoriously\ncomplex, involving manual review of medical notes, numerous cognitive tests,\nexpensive brain scans or spinal fluid tests. Information relevant to CI is\noften found in the electronic health records (EHRs) and can provide vital clues\nfor early diagnosis, but a manual review by experts is tedious and error prone.\nThis project develops a novel state-of-the-art automated screening pipeline for\nscalable and high-speed discovery of undetected CI in EHRs. To understand the\nlinguistic context from complex language structures in EHR, a database of 8,656\nsequences was constructed to train attention-based deep learning natural\nlanguage processing model to classify sequences. A patient level prediction\nmodel based on logistic regression was developed using the sequence level\nclassifier. The deep learning system achieved 93% accuracy and AUC = 0.98 to\nidentify patients who had no earlier diagnosis, dementia-related diagnosis\ncode, or dementia-related medications in their EHR. These patients would have\notherwise gone undetected or detected too late. The EHR screening pipeline was\ndeployed in NeuraHealthNLP, a web application for automated and real-time CI\nscreening by simply uploading EHRs in a browser. NeuraHealthNLP is cheaper,\nfaster, more accessible, and outperforms current clinical methods including\ntext-based analytics and machine learning approaches. It makes early diagnosis\nviable in regions with scarce health care services but accessible internet or\ncellular services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_T/0/1/0/all/0/1\">Tanish Tyagi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Magdamo_C/0/1/0/all/0/1\">Colin G. Magdamo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Noori_A/0/1/0/all/0/1\">Ayush Noori</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaozhi Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Deodhar_M/0/1/0/all/0/1\">Mayuresh Deodhar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhuoqiao Hong</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wendong Ge</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ye_E/0/1/0/all/0/1\">Elissa M. Ye</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Sheu_Y/0/1/0/all/0/1\">Yi-han Sheu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Alabsi_H/0/1/0/all/0/1\">Haitham Alabsi</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Brenner_L/0/1/0/all/0/1\">Laura Brenner</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Robbins_G/0/1/0/all/0/1\">Gregory K. Robbins</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_S/0/1/0/all/0/1\">Sahar Zafar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Benson_N/0/1/0/all/0/1\">Nicole Benson</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Moura_L/0/1/0/all/0/1\">Lidia Moura</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1\">John Hsu</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_Pozo_A/0/1/0/all/0/1\">Alberto Serrano-Pozo</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Prokopenko_D/0/1/0/all/0/1\">Dimitry Prokopenko</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Tanzi_R/0/1/0/all/0/1\">Rudolph E. Tanzi</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Hyman_B/0/1/0/all/0/1\">Bradley T.Hyman</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Blacker_D/0/1/0/all/0/1\">Deborah Blacker</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mukerji_S/0/1/0/all/0/1\">Shibani S. Mukerji</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Westover_M/0/1/0/all/0/1\">M. Brandon Westover</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudeshna Das</a> (1) ((1) Massachusetts General Hospital, Boston, MA, (2) McCance Center for Brain Health, Boston, MA)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching Tweets With Applicable Fact-Checks Across Languages. (arXiv:2202.07094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.07094","description":"<p>An important challenge for news fact-checking is the effective dissemination\nof existing fact-checks. This in turn brings the need for reliable methods to\ndetect previously fact-checked claims. In this paper, we focus on automatically\nfinding existing fact-checks for claims made in social media posts (tweets). We\nconduct both classification and retrieval experiments, in monolingual (English\nonly), multilingual (Spanish, Portuguese), and cross-lingual (Hindi-English)\nsettings using multilingual transformer models such as XLM-RoBERTa and\nmultilingual embeddings such as LaBSE and SBERT. We present promising results\nfor \"match\" classification (86% average accuracy) in four language pairs. We\nalso find that a BM25 baseline outperforms or is on par with state-of-the-art\nmultilingual embedding models for the retrieval task during our monolingual\nexperiments. We highlight and discuss NLP challenges while addressing this\nproblem in different languages, and we introduce a novel curated dataset of\nfact-checks and corresponding tweets for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1\">Ashkan Kazemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zehua Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Rosas_V/0/1/0/all/0/1\">Ver&#xf3;nica P&#xe9;rez-Rosas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PET: An Annotated Dataset for Process Extraction from Natural Language Text. (arXiv:2203.04860v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.04860","description":"<p>Process extraction from text is an important task of process discovery, for\nwhich various approaches have been developed in recent years. However, in\ncontrast to other information extraction tasks, there is a lack of\ngold-standard corpora of business process descriptions that are carefully\nannotated with all the entities and relationships of interest. Due to this, it\nis currently hard to compare the results obtained by extraction approaches in\nan objective manner, whereas the lack of annotated texts also prevents the\napplication of data-driven information extraction methodologies, typical of the\nnatural language processing field. Therefore, to bridge this gap, we present\nthe PET dataset, a first corpus of business process descriptions annotated with\nactivities, gateways, actors, and flow information. We present our new\nresource, including a variety of baselines to benchmark the difficulty and\nchallenges of business process extraction from text. PET can be accessed via\nhuggingface.co/datasets/patriziobellan/PET\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellan_P/0/1/0/all/0/1\">Patrizio Bellan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aa_H/0/1/0/all/0/1\">Han van der Aa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragoni_M/0/1/0/all/0/1\">Mauro Dragoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidini_C/0/1/0/all/0/1\">Chiara Ghidini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WCL-BBCD: A Contrastive Learning and Knowledge Graph Approach to Named Entity Recognition. (arXiv:2203.06925v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06925","description":"<p>Named Entity Recognition task is one of the core tasks of information\nextraction.Word ambiguity and word abbreviation are important reasons for the\nlow recognition rate of named entities. In this paper, we propose a novel named\nentity recognition model WCL-BBCD (Word Contrastive Learning with\nBERT-BiLSTM-CRF-DBpedia) incorporating the idea of contrastive learning. The\nmodel first trains the sentence pairs in the text, calculate similarity between\nwords in sentence pairs by cosine similarity, and fine-tunes the BERT model\nused for the named entity recognition task through the similarity, so as to\nalleviate word ambiguity. Then, the fine-tuned BERT model is combined with the\nBiLSTM-CRF model to perform the named entity recognition task. Finally, the\nrecognition results are corrected in combination with prior knowledge such as\nknowledge graphs, so as to alleviate the recognition caused by word\nabbreviations low-rate problem. Experimental results show that our model\noutperforms other similar model methods on the CoNLL-2003 English dataset and\nOntoNotes V5 English dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Renjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jian Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jilin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianxiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianjun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Code Switched and Code Mixed Speech Recognition for Indic languages. (arXiv:2203.16578v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16578","description":"<p>Training multilingual automatic speech recognition (ASR) systems is\nchallenging because acoustic and lexical information is typically language\nspecific. Training multilingual system for Indic languages is even more tougher\ndue to lack of open source datasets and results on different approaches. We\ncompare the performance of end to end multilingual speech recognition system to\nthe performance of monolingual models conditioned on language identification\n(LID). The decoding information from a multilingual model is used for language\nidentification and then combined with monolingual models to get an improvement\nof 50% WER across languages. We also propose a similar technique to solve the\nCode Switched problem and achieve a WER of 21.77 and 28.27 over Hindi-English\nand Bengali-English respectively. Our work talks on how transformer based ASR\nespecially wav2vec 2.0 can be applied in developing multilingual ASR and code\nswitched ASR for Indic languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Speech Recognition for Indic Languages using Language Model. (arXiv:2203.16595v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16595","description":"<p>We study the effect of applying a language model (LM) on the output of\nAutomatic Speech Recognition (ASR) systems for Indic languages. We fine-tune\nwav2vec $2.0$ models for $18$ Indic languages and adjust the results with\nlanguage models trained on text derived from a variety of sources. Our findings\ndemonstrate that the average Character Error Rate (CER) decreases by over $28$\n\\% and the average Word Error Rate (WER) decreases by about $36$ \\% after\ndecoding with LM. We show that a large LM may not provide a substantial\nimprovement as compared to a diverse one. We also demonstrate that high quality\ntranscriptions can be obtained on domain-specific data without retraining the\nASR model and show results on biomedical domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Word Error Rate a good evaluation metric for Speech Recognition in Indic Languages?. (arXiv:2203.16601v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16601","description":"<p>We propose a new method for the calculation of error rates in Automatic\nSpeech Recognition (ASR). This new metric is for languages that contain half\ncharacters and where the same character can be written in different forms. We\nimplement our methodology in Hindi which is one of the main languages from\nIndic context and we think this approach is scalable to other similar languages\ncontaining a large character set. We call our metrics Alternate Word Error Rate\n(AWER) and Alternate Character Error Rate (ACER).\n</p>\n<p>We train our ASR models using wav2vec 2.0\\cite{baevski2020wav2vec} for Indic\nlanguages. Additionally we use language models to improve our model\nperformance. Our results show a significant improvement in analyzing the error\nrates at word and character level and the interpretability of the ASR system is\nimproved upto $3$\\% in AWER and $7$\\% in ACER for Hindi. Our experiments\nsuggest that in languages which have complex pronunciation, there are multiple\nways of writing words without changing their meaning. In such cases AWER and\nACER will be more useful rather than WER and CER as metrics. Further, we open\nsource a new benchmarking dataset of 21 hours for Hindi with the new metric\nscripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Priyanshi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chadha_H/0/1/0/all/0/1\">Harveen Singh Chadha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anirudh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuriya_A/0/1/0/all/0/1\">Ankur Dhuriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhimwal_N/0/1/0/all/0/1\">Neeraj Chhimwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_R/0/1/0/all/0/1\">Rishabh Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raghavan_V/0/1/0/all/0/1\">Vivek Raghavan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTuit: Understanding Spanish language in Twitter through a native transformer. (arXiv:2204.03465v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03465","description":"<p>The appearance of complex attention-based language models such as BERT,\nRoberta or GPT-3 has allowed to address highly complex tasks in a plethora of\nscenarios. However, when applied to specific domains, these models encounter\nconsiderable difficulties. This is the case of Social Networks such as Twitter,\nan ever-changing stream of information written with informal and complex\nlanguage, where each message requires careful evaluation to be understood even\nby humans given the important role that context plays. Addressing tasks in this\ndomain through Natural Language Processing involves severe challenges. When\npowerful state-of-the-art multilingual language models are applied to this\nscenario, language specific nuances use to get lost in translation. To face\nthese challenges we present \\textbf{BERTuit}, the larger transformer proposed\nso far for Spanish language, pre-trained on a massive dataset of 230M Spanish\ntweets using RoBERTa optimization. Our motivation is to provide a powerful\nresource to better understand Spanish Twitter and to be used on applications\nfocused on this social network, with special emphasis on solutions devoted to\ntackle the spreading of misinformation in this platform. BERTuit is evaluated\non several tasks and compared against M-BERT, XLM-RoBERTa and XLM-T, very\ncompetitive multilingual transformers. The utility of our approach is shown\nwith applications, in this case: a zero-shot methodology to visualize groups of\nhoaxes and profiling authors spreading disinformation.\n</p>\n<p>Misinformation spreads wildly on platforms such as Twitter in languages other\nthan English, meaning performance of transformers may suffer when transferred\noutside English speaking communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1\">Javier Huertas-Tato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1\">Alejandro Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1\">David Camacho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05515","description":"<p>Compared with unimodal data, multimodal data can provide more features to\nhelp the model analyze the sentiment of data. Previous research works rarely\nconsider token-level feature fusion, and few works explore learning the common\nfeatures related to sentiment in multimodal data to help the model fuse\nmultimodal features. In this paper, we propose a Contrastive Learning and\nMulti-Layer Fusion (CLMLF) method for multimodal sentiment detection.\nSpecifically, we first encode text and image to obtain hidden representations,\nand then use a multi-layer fusion module to align and fuse the token-level\nfeatures of text and image. In addition to the sentiment analysis task, we also\ndesigned two contrastive learning tasks, label based contrastive learning and\ndata based contrastive learning tasks, which will help the model learn common\nfeatures related to sentiment in multimodal data. Extensive experiments\nconducted on three publicly available multimodal datasets demonstrate the\neffectiveness of our approach for multimodal sentiment detection compared with\nexisting methods. The codes are available for use at\nhttps://github.com/Link-Li/CLMLF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Conghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UTNLP at SemEval-2022 Task 6: A Comparative Analysis of Sarcasm Detection using generative-based and mutation-based data augmentation. (arXiv:2204.08198v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08198","description":"<p>Sarcasm is a term that refers to the use of words to mock, irritate, or amuse\nsomeone. It is commonly used on social media. The metaphorical and creative\nnature of sarcasm presents a significant difficulty for sentiment analysis\nsystems based on affective computing. The methodology and results of our team,\nUTNLP, in the SemEval-2022 shared task 6 on sarcasm detection are presented in\nthis paper. We put different models, and data augmentation approaches to the\ntest and report on which one works best. The tests begin with traditional\nmachine learning models and progress to transformer-based and attention-based\nmodels. We employed data augmentation based on data mutation and data\ngeneration. Using RoBERTa and mutation-based data augmentation, our best\napproach achieved an F1-sarcastic of 0.38 in the competition's evaluation\nphase. After the competition, we fixed our model's flaws and achieved an\nF1-sarcastic of 0.414.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1\">Amirhossein Abaskohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1\">Arash Rasouli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeraati_T/0/1/0/all/0/1\">Tanin Zeraati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrak_B/0/1/0/all/0/1\">Behnam Bahrak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUP: Curriculum Learning based Prompt Tuning for Implicit Event Argument Extraction. (arXiv:2205.00498v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.00498","description":"<p>Implicit event argument extraction (EAE) aims to identify arguments that\ncould scatter over the document. Most previous work focuses on learning the\ndirect relations between arguments and the given trigger, while the implicit\nrelations with long-range dependency are not well studied. Moreover, recent\nneural network based approaches rely on a large amount of labeled data for\ntraining, which is unavailable due to the high labelling cost. In this paper,\nwe propose a Curriculum learning based Prompt tuning (CUP) approach, which\nresolves implicit EAE by four learning stages. The stages are defined according\nto the relations with the trigger node in a semantic graph, which well captures\nthe long-range dependency between arguments and the trigger. In addition, we\nintegrate a prompt-based encoder-decoder model to elicit related knowledge from\npre-trained language models (PLMs) in each stage, where the prompt templates\nare adapted with the learning progress to enhance the reasoning for arguments.\nExperimental results on two well-known benchmark datasets show the great\nadvantages of our proposed approach. In particular, we outperform the\nstate-of-the-art models in both fully-supervised and low-data scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jian Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemAttack: Natural Textual Attacks via Different Semantic Spaces. (arXiv:2205.01287v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.01287","description":"<p>Recent studies show that pre-trained language models (LMs) are vulnerable to\ntextual adversarial attacks. However, existing attack methods either suffer\nfrom low attack success rates or fail to search efficiently in the\nexponentially large perturbation space. We propose an efficient and effective\nframework SemAttack to generate natural adversarial text by constructing\ndifferent semantic perturbation functions. In particular, SemAttack optimizes\nthe generated perturbations constrained on generic semantic spaces, including\ntypo space, knowledge space (e.g., WordNet), contextualized semantic space\n(e.g., the embedding space of BERT clusterings), or the combination of these\nspaces. Thus, the generated adversarial texts are more semantically close to\nthe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)\nlarge-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are\nstill vulnerable to SemAttack. We further demonstrate that SemAttack is general\nand able to generate natural adversarial texts for different languages (e.g.,\nEnglish and Chinese) with high attack success rates. Human evaluations also\nconfirm that our generated adversarial texts are natural and barely affect\nhuman performance. Our code is publicly available at\nhttps://github.com/AI-secure/SemAttack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Boxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chejian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Models to Express Their Uncertainty in Words. (arXiv:2205.14334v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.14334","description":"<p>We show that a GPT-3 model can learn to express uncertainty about its own\nanswers in natural language -- without use of model logits. When given a\nquestion, the model generates both an answer and a level of confidence (e.g.\n\"90% confidence\" or \"high confidence\"). These levels map to probabilities that\nare well calibrated. The model also remains moderately calibrated under\ndistribution shift, and is sensitive to uncertainty in its own answers, rather\nthan imitating human examples. To our knowledge, this is the first time a model\nhas been shown to express calibrated uncertainty about its own answers in\nnatural language. For testing calibration, we introduce the CalibratedMath\nsuite of tasks. We compare the calibration of uncertainty expressed in words\n(\"verbalized probability\") to uncertainty extracted from model logits. Both\nkinds of uncertainty are capable of generalizing calibration under distribution\nshift. We also provide evidence that GPT-3's ability to generalize calibration\ndepends on pre-trained latent representations that correlate with epistemic\nuncertainty over its answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephanie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilton_J/0/1/0/all/0/1\">Jacob Hilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_O/0/1/0/all/0/1\">Owain Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Aware Evaluation and Transformer Methods for Topic-Controllable Summarization. (arXiv:2206.04317v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04317","description":"<p>Topic-controllable summarization is an emerging research area with a wide\nrange of potential applications. However, existing approaches suffer from\nsignificant limitations. First, there is currently no established evaluation\nmetric for this task. Furthermore, existing methods built upon recurrent\narchitectures, which can significantly limit their performance compared to more\nrecent Transformer-based architectures, while they also require modifications\nto the model's architecture for controlling the topic. In this work, we propose\na new topic-oriented evaluation measure to automatically evaluate the generated\nsummaries based on the topic affinity between the generated summary and the\ndesired topic. We also conducted a user study that validates the reliability of\nthis measure. Finally, we propose simple, yet powerful methods for\ntopic-controllable summarization either incorporating topic embeddings into the\nmodel's architecture or employing control tokens to guide the summary\ngeneration. Experimental results show that control tokens can achieve better\nperformance compared to more complicated embedding-based approaches while being\nat the same time significantly faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passali_T/0/1/0/all/0/1\">Tatiana Passali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SsciBERT: A Pre-trained Language Model for Social Science Texts. (arXiv:2206.04510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04510","description":"<p>The academic literature of social sciences is the literature that records\nhuman civilization and studies human social problems. With the large-scale\ngrowth of this literature, ways to quickly find existing research on relevant\nissues have become an urgent demand for researchers. Previous studies, such as\nSciBERT, have shown that pre-training using domain-specific texts can improve\nthe performance of natural language processing tasks in those fields. However,\nthere is no pre-trained language model for social sciences, so this paper\nproposes a pre-trained model on many abstracts published in the Social Science\nCitation Index (SSCI) journals. The models, which are available on Github\n(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent\nperformance on discipline classification and abstract structure-function\nrecognition tasks with the social sciences literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Si Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Litao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongbo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensional Modeling of Emotions in Text with Appraisal Theories: Corpus Creation, Annotation Reliability, and Prediction. (arXiv:2206.05238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.05238","description":"<p>The most prominent tasks in emotion analysis are to assign emotions to texts\nand to understand how emotions manifest in language. An important observation\nfor natural language processing is that emotions can be communicated implicitly\nby referring to events alone, appealing to an empathetic, intersubjective\nunderstanding of events, even without explicitly mentioning an emotion name. In\npsychology, the class of emotion theories known as appraisal theories aims at\nexplaining the link between events and emotions. Appraisals can be formalized\nas variables that measure a cognitive evaluation by people living through an\nevent that they consider relevant. They include the assessment if an event is\nnovel, if the person considers themselves to be responsible, if it is in line\nwith the own goals, and many others. Such appraisals explain which emotions are\ndeveloped based on an event, e.g., that a novel situation can induce surprise\nor one with uncertain consequences could evoke fear. We analyze the suitability\nof appraisal theories for emotion analysis in text with the goal of\nunderstanding if appraisal concepts can reliably be reconstructed by\nannotators, if they can be predicted by text classifiers, and if appraisal\nconcepts help to identify emotion categories. To achieve that, we compile a\ncorpus by asking people to textually describe events that triggered particular\nemotions and to disclose their appraisals. Then, we ask readers to reconstruct\nemotions and appraisals from the text. This setup allows us to measure if\nemotions and appraisals can be recovered purely from text and provides a human\nbaseline to judge model's performance measures. Our comparison of text\nclassification methods to human annotators shows that both can reliably detect\nemotions and appraisals with similar performance. We further show that\nappraisal concepts improve the categorization of emotions in text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troiano_E/0/1/0/all/0/1\">Enrica Troiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberlander_L/0/1/0/all/0/1\">Laura Oberl&#xe4;nder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klinger_R/0/1/0/all/0/1\">Roman Klinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Spatial-temporal Concept based Explanation of 3D ConvNets. (arXiv:2206.05275v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05275","description":"<p>Recent studies have achieved outstanding success in explaining 2D image\nrecognition ConvNets. On the other hand, due to the computation cost and\ncomplexity of video data, the explanation of 3D video recognition ConvNets is\nrelatively less studied. In this paper, we present a 3D ACE (Automatic\nConcept-based Explanation) framework for interpreting 3D ConvNets. In our\napproach: (1) videos are represented using high-level supervoxels, which is\nstraightforward for human to understand; and (2) the interpreting framework\nestimates a score for each voxel, which reflects its importance in the decision\nprocedure. Experiments show that our method can discover spatial-temporal\nconcepts of different importance-levels, and thus can explore the influence of\nthe concepts on a target task, such as action classification, in-depth. The\ncodes are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Ying Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1\">Kensaku Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kato_J/0/1/0/all/0/1\">Jien Kato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superresolution and Segmentation of OCT scans using Multi-Stage adversarial Guided Attention Training. (arXiv:2206.05277v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05277","description":"<p>Optical coherence tomography (OCT) is one of the non-invasive and\neasy-to-acquire biomarkers (the thickness of the retinal layers, which is\ndetectable within OCT scans) being investigated to diagnose Alzheimer's disease\n(AD). This work aims to segment the OCT images automatically; however, it is a\nchallenging task due to various issues such as the speckle noise, small target\nregion, and unfavorable imaging conditions. In our previous work, we have\nproposed the multi-stage &amp; multi-discriminatory generative adversarial network\n(MultiSDGAN) to translate OCT scans in high-resolution segmentation labels. In\nthis investigation, we aim to evaluate and compare various combinations of\nchannel and spatial attention to the MultiSDGAN architecture to extract more\npowerful feature maps by capturing rich contextual relationships to improve\nsegmentation performance. Moreover, we developed and evaluated a guided\nmutli-stage attention framework where we incorporated a guided attention\nmechanism by forcing an L-1 loss between a specifically designed binary mask\nand the generated attention maps. Our ablation study results on the WVU-OCT\ndata-set in five-fold cross-validation (5-CV) suggest that the proposed\nMultiSDGAN with a serial attention module provides the most competitive\nperformance, and guiding the spatial attention feature maps by binary masks\nfurther improves the performance in our proposed network. Comparing the\nbaseline model with adding the guided-attention, our results demonstrated\nrelative improvements of 21.44% and 19.45% on the Dice coefficient and SSIM,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeihouni_P/0/1/0/all/0/1\">Paria Jeihouni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehzangi_O/0/1/0/all/0/1\">Omid Dehzangi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amireskandari_A/0/1/0/all/0/1\">Annahita Amireskandari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dabouei_A/0/1/0/all/0/1\">Ali Dabouei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rezai_A/0/1/0/all/0/1\">Ali Rezai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT. (arXiv:2206.05278v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05278","description":"<p>Single-photon emission computed tomography (SPECT) is a widely applied\nimaging approach for diagnosis of coronary artery diseases. Attenuation maps\n(u-maps) derived from computed tomography (CT) are utilized for attenuation\ncorrection (AC) to improve diagnostic accuracy of cardiac SPECT. However, SPECT\nand CT are obtained sequentially in clinical practice, which potentially\ninduces misregistration between the two scans. Convolutional neural networks\n(CNN) are powerful tools for medical image registration. Previous CNN-based\nmethods for cross-modality registration either directly concatenated two input\nmodalities as an early feature fusion or extracted image features using two\nseparate CNN modules for a late fusion. These methods do not fully extract or\nfuse the cross-modality information. Besides, deep-learning-based rigid\nregistration of cardiac SPECT and CT-derived u-maps has not been investigated\nbefore. In this paper, we propose a Dual-Branch Squeeze-Fusion-Excitation\n(DuSFE) module for the registration of cardiac SPECT and CT-derived u-maps.\nDuSFE fuses the knowledge from multiple modalities to recalibrate both\nchannel-wise and spatial features for each modality. DuSFE can be embedded at\nmultiple convolutional layers to enable feature fusion at different spatial\ndimensions. Our studies using clinical data demonstrated that a network\nembedded with DuSFE generated substantial lower registration errors and\ntherefore more accurate AC SPECT images than previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiongchao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_H/0/1/0/all/0/1\">Huidong Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1\">Xueqi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiazhen Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinusas_A/0/1/0/all/0/1\">Albert J. Sinusas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Onofrey_J/0/1/0/all/0/1\">John A. Onofrey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+liu_C/0/1/0/all/0/1\">Chi liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PILC: Practical Image Lossless Compression with an End-to-end GPU Oriented Neural Framework. (arXiv:2206.05279v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05279","description":"<p>Generative model based image lossless compression algorithms have seen a\ngreat success in improving compression ratio. However, the throughput for most\nof them is less than 1 MB/s even with the most advanced AI accelerated chips,\npreventing them from most real-world applications, which often require 100\nMB/s. In this paper, we propose PILC, an end-to-end image lossless compression\nframework that achieves 200 MB/s for both compression and decompression with a\nsingle NVIDIA Tesla V100 GPU, 10 times faster than the most efficient one\nbefore. To obtain this result, we first develop an AI codec that combines\nauto-regressive model and VQ-VAE which performs well in lightweight setting,\nthen we design a low complexity entropy coder that works well with our codec.\nExperiments show that our framework compresses better than PNG by a margin of\n30% in multiple datasets. We believe this is an important step to bring AI\ncompression forward to commercial use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kang_N/0/1/0/all/0/1\">Ning Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_S/0/1/0/all/0/1\">Shanzhao Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shifeng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_S/0/1/0/all/0/1\">Shutao Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less Is More: Linear Layers on CLIP Features as Powerful VizWiz Model. (arXiv:2206.05281v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05281","description":"<p>Current architectures for multi-modality tasks such as visual question\nanswering suffer from their high complexity. As a result, these architectures\nare difficult to train and require high computational resources. To address\nthese problems we present a CLIP-based architecture that does not require any\nfine-tuning of the feature extractors. A simple linear classifier is used on\nthe concatenated features of the image and text encoder. During training an\nauxiliary loss is added which operates on the answer types. The resulting\nclassification is then used as an attention gate on the answer class selection.\nOn the VizWiz 2022 Visual Question Answering Challenge we achieve 60.15 %\naccuracy on Task 1: Predict Answer to a Visual Question and AP score of 83.78 %\non Task 2: Predict Answerability of a Visual Question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deuser_F/0/1/0/all/0/1\">Fabian Deuser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habel_K/0/1/0/all/0/1\">Konrad Habel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosch_P/0/1/0/all/0/1\">Philipp J. R&#xf6;sch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_N/0/1/0/all/0/1\">Norbert Oswald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Estimate Shapley Values with Vision Transformers. (arXiv:2206.05282v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05282","description":"<p>Transformers have become a default architecture in computer vision, but\nunderstanding what drives their predictions remains a challenging problem.\nCurrent explanation approaches rely on attention values or input gradients, but\nthese give a limited understanding of a model's dependencies. Shapley values\noffer a theoretically sound alternative, but their computational cost makes\nthem impractical for large, high-dimensional models. In this work, we aim to\nmake Shapley values practical for vision transformers (ViTs). To do so, we\nfirst leverage an attention masking approach to evaluate ViTs with partial\ninformation, and we then develop a procedure for generating Shapley value\nexplanations via a separate, learned explainer model. Our experiments compare\nShapley values to many baseline methods (e.g., attention rollout, GradCAM,\nLRP), and we find that our approach provides more accurate explanations than\nany existing method for ViTs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Su-In Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Poissonian Blurred Image Deconvolution by Framelet based Local Minimal Prior. (arXiv:2206.05283v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05283","description":"<p>Image production tools do not always create a clear image, noisy and blurry\nimages are sometimes created. Among these cases, Poissonian noise is one of the\nmost famous noises that appear in medical images and images taken in astronomy.\nBlurred image with Poissonian noise obscures important details that are of\ngreat importance in medicine or astronomy. Therefore, studying and increasing\nthe quality of images that are affected by this type of noise is always\nconsidered by researchers. In this paper, in the first step, based on framelet\ntransform, a local minimal prior is introduced, and in the next step, this tool\ntogether with fractional calculation is used for Poissonian blurred image\ndeconvolution. In the following, the model is generalized to the blind case. To\nevaluate the performance of the presented model, several images such as real\nimages have been investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Parvaz_R/0/1/0/all/0/1\">Reza Parvaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation. (arXiv:2206.05284v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05284","description":"<p>Distributed learning has shown great potential in medical image analysis. It\nallows to use multi-center training data with privacy protection. However, data\ndistributions in local centers can vary from each other due to different\nimaging vendors, and annotation protocols. Such variation degrades the\nperformance of learning-based methods. To mitigate the influence, two groups of\nmethods have been proposed for different aims, i.e., the global methods and the\npersonalized methods. The former are aimed to improve the performance of a\nsingle global model for all test data from unseen centers (known as generic\ndata); while the latter target multiple models for each center (denoted as\nlocal data). However, little has been researched to achieve both goals\nsimultaneously. In this work, we propose a new framework of distributed\nlearning that bridges the gap between two groups, and improves the performance\nfor both generic and local data. Specifically, our method decouples the\npredictions for generic data and local data, via distribution-conditioned\nadaptation matrices. Results on multi-center left atrial (LA) MRI segmentation\nshowed that our method demonstrated superior performance over existing methods\non both generic and local data. Our code is available at\nhttps://github.com/key1589745/decouple_predict\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_Z/0/1/0/all/0/1\">Zheyao Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_F/0/1/0/all/0/1\">Fuping Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sihan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Labels to Priors in Capsule Endoscopy: A Prior Guided Approach for Improving Generalization with Few Labels. (arXiv:2206.05288v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05288","description":"<p>The lack of generalizability of deep learning approaches for the automated\ndiagnosis of pathologies in Wireless Capsule Endoscopy (WCE) has prevented any\nsignificant advantages from trickling down to real clinical practices. As a\nresult, disease management using WCE continues to depend on exhaustive manual\ninvestigations by medical experts. This explains its limited use despite\nseveral advantages. Prior works have considered using higher quality and\nquantity of labels as a way of tackling the lack of generalization, however\nthis is hardly scalable considering pathology diversity not to mention that\nlabeling large datasets encumbers the medical staff additionally. We propose\nusing freely available domain knowledge as priors to learn more robust and\ngeneralizable representations. We experimentally show that domain priors can\nbenefit representations by acting in proxy of labels, thereby significantly\nreducing the labeling requirement while still enabling fully unsupervised yet\npathology-aware learning. We use the contrastive objective along with\nprior-guided views during pretraining, where the view choices inspire\nsensitivity to pathological information. Extensive experiments on three\ndatasets show that our method performs better than (or closes gap with) the\nstate-of-the-art in the domain, establishing a new benchmark in pathology\nclassification and cross-dataset generalization, as well as scaling to unseen\npathology categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vats_A/0/1/0/all/0/1\">Anuja Vats</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammed_A/0/1/0/all/0/1\">Ahmed Mohammed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedersen_M/0/1/0/all/0/1\">Marius Pedersen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localized adversarial artifacts for compressed sensing MRI. (arXiv:2206.05289v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05289","description":"<p>As interest in deep neural networks (DNNs) for image reconstruction tasks\ngrows, their reliability has been called into question (Antun et al., 2020;\nGottschling et al., 2020). However, recent work has shown that compared to\ntotal variation (TV) minimization, they show similar robustness to adversarial\nnoise in terms of $\\ell^2$-reconstruction error (Genzel et al., 2022). We\nconsider a different notion of robustness, using the $\\ell^\\infty$-norm, and\nargue that localized reconstruction artifacts are a more relevant defect than\nthe $\\ell^2$-error. We create adversarial perturbations to undersampled MRI\nmeasurements which induce severe localized artifacts in the TV-regularized\nreconstruction. The same attack method is not as effective against DNN based\nreconstruction. Finally, we show that this phenomenon is inherent to\nreconstruction methods for which exact recovery can be guaranteed, as with\ncompressed sensing reconstructions with $\\ell^1$- or TV-minimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alaifari_R/0/1/0/all/0/1\">Rima Alaifari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alberti_G/0/1/0/all/0/1\">Giovanni S. Alberti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gauksson_T/0/1/0/all/0/1\">Tandri Gauksson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProActive: Self-Attentive Temporal Point Process Flows for Activity Sequences. (arXiv:2206.05291v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05291","description":"<p>Any human activity can be represented as a temporal sequence of actions\nperformed to achieve a certain goal. Unlike machine-made time series, these\naction sequences are highly disparate as the time taken to finish a similar\naction might vary between different persons. Therefore, understanding the\ndynamics of these sequences is essential for many downstream tasks such as\nactivity length prediction, goal prediction, etc. Existing neural approaches\nthat model an activity sequence are either limited to visual data or are task\nspecific, i.e., limited to next action or goal prediction. In this paper, we\npresent ProActive, a neural marked temporal point process (MTPP) framework for\nmodeling the continuous-time distribution of actions in an activity sequence\nwhile simultaneously addressing three high-impact problems -- next action\nprediction, sequence-goal prediction, and end-to-end sequence generation.\nSpecifically, we utilize a self-attention module with temporal normalizing\nflows to model the influence and the inter-arrival times between actions in a\nsequence. Moreover, for time-sensitive prediction, we perform an early\ndetection of sequence goal via a constrained margin-based optimization\nprocedure. This in-turn allows ProActive to predict the sequence goal using a\nlimited number of actions. Extensive experiments on sequences derived from\nthree activity recognition datasets show the significant accuracy boost of\nProActive over the state-of-the-art in terms of action and goal prediction, and\nthe first-ever application of end-to-end action sequence generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vinayak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EigenFairing: 3D Model Fairing using Image Coherence. (arXiv:2206.05309v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05309","description":"<p>A surface is often modeled as a triangulated mesh of 3D points and textures\nassociated with faces of the mesh. The 3D points could be either sampled from\nrange data or derived from a set of images using a stereo or\nStructure-from-Motion algorithm. When the points do not lie at critical points\nof maximum curvature or discontinuities of the real surface, faces of the mesh\ndo not lie close to the modeled surface. This results in textural artifacts,\nand the model is not perfectly coherent with a set of actual images -- the ones\nthat are used to texture-map its mesh. This paper presents a technique for\nperfecting the 3D surface model by repositioning its vertices so that it is\ncoherent with a set of observed images of the object. The textural artifacts\nand incoherence with images are due to the non-planarity of a surface patch\nbeing approximated by a planar face, as observed from multiple viewpoints.\nImage areas from the viewpoints are used to represent texture for the patch in\nEigenspace. The Eigenspace representation captures variations of texture, which\nwe seek to minimize. A coherence measure based on the difference between the\nface textures reconstructed from Eigenspace and the actual images is used to\nreposition the vertices so that the model is improved or faired. We refer to\nthis technique of model refinement as EigenFairing, by which the model is\nfaired, both geometrically and texturally, to better approximate the real\nsurface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pragyana Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amidi_O/0/1/0/all/0/1\">Omead Amidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanade_T/0/1/0/all/0/1\">Takeo Kanade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Instance Identification in Dynamic Environments. (arXiv:2206.05319v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05319","description":"<p>We study the problem of identifying object instances in a dynamic environment\nwhere people interact with the objects. In such an environment, objects'\nappearance changes dynamically by interaction with other entities, occlusion by\nhands, background change, etc. This leads to a larger intra-instance variation\nof appearance than in static environments. To discover the challenges in this\nsetting, we newly built a benchmark of more than 1,500 instances built on the\nEPIC-KITCHENS dataset which includes natural activities and conducted an\nextensive analysis of it. Experimental results suggest that (i) robustness\nagainst instance-specific appearance change (ii) integration of low-level\n(e.g., color, texture) and high-level (e.g., object category) features (iii)\nforeground feature selection on overlapping objects are required for further\nimprovement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1\">Takuma Yagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md Tasnimul Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Classifiers: Two-stage Classification for Robustness in Machine Learning. (arXiv:2206.05323v1 [cs.LG])","link":"http://arxiv.org/abs/2206.05323","description":"<p>The performance of machine learning models can significantly degrade under\ndistribution shifts of the data. We propose a new method for classification\nwhich can improve robustness to distribution shifts, by combining expert\nknowledge about the ``high-level\" structure of the data with standard\nclassifiers. Specifically, we introduce two-stage classifiers called\n\\textit{memory classifiers}. First, these identify prototypical data points --\n\\textit{memories} -- to cluster the training data. This step is based on\nfeatures designed with expert guidance; for instance, for image data they can\nbe extracted using digital image processing algorithms. Then, within each\ncluster, we learn local classifiers based on finer discriminating features, via\nstandard models like deep neural networks. We establish generalization bounds\nfor memory classifiers. We illustrate in experiments that they can improve\ngeneralization and robustness to distribution shifts on image datasets. We show\nimprovements which push beyond standard data augmentation techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Souradeep Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yahan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardis_E/0/1/0/all/0/1\">Elena Bernardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobriban_E/0/1/0/all/0/1\">Edgar Dobriban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Rendering of Neural SDFs through Reparameterization. (arXiv:2206.05344v1 [cs.GR])","link":"http://arxiv.org/abs/2206.05344","description":"<p>We present a method to automatically compute correct gradients with respect\nto geometric scene parameters in neural SDF renderers. Recent physically-based\ndifferentiable rendering techniques for meshes have used edge-sampling to\nhandle discontinuities, particularly at object silhouettes, but SDFs do not\nhave a simple parametric form amenable to sampling. Instead, our approach\nbuilds on area-sampling techniques and develops a continuous warping function\nfor SDFs to account for these discontinuities. Our method leverages the\ndistance to surface encoded in an SDF and uses quadrature on sphere tracer\npoints to compute this warping function. We further show that this can be done\nby subsampling the points to make the method tractable for neural SDFs. Our\ndifferentiable renderer can be used to optimize neural shapes from multi-view\nimages and produces comparable 3D reconstructions to recent SDF-based inverse\nrendering methods, without the need for 2D segmentation masks to guide the\ngeometry optimization and no volumetric approximations to the geometry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bangaru_S/0/1/0/all/0/1\">Sai Praveen Bangaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1\">Micha&#xeb;l Gharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tzu-Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_F/0/1/0/all/0/1\">Fujun Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Milo&#x161; Ha&#x161;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_S/0/1/0/all/0/1\">Sai Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zexiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_G/0/1/0/all/0/1\">Gilbert Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fr&#xe9;do Durand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Detection, Recognition, Deep Learning, and the Universal Law of Generalization. (arXiv:2206.05365v1 [cs.LG])","link":"http://arxiv.org/abs/2206.05365","description":"<p>Object detection and recognition are fundamental functions underlying the\nsuccess of species. Because the appearance of an object exhibits a large\nvariability, the brain has to group these different stimuli under the same\nobject identity, a process of generalization. Does the process of\ngeneralization follow some general principles or is it an ad-hoc\n\"bag-of-tricks\"? The Universal Law of Generalization provided evidence that\ngeneralization follows similar properties across a variety of species and\ntasks. Here we test the hypothesis that the internal representations underlying\ngeneralization reflect the natural properties of object detection and\nrecognition in our environment rather than the specifics of the system solving\nthese problems. By training a deep-neural-network with images of \"clear\" and\n\"camouflaged\" animals, we found that with a proper choice of category\nprototypes, the generalization functions are monotone decreasing, similar to\nthe generalization functions of biological systems. Our findings support the\nhypothesis of the study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rustom_F/0/1/0/all/0/1\">Faris B. Rustom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ogmen_H/0/1/0/all/0/1\">Haluk &#xd6;&#x11f;men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdanbakhsh_A/0/1/0/all/0/1\">Arash Yazdanbakhsh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Neural Radiance Fields for Novel View Synthesis with Transformer. (arXiv:2206.05375v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05375","description":"<p>We propose a Transformer-based NeRF (TransNeRF) to learn a generic neural\nradiance field conditioned on observed-view images for the novel view synthesis\ntask. By contrast, existing MLP-based NeRFs are not able to directly receive\nobserved views with an arbitrary number and require an auxiliary pooling-based\noperation to fuse source-view information, resulting in the missing of\ncomplicated relationships between source views and the target rendering view.\nFurthermore, current approaches process each 3D point individually and ignore\nthe local consistency of a radiance field scene representation. These\nlimitations potentially can reduce their performance in challenging real-world\napplications where large differences between source views and a novel rendering\nview may exist. To address these challenges, our TransNeRF utilizes the\nattention mechanism to naturally decode deep associations of an arbitrary\nnumber of source views into a coordinate-based scene representation. Local\nconsistency of shape and appearance are considered in the ray-cast space and\nthe surrounding-view space within a unified Transformer network. Experiments\ndemonstrate that our TransNeRF, trained on a wide variety of scenes, can\nachieve better performance in comparison to state-of-the-art image-based neural\nrendering methods in both scene-agnostic and per-scene finetuning scenarios\nespecially when there is a considerable gap between source views and a\nrendering view.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xinrui Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salcudean_S/0/1/0/all/0/1\">Septimiu Salcudean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast building segmentation from satellite imagery and few local labels. (arXiv:2206.05377v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05377","description":"<p>Innovations in computer vision algorithms for satellite image analysis can\nenable us to explore global challenges such as urbanization and land use change\nat the planetary level. However, domain shift problems are a common occurrence\nwhen trying to replicate models that drive these analyses to new areas,\nparticularly in the developing world. If a model is trained with imagery and\nlabels from one location, then it usually will not generalize well to new\nlocations where the content of the imagery and data distributions are\ndifferent. In this work, we consider the setting in which we have a single\nlarge satellite imagery scene over which we want to solve an applied problem --\nbuilding footprint segmentation. Here, we do not necessarily need to worry\nabout creating a model that generalizes past the borders of our scene but can\ninstead train a local model. We show that surprisingly few labels are needed to\nsolve the building segmentation problem with very high-resolution (0.5m/px)\nsatellite imagery with this setting in mind. Our best model trained with just\n527 sparse polygon annotations (an equivalent of 1500 x 1500 densely labeled\npixels) has a recall of 0.87 over held out footprints and a R2 of 0.93 on the\ntask of counting the number of buildings in 200 x 200-meter windows. We apply\nour models over high-resolution imagery in Amman, Jordan in a case study on\nurban change detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hogeun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gracia_N/0/1/0/all/0/1\">Nancy Lozano Gracia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaw_J/0/1/0/all/0/1\">Jon Kher Kaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sederholm_T/0/1/0/all/0/1\">Tina Sederholm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1\">Rahul Dodhia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark for Compositional Visual Reasoning. (arXiv:2206.05379v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05379","description":"<p>A fundamental component of human vision is our ability to parse complex\nvisual scenes and judge the relations between their constituent objects. AI\nbenchmarks for visual reasoning have driven rapid progress in recent years with\nstate-of-the-art systems now reaching human accuracy on some of these\nbenchmarks. Yet, a major gap remains in terms of the sample efficiency with\nwhich humans and AI systems learn new visual reasoning tasks. Humans'\nremarkable efficiency at learning has been at least partially attributed to\ntheir ability to harness compositionality -- such that they can efficiently\ntake advantage of previously gained knowledge when learning new tasks. Here, we\nintroduce a novel visual reasoning benchmark, Compositional Visual Relations\n(CVR), to drive progress towards the development of more data-efficient\nlearning algorithms. We take inspiration from fluidic intelligence and\nnon-verbal reasoning tests and describe a novel method for creating\ncompositions of abstract rules and associated image datasets at scale. Our\nproposed benchmark includes measures of sample efficiency, generalization and\ntransfer across task rules, as well as the ability to leverage\ncompositionality. We systematically evaluate modern neural architectures and\nfind that, surprisingly, convolutional architectures surpass transformer-based\narchitectures across all performance measures in most data regimes. However,\nall computational models are a lot less data efficient compared to humans even\nafter learning informative visual representations using self-supervision.\nOverall, we hope that our challenge will spur interest in the development of\nneural architectures that can learn to harness compositionality toward more\nefficient learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zerroug_A/0/1/0/all/0/1\">Aimen Zerroug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaishnav_M/0/1/0/all/0/1\">Mohit Vaishnav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colin_J/0/1/0/all/0/1\">Julien Colin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musslick_S/0/1/0/all/0/1\">Sebastian Musslick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer-based Self-Supervised Fish Segmentation in Underwater Videos. (arXiv:2206.05390v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05390","description":"<p>Underwater fish segmentation to estimate fish body measurements is still\nlargely unsolved due to the complex underwater environment. Relying on\nfully-supervised segmentation models requires collecting per-pixel labels,\nwhich is time-consuming and prone to overfitting. Self-supervised learning\nmethods can help avoid the requirement of large annotated training datasets,\nhowever, to be useful in real-world applications, they should achieve good\nsegmentation quality. In this paper, we introduce a Transformer-based method\nthat uses self-supervision for high-quality fish segmentation. Our proposed\nmodel is trained on videos -- without any annotations -- to perform fish\nsegmentation in underwater videos taken in situ in the wild. We show that when\ntrained on a set of underwater videos from one dataset, the proposed model\nsurpasses previous CNN-based and Transformer-based self-supervised methods and\nachieves performance relatively close to supervised methods on two new unseen\nunderwater video datasets. This demonstrates the great generalisability of our\nmodel and the fact that it does not need a pre-trained model. In addition, we\nshow that, due to its dense representation learning, our model is\ncompute-efficient. We provide quantitative and qualitative results that\ndemonstrate our model's significant capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_A/0/1/0/all/0/1\">Alzayat Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheaves_M/0/1/0/all/0/1\">Marcus Sheaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerry_D/0/1/0/all/0/1\">Dean Jerry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1\">Mostafa Rahimi Azghadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applications of Deep Learning in Fish Habitat Monitoring: A Tutorial and Survey. (arXiv:2206.05394v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05394","description":"<p>Marine ecosystems and their fish habitats are becoming increasingly important\ndue to their integral role in providing a valuable food source and conservation\noutcomes. Due to their remote and difficult to access nature, marine\nenvironments and fish habitats are often monitored using underwater cameras.\nThese cameras generate a massive volume of digital data, which cannot be\nefficiently analysed by current manual processing methods, which involve a\nhuman observer. DL is a cutting-edge AI technology that has demonstrated\nunprecedented performance in analysing visual data. Despite its application to\na myriad of domains, its use in underwater fish habitat monitoring remains\nunder explored. In this paper, we provide a tutorial that covers the key\nconcepts of DL, which help the reader grasp a high-level understanding of how\nDL works. The tutorial also explains a step-by-step procedure on how DL\nalgorithms should be developed for challenging applications such as underwater\nfish monitoring. In addition, we provide a comprehensive survey of key deep\nlearning techniques for fish habitat monitoring including classification,\ncounting, localization, and segmentation. Furthermore, we survey publicly\navailable underwater fish datasets, and compare various DL techniques in the\nunderwater fish monitoring domains. We also discuss some challenges and\nopportunities in the emerging field of deep learning for fish habitat\nprocessing. This paper is written to serve as a tutorial for marine scientists\nwho would like to grasp a high-level understanding of DL, develop it for their\napplications by following our step-by-step tutorial, and see how it is evolving\nto facilitate their research efforts. At the same time, it is suitable for\ncomputer scientists who would like to survey state-of-the-art DL-based\nmethodologies for fish habitat monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_A/0/1/0/all/0/1\">Alzayat Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheaves_M/0/1/0/all/0/1\">Marcus Sheaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jerry_D/0/1/0/all/0/1\">Dean Jerry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azghadi_M/0/1/0/all/0/1\">Mostafa Rahimi Azghadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E$^2$PN: Efficient SE(3)-Equivariant Point Network. (arXiv:2206.05398v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05398","description":"<p>This paper proposes a new point-cloud convolution structure that learns\nSE(3)-equivariant features. Compared with existing SE(3)-equivariant networks,\nour design is lightweight, simple, and flexible to be incorporated into general\npoint-cloud learning networks. We strike a balance between the complexity and\ncapacity of our model by selecting an unconventional domain for the feature\nmaps. We further reduce the computational load by properly discretizing\n$\\mathbb{R}^3$ to fully leverage the rotational symmetry. Moreover, we employ a\npermutation layer to recover the full SE(3) group from its quotient space.\nExperiments show that our method achieves comparable or superior performance in\nvarious tasks while consuming much less memory and running faster than existing\nwork. The proposed method can foster the adoption of equivariant feature\nlearning in various practical applications based on point clouds and inspire\nfuture developments of equivariant feature learning for real-world\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_W/0/1/0/all/0/1\">William A. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Definition Map Generation Technologies For Autonomous Driving: A Review. (arXiv:2206.05400v1 [cs.RO])","link":"http://arxiv.org/abs/2206.05400","description":"<p>Autonomous driving has been among the most popular and challenging topics in\nthe past few years. On the road to achieving full autonomy, researchers have\nutilized various sensors, such as LiDAR, camera, Inertial Measurement Unit\n(IMU), and GPS, and developed intelligent algorithms for autonomous driving\napplications such as object detection, object segmentation, obstacle avoidance,\nand path planning. High-definition (HD) maps have drawn lots of attention in\nrecent years. Because of the high precision and informative level of HD maps in\nlocalization, it has immediately become one of the critical components of\nautonomous driving. From big organizations like Baidu Apollo, NVIDIA, and\nTomTom to individual researchers, researchers have created HD maps for\ndifferent scenes and purposes for autonomous driving. It is necessary to review\nthe state-of-the-art methods for HD map generation. This paper reviews recent\nHD map generation technologies that leverage both 2D and 3D map generation.\nThis review introduces the concept of HD maps and their usefulness in\nautonomous driving and gives a detailed overview of HD map generation\ntechniques. We will also discuss the limitations of the current HD map\ngeneration technologies to motivate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhibin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_S/0/1/0/all/0/1\">Sabir Hossain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1\">Haoxiang Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xianke Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VAC2: Visual Analysis of Combined Causality in Event Sequences. (arXiv:2206.05420v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05420","description":"<p>Identifying causality behind complex systems plays a significant role in\ndifferent domains, such as decision making, policy implementations, and\nmanagement recommendations. However, existing causality studies on temporal\nevent sequences data mainly focus on individual causal discovery, which is\nincapable of exploiting combined causality. To fill the absence of combined\ncauses discovery on temporal event sequence data,eliminating and recruiting\nprinciples are defined to balance the effectiveness and controllability on\ncause combinations. We also leverage the Granger causality algorithm based on\nthe reactive point processes to describe impelling or inhibiting behavior\npatterns among entities. In addition, we design an informative and aesthetic\nvisual metaphor of \"electrocircuit\" to encode aggregated causality for ensuring\nour causality visualization is non-overlapping and non-intersecting. Diverse\nsorting strategies and aggregation layout are also embedded into our\nparallel-based, directed and weighted hypergraph for illustrating combined\ncausality. Our developed combined causality visual analysis system can help\nusers effectively explore combined causes as well as an individual cause. This\ninteractive system supports multi-level causality exploration with diverse\nordering strategies and a focus and context technique to help users obtain\ndifferent levels of information abstraction. The usefulness and effectiveness\nof the system are further evaluated by conducting a pilot user study and two\ncase studies on event sequence data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Sujia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Wang Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baofeng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1\">Ronghua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guodao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Access Control of Semantic Segmentation Models Using Encrypted Feature Maps. (arXiv:2206.05422v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05422","description":"<p>In this paper, we propose an access control method with a secret key for\nsemantic segmentation models for the first time so that unauthorized users\nwithout a secret key cannot benefit from the performance of trained models. The\nmethod enables us not only to provide a high segmentation performance to\nauthorized users but to also degrade the performance for unauthorized users. We\nfirst point out that, for the application of semantic segmentation,\nconventional access control methods which use encrypted images for\nclassification tasks are not directly applicable due to performance\ndegradation. Accordingly, in this paper, selected feature maps are encrypted\nwith a secret key for training and testing models, instead of input images. In\nan experiment, the protected models allowed authorized users to obtain almost\nthe same performance as that of non-protected models but also with robustness\nagainst unauthorized access without a key.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MaungMaung_A/0/1/0/all/0/1\">AprilPyone MaungMaung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1\">Sayaka Shiota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Precise Affordance Annotation for Egocentric Action Video Datasets. (arXiv:2206.05424v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05424","description":"<p>Object affordance is an important concept in human-object interaction,\nproviding information on action possibilities based on human motor capacity and\nobjects' physical property thus benefiting tasks such as action anticipation\nand robot imitation learning. However, existing datasets often: 1) mix up\naffordance with object functionality; 2) confuse affordance with goal-related\naction; and 3) ignore human motor capacity. This paper proposes an efficient\nannotation scheme to address these issues by combining goal-irrelevant motor\nactions and grasp types as affordance labels and introducing the concept of\nmechanical action to represent the action possibilities between two objects. We\nprovide new annotations by applying this scheme to the EPIC-KITCHENS dataset\nand test our annotation with tasks such as affordance recognition. We\nqualitatively verify that models trained with our annotation can distinguish\naffordance and mechanical actions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zecheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yagi_T/0/1/0/all/0/1\">Takuma Yagi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goutsu_Y/0/1/0/all/0/1\">Yusuke Goutsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned reconstruction with convergence guarantees. (arXiv:2206.05431v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05431","description":"<p>In recent years, deep learning has achieved remarkable empirical success for\nimage reconstruction. This has catalyzed an ongoing quest for precise\ncharacterization of correctness and reliability of data-driven methods in\ncritical use-cases, for instance in medical imaging. Notwithstanding the\nexcellent performance and efficacy of deep learning-based methods, concerns\nhave been raised regarding their stability, or lack thereof, with serious\npractical implications. Significant advances have been made in recent years to\nunravel the inner workings of data-driven image recovery methods, challenging\ntheir widely perceived black-box nature. In this article, we will specify\nrelevant notions of convergence for data-driven image reconstruction, which\nwill form the basis of a survey of learned methods with mathematically rigorous\nreconstruction guarantees. An example that is highlighted is the role of ICNN,\noffering the possibility to combine the power of deep learning with classical\nconvex regularization theory for devising methods that are provably convergent.\n</p>\n<p>This survey article is aimed at both methodological researchers seeking to\nadvance the frontiers of our understanding of data-driven image reconstruction\nmethods as well as practitioners, by providing an accessible description of\nconvergence concepts and by placing some of the existing empirical practices on\na solid mathematical foundation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhadip Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oktem_O/0/1/0/all/0/1\">Ozan &#xd6;ktem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereyra_M/0/1/0/all/0/1\">Marcelo Pereyra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Luminance-Guided Chrominance Image Enhancement for HEVC Intra Coding. (arXiv:2206.05432v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05432","description":"<p>In this paper, we propose a luminance-guided chrominance image enhancement\nconvolutional neural network for HEVC intra coding. Specifically, we firstly\ndevelop a gated recursive asymmetric-convolution block to restore each degraded\nchrominance image, which generates an intermediate output. Then, guided by the\nluminance image, the quality of this intermediate output is further improved,\nwhich finally produces the high-quality chrominance image. When our proposed\nmethod is adopted in the compression of color images with HEVC intra coding, it\nachieves 28.96% and 16.74% BD-rate gains over HEVC for the U and V images,\nrespectively, which accordingly demonstrate its superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hewei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Renwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shuyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xing Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1\">Bing Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Projection from Optical Coherence Tomography B-Scan without Retinal Layer Segmentation Supervision. (arXiv:2206.05472v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05472","description":"<p>Projection map (PM) from optical coherence tomography (OCT) B-scan is an\nimportant tool to diagnose retinal diseases, which typically requires retinal\nlayer segmentation. In this study, we present a novel end-to-end framework to\npredict PMs from B-scans. Instead of segmenting retinal layers explicitly, we\nrepresent them implicitly as predicted coordinates. By pixel interpolation on\nuniformly sampled coordinates between retinal layers, the corresponding PMs\ncould be easily obtained with pooling. Notably, all the operators are\ndifferentiable; therefore, this Differentiable Projection Module (DPM) enables\nend-to-end training with the ground truth of PMs rather than retinal layer\nsegmentation. Our framework produces high-quality PMs, significantly\noutperforming baselines, including a vanilla CNN without DPM and an\noptimization-based DPM without a deep prior. Furthermore, the proposed DPM, as\na novel neural representation of areas/volumes between curves/surfaces, could\nbe of independent interest for geometric deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rong_D/0/1/0/all/0/1\">Dingyi Rong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jiancheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ke_B/0/1/0/all/0/1\">Bilian Ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kaggle Kinship Recognition Challenge: Introduction of Convolution-Free Model to boost conventional. (arXiv:2206.05488v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05488","description":"<p>This work aims to explore a convolution-free base classifier that can be used\nto widen the variations of the conventional ensemble classifier. Specifically,\nwe propose Vision Transformers as base classifiers to combine with CNNs for a\nunique ensemble solution in Kaggle kinship recognition. In this paper, we\nverify our proposed idea by implementing and optimizing variants of the Vision\nTransformer model on top of the existing CNN models. The combined models\nachieve better scores than conventional ensemble classifiers based solely on\nCNN variants. We demonstrate that highly optimized CNN ensembles publicly\navailable on the Kaggle Discussion board can easily achieve a significant boost\nin ROC score by simply ensemble with variants of the Vision Transformer model\ndue to low correlation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Mingchuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_G/0/1/0/all/0/1\">Guangway Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yipeng Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation of OCR on Egocentric Data. (arXiv:2206.05496v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05496","description":"<p>In this paper, we evaluate state-of-the-art OCR methods on Egocentric data.\nWe annotate text in EPIC-KITCHENS images, and demonstrate that existing OCR\nmethods struggle with rotated text, which is frequently observed on objects\nbeing handled. We introduce a simple rotate-and-merge procedure which can be\napplied to pre-trained OCR models that halves the normalized edit distance\nerror. This suggests that future OCR attempts should incorporate rotation into\nmodel design and training procedures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popescu_V/0/1/0/all/0/1\">Valentin Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrett_T/0/1/0/all/0/1\">Toby Perrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review of Causality for Learning Algorithms in Medical Image Analysis. (arXiv:2206.05498v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05498","description":"<p>Medical image analysis is a vibrant research area that offers doctors and\nmedical practitioners invaluable insight and the ability to accurately diagnose\nand monitor disease. Machine learning provides an additional boost for this\narea. However, machine learning for medical image analysis is particularly\nvulnerable to natural biases like domain shifts that affect algorithmic\nperformance and robustness. In this paper we analyze machine learning for\nmedical image analysis within the framework of Technology Readiness Levels and\nreview how causal analysis methods can fill a gap when creating robust and\nadaptable medical image analysis algorithms. We review methods using causality\nin medical imaging AI/ML and find that causal analysis has the potential to\nmitigate critical problems for clinical translation but that uptake and\nclinical downstream research has been limited so far.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Real-world Single Image Deraining: A New Benchmark and Beyond. (arXiv:2206.05514v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05514","description":"<p>Single image deraining (SID) in real scenarios attracts increasing attention\nin recent years. Due to the difficulty in obtaining real-world rainy/clean\nimage pairs, previous real datasets suffer from low-resolution images,\nhomogeneous rain streaks, limited background variation, and even misalignment\nof image pairs, resulting in incomprehensive evaluation of SID methods. To\naddress these issues, we establish a new high-quality dataset named\nRealRain-1k, consisting of $1,120$ high-resolution paired clean and rainy\nimages with low- and high-density rain streaks, respectively. Images in\nRealRain-1k are automatically generated from a large number of real-world rainy\nvideo clips through a simple yet effective rain density-controllable filtering\nmethod, and have good properties of high image resolution, background\ndiversity, rain streaks variety, and strict spatial alignment. RealRain-1k also\nprovides abundant rain streak layers as a byproduct, enabling us to build a\nlarge-scale synthetic dataset named SynRain-13k by pasting the rain streak\nlayers on abundant natural images. Based on them and existing datasets, we\nbenchmark more than 10 representative SID methods on three tracks: (1) fully\nsupervised learning on RealRain-1k, (2) domain generalization to real datasets,\nand (3) syn-to-real transfer learning. The experimental results (1) show the\ndifference of representative methods in image restoration performance and model\ncomplexity, (2) validate the significance of the proposed datasets for model\ngeneralization, and (3) provide useful insights on the superiority of learning\nfrom diverse domains and shed lights on the future research on real-world SID.\nThe datasets will be released at https://github.com/hiker-lw/RealRain-1k\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Based MR Image Re-parameterization. (arXiv:2206.05516v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05516","description":"<p>Magnetic resonance (MR) image re-parameterization refers to the process of\ngenerating via simulations of an MR image with a new set of MRI scanning\nparameters. Different parameter values generate distinct contrast between\ndifferent tissues, helping identify pathologic tissue. Typically, more than one\nscan is required for diagnosis; however, acquiring repeated scans can be\ncostly, time-consuming, and difficult for patients. Thus, using MR image\nre-parameterization to predict and estimate the contrast in these imaging scans\ncan be an effective alternative. In this work, we propose a novel deep learning\n(DL) based convolutional model for MRI re-parameterization. Based on our\npreliminary results, DL-based techniques hold the potential to learn the\nnon-linearities that govern the re-parameterization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Narang_A/0/1/0/all/0/1\">Abhijeet Narang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_A/0/1/0/all/0/1\">Abhigyan Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pop_M/0/1/0/all/0/1\">Mihaela Pop</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebrahimi_M/0/1/0/all/0/1\">Mehran Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Two-stage Method for Non-extreme Value Salt-and-Pepper Noise Removal. (arXiv:2206.05520v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05520","description":"<p>There are several previous methods based on neural network can have great\nperformance in denoising salt and pepper noise. However, those methods are\nbased on a hypothesis that the value of salt and pepper noise is exactly 0 and\n255. It is not true in the real world. The result of those methods deviate\nsharply when the value is different from 0 and 255. To overcome this weakness,\nour method aims at designing a convolutional neural network to detect the noise\npixels in a wider range of value and then a filter is used to modify pixel\nvalue to 0, which is beneficial for further filtering. Additionally, another\nconvolutional neural network is used to conduct the denoising and restoration\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Renwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">YiKe Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simplified Un-Supervised Learning Based Approach for Ink Mismatch Detection in Handwritten Hyper-Spectral Document Images. (arXiv:2206.05539v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05539","description":"<p>Hyper-spectral imaging has become the latest trend in the field of optical\nimaging systems. Among various other applications, hyper-spectral imaging has\nbeen widely used for analysis of printed and handwritten documents. This paper\nproposes an efficient technique for estimating the number of different but\nvisibly similar inks present in a Hyper spectral Document Image. Our approach\nis based on un-supervised learning and does not require any prior knowledge of\nthe dataset. The algorithm was tested on the iVision HHID dataset and has\nachieved comparable results with the state of the algorithms present in the\nliterature. This work can prove to be effective when employed during the early\nstages of forgery detection in Hyper-spectral Document Images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Humayun_M/0/1/0/all/0/1\">Muhammad Farhan Humayun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_H/0/1/0/all/0/1\">Hassan Waseem Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvi_A/0/1/0/all/0/1\">Ahmed Ahsan Alvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surround-View Cameras based Holistic Visual Perception for Automated Driving. (arXiv:2206.05542v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05542","description":"<p>The formation of eyes led to the big bang of evolution. The dynamics changed\nfrom a primitive organism waiting for the food to come into contact for eating\nfood being sought after by visual sensors. The human eye is one of the most\nsophisticated developments of evolution, but it still has defects. Humans have\nevolved a biological perception algorithm capable of driving cars, operating\nmachinery, piloting aircraft, and navigating ships over millions of years.\nAutomating these capabilities for computers is critical for various\napplications, including self-driving cars, augmented reality, and architectural\nsurveying. Near-field visual perception in the context of self-driving cars can\nperceive the environment in a range of $0-10$ meters and 360{\\deg} coverage\naround the vehicle. It is a critical decision-making component in the\ndevelopment of safer automated driving. Recent advances in computer vision and\ndeep learning, in conjunction with high-quality sensors such as cameras and\nLiDARs, have fueled mature visual perception solutions. Until now, far-field\nperception has been the primary focus. Another significant issue is the limited\nprocessing power available for developing real-time applications. Because of\nthis bottleneck, there is frequently a trade-off between performance and\nrun-time efficiency. We concentrate on the following issues in order to address\nthem: 1) Developing near-field perception algorithms with high performance and\nlow computational complexity for various visual perception tasks such as\ngeometric and semantic tasks using convolutional neural networks. 2) Using\nMulti-Task Learning to overcome computational bottlenecks by sharing initial\nconvolutional layers between tasks and developing optimization strategies that\nbalance tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Continuous Learning Framework for Multi-modal Knowledge Discovery and Pre-training. (arXiv:2206.05555v1 [cs.CL])","link":"http://arxiv.org/abs/2206.05555","description":"<p>Multi-modal pre-training and knowledge discovery are two important research\ntopics in multi-modal machine learning. Nevertheless, none of existing works\nmake attempts to link knowledge discovery with knowledge guided multi-modal\npre-training. In this paper, we propose to unify them into a continuous\nlearning framework for mutual improvement. Taking the open-domain uni-modal\ndatasets of images and texts as input, we maintain a knowledge graph as the\nfoundation to support these two tasks. For knowledge discovery, a pre-trained\nmodel is used to identify cross-modal links on the graph. For model\npre-training, the knowledge graph is used as the external knowledge to guide\nthe model updating. These two steps are iteratively performed in our framework\nfor continuous learning. The experimental results on MS-COCO and Flickr30K with\nrespect to both knowledge discovery and the pre-trained model validate the\neffectiveness of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MammoDL: Mammographic Breast Density Estimation using Federated Learning. (arXiv:2206.05575v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05575","description":"<p>Assessing breast cancer risk from imaging remains a subjective process, in\nwhich radiologists employ computer aided detection (CAD) systems or qualitative\nvisual assessment to estimate breast percent density (PD). More advanced\nmachine learning (ML) models have become the most promising way to quantify\nbreast cancer risk for early, accurate, and equitable diagnoses, but training\nsuch models in medical research is often restricted to small,\nsingle-institution data. Since patient demographics and imaging characteristics\nmay vary considerably across imaging sites, models trained on\nsingle-institution data tend not to generalize well. In response to this\nproblem, MammoDL is proposed, an open-source software tool that leverages UNet\narchitecture to accurately estimate breast PD and complexity from digital\nmammography (DM). With the Open Federated Learning (OpenFL) library, this\nsolution enables secure training on datasets across multiple institutions.\nMammoDL is a leaner, more flexible model than its predecessors, boasting\nimproved generalization due to federation-enabled training on larger, more\nrepresentative datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Katti_K/0/1/0/all/0/1\">Keshava Katti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muthukrishnan_R/0/1/0/all/0/1\">Ramya Muthukrishnan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heyler_A/0/1/0/all/0/1\">Angelina Heyler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pati_S/0/1/0/all/0/1\">Sarthak Pati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alahari_A/0/1/0/all/0/1\">Aprupa Alahari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanborn_M/0/1/0/all/0/1\">Michael Sanborn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Conant_E/0/1/0/all/0/1\">Emily F. Conant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scott_C/0/1/0/all/0/1\">Christopher Scott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Winham_S/0/1/0/all/0/1\">Stacey Winham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vachon_C/0/1/0/all/0/1\">Celine Vachon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_P/0/1/0/all/0/1\">Pratik Chaudhari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kontos_D/0/1/0/all/0/1\">Despina Kontos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine learning approaches for COVID-19 detection from chest X-ray imaging: A Systematic Review. (arXiv:2206.05615v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05615","description":"<p>There is a necessity to develop affordable, and reliable diagnostic tools,\nwhich allow containing the COVID-19 spreading. Machine Learning (ML) algorithms\nhave been proposed to design support decision-making systems to assess chest\nX-ray images, which have proven to be useful to detect and evaluate disease\nprogression. Many research articles are published around this subject, which\nmakes it difficult to identify the best approaches for future work. This paper\npresents a systematic review of ML applied to COVID-19 detection using chest\nX-ray images, aiming to offer a baseline for researchers in terms of methods,\narchitectures, databases, and current limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Arteaga_Arteaga_H/0/1/0/all/0/1\">Harold Brayan Arteaga-Arteaga</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+delaPava_M/0/1/0/all/0/1\">Melissa delaPava</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Mora_Rubio_A/0/1/0/all/0/1\">Alejandro Mora-Rubio</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Bravo_Ortiz_M/0/1/0/all/0/1\">Mario Alejandro Bravo-Ort&#xed;z</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Alzate_Grisales_J/0/1/0/all/0/1\">Jesus Alejandro Alzate-Grisales</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Arias_Garzon_D/0/1/0/all/0/1\">Daniel Arias-Garz&#xf3;n</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Lopez_Murillo_L/0/1/0/all/0/1\">Luis Humberto L&#xf3;pez-Murillo</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Buitrago_Carmona_F/0/1/0/all/0/1\">Felipe Buitrago-Carmona</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Villa_Pulgarin_J/0/1/0/all/0/1\">Juan Pablo Villa-Pulgar&#xed;n</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Mercado_Ruiz_E/0/1/0/all/0/1\">Esteban Mercado-Ruiz</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Orozco_Arias_S/0/1/0/all/0/1\">Simon Orozco-Arias</a> (3 and 4), <a href=\"http://arxiv.org/find/eess/1/au:+Hassaballah_M/0/1/0/all/0/1\">M. Hassaballah</a> (5), <a href=\"http://arxiv.org/find/eess/1/au:+Iglesia_Vaya_M/0/1/0/all/0/1\">Maria de la Iglesia-Vaya</a> (6), <a href=\"http://arxiv.org/find/eess/1/au:+Cardona_Morales_O/0/1/0/all/0/1\">Oscar Cardona-Morales</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Tabares_Soto_R/0/1/0/all/0/1\">Reinel Tabares-Soto</a> (1) ((1) Department of Electronics and Automation, Universidad Aut&#xf3;noma de Manizales, Manizales, Colombia, (2) Department of Chemical Engineering, Universidad Nacional de Colombia, Manizales, Colombia, (3) Department of Computer Science, Universidad Aut&#xf3;noma de Manizales, Manizales, Colombia, (4) Department of Systems and informatics, Universidad de Caldas, Manizales, Colombia, (5) Faculty of Computers and Information, South Valley University, Qena, Egypt, (6) Unidad Mixta de Imagen Biom&#xe9;dica FISABIO-CIPF, Fundaci&#xf3;n para el Fomento de la Investigaci&#xf3;n Sanitario y Biom&#xe9;dica de la Comunidad Valenciana, Valencia, Spain)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning with Research Prototypes for Multi-Center MRI-based Detection of Prostate Cancer with Diverse Histopathology. (arXiv:2206.05617v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05617","description":"<p>Early prostate cancer detection and staging from MRI are extremely\nchallenging tasks for both radiologists and deep learning algorithms, but the\npotential to learn from large and diverse datasets remains a promising avenue\nto increase their generalization capability both within- and across clinics. To\nenable this for prototype-stage algorithms, where the majority of existing\nresearch remains, in this paper we introduce a flexible federated learning\nframework for cross-site training, validation, and evaluation of deep prostate\ncancer detection algorithms. Our approach utilizes an abstracted representation\nof the model architecture and data, which allows unpolished prototype deep\nlearning models to be trained without modification using the NVFlare federated\nlearning framework. Our results show increases in prostate cancer detection and\nclassification accuracy using a specialized neural network model and diverse\nprostate biopsy data collected at two University of California research\nhospitals, demonstrating the efficacy of our approach in adapting to different\ndatasets and improving MR-biomarker discovery. We open-source our FLtools\nsystem, which can be easily adapted to other deep learning projects for medical\nimaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_A/0/1/0/all/0/1\">Abhejit Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Redekop_E/0/1/0/all/0/1\">Ekaterina Redekop</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemisetti_A/0/1/0/all/0/1\">Anil Kemisetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_R/0/1/0/all/0/1\">Rushi Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Steven Raman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magudia_K/0/1/0/all/0/1\">Kirti Magudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_C/0/1/0/all/0/1\">Corey W. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_P/0/1/0/all/0/1\">Peder E. Z. Larson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic PET via Domain Translation of 3D MRI. (arXiv:2206.05618v1 [physics.med-ph])","link":"http://arxiv.org/abs/2206.05618","description":"<p>Historically, patient datasets have been used to develop and validate various\nreconstruction algorithms for PET/MRI and PET/CT. To enable such algorithm\ndevelopment, without the need for acquiring hundreds of patient exams, in this\npaper we demonstrate a deep learning technique to generate synthetic but\nrealistic whole-body PET sinograms from abundantly-available whole-body MRI.\nSpecifically, we use a dataset of 56 $^{18}$F-FDG-PET/MRI exams to train a 3D\nresidual UNet to predict physiologic PET uptake from whole-body T1-weighted\nMRI. In training we implemented a balanced loss function to generate realistic\nuptake across a large dynamic range and computed losses along tomographic lines\nof response to mimic the PET acquisition. The predicted PET images are forward\nprojected to produce synthetic PET time-of-flight (ToF) sinograms that can be\nused with vendor-provided PET reconstruction algorithms, including using\nCT-based attenuation correction (CTAC) and MR-based attenuation correction\n(MRAC). The resulting synthetic data recapitulates physiologic $^{18}$F-FDG\nuptake, e.g. high uptake localized to the brain and bladder, as well as uptake\nin liver, kidneys, heart and muscle. To simulate abnormalities with high\nuptake, we also insert synthetic lesions. We demonstrate that this synthetic\nPET data can be used interchangeably with real PET data for the PET\nquantification task of comparing CT and MR-based attenuation correction\nmethods, achieving $\\leq 7.6\\%$ error in mean-SUV compared to using real data.\nThese results together show that the proposed synthetic PET data pipeline can\nbe reasonably used for development, evaluation, and validation of PET/MRI\nreconstruction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Rajagopal_A/0/1/0/all/0/1\">Abhejit Rajagopal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Natsuaki_Y/0/1/0/all/0/1\">Yutaka Natsuaki</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wangerin_K/0/1/0/all/0/1\">Kristen Wangerin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hamdi_M/0/1/0/all/0/1\">Mahdjoub Hamdi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+An_H/0/1/0/all/0/1\">Hongyu An</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sunderland_J/0/1/0/all/0/1\">John J. Sunderland</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Laforest_R/0/1/0/all/0/1\">Richard Laforest</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kinahan_P/0/1/0/all/0/1\">Paul E. Kinahan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Larson_P/0/1/0/all/0/1\">Peder E.Z. Larson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hope_T/0/1/0/all/0/1\">Thomas A.Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Models for Automated Classification of Dog Emotional States from Facial Expressions. (arXiv:2206.05619v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05619","description":"<p>Similarly to humans, facial expressions in animals are closely linked with\nemotional states. However, in contrast to the human domain, automated\nrecognition of emotional states from facial expressions in animals is\nunderexplored, mainly due to difficulties in data collection and establishment\nof ground truth concerning emotional states of non-verbal users. We apply\nrecent deep learning techniques to classify (positive) anticipation and\n(negative) frustration of dogs on a dataset collected in a controlled\nexperimental setting. We explore the suitability of different backbones (e.g.\nResNet, ViT) under different supervisions to this task, and find that features\nof a self-supervised pretrained ViT (DINO-ViT) are superior to the other\nalternatives. To the best of our knowledge, this work is the first to address\nthe task of automatic classification of canine emotions on data acquired in a\ncontrolled experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boneh_Shitrit_T/0/1/0/all/0/1\">Tali Boneh-Shitrit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1\">Shir Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremhorst_A/0/1/0/all/0/1\">Annika Bremhorst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mills_D/0/1/0/all/0/1\">Daniel S. Mills</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riemer_S/0/1/0/all/0/1\">Stefanie Riemer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Dror Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamansky_A/0/1/0/all/0/1\">Anna Zamansky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Plastic Artificial Neural Networks: Exploring the Intersection between Neural Architecture Search and Continual Learning. (arXiv:2206.05625v1 [cs.AI])","link":"http://arxiv.org/abs/2206.05625","description":"<p>Despite the significant advances achieved in Artificial Neural Networks\n(ANNs), their design process remains notoriously tedious, depending primarily\non intuition, experience and trial-and-error. This human-dependent process is\noften time-consuming and prone to errors. Furthermore, the models are generally\nbound to their training contexts, with no considerations of changes to their\nsurrounding environments. Continual adaptability and automation of neural\nnetworks is of paramount importance to several domains where model\naccessibility is limited after deployment (e.g IoT devices, self-driving\nvehicles, etc). Additionally, even accessible models require frequent\nmaintenance post-deployment to overcome issues such as Concept/Data Drift,\nwhich can be cumbersome and restrictive. The current state of the art on\nadaptive ANNs is still a premature area of research; nevertheless, Neural\nArchitecture Search (NAS), a form of AutoML, and Continual Learning (CL) have\nrecently gained an increasing momentum in the Deep Learning research field,\naiming to provide more robust and adaptive ANN development frameworks. This\nstudy is the first extensive review on the intersection between AutoML and CL,\noutlining research directions for the different methods that can facilitate\nfull automation and lifelong plasticity in ANNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahawy_M/0/1/0/all/0/1\">Mohamed Shahawy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benkhelifa_E/0/1/0/all/0/1\">Elhadj Benkhelifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_D/0/1/0/all/0/1\">David White</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Deep-Learning Method for Bone Age Assessment. (arXiv:2206.05641v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05641","description":"<p>The bone age, reflecting the degree of development of the bones, can be used\nto predict the adult height and detect endocrine diseases of children. Both\nexaminations of radiologists and variability of operators have a significant\nimpact on bone age assessment. To decrease human intervention , machine\nlearning algorithms are used to assess the bone age automatically. However,\nconventional supervised deep-learning methods need pre-labeled data. In this\npaper, based on the convolutional auto-encoder with constraints (CCAE), an\nunsupervised deep-learning model proposed in the classification of the\nfingerprint, we propose this model for the classification of the bone age and\nbaptize it BA-CCAE. In the proposed BA-CCAE model, the key regions of the raw\nX-ray images of the bone age are encoded, yielding the latent vectors. The\nK-means clustering algorithm is used to obtain the final classifications by\ngrouping the latent vectors of the bone images. A set of experiments on the\nRadiological Society of North America pediatric bone age dataset (RSNA) show\nthat the accuracy of classifications at 48-month intervals is 76.15%. Although\nthe accuracy now is lower than most of the existing supervised models, the\nproposed BA-CCAE model can establish the classification of bone age without any\npre-labeled data, and to the best of our knowledge, the proposed BA-CCAE is one\nof the few trails using the unsupervised deep-learning method for the bone age\nassessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Wan-Jing Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yue-Jie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qi-Meng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Si-Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chi-Chun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fast Alternating Minimization Algorithm for Coded Aperture Snapshot Spectral Imaging Based on Sparsity and Deep Image Priors. (arXiv:2206.05647v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05647","description":"<p>Coded aperture snapshot spectral imaging (CASSI) is a technique used to\nreconstruct three-dimensional hyperspectral images (HSIs) from one or several\ntwo-dimensional projection measurements. However, fewer projection measurements\nor more spectral channels leads to a severly ill-posed problem, in which case\nregularization methods have to be applied. In order to significantly improve\nthe accuracy of reconstruction, this paper proposes a fast alternating\nminimization algorithm based on the sparsity and deep image priors (Fama-SDIP)\nof natural images. By integrating deep image prior (DIP) into the principle of\ncompressive sensing (CS) reconstruction, the proposed algorithm can achieve\nstate-of-the-art results without any training dataset. Extensive experiments\nshow that Fama-SDIP method significantly outperforms prevailing leading methods\non simulation and real HSI datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qile Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1\">Xianhong Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xudong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arce_G/0/1/0/all/0/1\">Gonzalo R. Arce</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indirect-Instant Attention Optimization for Crowd Counting in Dense Scenes. (arXiv:2206.05648v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05648","description":"<p>One of appealing approaches to guiding learnable parameter optimization, such\nas feature maps, is global attention, which enlightens network intelligence at\na fraction of the cost. However, its loss calculation process still falls\nshort: 1)We can only produce one-dimensional 'pseudo labels' for attention,\nsince the artificial threshold involved in the procedure is not robust; 2) The\nattention awaiting loss calculation is necessarily high-dimensional, and\ndecreasing it by convolution will inevitably introduce additional learnable\nparameters, thus confusing the source of the loss. To this end, we devise a\nsimple but efficient Indirect-Instant Attention Optimization (IIAO) module\nbased on SoftMax-Attention , which transforms high-dimensional attention map\ninto a one-dimensional feature map in the mathematical sense for loss\ncalculation midway through the network, while automatically providing adaptive\nmulti-scale fusion to feature pyramid module. The special transformation yields\nrelatively coarse features and, originally, the predictive fallibility of\nregions varies by crowd density distribution, so we tailor the Regional\nCorrelation Loss (RCLoss) to retrieve continuous error-prone regions and smooth\nspatial information . Extensive experiments have proven that our approach\nsurpasses previous SOTA methods in many benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Suyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Donghua Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TileGen: Tileable, Controllable Material Generation and Capture. (arXiv:2206.05649v1 [cs.GR])","link":"http://arxiv.org/abs/2206.05649","description":"<p>Recent methods (e.g. MaterialGAN) have used unconditional GANs to generate\nper-pixel material maps, or as a prior to reconstruct materials from input\nphotographs. These models can generate varied random material appearance, but\ndo not have any mechanism to constrain the generated material to a specific\ncategory or to control the coarse structure of the generated material, such as\nthe exact brick layout on a brick wall. Furthermore, materials reconstructed\nfrom a single input photo commonly have artifacts and are generally not\ntileable, which limits their use in practical content creation pipelines. We\npropose TileGen, a generative model for SVBRDFs that is specific to a material\ncategory, always tileable, and optionally conditional on a provided input\nstructure pattern. TileGen is a variant of StyleGAN whose architecture is\nmodified to always produce tileable (periodic) material maps. In addition to\nthe standard \"style\" latent code, TileGen can optionally take a condition\nimage, giving a user direct control over the dominant spatial (and optionally\ncolor) features of the material. For example, in brick materials, the user can\nspecify a brick layout and the brick color, or in leather materials, the\nlocations of wrinkles and folds. Our inverse rendering approach can find a\nmaterial perceptually matching a single target photograph by optimization. This\nreconstruction can also be conditional on a user-provided pattern. The\nresulting materials are tileable, can be larger than the target image, and are\neditable by varying the condition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xilong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Milo&#x161; Ha&#x161;an</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deschaintre_V/0/1/0/all/0/1\">Valentin Deschaintre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sunkavalli_K/0/1/0/all/0/1\">Kalyan Sunkavalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalantari_N/0/1/0/all/0/1\">Nima Kalantari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preprocessing Enhanced Image Compression for Machine Vision. (arXiv:2206.05650v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05650","description":"<p>Recently, more and more images are compressed and sent to the back-end\ndevices for the machine analysis tasks~(\\textit{e.g.,} object detection)\ninstead of being purely watched by humans. However, most traditional or learned\nimage codecs are designed to minimize the distortion of the human visual system\nwithout considering the increased demand from machine vision systems. In this\nwork, we propose a preprocessing enhanced image compression method for machine\nvision tasks to address this challenge. Instead of relying on the learned image\ncodecs for end-to-end optimization, our framework is built upon the traditional\nnon-differential codecs, which means it is standard compatible and can be\neasily deployed in practical applications. Specifically, we propose a neural\npreprocessing module before the encoder to maintain the useful semantic\ninformation for the downstream tasks and suppress the irrelevant information\nfor bitrate saving. Furthermore, our neural preprocessing module is\nquantization adaptive and can be used in different compression ratios. More\nimportantly, to jointly optimize the preprocessing module with the downstream\nmachine vision tasks, we introduce the proxy network for the traditional\nnon-differential codecs in the back-propagation stage. We provide extensive\nexperiments by evaluating our compression method for two representative\ndownstream tasks with different backbone networks. Experimental results show\nour method achieves a better trade-off between the coding bitrate and the\nperformance of the downstream machine vision tasks by saving about 20% bitrate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_G/0/1/0/all/0/1\">Guo Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_X/0/1/0/all/0/1\">Xingtong Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_T/0/1/0/all/0/1\">Tianxiong Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_J/0/1/0/all/0/1\">Jing Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STD-NET: Search of Image Steganalytic Deep-learning Architecture via Hierarchical Tensor Decomposition. (arXiv:2206.05651v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05651","description":"<p>Recent studies shows that the majority of existing deep steganalysis models\nhave a large amount of redundancy, which leads to a huge waste of storage and\ncomputing resources. The existing model compression method cannot flexibly\ncompress the convolutional layer in residual shortcut block so that a\nsatisfactory shrinking rate cannot be obtained. In this paper, we propose\nSTD-NET, an unsupervised deep-learning architecture search approach via\nhierarchical tensor decomposition for image steganalysis. Our proposed strategy\nwill not be restricted by various residual connections, since this strategy\ndoes not change the number of input and output channels of the convolution\nblock. We propose a normalized distortion threshold to evaluate the sensitivity\nof each involved convolutional layer of the base model to guide STD-NET to\ncompress target network in an efficient and unsupervised approach, and obtain\ntwo network structures of different shapes with low computation cost and\nsimilar performance compared with the original one. Extensive experiments have\nconfirmed that, on one hand, our model can achieve comparable or even better\ndetection performance in various steganalytic scenarios due to the great\nadaptivity of the obtained network architecture. On the other hand, the\nexperimental results also demonstrate that our proposed strategy is more\nefficient and can remove more redundancy compared with previous steganalytic\nnetwork compression methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Shunquan Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qiushi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Laiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiwu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking. (arXiv:2206.05683v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05683","description":"<p>Animal pose estimation and tracking (APT) is a fundamental task for detecting\nand tracking animal keypoints from a sequence of video frames. Previous\nanimal-related datasets focus either on animal tracking or single-frame animal\npose estimation, and never on both aspects. The lack of APT datasets hinders\nthe development and evaluation of video-based animal pose estimation and\ntracking methods, limiting real-world applications, e.g., understanding animal\nbehavior in wildlife conservation. To fill this gap, we make the first step and\npropose APT-36K, i.e., the first large-scale benchmark for animal pose\nestimation and tracking. Specifically, APT-36K consists of 2,400 video clips\ncollected and filtered from 30 animal species with 15 frames for each video,\nresulting in 36,000 frames in total. After manual annotation and careful\ndouble-check, high-quality keypoint and tracking annotations are provided for\nall the animal instances. Based on APT-36K, we benchmark several representative\nmodels on the following three tracks: (1) supervised animal pose estimation on\na single frame under intra- and inter-domain transfer learning settings, (2)\ninter-species domain generalization test for unseen animals, and (3) animal\npose estimation with animal tracking. Based on the experimental results, we\ngain some empirical insights and show that APT-36K provides a valuable animal\npose estimation and tracking benchmark, offering new challenges and\nopportunities for future research. The code and dataset will be made publicly\navailable at https://github.com/pandorgan/APT-36K.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1\">Long Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement. (arXiv:2206.05687v1 [cs.HC])","link":"http://arxiv.org/abs/2206.05687","description":"<p>Remote photoplethysmography (rPPG) based physiological measurement has great\napplication values in affective computing, non-contact health monitoring,\ntelehealth monitoring, etc, which has become increasingly important especially\nduring the COVID-19 pandemic. Existing methods are generally divided into two\ngroups. The first focuses on mining the subtle blood volume pulse (BVP) signals\nfrom face videos, but seldom explicitly models the noises that dominate face\nvideo content. They are susceptible to the noises and may suffer from poor\ngeneralization ability in unseen scenarios. The second focuses on modeling\nnoisy data directly, resulting in suboptimal performance due to the lack of\nregularity of these severe random noises. In this paper, we propose a\nDecomposition and Reconstruction Network (DRNet) focusing on the modeling of\nphysiological features rather than noisy data. A novel cycle loss is proposed\nto constrain the periodicity of physiological information. Besides, a\nplug-and-play Spatial Attention Block (SAB) is proposed to enhance features\nalong with the spatial location information. Furthermore, an efficient Patch\nCropping (PC) augmentation strategy is proposed to synthesize augmented samples\nwith different noise and features. Extensive experiments on different public\ndatasets as well as the cross-database testing demonstrate the effectiveness of\nour approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuhang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gongping Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model. (arXiv:2206.05695v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05695","description":"<p>Early prediction of pathological complete response (pCR) following\nneoadjuvant chemotherapy (NAC) for breast cancer plays a critical role in\nsurgical planning and optimizing treatment strategies. Recently, machine and\ndeep-learning based methods were suggested for early pCR prediction from\nmulti-parametric MRI (mp-MRI) data including dynamic contrast-enhanced MRI and\ndiffusion-weighted MRI (DWI) with moderate success. We introduce PD-DWI, a\nphysiologically decomposed DWI machine-learning model to predict pCR from DWI\nand clinical data. Our model first decomposes the raw DWI data into the various\nphysiological cues that are influencing the DWI signal and then uses the\ndecomposed data, in addition to clinical variables, as the input features of a\nradiomics-based XGBoost model. We demonstrated the added-value of our PD-DWI\nmodel over conventional machine-learning approaches for pCR prediction from\nmp-MRI data using the publicly available Breast Multi-parametric MRI for\nprediction of NAC Response (BMMR2) challenge. Our model substantially improves\nthe area under the curve (AUC), compared to the current best result on the\nleaderboard (0.8849 vs. 0.8397) for the challenge test set. PD-DWI has the\npotential to improve prediction of pCR following NAC for breast cancer, reduce\noverall mp-MRI acquisition times and eliminate the need for contrast-agent\ninjection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gilad_M/0/1/0/all/0/1\">Maya Gilad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPCN++: Differentiable Phase Correlation Network for Versatile Pose Registration. (arXiv:2206.05707v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05707","description":"<p>Pose registration is critical in vision and robotics. This paper focuses on\nthe challenging task of initialization-free pose registration up to 7DoF for\nhomogeneous and heterogeneous measurements. While recent learning-based methods\nshow promise using differentiable solvers, they either rely on heuristically\ndefined correspondences or are prone to local minima. We present a\ndifferentiable phase correlation (DPC) solver that is globally convergent and\ncorrespondence-free. When combined with simple feature extraction networks, our\ngeneral framework DPCN++ allows for versatile pose registration with arbitrary\ninitialization. Specifically, the feature extraction networks first learn dense\nfeature grids from a pair of homogeneous/heterogeneous measurements. These\nfeature grids are then transformed into a translation and scale invariant\nspectrum representation based on Fourier transform and spherical radial\naggregation, decoupling translation and scale from rotation. Next, the\nrotation, scale, and translation are independently and efficiently estimated in\nthe spectrum step-by-step using the DPC solver. The entire pipeline is\ndifferentiable and trained end-to-end. We evaluate DCPN++ on a wide range of\nregistration tasks taking different input modalities, including 2D bird's-eye\nview images, 3D object and scene measurements, and medical images. Experimental\nresults demonstrate that DCPN++ outperforms both classical and learning-based\nbaselines, especially on partially observed and heterogeneous measurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zexi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Haozhe Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haodong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuecheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haojian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Narrowing the Gap: Improved Detector Training with Noisy Location Annotations. (arXiv:2206.05708v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05708","description":"<p>Deep learning methods require massive of annotated data for optimizing\nparameters. For example, datasets attached with accurate bounding box\nannotations are essential for modern object detection tasks. However, labeling\nwith such pixel-wise accuracy is laborious and time-consuming, and elaborate\nlabeling procedures are indispensable for reducing man-made noise, involving\nannotation review and acceptance testing. In this paper, we focus on the impact\nof noisy location annotations on the performance of object detection approaches\nand aim to, on the user side, reduce the adverse effect of the noise. First,\nnoticeable performance degradation is experimentally observed for both\none-stage and two-stage detectors when noise is introduced to the bounding box\nannotations. For instance, our synthesized noise results in performance\ndecrease from 38.9% AP to 33.6% AP for FCOS detector on COCO test split, and\n37.8%AP to 33.7%AP for Faster R-CNN. Second, a self-correction technique based\non a Bayesian filter for prediction ensemble is proposed to better exploit the\nnoisy location annotations following a Teacher-Student learning paradigm.\nExperiments for both synthesized and real-world scenarios consistently\ndemonstrate the effectiveness of our approach, e.g., our method increases the\ndegraded performance of the FCOS detector from 33.6% AP to 35.6% AP on COCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shaoru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-based Spatial Transformer with Memory Replay for Multi-future Pedestrian Trajectory Prediction. (arXiv:2206.05712v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05712","description":"<p>Pedestrian trajectory prediction is an essential and challenging task for a\nvariety of real-life applications such as autonomous driving and robotic motion\nplanning. Besides generating a single future path, predicting multiple\nplausible future paths is becoming popular in some recent work on trajectory\nprediction. However, existing methods typically emphasize spatial interactions\nbetween pedestrians and surrounding areas but ignore the smoothness and\ntemporal consistency of predictions. Our model aims to forecast multiple paths\nbased on a historical trajectory by modeling multi-scale graph-based spatial\ntransformers combined with a trajectory smoothing algorithm named ``Memory\nReplay'' utilizing a memory graph. Our method can comprehensively exploit the\nspatial information as well as correct the temporally inconsistent trajectories\n(e.g., sharp turns). We also propose a new evaluation metric named ``Percentage\nof Trajectory Usage'' to evaluate the comprehensiveness of diverse multi-future\npredictions. Our extensive experiments show that the proposed model achieves\nstate-of-the-art performance on multi-future prediction and competitive results\nfor single-future prediction. Code released at\nhttps://github.com/Jacobieee/ST-MR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1\">Maurice Pagnucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowd Localization from Gaussian Mixture Scoped Knowledge and Scoped Teacher. (arXiv:2206.05717v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05717","description":"<p>Crowd localization is to predict each instance head position in crowd\nscenarios. Since the distance of instances being to the camera are variant,\nthere exists tremendous gaps among scales of instances within an image, which\nis called the intrinsic scale shift. The core reason of intrinsic scale shift\nbeing one of the most essential issues in crowd localization is that it is\nubiquitous in crowd scenes and makes scale distribution chaotic.\n</p>\n<p>To this end, the paper concentrates on access to tackle the chaos of the\nscale distribution incurred by intrinsic scale shift. We propose Gaussian\nMixture Scope (GMS) to regularize the chaotic scale distribution. Concretely,\nthe GMS utilizes a Gaussian mixture distribution to adapt to scale distribution\nand decouples the mixture model into sub-normal distributions to regularize the\nchaos within the sub-distributions. Then, an alignment is introduced to\nregularize the chaos among sub-distributions. However, despite that GMS is\neffective in regularizing the data distribution, it amounts to dislodging the\nhard samples in training set, which incurs overfitting. We assert that it is\nblamed on the block of transferring the latent knowledge exploited by GMS from\ndata to model. Therefore, a Scoped Teacher playing a role of bridge in\nknowledge transform is proposed. What' s more, the consistency regularization\nis also introduced to implement knowledge transform. To that effect, the\nfurther constraints are deployed on Scoped Teacher to derive feature\nconsistence between teacher and student end.\n</p>\n<p>With proposed GMS and Scoped Teacher implemented on five mainstream datasets\nof crowd localization, the extensive experiments demonstrate the superiority of\nour work. Moreover, comparing with existing crowd locators, our work achieves\nstate-of-the-art via F1-meansure comprehensively on five datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Juncheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Occlusion Of Adding New Category In Objection Detection. (arXiv:2206.05730v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05730","description":"<p>Building instance detection models that are data efficient and can handle\nrare object categories is an important challenge in computer vision. But data\ncollection methods and metrics are lack of research towards real scenarios\napplication using neural network. Here, we perform a systematic study of the\nObject Occlusion data collection and augmentation methods where we imitate\nobject occlusion relationship in target scenarios. However, we find that the\nsimple mechanism of object occlusion is good enough and can provide acceptable\naccuracy in real scenarios adding new category. We illustate that only adding\n15 images of new category in a half million training dataset with hundreds\ncategories, can give this new category 95% accuracy in unseen test dataset\nincluding thousands of images of this category.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Boyang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Meiyan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Shoulun Long</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseNeuS: Fast Generalizable Neural Surface Reconstruction from Sparse views. (arXiv:2206.05737v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05737","description":"<p>We introduce SparseNeuS, a novel neural rendering based method for the task\nof surface reconstruction from multi-view images. This task becomes more\ndifficult when only sparse images are provided as input, a scenario where\nexisting neural reconstruction approaches usually produce incomplete or\ndistorted results. Moreover, their inability of generalizing to unseen new\nscenes impedes their application in practice. Contrarily, SparseNeuS can\ngeneralize to new scenes and work well with sparse images (as few as 2 or 3).\nSparseNeuS adopts signed distance function (SDF) as the surface representation,\nand learns generalizable priors from image features by introducing geometry\nencoding volumes for generic surface prediction. Moreover, several strategies\nare introduced to effectively leverage sparse views for high-quality\nreconstruction, including 1) a multi-level geometry reasoning framework to\nrecover the surfaces in a coarse-to-fine manner; 2) a multi-scale color\nblending scheme for more reliable color prediction; 3) a consistency-aware\nfine-tuning scheme to control the inconsistent regions caused by occlusion and\nnoise. Extensive experiments demonstrate that our approach not only outperforms\nthe state-of-the-art methods, but also exhibits good efficiency,\ngeneralizability, and flexibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Fake News Detection with Adaptive Unimodal Representation Aggregation. (arXiv:2206.05741v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05741","description":"<p>The development of Internet technology has continuously intensified the\nspread and destructive power of rumors and fake news. Previous researches on\nmultimedia fake news detection include a series of complex feature extraction\nand fusion networks to achieve feature alignment between images and texts.\nHowever, what the multimodal features are composed of and how features from\ndifferent modalities affect the decision-making process are still open\nquestions. We present AURA, a multimodal fake news detection network with\nAdaptive Unimodal Representation Aggregation. We first extract representations\nrespectively from image pattern, image semantics and text, and multimodal\nrepresentations are generated by sending the semantic and linguistic\nrepresentations into an expert network. Then, we perform coarse-level fake news\ndetection and cross-modal cosistency learning according to the unimodal and\nmultimodal representations. The classification and consistency scores are\nmapped into modality-aware attention scores that readjust the features.\nFinally, we aggregation and classify the weighted features for refined fake\nnews detection. Comprehensive experiments on Weibo and Gossipcop prove that\nAURA can successfully beat several state-of-the-art FND schemes, where the\noverall prediction accuracy and the recall of fake news is steadily improved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1\">Qichao Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yangming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Zhenxing Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shiming Ge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Attack: Universal Adversarial Perturbation on Embodied Vision Navigation. (arXiv:2206.05751v1 [cs.LG])","link":"http://arxiv.org/abs/2206.05751","description":"<p>Embodied agents in vision navigation coupled with deep neural networks have\nattracted increasing attention. However, deep neural networks are vulnerable to\nmalicious adversarial noises, which may potentially cause catastrophic failures\nin Embodied Vision Navigation. Among these adversarial noises, universal\nadversarial perturbations (UAP), i.e., the image-agnostic perturbation applied\non each frame received by the agent, are more critical for Embodied Vision\nNavigation since they are computation-efficient and application-practical\nduring the attack. However, existing UAP methods do not consider the system\ndynamics of Embodied Vision Navigation. For extending UAP in the sequential\ndecision setting, we formulate the disturbed environment under the universal\nnoise $\\delta$, as a $\\delta$-disturbed Markov Decision Process ($\\delta$-MDP).\nBased on the formulation, we analyze the properties of $\\delta$-MDP and propose\ntwo novel Consistent Attack methods for attacking Embodied agents, which first\nconsider the dynamic of the MDP by estimating the disturbed Q function and the\ndisturbed distribution. In spite of victim models, our Consistent Attack can\ncause a significant drop in the performance for the Goalpoint task in habitat.\nExtensive experimental results indicate that there exist potential risks for\napplying Embodied Vision Navigation methods to the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiaoben_Y/0/1/0/all/0/1\">You Qiaoben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengyang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinning Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeATrans: Learning Segmentation-Assisted diagnosis model via Transforme. (arXiv:2206.05763v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05763","description":"<p>Clinically, the accurate annotation of lesions/tissues can significantly\nfacilitate the disease diagnosis. For example, the segmentation of optic\ndisc/cup (OD/OC) on fundus image would facilitate the glaucoma diagnosis, the\nsegmentation of skin lesions on dermoscopic images is helpful to the melanoma\ndiagnosis, etc. With the advancement of deep learning techniques, a wide range\nof methods proved the lesions/tissues segmentation can also facilitate the\nautomated disease diagnosis models. However, existing methods are limited in\nthe sense that they can only capture static regional correlations in the\nimages. Inspired by the global and dynamic nature of Vision Transformer, in\nthis paper, we propose Segmentation-Assisted diagnosis Transformer (SeATrans)\nto transfer the segmentation knowledge to the disease diagnosis network.\nSpecifically, we first propose an asymmetric multi-scale interaction strategy\nto correlate each single low-level diagnosis feature with multi-scale\nsegmentation features. Then, an effective strategy called SeA-block is adopted\nto vitalize diagnosis feature via correlated segmentation features. To model\nthe segmentation-diagnosis interaction, SeA-block first embeds the diagnosis\nfeature based on the segmentation information via the encoder, and then\ntransfers the embedding back to the diagnosis feature space by a decoder.\nExperimental results demonstrate that SeATrans surpasses a wide range of\nstate-of-the-art (SOTA) segmentation-assisted diagnosis methods on several\ndisease diagnosis tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semantic Consistency Feature Alignment Object Detection Model Based on Mixed-Class Distribution Metrics. (arXiv:2206.05765v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05765","description":"<p>Unsupervised domain adaptation is critical in various computer vision tasks,\nsuch as object detection, instance segmentation, etc. They attempt to reduce\ndomain bias-induced performance degradation while also promoting model\napplication speed. Previous works in domain adaptation object detection attempt\nto align image-level and instance-level shifts to eventually minimize the\ndomain discrepancy, but they may align single-class features to mixed-class\nfeatures in image-level domain adaptation because each image in the object\ndetection task may be more than one class and object. In order to achieve\nsingle-class with single-class alignment and mixed-class with mixed-class\nalignment, we treat the mixed-class of the feature as a new class and propose a\nmixed-classes $H-divergence$ for object detection to achieve homogenous feature\nalignment and reduce negative transfer. Then, a Semantic Consistency Feature\nAlignment Model (SCFAM) based on mixed-classes $H-divergence$ was also\npresented. To improve single-class and mixed-class semantic information and\naccomplish semantic separation, the SCFAM model proposes Semantic Prediction\nModels (SPM) and Semantic Bridging Components (SBC). And the weight of the pix\ndomain discriminator loss is then changed based on the SPM result to reduce\nsample imbalance. Extensive unsupervised domain adaption experiments on widely\nused datasets illustrate our proposed approach's robust object detection in\ndomain bias settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gou_L/0/1/0/all/0/1\">Lijun Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangcheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Whole-Slide Image Pyramids for Cancer Prognosis via Dual-Stream Networks. (arXiv:2206.05782v1 [eess.IV])","link":"http://arxiv.org/abs/2206.05782","description":"<p>The cancer prognosis on gigapixel Whole-Slide Images (WSIs) has always been a\nchallenging task. Most existing approaches focus solely on single-resolution\nimages. The multi-resolution schemes, utilizing image pyramids to enhance WSI\nvisual representations, have not yet been paid enough attention to. In order to\nexplore a multi-resolution solution for improving cancer prognosis accuracy,\nthis paper proposes a dual-stream architecture to model WSIs by an image\npyramid strategy. This architecture consists of two sub-streams: one is for\nlow-resolution WSIs, and the other is especially for high-resolution ones.\nCompared to other approaches, our scheme has three highlights: (i) there exists\na one-to-one relation between stream and resolution; (ii) a square pooling\nlayer is added to align the patches from two resolution streams, largely\nreducing computation cost and enabling a natural stream feature fusion; (iii) a\ncross-attention-based method is proposed to pool high-resolution patches\nspatially under the guidance of low-resolution ones. We validate our scheme on\nthree publicly-available datasets, a total number of 3,101 WSIs from 1,911\npatients. Experimental results verify that (1) hierarchical dual-stream\nrepresentation is more effective than single-stream ones for cancer prognosis,\ngaining an average C-Index rise of 5.0% and 1.8% on a single low-resolution and\nhigh-resolution stream, respectively; (2) our dual-stream scheme could\noutperform current state-of-the-art ones, by a 5.1% average improvement of\nC-Index; (3) the cancer diseases with observable survival differences could\nhave different preferences for model complexity. Our scheme could serve as an\nalternative tool for further facilitating WSI prognosis research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Pei Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_B/0/1/0/all/0/1\">Bo Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_F/0/1/0/all/0/1\">Feng Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_L/0/1/0/all/0/1\">Luping Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of Branch Specialization and its Application in Image Decomposition. (arXiv:2206.05810v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05810","description":"<p>Branched neural networks have been used extensively for a variety of tasks.\nBranches are sub-parts of the model that perform independent processing\nfollowed by aggregation. It is known that this setting induces a phenomenon\ncalled Branch Specialization, where different branches become experts in\ndifferent sub-tasks. Such observations were qualitative by nature. In this\nwork, we present a methodological analysis of Branch Specialization. We explain\nthe role of gradient descent in this phenomenon. We show that branched\ngenerative networks naturally decompose animal images to meaningful channels of\nfur, whiskers and spots and face images to channels such as different\nillumination components and face parts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brokman_J/0/1/0/all/0/1\">Jonathan Brokman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1\">Guy Gilboa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COLD Fusion: Calibrated and Ordinal Latent Distribution Fusion for Uncertainty-Aware Multimodal Emotion Recognition. (arXiv:2206.05833v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05833","description":"<p>Automatically recognising apparent emotions from face and voice is hard, in\npart because of various sources of uncertainty, including in the input data and\nthe labels used in a machine learning framework. This paper introduces an\nuncertainty-aware audiovisual fusion approach that quantifies modality-wise\nuncertainty towards emotion prediction. To this end, we propose a novel fusion\nframework in which we first learn latent distributions over audiovisual\ntemporal context vectors separately, and then constrain the variance vectors of\nunimodal latent distributions so that they represent the amount of information\neach modality provides w.r.t. emotion recognition. In particular, we impose\nCalibration and Ordinal Ranking constraints on the variance vectors of\naudiovisual latent distributions. When well-calibrated, modality-wise\nuncertainty scores indicate how much their corresponding predictions may differ\nfrom the ground truth labels. Well-ranked uncertainty scores allow the ordinal\nranking of different frames across the modalities. To jointly impose both these\nconstraints, we propose a softmax distributional matching loss. In both\nclassification and regression settings, we compare our uncertainty-aware fusion\nmodel with standard model-agnostic fusion baselines. Our evaluation on two\nemotion recognition corpora, AVEC 2019 CES and IEMOCAP, shows that audiovisual\nemotion recognition can considerably benefit from well-calibrated and\nwell-ranked latent uncertainty measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tellamekala_M/0/1/0/all/0/1\">Mani Kumar Tellamekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiriparian_S/0/1/0/all/0/1\">Shahin Amiriparian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1\">Elisabeth Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giesbrecht_T/0/1/0/all/0/1\">Timo Giesbrecht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1\">Michel Valstar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLIPv2: Unifying Localization and Vision-Language Understanding. (arXiv:2206.05836v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05836","description":"<p>We present GLIPv2, a grounded VL understanding model, that serves both\nlocalization tasks (e.g., object detection, instance segmentation) and\nVision-Language (VL) understanding tasks (e.g., VQA, image captioning). GLIPv2\nelegantly unifies localization pre-training and Vision-Language Pre-training\n(VLP) with three pre-training tasks: phrase grounding as a VL reformulation of\nthe detection task, region-word contrastive learning as a novel region-word\nlevel contrastive learning task, and the masked language modeling. This\nunification not only simplifies the previous multi-stage VLP procedure but also\nachieves mutual benefits between localization and understanding tasks.\nExperimental results show that a single GLIPv2 model (all model weights are\nshared) achieves near SoTA performance on various localization and\nunderstanding tasks. The model also shows (1) strong zero-shot and few-shot\nadaption performance on open-vocabulary object detection tasks and (2) superior\ngrounding capability on VL understanding tasks. Code will be released at\nhttps://github.com/microsoft/GLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralODF: Learning Omnidirectional Distance Fields for 3D Shape Representation. (arXiv:2206.05837v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05837","description":"<p>In visual computing, 3D geometry is represented in many different forms\nincluding meshes, point clouds, voxel grids, level sets, and depth images. Each\nrepresentation is suited for different tasks thus making the transformation of\none representation into another (forward map) an important and common problem.\nWe propose Omnidirectional Distance Fields (ODFs), a new 3D shape\nrepresentation that encodes geometry by storing the depth to the object's\nsurface from any 3D position in any viewing direction. Since rays are the\nfundamental unit of an ODF, it can be used to easily transform to and from\ncommon 3D representations like meshes or point clouds. Different from level set\nmethods that are limited to representing closed surfaces, ODFs are unsigned and\ncan thus model open surfaces (e.g., garments). We demonstrate that ODFs can be\neffectively learned with a neural network (NeuralODF) despite the inherent\ndiscontinuities at occlusion boundaries. We also introduce efficient forward\nmapping algorithms for transforming ODFs to and from common 3D representations.\nSpecifically, we introduce an efficient Jumping Cubes algorithm for generating\nmeshes from ODFs. Experiments demonstrate that NeuralODF can learn to capture\nhigh-quality shape by overfitting to a single object, and also learn to\ngeneralize on common shape categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Houchens_T/0/1/0/all/0/1\">Trevor Houchens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng-You Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duggal_S/0/1/0/all/0/1\">Shivam Duggal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Rao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficiency Comparison of AI classification algorithms for Image Detection and Recognition in Real-time. (arXiv:2206.05842v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05842","description":"<p>Face detection and identification is the most difficult and often used task\nin Artificial Intelligence systems. The goal of this study is to present and\ncompare the results of several face detection and recognition algorithms used\nin the system. This system begins with a training image of a human, then\ncontinues on to the test image, identifying the face, comparing it to the\ntrained face, and finally classifying it using OpenCV classifiers. This\nresearch will discuss the most effective and successful tactics used in the\nsystem, which are implemented using Python, OpenCV, and Matplotlib. It may also\nbe used in locations with CCTV, such as public spaces, shopping malls, and ATM\nbooths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nipun_M/0/1/0/all/0/1\">Musarrat Saberin Nipun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulaiman_R/0/1/0/all/0/1\">Rejwan Bin Sulaiman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kareem_A/0/1/0/all/0/1\">Amer Kareem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FisheyeEX: Polar Outpainting for Extending the FoV of Fisheye Lens. (arXiv:2206.05844v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05844","description":"<p>Fisheye lens gains increasing applications in computational photography and\nassisted driving because of its wide field of view (FoV). However, the fisheye\nimage generally contains invalid black regions induced by its imaging model. In\nthis paper, we present a FisheyeEX method that extends the FoV of the fisheye\nlens by outpainting the invalid regions, improving the integrity of captured\nscenes. Compared with the rectangle and undistorted image, there are two\nchallenges for fisheye image outpainting: irregular painting regions and\ndistortion synthesis. Observing the radial symmetry of the fisheye image, we\nfirst propose a polar outpainting strategy to extrapolate the coherent\nsemantics from the center to the outside region. Such an outpainting manner\nconsiders the distribution pattern of radial distortion and the circle\nboundary, boosting a more reasonable completion direction. For the distortion\nsynthesis, we propose a spiral distortion-aware perception module, in which the\nlearning path keeps consistent with the distortion prior of the fisheye image.\nSubsequently, a scene revision module rearranges the generated pixels with the\nestimated distortion to match the fisheye image, thus extending the FoV. In the\nexperiment, we evaluate the proposed FisheyeEX on three popular outdoor\ndatasets: Cityscapes, BDD100k, and KITTI, and one real-world fisheye image\ndataset. The results demonstrate that our approach significantly outperforms\nthe state-of-the-art methods, gaining around 27% more content beyond the\noriginal fisheye image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InBiaseD: Inductive Bias Distillation to Improve Generalization and Robustness through Shape-awareness. (arXiv:2206.05846v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05846","description":"<p>Humans rely less on spurious correlations and trivial cues, such as texture,\ncompared to deep neural networks which lead to better generalization and\nrobustness. It can be attributed to the prior knowledge or the high-level\ncognitive inductive bias present in the brain. Therefore, introducing\nmeaningful inductive bias to neural networks can help learn more generic and\nhigh-level representations and alleviate some of the shortcomings. We propose\nInBiaseD to distill inductive bias and bring shape-awareness to the neural\nnetworks. Our method includes a bias alignment objective that enforces the\nnetworks to learn more generic representations that are less vulnerable to\nunintended cues in the data which results in improved generalization\nperformance. InBiaseD is less susceptible to shortcut learning and also\nexhibits lower texture bias. The better representations also aid in improving\nrobustness to adversarial attacks and we hence plugin InBiaseD seamlessly into\nthe existing adversarial training schemes to show a better trade-off between\ngeneralization and robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1\">Shruthi Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Generalized Specialist Approach To Train Quality Resilient Snapshot Ensemble. (arXiv:2206.05853v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05853","description":"<p>Convolutional neural networks (CNNs) apply well with food image recognition\ndue to the ability to learn discriminative visual features. Nevertheless,\nrecognizing distorted images is challenging for existing CNNs. Hence, the study\nmodelled a generalized specialist approach to train a quality resilient\nensemble. The approach aids the models in the ensemble framework retain general\nskills of recognizing clean images and shallow skills of classifying noisy\nimages with one deep expertise area on a particular distortion. Subsequently, a\nnovel data augmentation random quality mixup (RQMixUp) is combined with\nsnapshot ensembling to train G-Specialist. During each training cycle of\nG-Specialist, a model is fine-tuned on the synthetic images generated by\nRQMixup, intermixing clean and distorted images of a particular distortion at a\nrandomly chosen level. Resultantly, each snapshot in the ensemble gained\nexpertise on several distortion levels, with shallow skills on other quality\ndistortions. Next, the filter outputs from diverse experts were fused for\nhigher accuracy. The learning process has no additional cost due to a single\ntraining process to train experts, compatible with a wide range of supervised\nCNNs for transfer learning. Finally, the experimental analysis on three\nreal-world food and a Malaysian food database showed significant improvement\nfor distorted images with competitive classification performance on pristine\nfood images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Ahmed Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zongying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Directed-Evolution Method for Sparsification and Compression of Neural Networks with Application to Object Identification and Segmentation and considerations of optimal quantization using small number of bits. (arXiv:2206.05859v1 [cs.LG])","link":"http://arxiv.org/abs/2206.05859","description":"<p>This work introduces Directed-Evolution (DE) method for sparsification of\nneural networks, where the relevance of parameters to the network accuracy is\ndirectly assessed and the parameters that produce the least effect on accuracy\nwhen tentatively zeroed are indeed zeroed. DE method avoids a potentially\ncombinatorial explosion of all possible candidate sets of parameters to be\nzeroed in large networks by mimicking evolution in the natural world. DE uses a\ndistillation context [5]. In this context, the original network is the teacher\nand DE evolves the student neural network to the sparsification goal while\nmaintaining minimal divergence between teacher and student. After the desired\nsparsification level is reached in each layer of the network by DE, a variety\nof quantization alternatives are used on the surviving parameters to find the\nlowest number of bits for their representation with acceptable loss of\naccuracy. A procedure to find optimal distribution of quantization levels in\neach sparsified layer is presented. Suitable final lossless encoding of the\nsurviving quantized parameters is used for the final parameter representation.\nDE was used in sample of representative neural networks using MNIST,\nFashionMNIST and COCO data sets with progressive larger networks. An 80 classes\nYOLOv3 with more than 60 million parameters network trained on COCO dataset\nreached 90% sparsification and correctly identifies and segments all objects\nidentified by the original network with more than 80% confidence using 4bit\nparameter quantization. Compression between 40x and 80x. It has not escaped the\nauthors that techniques from different methods can be nested. Once the best\nparameter set for sparsification is identified in a cycle of DE, a decision on\nzeroing only a sub-set of those parameters can be made using a combination of\ncriteria like parameter magnitude and Hessian approximations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Franca_Neto_L/0/1/0/all/0/1\">Luiz M Franca-Neto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TC-SfM: Robust Track-Community-Based Structure-from-Motion. (arXiv:2206.05866v1 [cs.CV])","link":"http://arxiv.org/abs/2206.05866","description":"<p>Structure-from-Motion (SfM) aims to recover 3D scene structures and camera\nposes based on the correspondences between input images, and thus the ambiguity\ncaused by duplicate structures (i.e., different structures with strong visual\nresemblance) always results in incorrect camera poses and 3D structures. To\ndeal with the ambiguity, most existing studies resort to additional constraint\ninformation or implicit inference by analyzing two-view geometries or feature\npoints. In this paper, we propose to exploit high-level information in the\nscene, i.e., the spatial contextual information of local regions, to guide the\nreconstruction. Specifically, a novel structure is proposed, namely,\n{\\textit{track-community}}, in which each community consists of a group of\ntracks and represents a local segment in the scene. A community detection\nalgorithm is used to partition the scene into several segments. Then, the\npotential ambiguous segments are detected by analyzing the neighborhood of\ntracks and corrected by checking the pose consistency. Finally, we perform\npartial reconstruction on each segment and align them with a novel\nbidirectional consistency cost function which considers both 3D-3D\ncorrespondences and pairwise relative camera poses. Experimental results\ndemonstrate that our approach can robustly alleviate reconstruction failure\nresulting from visually indistinguishable structures and accurately merge the\npartial reconstructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_L/0/1/0/all/0/1\">Linlin Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zihan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhaopeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jieqing Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantification and Analysis of Layer-wise and Pixel-wise Information Discarding. (arXiv:1906.04109v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1906.04109","description":"<p>This paper presents a method to explain how the information of each input\nvariable is gradually discarded during the forward propagation in a deep neural\nnetwork (DNN), which provides new perspectives to explain DNNs. We define two\ntypes of entropy-based metrics, i.e. (1) the discarding of pixel-wise\ninformation used in the forward propagation, and (2) the uncertainty of the\ninput reconstruction, to measure input information contained by a specific\nlayer from two perspectives. Unlike previous attribution metrics, the proposed\nmetrics ensure the fairness of comparisons between different layers of\ndifferent DNNs. We can use these metrics to analyze the efficiency of\ninformation processing in DNNs, which exhibits strong connections to the\nperformance of DNNs. We analyze information discarding in a pixel-wise manner,\nwhich is different from the information bottleneck theory measuring feature\ninformation w.r.t. the sample distribution. Experiments have shown the\neffectiveness of our metrics in analyzing classic DNNs and explaining existing\ndeep-learning techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing License Plates in Real-Time. (arXiv:1906.04376v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.04376","description":"<p>License plate detection and recognition (LPDR) is of growing importance for\nenabling intelligent transportation and ensuring the security and safety of the\ncities. However, LPDR faces a big challenge in a practical environment. The\nlicense plates can have extremely diverse sizes, fonts and colors, and the\nplate images are usually of poor quality caused by skewed capturing angles,\nuneven lighting, occlusion, and blurring. In applications such as surveillance,\nit often requires fast processing. To enable real-time and accurate license\nplate recognition, in this work, we propose a set of techniques: 1) a contour\nreconstruction method along with edge-detection to quickly detect the candidate\nplates; 2) a simple zero-one-alternation scheme to effectively remove the fake\ntop and bottom borders around plates to facilitate more accurate segmentation\nof characters on plates; 3) a set of techniques to augment the training data,\nincorporate SIFT features into the CNN network, and exploit transfer learning\nto obtain the initial parameters for more effective training; and 4) a\ntwo-phase verification procedure to determine the correct plate at low cost, a\nstatistical filtering in the plate detection stage to quickly remove unwanted\ncandidates, and the accurate CR results after the CR process to perform further\nplate verification without additional processing. We implement a complete LPDR\nsystem based on our algorithms. The experimental results demonstrate that our\nsystem can accurately recognize license plate in real-time. Additionally, it\nworks robustly under various levels of illumination and noise, and in the\npresence of car movement. Compared to peer schemes, our system is not only\namong the most accurate ones but is also the fastest, and can be easily applied\nto other scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Michael Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal, transferable and targeted adversarial attacks. (arXiv:1908.11332v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1908.11332","description":"<p>Deep Neural Networks have been found vulnerable re-cently. A kind of\nwell-designed inputs, which called adver-sarial examples, can lead the networks\nto make incorrectpredictions. Depending on the different scenarios, goalsand\ncapabilities, the difficulties of the attacks are different.For example, a\ntargeted attack is more difficult than a non-targeted attack, a universal\nattack is more difficult than anon-universal attack, a transferable attack is\nmore difficultthan a nontransferable one. The question is: Is there existan\nattack that can meet all these requirements? In this pa-per, we answer this\nquestion by producing a kind of attacksunder these conditions. We learn a\nuniversal mapping tomap the sources to the adversarial examples. These\nexam-ples can fool classification networks to classify all of theminto one\ntargeted class, and also have strong transferability.Our code is released at:\nxxxxx.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1\">Rao Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detached Error Feedback for Distributed SGD with Random Sparsification. (arXiv:2004.05298v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.05298","description":"<p>The communication bottleneck has been a critical problem in large-scale\ndistributed deep learning. In this work, we study distributed SGD with random\nblock-wise sparsification as the gradient compressor, which is ring-allreduce\ncompatible and highly computation-efficient but leads to inferior performance.\nTo tackle this important issue, we improve the communication-efficient\ndistributed SGD from a novel aspect, that is, the trade-off between the\nvariance and second moment of the gradient. With this motivation, we propose a\nnew detached error feedback (DEF) algorithm, which shows better convergence\nbound than error feedback for non-convex problems. We also propose DEF-A to\naccelerate the generalization of DEF at the early stages of the training, which\nshows better generalization bounds than DEF. Furthermore, we establish the\nconnection between communication-efficient distributed SGD and SGD with iterate\naveraging (SGD-IA) for the first time. Extensive deep learning experiments show\nsignificant empirical improvement of the proposed methods under various\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1\">An Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Based Sorghum Head Counting When You Only Look Once. (arXiv:2009.11929v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.11929","description":"<p>Modern trends in digital agriculture have seen a shift towards artificial\nintelligence for crop quality assessment and yield estimation. In this work, we\ndocument how a parameter tuned single-shot object detection algorithm can be\nused to identify and count sorghum head from aerial drone images. Our approach\ninvolves a novel exploratory analysis that identified key structural elements\nof the sorghum images and motivated the selection of parameter-tuned anchor\nboxes that contributed significantly to performance. These insights led to the\ndevelopment of a deep learning model that outperformed the baseline model and\nachieved an out-of-sample mean average precision of 0.95.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosley_L/0/1/0/all/0/1\">Lawrence Mosley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1\">Hieu Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_Y/0/1/0/all/0/1\">Yogesh Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hare_E/0/1/0/all/0/1\">Eric Hare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing A Large Scale 3D Face Dataset for Deep 3D Face Identification. (arXiv:2010.08391v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.08391","description":"<p>Deep learning methods have brought many breakthroughs to computer vision,\nespecially in 2D face recognition. However, the bottleneck of deep learning\nbased 3D face recognition is that it is difficult to collect millions of 3D\nfaces, whether for industry or academia. In view of this situation, there are\nmany methods to generate more 3D faces from existing 3D faces through 3D face\ndata augmentation, which are used to train deep 3D face recognition models.\nHowever, to the best of our knowledge, there is no method to generate 3D faces\nfrom 2D face images for training deep 3D face recognition models. This letter\nfocuses on the role of reconstructed 3D facial surfaces in 3D face\nidentification and proposes a framework of 2D-aided deep 3D face\nidentification. In particular, we propose to reconstruct millions of 3D face\nscans from a large scale 2D face database (i.e.VGGFace2), using a deep learning\nbased 3D face reconstruction method (i.e.ExpNet). Then, we adopt a two-phase\ntraining approach: In the first phase, we use millions of face images to\npre-train the deep convolutional neural network (DCNN), and in the second\nphase, we use normal component images (NCI) of reconstructed 3D face scans to\ntrain the DCNN. Extensive experimental results illustrate that the proposed\napproach can greatly improve the rank-1 score of 3D face identification on the\nFRGC v2.0, the Bosphorus, and the BU-3DFE 3D face databases, compared to the\nmodel trained by 2D face images. Finally, our proposed approach achieves\nstate-of-the-art rank-1 scores on the FRGC v2.0 (97.6%), Bosphorus (98.4%), and\nBU-3DFE (98.8%) databases. The experimental results show that the reconstructed\n3D facial surfaces are useful and our 2D-aided deep 3D face identification\nframework is meaningful, facing the scarcity of 3D faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cuican Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huibin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPU-Net: Self-Supervised Point Cloud Upsampling by Coarse-to-Fine Reconstruction with Self-Projection Optimization. (arXiv:2012.04439v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04439","description":"<p>The task of point cloud upsampling aims to acquire dense and uniform point\nsets from sparse and irregular point sets. Although significant progress has\nbeen made with deep learning models, state-of-the-art methods require\nground-truth dense point sets as the supervision, which makes them limited to\nbe trained under synthetic paired training data and not suitable to be under\nreal-scanned sparse data. However, it is expensive and tedious to obtain large\nnumbers of paired sparse-dense point sets as supervision from real-scanned\nsparse data. To address this problem, we propose a self-supervised point cloud\nupsampling network, named SPU-Net, to capture the inherent upsampling patterns\nof points lying on the underlying object surface. Specifically, we propose a\ncoarse-to-fine reconstruction framework, which contains two main components:\npoint feature extraction and point feature expansion, respectively. In the\npoint feature extraction, we integrate the self-attention module with the graph\nconvolution network (GCN) to capture context information inside and among local\nregions simultaneously. In the point feature expansion, we introduce a\nhierarchically learnable folding strategy to generate upsampled point sets with\nlearnable 2D grids. Moreover, to further optimize the noisy points in the\ngenerated point sets, we propose a novel self-projection optimization\nassociated with uniform and reconstruction terms as a joint loss to facilitate\nthe self-supervised point cloud upsampling. We conduct various experiments on\nboth synthetic and real-scanned datasets, and the results demonstrate that we\nachieve comparable performances to state-of-the-art supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinhai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Salient Object Detection via Integrity Learning. (arXiv:2101.07663v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.07663","description":"<p>Although current salient object detection (SOD) works have achieved\nsignificant progress, they are limited when it comes to the integrity of the\npredicted salient regions. We define the concept of integrity at both a micro\nand macro level. Specifically, at the micro level, the model should highlight\nall parts that belong to a certain salient object. Meanwhile, at the macro\nlevel, the model needs to discover all salient objects in a given image. To\nfacilitate integrity learning for SOD, we design a novel Integrity Cognition\nNetwork (ICON), which explores three important components for learning strong\nintegrity features. 1) Unlike existing models, which focus more on feature\ndiscriminability, we introduce a diverse feature aggregation (DFA) component to\naggregate features with various receptive fields (i.e., kernel shape and\ncontext) and increase feature diversity. Such diversity is the foundation for\nmining the integral salient objects. 2) Based on the DFA features, we introduce\nan integrity channel enhancement (ICE) component with the goal of enhancing\nfeature channels that highlight the integral salient objects, while suppressing\nthe other distracting ones. 3) After extracting the enhanced features, the\npart-whole verification (PWV) method is employed to determine whether the part\nand whole object features have strong agreement. Such part-whole agreements can\nfurther improve the micro-level integrity for each salient object. To\ndemonstrate the effectiveness of our ICON, comprehensive experiments are\nconducted on seven challenging benchmarks. Our ICON outperforms the baseline\nmethods in terms of a wide range of metrics. Notably, our ICON achieves about\n10% relative improvement over the previous best model in terms of average false\nnegative ratio (FNR), on six datasets. Codes and results are available at:\nhttps://github.com/mczhuge/ICON.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuge_M/0/1/0/all/0/1\">Mingchen Zhuge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliminate Deviation with Deviation for Data Augmentation and a General Multi-modal Data Learning Method. (arXiv:2101.08533v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.08533","description":"<p>One of the challenges of computer vision is that it needs to adapt to color\ndeviations in changeable environments. Therefore, minimizing the adverse\neffects of color deviation on the prediction is one of the main goals of vision\ntask. Current solutions focus on using generative models to augment training\ndata to enhance the invariance of input variation. However, such methods often\nintroduce new noise, which limits the gain from generated data. To this end,\nthis paper proposes a strategy eliminate deviation with deviation, which is\nnamed Random Color Dropout (RCD). Our hypothesis is that if there are color\ndeviation between the query image and the gallery image, the retrieval results\nof some examples will be better after ignoring the color information.\nSpecifically, this strategy balances the weights between color features and\ncolor-independent features in the neural network by dropouting partial color\ninformation in the training data, so as to overcome the effect of color\ndevitaion. The proposed RCD can be combined with various existing ReID models\nwithout changing the learning strategy, and can be applied to other computer\nvision fields, such as object detection. Experiments on several ReID baselines\nand three common large-scale datasets such as Market1501, DukeMTMC, and MSMT17\nhave verified the effectiveness of this method. Experiments on Cross-domain\ntests have shown that this strategy is significant eliminating the domain gap.\nFurthermore, in order to understand the working mechanism of RCD, we analyzed\nthe effectiveness of this strategy from the perspective of classification,\nwhich reveals that it may be better to utilize many instead of all of color\ninformation in visual tasks with strong domain variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yunpeng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lifei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GANav: Efficient Terrain Segmentation for Robot Navigation in Unstructured Outdoor Environments. (arXiv:2103.04233v4 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2103.04233","description":"<p>We propose GANav, a novel group-wise attention mechanism to identify safe and\nnavigable regions in off-road terrains and unstructured environments from RGB\nimages. Our approach classifies terrains based on their navigability levels\nusing coarse-grained semantic segmentation. Our novel group-wise attention loss\nenables any backbone network to explicitly focus on the different groups'\nfeatures with low spatial resolution. Our design leads to efficient inference\nwhile maintaining a high level of accuracy compared to existing SOTA methods.\nOur extensive evaluations on the RUGD and RELLIS-3D datasets shows that GANav\nachieves an improvement over the SOTA mIoU by 2.25-39.05% on RUGD and\n5.17-19.06% on RELLIS-3D. We interface GANav with a deep reinforcement\nlearning-based navigation algorithm and highlight its benefits in terms of\nnavigation in real-world unstructured terrains. We integrate our GANav-based\nnavigation algorithm with ClearPath Jackal and Husky robots, and observe an\nincrease of 10% in terms of success rate, 2-47% in terms of selecting the\nsurface with the best navigability and a decrease of 4.6-13.9% in trajectory\nroughness. Further, GANav reduces the false positive rate of forbidden regions\nby 37.79%. Code, videos, and a full technical report are available at\nhttps://gamma.umd.edu/offroad/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kothandaraman_D/0/1/0/all/0/1\">Divya Kothandaraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathyamoorthy_A/0/1/0/all/0/1\">Adarsh Jagan Sathyamoorthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weerakoon_K/0/1/0/all/0/1\">Kasun Weerakoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VGNMN: Video-grounded Neural Module Network to Video-Grounded Language Tasks. (arXiv:2104.07921v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07921","description":"<p>Neural module networks (NMN) have achieved success in image-grounded tasks\nsuch as Visual Question Answering (VQA) on synthetic images. However, very\nlimited work on NMN has been studied in the video-grounded dialogue tasks.\nThese tasks extend the complexity of traditional visual tasks with the\nadditional visual temporal variance and language cross-turn dependencies.\nMotivated by recent NMN approaches on image-grounded tasks, we introduce\nVideo-grounded Neural Module Network (VGNMN) to model the information retrieval\nprocess in video-grounded language tasks as a pipeline of neural modules. VGNMN\nfirst decomposes all language components in dialogues to explicitly resolve any\nentity references and detect corresponding action-based inputs from the\nquestion. The detected entities and actions are used as parameters to\ninstantiate neural module networks and extract visual cues from the video. Our\nexperiments show that VGNMN can achieve promising performance on a challenging\nvideo-grounded dialogue benchmark as well as a video QA benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularized Deep Linear Discriminant Analysis. (arXiv:2105.07129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.07129","description":"<p>As a non-linear extension of the classic Linear Discriminant Analysis(LDA),\nDeep Linear Discriminant Analysis(DLDA) replaces the original Categorical Cross\nEntropy(CCE) loss function with eigenvalue-based loss function to make a deep\nneural network(DNN) able to learn linearly separable hidden representations. In\nthis paper, we first point out DLDA focuses on training the cooperative\ndiscriminative ability of all the dimensions in the latent subspace, while put\nless emphasis on training the separable capacity of single dimension. To\nimprove DLDA, a regularization method on within-class scatter matrix is\nproposed to strengthen the discriminative ability of each dimension, and also\nkeep them complement each other. Experiment results on STL-10, CIFAR-10 and\nPediatric Pneumonic Chest X-ray Dataset showed that our proposed regularization\nmethod Regularized Deep Linear Discriminant Analysis(RDLDA) outperformed DLDA\nand conventional neural network with CCE as objective. To further improve the\ndiscriminative ability of RDLDA in the local space, an algorithm named Subclass\nRDLDA is also proposed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Deep Learning Techniques for Epileptic Seizures Detection and Prediction Based on Neuroimaging Modalities: Methods, Challenges, and Future Works. (arXiv:2105.14278v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14278","description":"<p>Since epilepsy happens due to abnormal activity in the brain, seizures can\naffect any process your brain handles. Some signs and symptoms of seizures\ninclude confusion, abnormal staring, and rapid, sudden, and uncontrollable hand\nmovements. Epileptic seizure detection methods involve neurological exams,\nblood tests, neuropsychological tests, and neuroimaging modalities. Among\nthese, neuroimaging modalities have received considerable attention from\nspecialist physicians. One method to facilitate the accurate and fast diagnosis\nof epileptic seizures is to employ computer-aided diagnosis systems (CADS)\nbased on deep learning (DL) and neuroimaging modalities. This paper has studied\na comprehensive overview of DL methods exploited for epileptic seizures\ndetection and prediction using neuroimaging modalities. First, DL-based CADS\nfor the epileptic seizures detection and prediction using neuroimaging\nmodalities are discussed. Also, descriptions of various datasets, preprocessing\nalgorithms, and DL models which have been used for epileptic seizures detection\nand prediction have been included. Then, research on rehabilitation tools has\nbeen presented, which contains brain-computer interface (BCI), implantable,\ncloud computing, internet of things (IoT), hardware implementation of DL\ntechniques on field-programmable gate array (FPGA), etc. In the discussion\nsection, a comparison has been carried out between research on epileptic\nseizure detection and prediction. The most important challenges in epileptic\nseizures detection and prediction using neuroimaging modalities and DL models\nhave been expressed. In addition, future work proposal in the fields of\ndatasets, DL, rehabilitation, and hardware models has been proposed. The final\nsection is dedicated to the conclusion and incorporates the most significant\nfindings in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghassemi_N/0/1/0/all/0/1\">Navid Ghassemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodatars_M/0/1/0/all/0/1\">Marjane Khodatars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jafari_M/0/1/0/all/0/1\">Mahboobeh Jafari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moridian_P/0/1/0/all/0/1\">Parisa Moridian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yinan Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan Manuel Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1\">Javier Ram&#xed;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_U/0/1/0/all/0/1\">U. Rajendra Acharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.01100","description":"<p>During lung radiotherapy, the position of infrared reflective objects on the\nchest can be recorded to estimate the tumor location. However, radiotherapy\nsystems have a latency inherent to robot control limitations that impedes the\nradiation delivery precision. Prediction with online learning of recurrent\nneural networks (RNN) allows for adaptation to non-stationary respiratory\nsignals, but classical methods such as RTRL and truncated BPTT are respectively\nslow and biased. This study investigates the capabilities of unbiased online\nrecurrent optimization (UORO) to forecast respiratory motion and enhance safety\nin lung radiotherapy.\n</p>\n<p>We used 9 observation records of the 3D position of 3 external markers on the\nchest and abdomen of healthy individuals breathing during intervals from 73s to\n222s. The sampling frequency was 10Hz, and the amplitudes of the recorded\ntrajectories range from 6mm to 40mm in the superior-inferior direction. We\nforecast the 3D location of each marker simultaneously with a horizon value\nbetween 0.1s and 2.0s, using an RNN trained with UORO. We compare its\nperformance with an RNN trained with RTRL, LMS, and offline linear regression.\nWe provide closed-form expressions for quantities involved in the gradient loss\ncalculation in UORO, thereby making its implementation efficient. Training and\ncross-validation were performed during the first minute of each sequence.\n</p>\n<p>On average over the horizon values considered and the 9 sequences, UORO\nachieves the lowest root-mean-square (RMS) error and maximum error among the\ncompared algorithms. These errors are respectively equal to 1.3mm and 8.8mm,\nand the prediction time per time step was lower than 2.8ms (Dell Intel core\ni9-9900K 3.60 GHz). Linear regression has the lowest RMS error for the horizon\nvalues 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,\nand UORO for horizon values greater than 0.6s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pohl_M/0/1/0/all/0/1\">Michel Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uesaka_M/0/1/0/all/0/1\">Mitsuru Uesaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takahashi_H/0/1/0/all/0/1\">Hiroyuki Takahashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demachi_K/0/1/0/all/0/1\">Kazuyuki Demachi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chhatkuli_R/0/1/0/all/0/1\">Ritu Bhusal Chhatkuli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Temporal Action Detection with Transformer. (arXiv:2106.10271v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10271","description":"<p>Temporal action detection (TAD) aims to determine the semantic label and the\nboundaries of every action instance in an untrimmed video. Previous methods\ntackle this task with complex pipelines. In this paper, we propose an\nend-to-end temporal action detection Transformer (TadTR) with a simple set\nprediction pipeline. Given a small set of learnable embeddings called action\nqueries, TadTR adaptively extracts temporal context from the video for each\nquery and directly predicts action instances. To adapt Transformer for TAD, we\npropose three improvements to enhance its locality awareness. The core is a\ntemporal deformable attention module that selectively attends to a sparse set\nof key snippets in a video. A segment refinement mechanism and an actionness\nregression head are designed to refine the boundaries and confidence of the\npredicted instances, respectively. TadTR requires lower computation cost than\nprevious detectors while preserving remarkable performance. As a self-contained\ndetector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and\nHACS Segments (32.09% mAP). Combined with an extra action classifier, it\nobtains 36.75% mAP on ActivityNet-1.3. Our code is available at\n\\url{https://github.com/xlliu7/TadTR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qimeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Statistics Guided Label Refurbishment Mechanism: Mitigating Label Noise in Medical Image Classification. (arXiv:2106.12284v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.12284","description":"<p>Purpose: Deep neural networks (DNNs) have been widely applied in medical\nimage classification, benefiting from its powerful mapping capability among\nmedical images. However, these existing deep learning-based methods depend on\nan enormous amount of carefully labeled images. Meanwhile, noise is inevitably\nintroduced in the labeling process, degrading the performance of models. Hence,\nit's significant to devise robust training strategies to mitigate label noise\nin the medical image classification tasks. Methods: In this work, we propose a\nnovel Bayesian statistics guided label refurbishment mechanism (BLRM) for DNNs\nto prevent overfitting noisy images. BLRM utilizes maximum a posteriori\nprobability (MAP) in the Bayesian statistics and the exponentially\ntime-weighted technique to selectively correct the labels of noisy images. The\ntraining images are purified gradually with the training epochs when BLRM is\nactivated, further improving classification performance. Results: Comprehensive\nexperiments on both synthetic noisy images (public OCT &amp; Messidor datasets) and\nreal-world noisy images (ANIMAL-10N) demonstrate that BLRM refurbishes the\nnoisy labels selectively, curbing the adverse effects of noisy data. Also, the\nanti-noise BLRM integrated with DNNs are effective at different noise ratio and\nare independent of backbone DNN architectures. In addition, BLRM is superior to\nstate-of-the-art comparative methods of anti-noise. Conclusions: These\ninvestigations indicate that the proposed BLRM is well capable of mitigating\nlabel noise in medical image classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mengdi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Ximeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1\">Mufeng Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhe Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiangxi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chuanqing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1\">Qiushi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yanye Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imaging dynamics beneath turbid media via parallelized single-photon detection. (arXiv:2107.01422v4 [physics.optics] UPDATED)","link":"http://arxiv.org/abs/2107.01422","description":"<p>Noninvasive optical imaging through dynamic scattering media has numerous\nimportant biomedical applications but still remains a challenging task. While\nstandard diffuse imaging methods measure optical absorption or fluorescent\nemission, it is also well-established that the temporal correlation of\nscattered coherent light diffuses through tissue much like optical intensity.\nFew works to date, however, have aimed to experimentally measure and process\nsuch temporal correlation data to demonstrate deep-tissue video reconstruction\nof decorrelation dynamics. In this work, we utilize a single-photon avalanche\ndiode (SPAD) array camera to simultaneously monitor the temporal dynamics of\nspeckle fluctuations at the single-photon level from 12 different phantom\ntissue surface locations delivered via a customized fiber bundle array. We then\napply a deep neural network to convert the acquired single-photon measurements\ninto video of scattering dynamics beneath rapidly decorrelating tissue\nphantoms. We demonstrate the ability to reconstruct images of transient\n(0.1-0.4s) dynamic events occurring up to 8 mm beneath a decorrelating tissue\nphantom with millimeter-scale resolution, and highlight how our model can\nflexibly extend to monitor flow speed within buried phantom vessels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim Jonsson</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kreiss_L/0/1/0/all/0/1\">Lucas Kreiss</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINERVAS: Massive INterior EnviRonments VirtuAl Synthesis. (arXiv:2107.06149v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06149","description":"<p>With the rapid development of data-driven techniques, data has played an\nessential role in various computer vision tasks. Many realistic and synthetic\ndatasets have been proposed to address different problems. However, there are\nlots of unresolved challenges: (1) the creation of dataset is usually a tedious\nprocess with manual annotations, (2) most datasets are only designed for a\nsingle specific task, (3) the modification or randomization of the 3D scene is\ndifficult, and (4) the release of commercial 3D data may encounter copyright\nissue. This paper presents MINERVAS, a Massive INterior EnviRonments VirtuAl\nSynthesis system, to facilitate the 3D scene modification and the 2D image\nsynthesis for various vision tasks. In particular, we design a programmable\npipeline with Domain-Specific Language, allowing users to (1) select scenes\nfrom the commercial indoor scene database, (2) synthesize scenes for different\ntasks with customized rules, and (3) render various imagery data, such as\nvisual color, geometric structures, semantic label. Our system eases the\ndifficulty of customizing massive numbers of scenes for different tasks and\nrelieves users from manipulating fine-grained scene configurations by providing\nuser-controllable randomness using multi-level samplers. Most importantly, it\nempowers users to access commercial scene databases with millions of indoor\nscenes and protects the copyright of core data assets, e.g., 3D CAD models. We\ndemonstrate the validity and flexibility of our system by using our synthesized\ndata to improve the performance on different kinds of computer vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Haocheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiaxiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1\">Rui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_a/0/1/0/all/0/1\">and Yuchi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weighted Intersection over Union (wIoU): A New Evaluation Metric for Image Segmentation. (arXiv:2107.09858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09858","description":"<p>In this paper, we propose a novel evaluation metric for performance\nevaluation of semantic segmentation. In recent years, many studies have tried\nto train pixel-level classifiers on large-scale image datasets to perform\naccurate semantic segmentation. The goal of semantic segmentation is to assign\na class label of each pixel in the scene. It has various potential applications\nin computer vision fields e.g., object detection, classification, scene\nunderstanding and Etc. To validate the proposed wIoU evaluation metric, we\ntested state-of-the art methods on public benchmark datasets (e.g., KITTI)\nbased on the proposed wIoU metric and compared with other conventional\nevaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yeong-Jun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Shots Are All You Need: A Progressive Few Shot Learning Approach for Low Resource Handwritten Text Recognition. (arXiv:2107.10064v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10064","description":"<p>Handwritten text recognition in low resource scenarios, such as manuscripts\nwith rare alphabets, is a challenging problem. The main difficulty comes from\nthe very few annotated data and the limited linguistic information (e.g.\ndictionaries and language models). Thus, we propose a few-shot learning-based\nhandwriting recognition approach that significantly reduces the human labor\nannotation process, requiring only few images of each alphabet symbol. The\nmethod consists in detecting all the symbols of a given alphabet in a textline\nimage and decoding the obtained similarity scores to the final sequence of\ntranscribed symbols. Our model is first pretrained on synthetic line images\ngenerated from any alphabet, even though different from the target domain. A\nsecond training step is then applied to diminish the gap between the source and\ntarget data. Since this retraining would require annotation of thousands of\nhandwritten symbols together with their bounding boxes, we propose to avoid\nsuch human effort through an unsupervised progressive learning approach that\nautomatically assigns pseudo-labels to the non-annotated data. The evaluation\non different manuscript datasets show that our model can lead to competitive\nresults with a significant reduction in human effort. The code will be publicly\navailable in this repository: \\url{https://github.com/dali92002/HTRbyMatching}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Megyesi_B/0/1/0/all/0/1\">Be&#xe1;ta Megyesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Content-aware Directed Propagation Network with Pixel Adaptive Kernel Attention. (arXiv:2107.13144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13144","description":"<p>Convolutional neural networks (CNNs) have been not only widespread but also\nachieved noticeable results on numerous applications including image\nclassification, restoration, and generation. Although the weight-sharing\nproperty of convolutions makes them widely adopted in various tasks, its\ncontent-agnostic characteristic can also be considered a major drawback. To\nsolve this problem, in this paper, we propose a novel operation, called pixel\nadaptive kernel attention (PAKA). PAKA provides directivity to the filter\nweights by multiplying spatially varying attention from learnable features. The\nproposed method infers pixel-adaptive attention maps along the channel and\nspatial directions separately to address the decomposed model with fewer\nparameters. Our method is trainable in an end-to-end manner and applicable to\nany CNN-based models. In addition, we propose an improved information\naggregation module with PAKA, called the hierarchical PAKA module (HPM). We\ndemonstrate the superiority of our HPM by presenting state-of-the-art\nperformance on semantic segmentation compared to the conventional information\naggregation modules. We validate the proposed method through additional\nablation studies and visualizing the effect of PAKA providing directivity to\nthe weights of convolutions. We also show the generalizability of the proposed\nmethod by applying it to multi-modal tasks especially color-guided depth map\nsuper-resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sagong_M/0/1/0/all/0/1\">Min-Cheol Sagong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yoon-Jae Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Sung-Jea Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-Temporal Sparsity. (arXiv:2108.02297v5 [cs.AR] UPDATED)","link":"http://arxiv.org/abs/2108.02297","description":"<p>Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time-sequential data such as speech recognition. Unlike\nprevious LSTM accelerators that either exploit spatial weight sparsity or\ntemporal activation sparsity, this paper proposes a new accelerator called\n\"Spartus\" that exploits spatio-temporal sparsity to achieve ultra-low latency\ninference. Spatial sparsity is induced using a new Column-Balanced Targeted\nDropout (CBTD) structured pruning method, producing structured sparse weight\nmatrices for a balanced workload. The pruned networks running on Spartus\nhardware achieve weight sparsity levels of up to 96% and 94% with negligible\naccuracy loss on the TIMIT and the Librispeech datasets. To induce temporal\nsparsity in LSTM, we extend the previous DeltaGRU method to the DeltaLSTM\nmethod. Combining spatio-temporal sparsity with CBTD and DeltaLSTM saves on\nweight memory access and associated arithmetic operations. The Spartus\narchitecture is scalable and supports real-time online speech recognition when\nimplemented on small and large FPGAs. Spartus per-sample latency for a single\nDeltaLSTM layer of 1024 neurons averages 1 us. Exploiting spatio-temporal\nsparsity on our test LSTM network using the TIMIT dataset leads to 46X speedup\nof Spartus over its theoretical hardware performance to achieve 9.4 TOp/s\neffective batch-1 throughput and 1.1 TOp/s/W power efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer Learning Gaussian Anomaly Detection by Fine-tuning Representations. (arXiv:2108.04116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.04116","description":"<p>Current state-of-the-art anomaly detection (AD) methods exploit the powerful\nrepresentations yielded by large-scale ImageNet training. However, catastrophic\nforgetting prevents the successful fine-tuning of pre-trained representations\non new datasets in the semi-supervised setting, and representations are\ntherefore commonly fixed. In our work, we propose a new method to overcome\ncatastrophic forgetting and thus successfully fine-tune pre-trained\nrepresentations for AD in the transfer learning setting. Specifically, we\ninduce a multivariate Gaussian distribution for the normal class based on the\nlinkage between generative and discriminative modeling, and use the Mahalanobis\ndistance of normal images to the estimated distribution as training objective.\nWe additionally propose to use augmentations commonly employed for vicinal risk\nminimization in a validation scheme to detect onset of catastrophic forgetting.\nExtensive evaluations on the public MVTec dataset reveal that a new state of\nthe art is achieved by our method in the AD task while simultaneously achieving\nanomaly segmentation performance comparable to prior state of the art. Further,\nablation studies demonstrate the importance of the induced Gaussian\ndistribution as well as the robustness of the proposed fine-tuning scheme with\nrespect to the choice of augmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rippel_O/0/1/0/all/0/1\">Oliver Rippel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavan_A/0/1/0/all/0/1\">Arnav Chavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chucai Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A MIMO Radar-based Few-Shot Learning Approach for Human-ID. (arXiv:2110.08595v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2110.08595","description":"<p>Radar for deep learning-based human identification has become a research area\nof increasing interest. It has been shown that micro-Doppler ($\\mu$-D) can\nreflect the walking behavior through capturing the periodic limbs'\nmicro-motions. One of the main aspects is maximizing the number of included\nclasses while considering the real-time and training dataset size constraints.\nIn this paper, a multiple-input-multiple-output (MIMO) radar is used to\nformulate micro-motion spectrograms of the elevation angular velocity\n($\\mu$-$\\omega$). The effectiveness of concatenating this newly-formulated\nspectrogram with the commonly used $\\mu$-D is investigated. To accommodate for\nnon-constrained real walking motion, an adaptive cycle segmentation framework\nis utilized and a metric learning network is trained on half gait cycles\n($\\approx$ 0.5 s). Studies on the effects of various numbers of classes\n(5--20), different dataset sizes, and varying observation time windows 1--2 s\nare conducted. A non-constrained walking dataset of 22 subjects is collected\nwith different aspect angles with respect to the radar. The proposed few-shot\nlearning (FSL) approach achieves a classification error of 11.3 % with only 2\nmin of training data per subject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Weller_P/0/1/0/all/0/1\">Pascal Weller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aziz_F/0/1/0/all/0/1\">Fady Aziz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abdulatif_S/0/1/0/all/0/1\">Sherif Abdulatif</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schneider_U/0/1/0/all/0/1\">Urs Schneider</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huber_M/0/1/0/all/0/1\">Marco F. Huber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLOOB: Modern Hopfield Networks with InfoLOOB Outperform CLIP. (arXiv:2110.11316v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11316","description":"<p>CLIP yielded impressive results on zero-shot transfer learning tasks and is\nconsidered as a foundation model like BERT or GPT3. CLIP vision models that\nhave a rich representation are pre-trained using the InfoNCE objective and\nnatural language supervision before they are fine-tuned on particular tasks.\nThough CLIP excels at zero-shot transfer learning, it suffers from an\nexplaining away problem, that is, it focuses on one or few features, while\nneglecting other relevant features. This problem is caused by insufficiently\nextracting the covariance structure in the original multi-modal data. We\nsuggest to use modern Hopfield networks to tackle the problem of explaining\naway. Their retrieved embeddings have an enriched covariance structure derived\nfrom co-occurrences of features in the stored embeddings. However, modern\nHopfield networks increase the saturation effect of the InfoNCE objective which\nhampers learning. We propose to use the InfoLOOB objective to mitigate this\nsaturation effect. We introduce the novel ``Contrastive Leave One Out Boost''\n(CLOOB), which uses modern Hopfield networks for covariance enrichment together\nwith the InfoLOOB objective. In experiments we compare CLOOB to CLIP after\npre-training on the Conceptual Captions and the YFCC dataset with respect to\ntheir zero-shot transfer learning performance on other datasets. CLOOB\nconsistently outperforms CLIP at zero-shot transfer learning across all\nconsidered architectures and datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furst_A/0/1/0/all/0/1\">Andreas F&#xfc;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rumetshofer_E/0/1/0/all/0/1\">Elisabeth Rumetshofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehner_J/0/1/0/all/0/1\">Johannes Lehner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Viet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramsauer_H/0/1/0/all/0/1\">Hubert Ramsauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreil_D/0/1/0/all/0/1\">David Kreil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kopp_M/0/1/0/all/0/1\">Michael Kopp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1\">G&#xfc;nter Klambauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bitto_Nemling_A/0/1/0/all/0/1\">Angela Bitto-Nemling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1\">Sepp Hochreiter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Person Re-identification with Multi-Modal Joint Defence. (arXiv:2111.09571v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09571","description":"<p>The Person Re-identification (ReID) system based on metric learning has been\nproved to inherit the vulnerability of deep neural networks (DNNs), which are\neasy to be fooled by adversarail metric attacks. Existing work mainly relies on\nadversarial training for metric defense, and more methods have not been fully\nstudied. By exploring the impact of attacks on the underlying features, we\npropose targeted methods for metric attacks and defence methods. In terms of\nmetric attack, we use the local color deviation to construct the intra-class\nvariation of the input to attack color features. In terms of metric defenses,\nwe propose a joint defense method which includes two parts of proactive defense\nand passive defense. Proactive defense helps to enhance the robustness of the\nmodel to color variations and the learning of structure relations across\nmultiple modalities by constructing different inputs from multimodal images,\nand passive defense exploits the invariance of structural features in a\nchanging pixel space by circuitous scaling to preserve structural features\nwhile eliminating some of the adversarial noise. Extensive experiments\ndemonstrate that the proposed joint defense compared with the existing\nadversarial metric defense methods which not only against multiple attacks at\nthe same time but also has not significantly reduced the generalization\ncapacity of the model. The code is available at\nhttps://github.com/finger-monkey/multi-modal_joint_defence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yunpeng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lifei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XnODR and XnIDR: Two Accurate and Fast Fully Connected Layers For Convolutional Neural Networks. (arXiv:2111.10854v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10854","description":"<p>Although Capsule Network is powerful at defining the positional relationship\nbetween features in deep neural networks for visual recognition tasks, it is\ncomputationally expensive and not suitable for running on mobile devices. The\nbottleneck is in the computational complexity of the Dynamic Routing mechanism\nused between the capsules. On the other hand, XNOR-Net is fast and\ncomputationally efficient, though it suffers from low accuracy due to\ninformation loss in the binarization process. To address the computational\nburdens of the Dynamic Routing mechanism, this paper proposes new Fully\nConnected (FC) layers by xnorizing the linear projector outside or inside the\nDynamic Routing within the CapsFC layer. Specifically, our proposed FC layers\nhave two versions, XnODR (Xnorize the Linear Projection Linear Projector\nOutside Dynamic Routing) and XnIDR (Xnorize the Linear Projection Linear\nProjector Inside Dynamic Routing). To test the generalization of both XnODR and\nXnIDR, we insert them into two different networks, MobileNet V2 and ResNet-50.\nOur experiments on three datasets, MNIST, CIFAR-10, and MultiMNIST validate\ntheir effectiveness. The results demonstrate that both XnODR and XnIDR help\nnetworks to have high accuracy with lower FLOPs and fewer parameters (e.g.,\n95.32\\% correctness with 2.99M parameters and 312.04M FLOPs on CIFAR-10).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_A/0/1/0/all/0/1\">Ali Pourramezan Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahoor_M/0/1/0/all/0/1\">Mohammad H. Mahoor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-shape Transformer for Underwater Image Enhancement. (arXiv:2111.11843v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11843","description":"<p>The light absorption and scattering of underwater impurities lead to poor\nunderwater imaging quality. The existing data-driven based underwater image\nenhancement (UIE) techniques suffer from the lack of a large-scale dataset\ncontaining various underwater scenes and high-fidelity reference images.\nBesides, the inconsistent attenuation in different color channels and space\nareas is not fully considered for boosted enhancement. In this work, we\nconstructed a large-scale underwater image (LSUI) dataset including 5004 image\npairs, and reported an U-shape Transformer network where the transformer model\nis for the first time introduced to the UIE task. The U-shape Transformer is\nintegrated with a channel-wise multi-scale feature fusion transformer (CMSFFT)\nmodule and a spatial-wise global feature modeling transformer (SGFMT) module,\nwhich reinforce the network's attention to the color channels and space areas\nwith more serious attenuation. Meanwhile, in order to further improve the\ncontrast and saturation, a novel loss function combining RGB, LAB and LCH color\nspaces is designed following the human vision principle. The extensive\nexperiments on available datasets validate the state-of-the-art performance of\nthe reported technique with more than 2dB superiority.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Lintao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chunli Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_L/0/1/0/all/0/1\">Liheng Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car. (arXiv:2112.02604v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02604","description":"<p>Prediction of pedestrian behavior is critical for fully autonomous vehicles\nto drive in busy city streets safely and efficiently. The future autonomous\ncars need to fit into mixed conditions with not only technical but also social\ncapabilities. As more algorithms and datasets have been developed to predict\npedestrian behaviors, these efforts lack the benchmark labels and the\ncapability to estimate the temporal-dynamic intent changes of the pedestrians,\nprovide explanations of the interaction scenes, and support algorithms with\nsocial intelligence. This paper proposes and shares another benchmark dataset\ncalled the IUPUI-CSRC Pedestrian Situated Intent (PSI) data with two innovative\nlabels besides comprehensive computer vision labels. The first novel label is\nthe dynamic intent changes for the pedestrians to cross in front of the\nego-vehicle, achieved from 24 drivers with diverse backgrounds. The second one\nis the text-based explanations of the driver reasoning process when estimating\npedestrian intents and predicting their behaviors during the interaction\nperiod. These innovative labels can enable several computer vision tasks,\nincluding pedestrian intent/behavior prediction, vehicle-pedestrian interaction\nsegmentation, and video-to-language mapping for explainable algorithms. The\nreleased dataset can fundamentally improve the development of pedestrian\nbehavior prediction models and develop socially intelligent autonomous cars to\ninteract with pedestrians efficiently. The dataset has been evaluated with\ndifferent tasks and is released to the public to access.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tina Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_T/0/1/0/all/0/1\">Taotao Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1\">Renran Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaobin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domeyer_J/0/1/0/all/0/1\">Joshua Domeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toyoda_H/0/1/0/all/0/1\">Heishiro Toyoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherony_R/0/1/0/all/0/1\">Rini Sherony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Deep Regression Forests with Consideration of Ranking Fairness. (arXiv:2112.06455v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06455","description":"<p>Deep discriminative models (DDMs), e.g. deep regression forests and deep\ndecision forests, have been extensively studied recently to solve problems such\nas facial age estimation, head pose estimation, etc.. Due to a shortage of\nwell-labeled data that does not have noise and imbalanced distribution\nproblems, learning DDMs is always challenging. Existing methods usually tackle\nthese challenges through learning more discriminative features or re-weighting\nsamples. We argue that learning DDMs gradually, from easy to hard, is more\nreasonable, for two reasons. First, this is more consistent with the cognitive\nprocess of human beings. Second, noisy as well as underrepresented examples can\nbe distinguished by virtue of previously learned knowledge. Thus, we resort to\na gradual learning strategy -- self-paced learning (SPL). Then, a natural\nquestion arises: can SPL lead DDMs to achieve more robust and less biased\nsolutions? To answer this question, this paper proposes a new SPL method: easy\nand underrepresented examples first, for learning DDMs. This tackles the\nfundamental ranking and selection problem in SPL from a new perspective:\nfairness. Our idea is fundamental and can be easily combined with a variety of\nDDMs. Extensive experimental results on three computer vision tasks, i.e.,\nfacial age estimation, head pose estimation, and gaze estimation, show our new\nmethod gains considerable performance improvement in both accuracy and\nfairness. Source code is available at https://github.com/learninginvision/SPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingming Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yali Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking Outside the Box to Ground Language in 3D Scenes. (arXiv:2112.08879v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08879","description":"<p>Most language grounding models learn to select the referred object from a\npool of object proposals provided by a pre-trained detector. This object\nproposal bottleneck is limiting because an utterance may refer to visual\nentities at various levels of granularity, such as the chair, the leg of a\nchair, or the tip of the front leg of a chair, which may be missed by the\ndetector. Recently, MDETR introduced a language grounding model for 2D images\nthat do not have such a box proposal bottleneck; instead of selecting objects\nfrom a proposal pool, it instead decodes the referenced object boxes directly\nfrom image and language features and achieves big leaps in performance. We\npropose a language grounding model for 3D scenes built on MDETR, which we call\nBEAUTY-DETR, from bottom-up and top-down DETR. BEAUTY-DETR attends on an\nadditional object proposal pool computed bottom-up from a pre-trained detector.\nYet it decodes referenced objects without selecting them from the pool. In this\nway, it uses powerful object detectors to help ground language without being\nrestricted by their misses. Second, BEAUTY-DETR augments supervision from\nlanguage grounding annotations by configuring object detection annotations as\nlanguage prompts to be grounded in images. The proposed model sets a new\nstate-of-the-art across popular 3D language grounding benchmarks with\nsignificant performance gains over previous 3D approaches (12.6% on SR3D, 11.6%\non NR3D and 6.3% on ScanRefer). It outperforms a straightforward MDETR for the\n3D point clouds method we implemented by 6.7% on SR3D, 11.8% on NR3D and 5% on\nthe ScanRefer benchmark. When applied to language grounding in 2D images, it\nperforms on par with MDETR. We ablate each of the design choices of the model\nand quantify their contribution to performance. Code and checkpoints are\navailable at https://github.com/nickgkan/beauty_detr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1\">Nikolaos Gkanatsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mediratta_I/0/1/0/all/0/1\">Ishita Mediratta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1\">Katerina Fragkiadaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multispectral image fusion by super pixel statistics. (arXiv:2112.11329v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11329","description":"<p>Multispectral image fusion is a fundamental problem of image processing and\nremote sensing. This problem is addressed by both classic and deep learning\napproaches. This paper is focused on the classic solutions that can work in\nreal-time systems and introduces a new novel approach to this group of works.\nThe proposed method carries out multispectral image fusion based on the content\nof the fused images. Furthermore, it relies on an analysis of the level of\ninformation of segmented superpixels in the fused inputs. Specifically, the\nproposed method addresses the task of visible color RGB to Near-Infrared (NIR)\nfusion. The RGB image captures the color of the scene while the NIR channel\ncaptures details and sees beyond haze and clouds. Since each channel senses\ndifferent information of the scene, their multispectral fusion is challenging\nand interesting. Therefore, the proposed method is designed to produce a fusion\nthat contains the relevant content of each spectra. The experiments of this\nmanuscript show that the proposed method is visually informative with respect\nto other classic fusion methods. Moreover, it can be run fastly on embedded\ndevices without heavy computation requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ofir_N/0/1/0/all/0/1\">Nati Ofir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution. (arXiv:2201.01014v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01014","description":"<p>Infrared small target super-resolution (SR) aims to recover reliable and\ndetailed high-resolution image with highcontrast targets from its\nlow-resolution counterparts. Since the infrared small target lacks color and\nfine structure information, it is significant to exploit the supplementary\ninformation among sequence images to enhance the target. In this paper, we\npropose the first infrared small target SR method named local motion and\ncontrast prior driven deep network (MoCoPnet) to integrate the domain knowledge\nof infrared small target into deep network, which can mitigate the intrinsic\nfeature scarcity of infrared small targets. Specifically, motivated by the\nlocal motion prior in the spatio-temporal dimension, we propose a local\nspatiotemporal attention module to perform implicit frame alignment and\nincorporate the local spatio-temporal information to enhance the local features\n(especially for small targets). Motivated by the local contrast prior in the\nspatial dimension, we propose a central difference residual group to\nincorporate the central difference convolution into the feature extraction\nbackbone, which can achieve center-oriented gradient-aware feature extraction\nto further improve the target contrast. Extensive experiments have demonstrated\nthat our method can recover accurate spatial dependency and improve the target\ncontrast. Comparative results show that MoCoPnet can outperform the\nstate-of-the-art video SR and single image SR methods in terms of both SR\nperformance and target enhancement. Based on the SR results, we further\ninvestigate the influence of SR on infrared small target detection and the\nexperimental results demonstrate that MoCoPnet promotes the detection\nperformance. The code is available at https://github.com/XinyiYing/MoCoPnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ying_X/0/1/0/all/0/1\">Xinyi Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_W/0/1/0/all/0/1\">Weidong Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zaiping Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralMLS: Geometry-Aware Control Point Deformation. (arXiv:2201.01873v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2201.01873","description":"<p>We introduce NeuralMLS, a space-based deformation technique, guided by a set\nof displaced control points. We leverage the power of neural networks to inject\nthe underlying shape geometry into the deformation parameters. The goal of our\ntechnique is to enable a realistic and intuitive shape deformation. Our method\nis built upon moving least-squares (MLS), since it minimizes a weighted sum of\nthe given control point displacements. Traditionally, the influence of each\ncontrol point on every point in space (i.e., the weighting function) is defined\nusing inverse distance heuristics. In this work, we opt to learn the weighting\nfunction, by training a neural network on the control points from a single\ninput shape, and exploit the innate smoothness of neural networks. Our\ngeometry-aware control point deformation is agnostic to the surface\nrepresentation and quality; it can be applied to point clouds or meshes,\nincluding non-manifold and disconnected surface soups. We show that our\ntechnique facilitates intuitive piecewise smooth deformations, which are well\nsuited for manufactured objects. We show the advantages of our approach\ncompared to existing surface and space-based deformation techniques, both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shechter_M/0/1/0/all/0/1\">Meitar Shechter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanocka_R/0/1/0/all/0/1\">Rana Hanocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzer_G/0/1/0/all/0/1\">Gal Metzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1\">Raja Giryes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Mix-normalization Method for Generalizable Multi-source Person Re-identification. (arXiv:2201.09846v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09846","description":"<p>Person re-identification (Re-ID) has achieved great success in the supervised\nscenario. However, it is difficult to directly transfer the supervised model to\narbitrary unseen domains due to the model overfitting to the seen source\ndomains. In this paper, we aim to tackle the generalizable multi-source person\nRe-ID task (i.e., there are multiple available source domains, and the testing\ndomain is unseen during training) from the data augmentation perspective, thus\nwe put forward a novel method, termed MixNorm, which consists of domain-aware\nmix-normalization (DMN) and domain-ware center regularization (DCR). Different\nfrom the conventional data augmentation, the proposed domain-aware\nmix-normalization to enhance the diversity of features during training from the\nnormalization view of the neural network, which can effectively alleviate the\nmodel overfitting to the source domains, so as to boost the generalization\ncapability of the model in the unseen domain. To better learn the\ndomain-invariant model, we further develop the domain-aware center\nregularization to better map the produced diverse features into the same space.\nExtensive experiments on multiple benchmark datasets validate the effectiveness\nof the proposed method and show that the proposed method can outperform the\nstate-of-the-art methods. Besides, further analysis also reveals the\nsuperiority of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xin Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Moving Vehicle Detection from Audio-Visual Cues. (arXiv:2201.12771v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12771","description":"<p>Robust detection of moving vehicles is a critical task for any autonomously\noperating outdoor robot or self-driving vehicle. Most modern approaches for\nsolving this task rely on training image-based detectors using large-scale\nvehicle detection datasets such as nuScenes or the Waymo Open Dataset.\nProviding manual annotations is an expensive and laborious exercise that does\nnot scale well in practice. To tackle this problem, we propose a\nself-supervised approach that leverages audio-visual cues to detect moving\nvehicles in videos. Our approach employs contrastive learning for localizing\nvehicles in images from corresponding pairs of images and recorded audio. In\nextensive experiments carried out with a real-world dataset, we demonstrate\nthat our approach provides accurate detections of moving vehicles and does not\nrequire manual annotations. We furthermore show that our model can be used as a\nteacher to supervise an audio-only detection model. This student model is\ninvariant to illumination changes and thus effectively bridges the domain gap\ninherent to models leveraging exclusively vision as the predominant modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zurn_J/0/1/0/all/0/1\">Jannik Z&#xfc;rn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COIN++: Neural Compression Across Modalities. (arXiv:2201.12904v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12904","description":"<p>Neural compression algorithms are typically based on autoencoders that\nrequire specialized encoder and decoder architectures for different data\nmodalities. In this paper, we propose COIN++, a neural compression framework\nthat seamlessly handles a wide range of data modalities. Our approach is based\non converting data to implicit neural representations, i.e. neural functions\nthat map coordinates (such as pixel locations) to features (such as RGB\nvalues). Then, instead of storing the weights of the implicit neural\nrepresentation directly, we store modulations applied to a meta-learned base\nnetwork as a compressed code for the data. We further quantize and entropy code\nthese modulations, leading to large compression gains while reducing encoding\ntime by two orders of magnitude compared to baselines. We empirically\ndemonstrate the effectiveness of our method by compressing various data\nmodalities, from images and audio to medical and climate data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_E/0/1/0/all/0/1\">Emilien Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loya_H/0/1/0/all/0/1\">Hrushikesh Loya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadeh_M/0/1/0/all/0/1\">Milad Alizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golinski_A/0/1/0/all/0/1\">Adam Goli&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network-level Safety Metrics for Overall Traffic Safety Assessment: A Case Study. (arXiv:2201.13229v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13229","description":"<p>Driving safety analysis has recently experienced unprecedented improvements\nthanks to technological advances in precise positioning sensors, artificial\nintelligence (AI)-based safety features, autonomous driving systems, connected\nvehicles, high-throughput computing, and edge computing servers. Particularly,\ndeep learning (DL) methods empowered volume video processing to extract\nsafety-related features from massive videos captured by roadside units (RSU).\nSafety metrics are commonly used measures to investigate crashes and\nnear-conflict events. However, these metrics provide limited insight into the\noverall network-level traffic management. On the other hand, some safety\nassessment efforts are devoted to processing crash reports and identifying\nspatial and temporal patterns of crashes that correlate with road geometry,\ntraffic volume, and weather conditions. This approach relies merely on crash\nreports and ignores the rich information of traffic videos that can help\nidentify the role of safety violations in crashes. To bridge these two\nperspectives, we define a new set of network-level safety metrics (NSM) to\nassess the overall safety profile of traffic flow by processing imagery taken\nby RSU cameras. Our analysis suggests that NSMs show significant statistical\nassociations with crash rates. This approach is different than simply\ngeneralizing the results of individual crash analyses, since all vehicles\ncontribute to calculating NSMs, not only the ones involved in crash incidents.\nThis perspective considers the traffic flow as a complex dynamic system where\nactions of some nodes can propagate through the network and influence the crash\nrisk for other nodes. We also provide a comprehensive review of surrogate\nsafety metrics (SSM) in the Appendix A.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1\">Abolfazl Razi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_B/0/1/0/all/0/1\">Brendan Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_J/0/1/0/all/0/1\">Jason Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_J/0/1/0/all/0/1\">John Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wishart_J/0/1/0/all/0/1\">Jeffrey Wishart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Head_L/0/1/0/all/0/1\">Larry Head</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baca_A/0/1/0/all/0/1\">Alonso Granados Baca</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On-Sensor Binarized Fully Convolutional Neural Network with A Pixel Processor Array. (arXiv:2202.00836v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00836","description":"<p>This work presents a method to implement fully convolutional neural networks\n(FCNs) on Pixel Processor Array (PPA) sensors, and demonstrates coarse\nsegmentation and object localisation tasks. We design and train binarized FCN\nfor both binary weights and activations using batchnorm, group convolution, and\nlearnable threshold for binarization, producing networks small enough to be\nembedded on the focal plane of the PPA, with limited local memory resources,\nand using parallel elementary add/subtract, shifting, and bit operations only.\nWe demonstrate the first implementation of an FCN on a PPA device, performing\nthree convolution layers entirely in the pixel-level processors. We use this\narchitecture to demonstrate inference generating heat maps for object\nsegmentation and localisation at over 280 FPS using the SCAMP-5 PPA vision\nchip.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_L/0/1/0/all/0/1\">Laurie Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dudek_P/0/1/0/all/0/1\">Piotr Dudek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayol_Cuevas_W/0/1/0/all/0/1\">Walterio Mayol-Cuevas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking and Analyzing Point Cloud Classification under Corruptions. (arXiv:2202.03377v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03377","description":"<p>3D perception, especially point cloud classification, has achieved\nsubstantial progress. However, in real-world deployment, point cloud\ncorruptions are inevitable due to the scene complexity, sensor inaccuracy, and\nprocessing imprecision. In this work, we aim to rigorously benchmark and\nanalyze point cloud classification under corruptions. To conduct a systematic\ninvestigation, we first provide a taxonomy of common 3D corruptions and\nidentify the atomic corruptions. Then, we perform a comprehensive evaluation on\na wide range of representative point cloud models to understand their\nrobustness and generalizability. Our benchmark results show that although point\ncloud classification performance improves over time, the state-of-the-art\nmethods are on the verge of being less robust. Based on the obtained\nobservations, we propose several effective techniques to enhance point cloud\nclassifier robustness. We hope our comprehensive benchmark, in-depth analysis,\nand proposed techniques could spark future research in robust 3D perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Distance Weighted Cross-Entropy Loss for Ulcerative Colitis Severity Estimation. (arXiv:2202.05167v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.05167","description":"<p>In scoring systems used to measure the endoscopic activity of ulcerative\ncolitis, such as Mayo endoscopic score or Ulcerative Colitis Endoscopic Index\nSeverity, levels increase with severity of the disease activity. Such relative\nranking among the scores makes it an ordinal regression problem. On the other\nhand, most studies use categorical cross-entropy loss function to train deep\nlearning models, which is not optimal for the ordinal regression problem. In\nthis study, we propose a novel loss function, class distance weighted\ncross-entropy (CDW-CE), that respects the order of the classes and takes the\ndistance of the classes into account in calculation of the cost. Experimental\nevaluations show that models trained with CDW-CE outperform the models trained\nwith conventional categorical cross-entropy and other commonly used loss\nfunctions which are designed for the ordinal regression problems. In addition,\nthe class activation maps of models trained with CDW-CE loss are more\nclass-discriminative and they are found to be more reasonable by the domain\nexperts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ergenc_I/0/1/0/all/0/1\">Ilkay Ergenc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kani_H/0/1/0/all/0/1\">Haluk Tarik Kani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alahdab_Y/0/1/0/all/0/1\">Yesim Ozen Alahdab</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atug_O/0/1/0/all/0/1\">Ozlen Atug</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opinions Vary? Diagnosis First!. (arXiv:2202.06505v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.06505","description":"<p>With the advancement of deep learning techniques, an increasing number of\nmethods have been proposed for optic disc and cup (OD/OC) segmentation from the\nfundus images. Clinically, OD/OC segmentation is often annotated by multiple\nclinical experts to mitigate the personal bias. However, it is hard to train\nthe automated deep learning models on multiple labels. A common practice to\ntackle the issue is majority vote, e.g., taking the average of multiple labels.\nHowever such a strategy ignores the different expertness of medical experts.\nMotivated by the observation that OD/OC segmentation is often used for the\nglaucoma diagnosis clinically, in this paper, we propose a novel strategy to\nfuse the multi-rater OD/OC segmentation labels via the glaucoma diagnosis\nperformance. Specifically, we assess the expertness of each rater through an\nattentive glaucoma diagnosis network. For each rater, its contribution for the\ndiagnosis will be reflected as an expertness map. To ensure the expertness maps\nare general for different glaucoma diagnosis models, we further propose an\nExpertness Generator (ExpG) to eliminate the high-frequency components in the\noptimization process. Based on the obtained expertness maps, the multi-rater\nlabels can be fused as a single ground-truth which we dubbed as Diagnosis First\nGround-truth (DiagFirstGT). Experimental results show that by using DiagFirstGT\nas ground-truth, OD/OC segmentation networks will predict the masks with\nsuperior glaucoma diagnosis performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_D/0/1/0/all/0/1\">Dalu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaowei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Wenshuo Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shang_F/0/1/0/all/0/1\">Fangxin Shang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yehui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Light Fields for Super-Resolution and Disparity Estimation. (arXiv:2202.10603v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.10603","description":"<p>Light field (LF) cameras record both intensity and directions of light rays,\nand encode 3D scenes into 4D LF images. Recently, many convolutional neural\nnetworks (CNNs) have been proposed for various LF image processing tasks.\nHowever, it is challenging for CNNs to effectively process LF images since the\nspatial and angular information are highly inter-twined with varying\ndisparities. In this paper, we propose a generic mechanism to disentangle these\ncoupled information for LF image processing. Specifically, we first design a\nclass of domain-specific convolutions to disentangle LFs from different\ndimensions, and then leverage these disentangled features by designing\ntask-specific modules. Our disentangling mechanism can well incorporate the LF\nstructure prior and effectively handle 4D LF data. Based on the proposed\nmechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp)\nfor spatial super-resolution, angular super-resolution and disparity\nestimation. Experimental results show that our networks achieve\nstate-of-the-art performance on all these three tasks, which demonstrates the\neffectiveness, efficiency, and generality of our disentangling mechanism.\nProject page: https://yingqianwang.github.io/DistgLF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_G/0/1/0/all/0/1\">Gaochang Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice-Face Homogeneity Tells Deepfake. (arXiv:2203.02195v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02195","description":"<p>Detecting forgery videos is highly desirable due to the abuse of deepfake.\nExisting detection approaches contribute to exploring the specific artifacts in\ndeepfake videos and fit well on certain data. However, the growing technique on\nthese artifacts keeps challenging the robustness of traditional deepfake\ndetectors. As a result, the development of generalizability of these approaches\nhas reached a blockage. To address this issue, given the empirical results that\nthe identities behind voices and faces are often mismatched in deepfake videos,\nand the voices and faces have homogeneity to some extent, in this paper, we\npropose to perform the deepfake detection from an unexplored voice-face\nmatching view. To this end, a voice-face matching method is devised to measure\nthe matching degree of these two. Nevertheless, training on specific deepfake\ndatasets makes the model overfit certain traits of deepfake algorithms. We\ninstead, advocate a method that quickly adapts to untapped forgery, with a\npre-training then fine-tuning paradigm. Specifically, we first pre-train the\nmodel on a generic audio-visual dataset, followed by the fine-tuning on\ndownstream deepfake data. We conduct extensive experiments over three widely\nexploited deepfake datasets - DFDC, FakeAVCeleb, and DeepfakeTIMIT. Our method\nobtains significant performance gains as compared to other state-of-the-art\ncompetitors. It is also worth noting that our method already achieves\ncompetitive results when fine-tuned on limited deepfake data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Harry Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Optical Flow, Depth, and Scene Flow without Real-World Labels. (arXiv:2203.15089v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15089","description":"<p>Self-supervised monocular depth estimation enables robots to learn 3D\nperception from raw video streams. This scalable approach leverages projective\ngeometry and ego-motion to learn via view synthesis, assuming the world is\nmostly static. Dynamic scenes, which are common in autonomous driving and\nhuman-robot interaction, violate this assumption. Therefore, they require\nmodeling dynamic objects explicitly, for instance via estimating pixel-wise 3D\nmotion, i.e. scene flow. However, the simultaneous self-supervised learning of\ndepth and scene flow is ill-posed, as there are infinitely many combinations\nthat result in the same 3D point. In this paper we propose DRAFT, a new method\ncapable of jointly learning depth, optical flow, and scene flow by combining\nsynthetic data with geometric self-supervision. Building upon the RAFT\narchitecture, we learn optical flow as an intermediate task to bootstrap depth\nand scene flow learning via triangulation. Our algorithm also leverages\ntemporal and geometric consistency losses across tasks to improve multi-task\nlearning. Our DRAFT architecture simultaneously establishes a new state of the\nart in all three tasks in the self-supervised monocular setting on the standard\nKITTI benchmark. Project page: https://sites.google.com/tri.global/draft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuan-Hui Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EResFD: Rediscovery of the Effectiveness of Standard Convolution for Lightweight Face Detection. (arXiv:2204.01209v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01209","description":"<p>This paper analyses the design choices of face detection architecture that\nimprove efficiency between computation cost and accuracy. Specifically, we\nre-examine the effectiveness of the standard convolutional block as a\nlightweight backbone architecture on face detection. Unlike the current\ntendency of lightweight architecture design, which heavily utilizes depthwise\nseparable convolution layers, we show that heavily channel-pruned standard\nconvolution layer can achieve better accuracy and inference speed when using a\nsimilar parameter size. This observation is supported by the analyses\nconcerning the characteristics of the target data domain, face. Based on our\nobservation, we propose to employ ResNet with a highly reduced channel, which\nsurprisingly allows high efficiency compared to other mobile-friendly networks\n(e.g., MobileNet-V1,-V2,-V3). From the extensive experiments, we show that the\nproposed backbone can replace that of the state-of-the-art face detector with a\nfaster inference speed. Also, we further propose a new feature aggregation\nmethod maximizing the detection performance. Our proposed detector EResFD\nobtained 80.4% mAP on WIDER FACE Hard subset which only takes 37.7 ms for VGA\nimage inference in on CPU. Code will be available at\nhttps://github.com/clovaai/EResFD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Joonhyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Joonsang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"This is my unicorn, Fluffy\": Personalizing frozen vision-language representations. (arXiv:2204.01694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01694","description":"<p>Large Vision &amp; Language models pretrained on web-scale data provide\nrepresentations that are invaluable for numerous V&amp;L problems. However, it is\nunclear how they can be used for reasoning about user-specific visual concepts\nin unstructured language. This problem arises in multiple domains, from\npersonalized image retrieval to personalized interaction with smart devices. We\nintroduce a new learning setup called Personalized Vision &amp; Language (PerVL)\nwith two new benchmark datasets for retrieving and segmenting user-specific\n\"personalized\" concepts \"in the wild\". In PerVL, one should learn personalized\nconcepts (1) independently of the downstream task (2) allowing a pretrained\nmodel to reason about them with free language, and (3) does not require\npersonalized negative examples. We propose an architecture for solving PerVL\nthat operates by extending the input vocabulary of a pretrained model with new\nword embeddings for the new personalized concepts. The model can then reason\nabout them by simply using them in a sentence. We demonstrate that our approach\nlearns personalized visual concepts from a few examples and can effectively\napply them in image retrieval and semantic segmentation using rich textual\nqueries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meirom_E/0/1/0/all/0/1\">Eli A. Meirom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atzmon_Y/0/1/0/all/0/1\">Yuval Atzmon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transient motion classification through turbid volumes via parallelized single-photon detection and deep contrastive embedding. (arXiv:2204.01733v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.01733","description":"<p>Fast noninvasive probing of spatially varying decorrelating events, such as\ncerebral blood flow beneath the human skull, is an essential task in various\nscientific and clinical settings. One of the primary optical techniques used is\ndiffuse correlation spectroscopy (DCS), whose classical implementation uses a\nsingle or few single-photon detectors, resulting in poor spatial localization\naccuracy and relatively low temporal resolution. Here, we propose a technique\ntermed Classifying Rapid decorrelation Events via Parallelized single photon\ndEtection (CREPE)}, a new form of DCS that can probe and classify different\ndecorrelating movements hidden underneath turbid volume with high sensitivity\nusing parallelized speckle detection from a $32\\times32$ pixel SPAD array. We\nevaluate our setup by classifying different spatiotemporal-decorrelating\npatterns hidden beneath a 5mm tissue-like phantom made with rapidly\ndecorrelating dynamic scattering media. Twelve multi-mode fibers are used to\ncollect scattered light from different positions on the surface of the tissue\nphantom. To validate our setup, we generate perturbed decorrelation patterns by\nboth a digital micromirror device (DMD) modulated at multi-kilo-hertz rates, as\nwell as a vessel phantom containing flowing fluid. Along with a deep\ncontrastive learning algorithm that outperforms classic unsupervised learning\nmethods, we demonstrate our approach can accurately detect and classify\ndifferent transient decorrelation events (happening in 0.1-0.4s) underneath\nturbid scattering media, without any data labeling. This has the potential to\nbe applied to noninvasively monitor deep tissue motion patterns, for example\nidentifying normal or abnormal cerebral blood flow events, at multi-Hertz rates\nwithin a compact and static detection probe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shiqi Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wenhui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonsson_J/0/1/0/all/0/1\">Joakim J&#xf6;nsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_R/0/1/0/all/0/1\">Ruobing Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McKee_P/0/1/0/all/0/1\">Paul McKee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kanghyun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Konda_P/0/1/0/all/0/1\">Pavan Chandra Konda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_K/0/1/0/all/0/1\">Kevin C. Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreiss_L/0/1/0/all/0/1\">Lucas Krei&#xdf;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Haoqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Berrocal_E/0/1/0/all/0/1\">Edouard Berrocal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huettel_S/0/1/0/all/0/1\">Scott Huettel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Horstmeyer_R/0/1/0/all/0/1\">Roarke Horstmeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Permanence Emerges in a Random Walk along Memory. (arXiv:2204.01784v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01784","description":"<p>This paper proposes a self-supervised objective for learning representations\nthat localize objects under occlusion - a property known as object permanence.\nA central question is the choice of learning signal in cases of total\nocclusion. Rather than directly supervising the locations of invisible objects,\nwe propose a self-supervised objective that requires neither human annotation,\nnor assumptions about object dynamics. We show that object permanence can\nemerge by optimizing for temporal coherence of memory: we fit a Markov walk\nalong a space-time graph of memories, where the states in each time step are\nnon-Markovian features from a sequence encoder. This leads to a memory\nrepresentation that stores occluded objects and predicts their motion, to\nbetter localize them. The resulting model outperforms existing approaches on\nseveral datasets of increasing complexity and realism, despite requiring\nminimal supervision, and hence being broadly applicable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1\">Pavel Tokmakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabri_A/0/1/0/all/0/1\">Allan Jabri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Frame Self-Supervised Depth with Transformers. (arXiv:2204.07616v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07616","description":"<p>Multi-frame depth estimation improves over single-frame approaches by also\nleveraging geometric relationships between images via feature matching, in\naddition to learning appearance-based features. In this paper we revisit\nfeature matching for self-supervised monocular depth estimation, and propose a\nnovel transformer architecture for cost volume generation. We use\ndepth-discretized epipolar sampling to select matching candidates, and refine\npredictions through a series of self- and cross-attention layers. These layers\nsharpen the matching probability between pixel features, improving over\nstandard similarity metrics prone to ambiguities and local minima. The refined\ncost volume is decoded into depth estimates, and the whole pipeline is trained\nend-to-end from videos using only a photometric objective. Experiments on the\nKITTI and DDAD datasets show that our DepthFormer architecture establishes a\nnew state of the art in self-supervised monocular depth estimation, and is even\ncompetitive with highly specialized supervised single-frame architectures. We\nalso show that our learned cross-attention network yields representations\ntransferable across datasets, increasing the effectiveness of pre-training\nstrategies. Project page: https://sites.google.com/tri.global/depthformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimMC: Simple Masked Contrastive Learning of Skeleton Representations for Unsupervised Person Re-Identification. (arXiv:2204.09826v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09826","description":"<p>Recent advances in skeleton-based person re-identification (re-ID) obtain\nimpressive performance via either hand-crafted skeleton descriptors or skeleton\nrepresentation learning with deep learning paradigms. However, they typically\nrequire skeletal pre-modeling and label information for training, which leads\nto limited applicability of these methods. In this paper, we focus on\nunsupervised skeleton-based person re-ID, and present a generic Simple Masked\nContrastive learning (SimMC) framework to learn effective representations from\nunlabeled 3D skeletons for person re-ID. Specifically, to fully exploit\nskeleton features within each skeleton sequence, we first devise a masked\nprototype contrastive learning (MPC) scheme to cluster the most typical\nskeleton features (skeleton prototypes) from different subsequences randomly\nmasked from raw sequences, and contrast the inherent similarity between\nskeleton features and different prototypes to learn discriminative skeleton\nrepresentations without using any label. Then, considering that different\nsubsequences within the same sequence usually enjoy strong correlations due to\nthe nature of motion continuity, we propose the masked intra-sequence\ncontrastive learning (MIC) to capture intra-sequence pattern consistency\nbetween subsequences, so as to encourage learning more effective skeleton\nrepresentations for person re-ID. Extensive experiments validate that the\nproposed SimMC outperforms most state-of-the-art skeleton-based methods. We\nfurther show its scalability and efficiency in enhancing the performance of\nexisting models. Our codes are available at https://github.com/Kali-Hac/SimMC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1\">Haocong Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention Emerges from Recurrent Sparse Reconstruction. (arXiv:2204.10962v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10962","description":"<p>Visual attention helps achieve robust perception under noise, corruption, and\ndistribution shifts in human vision, which are areas where modern neural\nnetworks still fall short. We present VARS, Visual Attention from Recurrent\nSparse reconstruction, a new attention formulation built on two prominent\nfeatures of the human visual attention mechanism: recurrency and sparsity.\nRelated features are grouped together via recurrent connections between\nneurons, with salient objects emerging via sparse regularization. VARS adopts\nan attractor network with recurrent connections that converges toward a stable\npattern over time. Network layers are represented as ordinary differential\nequations (ODEs), formulating attention as a recurrent attractor network that\nequivalently optimizes the sparse reconstruction of input using a dictionary of\n\"templates\" encoding underlying patterns of data. We show that self-attention\nis a special case of VARS with a single-step optimization and no sparsity\nconstraint. VARS can be readily used as a replacement for self-attention in\npopular vision transformers, consistently improving their robustness across\nvarious benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Baifeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Neel Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelViT: Concept-guided Vision Transformer for Visual Relational Reasoning. (arXiv:2204.11167v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11167","description":"<p>Reasoning about visual relationships is central to how humans interpret the\nvisual world. This task remains challenging for current deep learning\nalgorithms since it requires addressing three key technical problems jointly:\n1) identifying object entities and their properties, 2) inferring semantic\nrelations between pairs of entities, and 3) generalizing to novel\nobject-relation combinations, i.e., systematic generalization. In this work, we\nuse vision transformers (ViTs) as our base model for visual reasoning and make\nbetter use of concepts defined as object entities and their relations to\nimprove the reasoning ability of ViTs. Specifically, we introduce a novel\nconcept-feature dictionary to allow flexible image feature retrieval at\ntraining time with concept keys. This dictionary enables two new concept-guided\nauxiliary tasks: 1) a global task for promoting relational reasoning, and 2) a\nlocal task for facilitating semantic object-centric correspondence learning. To\nexamine the systematic generalization of visual reasoning models, we introduce\nsystematic splits for the standard HICO and GQA benchmarks. We show the\nresulting model, Concept-guided Vision Transformer (or RelViT for short)\nsignificantly outperforms prior approaches on HICO and GQA by 16% and 13% in\nthe original split, and by 43% and 18% in the systematic split. Our ablation\nanalyses also reveal our model's compatibility with multiple ViT variants and\nrobustness to hyper-parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1\">Weili Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huaizu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework. (arXiv:2205.03860v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03860","description":"<p>Vision-language pre-training (VLP) on large-scale datasets has shown premier\nperformance on various downstream tasks. A complete and fair benchmark (i.e.,\nincluding large-scale pre-training datasets and diverse downstream tasks) is\nessential for VLP. While there are plenty of benchmarks with English corpus,\nbuilding a rich benchmark for VLP with other languages, such as Chinese,\nremains a critical problem. To this end, we build a large-scale Chinese\ncross-modal benchmark called Zero for the research community to fairly compare\nVLP models. We release two pre-training datasets and five fine-tuning datasets\nfor downstream tasks. Alongside, we propose a novel pre-training framework of\npre-Ranking + Ranking for cross-modal learning. Specifically, we apply global\ncontrastive pre-ranking to learn the individual representations of images and\ntexts, respectively. We then fuse the representations in a fine-grained ranking\nmanner via an image-text cross encoder and a text-image cross encoder. To\nfurther enhance the capability of the model, we propose a two-way distillation\nstrategy consisting of target-guided Distillation and feature-guided\nDistillation. For brevity, we name our model R2D2. We achieve state-of-the-art\nperformance on four public cross-modal datasets and the proposed five\ndownstream datasets. When conducting zero-shot tasks on Flickr30k-CN, COCO-CN,\nand MUGE, R2D2 pre-trained on a 250 million dataset achieves significant\nimprovements of 4.7%, 5.4%, and 6.3% in mean recall compared to the\nstate-of-the-art. The datasets, models, and codes are available at\nhttps://github.com/yuxie11/R2D2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Chunyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Heng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jincheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fanjing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morimitsu_H/0/1/0/all/0/1\">Henrique Morimitsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_D/0/1/0/all/0/1\">Dawei Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomy-aware Self-supervised Learning for Anomaly Detection in Chest Radiographs. (arXiv:2205.04282v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04282","description":"<p>Large numbers of labeled medical images are essential for the accurate\ndetection of anomalies, but manual annotation is labor-intensive and\ntime-consuming. Self-supervised learning (SSL) is a training method to learn\ndata-specific features without manual annotation. Several SSL-based models have\nbeen employed in medical image anomaly detection. These SSL methods effectively\nlearn representations in several field-specific images, such as natural and\nindustrial product images. However, owing to the requirement of medical\nexpertise, typical SSL-based models are inefficient in medical image anomaly\ndetection. We present an SSL-based model that enables anatomical\nstructure-based unsupervised anomaly detection (UAD). The model employs the\nanatomy-aware pasting (AnatPaste) augmentation tool. AnatPaste employs a\nthreshold-based lung segmentation pretext task to create anomalies in normal\nchest radiographs, which are used for model pretraining. These anomalies are\nsimilar to real anomalies and help the model recognize them. We evaluate our\nmodel on three opensource chest radiograph datasets. Our model exhibit area\nunder curves (AUC) of 92.1%, 78.7%, and 81.9%, which are the highest among\nexisting UAD models. This is the first SSL model to employ anatomical\ninformation as a pretext task. AnatPaste can be applied in various deep\nlearning models and downstream tasks. It can be employed for other modalities\nby fixing appropriate segmentation. Our code is publicly available at:\nhttps://github.com/jun-sato/AnatPaste.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sato_J/0/1/0/all/0/1\">Junya Sato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_Y/0/1/0/all/0/1\">Yuki Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wataya_T/0/1/0/all/0/1\">Tomohiro Wataya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishigaki_D/0/1/0/all/0/1\">Daiki Nishigaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kita_K/0/1/0/all/0/1\">Kosuke Kita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagata_K/0/1/0/all/0/1\">Kazuki Yamagata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomiyama_N/0/1/0/all/0/1\">Noriyuki Tomiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kido_S/0/1/0/all/0/1\">Shoji Kido</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Deep Learning Methods in Medical Imaging Diagnosis: A Survey. (arXiv:2205.04766v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.04766","description":"<p>The remarkable success of deep learning has prompted interest in its\napplication to medical imaging diagnosis. Even though state-of-the-art deep\nlearning models have achieved human-level accuracy on the classification of\ndifferent types of medical data, these models are hardly adopted in clinical\nworkflows, mainly due to their lack of interpretability. The black-box-ness of\ndeep learning models has raised the need for devising strategies to explain the\ndecision process of these models, leading to the creation of the topic of\neXplainable Artificial Intelligence (XAI). In this context, we provide a\nthorough survey of XAI applied to medical imaging diagnosis, including visual,\ntextual, example-based and concept-based explanation methods. Moreover, this\nwork reviews the existing medical imaging datasets and the existing metrics for\nevaluating the quality of the explanations. In addition, we include a\nperformance comparison among a set of report generation-based methods. Finally,\nthe major challenges in applying XAI to medical imaging and the future research\ndirections on the topic are also discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Patricio_C/0/1/0/all/0/1\">Cristiano Patr&#xed;cio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neves_J/0/1/0/all/0/1\">Jo&#xe3;o C. Neves</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1\">Lu&#xed;s F. Teixeira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSO-Convolutional Neural Networks with Heterogeneous Learning Rate. (arXiv:2205.10456v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10456","description":"<p>Convolutional Neural Networks (ConvNets or CNNs) have been candidly deployed\nin the scope of computer vision and related fields. Nevertheless, the dynamics\nof training of these neural networks lie still elusive: it is hard and\ncomputationally expensive to train them. A myriad of architectures and training\nstrategies have been proposed to overcome this challenge and address several\nproblems in image processing such as speech, image and action recognition as\nwell as object detection. In this article, we propose a novel Particle Swarm\nOptimization (PSO) based training for ConvNets. In such framework, the vector\nof weights of each ConvNet is typically cast as the position of a particle in\nphase space whereby PSO collaborative dynamics intertwines with Stochastic\nGradient Descent (SGD) in order to boost training performance and\ngeneralization. Our approach goes as follows: i) [regular phase] each ConvNet\nis trained independently via SGD; ii) [collaborative phase] ConvNets share\namong themselves their current vector of weights (or particle-position) along\nwith their gradient estimates of the Loss function. Distinct step sizes are\ncoined by distinct ConvNets. By properly blending ConvNets with large (possibly\nrandom) step-sizes along with more conservative ones, we propose an algorithm\nwith competitive performance with respect to other PSO-based approaches on\nCifar-10 (accuracy of 98.31%). These accuracy levels are obtained by resorting\nto only four ConvNets -- such results are expected to scale with the number of\ncollaborative ConvNets accordingly. We make our source codes available for\ndownload https://github.com/leonlha/PSO-ConvNet-Dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phong_N/0/1/0/all/0/1\">Nguyen Huu Phong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1\">Augusto Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1\">Bernardete Ribeiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Orchestra: Unsupervised Federated Learning via Globally Consistent Clustering. (arXiv:2205.11506v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.11506","description":"<p>Federated learning is generally used in tasks where labels are readily\navailable (e.g., next word prediction). Relaxing this constraint requires\ndesign of unsupervised learning techniques that can support desirable\nproperties for federated training: robustness to statistical/systems\nheterogeneity, scalability with number of participants, and communication\nefficiency. Prior work on this topic has focused on directly extending\ncentralized self-supervised learning techniques, which are not designed to have\nthe properties listed above. To address this situation, we propose Orchestra, a\nnovel unsupervised federated learning technique that exploits the federation's\nhierarchy to orchestrate a distributed clustering task and enforce a globally\nconsistent partitioning of clients' data into discriminable clusters. We show\nthe algorithmic pipeline in Orchestra guarantees good generalization\nperformance under a linear probe, allowing it to outperform alternative\ntechniques in a broad range of conditions, including variation in\nheterogeneity, number of clients, participation ratio, and local epochs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chi Ian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawsar_F/0/1/0/all/0/1\">Fahim Kawsar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Akhil Mathur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Model Generalization for Monocular 3D Object Detection. (arXiv:2205.11664v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.11664","description":"<p>Monocular 3D object detection (Mono3D) has achieved tremendous improvements\nwith emerging large-scale autonomous driving datasets and the rapid development\nof deep learning techniques. However, caused by severe domain gaps (e.g., the\nfield of view (FOV), pixel size, and object size among datasets), Mono3D\ndetectors have difficulty in generalization, leading to drastic performance\ndegradation on unseen domains. To solve these issues, we combine the\nposition-invariant transform and multi-scale training with the pixel-size depth\nstrategy to construct an effective unified camera-generalized paradigm (CGP).\nIt fully considers discrepancies in the FOV and pixel size of images captured\nby different cameras. Moreover, we further investigate the obstacle in\nquantitative metrics when cross-dataset inference through an exhaustive\nsystematic study. We discern that the size bias of prediction leads to a\ncolossal failure. Hence, we propose the 2D-3D geometry-consistent object\nscaling strategy (GCOS) to bridge the gap via an instance-level augment. Our\nmethod called DGMono3D achieves remarkable performance on all evaluated\ndatasets and surpasses the SoTA unsupervised domain adaptation scheme even\nwithout utilizing data on the target domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Map-based Features for Efficient Attention-based Vehicle Motion Prediction. (arXiv:2205.13071v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2205.13071","description":"<p>Motion prediction (MP) of multiple agents is a crucial task in arbitrarily\ncomplex environments, from social robots to self-driving cars. Current\napproaches tackle this problem using end-to-end networks, where the input data\nis usually a rendered top-view of the scene and the past trajectories of all\nthe agents; leveraging this information is a must to obtain optimal\nperformance. In that sense, a reliable Autonomous Driving (AD) system must\nproduce reasonable predictions on time, however, despite many of these\napproaches use simple ConvNets and LSTMs, models might not be efficient enough\nfor real-time applications when using both sources of information (map and\ntrajectory history). Moreover, the performance of these models highly depends\non the amount of training data, which can be expensive (particularly the\nannotated HD maps). In this work, we explore how to achieve competitive\nperformance on the Argoverse 1.0 Benchmark using efficient attention-based\nmodels, which take as input the past trajectories and map-based features from\nminimal map information to ensure efficient and reliable MP. These features\nrepresent interpretable information as the driveable area and plausible goal\npoints, in opposition to black-box CNN-based methods for map processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Huelamo_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Hu&#xe9;lamo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_M/0/1/0/all/0/1\">Miguel Ortiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sequential Contexts using Transformer for 3D Hand Pose Estimation. (arXiv:2206.00171v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00171","description":"<p>3D hand pose estimation (HPE) is the process of locating the joints of the\nhand in 3D from any visual input. HPE has recently received an increased amount\nof attention due to its key role in a variety of human-computer interaction\napplications. Recent HPE methods have demonstrated the advantages of employing\nvideos or multi-view images, allowing for more robust HPE systems. Accordingly,\nin this study, we propose a new method to perform Sequential learning with\nTransformer for Hand Pose (SeTHPose) estimation. Our SeTHPose pipeline begins\nby extracting visual embeddings from individual hand images. We then use a\ntransformer encoder to learn the sequential context along time or viewing\nangles and generate accurate 2D hand joint locations. Then, a graph\nconvolutional neural network with a U-Net configuration is used to convert the\n2D hand joint locations to 3D poses. Our experiments show that SeTHPose\nperforms well on both hand sequence varieties, temporal and angular. Also,\nSeTHPose outperforms other methods in the field to achieve new state-of-the-art\nresults on two public available sequential datasets, STB and MuViHand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khaleghi_L/0/1/0/all/0/1\">Leyla Khaleghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marshall_J/0/1/0/all/0/1\">Joshua Marshall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography. (arXiv:2206.02225v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.02225","description":"<p>Displacement estimation is a critical step of virtually all Ultrasound\nElastography (USE) techniques. Two main features make this task unique compared\nto the general optical flow problem: the high-frequency nature of ultrasound\nradio-frequency (RF) data and the governing laws of physics on the displacement\nfield. Recently, the architecture of the optical flow networks has been\nmodified to be able to use RF data. Also, semi-supervised and unsupervised\ntechniques have been employed for USE by considering prior knowledge of\ndisplacement continuity in the form of the first- and second-derivative\nregularizers. Despite these attempts, no work has considered the tissue\ncompression pattern, and displacements in axial and lateral directions have\nbeen assumed to be independent. However, tissue motion pattern is governed by\nlaws of physics in USE, rendering the axial and the lateral displacements\nhighly correlated. In this paper, we propose Physically Inspired ConsTraint for\nUnsupervised Regularized Elastography (PICTURE), where we impose constraints on\nthe Poisson's ratio to improve lateral displacement estimates. Experiments on\nphantom and in vivo data show that PICTURE substantially improves the quality\nof the lateral displacement estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tehrani_A/0/1/0/all/0/1\">Ali K. Z. Tehrani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JigsawHSI: a network for Hyperspectral Image classification. (arXiv:2206.02327v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02327","description":"<p>This article describes Jigsaw, a convolutional neural network (CNN) used in\ngeosciences and based on Inception but tailored for geoscientific analyses.\nIntroduces JigsawHSI (based on Jigsaw) and uses it on the land-use land-cover\n(LULC) classification problem with the Indian Pines, Pavia University and\nSalinas hyperspectral image data sets. The network is compared against\nHybridSN, a spectral-spatial 3D-CNN followed by 2D-CNN that achieves\nstate-of-the-art results on the datasets. This short article proves that\nJigsawHSI is able to meet or exceed HybridSN's performance in all three cases.\nAdditionally, the use of jigsaw in geosciences is highlighted, while the code\nand toolkit are made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moraga_J/0/1/0/all/0/1\">Jaime Moraga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duzgun_H/0/1/0/all/0/1\">H. Sebnem Duzgun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03544","description":"<p>Reconstructing natural videos from fMRI brain recordings is very challenging,\nfor two main reasons: (i) As fMRI data acquisition is difficult, we only have a\nlimited amount of supervised samples, which is not enough to cover the huge\nspace of natural videos; and (ii) The temporal resolution of fMRI recordings is\nmuch lower than the frame rate of natural videos. In this paper, we propose a\nself-supervised approach for natural-movie reconstruction. By employing\ncycle-consistency over Encoding-Decoding natural videos, we can: (i) exploit\nthe full framerate of the training videos, and not be limited only to clips\nthat correspond to fMRI recordings; (ii) exploit massive amounts of external\nnatural videos which the subjects never saw inside the fMRI machine. These\nenable increasing the applicable training data by several orders of magnitude,\nintroducing natural video priors to the decoding network, as well as temporal\ncoherence. Our approach significantly outperforms competing methods, since\nthose train only on the limited supervised data. We further introduce a new and\nsimple temporal prior of natural videos, which - when folded into our fMRI\ndecoder further - allows us to reconstruct videos at a higher frame-rate (HFR)\nof up to x8 of the original fMRI sample rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kupershmidt_G/0/1/0/all/0/1\">Ganit Kupershmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beliy_R/0/1/0/all/0/1\">Roman Beliy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1\">Guy Gaziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior. (arXiv:2206.03858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03858","description":"<p>Inverse rendering is an ill-posed problem. Previous work has sought to\nresolve this by focussing on priors for object or scene shape or appearance. In\nthis work, we instead focus on a prior for natural illuminations. Current\nmethods rely on spherical harmonic lighting or other generic representations\nand, at best, a simplistic prior on the parameters. We propose a conditional\nneural field representation based on a variational auto-decoder with a SIREN\nnetwork and, extending Vector Neurons, build equivariance directly into the\nnetwork. Using this we develop a rotation-equivariant, high dynamic range (HDR)\nneural illumination model that is compact and able to express complex,\nhigh-frequency features of natural environment maps. Training our model on a\ncurated dataset of 1.6K HDR environment maps of natural scenes, we compare it\nagainst traditional representations, demonstrate its applicability for an\ninverse rendering task and show environment map completion from partial\nobservations. A PyTorch implementation, our dataset and trained models can be\nfound at jadgardner.github.io/RENI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">James A. D. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_W/0/1/0/all/0/1\">William A. P. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fusion Mixture-of-Experts are Domain Generalizable Learners. (arXiv:2206.04046v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04046","description":"<p>Domain generalization (DG) aims at learning generalizable models under\ndistribution shifts to avoid redundantly overfitting massive training data.\nPrevious works with complex loss design and gradient constraint have not yet\nled to empirical success on large-scale benchmarks. In this work, we reveal the\nmixture-of-experts (MoE) model's generalizability on DG by leveraging to\ndistributively handle multiple aspects of the predictive features across\ndomains. To this end, we propose Sparse Fusion Mixture-of-Experts (SF-MoE),\nwhich incorporates sparsity and fusion mechanisms into the MoE framework to\nkeep the model both sparse and predictive. SF-MoE has two dedicated modules: 1)\nsparse block and 2) fusion block, which disentangle and aggregate the diverse\nlearned signals of an object, respectively. Extensive experiments demonstrate\nthat SF-MoE is a domain-generalizable learner on large-scale benchmarks. It\noutperforms state-of-the-art counterparts by more than 2% across 5 large-scale\nDG datasets (e.g., DomainNet), with the same or even lower computational costs.\nWe further reveal the internal mechanism of SF-MoE from distributed\nrepresentation perspective (e.g., visual attributes). We hope this framework\ncould facilitate future research to push generalizable object recognition to\nthe real world. Code and models are released at\nhttps://github.com/Luodian/SF-MoE-DG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yezhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Cues Lead to a Strong Multi-Object Tracker. (arXiv:2206.04656v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04656","description":"<p>For a long time, the most common paradigm in Multi-Object Tracking was\ntracking-by-detection (TbD), where objects are first detected and then\nassociated over video frames. For association, most models resource to motion\nand appearance cues. While still relying on these cues, recent approaches based\non, e.g., attention have shown an ever-increasing need for training data and\noverall complex frameworks. We claim that 1) strong cues can be obtained from\nlittle amounts of training data if some key design choices are applied, 2)\ngiven these strong cues, standard Hungarian matching-based association is\nenough to obtain impressive results. Our main insight is to identify key\ncomponents that allow a standard reidentification network to excel at\nappearance-based tracking. We extensively analyze its failure cases and show\nthat a combination of our appearance features with a simple motion model leads\nto strong tracking results. Our model achieves state-of-the-art performance on\nMOT17 and MOT20 datasets outperforming previous state-of-the-art trackers by up\nto 5.4pp in IDF1 and 4.4pp in HOTA. We will release the code and models after\nthe paper's acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seidenschwarz_J/0/1/0/all/0/1\">Jenny Seidenschwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braso_G/0/1/0/all/0/1\">Guillem Bras&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation. (arXiv:2206.04682v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.04682","description":"<p>Accurately segmenting temporal frames of cine magnetic resonance imaging\n(MRI) is a crucial step in various real-time MRI guided cardiac interventions.\nTo achieve fast and accurate visual assistance, there are strict requirements\non the maximum latency and minimum throughput of the segmentation framework.\nState-of-the-art neural networks on this task are mostly hand-crafted to\nsatisfy these constraints while achieving high accuracy. On the other hand,\nwhile existing literature have demonstrated the power of neural architecture\nsearch (NAS) in automatically identifying the best neural architectures for\nvarious medical applications, they are mostly guided by accuracy, sometimes\nwith computation complexity, and the importance of real-time constraints are\noverlooked. A major challenge is that such constraints are non-differentiable\nand are thus not compatible with the widely used differentiable NAS frameworks.\nIn this paper, we present a strategy that directly handles real-time\nconstraints in a differentiable NAS framework named RT-DNAS. Experiments on\nextended 2017 MICCAI ACDC dataset show that compared with state-of-the-art\nmanually and automatically designed architectures, RT-DNAS is able to identify\nones with better accuracy while satisfying the real-time constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Q/0/1/0/all/0/1\">Qing Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_S/0/1/0/all/0/1\">Shunjie Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_C/0/1/0/all/0/1\">Cong Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lei Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuo_C/0/1/0/all/0/1\">Cheng Zhuo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-MIA: COVID-19 Detection & Severity Analysis through Medical Imaging. (arXiv:2206.04732v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.04732","description":"<p>This paper presents the baseline approach for the organized 2nd Covid-19\nCompetition, occurring in the framework of the AIMIA Workshop in the European\nConference on Computer Vision (ECCV 2022). It presents the COV19-CT-DB database\nwhich is annotated for COVID-19 detction, consisting of about 7,700 3-D CT\nscans. Part of the database consisting of Covid-19 cases is further annotated\nin terms of four Covid-19 severity conditions. We have split the database and\nthe latter part of it in training, validation and test datasets. The former two\ndatasets are used for training and validation of machine learning models, while\nthe latter will be used for evaluation of the developed models. The baseline\napproach consists of a deep learning approach, based on a CNN-RNN network and\nreport its performance on the COVID19-CT-DB database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arsenos_A/0/1/0/all/0/1\">Anastasios Arsenos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kollias_S/0/1/0/all/0/1\">Stefanos Kollias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/"}}]}]}