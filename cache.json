{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Retrieval-Augmented Transformer for Image Captioning. (arXiv:2207.13162v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13162","description":"<p>Image captioning models aim at connecting Vision and Language by providing\nnatural language descriptions of input images. In the past few years, the task\nhas been tackled by learning parametric models and proposing visual feature\nextraction advancements or by modeling better multi-modal connections. In this\npaper, we investigate the development of an image captioning approach with a\nkNN memory, with which knowledge can be retrieved from an external corpus to\naid the generation process. Our architecture combines a knowledge retriever\nbased on visual similarities, a differentiable encoder, and a kNN-augmented\nattention layer to predict tokens based on the past context and on text\nretrieved from the external memory. Experimental results, conducted on the COCO\ndataset, demonstrate that employing an explicit external memory can aid the\ngeneration process and increase caption quality. Our work opens up new avenues\nfor improving image captioning models at larger scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1\">Sara Sarto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Intent Classification and Slot-Filling Datasets for Task-Oriented Dialog. (arXiv:2207.13211v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13211","description":"<p>Interest in dialog systems has grown substantially in the past decade. By\nextension, so too has interest in developing and improving intent\nclassification and slot-filling models, which are two components that are\ncommonly used in task-oriented dialog systems. Moreover, good evaluation\nbenchmarks are important in helping to compare and analyze systems that\nincorporate such models. Unfortunately, much of the literature in the field is\nlimited to analysis of relatively few benchmark datasets. In an effort to\npromote more robust analyses of task-oriented dialog systems, we have conducted\na survey of publicly available datasets for the tasks of intent classification\nand slot-filling. We catalog the important characteristics of each dataset, and\noffer discussion on the applicability, strengths, and weaknesses of each. Our\ngoal is that this survey aids in increasing the accessibility of these\ndatasets, which we hope will enable their use in future evaluations of intent\nclassification and slot-filling models for task-oriented dialog systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1\">Stefan Larson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leach_K/0/1/0/all/0/1\">Kevin Leach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base. (arXiv:2207.13242v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13242","description":"<p>Knowledge-based visual question answering (KVQA) task aims to answer\nquestions that require additional external knowledge as well as an\nunderstanding of images and questions. Recent studies on KVQA inject an\nexternal knowledge in a multi-modal form, and as more knowledge is used,\nirrelevant information may be added and can confuse the question answering. In\norder to properly use the knowledge, this study proposes the following: 1) we\nintroduce a novel semantic inconsistency measure computed from caption\nuncertainty and semantic similarity; 2) we suggest a new external knowledge\nassimilation method based on the semantic inconsistency measure and apply it to\nintegrate explicit knowledge and implicit knowledge for KVQA; 3) the proposed\nmethod is evaluated with the OK-VQA dataset and achieves the state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chae_J/0/1/0/all/0/1\">Jinyeong Chae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihie Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13243","description":"<p>The last decade of machine learning has seen drastic increases in scale and\ncapabilities, and deep neural networks (DNNs) are increasingly being deployed\nacross a wide range of domains. However, the inner workings of DNNs are\ngenerally difficult to understand, raising concerns about the safety of using\nthese systems without a rigorous understanding of how they function. In this\nsurvey, we review literature on techniques for interpreting the inner\ncomponents of DNNs, which we call \"inner\" interpretability methods.\nSpecifically, we review methods for interpreting weights, neurons, subnetworks,\nand latent representations with a focus on how these techniques relate to the\ngoal of designing safer, more trustworthy AI systems. We also highlight\nconnections between interpretability and work in modularity, adversarial\nrobustness, continual learning, network compression, and studying the human\nvisual system. Finally, we discuss key challenges and argue for future work in\ninterpretability for AI safety that focuses on diagnostics, benchmarking, and\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raukur_T/0/1/0/all/0/1\">Tilman R&#xe4;ukur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Anson Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Information and Commonsense Based Prompt for Emotion Recognition in Conversation. (arXiv:2207.13254v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13254","description":"<p>Emotion recognition in conversation (ERC) aims to detect the emotion for each\nutterance in a given conversation. The newly proposed ERC models have leveraged\npre-trained language models (PLMs) with the paradigm of pre-training and\nfine-tuning to obtain good performance. However, these models seldom exploit\nPLMs' advantages thoroughly, and perform poorly for the conversations lacking\nexplicit emotional expressions. In order to fully leverage the latent knowledge\nrelated to the emotional expressions in utterances, we propose a novel ERC\nmodel CISPER with the new paradigm of prompt and language model (LM) tuning.\nSpecifically, CISPER is equipped with the prompt blending the contextual\ninformation and commonsense related to the interlocutor's utterances, to\nachieve ERC more effectively. Our extensive experiments demonstrate CISPER's\nsuperior performance over the state-of-the-art ERC models, and the\neffectiveness of leveraging these two kinds of significant prompt information\nfor performance gains. To reproduce our experimental results conveniently,\nCISPER's sourcecode and the datasets have been shared at\nhttps://github.com/DeqingYang/CISPER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jingjie Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Deqing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Siyu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1\">Caiyan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yanghua Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RealTime QA: What's the Answer Right Now?. (arXiv:2207.13332v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13332","description":"<p>We introduce RealTime QA, a dynamic question answering (QA) platform that\nannounces questions and evaluates systems on a regular basis (weekly in this\nversion). RealTime QA inquires about the current world, and QA systems need to\nanswer questions about novel events or information. It therefore challenges\nstatic, conventional assumptions in open domain QA datasets and pursues,\ninstantaneous applications. We build strong baseline models upon large\npretrained language models, including GPT-3 and T5. Our benchmark is an ongoing\neffort, and this preliminary report presents real-time evaluation results over\nthe past month. Our experimental results show that GPT-3 can often properly\nupdate its generation results, based on newly-retrieved documents, highlighting\nthe importance of up-to-date information retrieval. Nonetheless, we find that\nGPT-3 tends to return outdated answers when retrieved documents do not provide\nsufficient information to find an answer. This suggests an important avenue for\nfuture research: can an open domain QA system identify such unanswerable cases\nand communicate with the user or even the retrieval module to modify the\nretrieval results? We hope that RealTime QA will spur progress in instantaneous\napplications of question answering and beyond.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasai_J/0/1/0/all/0/1\">Jungo Kasai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1\">Keisuke Sakaguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_Y/0/1/0/all/0/1\">Yoichi Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1\">Akari Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Neighbors Enough? Multi-Head Neural n-gram can be Alternative to Self-attention. (arXiv:2207.13354v1 [cs.CL])","link":"http://arxiv.org/abs/2207.13354","description":"<p>Impressive performance of Transformer has been attributed to self-attention,\nwhere dependencies between entire input in a sequence are considered at every\nposition. In this work, we reform the neural $n$-gram model, which focuses on\nonly several surrounding representations of each position, with the multi-head\nmechanism as in Vaswani et al.(2017). Through experiments on\nsequence-to-sequence tasks, we show that replacing self-attention in\nTransformer with multi-head neural $n$-gram can achieve comparable or better\nperformance than Transformer. From various analyses on our proposed method, we\nfind that multi-head neural $n$-gram is complementary to self-attention, and\ntheir combinations can further improve performance of vanilla Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loem_M/0/1/0/all/0/1\">Mengsay Loem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaneko_M/0/1/0/all/0/1\">Masahiro Kaneko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okazaki_N/0/1/0/all/0/1\">Naoaki Okazaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modelling Social Context for Fake News Detection: A Graph Neural Network Based Approach. (arXiv:2207.13500v1 [cs.SI])","link":"http://arxiv.org/abs/2207.13500","description":"<p>Detection of fake news is crucial to ensure the authenticity of information\nand maintain the news ecosystems reliability. Recently, there has been an\nincrease in fake news content due to the recent proliferation of social media\nand fake content generation techniques such as Deep Fake. The majority of the\nexisting modalities of fake news detection focus on content based approaches.\nHowever, most of these techniques fail to deal with ultra realistic synthesized\nmedia produced by generative models. Our recent studies find that the\npropagation characteristics of authentic and fake news are distinguishable,\nirrespective of their modalities. In this regard, we have investigated the\nauxiliary information based on social context to detect fake news. This paper\nhas analyzed the social context of fake news detection with a hybrid graph\nneural network based approach. This hybrid model is based on integrating a\ngraph neural network on the propagation of news and bi directional encoder\nrepresentations from the transformers model on news content to learn the text\nfeatures. Thus this proposed approach learns the content as well as the context\nfeatures and hence able to outperform the baseline models with an f1 score of\n0.91 on PolitiFact and 0.93 on the Gossipcop dataset, respectively\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saikia_P/0/1/0/all/0/1\">Pallabi Saikia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gundale_K/0/1/0/all/0/1\">Kshitij Gundale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ankit Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jadeja_D/0/1/0/all/0/1\">Dev Jadeja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Harvi Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_M/0/1/0/all/0/1\">Mohendra Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIMIB at TREC 2021 Clinical Trials Track. (arXiv:2207.13514v1 [cs.IR])","link":"http://arxiv.org/abs/2207.13514","description":"<p>This contribution summarizes the participation of the UNIMIB team to the TREC\n2021 Clinical Trials Track. We have investigated the effect of different query\nrepresentations combined with several retrieval models on the retrieval\nperformance. First, we have implemented a neural re-ranking approach to study\nthe effectiveness of dense text representations. Additionally, we have\ninvestigated the effectiveness of a novel decision-theoretic model for\nrelevance estimation. Finally, both of the above relevance models have been\ncompared with standard retrieval approaches. In particular, we combined a\nkeyword extraction method with a standard retrieval process based on the BM25\nmodel and a decision-theoretic relevance model that exploits the\ncharacteristics of this particular search task. The obtained results show that\nthe proposed keyword extraction method improves 84% of the queries over the\nTREC's median NDCG@10 measure when combined with either traditional or\ndecision-theoretic relevance models. Moreover, regarding RPEC@10, the employed\ndecision-theoretic model improves 85% of the queries over the reported TREC's\nmedian value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peikos_G/0/1/0/all/0/1\">Georgios Peikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espitia_O/0/1/0/all/0/1\">Oscar Espitia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasi_G/0/1/0/all/0/1\">Gabriella Pasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Variational AutoEncoder for Transformers with Nonparametric Variational Information Bottleneck. (arXiv:2207.13529v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13529","description":"<p>We propose a VAE for Transformers by developing a variational information\nbottleneck regulariser for Transformer embeddings. We formalise the embedding\nspace of Transformer encoders as mixture probability distributions, and use\nBayesian nonparametrics to derive a nonparametric variational information\nbottleneck (NVIB) for such attention-based embeddings. The variable number of\nmixture components supported by nonparametric methods captures the variable\nnumber of vectors supported by attention, and the exchangeability of our\nnonparametric distributions captures the permutation invariance of attention.\nThis allows NVIB to regularise the number of vectors accessible with attention,\nas well as the amount of information in individual vectors. By regularising the\ncross-attention of a Transformer encoder-decoder with NVIB, we propose a\nnonparametric variational autoencoder (NVAE). Initial experiments on training a\nNVAE on natural language text show that the induced embedding space has the\ndesired properties of a VAE for Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fehr_F/0/1/0/all/0/1\">Fabio Fehr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extract Free Dense Labels from CLIP. (arXiv:2112.01071v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01071","description":"<p>Contrastive Language-Image Pre-training (CLIP) has made a remarkable\nbreakthrough in open-vocabulary zero-shot image recognition. Many recent\nstudies leverage the pre-trained CLIP models for image-level classification and\nmanipulation. In this paper, we wish examine the intrinsic potential of CLIP\nfor pixel-level dense prediction, specifically in semantic segmentation. To\nthis end, with minimal modification, we show that MaskCLIP yields compelling\nsegmentation results on open concepts across various datasets in the absence of\nannotations and fine-tuning. By adding pseudo labeling and self-training,\nMaskCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods\nby large margins, e.g., mIoUs of unseen classes on PASCAL VOC/PASCAL\nContext/COCO Stuff are improved from 35.6/20.7/30.3 to 86.1/66.7/54.7. We also\ntest the robustness of MaskCLIP under input corruption and evaluate its\ncapability in discriminating fine-grained objects and novel concepts. Our\nfinding suggests that MaskCLIP can serve as a new reliable source of\nsupervision for dense prediction tasks to achieve annotation-free segmentation.\nSource code is available at https://github.com/chongzhou96/MaskCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Media as an Instant Source of Feedback on Water Quality. (arXiv:2202.04462v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.04462","description":"<p>This paper focuses on an important environmental challenge; namely, water\nquality by analyzing the potential of social media as an immediate source of\nfeedback. The main goal of the work is to automatically analyze and retrieve\nsocial media posts relevant to water quality with particular attention to posts\ndescribing different aspects of water quality, such as watercolor, smell,\ntaste, and related illnesses. To this aim, we propose a novel framework\nincorporating different preprocessing, data augmentation, and classification\ntechniques. In total, three different Neural Networks (NNs) architectures,\nnamely (i) Bidirectional Encoder Representations from Transformers (BERT), (ii)\nRobustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and (iii) custom\nLong short-term memory (LSTM) model, are employed in a merit-based fusion\nscheme. For merit-based weight assignment to the models, several optimization\nand search techniques are compared including a Particle Swarm Optimization\n(PSO), a Genetic Algorithm (GA), Brute Force (BF), Nelder-Mead, and Powell's\noptimization methods. We also provide an evaluation of the individual models\nwhere the highest F1-score of 0.81 is obtained with the BERT model. In\nmerit-based fusion, overall better results are obtained with BF achieving an\nF1-score score of 0.852.\n</p>\n<p>We also provide comparison against existing methods, where a significant\nimprovement for our proposed solutions is obtained. We believe such rigorous\nanalysis of this relatively new topic will provide a baseline for future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Khubaib Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayub_M/0/1/0/all/0/1\">Muhammad Asif Ayub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_J/0/1/0/all/0/1\">Jebran Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reward Modeling for Mitigating Toxicity in Transformer-based Language Models. (arXiv:2202.09662v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.09662","description":"<p>Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faal_F/0/1/0/all/0/1\">Farshid Faal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_K/0/1/0/all/0/1\">Ketra Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jia Yuan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering. (arXiv:2202.13296v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13296","description":"<p>Recent works on knowledge base question answering (KBQA) retrieve subgraphs\nfor easier reasoning. A desired subgraph is crucial as a small one may exclude\nthe answer but a large one might introduce more noises. However, the existing\nretrieval is either heuristic or interwoven with the reasoning, causing\nreasoning on the partial subgraphs, which increases the reasoning bias when the\nintermediate supervision is missing. This paper proposes a trainable subgraph\nretriever (SR) decoupled from the subsequent reasoning process, which enables a\nplug-and-play framework to enhance any subgraph-oriented KBQA model. Extensive\nexperiments demonstrate SR achieves significantly better retrieval and QA\nperformance than existing retrieval methods. Via weakly supervised pre-training\nas well as the end-to-end fine-tuning, SRl achieves new state-of-the-art\nperformance when combined with NSM, a subgraph-oriented reasoner, for\nembedding-based KBQA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaokang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jifan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cuiping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Textual Embedding against Word-level Adversarial Attacks. (arXiv:2202.13817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.13817","description":"<p>We attribute the vulnerability of natural language processing models to the\nfact that similar inputs are converted to dissimilar representations in the\nembedding space, leading to inconsistent outputs, and we propose a novel robust\ntraining method, termed Fast Triplet Metric Learning (FTML). Specifically, we\nargue that the original sample should have similar representation with its\nadversarial counterparts and distinguish its representation from other samples\nfor better robustness. To this end, we adopt the triplet metric learning into\nthe standard training to pull words closer to their positive samples (i.e.,\nsynonyms) and push away their negative samples (i.e., non-synonyms) in the\nembedding space. Extensive experiments demonstrate that FTML can significantly\npromote the model robustness against various advanced adversarial attacks while\nkeeping competitive classification accuracy on original samples. Besides, our\nmethod is efficient as it only needs to adjust the embedding and introduces\nvery little overhead on the standard training. Our work shows great potential\nof improving the textual robustness through robust word embedding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedVLN: Privacy-preserving Federated Vision-and-Language Navigation. (arXiv:2203.14936v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.14936","description":"<p>Data privacy is a central problem for embodied agents that can perceive the\nenvironment, communicate with humans, and act in the real world. While helping\nhumans complete tasks, the agent may observe and process sensitive information\nof users, such as house environments, human activities, etc. In this work, we\nintroduce privacy-preserving embodied agent learning for the task of\nVision-and-Language Navigation (VLN), where an embodied agent navigates house\nenvironments by following natural language instructions. We view each house\nenvironment as a local client, which shares nothing other than local updates\nwith the cloud server and other clients, and propose a novel federated\nvision-and-language navigation (FedVLN) framework to protect data privacy\nduring both training and pre-exploration. Particularly, we propose a\ndecentralized training strategy to limit the data of each client to its local\nmodel training and a federated pre-exploration method to do partial model\naggregation to improve model generalizability to unseen environments. Extensive\nresults on R2R and RxR datasets show that under our FedVLN framework,\ndecentralized VLN models achieve comparable results with centralized training\nwhile protecting seen environment privacy, and federated pre-exploration\nsignificantly outperforms centralized pre-exploration while preserving unseen\nenvironment privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate detection of sepsis at ED triage using machine learning with clinical natural language processing. (arXiv:2204.07657v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.07657","description":"<p>Sepsis is a life-threatening condition with organ dysfunction and is a\nleading cause of death and critical illness worldwide. Accurate detection of\nsepsis during emergency department triage would allow early initiation of lab\nanalysis, antibiotic administration, and other sepsis treatment protocols. The\npurpose of this study was to determine whether EHR data can be extracted and\nsynthesized with the latest machine learning algorithms (KATE Sepsis) and\nclinical natural language processing to produce accurate sepsis models, and\ncompare KATE Sepsis performance with existing sepsis screening protocols, such\nas SIRS and qSOFA. A machine learning model (KATE Sepsis) was developed using\npatient encounters with triage data from 16 participating hospitals. KATE\nSepsis, SIRS, standard screening (SIRS with source of infection) and qSOFA were\ntested in three settings. Cohort-A was a retrospective analysis on medical\nrecords from a single Site 1. Cohort-B was a prospective analysis of Site 1.\nCohort-C was a retrospective analysis on Site 1 with 15 additional sites.\nAcross all cohorts, KATE Sepsis demonstrates an AUC of 0.94-0.963 with\n73-74.87% TPR and 3.76-7.17% FPR. Standard screening demonstrates an AUC of\n0.682-0.726 with 39.39-51.19% TPR and 2.9-6.02% FPR. The qSOFA protocol\ndemonstrates an AUC of 0.544-0.56, with 10.52-13.18% TPR and 1.22-1.68% FPR.\nFor severe sepsis, across all cohorts, KATE Sepsis demonstrates an AUC of\n0.935-0.972 with 70-82.26% TPR and 4.64-8.62% FPR. For septic shock, across all\ncohorts, KATE Sepsis demonstrates an AUC of 0.96-0.981 with 85.71-89.66% TPR\nand 4.85-8.8% FPR. SIRS, standard screening, and qSOFA demonstrate low AUC and\nTPR for severe sepsis and septic shock detection. KATE Sepsis provided\nsubstantially better sepsis detection performance in triage than commonly used\nscreening protocols.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ivanov_O/0/1/0/all/0/1\">Oleksandr Ivanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molander_K/0/1/0/all/0/1\">Karin Molander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunne_R/0/1/0/all/0/1\">Robert Dunne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Stephen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masek_K/0/1/0/all/0/1\">Kevin Masek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_E/0/1/0/all/0/1\">Erica Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lisa Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Travers_D/0/1/0/all/0/1\">Debbie Travers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brecher_D/0/1/0/all/0/1\">Deena Brecher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delaney_D/0/1/0/all/0/1\">Deb Delaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montgomery_K/0/1/0/all/0/1\">Kyla Montgomery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reilly_C/0/1/0/all/0/1\">Christian Reilly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-SimCut: A Simple Strategy for Boosting Neural Machine Translation. (arXiv:2206.02368v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.02368","description":"<p>We introduce Bi-SimCut: a simple but effective training strategy to boost\nneural machine translation (NMT) performance. It consists of two procedures:\nbidirectional pretraining and unidirectional finetuning. Both procedures\nutilize SimCut, a simple regularization method that forces the consistency\nbetween the output distributions of the original and the cutoff sentence pairs.\nWithout leveraging extra dataset via back-translation or integrating\nlarge-scale pretrained model, Bi-SimCut achieves strong translation performance\nacross five translation benchmarks (data sizes range from 160K to 20.2M): BLEU\nscores of 31.16 for en -&gt; de and 38.37 for de -&gt; en on the IWSLT14 dataset,\n30.78 for en -&gt; de and 35.15 for de -&gt; en on the WMT14 dataset, and 27.17 for\nzh -&gt; en on the WMT17 dataset. SimCut is not a new method, but a version of\nCutoff (Shen et al., 2020) simplified and adapted for NMT, and it could be\nconsidered as a perturbation-based method. Given the universality and\nsimplicity of SimCut and Bi-SimCut, we believe they can serve as strong\nbaselines for future NMT research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity. (arXiv:2207.13129v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13129","description":"<p>We propose transferability from Large Geometric Vicinity (LGV), a new\ntechnique to increase the transferability of black-box adversarial attacks. LGV\nstarts from a pretrained surrogate model and collects multiple weight sets from\na few additional training epochs with a constant and high learning rate. LGV\nexploits two geometric properties that we relate to transferability. First,\nmodels that belong to a wider weight optimum are better surrogates. Second, we\nidentify a subspace able to generate an effective surrogate ensemble among this\nwider optimum. Through extensive experiments, we show that LGV alone\noutperforms all (combinations of) four established test-time transformations by\n1.8 to 59.9 percentage points. Our findings shed new light on the importance of\nthe geometry of the weight space to explain the transferability of adversarial\nexamples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gubri_M/0/1/0/all/0/1\">Martin Gubri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordy_M/0/1/0/all/0/1\">Maxime Cordy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadakis_M/0/1/0/all/0/1\">Mike Papadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traon_Y/0/1/0/all/0/1\">Yves Le Traon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_K/0/1/0/all/0/1\">Koushik Sen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Evidential Learning for Few-Shot Classification. (arXiv:2207.13137v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13137","description":"<p>Few-Shot Classification(FSC) aims to generalize from base classes to novel\nclasses given very limited labeled samples, which is an important step on the\npath toward human-like machine learning. State-of-the-art solutions involve\nlearning to find a good metric and representation space to compute the distance\nbetween samples. Despite the promising accuracy performance, how to model\nuncertainty for metric-based FSC methods effectively is still a challenge. To\nmodel uncertainty, We place a distribution over class probability based on the\ntheory of evidence. As a result, uncertainty modeling and metric learning can\nbe decoupled. To reduce the uncertainty of classification, we propose a\nBayesian evidence fusion theorem. Given observed samples, the network learns to\nget posterior distribution parameters given the prior parameters produced by\nthe pre-trained network. Detailed gradient analysis shows that our method\nprovides a smooth optimization target and can capture the uncertainty. The\nproposed method is agnostic to metric learning strategies and can be\nimplemented as a plug-and-play module. We integrate our method into several\nnewest FSC methods and demonstrate the improved accuracy and uncertainty\nquantification on standard FSC benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Linghu_X/0/1/0/all/0/1\">Xiongkun Linghu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1\">Yihang Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shengsen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining. (arXiv:2207.13148v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13148","description":"<p>Rich temporal information and variations in viewpoints make video data an\nattractive choice for learning image representations using unsupervised\ncontrastive learning (UCL) techniques. State-of-the-art (SOTA) contrastive\nlearning techniques consider frames within a video as positives in the\nembedding space, whereas the frames from other videos are considered negatives.\nWe observe that unlike multiple views of an object in natural scene videos, an\nUltrasound (US) video captures different 2D slices of an organ. Hence, there is\nalmost no similarity between the temporally distant frames of even the same US\nvideo. In this paper we propose to instead utilize such frames as hard\nnegatives. We advocate mining both intra-video and cross-video negatives in a\nhardness-sensitive negative mining curriculum in a UCL framework to learn rich\nimage representations. We deploy our framework to learn the representations of\nGallbladder (GB) malignancy from US videos. We also construct the first\nlarge-scale US video dataset containing 64 videos and 15,800 frames for\nlearning GB representations. We show that the standard ResNet50 backbone\ntrained with our framework improves the accuracy of models pretrained with SOTA\nUCL techniques as well as supervised pretrained models on ImageNet for the GB\nmalignancy detection task by 2-6%. We further validate the generalizability of\nour method on a publicly available lung US image dataset of COVID-19\npathologies and show an improvement of 1.5% compared to SOTA. Source code,\ndataset, and models are available at https://gbc-iitd.github.io/usucl.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Basu_S/0/1/0/all/0/1\">Soumen Basu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singla_S/0/1/0/all/0/1\">Somanshu Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_M/0/1/0/all/0/1\">Mayank Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rana_P/0/1/0/all/0/1\">Pratyaksha Rana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_P/0/1/0/all/0/1\">Pankaj Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TINYCD: A (Not So) Deep Learning Model For Change Detection. (arXiv:2207.13159v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13159","description":"<p>The aim of change detection (CD) is to detect changes occurred in the same\narea by comparing two images of that place taken at different times. The\nchallenging part of the CD is to keep track of the changes the user wants to\nhighlight, such as new buildings, and to ignore changes due to external factors\nsuch as environmental, lighting condition, fog or seasonal changes. Recent\ndevelopments in the field of deep learning enabled researchers to achieve\noutstanding performance in this area. In particular, different mechanisms of\nspace-time attention allowed to exploit the spatial features that are extracted\nfrom the models and to correlate them also in a temporal way by exploiting both\nthe available images. The downside is that the models have become increasingly\ncomplex and large, often unfeasible for edge applications. These are\nlimitations when the models must be applied to the industrial field or in\napplications requiring real-time performances. In this work we propose a novel\nmodel, called TinyCD, demonstrating to be both lightweight and effective, able\nto achieve performances comparable or even superior to the current state of the\nart with 13-150X fewer parameters. In our approach we have exploited the\nimportance of low-level features to compare images. To do this, we use only few\nbackbone blocks. This strategy allow us to keep the number of network\nparameters low. To compose the features extracted from the two images, we\nintroduce a novel, economical in terms of parameters, mixing block capable of\ncross correlating features in both space and time domains. Finally, to fully\nexploit the information contained in the computed features, we define the\nPW-MLP block able to perform a pixel wise classification. Source code, models\nand results are available here:\nhttps://github.com/AndreaCodegoni/Tiny_model_4_CD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Codegoni_A/0/1/0/all/0/1\">Andrea Codegoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lombardi_G/0/1/0/all/0/1\">Gabriele Lombardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_A/0/1/0/all/0/1\">Alessandro Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval-Augmented Transformer for Image Captioning. (arXiv:2207.13162v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13162","description":"<p>Image captioning models aim at connecting Vision and Language by providing\nnatural language descriptions of input images. In the past few years, the task\nhas been tackled by learning parametric models and proposing visual feature\nextraction advancements or by modeling better multi-modal connections. In this\npaper, we investigate the development of an image captioning approach with a\nkNN memory, with which knowledge can be retrieved from an external corpus to\naid the generation process. Our architecture combines a knowledge retriever\nbased on visual similarities, a differentiable encoder, and a kNN-augmented\nattention layer to predict tokens based on the past context and on text\nretrieved from the external memory. Experimental results, conducted on the COCO\ndataset, demonstrate that employing an explicit external memory can aid the\ngeneration process and increase caption quality. Our work opens up new avenues\nfor improving image captioning models at larger scale.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarto_S/0/1/0/all/0/1\">Sara Sarto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO and Mask R-CNN for Vehicle Number Plate Identification. (arXiv:2207.13165v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13165","description":"<p>License plate scanners have grown in popularity in parking lots during the\npast few years. In order to quickly identify license plates, traditional plate\nrecognition devices used in parking lots employ a fixed source of light and\nshooting angles. For skewed angles, such as license plate images taken with\nultra-wide angle or fisheye lenses, deformation of the license plate\nrecognition plate can also be quite severe, impairing the ability of standard\nlicense plate recognition systems to identify the plate. Mask RCNN gadget that\nmay be utilised for oblique pictures and various shooting angles. The results\nof the experiments show that the suggested design will be capable of\nclassifying license plates with bevel angles larger than 0/60. Character\nrecognition using the suggested Mask R-CNN approach has advanced significantly\nas well. The proposed Mask R-CNN method has also achieved significant progress\nin character recognition, which is tilted more than 45 degrees as compared to\nthe strategy of employing the YOLOv2 model. Experiment results also suggest\nthat the methodology presented in the open data plate collecting is better than\nother techniques (known as the AOLP dataset).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganjoo_S/0/1/0/all/0/1\">Siddharth Ganjoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR-to-EO Image Translation with Multi-Conditional Adversarial Networks. (arXiv:2207.13184v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13184","description":"<p>This paper explores the use of multi-conditional adversarial networks for\nSAR-to-EO image translation. Previous methods condition adversarial networks\nonly on the input SAR. We show that incorporating multiple complementary\nmodalities such as Google maps and IR can further improve SAR-to-EO image\ntranslation especially on preserving sharp edges of manmade objects. We\ndemonstrate effectiveness of our approach on a diverse set of datasets\nincluding SEN12MS, DFC2020, and SpaceNet6. Our experimental results suggest\nthat additional information provided by complementary modalities improves the\nperformance of SAR-to-EO image translation compared to the models trained on\npaired SAR and EO data only. To best of our knowledge, our approach is the\nfirst to leverage multiple modalities for improving SAR-to-EO image translation\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1\">Armando Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Prafull Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newey_M/0/1/0/all/0/1\">Michael Newey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-Based Keypoint Registration for Fetoscopic Mosaicking. (arXiv:2207.13185v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13185","description":"<p>In Twin-to-Twin Transfusion Syndrome (TTTS), abnormal vascular anastomoses in\nthe monochorionic placenta can produce uneven blood flow between the two\nfetuses. In the current practice, TTTS is treated surgically by closing\nabnormal anastomoses using laser ablation. This surgery is minimally invasive\nand relies on fetoscopy. Limited field of view makes anastomosis identification\na challenging task for the surgeon. To tackle this challenge, we propose a\nlearning-based framework for in-vivo fetoscopy frame registration for\nfield-of-view expansion. The novelties of this framework relies on a\nlearning-based keypoint proposal network and an encoding strategy to filter (i)\nirrelevant keypoints based on fetoscopic image segmentation and (ii)\ninconsistent homographies. We validate of our framework on a dataset of 6\nintraoperative sequences from 6 TTTS surgeries from 6 different women against\nthe most recent state of the art algorithm, which relies on the segmentation of\nplacenta vessels. The proposed framework achieves higher performance compared\nto the state of the art, paving the way for robust mosaicking to provide\nsurgeons with context awareness during TTTS surgery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Casella_A/0/1/0/all/0/1\">Alessandro Casella</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bano_S/0/1/0/all/0/1\">Sophia Bano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasconcelos_F/0/1/0/all/0/1\">Francisco Vasconcelos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+David_A/0/1/0/all/0/1\">Anna L. David</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paladini_D/0/1/0/all/0/1\">Dario Paladini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deprest_J/0/1/0/all/0/1\">Jan Deprest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mattos_L/0/1/0/all/0/1\">Leonardo S. Mattos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Model-Based Architectures for Inverse Problems under Mismatched Priors. (arXiv:2207.13200v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13200","description":"<p>There is a growing interest in deep model-based architectures (DMBAs) for\nsolving imaging inverse problems by combining physical measurement models and\nlearned image priors specified using convolutional neural nets (CNNs). For\nexample, well-known frameworks for systematically designing DMBAs include\nplug-and-play priors (PnP), deep unfolding (DU), and deep equilibrium models\n(DEQ). While the empirical performance and theoretical properties of DMBAs have\nbeen widely investigated, the existing work in the area has primarily focused\non their performance when the desired image prior is known exactly. This work\naddresses the gap in the prior work by providing new theoretical and numerical\ninsights into DMBAs under mismatched CNN priors. Mismatched priors arise\nnaturally when there is a distribution shift between training and testing data,\nfor example, due to test images being from a different distribution than images\nused for training the CNN prior. They also arise when the CNN prior used for\ninference is an approximation of some desired statistical estimator (MAP or\nMMSE). Our theoretical analysis provides explicit error bounds on the solution\ndue to the mismatched CNN priors under a set of clearly specified assumptions.\nOur numerical results compare the empirical performance of DMBAs under\nrealistic distribution shifts and approximate statistical estimators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shoushtari_S/0/1/0/all/0/1\">Shirin Shoushtari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yuyang Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamilov_U/0/1/0/all/0/1\">Ulugbek S. Kamilov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-McBert: A Multi-choice Self-supervised Framework for Point Cloud Pre-training. (arXiv:2207.13226v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13226","description":"<p>Masked language modeling (MLM) has become one of the most successful\nself-supervised pre-training task. Inspired by its success, Point-Bert, as a\npioneer work in point cloud, proposed masked point modeling (MPM) to pre-train\npoint transformer on large scale unanotated dataset. Despite its great\nperformance, we find inherent difference between language and point cloud tends\nto cause ambiguous tokenization for point cloud. For point cloud, there doesn't\nexist a gold standard for point cloud tokenization. Although Point-Bert\nintroduce a discrete Variational AutoEncoder (dVAE) as tokenizer to allocate\ntoken ids to local patches, it tends to generate ambigious token ids for local\npatches. We find this imperfect tokenizer might generate different token ids\nfor semantically-similar patches and same token ids for semantically-dissimilar\npatches. To tackle above problem, we propose our Point-McBert, a pre-training\nframework with eased and refined supervision signals. Specifically, we ease the\nprevious single-choice constraint on patches, and provide multi-choice token\nids for each patch as supervision. Moreover, we utilitze the high-level\nsemantics learned by transformer to further refine our supervision signals.\nExtensive experiments on point cloud classification, few-shot classification\nand part segmentation tasks demonstrate the superiority of our method, e.g.,\nthe pre-trained transformer achieves 94.1% accuracy on ModelNet40, 84.28%\naccuracy on the hardest setting of ScanObjectNN and new state-of-the-art\nperformance on few-shot learning. We also demonstrate that our method not only\nimproves the performance of Point-Bert on all downstream tasks, but also incurs\nalmost no extra computational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1\">Kexue Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Mingzhi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Manning Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mid-level Representation Enhancement and Graph Embedded Uncertainty Suppressing for Facial Expression Recognition. (arXiv:2207.13235v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13235","description":"<p>Facial expression is an essential factor in conveying human emotional states\nand intentions. Although remarkable advancement has been made in facial\nexpression recognition (FER) task, challenges due to large variations of\nexpression patterns and unavoidable data uncertainties still remain. In this\npaper, we propose mid-level representation enhancement (MRE) and graph embedded\nuncertainty suppressing (GUS) addressing these issues. On one hand, MRE is\nintroduced to avoid expression representation learning being dominated by a\nlimited number of highly discriminative patterns. On the other hand, GUS is\nintroduced to suppress the feature ambiguity in the representation space. The\nproposed method not only has stronger generalization capability to handle\ndifferent variations of expression patterns but also more robustness to capture\nexpression representations. Experimental evaluation on Aff-Wild2 have verified\nthe effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zeyu Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_X/0/1/0/all/0/1\">Xu Juan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuaiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Image Synthesis and Self-supervised Feature Adaptation for Cross-Modality Biomedical Image Segmentation. (arXiv:2207.13240v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13240","description":"<p>This work presents a novel framework CISFA (Contrastive Image synthesis and\nSelf-supervised Feature Adaptation)that builds on image domain translation and\nunsupervised feature adaptation for cross-modality biomedical image\nsegmentation. Different from existing works, we use a one-sided generative\nmodel and add a weighted patch-wise contrastive loss between sampled patches of\nthe input image and the corresponding synthetic image, which serves as shape\nconstraints. Moreover, we notice that the generated images and input images\nshare similar structural information but are in different modalities. As such,\nwe enforce contrastive losses on the generated images and the input images to\ntrain the encoder of a segmentation model to minimize the discrepancy between\npaired images in the learned embedding space. Compared with existing works that\nrely on adversarial learning for feature adaptation, such a method enables the\nencoder to learn domain-independent features in a more explicit way. We\nextensively evaluate our methods on segmentation tasks containing CT and MRI\nimages for abdominal cavities and whole hearts. Experimental results show that\nthe proposed framework not only outputs synthetic images with less distortion\nof organ shapes, but also outperforms state-of-the-art domain adaptation\nmethods by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinrong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Corey Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-based Visual Question Answering: Estimating Semantic Inconsistency between Image and Knowledge Base. (arXiv:2207.13242v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13242","description":"<p>Knowledge-based visual question answering (KVQA) task aims to answer\nquestions that require additional external knowledge as well as an\nunderstanding of images and questions. Recent studies on KVQA inject an\nexternal knowledge in a multi-modal form, and as more knowledge is used,\nirrelevant information may be added and can confuse the question answering. In\norder to properly use the knowledge, this study proposes the following: 1) we\nintroduce a novel semantic inconsistency measure computed from caption\nuncertainty and semantic similarity; 2) we suggest a new external knowledge\nassimilation method based on the semantic inconsistency measure and apply it to\nintegrate explicit knowledge and implicit knowledge for KVQA; 3) the proposed\nmethod is evaluated with the OK-VQA dataset and achieves the state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chae_J/0/1/0/all/0/1\">Jinyeong Chae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jihie Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks. (arXiv:2207.13243v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13243","description":"<p>The last decade of machine learning has seen drastic increases in scale and\ncapabilities, and deep neural networks (DNNs) are increasingly being deployed\nacross a wide range of domains. However, the inner workings of DNNs are\ngenerally difficult to understand, raising concerns about the safety of using\nthese systems without a rigorous understanding of how they function. In this\nsurvey, we review literature on techniques for interpreting the inner\ncomponents of DNNs, which we call \"inner\" interpretability methods.\nSpecifically, we review methods for interpreting weights, neurons, subnetworks,\nand latent representations with a focus on how these techniques relate to the\ngoal of designing safer, more trustworthy AI systems. We also highlight\nconnections between interpretability and work in modularity, adversarial\nrobustness, continual learning, network compression, and studying the human\nvisual system. Finally, we discuss key challenges and argue for future work in\ninterpretability for AI safety that focuses on diagnostics, benchmarking, and\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raukur_T/0/1/0/all/0/1\">Tilman R&#xe4;ukur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_A/0/1/0/all/0/1\">Anson Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1\">Dylan Hadfield-Menell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concurrent Subsidiary Supervision for Unsupervised Source-Free Domain Adaptation. (arXiv:2207.13247v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13247","description":"<p>The prime challenge in unsupervised domain adaptation (DA) is to mitigate the\ndomain shift between the source and target domains. Prior DA works show that\npretext tasks could be used to mitigate this domain shift by learning domain\ninvariant representations. However, in practice, we find that most existing\npretext tasks are ineffective against other established techniques. Thus, we\ntheoretically analyze how and when a subsidiary pretext task could be leveraged\nto assist the goal task of a given DA problem and develop objective subsidiary\ntask suitability criteria. Based on this criteria, we devise a novel process of\nsticker intervention and cast sticker classification as a supervised subsidiary\nDA problem concurrent to the goal task unsupervised DA. Our approach not only\nimproves goal task adaptation performance, but also facilitates\nprivacy-oriented source-free DA i.e. without concurrent source-target access.\nExperiments on the standard Office-31, Office-Home, DomainNet, and VisDA\nbenchmarks demonstrate our superiority for both single-source and multi-source\nsource-free DA. Our approach also complements existing non-source-free works,\nachieving leading performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_J/0/1/0/all/0/1\">Jogendra Nath Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhambri_S/0/1/0/all/0/1\">Suvaansh Bhambri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1\">Akshay Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_H/0/1/0/all/0/1\">Hiran Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AADG: Automatic Augmentation for Domain Generalization on Retinal Image Segmentation. (arXiv:2207.13249v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13249","description":"<p>Convolutional neural networks have been widely applied to medical image\nsegmentation and have achieved considerable performance. However, the\nperformance may be significantly affected by the domain gap between training\ndata (source domain) and testing data (target domain). To address this issue,\nwe propose a data manipulation based domain generalization method, called\nAutomated Augmentation for Domain Generalization (AADG). Our AADG framework can\neffectively sample data augmentation policies that generate novel domains and\ndiversify the training set from an appropriate search space. Specifically, we\nintroduce a novel proxy task maximizing the diversity among multiple augmented\nnovel domains as measured by the Sinkhorn distance in a unit sphere space,\nmaking automated augmentation tractable. Adversarial training and deep\nreinforcement learning are employed to efficiently search the objectives.\nQuantitative and qualitative experiments on 11 publicly-accessible fundus image\ndatasets (four for retinal vessel segmentation, four for optic disc and cup\n(OD/OC) segmentation and three for retinal lesion segmentation) are\ncomprehensively performed. Two OCTA datasets for retinal vasculature\nsegmentation are further involved to validate cross-modality generalization.\nOur proposed AADG exhibits state-of-the-art generalization performance and\noutperforms existing approaches by considerable margins on retinal vessel,\nOD/OC and lesion segmentation tasks. The learned policies are empirically\nvalidated to be model-agnostic and can transfer well to other models. The\nsource code is available at https://github.com/CRazorback/AADG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1\">Junyan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiqi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yijin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_P/0/1/0/all/0/1\">Pujin Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Self-attention Modeling with Temporal Patch Shift for Action Recognition. (arXiv:2207.13259v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13259","description":"<p>Transformer-based methods have recently achieved great advancement on 2D\nimage-based vision tasks. For 3D video-based tasks such as action recognition,\nhowever, directly applying spatiotemporal transformers on video data will bring\nheavy computation and memory burdens due to the largely increased number of\npatches and the quadratic complexity of self-attention computation. How to\nefficiently and effectively model the 3D self-attention of video data has been\na great challenge for transformers. In this paper, we propose a Temporal Patch\nShift (TPS) method for efficient 3D self-attention modeling in transformers for\nvideo-based action recognition. TPS shifts part of patches with a specific\nmosaic pattern in the temporal dimension, thus converting a vanilla spatial\nself-attention operation to a spatiotemporal one with little additional cost.\nAs a result, we can compute 3D self-attention using nearly the same computation\nand memory cost as 2D self-attention. TPS is a plug-and-play module and can be\ninserted into existing 2D transformer models to enhance spatiotemporal feature\nlearning. The proposed method achieves competitive performance with\nstate-of-the-arts on Something-something V1 &amp; V2, Diving-48, and Kinetics400\nwhile being much more efficient on computation and memory cost. The source code\nof TPS can be found at https://github.com/MartinXM/TPS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wangmeng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Biao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xihan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-specific 6-DoF Object Pose Estimation from Minimal Annotations. (arXiv:2207.13264v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13264","description":"<p>In many robotic applications, the environment setting in which the 6-DoF pose\nestimation of a known, rigid object and its subsequent grasping is to be\nperformed, remains nearly unchanging and might even be known to the robot in\nadvance. In this paper, we refer to this problem as instance-specific pose\nestimation: the robot is expected to estimate the pose with a high degree of\naccuracy in only a limited set of familiar scenarios. Minor changes in the\nscene, including variations in lighting conditions and background appearance,\nare acceptable but drastic alterations are not anticipated. To this end, we\npresent a method to rapidly train and deploy a pipeline for estimating the\ncontinuous 6-DoF pose of an object from a single RGB image. The key idea is to\nleverage known camera poses and rigid body geometry to partially automate the\ngeneration of a large labeled dataset. The dataset, along with sufficient\ndomain randomization, is then used to supervise the training of deep neural\nnetworks for predicting semantic keypoints. Experimentally, we demonstrate the\nconvenience and effectiveness of our proposed method to accurately estimate\nobject pose requiring only a very small amount of manual annotation for\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rohan Pratap Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumagai_I/0/1/0/all/0/1\">Iori Kumagai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabas_A/0/1/0/all/0/1\">Antonio Gabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benallegue_M/0/1/0/all/0/1\">Mehdi Benallegue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshiyasu_Y/0/1/0/all/0/1\">Yusuke Yoshiyasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanehiro_F/0/1/0/all/0/1\">Fumio Kanehiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fault Detection and Classification of Aerospace Sensors using a VGG16-based Deep Neural Network. (arXiv:2207.13267v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13267","description":"<p>Compared with traditional model-based fault detection and classification\n(FDC) methods, deep neural networks (DNN) prove to be effective for the\naerospace sensors FDC problems. However, time being consumed in training the\nDNN is excessive, and explainability analysis for the FDC neural network is\nstill underwhelming. A concept known as imagefication-based intelligent FDC has\nbeen studied in recent years. This concept advocates to stack the sensors\nmeasurement data into an image format, the sensors FDC issue is then\ntransformed to abnormal regions detection problem on the stacked image, which\nmay well borrow the recent advances in the machine vision vision realm.\nAlthough promising results have been claimed in the imagefication-based\nintelligent FDC researches, due to the low size of the stacked image, small\nconvolutional kernels and shallow DNN layers were used, which hinders the FDC\nperformance. In this paper, we first propose a data augmentation method which\ninflates the stacked image to a larger size (correspondent to the VGG16 net\ndeveloped in the machine vision realm). The FDC neural network is then trained\nvia fine-tuning the VGG16 directly. To truncate and compress the FDC net size\n(hence its running time), we perform model pruning on the fine-tuned net. Class\nactivation mapping (CAM) method is also adopted for explainability analysis of\nthe FDC net to verify its internal operations. Via data augmentation,\nfine-tuning from VGG16, and model pruning, the FDC net developed in this paper\nclaims an FDC accuracy 98.90% across 4 aircraft at 5 flight conditions (running\ntime 26 ms). The CAM results also verify the FDC net w.r.t. its internal\noperations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunmei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jinyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1\">Jianliang Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yiqun Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Graph-constrained Vectorized Floorplan Generation with Panoptic Refinement. (arXiv:2207.13268v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13268","description":"<p>The automatic generation of floorplans given user inputs has great potential\nin architectural design and has recently been explored in the computer vision\ncommunity. However, the majority of existing methods synthesize floorplans in\nthe format of rasterized images, which are difficult to edit or customize. In\nthis paper, we aim to synthesize floorplans as sequences of 1-D vectors, which\neases user interaction and design customization. To generate high fidelity\nvectorized floorplans, we propose a novel two-stage framework, including a\ndraft stage and a multi-round refining stage. In the first stage, we encode the\nroom connectivity graph input by users with a graph convolutional network\n(GCN), then apply an autoregressive transformer network to generate an initial\nfloorplan sequence. To polish the initial design and generate more visually\nappealing floorplans, we further propose a novel panoptic refinement\nnetwork(PRN) composed of a GCN and a transformer network. The PRN takes the\ninitial generated sequence as input and refines the floorplan design while\nencouraging the correct room connectivity with our proposed geometric loss. We\nhave conducted extensive experiments on a real-world floorplan dataset, and the\nresults show that our method achieves state-of-the-art performance under\ndifferent settings and evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Yuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Jose Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhawat_K/0/1/0/all/0/1\">Krishnendra Shekhawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zihan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vector Quantized Image-to-Image Translation. (arXiv:2207.13286v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13286","description":"<p>Current image-to-image translation methods formulate the task with\nconditional generation models, leading to learning only the recolorization or\nregional changes as being constrained by the rich structural information\nprovided by the conditional contexts. In this work, we propose introducing the\nvector quantization technique into the image-to-image translation framework.\nThe vector quantized content representation can facilitate not only the\ntranslation, but also the unconditional distribution shared among different\ndomains. Meanwhile, along with the disentangled style representation, the\nproposed method further enables the capability of image extension with\nflexibility in both intra- and inter-domains. Qualitative and quantitative\nexperiments demonstrate that our framework achieves comparable performance to\nthe state-of-the-art image-to-image translation and image extension methods.\nCompared to methods for individual tasks, the proposed method, as a unified\nframework, unleashes applications combining image-to-image translation,\nunconditional generation, and image extension altogether. For example, it\nprovides style variability for image generation and extension, and equips\nimage-to-image translation with further extension capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shin-I Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1\">Hung-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applied Computer Vision on 2-Dimensional Lung X-Ray Images for Assisted Medical Diagnosis of Pneumonia. (arXiv:2207.13295v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13295","description":"<p>This study focuses on the application of a specific subfield of artificial\nintelligence referred to as computer vision in the analysis of 2-dimensional\nlung x-ray images for the assisted medical diagnosis of ordinary pneumonia.\n</p>\n<p>A convolutional neural network algorithm was implemented in a Python-coded,\nFlask-based web application that can analyze x-ray images for the detection of\nordinary pneumonia. Since convolutional neural network algorithms rely on\nmachine learning for the identification and detection of patterns, a technique\nreferred to as transfer learning was implemented to train the neural network in\nthe identification and detection of patterns within the dataset. Open-source\nlung x-ray images were used as training data to create a knowledge base that\nserved as the core element of the web application and the experimental design\nemployed a 5-Trial Confirmatory Test for the validation of the web application.\n</p>\n<p>The results of the 5-Trial Confirmatory Test show the calculation of\nDiagnostic Precision Percentage per Trial, General Diagnostic Precision\nPercentage, and General Diagnostic Error Percentage while the Confusion Matrix\nfurther shows the relationship between the label and the corresponding\ndiagnosis result of the web application on each test images.\n</p>\n<p>The developed web application can be used by medical practitioners in\nA.I.-assisted diagnosis of ordinary pneumonia, and by researchers in the fields\nof computer science and bioinformatics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ligueran_R/0/1/0/all/0/1\">Ralph Joseph S.D. Ligueran</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Santos_M/0/1/0/all/0/1\">Manuel Luis C. Delos Santos</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Tinio_D/0/1/0/all/0/1\">Dr. Ronaldo S. Tinio</a> (3), <a href=\"http://arxiv.org/find/eess/1/au:+Valencia_E/0/1/0/all/0/1\">Emmanuel H. Valencia</a> (4) ((1)(2)(4) Asian Institute of Computer Studies, (3) Pamantasan ng Lungsod ng Valenzuela)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPS-GLASS: Learning Nighttime Semantic Segmentation Using Daytime Video and GPS data. (arXiv:2207.13297v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13297","description":"<p>Semantic segmentation for autonomous driving should be robust against various\nin-the-wild environments. Nighttime semantic segmentation is especially\nchallenging due to a lack of annotated nighttime images and a large domain gap\nfrom daytime images with sufficient annotation. In this paper, we propose a\nnovel GPS-based training framework for nighttime semantic segmentation. Given\nGPS-aligned pairs of daytime and nighttime images, we perform cross-domain\ncorrespondence matching to obtain pixel-level pseudo supervision. Moreover, we\nconduct flow estimation between daytime video frames and apply GPS-based\nscaling to acquire another pixel-level pseudo supervision. Using these pseudo\nsupervisions with a confidence map, we train a nighttime semantic segmentation\nnetwork without any annotation from nighttime images. Experimental results\ndemonstrate the effectiveness of the proposed method on several nighttime\nsemantic segmentation datasets. Our source code is available at\nhttps://github.com/jimmy9704/GPS-GLASS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hongjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Changwoo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Attention All NeRF Needs?. (arXiv:2207.13298v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13298","description":"<p>We present Generalizable NeRF Transformer (GNT), a pure, unified\ntransformer-based architecture that efficiently reconstructs Neural Radiance\nFields (NeRFs) on the fly from source views. Unlike prior works on NeRF that\noptimize a per-scene implicit representation by inverting a handcrafted\nrendering equation, GNT achieves generalizable neural scene representation and\nrendering, by encapsulating two transformer-based stages. The first stage of\nGNT, called view transformer, leverages multi-view geometry as an inductive\nbias for attention-based scene representation, and predicts coordinate-aligned\nfeatures by aggregating information from epipolar lines on the neighboring\nviews. The second stage of GNT, named ray transformer, renders novel views by\nray marching and directly decodes the sequence of sampled point features using\nthe attention mechanism. Our experiments demonstrate that when optimized on a\nsingle scene, GNT can successfully reconstruct NeRF without explicit rendering\nformula, and even improve the PSNR by ~1.3dB on complex scenes due to the\nlearnable ray renderer. When trained across various scenes, GNT consistently\nachieves the state-of-the-art performance when transferring to forward-facing\nLLFF dataset (LPIPS ~20%, SSIM ~25%$) and synthetic blender dataset (LPIPS\n~20%, SSIM ~4%). In addition, we show that depth and occlusion can be inferred\nfrom the learned attention maps, which implies that the pure attention\nmechanism is capable of learning a physically-grounded rendering process. All\nthese results bring us one step closer to the tantalizing hope of utilizing\ntransformers as the \"universal modeling tool\" even for graphics. Please refer\nto our project page for video results: https://vita-group.github.io/GNT/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+T_M/0/1/0/all/0/1\">Mukund Varma T</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_S/0/1/0/all/0/1\">Subhashini Venugopalan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-ABN: Learning to Generate Sharp Attention Maps for Action Recognition. (arXiv:2207.13306v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13306","description":"<p>In this paper we propose an extension of the Attention Branch Network (ABN)\nby using instance segmentation for generating sharper attention maps for action\nrecognition. Methods for visual explanation such as Grad-CAM usually generate\nblurry maps which are not intuitive for humans to understand, particularly in\nrecognizing actions of people in videos. Our proposed method, Object-ABN,\ntackles this issue by introducing a new mask loss that makes the generated\nattention maps close to the instance segmentation result. Further the PC loss\nand multiple attention maps are introduced to enhance the sharpness of the maps\nand improve the performance of classification. Experimental results with UCF101\nand SSv2 shows that the generated maps by the proposed method are much clearer\nqualitatively and quantitatively than those of the original ABN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nitta_T/0/1/0/all/0/1\">Tomoya Nitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirakawa_T/0/1/0/all/0/1\">Tsubasa Hirakawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujiyoshi_H/0/1/0/all/0/1\">Hironobu Fujiyoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamaki_T/0/1/0/all/0/1\">Toru Tamaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Selective Aggregation for Knowledge Amalgamation. (arXiv:2207.13309v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13309","description":"<p>In this paper, we explore a new knowledge-amalgamation problem, termed\nFederated Selective Aggregation (FedSA). The goal of FedSA is to train a\nstudent model for a new task with the help of several decentralized teachers,\nwhose pre-training tasks and data are different and agnostic. Our motivation\nfor investigating such a problem setup stems from a recent dilemma of model\nsharing. Many researchers or institutes have spent enormous resources on\ntraining large and competent networks. Due to the privacy, security, or\nintellectual property issues, they are, however, not able to share their own\npre-trained models, even if they wish to contribute to the community. The\nproposed FedSA offers a solution to this dilemma and makes it one step further\nsince, again, the learned student may specialize in a new task different from\nall of the teachers. To this end, we proposed a dedicated strategy for handling\nFedSA. Specifically, our student-training process is driven by a novel\nsaliency-based approach that adaptively selects teachers as the participants\nand integrates their representative capabilities into the student. To evaluate\nthe effectiveness of FedSA, we conduct experiments on both single-task and\nmulti-task settings. Experimental results demonstrate that FedSA effectively\namalgamates knowledge from decentralized models and achieves competitive\nperformance to centralized baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Donglin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Ruonan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Gongfan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jie Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zunlei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Li Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1\">Mingli Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Portrait Interpretation and a Benchmark. (arXiv:2207.13315v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13315","description":"<p>We propose a task we name Portrait Interpretation and construct a dataset\nnamed Portrait250K for it. Current researches on portraits such as human\nattribute recognition and person re-identification have achieved many\nsuccesses, but generally, they: 1) may lack mining the interrelationship\nbetween various tasks and the possible benefits it may bring; 2) design deep\nmodels specifically for each task, which is inefficient; 3) may be unable to\ncope with the needs of a unified model and comprehensive perception in actual\nscenes. In this paper, the proposed portrait interpretation recognizes the\nperception of humans from a new systematic perspective. We divide the\nperception of portraits into three aspects, namely Appearance, Posture, and\nEmotion, and design corresponding sub-tasks for each aspect. Based on the\nframework of multi-task learning, portrait interpretation requires a\ncomprehensive description of static attributes and dynamic states of portraits.\nTo invigorate research on this new task, we construct a new dataset that\ncontains 250,000 images labeled with identity, gender, age, physique, height,\nexpression, and posture of the whole body and arms. Our dataset is collected\nfrom 51 movies, hence covering extensive diversity. Furthermore, we focus on\nrepresentation learning for portrait interpretation and propose a baseline that\nreflects our systematic perspective. We also propose an appropriate metric for\nthis task. Our experimental results demonstrate that combining the tasks\nrelated to portrait interpretation can yield benefits. Code and dataset will be\nmade public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhaopeng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yali Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengjin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NICEST: Noisy Label Correction and Training for Robust Scene Graph Generation. (arXiv:2207.13316v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13316","description":"<p>Nearly all existing scene graph generation (SGG) models have overlooked the\nground-truth annotation qualities of mainstream SGG datasets, i.e., they\nassume: 1) all the manually annotated positive samples are equally correct; 2)\nall the un-annotated negative samples are absolutely background. In this paper,\nwe argue that neither of the assumptions applies to SGG: there are numerous\nnoisy ground-truth predicate labels that break these two assumptions and harm\nthe training of unbiased SGG models. To this end, we propose a novel NoIsy\nlabel CorrEction and Sample Training strategy for SGG: NICEST. Specifically, it\nconsists of two parts: NICE and NIST, which rule out these noisy label issues\nby generating high-quality samples and the effective training strategy,\nrespectively. NICE first detects noisy samples and then reassigns them more\nhigh-quality soft predicate labels. NIST is a multi-teacher knowledge\ndistillation based training strategy, which enables the model to learn unbiased\nfusion knowledge. And a dynamic trade-off weighting strategy in NIST is\ndesigned to penalize the bias of different teachers. Due to the model-agnostic\nnature of both NICE and NIST, our NICEST can be seamlessly incorporated into\nany SGG architecture to boost its performance on different predicate\ncategories. In addition, to better evaluate the generalization of SGG models,\nwe further propose a new benchmark VG-OOD, by re-organizing the prevalent VG\ndataset and deliberately making the predicate distributions of the training and\ntest sets as different as possible for each subject-object category pair. This\nnew benchmark helps disentangle the influence of subject-object category based\nfrequency biases. Extensive ablations and results on different backbones and\ntasks have attested to the effectiveness and generalization ability of each\ncomponent of NICEST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hanrong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Embedding Makes Hierarchical Vision Transformer Stronger. (arXiv:2207.13317v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13317","description":"<p>Vision Transformers (ViTs) have recently dominated a range of computer vision\ntasks, yet it suffers from low training data efficiency and inferior local\nsemantic representation capability without appropriate inductive bias.\nConvolutional neural networks (CNNs) inherently capture regional-aware\nsemantics, inspiring researchers to introduce CNNs back into the architecture\nof the ViTs to provide desirable inductive bias for ViTs. However, is the\nlocality achieved by the micro-level CNNs embedded in ViTs good enough? In this\npaper, we investigate the problem by profoundly exploring how the macro\narchitecture of the hybrid CNNs/ViTs enhances the performances of hierarchical\nViTs. Particularly, we study the role of token embedding layers, alias\nconvolutional embedding (CE), and systemically reveal how CE injects desirable\ninductive bias in ViTs. Besides, we apply the optimal CE configuration to 4\nrecently released state-of-the-art ViTs, effectively boosting the corresponding\nperformances. Finally, a family of efficient hybrid CNNs/ViTs, dubbed CETNets,\nare released, which may serve as generic vision backbones. Specifically,\nCETNets achieve 84.9% Top-1 accuracy on ImageNet-1K (training from scratch),\n48.6% box mAP on the COCO benchmark, and 51.6% mIoU on the ADE20K,\nsubstantially improving the performances of the corresponding state-of-the-art\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongmin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Li Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhitong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generator Knows What Discriminator Should Learn in Unconditional GANs. (arXiv:2207.13320v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13320","description":"<p>Recent methods for conditional image generation benefit from dense\nsupervision such as segmentation label maps to achieve high-fidelity. However,\nit is rarely explored to employ dense supervision for unconditional image\ngeneration. Here we explore the efficacy of dense supervision in unconditional\ngeneration and find generator feature maps can be an alternative of\ncost-expensive semantic label maps. From our empirical evidences, we propose a\nnew generator-guided discriminator regularization(GGDR) in which the generator\nfeature maps supervise the discriminator to have rich semantic representations\nin unconditional generation. In specific, we employ an U-Net architecture for\ndiscriminator, which is trained to predict the generator feature maps given\nfake images as inputs. Extensive experiments on mulitple datasets show that our\nGGDR consistently improves the performance of baseline methods in terms of\nquantitative and qualitative aspects. Code is available at\nhttps://github.com/naver-ai/GGDR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gayoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yunjey Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaMarks: Defending Against Deep Learning Model Extraction Using Dynamic Watermarking. (arXiv:2207.13321v1 [cs.CR])","link":"http://arxiv.org/abs/2207.13321","description":"<p>The functionality of a deep learning (DL) model can be stolen via model\nextraction where an attacker obtains a surrogate model by utilizing the\nresponses from a prediction API of the original model. In this work, we propose\na novel watermarking technique called DynaMarks to protect the intellectual\nproperty (IP) of DL models against such model extraction attacks in a black-box\nsetting. Unlike existing approaches, DynaMarks does not alter the training\nprocess of the original model but rather embeds watermark into a surrogate\nmodel by dynamically changing the output responses from the original model\nprediction API based on certain secret parameters at inference runtime. The\nexperimental outcomes on Fashion MNIST, CIFAR-10, and ImageNet datasets\ndemonstrate the efficacy of DynaMarks scheme to watermark surrogate models\nwhile preserving the accuracies of the original models deployed in edge\ndevices. In addition, we also perform experiments to evaluate the robustness of\nDynaMarks against various watermark removal strategies, thus allowing a DL\nmodel owner to reliably prove model ownership.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Abhishek Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_D/0/1/0/all/0/1\">Daniel Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuntao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Ankur Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SiRi: A Simple Selective Retraining Mechanism for Transformer-based Visual Grounding. (arXiv:2207.13325v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13325","description":"<p>In this paper, we investigate how to achieve better visual grounding with\nmodern vision-language transformers, and propose a simple yet powerful\nSelective Retraining (SiRi) mechanism for this challenging task. Particularly,\nSiRi conveys a significant principle to the research of visual grounding, i.e.,\na better initialized vision-language encoder would help the model converge to a\nbetter local minimum, advancing the performance accordingly. In specific, we\ncontinually update the parameters of the encoder as the training goes on, while\nperiodically re-initialize rest of the parameters to compel the model to be\nbetter optimized based on an enhanced encoder. SiRi can significantly\noutperform previous approaches on three popular benchmarks. Specifically, our\nmethod achieves 83.04% Top1 accuracy on RefCOCO+ testA, outperforming the\nstate-of-the-art approaches (training from scratch) by more than 10.21%.\nAdditionally, we reveal that SiRi performs surprisingly superior even with\nlimited training data. We also extend it to transformer-based visual grounding\nmodels and other vision-language tasks to verify the validity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_M/0/1/0/all/0/1\">Mengxue Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Q/0/1/0/all/0/1\">Qiqi Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Attacks in Graph Spectral Domain: When 3D Geometry Meets Graph Signal Processing. (arXiv:2207.13326v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13326","description":"<p>With the increasing attention in various 3D safety-critical applications,\npoint cloud learning models have been shown to be vulnerable to adversarial\nattacks. Although existing 3D attack methods achieve high success rates, they\ndelve into the data space with point-wise perturbation, which may neglect the\ngeometric characteristics. Instead, we propose point cloud attacks from a new\nperspective -- the graph spectral domain attack, aiming to perturb graph\ntransform coefficients in the spectral domain that corresponds to varying\ncertain geometric structure. Specifically, leveraging on graph signal\nprocessing, we first adaptively transform the coordinates of points onto the\nspectral domain via graph Fourier transform (GFT) for compact representation.\nThen, we analyze the influence of different spectral bands on the geometric\nstructure, based on which we propose to perturb the GFT coefficients via a\nlearnable graph spectral filter. Considering the low-frequency components\nmainly contribute to the rough shape of the 3D object, we further introduce a\nlow-frequency constraint to limit perturbations within imperceptible\nhigh-frequency components. Finally, the adversarial point cloud is generated by\ntransforming the perturbed spectral representation back to the data domain via\nthe inverse GFT. Experimental results demonstrate the effectiveness of the\nproposed attack in terms of both the imperceptibility and attack success rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stream UNET Networks for Semantic Segmentation in Medical Images. (arXiv:2207.13337v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13337","description":"<p>Recent advances of semantic image segmentation greatly benefit from deeper\nand larger Convolutional Neural Network (CNN) models. Compared to image\nsegmentation in the wild, properties of both medical images themselves and of\nexisting medical datasets hinder training deeper and larger models because of\noverfitting. To this end, we propose a novel two-stream UNET architecture for\nautomatic end-to-end medical image segmentation, in which intensity value and\ngradient vector flow (GVF) are two inputs for each stream, respectively. We\ndemonstrate that two-stream CNNs with more low-level features greatly benefit\nsemantic segmentation for imperfect medical image datasets. Our proposed\ntwo-stream networks are trained and evaluated on the popular medical image\nsegmentation benchmarks, and the results are competitive with the state of the\nart. The code will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1\">Ke Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ALBench: A Framework for Evaluating Active Learning in Object Detection. (arXiv:2207.13339v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13339","description":"<p>Active learning is an important technology for automated machine learning\nsystems. In contrast to Neural Architecture Search (NAS) which aims at\nautomating neural network architecture design, active learning aims at\nautomating training data selection. It is especially critical for training a\nlong-tailed task, in which positive samples are sparsely distributed. Active\nlearning alleviates the expensive data annotation issue through incrementally\ntraining models powered with efficient data selection. Instead of annotating\nall unlabeled samples, it iteratively selects and annotates the most valuable\nsamples. Active learning has been popular in image classification, but has not\nbeen fully explored in object detection. Most of current approaches on object\ndetection are evaluated with different settings, making it difficult to fairly\ncompare their performance. To facilitate the research in this field, this paper\ncontributes an active learning benchmark framework named as ALBench for\nevaluating active learning in object detection. Developed on an automatic deep\nmodel training system, this ALBench framework is easy-to-use, compatible with\ndifferent active learning algorithms, and ensures the same training and testing\nprotocols. We hope this automated benchmark system help researchers to easily\nreproduce literature's performance and have objective comparisons with prior\narts. The code will be release through Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhanpeng Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takezoe_R/0/1/0/all/0/1\">Rinyoichi Takezoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenze Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li-Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_V/0/1/0/all/0/1\">Vijay K. Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointFix: Learning to Fix Domain Bias for Robust Online Stereo Adaptation. (arXiv:2207.13340v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13340","description":"<p>Online stereo adaptation tackles the domain shift problem, caused by\ndifferent environments between synthetic (training) and real (test) datasets,\nto promptly adapt stereo models in dynamic real-world applications such as\nautonomous driving. However, previous methods often fail to counteract\nparticular regions related to dynamic objects with more severe environmental\nchanges. To mitigate this issue, we propose to incorporate an auxiliary\npoint-selective network into a meta-learning framework, called PointFix, to\nprovide a robust initialization of stereo models for online stereo adaptation.\nIn a nutshell, our auxiliary network learns to fix local variants intensively\nby effectively back-propagating local information through the meta-gradient for\nthe robust initialization of the baseline model. This network is\nmodel-agnostic, so can be used in any kind of architectures in a plug-and-play\nmanner. We conduct extensive experiments to verify the effectiveness of our\nmethod under three adaptation settings such as short-, mid-, and long-term\nsequences. Experimental results show that the proper initialization of the base\nstereo model by the auxiliary network enables our learning paradigm to achieve\nstate-of-the-art performance at inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwonyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jungin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jiyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_D/0/1/0/all/0/1\">Dongbo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1\">Kwanghoon Sohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inverse Airborne Optical Sectioning. (arXiv:2207.13344v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13344","description":"<p>We present Inverse Airborne Optical Sectioning (IAOS) an optical analogy to\nInverse Synthetic Aperture Radar (ISAR). Moving targets, such as walking\npeople, that are heavily occluded by vegetation can be made visible and tracked\nwith a stationary optical sensor (e.g., a hovering camera drone above forest).\nWe introduce the principles of IAOS (i.e., inverse synthetic aperture imaging),\nexplain how the signal of occluders can be further suppressed by filtering the\nRadon transform of the image integral, and present how targets motion\nparameters can be estimated manually and automatically. Finally, we show that\nwhile tracking occluded targets in conventional aerial images is infeasible, it\nbecomes efficiently possible in integral images that result from IAOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nathan_R/0/1/0/all/0/1\">Rakesh John Amala Arokia Nathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_I/0/1/0/all/0/1\">Indrajit Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bimber_O/0/1/0/all/0/1\">Oliver Bimber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Traffic Sign Detection With Event Cameras and DCNN. (arXiv:2207.13345v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13345","description":"<p>In recent years, event cameras (DVS - Dynamic Vision Sensors) have been used\nin vision systems as an alternative or supplement to traditional cameras. They\nare characterised by high dynamic range, high temporal resolution, low latency,\nand reliable performance in limited lighting conditions -- parameters that are\nparticularly important in the context of advanced driver assistance systems\n(ADAS) and self-driving cars. In this work, we test whether these rather novel\nsensors can be applied to the popular task of traffic sign detection. To this\nend, we analyse different representations of the event data: event frame, event\nfrequency, and the exponentially decaying time surface, and apply video frame\nreconstruction using a deep neural network called FireNet. We use the deep\nconvolutional neural network YOLOv4 as a detector. For particular\nrepresentations, we obtain a detection accuracy in the range of 86.9-88.9%\nmAP@0.5. The use of a fusion of the considered representations allows us to\nobtain a detector with higher accuracy of 89.9% mAP@0.5. In comparison, the\ndetector for the frames reconstructed with FireNet is characterised by an\naccuracy of 72.67% mAP@0.5. The results obtained illustrate the potential of\nevent cameras in automotive applications, either as standalone sensors or in\nclose cooperation with typical frame-based cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wzorek_P/0/1/0/all/0/1\">Piotr Wzorek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One-Trimap Video Matting. (arXiv:2207.13353v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13353","description":"<p>Recent studies made great progress in video matting by extending the success\nof trimap-based image matting to the video domain. In this paper, we push this\ntask toward a more practical setting and propose One-Trimap Video Matting\nnetwork (OTVM) that performs video matting robustly using only one\nuser-annotated trimap. A key of OTVM is the joint modeling of trimap\npropagation and alpha prediction. Starting from baseline trimap propagation and\nalpha prediction networks, our OTVM combines the two networks with an\nalpha-trimap refinement module to facilitate information flow. We also present\nan end-to-end training strategy to take full advantage of the joint model. Our\njoint modeling greatly improves the temporal stability of trimap propagation\ncompared to the previous decoupled methods. We evaluate our model on two latest\nvideo matting benchmarks, Deep Video Matting and VideoMatting108, and\noutperform state-of-the-art by significant margins (MSE improvements of 56.4%\nand 56.7%, respectively). The source code and model are available online:\nhttps://github.com/Hongje/OTVM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seong_H/0/1/0/all/0/1\">Hongje Seong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seoung Wug Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Euntai Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Appearance-motion Normality for Video Anomaly Detection. (arXiv:2207.13361v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13361","description":"<p>Video anomaly detection is a challenging task in the computer vision\ncommunity. Most single task-based methods do not consider the independence of\nunique spatial and temporal patterns, while two-stream structures lack the\nexploration of the correlations. In this paper, we propose spatial-temporal\nmemories augmented two-stream auto-encoder framework, which learns the\nappearance normality and motion normality independently and explores the\ncorrelations via adversarial learning. Specifically, we first design two proxy\ntasks to train the two-stream structure to extract appearance and motion\nfeatures in isolation. Then, the prototypical features are recorded in the\ncorresponding spatial and temporal memory pools. Finally, the encoding-decoding\nnetwork performs adversarial learning with the discriminator to explore the\ncorrelations between spatial and temporal patterns. Experimental results show\nthat our framework outperforms the state-of-the-art methods, achieving AUCs of\n98.1% and 89.8% on UCSD Ped2 and CUHK Avenue datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaoguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Liang Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camouflaged Object Detection via Context-aware Cross-level Fusion. (arXiv:2207.13362v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13362","description":"<p>Camouflaged object detection (COD) aims to identify the objects that conceal\nthemselves in natural scenes. Accurate COD suffers from a number of challenges\nassociated with low boundary contrast and the large variation of object\nappearances, e.g., object size and shape. To address these challenges, we\npropose a novel Context-aware Cross-level Fusion Network (C2F-Net), which fuses\ncontext-aware cross-level features for accurately identifying camouflaged\nobjects. Specifically, we compute informative attention coefficients from\nmulti-level features with our Attention-induced Cross-level Fusion Module\n(ACFM), which further integrates the features under the guidance of attention\ncoefficients. We then propose a Dual-branch Global Context Module (DGCM) to\nrefine the fused features for informative feature representations by exploiting\nrich global context information. Multiple ACFMs and DGCMs are integrated in a\ncascaded manner for generating a coarse prediction from high-level features.\nThe coarse prediction acts as an attention map to refine the low-level features\nbefore passing them to our Camouflage Inference Module (CIM) to generate the\nfinal prediction. We perform extensive experiments on three widely used\nbenchmark datasets and compare C2F-Net with state-of-the-art (SOTA) models. The\nresults show that C2F-Net is an effective COD model and outperforms SOTA models\nremarkably. Further, an evaluation on polyp segmentation datasets demonstrates\nthe promising potentials of our C2F-Net in COD downstream applications. Our\ncode is publicly available at: https://github.com/Ben57882/C2FNet-TSCVT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si-Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu-Jia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1\">Ge-Peng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ya-Feng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Clustering with Features from Self-Supervised Pretraining. (arXiv:2207.13364v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13364","description":"<p>A deep clustering model conceptually consists of a feature extractor that\nmaps data points to a latent space, and a clustering head that groups data\npoints into clusters in the latent space. Although the two components used to\nbe trained jointly in an end-to-end fashion, recent works have proved it\nbeneficial to train them separately in two stages. In the first stage, the\nfeature extractor is trained via self-supervised learning, which enables the\npreservation of the cluster structures among the data points. To preserve the\ncluster structures even better, we propose to replace the first stage with\nanother model that is pretrained on a much larger dataset via self-supervised\nlearning. The method is simple and might suffer from domain shift. Nonetheless,\nwe have empirically shown that it can achieve superior clustering performance.\nWhen a vision transformer (ViT) architecture is used for feature extraction,\nour method has achieved clustering accuracy 94.0%, 55.6% and 97.9% on CIFAR-10,\nCIFAR-100 and STL-10 respectively. The corresponding previous state-of-the-art\nresults are 84.3%, 47.7% and 80.8%. Our code will be available online with the\npublication of the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingzhi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Nevin L. Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing transformations for contrastive learning in a differentiable framework. (arXiv:2207.13367v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13367","description":"<p>Current contrastive learning methods use random transformations sampled from\na large list of transformations, with fixed hyperparameters, to learn\ninvariance from an unannotated database. Following previous works that\nintroduce a small amount of supervision, we propose a framework to find optimal\ntransformations for contrastive learning using a differentiable transformation\nnetwork. Our method increases performances at low annotated data regime both in\nsupervision accuracy and in convergence speed. In contrast to previous work, no\ngenerative model is needed for transformation optimization. Transformed images\nkeep relevant information to solve the supervised task, here classification.\nExperiments were performed on 34000 2D slices of brain Magnetic Resonance\nImages and 11200 chest X-ray images. On both datasets, with 10% of labeled\ndata, our model achieves better performances than a fully supervised model with\n100% labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruppli_C/0/1/0/all/0/1\">Camille Ruppli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardon_R/0/1/0/all/0/1\">Roberto Ardon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Video Deblurring Guided by Motion Magnitude. (arXiv:2207.13374v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13374","description":"<p>Video deblurring is a highly under-constrained problem due to the spatially\nand temporally varying blur. An intuitive approach for video deblurring\nincludes two steps: a) detecting the blurry region in the current frame; b)\nutilizing the information from clear regions in adjacent frames for current\nframe deblurring. To realize this process, our idea is to detect the pixel-wise\nblur level of each frame and combine it with video deblurring. To this end, we\npropose a novel framework that utilizes the motion magnitude prior (MMP) as\nguidance for efficient deep video deblurring. Specifically, as the pixel\nmovement along its trajectory during the exposure time is positively correlated\nto the level of motion blur, we first use the average magnitude of optical flow\nfrom the high-frequency sharp frames to generate the synthetic blurry frames\nand their corresponding pixel-wise motion magnitude maps. We then build a\ndataset including the blurry frame and MMP pairs. The MMP is then learned by a\ncompact CNN by regression. The MMP consists of both spatial and temporal blur\nlevel information, which can be further integrated into an efficient recurrent\nneural network (RNN) for video deblurring. We conduct intensive experiments to\nvalidate the effectiveness of the proposed methods on the public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yusheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yunfan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ye Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamashita_A/0/1/0/all/0/1\">Atsushi Yamashita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Hard Noise in Long-Tailed Sample Distribution. (arXiv:2207.13378v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13378","description":"<p>Conventional de-noising methods rely on the assumption that all samples are\nindependent and identically distributed, so the resultant classifier, though\ndisturbed by noise, can still easily identify the noises as the outliers of\ntraining distribution. However, the assumption is unrealistic in large-scale\ndata that is inevitably long-tailed. Such imbalanced training data makes a\nclassifier less discriminative for the tail classes, whose previously \"easy\"\nnoises are now turned into \"hard\" ones -- they are almost as outliers as the\nclean tail samples. We introduce this new challenge as Noisy Long-Tailed\nClassification (NLT). Not surprisingly, we find that most de-noising methods\nfail to identify the hard noises, resulting in significant performance drop on\nthe three proposed NLT benchmarks: ImageNet-NLT, Animal10-NLT, and Food101-NLT.\nTo this end, we design an iterative noisy learning framework called\nHard-to-Easy (H2E). Our bootstrapping philosophy is to first learn a classifier\nas noise identifier invariant to the class and context distributional changes,\nreducing \"hard\" noises to \"easy\" ones, whose removal further improves the\ninvariance. Experimental results show that our H2E outperforms state-of-the-art\nde-noising methods and their ablations on long-tailed settings while\nmaintaining a stable performance on the conventional balanced settings.\nDatasets and codes are available at https://github.com/yxymessi/H2E-Framework\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1\">Xuanyu Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Kaihua Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joo-Hwee Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Closer to Your Enemy: Learning to Attack via Teacher-student Mimicking. (arXiv:2207.13381v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13381","description":"<p>This paper aims to generate realistic attack samples of person\nre-identification, ReID, by reading the enemy's mind (VM). In this paper, we\npropose a novel inconspicuous and controllable ReID attack baseline, LCYE, to\ngenerate adversarial query images. Concretely, LCYE first distills VM's\nknowledge via teacher-student memory mimicking in the proxy task. Then this\nknowledge prior acts as an explicit cipher conveying what is essential and\nrealistic, believed by VM, for accurate adversarial misleading. Besides,\nbenefiting from the multiple opposing task framework of LCYE, we further\ninvestigate the interpretability and generalization of ReID models from the\nview of the adversarial attack, including cross-domain adaption, cross-model\nconsensus, and online learning process. Extensive experiments on four ReID\nbenchmarks show that our method outperforms other state-of-the-art attackers\nwith a large margin in white-box, black-box, and target attacks. Our code is\nnow available at https://gitfront.io/r/user-3704489/mKXusqDT4ffr/LCYE/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingejie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiqing Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sirui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_D/0/1/0/all/0/1\">Dingwen Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Statistical Keystroke Synthesis for Improved Bot Detection. (arXiv:2207.13394v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13394","description":"<p>This work proposes two statistical approaches for the synthesis of keystroke\nbiometric data based on Universal and User-dependent Models. Both approaches\nare validated on the bot detection task, using the keystroke synthetic data to\nbetter train the systems. Our experiments include a dataset with 136 million\nkeystroke events from 168,000 subjects. We have analyzed the performance of the\ntwo synthesis approaches through qualitative and quantitative experiments.\nDifferent bot detectors are considered based on two supervised classifiers\n(Support Vector Machine and Long Short-Term Memory network) and a learning\nframework including human and generated samples. Our results prove that the\nproposed statistical approaches are able to generate realistic human-like\nsynthetic keystroke samples. Also, the classification results suggest that in\nscenarios with large labeled data, these synthetic samples can be detected with\nhigh accuracy. However, in few-shot learning scenarios it represents an\nimportant challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DeAlcala_D/0/1/0/all/0/1\">Daniel DeAlcala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1\">Aythami Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1\">Ruben Tolosana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acien_A/0/1/0/all/0/1\">Alejandro Acien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_S/0/1/0/all/0/1\">Santiago Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_M/0/1/0/all/0/1\">Miguel A. Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Moises Diaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-Train Adaptive MobileNet for Fast Anti-Spoofing. (arXiv:2207.13410v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13410","description":"<p>Many applications require high accuracy of neural networks as well as low\nlatency and user data privacy guaranty. Face anti-spoofing is one of such\ntasks. However, a single model might not give the best results for different\ndevice performance categories, while training multiple models is time\nconsuming. In this work we present Post-Train Adaptive (PTA) block. Such a\nblock is simple in structure and offers a drop-in replacement for the\nMobileNetV2 Inverted Residual block. The PTA block has multiple branches with\ndifferent computation costs. The branch to execute can be selected on-demand\nand at runtime; thus, offering different inference times and configuration\ncapability for multiple device tiers. Crucially, the model is trained once and\ncan be easily reconfigured after training, even directly on a mobile device. In\naddition, the proposed approach shows substantially better overall performance\nin comparison to the original MobileNetV2 as tested on CelebA-Spoof dataset.\nDifferent PTA block configurations are sampled at training time, which also\ndecreases overall wall-clock time needed to train the model. While we present\ncomputational results for the anti-spoofing problem, the MobileNetV2 with PTA\nblocks is applicable to any problem solvable with convolutional neural\nnetworks, which makes the results presented practically significant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khabarlak_K/0/1/0/all/0/1\">Kostiantyn Khabarlak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransNorm: Transformer Provides a Strong Spatial Normalization Mechanism for a Deep Segmentation Model. (arXiv:2207.13415v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13415","description":"<p>In the past few years, convolutional neural networks (CNNs), particularly\nU-Net, have been the prevailing technique in the medical image processing era.\nSpecifically, the seminal U-Net, as well as its alternatives, have successfully\nmanaged to address a wide variety of medical image segmentation tasks. However,\nthese architectures are intrinsically imperfect as they fail to exhibit\nlong-range interactions and spatial dependencies leading to a severe\nperformance drop in the segmentation of medical images with variable shapes and\nstructures. Transformers, preliminary proposed for sequence-to-sequence\nprediction, have arisen as surrogate architectures to precisely model global\ninformation assisted by the self-attention mechanism. Despite being feasibly\ndesigned, utilizing a pure Transformer for image segmentation purposes can\nresult in limited localization capacity stemming from inadequate low-level\nfeatures. Thus, a line of research strives to design robust variants of\nTransformer-based U-Net. In this paper, we propose Trans-Norm, a novel deep\nsegmentation framework which concomitantly consolidates a Transformer module\ninto both encoder and skip-connections of the standard U-Net. We argue that the\nexpedient design of skip-connections can be crucial for accurate segmentation\nas it can assist in feature fusion between the expanding and contracting paths.\nIn this respect, we derive a Spatial Normalization mechanism from the\nTransformer module to adaptively recalibrate the skip connection path.\nExtensive experiments across three typical tasks for medical image segmentation\ndemonstrate the effectiveness of TransNorm. The codes and trained models are\npublicly available at https://github.com/rezazad68/transnorm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1\">Reza Azad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AL_Antary_M/0/1/0/all/0/1\">Mohammad T. AL-Antary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidari_M/0/1/0/all/0/1\">Moein Heidari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1\">Dorit Merhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hardly Perceptible Trojan Attack against Neural Networks with Bit Flips. (arXiv:2207.13417v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13417","description":"<p>The security of deep neural networks (DNNs) has attracted increasing\nattention due to their widespread use in various applications. Recently, the\ndeployed DNNs have been demonstrated to be vulnerable to Trojan attacks, which\nmanipulate model parameters with bit flips to inject a hidden behavior and\nactivate it by a specific trigger pattern. However, all existing Trojan attacks\nadopt noticeable patch-based triggers (e.g., a square pattern), making them\nperceptible to humans and easy to be spotted by machines. In this paper, we\npresent a novel attack, namely hardly perceptible Trojan attack (HPT). HPT\ncrafts hardly perceptible Trojan images by utilizing the additive noise and per\npixel flow field to tweak the pixel values and positions of the original\nimages, respectively. To achieve superior attack performance, we propose to\njointly optimize bit flips, additive noise, and flow field. Since the weight\nbits of the DNNs are binary, this problem is very hard to be solved. We handle\nthe binary constraint with equivalent replacement and provide an effective\noptimization algorithm. Extensive experiments on CIFAR-10, SVHN, and ImageNet\ndatasets show that the proposed HPT can generate hardly perceptible Trojan\nimages, while achieving comparable or better attack performance compared to the\nstate-of-the-art methods. The code is available at:\nhttps://github.com/jiawangbai/HPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiawang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1\">Kuofeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Efficacy of Softmax for Lightweight Non-Local Neural Networks. (arXiv:2207.13423v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13423","description":"<p>Non-local (NL) block is a popular module that demonstrates the capability to\nmodel global contexts. However, NL block generally has heavy computation and\nmemory costs, so it is impractical to apply the block to high-resolution\nfeature maps. In this paper, to investigate the efficacy of NL block, we\nempirically analyze if the magnitude and direction of input feature vectors\nproperly affect the attention between vectors. The results show the inefficacy\nof softmax operation which is generally used to normalize the attention map of\nthe NL block. Attention maps normalized with softmax operation highly rely upon\nmagnitude of key vectors, and performance is degenerated if the magnitude\ninformation is removed. By replacing softmax operation with the scaling factor,\nwe demonstrate improved performance on CIFAR-10, CIFAR-100, and Tiny-ImageNet.\nIn Addition, our method shows robustness to embedding channel reduction and\nembedding weight initialization. Notably, our method makes multi-head attention\nemployable without additional computational cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yooshin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hanbyel Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Jaesung Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Hyeong Gwon Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Pix2Vox++ for 3D Cardiac Reconstruction from 2D echo views. (arXiv:2207.13424v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13424","description":"<p>Accurate geometric quantification of the human heart is a key step in the\ndiagnosis of numerous cardiac diseases, and in the management of cardiac\npatients. Ultrasound imaging is the primary modality for cardiac imaging,\nhowever acquisition requires high operator skill, and its interpretation and\nanalysis is difficult due to artifacts. Reconstructing cardiac anatomy in 3D\ncan enable discovery of new biomarkers and make imaging less dependent on\noperator expertise, however most ultrasound systems only have 2D imaging\ncapabilities. We propose both a simple alteration to the Pix2Vox++ networks for\na sizeable reduction in memory usage and computational complexity, and a\npipeline to perform reconstruction of 3D anatomy from 2D standard cardiac\nviews, effectively enabling 3D anatomical reconstruction from limited 2D data.\nWe evaluate our pipeline using synthetically generated data achieving accurate\n3D whole-heart reconstructions (peak intersection over union score &gt; 0.88) from\njust two standard anatomical 2D views of the heart. We also show preliminary\nresults using real echo images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Stojanovski_D/0/1/0/all/0/1\">David Stojanovski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hermida_U/0/1/0/all/0/1\">Uxio Hermida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muffoletto_M/0/1/0/all/0/1\">Marica Muffoletto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lamata_P/0/1/0/all/0/1\">Pablo Lamata</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gomez_A/0/1/0/all/0/1\">Alberto Gomez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging GAN Priors for Few-Shot Part Segmentation. (arXiv:2207.13428v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13428","description":"<p>Few-shot part segmentation aims to separate different parts of an object\ngiven only a few annotated samples. Due to the challenge of limited data,\nexisting works mainly focus on learning classifiers over pre-trained features,\nfailing to learn task-specific features for part segmentation. In this paper,\nwe propose to learn task-specific features in a \"pre-training\"-\"fine-tuning\"\nparadigm. We conduct prompt designing to reduce the gap between the pre-train\ntask (i.e., image generation) and the downstream task (i.e., part\nsegmentation), so that the GAN priors for generation can be leveraged for\nsegmentation. This is achieved by projecting part segmentation maps into the\nRGB space and conducting interpolation between RGB segmentation maps and\noriginal images. Specifically, we design a fine-tuning strategy to\nprogressively tune an image generator into a segmentation generator, where the\nsupervision of the generator varying from images to segmentation maps by\ninterpolation. Moreover, we propose a two-stream architecture, i.e., a\nsegmentation stream to generate task-specific features, and an image stream to\nprovide spatial constraints. The image stream can be regarded as a\nself-supervised auto-encoder, and this enables our model to benefit from\nlarge-scale support images. Overall, this work is an attempt to explore the\ninternal relevance between generation tasks and perception tasks by prompt\ndesigning. Extensive experiments show that our model can achieve\nstate-of-the-art performance on several part segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Mengya Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Heliang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Concept Drift Challenge in Multimedia Anomaly Detection: A Case Study with Facial Datasets. (arXiv:2207.13430v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13430","description":"<p>Anomaly detection in multimedia datasets is a widely studied area. Yet, the\nconcept drift challenge in data has been ignored or poorly handled by the\nmajority of the anomaly detection frameworks. The state-of-the-art approaches\nassume that the data distribution at training and deployment time will be the\nsame. However, due to various real-life environmental factors, the data may\nencounter drift in its distribution or can drift from one class to another in\nthe late future. Thus, a one-time trained model might not perform adequately.\nIn this paper, we systematically investigate the effect of concept drift on\nvarious detection models and propose a modified Adaptive Gaussian Mixture Model\n(AGMM) based framework for anomaly detection in multimedia data. In contrast to\nthe baseline AGMM, the proposed extension of AGMM remembers the past for a\nlonger period in order to handle the drift better. Extensive experimental\nanalysis shows that the proposed model better handles the drift in data as\ncompared with the baseline AGMM. Further, to facilitate research and comparison\nwith the proposed framework, we contribute three multimedia datasets\nconstituting faces as samples. The face samples of individuals correspond to\nthe age difference of more than ten years to incorporate a longer temporal\ncontext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumari_P/0/1/0/all/0/1\">Pratibha Kumari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_P/0/1/0/all/0/1\">Priyankar Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atrey_P/0/1/0/all/0/1\">Pradeep K. Atrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saini_M/0/1/0/all/0/1\">Mukesh Saini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-To-End Audiovisual Feature Fusion for Active Speaker Detection. (arXiv:2207.13434v1 [cs.SD])","link":"http://arxiv.org/abs/2207.13434","description":"<p>Active speaker detection plays a vital role in human-machine interaction.\nRecently, a few end-to-end audiovisual frameworks emerged. However, these\nmodels' inference time was not explored and are not applicable for real-time\napplications due to their complexity and large input size. In addition, they\nexplored a similar feature extraction strategy that employs the ConvNet on\naudio and visual inputs. This work presents a novel two-stream end-to-end\nframework fusing features extracted from images via VGG-M with raw Mel\nFrequency Cepstrum Coefficients features extracted from the audio waveform. The\nnetwork has two BiGRU layers attached to each stream to handle each stream's\ntemporal dynamic before fusion. After fusion, one BiGRU layer is attached to\nmodel the joint temporal dynamics. The experiment result on the\nAVA-ActiveSpeaker dataset indicates that our new feature extraction strategy\nshows more robustness to noisy signals and better inference time than models\nthat employed ConvNet on both modalities. The proposed model predicts within\n44.41 ms, which is fast enough for real-time applications. Our best-performing\nmodel attained 88.929% accuracy, nearly the same detection result as\nstate-of-the-art -work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tesema_F/0/1/0/all/0/1\">Fiseha B. Tesema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zheyuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shiqiang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jason Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Scene Graph Generation. (arXiv:2207.13440v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13440","description":"<p>The task of scene graph generation entails identifying object entities and\ntheir corresponding interaction predicates in a given image (or video). Due to\nthe combinatorially large solution space, existing approaches to scene graph\ngeneration assume certain factorization of the joint distribution to make the\nestimation feasible (e.g., assuming that objects are conditionally independent\nof predicate predictions). However, this fixed factorization is not ideal under\nall scenarios (e.g., for images where an object entailed in interaction is\nsmall and not discernible on its own). In this work, we propose a novel\nframework for scene graph generation that addresses this limitation, as well as\nintroduces dynamic conditioning on the image, using message passing in a Markov\nRandom Field. This is implemented as an iterative refinement procedure wherein\neach modification is conditioned on the graph generated in the previous\niteration. This conditioning across refinement steps allows joint reasoning\nover entities and relations. This framework is realized via a novel and\nend-to-end trainable transformer-based architecture. In addition, the proposed\nframework can improve existing approach performance. Through extensive\nexperiments on Visual Genome and Action Genome benchmark datasets we show\nimproved performance on the scene graph generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_S/0/1/0/all/0/1\">Siddhesh Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skimming, Locating, then Perusing: A Human-Like Framework for Natural Language Video Localization. (arXiv:2207.13450v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13450","description":"<p>This paper addresses the problem of natural language video localization\n(NLVL). Almost all existing works follow the \"only look once\" framework that\nexploits a single model to directly capture the complex cross- and self-modal\nrelations among video-query pairs and retrieve the relevant segment. However,\nwe argue that these methods have overlooked two indispensable characteristics\nof an ideal localization method: 1) Frame-differentiable: considering the\nimbalance of positive/negative video frames, it is effective to highlight\npositive frames and weaken negative ones during the localization. 2)\nBoundary-precise: to predict the exact segment boundary, the model should\ncapture more fine-grained differences between consecutive frames since their\nvariations are often smooth. To this end, inspired by how humans perceive and\nlocalize a segment, we propose a two-step human-like framework called\nSkimming-Locating-Perusing (SLP). SLP consists of a Skimming-and-Locating (SL)\nmodule and a Bi-directional Perusing (BP) module. The SL module first refers to\nthe query semantic and selects the best matched frame from the video while\nfiltering out irrelevant frames. Then, the BP module constructs an initial\nsegment based on this frame, and dynamically updates it by exploring its\nadjacent frames until no frame shares the same activity semantic. Experimental\nresults on three challenging benchmarks show that our SLP is superior to the\nstate-of-the-art methods and localizes more precise segment boundaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing the Vision and Language Bias for Temporal Sentence Grounding. (arXiv:2207.13457v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13457","description":"<p>Temporal sentence grounding (TSG) is an important yet challenging task in\nmultimedia information retrieval. Although previous TSG methods have achieved\ndecent performance, they tend to capture the selection biases of frequently\nappeared video-query pairs in the dataset rather than present robust multimodal\nreasoning abilities, especially for the rarely appeared pairs. In this paper,\nwe study the above issue of selection biases and accordingly propose a\nDebiasing-TSG (D-TSG) model to filter and remove the negative biases in both\nvision and language modalities for enhancing the model generalization ability.\nSpecifically, we propose to alleviate the issue from two perspectives: 1)\nFeature distillation. We built a multi-modal debiasing branch to firstly\ncapture the vision and language biases, and then apply a bias identification\nmodule to explicitly recognize the true negative biases and remove them from\nthe benign multi-modal representations. 2) Contrastive sample generation. We\nconstruct two types of negative samples to enforce the model to accurately\nlearn the aligned multi-modal semantics and make complete semantic reasoning.\nWe apply the proposed model to both commonly and rarely appeared TSG cases, and\ndemonstrate its effectiveness by achieving the state-of-the-art performance on\nthree benchmark datasets (ActivityNet Caption, TACoS, and Charades-STA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VICTOR: Visual Incompatibility Detection with Transformers and Fashion-specific contrastive pre-training. (arXiv:2207.13458v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13458","description":"<p>In order to consider fashion outfits as aesthetically pleasing, the garments\nthat constitute them need to be compatible in terms of visual aspects, such as\nstyle, category and color. With the advent and omnipresence of computer vision\ndeep learning models, increased interest has also emerged for the task of\nvisual compatibility detection with the aim to develop quality fashion outfit\nrecommendation systems. Previous works have defined visual compatibility as a\nbinary classification task with items in a garment being considered as fully\ncompatible or fully incompatible. However, this is not applicable to Outfit\nMaker applications where users create their own outfits and need to know which\nspecific items may be incompatible with the rest of the outfit. To address\nthis, we propose the Visual InCompatibility TransfORmer (VICTOR) that is\noptimized for two tasks: 1) overall compatibility as regression and 2) the\ndetection of mismatching items. Unlike previous works that either rely on\nfeature extraction from ImageNet-pretrained models or by end-to-end fine\ntuning, we utilize fashion-specific contrastive language-image pre-training for\nfine tuning computer vision neural networks on fashion imagery. Moreover, we\nbuild upon the Polyvore outfit benchmark to generate partially mismatching\noutfits, creating a new dataset termed Polyvore-MISFITs, that is used to train\nVICTOR. A series of ablation and comparative analyses show that the proposed\narchitecture can compete and even surpass the current state-of-the-art on\nPolyvore datasets while reducing the instance-wise floating operations by 88%,\nstriking a balance between high performance and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Stefanos-Iordanis Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1\">Christos Koutlis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive sampling for scanning pixel cameras. (arXiv:2207.13460v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13460","description":"<p>A scanning pixel camera is a novel low-cost, low-power sensor that is not\ndiffraction limited. It produces data as a sequence of samples extracted from\nvarious parts of the scene during the course of a scan. It can provide very\ndetailed images at the expense of samplerates and slow image acquisition time.\nThis paper proposes a new algorithm which allows the sensor to adapt the\nsamplerate over the course of this sequence. This makes it possible to overcome\nsome of these limitations by minimising the bandwidth and time required to\nimage and transmit a scene, while maintaining image quality. We examine\napplications to image classification and semantic segmentation and are able to\nachieve similar results compared to a fully sampled input, while using 80%\nfewer samples\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duman_Y/0/1/0/all/0/1\">Yusuf Duman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guillemaut_J/0/1/0/all/0/1\">Jean-Yves Guillemaut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1\">Simon Hadfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards the Probabilistic Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction. (arXiv:2207.13464v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13464","description":"<p>The best way to combine the results of deep learning with standard 3D\nreconstruction pipelines remains an open problem. While systems that pass the\noutput of traditional multi-view stereo approaches to a network for\nregularisation or refinement currently seem to get the best results, it may be\npreferable to treat deep neural networks as separate components whose results\ncan be probabilistically fused into geometry-based systems. Unfortunately, the\nerror models required to do this type of fusion are not well understood, with\nmany different approaches being put forward. Recently, a few systems have\nachieved good results by having their networks predict probability\ndistributions rather than single values. We propose using this approach to fuse\na learned single-view depth prior into a standard 3D reconstruction system.\n</p>\n<p>Our system is capable of incrementally producing dense depth maps for a set\nof keyframes. We train a deep neural network to predict discrete, nonparametric\nprobability distributions for the depth of each pixel from a single image. We\nthen fuse this \"probability volume\" with another probability volume based on\nthe photometric consistency between subsequent frames and the keyframe image.\nWe argue that combining the probability volumes from these two sources will\nresult in a volume that is better conditioned. To extract depth maps from the\nvolume, we minimise a cost function that includes a regularisation term based\non network predicted surface normals and occlusion boundaries. Through a series\nof experiments, we demonstrate that each of these components improves the\noverall performance of the system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Laidlow_T/0/1/0/all/0/1\">Tristan Laidlow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Czarnowski_J/0/1/0/all/0/1\">Jan Czarnowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicastro_A/0/1/0/all/0/1\">Andrea Nicastro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_R/0/1/0/all/0/1\">Ronald Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leutenegger_S/0/1/0/all/0/1\">Stefan Leutenegger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PASTA-GAN++: A Versatile Framework for High-Resolution Unpaired Virtual Try-on. (arXiv:2207.13475v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13475","description":"<p>Image-based virtual try-on is one of the most promising applications of\nhuman-centric image generation due to its tremendous real-world potential. In\nthis work, we take a step forwards to explore versatile virtual try-on\nsolutions, which we argue should possess three main properties, namely, they\nshould support unsupervised training, arbitrary garment categories, and\ncontrollable garment editing. To this end, we propose a\ncharacteristic-preserving end-to-end network, the PAtch-routed\nSpaTially-Adaptive GAN++ (PASTA-GAN++), to achieve a versatile system for\nhigh-resolution unpaired virtual try-on. Specifically, our PASTA-GAN++ consists\nof an innovative patch-routed disentanglement module to decouple the intact\ngarment into normalized patches, which is capable of retaining garment style\ninformation while eliminating the garment spatial information, thus alleviating\nthe overfitting issue during unsupervised training. Furthermore, PASTA-GAN++\nintroduces a patch-based garment representation and a patch-guided parsing\nsynthesis block, allowing it to handle arbitrary garment categories and support\nlocal garment editing. Finally, to obtain try-on results with realistic texture\ndetails, PASTA-GAN++ incorporates a novel spatially-adaptive residual module to\ninject the coarse warped garment feature into the generator. Extensive\nexperiments on our newly collected UnPaired virtual Try-on (UPT) dataset\ndemonstrate the superiority of PASTA-GAN++ over existing SOTAs and its ability\nfor controllable garment editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zaiyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoye Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Feida Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTransition: Learning to Recommend Video Transition Effects. (arXiv:2207.13479v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13479","description":"<p>Video transition effects are widely used in video editing to connect shots\nfor creating cohesive and visually appealing videos. However, it is challenging\nfor non-professionals to choose best transitions due to the lack of\ncinematographic knowledge and design skills. In this paper, we present the\npremier work on performing automatic video transitions recommendation (VTR):\ngiven a sequence of raw video shots and companion audio, recommend video\ntransitions for each pair of neighboring shots. To solve this task, we collect\na large-scale video transition dataset using publicly available video templates\non editing softwares. Then we formulate VTR as a multi-modal retrieval problem\nfrom vision/audio to video transitions and propose a novel multi-modal matching\nframework which consists of two parts. First we learn the embedding of video\ntransitions through a video transition classification task. Then we propose a\nmodel to learn the matching correspondence from vision/audio inputs to video\ntransitions. Specifically, the proposed model employs a multi-modal transformer\nto fuse vision and audio information, as well as capture the context cues in\nsequential transition outputs. Through both quantitative and qualitative\nexperiments, we clearly demonstrate the effectiveness of our method. Notably,\nin the comprehensive user study, our method receives comparable scores compared\nwith professional editors while improving the video editing efficiency by\n\\textbf{300\\scalebox{1.25}{$\\times$}}. We hope our work serves to inspire other\nresearchers to work on this new task. The dataset and codes are public at\n\\url{https://github.com/acherstyx/AutoTransition}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yaojie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaojie Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time to augment contrastive learning. (arXiv:2207.13492v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13492","description":"<p>Biological vision systems are unparalleled in their ability to learn visual\nrepresentations without supervision. In machine learning, contrastive learning\n(CL) has led to major advances in forming object representations in an\nunsupervised fashion. These systems learn representations invariant to\naugmentation operations over images, like cropping or flipping. In contrast,\nbiological vision systems exploit the temporal structure of the visual\nexperience. This gives access to augmentations not commonly used in CL, like\nwatching the same object from multiple viewpoints or against different\nbackgrounds. Here, we systematically investigate and compare the potential\nbenefits of such time-based augmentations for learning object categories. Our\nresults show that time-based augmentations achieve large performance gains over\nstate-of-the-art image augmentations. Specifically, our analyses reveal that:\n1) 3-D object rotations drastically improve the learning of object categories;\n2) viewing objects against changing backgrounds is vital for learning to\ndiscard background-related information. Overall, we conclude that time-based\naugmentations can greatly improve contrastive learning, narrowing the gap\nbetween artificial and biological vision systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aubret_A/0/1/0/all/0/1\">Arthur Aubret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_M/0/1/0/all/0/1\">Markus Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teuliere_C/0/1/0/all/0/1\">C&#xe9;line Teuli&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1\">Jochen Triesch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable multi-task, multi-domain deep segmentation of sparse pediatric imaging datasets via multi-scale contrastive regularization and multi-joint anatomical priors. (arXiv:2207.13502v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13502","description":"<p>Clinical diagnosis of the pediatric musculoskeletal system relies on the\nanalysis of medical imaging examinations. In the medical image processing\npipeline, semantic segmentation using deep learning algorithms enables an\nautomatic generation of patient-specific three-dimensional anatomical models\nwhich are crucial for morphological evaluation. However, the scarcity of\npediatric imaging resources may result in reduced accuracy and generalization\nperformance of individual deep segmentation models. In this study, we propose\nto design a novel multi-task, multi-domain learning framework in which a single\nsegmentation network is optimized over the union of multiple datasets arising\nfrom distinct parts of the anatomy. Unlike previous approaches, we\nsimultaneously consider multiple intensity domains and segmentation tasks to\novercome the inherent scarcity of pediatric data while leveraging shared\nfeatures between imaging datasets. To further improve generalization\ncapabilities, we employ a transfer learning scheme from natural image\nclassification, along with a multi-scale contrastive regularization aimed at\npromoting domain-specific clusters in the shared representations, and\nmulti-joint anatomical priors to enforce anatomically consistent predictions.\nWe evaluate our contributions for performing bone segmentation using three\nscarce and pediatric imaging datasets of the ankle, knee, and shoulder joints.\nOur results demonstrate that the proposed approach outperforms individual,\ntransfer, and shared segmentation schemes in Dice metric with statistically\nsufficient margins. The proposed model brings new perspectives towards\nintelligent use of imaging resources and better management of pediatric\nmusculoskeletal disorders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boutillon_A/0/1/0/all/0/1\">Arnaud Boutillon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Conze_P/0/1/0/all/0/1\">Pierre-Henri Conze</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pons_C/0/1/0/all/0/1\">Christelle Pons</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burdin_V/0/1/0/all/0/1\">Val&#xe9;rie Burdin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borotikar_B/0/1/0/all/0/1\">Bhushan Borotikar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Forgery Detection Challenge 2022: Push the Frontier of Unconstrained and Diverse Forgery Detection. (arXiv:2207.13505v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13505","description":"<p>In this paper, we present the Multi-Forgery Detection Challenge held\nconcurrently with the IEEE Computer Society Workshop on Biometrics at CVPR\n2022. Our Multi-Forgery Detection Challenge aims to detect automatic image\nmanipulations including but not limited to image editing, image synthesis,\nimage generation, image photoshop, etc. Our challenge has attracted 674 teams\nfrom all over the world, with about 2000 valid result submission counts. We\ninvited the Top 10 teams to present their solutions to the challenge, from\nwhich three teams are awarded prizes in the grand finale. In this paper, we\npresent the solutions from the Top 3 teams, in order to boost the research work\nin the field of image forgery detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianshu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kewei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_X/0/1/0/all/0/1\">Xuning Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Boyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Ying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_Y/0/1/0/all/0/1\">Yingying Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengfei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Satellite Image Based Cross-view Localization for Autonomous Vehicle. (arXiv:2207.13506v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13506","description":"<p>Existing spatial localization techniques for autonomous vehicles mostly use a\npre-built 3D-HD map, often constructed using a survey-grade 3D mapping vehicle,\nwhich is not only expensive but also laborious. This paper shows that by using\nan off-the-shelf high-definition satellite image as a ready-to-use map, we are\nable to achieve cross-view vehicle localization up to a satisfactory accuracy,\nproviding a cheaper and more practical way for localization. Although the idea\nof using satellite images for cross-view localization is not new, previous\nmethods almost exclusively treat the task as image retrieval, namely matching a\nvehicle-captured ground-view image with the satellite image. This paper\npresents a novel cross-view localization method, which departs from the common\nwisdom of image retrieval. Specifically, our method develops (1) a\nGeometric-align Feature Extractor (GaFE) that leverages measured 3D points to\nbridge the geometric gap between ground view and overhead view, (2) a Pose\nAware Branch (PAB) adopting a triplet loss to encourage pose-aware feature\nextracting, and (3) a Recursive Pose Refine Branch (RPRB) using the\nLevenberg-Marquardt (LM) algorithm to align the initial pose towards the true\nvehicle pose iteratively. Our method is validated on KITTI and Ford Multi-AV\nSeasonal datasets as ground view and Google Maps as the satellite view. The\nresults demonstrate the superiority of our method in cross-view localization\nwith spatial and angular errors within 1 meter and $2^\\circ$, respectively. The\ncode will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning with Contrastive Vision Transformer. (arXiv:2207.13516v1 [cs.LG])","link":"http://arxiv.org/abs/2207.13516","description":"<p>Online continual learning (online CL) studies the problem of learning\nsequential tasks from an online data stream without task boundaries, aiming to\nadapt to new data while alleviating catastrophic forgetting on the past tasks.\nThis paper proposes a framework Contrastive Vision Transformer (CVT), which\ndesigns a focal contrastive learning strategy based on a transformer\narchitecture, to achieve a better stability-plasticity trade-off for online CL.\nSpecifically, we design a new external attention mechanism for online CL that\nimplicitly captures previous tasks' information. Besides, CVT contains\nlearnable focuses for each class, which could accumulate the knowledge of\nprevious classes to alleviate forgetting. Based on the learnable focuses, we\ndesign a focal contrastive loss to rebalance contrastive learning between new\nand past classes and consolidate previously learned representations. Moreover,\nCVT contains a dual-classifier structure for decoupling learning current\nclasses and balancing all observed classes. The extensive experimental results\nshow that our approach achieves state-of-the-art performance with even fewer\nparameters on online CL benchmarks and effectively alleviates the catastrophic\nforgetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yajing Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiaxian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Future Unruptured Intracranial Aneurysm Growth Prediction using Mesh Convolutional Neural Networks. (arXiv:2207.13518v1 [eess.IV])","link":"http://arxiv.org/abs/2207.13518","description":"<p>The growth of unruptured intracranial aneurysms (UIAs) is a predictor of\nrupture. Therefore, for further imaging surveillance and treatment planning, it\nis important to be able to predict if an UIA is likely to grow based on an\ninitial baseline Time-of-Flight MRA (TOF-MRA). It is known that the size and\nshape of UIAs are predictors of aneurysm growth and/or rupture. We perform a\nfeasibility study of using a mesh convolutional neural network for future UIA\ngrowth prediction from baseline TOF-MRAs. We include 151 TOF-MRAs, with 169\nUIAs where 49 UIAs were classified as growing and 120 as stable, based on the\nclinical definition of growth (&gt;1 mm increase in size in follow-up scan). UIAs\nwere segmented from TOF-MRAs and meshes were automatically generated. We\ninvestigate the input of both UIA mesh only and region-of-interest (ROI) meshes\nincluding UIA and surrounding parent vessels. We develop a classification model\nto predict UIAs that will grow or remain stable. The model consisted of a mesh\nconvolutional neural network including additional novel input edge features of\nshape index and curvedness which describe the surface topology. It was\ninvestigated if input edge mid-point co-ordinates influenced the model\nperformance. The model with highest AUC (63.8%) for growth prediction was using\nUIA meshes with input edge mid-point co-ordinate features (average F1 score =\n62.3%, accuracy = 66.9%, sensitivity = 57.3%, specificity = 70.8%). We present\na future UIA growth prediction model based on a mesh convolutional neural\nnetwork with promising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Timmins_K/0/1/0/all/0/1\">Kimberley M. Timmins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamphuis_M/0/1/0/all/0/1\">Maarten J. Kamphuis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vos_I/0/1/0/all/0/1\">Iris N. Vos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Velthuis_B/0/1/0/all/0/1\">Birgitta K. Velthuis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schaaf_I/0/1/0/all/0/1\">Irene C. van der Schaaf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuijf_H/0/1/0/all/0/1\">Hugo J. Kuijf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Masked Autoencoders are Stronger Vision Learners. (arXiv:2207.13532v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13532","description":"<p>Masked image modeling (MIM) has achieved promising results on various vision\ntasks. However, the limited discriminability of learned representation\nmanifests there is still plenty to go for making a stronger vision learner.\nTowards this goal, we propose Contrastive Masked Autoencoders (CMAE), a new\nself-supervised pre-training method for learning more comprehensive and capable\nvision representations. By elaboratively unifying contrastive learning (CL) and\nmasked image model (MIM) through novel designs, CMAE leverages their respective\nadvantages and learns representations with both strong instance\ndiscriminability and local perceptibility. Specifically, CMAE consists of two\nbranches where the online branch is an asymmetric encoder-decoder and the\ntarget branch is a momentum updated encoder. During training, the online\nencoder reconstructs original images from latent representations of masked\nimages to learn holistic features. The target encoder, fed with the full\nimages, enhances the feature discriminability via contrastive learning with its\nonline counterpart. To make CL compatible with MIM, CMAE introduces two new\ncomponents, i.e. pixel shift for generating plausible positive views and\nfeature decoder for complementing features of contrastive pairs. Thanks to\nthese novel designs, CMAE effectively improves the representation quality and\ntransfer performance over its MIM counterpart. CMAE achieves the\nstate-of-the-art performance on highly competitive benchmarks of image\nclassification, semantic segmentation and object detection. Notably, CMAE-Base\nachieves $85.3\\%$ top-1 accuracy on ImageNet and $52.5\\%$ mIoU on ADE20k,\nsurpassing previous best results by $0.7\\%$ and $1.8\\%$ respectively. Codes\nwill be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhicheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xiaojie Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chengze Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Q/0/1/0/all/0/1\">Qibin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1\">Dongmei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaohui Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstracting Sketches through Simple Primitives. (arXiv:2207.13543v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13543","description":"<p>Humans show high-level of abstraction capabilities in games that require\nquickly communicating object information. They decompose the message content\ninto multiple parts and communicate them in an interpretable protocol. Toward\nequipping machines with such capabilities, we propose the Primitive-based\nSketch Abstraction task where the goal is to represent sketches using a fixed\nset of drawing primitives under the influence of a budget. To solve this task,\nour Primitive-Matching Network (PMN), learns interpretable abstractions of a\nsketch in a self supervised manner. Specifically, PMN maps each stroke of a\nsketch to its most similar primitive in a given set, predicting an affine\ntransformation that aligns the selected primitive to the target stroke. We\nlearn this stroke-to-primitive mapping end-to-end with a distance-transform\nloss that is minimal when the original sketch is precisely reconstructed with\nthe predicted primitives. Our PMN abstraction empirically achieves the highest\nperformance on sketch recognition and sketch-based image retrieval given a\ncommunication budget, while at the same time being highly interpretable. This\nopens up new possibilities for sketch analysis, such as comparing sketches by\nextracting the most relevant primitives that define an object category. Code is\navailable at https://github.com/ExplainableML/sketch-primitives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alaniz_S/0/1/0/all/0/1\">Stephan Alaniz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1\">Anjan Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcos_D/0/1/0/all/0/1\">Diego Marcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Proper Orthogonal Decomposition approach for parameters reduction of Single Shot Detector networks. (arXiv:2207.13551v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13551","description":"<p>As a major breakthrough in artificial intelligence and deep learning,\nConvolutional Neural Networks have achieved an impressive success in solving\nmany problems in several fields including computer vision and image processing.\nReal-time performance, robustness of algorithms and fast training processes\nremain open problems in these contexts. In addition object recognition and\ndetection are challenging tasks for resource-constrained embedded systems,\ncommonly used in the industrial sector. To overcome these issues, we propose a\ndimensionality reduction framework based on Proper Orthogonal Decomposition, a\nclassical model order reduction technique, in order to gain a reduction in the\nnumber of hyperparameters of the net. We have applied such framework to SSD300\narchitecture using PASCAL VOC dataset, demonstrating a reduction of the network\ndimension and a remarkable speedup in the fine-tuning of the network in a\ntransfer learning context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meneghetti_L/0/1/0/all/0/1\">Laura Meneghetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demo_N/0/1/0/all/0/1\">Nicola Demo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozza_G/0/1/0/all/0/1\">Gianluigi Rozza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D3C2-Net: Dual-Domain Deep Convolutional Coding Network for Compressive Sensing. (arXiv:2207.13560v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13560","description":"<p>Mapping optimization algorithms into neural networks, deep unfolding networks\n(DUNs) have achieved impressive success in compressive sensing (CS). From the\nperspective of optimization, DUNs inherit a well-defined and interpretable\nstructure from iterative steps. However, from the viewpoint of neural network\ndesign, most existing DUNs are inherently established based on traditional\nimage-domain unfolding, which takes one-channel images as inputs and outputs\nbetween adjacent stages, resulting in insufficient information transmission\ncapability and inevitable loss of the image details. In this paper, to break\nthe above bottleneck, we first propose a generalized dual-domain optimization\nframework, which is general for inverse imaging and integrates the merits of\nboth (1) image-domain and (2) convolutional-coding-domain priors to constrain\nthe feasible region in the solution space. By unfolding the proposed framework\ninto deep neural networks, we further design a novel Dual-Domain Deep\nConvolutional Coding Network (D3C2-Net) for CS imaging with the capability of\ntransmitting high-throughput feature-level image representation through all the\nunfolded stages. Experiments on natural and MR images demonstrate that our\nD3C2-Net achieves higher performance and better accuracy-complexity trade-offs\nthan other state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight and Progressively-Scalable Networks for Semantic Segmentation. (arXiv:2207.13600v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13600","description":"<p>Multi-scale learning frameworks have been regarded as a capable class of\nmodels to boost semantic segmentation. The problem nevertheless is not trivial\nespecially for the real-world deployments, which often demand high efficiency\nin inference latency. In this paper, we thoroughly analyze the design of\nconvolutional blocks (the type of convolutions and the number of channels in\nconvolutions), and the ways of interactions across multiple scales, all from\nlightweight standpoint for semantic segmentation. With such in-depth\ncomparisons, we conclude three principles, and accordingly devise Lightweight\nand Progressively-Scalable Networks (LPS-Net) that novelly expands the network\ncomplexity in a greedy manner. Technically, LPS-Net first capitalizes on the\nprinciples to build a tiny network. Then, LPS-Net progressively scales the tiny\nnetwork to larger ones by expanding a single dimension (the number of\nconvolutional blocks, the number of channels, or the input resolution) at one\ntime to meet the best speed/accuracy tradeoff. Extensive experiments conducted\non three datasets consistently demonstrate the superiority of LPS-Net over\nseveral efficient semantic segmentation methods. More remarkably, our LPS-Net\nachieves 73.4% mIoU on Cityscapes test set, with the speed of 413.5FPS on an\nNVIDIA GTX 1080Ti, leading to a performance improvement by 1.5% and a 65%\nspeed-up against the state-of-the-art STDC. Code is available at\n\\url{https://github.com/YihengZhang-CV/LPS-Net}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Ting Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhaofan Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Radiance Transfer Fields for Relightable Novel-view Synthesis with Global Illumination. (arXiv:2207.13607v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13607","description":"<p>Given a set of images of a scene, the re-rendering of this scene from novel\nviews and lighting conditions is an important and challenging problem in\nComputer Vision and Graphics. On the one hand, most existing works in Computer\nVision usually impose many assumptions regarding the image formation process,\ne.g. direct illumination and predefined materials, to make scene parameter\nestimation tractable. On the other hand, mature Computer Graphics tools allow\nmodeling of complex photo-realistic light transport given all the scene\nparameters. Combining these approaches, we propose a method for scene\nrelighting under novel views by learning a neural precomputed radiance transfer\nfunction, which implicitly handles global illumination effects using novel\nenvironment maps. Our method can be solely supervised on a set of real images\nof the scene under a single unknown lighting condition. To disambiguate the\ntask during training, we tightly integrate a differentiable path tracer in the\ntraining process and propose a combination of a synthesized OLAT and a real\nimage loss. Results show that the recovered disentanglement of scene parameters\nimproves significantly over the current state of the art and, thus, also our\nre-rendering results are more realistic and accurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Linjie Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1\">Ayush Tewari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leimkuehler_T/0/1/0/all/0/1\">Thomas Leimkuehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1\">Marc Habermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semi-automatic Cell Tracking Process Towards Completing the 4D Atlas of C. elegans Development. (arXiv:2207.13611v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13611","description":"<p>The nematode Caenorhabditis elegans (C. elegans) is used as a model organism\nto better understand developmental biology and neurobiology. C. elegans\nfeatures an invariant cell lineage, which has been catalogued and observed\nusing fluorescence microscopy images. However, established methods to track\ncells in late-stage development fail to generalize once sporadic muscular\ntwitching has begun. We build upon methodology which uses skin cells as\nfiducial markers to carry out cell tracking despite random twitching. In\nparticular, we present a cell nucleus segmentation and tracking procedure which\nwas integrated into a 3D rendering GUI to improve efficiency in tracking cells\nacross late-stage development. Results on images depicting aforementioned\nmuscle cell nuclei across three test embryos suggest the fiducial markers in\nconjunction with a classic tracking paradigm overcome sporadic twitching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauziere_A/0/1/0/all/0/1\">Andrew Lauziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christensen_R/0/1/0/all/0/1\">Ryan Christensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_H/0/1/0/all/0/1\">Hari Shroff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Deep Learning to Detecting Deepfakes. (arXiv:2207.13644v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13644","description":"<p>In the recent years, social media has grown to become a major source of\ninformation for many online users. This has given rise to the spread of\nmisinformation through deepfakes. Deepfakes are videos or images that replace\none persons face with another computer-generated face, often a more\nrecognizable person in society. With the recent advances in technology, a\nperson with little technological experience can generate these videos. This\nenables them to mimic a power figure in society, such as a president or\ncelebrity, creating the potential danger of spreading misinformation and other\nnefarious uses of deepfakes. To combat this online threat, researchers have\ndeveloped models that are designed to detect deepfakes. This study looks at\nvarious deepfake detection models that use deep learning algorithms to combat\nthis looming threat. This survey focuses on providing a comprehensive overview\nof the current state of deepfake detection models and the unique approaches\nmany researchers take to solving this problem. The benefits, limitations, and\nsuggestions for future work will be thoroughly discussed throughout this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallet_J/0/1/0/all/0/1\">Jacob Mallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_R/0/1/0/all/0/1\">Rushit Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seliya_N/0/1/0/all/0/1\">Naeem Seliya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanamala_M/0/1/0/all/0/1\">Mounika Vanamala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Interpolation: Time-Arbitrary Frame Interpolation via Dual Meta-Learning. (arXiv:2207.13670v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13670","description":"<p>Existing video frame interpolation methods can only interpolate the frame at\na given intermediate time-step, e.g. 1/2. In this paper, we aim to explore a\nmore generalized kind of video frame interpolation, that at an arbitrary\ntime-step. To this end, we consider processing different time-steps with\nadaptively generated convolutional kernels in a unified way with the help of\nmeta-learning. Specifically, we develop a dual meta-learned frame interpolation\nframework to synthesize intermediate frames with the guidance of context\ninformation and optical flow as well as taking the time-step as side\ninformation. First, a content-aware meta-learned flow refinement module is\nbuilt to improve the accuracy of the optical flow estimation based on the\ndown-sampled version of the input frames. Second, with the refined optical flow\nand the time-step as the input, a motion-aware meta-learned frame interpolation\nmodule generates the convolutional kernels for every pixel used in the\nconvolution operations on the feature map of the coarse warped version of the\ninput frames to generate the predicted frame. Extensive qualitative and\nquantitative evaluations, as well as ablation studies, demonstrate that, via\nintroducing meta-learning in our framework in such a well-designed way, our\nmethod not only achieves superior performance to state-of-the-art frame\ninterpolation approaches but also owns an extended capacity to support the\ninterpolation at an arbitrary time-step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shixing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yiyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wei Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-layer Representation Learning for Robust OOD Image Classification. (arXiv:2207.13678v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13678","description":"<p>Convolutional Neural Networks have become the norm in image classification.\nNevertheless, their difficulty to maintain high accuracy across datasets has\nbecome apparent in the past few years. In order to utilize such models in\nreal-world scenarios and applications, they must be able to provide trustworthy\npredictions on unseen data. In this paper, we argue that extracting features\nfrom a CNN's intermediate layers can assist in the model's final prediction.\nSpecifically, we adapt the Hypercolumns method to a ResNet-18 and find a\nsignificant increase in the model's accuracy, when evaluating on the NICO\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ballas_A/0/1/0/all/0/1\">Aristotelis Ballas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diou_C/0/1/0/all/0/1\">Christos Diou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shift-tolerant Perceptual Similarity Metric. (arXiv:2207.13686v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13686","description":"<p>Existing perceptual similarity metrics assume an image and its reference are\nwell aligned. As a result, these metrics are often sensitive to a small\nalignment error that is imperceptible to the human eyes. This paper studies the\neffect of small misalignment, specifically a small shift between the input and\nreference image, on existing metrics, and accordingly develops a shift-tolerant\nsimilarity metric. This paper builds upon LPIPS, a widely used learned\nperceptual similarity metric, and explores architectural design considerations\nto make it robust against imperceptible misalignment. Specifically, we study a\nwide spectrum of neural network elements, such as anti-aliasing filtering,\npooling, striding, padding, and skip connection, and discuss their roles in\nmaking a robust metric. Based on our studies, we develop a new deep neural\nnetwork-based perceptual similarity metric. Our experiments show that our\nmetric is tolerant to imperceptible shifts while being consistent with the\nhuman similarity judgment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghildyal_A/0/1/0/all/0/1\">Abhijay Ghildyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShAPO: Implicit Representations for Multi-Object Shape, Appearance, and Pose Optimization. (arXiv:2207.13691v1 [cs.CV])","link":"http://arxiv.org/abs/2207.13691","description":"<p>Our method studies the complex task of object-centric 3D understanding from a\nsingle RGB-D observation. As it is an ill-posed problem, existing methods\nsuffer from low performance for both 3D shape and 6D pose and size estimation\nin complex multi-object scenarios with occlusions. We present ShAPO, a method\nfor joint multi-object detection, 3D textured reconstruction, 6D object pose\nand size estimation. Key to ShAPO is a single-shot pipeline to regress shape,\nappearance and pose latent codes along with the masks of each object instance,\nwhich is then further refined in a sparse-to-dense fashion. A novel\ndisentangled shape and appearance database of priors is first learned to embed\nobjects in their respective shape and appearance space. We also propose a\nnovel, octree-based differentiable optimization step, allowing us to further\nimprove object shape, pose and appearance simultaneously under the learned\nlatent space, in an analysis-by-synthesis fashion. Our novel joint implicit\ntextured object representation allows us to accurately identify and reconstruct\nnovel unseen objects without having access to their 3D meshes. Through\nextensive experiments, we show that our method, trained on simulated indoor\nscenes, accurately regresses the shape, appearance and pose of novel objects in\nthe real-world with minimal fine-tuning. Our method significantly out-performs\nall baselines on the NOCS dataset with an 8% absolute improvement in mAP for 6D\npose estimation. Project page:\nhttps://zubair-irshad.github.io/projects/ShAPO.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1\">Muhammad Zubair Irshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Gait Database for Normal Walk Collected by Smartphone Accelerometer. (arXiv:1905.03109v4 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/1905.03109","description":"<p>Gait recognition is the characterization of unique biometric patterns\nassociated with each individual which can be utilized to identify a person\nwithout direct contact. A public gait database with a relatively large number\nof subjects can provide a great opportunity for future studies to build and\nvalidate gait authentication models. The goal of this study is to introduce a\ncomprehensive gait database of 93 human subjects who walked between two\nendpoints (320 meters) during two different sessions and record their gait data\nusing two smartphones, one attached to the right thigh and another one on the\nleft side of the waist. This data is collected to be utilized by a deep\nlearning-based method that requires enough time points. The metadata including\nage, gender, smoking, daily exercise time, height, and weight of an individual\nis recorded. this data set is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vajdi_A/0/1/0/all/0/1\">Amir Vajdi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zaghian_M/0/1/0/all/0/1\">Mohammad Reza Zaghian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dehkordi_N/0/1/0/all/0/1\">Nazli Rafei Dehkordi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rastegari_E/0/1/0/all/0/1\">Elham Rastegari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maroofi_K/0/1/0/all/0/1\">Kian Maroofi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farahmand_S/0/1/0/all/0/1\">Saman Farahmand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1\">Shaohua Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pomplun_M/0/1/0/all/0/1\">Marc Pomplun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Haspel_N/0/1/0/all/0/1\">Nurit Haspel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bayat_A/0/1/0/all/0/1\">Akram Bayat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Analysis for Semantic Segmentation with Applications on Feature Truncation and Weak Annotation. (arXiv:2012.14123v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14123","description":"<p>It is well known that semantic segmentation neural networks (SSNNs) produce\ndense segmentation maps to resolve the objects' boundaries while restrict the\nprediction on down-sampled grids to alleviate the computational cost. A\nstriking balance between the accuracy and the training cost of the SSNNs such\nas U-Net exists. We propose a spectral analysis to investigate the correlations\namong the resolution of the down sampled grid, the loss function and the\naccuracy of the SSNNs. By analyzing the network back-propagation process in\nfrequency domain, we discover that the traditional loss function,\ncross-entropy, and the key features of CNN are mainly affected by the\nlow-frequency components of segmentation labels. Our discoveries can be applied\nto SSNNs in several ways including (i) determining an efficient low resolution\ngrid for resolving the segmentation maps (ii) pruning the networks by\ntruncating the high frequency decoder features for saving computation costs,\nand (iii) using block-wise weak annotation for saving the labeling time.\nExperimental results shown in this paper agree with our spectral analysis for\nthe networks such as DeepLab V3+ and Deep Aggregation Net (DAN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chin-Tien Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS Imagery. (arXiv:2103.12434v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12434","description":"<p>Depleting lake ice is a climate change indicator, just like sea-level rise or\nglacial retreat. Monitoring Lake Ice Phenology (LIP) is useful because\nlong-term freezing and thawing patterns serve as sentinels to understand\nregional and global climate change. We report a study for the Oberengadin\nregion of Switzerland, where several small- and medium-sized mountain lakes are\nlocated. We observe the LIP events, such as freeze-up, break-up and ice cover\nduration, across two decades (2000-2020) from optical satellite images. We\nanalyse the time series of MODIS imagery by estimating spatially resolved maps\nof lake ice for these Alpine lakes with supervised machine learning. To train\nthe classifier we rely on reference data annotated manually based on webcam\nimages. From the ice maps, we derive long-term LIP trends. Since the webcam\ndata are only available for two winters, we cross-check our results against the\noperational MODIS and VIIRS snow products. We find a change in complete freeze\nduration of -0.76 and -0.89 days per annum for lakes Sils and Silvaplana,\nrespectively. Furthermore, we observe plausible correlations of the LIP trends\nwith climate data measured at nearby meteorological stations. We notice that\nmean winter air temperature has a negative correlation with the freeze duration\nand break-up events and a positive correlation with the freeze-up events.\nAdditionally, we observe a strong negative correlation of sunshine during the\nwinter months with the freeze duration and break-up events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1\">Manu Tom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1\">Emmanuel Baltsavias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Adversarial Learning for Real-world Image Super-resolution. (arXiv:2104.09008v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09008","description":"<p>Current deep image super-resolution (SR) approaches attempt to restore\nhigh-resolution images from down-sampled images or by assuming degradation from\nsimple Gaussian kernels and additive noises. However, such simple image\nprocessing techniques represent crude approximations of the real-world\nprocedure of lowering image resolution. In this paper, we propose a more\nrealistic process to lower image resolution by introducing a new Kernel\nAdversarial Learning Super-resolution (KASR) framework to deal with the\nreal-world image SR problem. In the proposed framework, degradation kernels and\nnoises are adaptively modeled rather than explicitly specified. Moreover, we\nalso propose an iterative supervision process and high-frequency selective\nobjective to further boost the model SR reconstruction accuracy. Extensive\nexperiments validate the effectiveness of the proposed framework on real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A new perspective on the approximation capability of GNNs. (arXiv:2106.08992v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.08992","description":"<p>Graph Neural Networks (GNNs) are a broad class of connectionist models for\ngraph processing. Recent studies have shown that GNNs can approximate any\nfunction on graphs, modulo the equivalence relation on nodes defined by the\nWeisfeiler - Lehman test. However, these results suffer from some limitations,\nboth because they were derived using the Stone-Weierstrass theorem - which is\nexistential in nature -, and because they assume that the target function to be\napproximated must be continuous. In this paper, we propose an alternative way\nto demonstrate the approximation capability of GNNs that overcomes these\nlimitations. In particular, some new results are proved, which allow to: (1)\ndefine GNN architectures capable of obtaining a given approximation; (2) show\nthat the Weisfeiler-Lehman test converges in r+1 steps, where r is the diameter\nof the graph; (3) derive a formal relationship between the Weisfeiler-Lehman\ntest and the unfolding trees, that is trees that can be built by visiting the\ngraph starting from a given node. These results provide a more comprehensive\nunderstanding of the approximation power of GNNs, definitely showing that the\n1-WL test and the unfolding tree concepts can be used interchangeably to study\nthe their expressiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DInverno_G/0/1/0/all/0/1\">Giuseppe Alessio D&#x27;Inverno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianchini_M/0/1/0/all/0/1\">Monica Bianchini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampoli_M/0/1/0/all/0/1\">Maria Lucia Sampoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarselli_F/0/1/0/all/0/1\">Franco Scarselli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Certified Segmentation via Randomized Smoothing. (arXiv:2107.00228v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00228","description":"<p>We present a new certification method for image and point cloud segmentation\nbased on randomized smoothing. The method leverages a novel scalable algorithm\nfor prediction and certification that correctly accounts for multiple testing,\nnecessary for ensuring statistical guarantees. The key to our approach is\nreliance on established multiple-testing correction mechanisms as well as the\nability to abstain from classifying single pixels or points while still\nrobustly segmenting the overall input. Our experimental evaluation on synthetic\ndata and challenging datasets, such as Pascal Context, Cityscapes, and\nShapeNet, shows that our algorithm can achieve, for the first time, competitive\naccuracy and certification guarantees on real-world segmentation tasks. We\nprovide an implementation at https://github.com/eth-sri/segmentation-smoothing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1\">Maximilian Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stacked BNAS: Rethinking Broad Convolutional Neural Network for Neural Architecture Search. (arXiv:2111.07722v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07722","description":"<p>Different from other deep scalable architecture-based NAS approaches, Broad\nNeural Architecture Search (BNAS) proposes a broad scalable architecture which\nconsists of convolution and enhancement blocks, dubbed Broad Convolutional\nNeural Network (BCNN), as the search space for amazing efficiency improvement.\nBCNN reuses the topologies of cells in the convolution block so that BNAS can\nemploy few cells for efficient search. Moreover, multi-scale feature fusion and\nknowledge embedding are proposed to improve the performance of BCNN with\nshallow topology. However, BNAS suffers some drawbacks: 1) insufficient\nrepresentation diversity for feature fusion and enhancement and 2) time\nconsumption of knowledge embedding design by human experts. This paper proposes\nStacked BNAS, whose search space is a developed broad scalable architecture\nnamed Stacked BCNN, with better performance than BNAS. On the one hand, Stacked\nBCNN treats mini BCNN as a basic block to preserve comprehensive representation\nand deliver powerful feature extraction ability. For multi-scale feature\nenhancement, each mini BCNN feeds the outputs of deep and broad cells to the\nenhancement cell. For multi-scale feature fusion, each mini BCNN feeds the\noutputs of deep, broad and enhancement cells to the output node. On the other\nhand, Knowledge Embedding Search (KES) is proposed to learn appropriate\nknowledge embeddings in a differentiable way. Moreover, the basic unit of KES\nis an over-parameterized knowledge embedding module that consists of all\npossible candidate knowledge embeddings. Experimental results show that 1)\nStacked BNAS obtains better performance than BNAS-v2 on both CIFAR-10 and\nImageNet, 2) the proposed KES algorithm contributes to reducing the parameters\nof the learned architecture with satisfactory performance, and 3) Stacked BNAS\ndelivers a state-of-the-art efficiency of 0.02 GPU days.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zixiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nannan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">C.L.Philip Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling. (arXiv:2111.12085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12085","description":"<p>We propose UniTAB that Unifies Text And Box outputs for grounded\nvision-language (VL) modeling. Grounded VL tasks such as grounded captioning\nrequire the model to generate a text description and align predicted words with\nobject regions. To achieve this, models must generate desired text and box\noutputs together, and meanwhile indicate the alignments between words and\nboxes. In contrast to existing solutions that use multiple separate modules for\ndifferent outputs, UniTAB represents both text and box outputs with a shared\ntoken sequence, and introduces a special &lt;obj&gt; token to naturally indicate\nword-box alignments in the sequence. UniTAB thus could provide a more\ncomprehensive and interpretable image description, by freely grounding\ngenerated words to object regions. On grounded captioning, UniTAB presents a\nsimpler solution with a single output head, and significantly outperforms state\nof the art in both grounding and captioning evaluations. On general VL tasks\nthat have different desired output formats (i.e., text, box, or their\ncombination), UniTAB with a single network achieves better or comparable\nperformance than task-specific state of the art. Experiments cover 7 VL\nbenchmarks, including grounded captioning, visual grounding, image captioning,\nand visual question answering. Furthermore, UniTAB's unified multi-task network\nand the task-agnostic output sequence design make the model parameter efficient\nand generalizable to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhengyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1\">Faisal Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yumao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTQ4ViT: Post-Training Quantization Framework for Vision Transformers with Twin Uniform Quantization. (arXiv:2111.12293v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12293","description":"<p>Quantization is one of the most effective methods to compress neural\nnetworks, which has achieved great success on convolutional neural networks\n(CNNs). Recently, vision transformers have demonstrated great potential in\ncomputer vision. However, previous post-training quantization methods performed\nnot well on vision transformer, resulting in more than 1% accuracy drop even in\n8-bit quantization. Therefore, we analyze the problems of quantization on\nvision transformers. We observe the distributions of activation values after\nsoftmax and GELU functions are quite different from the Gaussian distribution.\nWe also observe that common quantization metrics, such as MSE and cosine\ndistance, are inaccurate to determine the optimal scaling factor. In this\npaper, we propose the twin uniform quantization method to reduce the\nquantization error on these activation values. And we propose to use a Hessian\nguided metric to evaluate different scaling factors, which improves the\naccuracy of calibration at a small cost. To enable the fast quantization of\nvision transformers, we develop an efficient framework, PTQ4ViT. Experiments\nshow the quantized vision transformers achieve near-lossless prediction\naccuracy (less than 0.5% drop at 8-bit quantization) on the ImageNet\nclassification task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihang Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chenhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yiqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangyu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13650","description":"<p>Fair representation learning transforms user data into a representation that\nensures fairness and utility regardless of the downstream application. However,\nlearning individually fair representations, i.e., guaranteeing that similar\nindividuals are treated similarly, remains challenging in high-dimensional\nsettings such as computer vision. In this work, we introduce LASSI, the first\nrepresentation learning method for certifying individual fairness of\nhigh-dimensional data. Our key insight is to leverage recent advances in\ngenerative modeling to capture the set of similar individuals in the generative\nlatent space. This enables us to learn individually fair representations that\nmap similar individuals close together by using adversarial training to\nminimize the distance between their representations. Finally, we employ\nrandomized smoothing to provably map similar individuals close together, in\nturn ensuring that local robustness verification of the downstream application\nresults in end-to-end fairness certification. Our experimental evaluation on\nchallenging real-world image data demonstrates that our method increases\ncertified individual fairness by up to 90% without significantly affecting task\nutility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peychev_M/0/1/0/all/0/1\">Momchil Peychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1\">Anian Ruoss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1\">Mislav Balunovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1\">Maximilian Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Representations for Deep Skeletal Animation. (arXiv:2111.13907v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13907","description":"<p>Data-driven character animation techniques rely on the existence of a\nproperly established model of motion, capable of describing its rich context.\nHowever, commonly used motion representations often fail to accurately encode\nthe full articulation of motion, or present artifacts. In this work, we address\nthe fundamental problem of finding a robust pose representation for motion\nmodeling, suitable for deep character animation, one that can better constrain\nposes and faithfully capture nuances correlated with skeletal characteristics.\nOur representation is based on dual quaternions, the mathematical abstractions\nwith well-defined operations, which simultaneously encode rotational and\npositional orientation, enabling a hierarchy-aware encoding, centered around\nthe root. We demonstrate that our representation overcomes common motion\nartifacts, and assess its performance compared to other popular\nrepresentations. We conduct an ablation study to evaluate the impact of various\nlosses that can be incorporated during learning. Leveraging the fact that our\nrepresentation implicitly encodes skeletal motion attributes, we train a\nnetwork on a dataset comprising of skeletons with different proportions,\nwithout the need to retarget them first to a universal skeleton, which causes\nsubtle motion elements to be missed. We show that smooth and natural poses can\nbe achieved, paving the way for fascinating applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Andreou_N/0/1/0/all/0/1\">Nefeli Andreou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aristidou_A/0/1/0/all/0/1\">Andreas Aristidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysanthou_Y/0/1/0/all/0/1\">Yiorgos Chrysanthou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extract Free Dense Labels from CLIP. (arXiv:2112.01071v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01071","description":"<p>Contrastive Language-Image Pre-training (CLIP) has made a remarkable\nbreakthrough in open-vocabulary zero-shot image recognition. Many recent\nstudies leverage the pre-trained CLIP models for image-level classification and\nmanipulation. In this paper, we wish examine the intrinsic potential of CLIP\nfor pixel-level dense prediction, specifically in semantic segmentation. To\nthis end, with minimal modification, we show that MaskCLIP yields compelling\nsegmentation results on open concepts across various datasets in the absence of\nannotations and fine-tuning. By adding pseudo labeling and self-training,\nMaskCLIP+ surpasses SOTA transductive zero-shot semantic segmentation methods\nby large margins, e.g., mIoUs of unseen classes on PASCAL VOC/PASCAL\nContext/COCO Stuff are improved from 35.6/20.7/30.3 to 86.1/66.7/54.7. We also\ntest the robustness of MaskCLIP under input corruption and evaluate its\ncapability in discriminating fine-grained objects and novel concepts. Our\nfinding suggests that MaskCLIP can serve as a new reliable source of\nsupervision for dense prediction tasks to achieve annotation-free segmentation.\nSource code is available at https://github.com/chongzhou96/MaskCLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bo Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Synthesize a Large-Scale and Trainable Micro-Expression Dataset?. (arXiv:2112.01730v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01730","description":"<p>This paper does not contain technical novelty but introduces our key\ndiscoveries in a data generation protocol, a database and insights.We aim to\naddress the lack of large-scale datasets in micro-expression (MiE) recognition\ndue to the prohibitive cost of data collection, which renders large-scale\ntraining less feasible. To this end, we develop a protocol to automatically\nsynthesize large scale MiE training data that allow us to train improved\nrecognition models for real-world test data. Specifically, we discover three\ntypes of Action Units (AUs) that can constitute trainable MiEs. These AUs come\nfrom real-world MiEs, early frames of macro-expression videos, and the\nrelationship between AUs and expression categories defined by human expert\nknowledge. With these AUs, our protocol then employs large numbers of face\nimages of various identities and an off-the-shelf face generator for MiE\nsynthesis, yielding the MiE-X dataset. MiE recognition models are trained or\npre-trained on MiE-X and evaluated on real-world test sets, where very\ncompetitive accuracy is obtained. Experimental results not only validate the\neffectiveness of the discovered AUs and MiE-X dataset but also reveal some\ninteresting properties of MiEs: they generalize across faces, are close to\nearly-stage macro-expressions, and can be manually defined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video Retrieval. (arXiv:2112.01832v3 [cs.MM] UPDATED)","link":"http://arxiv.org/abs/2112.01832","description":"<p>In this paper we revisit feature fusion, an old-fashioned topic, in the new\ncontext of text-to-video retrieval. Different from previous research that\nconsiders feature fusion only at one end, let it be video or text, we aim for\nfeature fusion for both ends within a unified framework. We hypothesize that\noptimizing the convex combination of the features is preferred to modeling\ntheir correlations by computationally heavy multi-head self attention. We\npropose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature\nfusion at both early and late stages and at both video and text ends, making it\na powerful method for exploiting diverse (off-the-shelf) features. The\ninterpretability of LAFF can be used for feature selection. Extensive\nexperiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and\nTRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Aozhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fangming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Style Transfer and Unpaired Image-to-Image Translation to deal with the Domain Shift Problem on Spheroid Segmentation. (arXiv:2112.09043v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09043","description":"<p>Background and objectives. Domain shift is a generalisation problem of\nmachine learning models that occurs when the data distribution of the training\nset is different to the data distribution encountered by the model when it is\ndeployed. This is common in the context of biomedical image segmentation due to\nthe variance of experimental conditions, equipment, and capturing settings. In\nthis work, we address this challenge by studying both neural style transfer\nalgorithms and unpaired image-to-image translation methods in the context of\nthe segmentation of tumour spheroids.\n</p>\n<p>Methods. We have illustrated the domain shift problem in the context of\nspheroid segmentation with 4 deep learning segmentation models that achieved an\nIoU over 97% when tested with images following the training distribution, but\nwhose performance decreased up to an 84\\% when applied to images captured under\ndifferent conditions. In order to deal with this problem, we have explored 3\nstyle transfer algorithms (NST, deep image analogy, and STROTSS), and 6\nunpaired image-to-image translations algorithms (CycleGAN, DualGAN, ForkGAN,\nGANILLA, CUT, and FastCUT). These algorithms have been integrated into a\nhigh-level API that facilitates their application to other contexts where the\ndomain-shift problem occurs.\n</p>\n<p>Results. We have considerably improved the performance of the 4 segmentation\nmodels when applied to images captured under different conditions by using both\nstyle transfer and image-to-image translation algorithms. In particular, there\nare 2 style transfer algorithms (NST and deep image analogy) and 1 unpaired\nimage-to-image translations algorithm (CycleGAN) that improve the IoU of the\nmodels in a range from 0.24 to 76.07. Therefore, reaching a similar performance\nto the one obtained with the models are applied to images following the\ntraining distribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Dominguez_M/0/1/0/all/0/1\">Manuel Garc&#xed;a-Dom&#xed;nguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dominguez_C/0/1/0/all/0/1\">C&#xe9;sar Dom&#xed;nguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heras_J/0/1/0/all/0/1\">J&#xf3;nathan Heras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mata_E/0/1/0/all/0/1\">Eloy Mata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascual_V/0/1/0/all/0/1\">Vico Pascual</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.13592","description":"<p>As information exists in various modalities in real world, effective\ninteraction and fusion among multimodal information plays a key role for the\ncreation and perception of multimodal data in computer vision and deep learning\nresearch. With superb power in modelling the interaction among multimodal\ninformation, multimodal image synthesis and editing has become a hot research\ntopic in recent years. Instead of providing explicit guidance for network\ntraining, multimodal guidance offers intuitive and flexible means for image\nsynthesis and editing. On the other hand, this field is also facing several\nchallenges in alignment of features with inherent modality gaps, synthesis of\nhigh-resolution images, faithful evaluation metrics, etc. In this survey, we\ncomprehensively contextualize the advance of the recent multimodal image\nsynthesis and editing and formulate taxonomies according to data modality and\nmodel architectures. We start with an introduction to different types of\nguidance modalities in image synthesis and editing. We then describe multimodal\nimage synthesis and editing approaches extensively with detailed frameworks\nincluding Generative Adversarial Networks (GANs), Auto-regressive models,\nDiffusion models, Neural Radiance Fields (NeRF) and other methods. This is\nfollowed by a comprehensive description of benchmark datasets and corresponding\nevaluation metrics as widely adopted in multimodal image synthesis and editing,\nas well as detailed comparisons of various synthesis methods with analysis of\nrespective advantages and limitations. Finally, we provide insights about the\ncurrent research challenges and possible directions for future research. We\nhope this survey could lay a sound and valuable foundation for future\ndevelopment of multimodal image synthesis and editing. A project associated\nwith this survey is available at https://github.com/fnzhan/MISE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1\">Adam Kortylewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Imitation Learning from Video using a State Observer. (arXiv:2202.00243v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2202.00243","description":"<p>The imitation learning research community has recently made significant\nprogress towards the goal of enabling artificial agents to imitate behaviors\nfrom video demonstrations alone. However, current state-of-the-art approaches\ndeveloped for this problem exhibit high sample complexity due, in part, to the\nhigh-dimensional nature of video observations. Towards addressing this issue,\nwe introduce here a new algorithm called Visual Generative Adversarial\nImitation from Observation using a State Observer VGAIfO-SO. At its core,\nVGAIfO-SO seeks to address sample inefficiency using a novel, self-supervised\nstate observer, which provides estimates of lower-dimensional proprioceptive\nstate representations from high-dimensional images. We show experimentally in\nseveral continuous control environments that VGAIfO-SO is more sample efficient\nthan other IfO algorithms at learning from video-only demonstrations and can\nsometimes even achieve performance close to the Generative Adversarial\nImitation from Observation (GAIfO) algorithm that has privileged access to the\ndemonstrator's proprioceptive state information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1\">Faraz Torabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining the manifolds of deep generative models for multiple data-consistent solutions of ill-posed tomographic imaging problems. (arXiv:2202.05311v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.05311","description":"<p>Tomographic imaging is in general an ill-posed inverse problem. Typically, a\nsingle regularized image estimate of the sought-after object is obtained from\ntomographic measurements. However, there may be multiple objects that are all\nconsistent with the same measurement data. The ability to generate such\nalternate solutions is important because it may enable new assessments of\nimaging systems. In principle, this can be achieved by means of posterior\nsampling methods. In recent years, deep neural networks have been employed for\nposterior sampling with promising results. However, such methods are not yet\nfor use with large-scale tomographic imaging applications. On the other hand,\nempirical sampling methods may be computationally feasible for large-scale\nimaging systems and enable uncertainty quantification for practical\napplications. Empirical sampling involves solving a regularized inverse problem\nwithin a stochastic optimization framework to obtain alternate data-consistent\nsolutions. In this work, a new empirical sampling method is proposed that\ncomputes multiple solutions of a tomographic inverse problem that are\nconsistent with the same acquired measurement data. The method operates by\nrepeatedly solving an optimization problem in the latent space of a style-based\ngenerative adversarial network (StyleGAN), and was inspired by the Photo\nUpsampling via Latent Space Exploration (PULSE) method that was developed for\nsuper-resolution tasks. The proposed method is demonstrated and analyzed via\nnumerical studies that involve two stylized tomographic imaging modalities.\nThese studies establish the ability of the method to perform efficient\nempirical sampling and uncertainty quantification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villa_U/0/1/0/all/0/1\">Umberto Villa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Multi-Object Dynamics with Compositional Neural Radiance Fields. (arXiv:2202.11855v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11855","description":"<p>We present a method to learn compositional multi-object dynamics models from\nimage observations based on implicit object encoders, Neural Radiance Fields\n(NeRFs), and graph neural networks. NeRFs have become a popular choice for\nrepresenting scenes due to their strong 3D prior. However, most NeRF approaches\nare trained on a single scene, representing the whole scene with a global\nmodel, making generalization to novel scenes, containing different numbers of\nobjects, challenging. Instead, we present a compositional, object-centric\nauto-encoder framework that maps multiple views of the scene to a set of latent\nvectors representing each object separately. The latent vectors parameterize\nindividual NeRFs from which the scene can be reconstructed. Based on those\nlatent vectors, we train a graph neural network dynamics model in the latent\nspace to achieve compositionality for dynamics prediction. A key feature of our\napproach is that the latent vectors are forced to encode 3D information through\nthe NeRF decoder, which enables us to incorporate structural priors in learning\nthe dynamics models, making long-term predictions more stable compared to\nseveral baselines. Simulated and real world experiments show that our method\ncan model and learn the dynamics of compositional scenes including rigid and\ndeformable objects. Video: https://dannydriess.github.io/compnerfdyn/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1\">Danny Driess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunzhu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tedrake_R/0/1/0/all/0/1\">Russ Tedrake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toussaint_M/0/1/0/all/0/1\">Marc Toussaint</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object discovery and representation networks. (arXiv:2203.08777v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08777","description":"<p>The promise of self-supervised learning (SSL) is to leverage large amounts of\nunlabeled data to solve complex tasks. While there has been excellent progress\nwith simple, image-level learning, recent methods have shown the advantage of\nincluding knowledge of image structure. However, by introducing hand-crafted\nimage segmentations to define regions of interest, or specialized augmentation\nstrategies, these methods sacrifice the simplicity and generality that makes\nSSL so powerful. Instead, we propose a self-supervised learning paradigm that\ndiscovers this image structure by itself. Our method, Odin, couples object\ndiscovery and representation networks to discover meaningful image\nsegmentations without any supervision. The resulting learning paradigm is\nsimpler, less brittle, and more general, and achieves state-of-the-art transfer\nlearning results for object detection and instance segmentation on COCO, and\nsemantic segmentation on PASCAL and Cityscapes, while strongly surpassing\nsupervised pre-training for video segmentation on DAVIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1\">Olivier J. H&#xe9;naff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1\">Skanda Koppula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelhamer_E/0/1/0/all/0/1\">Evan Shelhamer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoran_D/0/1/0/all/0/1\">Daniel Zoran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arandjelovic_R/0/1/0/all/0/1\">Relja Arandjelovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-fidelity GAN Inversion with Padding Space. (arXiv:2203.11105v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11105","description":"<p>Inverting a Generative Adversarial Network (GAN) facilitates a wide range of\nimage editing tasks using pre-trained generators. Existing methods typically\nemploy the latent space of GANs as the inversion space yet observe the\ninsufficient recovery of spatial details. In this work, we propose to involve\nthe padding space of the generator to complement the latent space with spatial\ninformation. Concretely, we replace the constant padding (e.g., usually zeros)\nused in convolution layers with some instance-aware coefficients. In this way,\nthe inductive bias assumed in the pre-trained model can be appropriately\nadapted to fit each individual image. Through learning a carefully designed\nencoder, we manage to improve the inversion quality both qualitatively and\nquantitatively, outperforming existing alternatives. We then demonstrate that\nsuch a space extension barely affects the native GAN manifold, hence we can\nstill reuse the prior knowledge learned by GANs for various downstream\napplications. Beyond the editing tasks explored in prior arts, our approach\nallows a more flexible image manipulation, such as the separate control of face\ncontour and facial details, and enables a novel editing manner where users can\ncustomize their own manipulations highly efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qingyan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yinghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiapeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.13883","description":"<p>As social media platforms are evolving from text-based forums into\nmulti-modal environments, the nature of misinformation in social media is also\nchanging accordingly. Taking advantage of the fact that visual modalities such\nas images and videos are more favorable and attractive to the users, and\ntextual contents are sometimes skimmed carelessly, misinformation spreaders\nhave recently targeted contextual correlations between modalities e.g., text\nand image. Thus, many research efforts have been put into development of\nautomatic techniques for detecting possible cross-modal discordances in\nweb-based media. In this work, we aim to analyze, categorize and identify\nexisting approaches in addition to challenges and shortcomings they face in\norder to unearth new opportunities in furthering the research in the field of\nmulti-modal misinformation detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1\">Sara Abdali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedVLN: Privacy-preserving Federated Vision-and-Language Navigation. (arXiv:2203.14936v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.14936","description":"<p>Data privacy is a central problem for embodied agents that can perceive the\nenvironment, communicate with humans, and act in the real world. While helping\nhumans complete tasks, the agent may observe and process sensitive information\nof users, such as house environments, human activities, etc. In this work, we\nintroduce privacy-preserving embodied agent learning for the task of\nVision-and-Language Navigation (VLN), where an embodied agent navigates house\nenvironments by following natural language instructions. We view each house\nenvironment as a local client, which shares nothing other than local updates\nwith the cloud server and other clients, and propose a novel federated\nvision-and-language navigation (FedVLN) framework to protect data privacy\nduring both training and pre-exploration. Particularly, we propose a\ndecentralized training strategy to limit the data of each client to its local\nmodel training and a federated pre-exploration method to do partial model\naggregation to improve model generalizability to unseen environments. Extensive\nresults on R2R and RxR datasets show that under our FedVLN framework,\ndecentralized VLN models achieve comparable results with centralized training\nwhile protecting seen environment privacy, and federated pre-exploration\nsignificantly outperforms centralized pre-exploration while preserving unseen\nenvironment privacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPS-NeRF: Generalizable 3D Human Rendering from Multiview Images. (arXiv:2203.16875v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16875","description":"<p>There has been rapid progress recently on 3D human rendering, including novel\nview synthesis and pose animation, based on the advances of neural radiance\nfields (NeRF). However, most existing methods focus on person-specific training\nand their training typically requires multi-view videos. This paper deals with\na new challenging task -- rendering novel views and novel poses for a person\nunseen in training, using only multiview images as input. For this task, we\npropose a simple yet effective method to train a generalizable NeRF with\nmultiview images as conditional input. The key ingredient is a dedicated\nrepresentation combining a canonical NeRF and a volume deformation scheme.\nUsing a canonical space enables our method to learn shared properties of human\nand easily generalize to different people. Volume deformation is used to\nconnect the canonical space with input and target images and query image\nfeatures for radiance and density prediction. We leverage the parametric 3D\nhuman model fitted on the input images to derive the deformation, which works\nquite well in practice when combined with our canonical NeRF. The experiments\non both real and synthetic data with the novel view synthesis and pose\nanimation tasks collectively demonstrate the efficacy of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiangjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jongyoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation. (arXiv:2204.00833v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00833","description":"<p>Pixel synthesis is a promising research paradigm for image generation, which\ncan well exploit pixel-wise prior knowledge for generation. However, existing\nmethods still suffer from excessive memory footprint and computation overhead.\nIn this paper, we propose a progressive pixel synthesis network towards\nefficient image generation, coined as PixelFolder. Specifically, PixelFolder\nformulates image generation as a progressive pixel regression problem and\nsynthesizes images via a multi-stage structure, which can greatly reduce the\noverhead caused by large tensor transformations. In addition, we introduce\nnovel pixel folding operations to further improve model efficiency while\nmaintaining pixel-wise prior knowledge for end-to-end regression. With these\ninnovative designs, we greatly reduce the expenditure of pixel synthesis, e.g.,\nreducing 89% computation and 53% parameters compared with the latest pixel\nsynthesis method CIPS. To validate our approach, we conduct extensive\nexperiments on two benchmark datasets, namely FFHQ and LSUN Church. The\nexperimental results show that with much less expenditure, PixelFolder obtains\nnew state-of-the-art (SOTA) performance on two benchmark datasets, i.e., 3.77\nFID and 2.45 FID on FFHQ and LSUN Church, respectively.Meanwhile, PixelFolder\nis also more efficient than the SOTA methods like StyleGAN2, reducing about 72%\ncomputation and 31% parameters, respectively. These results greatly validate\nthe effectiveness of the proposed PixelFolder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yiyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaoshuai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Vision Transformers by Revisiting High-frequency Components. (arXiv:2204.00993v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00993","description":"<p>The transformer models have shown promising effectiveness in dealing with\nvarious vision tasks. However, compared with training Convolutional Neural\nNetwork (CNN) models, training Vision Transformer (ViT) models is more\ndifficult and relies on the large-scale training set. To explain this\nobservation we make a hypothesis that \\textit{ViT models are less effective in\ncapturing the high-frequency components of images than CNN models}, and verify\nit by a frequency analysis. Inspired by this finding, we first investigate the\neffects of existing techniques for improving ViT models from a new frequency\nperspective, and find that the success of some techniques (e.g., RandAugment)\ncan be attributed to the better usage of the high-frequency components. Then,\nto compensate for this insufficient ability of ViT models, we propose HAT,\nwhich directly augments high-frequency components of images via adversarial\ntraining. We show that HAT can consistently boost the performance of various\nViT models (e.g., +1.2% for ViT-B, +0.5% for Swin-B), and especially enhance\nthe advanced model VOLO-D5 to 87.3% that only uses ImageNet-1K data, and the\nsuperiority can also be maintained on out-of-distribution data and transferred\nto downstream tasks. The code is available at:\nhttps://github.com/jiawangbai/HAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiawang Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Co-Teaching for Unsupervised Domain Adaptation and Expansion. (arXiv:2204.01210v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01210","description":"<p>Unsupervised Domain Adaptation (UDA) is known to trade a model's performance\non a source domain for improving its performance on a target domain. To resolve\nthe issue, Unsupervised Domain Expansion (UDE) has been proposed recently to\nadapt the model for the target domain as UDA does, and in the meantime maintain\nits performance on the source domain. For both UDA and UDE, a model tailored to\na given domain, let it be the source or the target domain, is assumed to well\nhandle samples from the given domain. We question the assumption by reporting\nthe existence of cross-domain visual ambiguity: Due to the lack of a crystally\nclear boundary between the two domains, samples from one domain can be visually\nclose to the other domain. We exploit this finding and accordingly propose in\nthis paper Co-Teaching (CT) that consists of knowledge distillation based CT\n(kdCT) and mixup based CT (miCT). Specifically, kdCT transfers knowledge from a\nleader-teacher network and an assistant-teacher network to a student network,\nso the cross-domain visual ambiguity will be better handled by the student.\nMeanwhile, miCT further enhances the generalization ability of the student.\nComprehensive experiments on two image-classification benchmarks and two\ndriving-scene-segmentation benchmarks justify the viability of the proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_K/0/1/0/all/0/1\">Kaibin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1\">Qijie Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xirong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TALLFormer: Temporal Action Localization with a Long-memory Transformer. (arXiv:2204.01680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01680","description":"<p>Most modern approaches in temporal action localization divide this problem\ninto two parts: (i) short-term feature extraction and (ii) long-range temporal\nboundary localization. Due to the high GPU memory cost caused by processing\nlong untrimmed videos, many methods sacrifice the representational power of the\nshort-term feature extractor by either freezing the backbone or using a small\nspatial video resolution. This issue becomes even worse with the recent video\ntransformer models, many of which have quadratic memory complexity. To address\nthese issues, we propose TALLFormer, a memory-efficient and end-to-end\ntrainable Temporal Action Localization Transformer with Long-term memory. Our\nlong-term memory mechanism eliminates the need for processing hundreds of\nredundant video frames during each training iteration, thus, significantly\nreducing the GPU memory consumption and training time. These efficiency savings\nallow us (i) to use a powerful video transformer feature extractor without\nfreezing the backbone or reducing the spatial video resolution, while (ii) also\nmaintaining long-range temporal boundary localization capability. With only RGB\nframes as input and no external action recognition classifier, TALLFormer\noutperforms previous state-of-the-arts by a large margin, achieving an average\nmAP of 59.1% on THUMOS14 and 35.6% on ActivityNet-1.3. The code is public\navailable: https://github.com/klauscc/TALLFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Feng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1\">Gedas Bertasius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Baselines for Image Restoration. (arXiv:2204.04676v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04676","description":"<p>Although there have been significant advances in the field of image\nrestoration recently, the system complexity of the state-of-the-art (SOTA)\nmethods is increasing as well, which may hinder the convenient analysis and\ncomparison of methods. In this paper, we propose a simple baseline that exceeds\nthe SOTA methods and is computationally efficient. To further simplify the\nbaseline, we reveal that the nonlinear activation functions, e.g. Sigmoid,\nReLU, GELU, Softmax, etc. are not necessary: they could be replaced by\nmultiplication or removed. Thus, we derive a Nonlinear Activation Free Network,\nnamely NAFNet, from the baseline. SOTA results are achieved on various\nchallenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring),\nexceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs;\n40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28\ndB with less than half of its computational costs. The code and the pre-trained\nmodels are released at https://github.com/megvii-research/NAFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06681","description":"<p>The MINSU(Mobile Inventory and Scanning Unit) algorithm uses the\ncomputational vision analysis method to record the residual quantity/fullness\nof the cabinet. To do so, it goes through a five-step method: object detection,\nforeground subtraction, K-means clustering, percentage estimation, and\ncounting. The input image goes through the object detection method to analyze\nthe specific position of the cabinets in terms of coordinates. After doing so,\nit goes through the foreground subtraction method to make the image more\nfocus-able to the cabinet itself by removing the background (some manual work\nmay have to be done such as selecting the parts that were not grab cut by the\nalgorithm). In the K-means clustering method, the multi-colored image turns\ninto a 3 colored monotonous image for quicker and more accurate analysis. At\nlast, the image goes through percentage estimation and counting. In these two\nmethods, the proportion that the material inside the cabinet is found in\npercentage which then is used to approximate the number of materials inside.\nHad this project been successful, the residual quantity management could solve\nthe problem addressed earlier in the introduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_J/0/1/0/all/0/1\">Jihoon Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Byungkon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyeob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seunghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TJ4DRadSet: A 4D Radar Dataset for Autonomous Driving. (arXiv:2204.13483v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.13483","description":"<p>The next-generation high-resolution automotive radar (4D radar) can provide\nadditional elevation measurement and denser point clouds, which has great\npotential for 3D sensing in autonomous driving. In this paper, we introduce a\ndataset named TJ4DRadSet with 4D radar points for autonomous driving research.\nThe dataset was collected in various driving scenarios, with a total of 7757\nsynchronized frames in 44 consecutive sequences, which are well annotated with\n3D bounding boxes and track ids. We provide a 4D radar-based 3D object\ndetection baseline for our dataset to demonstrate the effectiveness of deep\nlearning methods for 4D radar point clouds. The dataset can be accessed via the\nfollowing link: https://github.com/TJRadarLab/TJ4DRadSet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lianqing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhixiong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xichan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_K/0/1/0/all/0/1\">Kai Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weiqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sihan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_M/0/1/0/all/0/1\">Mengyue Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Libo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jie Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-RealSR: Metric Learning based Interactive Modulation for Real-World Super-Resolution. (arXiv:2205.05065v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.05065","description":"<p>Interactive image restoration aims to restore images by adjusting several\ncontrolling coefficients, which determine the restoration strength. Existing\nmethods are restricted in learning the controllable functions under the\nsupervision of known degradation types and levels. They usually suffer from a\nsevere performance drop when the real degradation is different from their\nassumptions. Such a limitation is due to the complexity of real-world\ndegradations, which can not provide explicit supervision to the interactive\nmodulation during training. However, how to realize the interactive modulation\nin real-world super-resolution has not yet been studied. In this work, we\npresent a Metric Learning based Interactive Modulation for Real-World\nSuper-Resolution (MM-RealSR). Specifically, we propose an unsupervised\ndegradation estimation strategy to estimate the degradation level in real-world\nscenarios. Instead of using known degradation levels as explicit supervision to\nthe interactive mechanism, we propose a metric learning strategy to map the\nunquantifiable degradation levels in real-world scenarios to a metric space,\nwhich is trained in an unsupervised manner. Moreover, we introduce an anchor\npoint strategy in the metric learning process to normalize the distribution of\nmetric space. Extensive experiments demonstrate that the proposed MM-RealSR\nachieves excellent modulation and restoration performance in real-world\nsuper-resolution. Codes are available at\nhttps://github.com/TencentARC/MM-RealSR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mou_C/0/1/0/all/0/1\">Chong Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanze Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VQFR: Blind Face Restoration with Vector-Quantized Dictionary and Parallel Decoder. (arXiv:2205.06803v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06803","description":"<p>Although generative facial prior and geometric prior have recently\ndemonstrated high-quality results for blind face restoration, producing\nfine-grained facial details faithful to inputs remains a challenging problem.\nMotivated by the classical dictionary-based methods and the recent vector\nquantization (VQ) technique, we propose a VQ-based face restoration method -\nVQFR. VQFR takes advantage of high-quality low-level feature banks extracted\nfrom high-quality faces and can thus help recover realistic facial details.\nHowever, the simple application of the VQ codebook cannot achieve good results\nwith faithful details and identity preservation. Therefore, we further\nintroduce two special network designs. 1). We first investigate the compression\npatch size in the VQ codebook and find that the VQ codebook designed with a\nproper compression patch size is crucial to balance the quality and fidelity.\n2). To further fuse low-level features from inputs while not \"contaminating\"\nthe realistic details generated from the VQ codebook, we proposed a parallel\ndecoder consisting of a texture decoder and a main decoder. Those two decoders\nthen interact with a texture warping module with deformable convolution.\nEquipped with the VQ codebook as a facial detail dictionary and the parallel\ndecoder design, the proposed VQFR can largely enhance the restored quality of\nfacial details while keeping the fidelity to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuchao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liangbin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01714","description":"<p>Large text-guided diffusion models, such as DALLE-2, are able to generate\nstunning photorealistic images given natural language descriptions. While such\nmodels are highly flexible, they struggle to understand the composition of\ncertain concepts, such as confusing the attributes of different objects or\nrelations between objects. In this paper, we propose an alternative structured\napproach for compositional generation using diffusion models. An image is\ngenerated by composing a set of diffusion models, with each of them modeling a\ncertain component of the image. To do this, we interpret diffusion models as\nenergy-based models in which the data distributions defined by the energy\nfunctions may be explicitly combined. The proposed method can generate scenes\nat test time that are substantially more complex than those seen in training,\ncomposing sentence descriptions, object relations, human facial attributes, and\neven generalizing to new combinations that are rarely seen in the real world.\nWe further illustrate how our approach may be used to compose pre-trained\ntext-guided diffusion models and generate photorealistic images containing all\nthe details described in the input descriptions, including the binding of\ncertain object attributes that have been shown difficult for DALLE-2. These\nresults point to the effectiveness of the proposed method in promoting\nstructured generalization for visual generation. Project page:\nhttps://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperTickets: Drawing Task-Agnostic Lottery Tickets from Supernets via Jointly Architecture Searching and Parameter Pruning. (arXiv:2207.03677v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.03677","description":"<p>Neural architecture search (NAS) has demonstrated amazing success in\nsearching for efficient deep neural networks (DNNs) from a given supernet. In\nparallel, the lottery ticket hypothesis has shown that DNNs contain small\nsubnetworks that can be trained from scratch to achieve a comparable or higher\naccuracy than original DNNs. As such, it is currently a common practice to\ndevelop efficient DNNs via a pipeline of first search and then prune.\nNevertheless, doing so often requires a search-train-prune-retrain process and\nthus prohibitive computational cost. In this paper, we discover for the first\ntime that both efficient DNNs and their lottery subnetworks (i.e., lottery\ntickets) can be directly identified from a supernet, which we term as\nSuperTickets, via a two-in-one training scheme with jointly architecture\nsearching and parameter pruning. Moreover, we develop a progressive and unified\nSuperTickets identification strategy that allows the connectivity of\nsubnetworks to change during supernet training, achieving better accuracy and\nefficiency trade-offs than conventional sparse training. Finally, we evaluate\nwhether such identified SuperTickets drawn from one task can transfer well to\nother tasks, validating their potential of handling multiple tasks\nsimultaneously. Extensive experiments and ablation studies on three tasks and\nfour benchmark datasets validate that our proposed SuperTickets achieve boosted\naccuracy and efficiency trade-offs than both typical NAS and pruning pipelines,\nregardless of having retraining or not. Codes and pretrained models are\navailable at https://github.com/RICE-EIC/SuperTickets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoran You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Baopu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhanyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xu Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingyan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PseudoClick: Interactive Image Segmentation with Click Imitation. (arXiv:2207.05282v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.05282","description":"<p>The goal of click-based interactive image segmentation is to obtain precise\nobject segmentation masks with limited user interaction, i.e., by a minimal\nnumber of user clicks. Existing methods require users to provide all the\nclicks: by first inspecting the segmentation mask and then providing points on\nmislabeled regions, iteratively. We ask the question: can our model directly\npredict where to click, so as to further reduce the user interaction cost? To\nthis end, we propose {\\PseudoClick}, a generic framework that enables existing\nsegmentation networks to propose candidate next clicks. These automatically\ngenerated clicks, termed pseudo clicks in this work, serve as an imitation of\nhuman clicks to refine the segmentation mask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Meng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Planche_B/0/1/0/all/0/1\">Benjamin Planche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaGAN: Towards Diverse Few-shot Image Generation with Sample-Specific Delta. (arXiv:2207.10271v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.10271","description":"<p>Learning to generate new images for a novel category based on only a few\nimages, named as few-shot image generation, has attracted increasing research\ninterest. Several state-of-the-art works have yielded impressive results, but\nthe diversity is still limited. In this work, we propose a novel Delta\nGenerative Adversarial Network (DeltaGAN), which consists of a reconstruction\nsubnetwork and a generation subnetwork. The reconstruction subnetwork captures\nintra-category transformation, i.e., delta, between same-category pairs. The\ngeneration subnetwork generates sample-specific delta for an input image, which\nis combined with this input image to generate a new image within the same\ncategory. Besides, an adversarial delta matching loss is designed to link the\nabove two subnetworks together. Extensive experiments on six benchmark datasets\ndemonstrate the effectiveness of our proposed method. Our code is available at\nhttps://github.com/bcmi/DeltaGAN-Few-Shot-Image-Generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yan Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianfu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Behind Every Domain There is a Shift: Adapting Distortion-aware Vision Transformers for Panoramic Semantic Segmentation. (arXiv:2207.11860v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.11860","description":"<p>In this paper, we address panoramic semantic segmentation, which provides a\nfull-view and dense-pixel understanding of surroundings in a holistic way.\nPanoramic segmentation is under-explored due to two critical challenges: (1)\nimage distortions and object deformations on panoramas; (2) lack of annotations\nfor training panoramic segmenters. To tackle these problems, we propose a\nTransformer for Panoramic Semantic Segmentation (Trans4PASS) architecture.\nFirst, to enhance distortion awareness, Trans4PASS, equipped with Deformable\nPatch Embedding (DPE) and Deformable MLP (DMLP) modules, is capable of handling\nobject deformations and image distortions whenever (before or after adaptation)\nand wherever (shallow or deep levels) by design. We further introduce the\nupgraded Trans4PASS+ model, featuring DMLPv2 with parallel token mixing to\nimprove the flexibility and generalizability in modeling discriminative cues.\nSecond, we propose a Mutual Prototypical Adaptation (MPA) strategy for\nunsupervised domain adaptation. Third, aside from Pinhole-to-Panoramic\n(Pin2Pan) adaptation, we create a new dataset (SynPASS) with 9,080 panoramic\nimages to explore a Synthetic-to-Real (Syn2Real) adaptation scheme in 360{\\deg}\nimagery. Extensive experiments are conducted, which cover indoor and outdoor\nscenarios, and each of them is investigated with Pin2Pan and Syn2Real regimens.\nTrans4PASS+ achieves state-of-the-art performances on four domain adaptive\npanoramic semantic segmentation benchmarks. Code is available at\nhttps://github.com/jamycheung/Trans4PASS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiss_S/0/1/0/all/0/1\">Simon Rei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chaoxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Haodong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PTGCF: Printing Texture Guided Color Fusion for Impressionism Oil Painting Style Rendering. (arXiv:2207.12585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.12585","description":"<p>As a major branch of Non-Photorealistic Rendering (NPR), image stylization\nmainly uses the computer algorithms to render a photo into an artistic\npainting. Recent work has shown that the extraction of style information such\nas stroke texture and color of the target style image is the key to image\nstylization. Given its stroke texture and color characteristics, a new stroke\nrendering method is proposed, which fully considers the tonal characteristics\nand the representative color of the original oil painting, in order to fit the\ntone of the original oil painting image into the stylized image and make it\nclose to the artist's creative effect. The experiments have validated the\nefficacy of the proposed model. This method would be more suitable for the\nworks of pointillism painters with a relatively uniform sense of direction,\nespecially for natural scenes. When the original painting brush strokes have a\nclearer sense of direction, using this method to simulate brushwork texture\nfeatures can be less satisfactory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1\">Jing Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li&#x27;e Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yijun Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis. (arXiv:2207.13064v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.13064","description":"<p>As tools for content editing mature, and artificial intelligence (AI) based\nalgorithms for synthesizing media grow, the presence of manipulated content\nacross online media is increasing. This phenomenon causes the spread of\nmisinformation, creating a greater need to distinguish between \"real\" and\n\"manipulated\" content. To this end, we present VideoSham, a dataset consisting\nof 826 videos (413 real and 413 manipulated). Many of the existing deepfake\ndatasets focus exclusively on two types of facial manipulations -- swapping\nwith a different subject's face or altering the existing face. VideoSham, on\nthe other hand, contains more diverse, context-rich, and human-centric,\nhigh-resolution videos manipulated using a combination of 6 different spatial\nand temporal attacks. Our analysis shows that state-of-the-art manipulation\ndetection algorithms only work for a few specific attacks and do not scale well\non VideoSham. We performed a user study on Amazon Mechanical Turk with 1200\nparticipants to understand if they can differentiate between the real and\nmanipulated videos in VideoSham. Finally, we dig deeper into the strengths and\nweaknesses of performances by humans and SOTA-algorithms to identify gaps that\nneed to be filled with better AI algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1\">Trisha Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Ritwik Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swaminathan_V/0/1/0/all/0/1\">Viswanathan Swaminathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization of Batch Whitening by Convolutional Unit Optimization. (arXiv:2108.10629v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2108.10629","description":"<p>Batch Whitening is a technique that accelerates and stabilizes training by\ntransforming input features to have a zero mean (Centering) and a unit variance\n(Scaling), and by removing linear correlation between channels (Decorrelation).\nIn commonly used structures, which are empirically optimized with Batch\nNormalization, the normalization layer appears between convolution and\nactivation function. Following Batch Whitening studies have employed the same\nstructure without further analysis; even Batch Whitening was analyzed on the\npremise that the input of a linear layer is whitened. To bridge the gap, we\npropose a new Convolutional Unit that is in line with the theory, and our\nmethod generally improves the performance of Batch Whitening. Moreover, we show\nthe inefficacy of the original Convolutional Unit by investigating rank and\ncorrelation of features. As our method is employable off-the-shelf whitening\nmodules, we use Iterative Normalization (IterNorm), the state-of-the-art\nwhitening module, and obtain significantly improved performance on five image\nclassification datasets: CIFAR-10, CIFAR-100, CUB-200-2011, Stanford Dogs, and\nImageNet. Notably, we verify that our method improves stability and performance\nof whitening when using large learning rate, group size, and iteration number.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yooshin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hanbyel Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}