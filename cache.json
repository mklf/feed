{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Cross-linguistically Consistent Semantic and Syntactic Annotation of Child-directed Speech. (arXiv:2109.10952v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10952","description":"<p>While corpora of child speech and child-directed speech (CDS) have enabled\nmajor contributions to the study of child language acquisition, semantic\nannotation for such corpora is still scarce and lacks a uniform standard. We\ncompile two CDS corpora with sentential logical forms, one in English and the\nother in Hebrew. In compiling the corpora we employ a methodology that enforces\na cross-linguistically consistent representation, building on recent advances\nin dependency representation and semantic parsing. The corpora are based on a\nsizable portion of Brown's Adam corpus from CHILDES (about 80% of its\nchild-directed utterances), and to all child-directed utterances from Berman's\nHebrew CHILDES corpus Hagar.\n</p>\n<p>We begin by annotating the corpora with the Universal Dependencies (UD)\nscheme for syntactic annotation, motivated by its applicability to a wide\nvariety of domains and languages. We then proceed by applying an automatic\nmethod for transducing sentential logical forms (LFs) from UD structures. The\ntwo representations have complementary strengths: UD structures are\nlanguage-neutral and support direct annotation, whereas LFs are neutral as to\nthe interface between syntax and semantics, and transparently encode semantic\ndistinctions. We verify the quality of the annotated UD annotation using an\ninter-annotator agreement study. We then demonstrate the utility of the\ncompiled corpora through a longitudinal corpus study of the prevalence of\ndifferent syntactic and semantic phenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szubert_I/0/1/0/all/0/1\">Ida Szubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibbon_S/0/1/0/all/0/1\">Samuel Gibbon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwater_S/0/1/0/all/0/1\">Sharon Goldwater</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Fact-checking with Human-in-the-Loop. (arXiv:2109.10992v1 [cs.CL])","link":"http://arxiv.org/abs/2109.10992","description":"<p>Researchers have been investigating automated solutions for fact-checking in\na variety of fronts. However, current approaches often overlook the fact that\nthe amount of information released every day is escalating, and a large amount\nof them overlap. Intending to accelerate fact-checking, we bridge this gap by\ngrouping similar messages and summarizing them into aggregated claims.\nSpecifically, we first clean a set of social media posts (e.g., tweets) and\nbuild a graph of all posts based on their semantics; Then, we perform two\nclustering methods to group the messages for further claim summarization. We\nevaluate the summaries both quantitatively with ROUGE scores and qualitatively\nwith human evaluation. We also generate a graph of summaries to verify that\nthere is no significant overlap among them. The results reduced 28,818 original\nmessages to 700 summary claims, showing the potential to speed up the\nfact-checking process by organizing and selecting representative claims from\nmassive disorganized and redundant messages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vega_Oliveros_D/0/1/0/all/0/1\">Didier Vega-Oliveros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seibt_T/0/1/0/all/0/1\">Tais Seibt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1\">Anderson Rocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT. (arXiv:2109.11010v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11010","description":"<p>Alzheimers disease is a fatal progressive brain disorder that worsens with\ntime. It is high time we have inexpensive and quick clinical diagnostic\ntechniques for early detection and care. In previous studies, various Machine\nLearning techniques and Pre-trained Deep Learning models have been used in\nconjunction with the extraction of various acoustic and linguistic features.\nOur study focuses on three models for the classification task in the ADReSS\n(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021\nChallenge. We use the well-balanced dataset provided by the ADReSS Challenge\nfor training and validating our models. Model 1 uses various acoustic features\nfrom the eGeMAPs feature-set, Model 2 uses various linguistic features that we\ngenerated from auto-generated transcripts and Model 3 uses the auto-generated\ntranscripts directly to extract features using a Pre-trained BERT and TF-IDF.\nThese models are described in detail in the models section.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valsaraj_A/0/1/0/all/0/1\">Akshay Valsaraj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madala_I/0/1/0/all/0/1\">Ithihas Madala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_N/0/1/0/all/0/1\">Nikhil Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baths_V/0/1/0/all/0/1\">Veeky Baths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Decomposition for Table-based Fact Verification. (arXiv:2109.11020v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11020","description":"<p>Fact verification based on structured data is challenging as it requires\nmodels to understand both natural language and symbolic operations performed\nover tables. Although pre-trained language models have demonstrated a strong\ncapability in verifying simple statements, they struggle with complex\nstatements that involve multiple operations. In this paper, we improve fact\nverification by decomposing complex statements into simpler subproblems.\nLeveraging the programs synthesized by a weakly supervised semantic parser, we\npropose a program-guided approach to constructing a pseudo dataset for\ndecomposition model training. The subproblems, together with their predicted\nanswers, serve as the intermediate evidence to enhance our fact verification\nmodel. Experiments show that our proposed approach achieves the new\nstate-of-the-art performance, an 82.7\\% accuracy, on the TabFact benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Poisson Stochastic Beam Search. (arXiv:2109.11034v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11034","description":"<p>Beam search is the default decoding strategy for many sequence generation\ntasks in NLP. The set of approximate K-best items returned by the algorithm is\na useful summary of the distribution for many applications; however, the\ncandidates typically exhibit high overlap and may give a highly biased estimate\nfor expectations under our model. These problems can be addressed by instead\nusing stochastic decoding strategies. In this work, we propose a new method for\nturning beam search into a stochastic process: Conditional Poisson stochastic\nbeam search. Rather than taking the maximizing set at each iteration, we sample\nK candidates without replacement according to the conditional Poisson sampling\ndesign. We view this as a more natural alternative to Kool et. al. 2019's\nstochastic beam search (SBS). Furthermore, we show how samples generated under\nthe CPSBS design can be used to build consistent estimators and sample diverse\nsets from sequence models. In our experiments, we observe CPSBS produces lower\nvariance and more efficient estimators than SBS, even showing improvements in\nhigh entropy settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Afra Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viera_T/0/1/0/all/0/1\">Tim Viera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models. (arXiv:2109.11058v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11058","description":"<p>Prior work has shown that structural supervision helps English language\nmodels learn generalizations about syntactic phenomena such as subject-verb\nagreement. However, it remains unclear if such an inductive bias would also\nimprove language models' ability to learn grammatical dependencies in\ntypologically different languages. Here we investigate this question in\nMandarin Chinese, which has a logographic, largely syllable-based writing\nsystem; different word order; and sparser morphology than English. We train\nLSTMs, Recurrent Neural Network Grammars, Transformer language models, and\nTransformer-parameterized generative parsing models on two Mandarin Chinese\ndatasets of different sizes. We evaluate the models' ability to learn different\naspects of Mandarin grammar that assess syntactic and semantic relationships.\nWe find suggestive evidence that structural supervision helps with representing\nsyntactic state across intervening content and improves performance in low-data\nsettings, suggesting that the benefits of hierarchical inductive biases in\nacquiring dependency relationships may extend beyond English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jennifer Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1\">Peng Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender. (arXiv:2109.11061v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11061","description":"<p>Individuals signal aspects of their identity and beliefs through linguistic\nchoices. Studying these choices in aggregate allows us to examine large-scale\nattitude shifts within a population. Here, we develop computational methods to\nstudy word choice within a sociolinguistic lexical variable -- alternate words\nused to express the same concept -- in order to test for change in the United\nStates towards sexuality and gender. We examine two variables: i) referents to\nsignificant others, such as the word \"partner\" and ii) referents to an\nindefinite person, both of which could optionally be marked with gender. The\nlinguistic choices in each variable allow us to study increased rates of\nacceptances of gay marriage and gender equality, respectively. In longitudinal\nanalyses across Twitter and Reddit over 87M messages, we demonstrate that\nattitudes are changing but that these changes are driven by specific\ndemographics within the United States. Further, in a quasi-causal analysis, we\nshow that passages of Marriage Equality Acts in different states are drivers of\nlinguistic change.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+CH_Wang_S/0/1/0/all/0/1\">Sky CH-Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Actionable Conversational Quality Indicators for Improving Task-Oriented Dialog Systems. (arXiv:2109.11064v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11064","description":"<p>Automatic dialog systems have become a mainstream part of online customer\nservice. Many such systems are built, maintained, and improved by customer\nservice specialists, rather than dialog systems engineers and computer\nprogrammers. As conversations between people and machines become commonplace,\nit is critical to understand what is working, what is not, and what actions can\nbe taken to reduce the frequency of inappropriate system responses. These\nanalyses and recommendations need to be presented in terms that directly\nreflect the user experience rather than the internal dialog processing.\n</p>\n<p>This paper introduces and explains the use of Actionable Conversational\nQuality Indicators (ACQIs), which are used both to recognize parts of dialogs\nthat can be improved, and to recommend how to improve them. This combines\nbenefits of previous approaches, some of which have focused on producing dialog\nquality scoring while others have sought to categorize the types of errors the\ndialog system is making.\n</p>\n<p>We demonstrate the effectiveness of using ACQIs on LivePerson internal dialog\nsystems used in commercial customer service applications, and on the publicly\navailable CMU LEGOv2 conversational dataset (Raux et al. 2005). We report on\nthe annotation and analysis of conversational datasets showing which ACQIs are\nimportant to fix in various situations.\n</p>\n<p>The annotated datasets are then used to build a predictive model which uses a\nturn-based vector embedding of the message texts and achieves an 79% weighted\naverage f1-measure at the task of finding the correct ACQI for a given\nconversation. We predict that if such a model worked perfectly, the range of\npotential improvement actions a bot-builder must consider at each turn could be\nreduced by an average of 81%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Higgins_M/0/1/0/all/0/1\">Michael Higgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Widdows_D/0/1/0/all/0/1\">Dominic Widdows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brew_C/0/1/0/all/0/1\">Chris Brew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christian_G/0/1/0/all/0/1\">Gwen Christian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1\">Andrew Maurer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_M/0/1/0/all/0/1\">Matthew Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathi_S/0/1/0/all/0/1\">Sujit Mathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hazare_A/0/1/0/all/0/1\">Akshay Hazare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonev_G/0/1/0/all/0/1\">George Bonev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockey_B/0/1/0/all/0/1\">Beth Ann Hockey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howell_K/0/1/0/all/0/1\">Kristen Howell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_J/0/1/0/all/0/1\">Joe Bradley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Universal Dense Retrieval for Open-domain Question Answering. (arXiv:2109.11085v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11085","description":"<p>In open-domain question answering, a model receives a text question as input\nand searches for the correct answer using a large evidence corpus. The\nretrieval step is especially difficult as typical evidence corpora have\n\\textit{millions} of documents, each of which may or may not have the correct\nanswer to the question. Very recently, dense models have replaced sparse\nmethods as the de facto retrieval method. Rather than focusing on lexical\noverlap to determine similarity, dense methods build an encoding function that\ncaptures semantic similarity by learning from a small collection of\nquestion-answer or question-context pairs. In this paper, we investigate dense\nretrieval models in the context of open-domain question answering across\ndifferent input distributions. To do this, first we introduce an entity-rich\nquestion answering dataset constructed from Wikidata facts and demonstrate\ndense models are unable to generalize to unseen input question distributions.\nSecond, we perform analyses aimed at better understanding the source of the\nproblem and propose new training techniques to improve out-of-domain\nperformance on a wide variety of datasets. We encourage the field to further\ninvestigate the creation of a single, universal dense retrieval model that\ngeneralizes well across all input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles. (arXiv:2109.11087v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11087","description":"<p>A riddle is a question or statement with double or veiled meanings, followed\nby an unexpected answer. Solving riddle is a challenging task for both machine\nand human, testing the capability of understanding figurative, creative natural\nlanguage and reasoning with commonsense knowledge. We introduce BiRdQA, a\nbilingual multiple-choice question answering dataset with 6614 English riddles\nand 8751 Chinese riddles. For each riddle-answer pair, we provide four\ndistractors with additional information from Wikipedia. The distractors are\nautomatically generated at scale with minimal bias. Existing monolingual and\nmultilingual QA models fail to perform well on our dataset, indicating that\nthere is a long way to go before machine can beat human on solving tricky\nriddles. The dataset has been released to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing. (arXiv:2109.11105v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11105","description":"<p>We aim to identify how different components in the KD pipeline affect the\nresulting performance and how much the optimal KD pipeline varies across\ndifferent datasets/tasks, such as the data augmentation policy, the loss\nfunction, and the intermediate representation for transferring the knowledge\nbetween teacher and student. To tease apart their effects, we propose\nDistiller, a meta KD framework that systematically combines a broad range of\ntechniques across different stages of the KD pipeline, which enables us to\nquantify each component's contribution. Within Distiller, we unify commonly\nused objectives for distillation of intermediate representations under a\nuniversal mutual information (MI) objective and propose a class of MI-$\\alpha$\nobjective functions with better bias/variance trade-off for estimating the MI\nbetween the teacher and the student. On a diverse set of NLP datasets, the best\nDistiller configurations are identified via large-scale hyperparameter\noptimization. Our experiments reveal the following: 1) the approach used to\ndistill the intermediate representations is the most important factor in KD\nperformance, 2) among different objectives for intermediate distillation,\nMI-$\\alpha$ performs the best, and 3) data augmentation provides a large boost\nfor small training datasets or small student networks. Moreover, we find that\ndifferent datasets/tasks prefer different KD algorithms, and thus propose a\nsimple AutoDistiller algorithm that can recommend a good KD pipeline for a new\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xingjian Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">Jonas Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1\">Zha Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Language Model Meta-Pretraining. (arXiv:2109.11129v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11129","description":"<p>The success of pretrained cross-lingual language models relies on two\nessential abilities, i.e., generalization ability for learning downstream tasks\nin a source language, and cross-lingual transferability for transferring the\ntask knowledge to other languages. However, current methods jointly learn the\ntwo abilities in a single-phase cross-lingual pretraining process, resulting in\na trade-off between generalization and cross-lingual transfer. In this paper,\nwe propose cross-lingual language model meta-pretraining, which learns the two\nabilities in different training phases. Our method introduces an additional\nmeta-pretraining phase before cross-lingual pretraining, where the model learns\ngeneralization ability on a large-scale monolingual corpus. Then, the model\nfocuses on learning cross-lingual transfer on a multilingual corpus.\nExperimental results show that our method improves both generalization and\ncross-lingual transfer, and produces better-aligned representations across\ndifferent languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Luyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yu Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Online Learning from Human Feedback for Neural Machine Translation. (arXiv:2109.11136v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11136","description":"<p>We study the problem of online learning with human feedback in the\nhuman-in-the-loop machine translation, in which the human translators revise\nthe machine-generated translations and then the corrected translations are used\nto improve the neural machine translation (NMT) system. However, previous\nmethods require online model updating or additional translation memory networks\nto achieve high-quality performance, making them inflexible and inefficient in\npractice. In this paper, we propose a novel non-parametric online learning\nmethod without changing the model structure. This approach introduces two\nk-nearest-neighbor (KNN) modules: one module memorizes the human feedback,\nwhich is the correct sentences provided by human translators, while the other\nbalances the usage of the history human feedback and original NMT models\nadaptively. Experiments conducted on EMEA and JRC-Acquis benchmarks demonstrate\nthat our proposed method obtains substantial improvements on translation\naccuracy and achieves better adaptation performance with less repeating human\ncorrection operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1\">Haoran Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint speaker diarisation and tracking in switching state-space model. (arXiv:2109.11140v1 [cs.SD])","link":"http://arxiv.org/abs/2109.11140","description":"<p>Speakers may move around while diarisation is being performed. When a\nmicrophone array is used, the instantaneous locations of where the sounds\noriginated from can be estimated, and previous investigations have shown that\nsuch information can be complementary to speaker embeddings in the diarisation\ntask. However, these approaches often assume that speakers are fairly\nstationary throughout a meeting. This paper relaxes this assumption, by\nproposing to explicitly track the movements of speakers while jointly\nperforming diarisation within a unified model. A state-space model is proposed,\nwhere the hidden state expresses the identity of the current active speaker and\nthe predicted locations of all speakers. The model is implemented as a particle\nfilter. Experiments on a Microsoft rich meeting transcription task show that\nthe proposed joint location tracking and diarisation approach is able to\nperform comparably with other methods that use location information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Jeremy H. M. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yifan Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Information Extraction as a Unified Text-to-Triple Translation. (arXiv:2109.11171v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11171","description":"<p>We cast a suite of information extraction tasks into a text-to-triple\ntranslation framework. Instead of solving each task relying on task-specific\ndatasets and models, we formalize the task as a translation between\ntask-specific input text and output triples. By taking the task-specific input,\nwe enable a task-agnostic translation by leveraging the latent knowledge that a\npre-trained language model has about the task. We further demonstrate that a\nsimple pre-training task of predicting which relational information corresponds\nto which input text is an effective way to produce task-specific outputs. This\nenables the zero-shot transfer of our framework to downstream tasks. We study\nthe zero-shot performance of this framework on open information extraction\n(OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and\nfactual probe (Google-RE and T-REx). The model transfers non-trivially to most\ntasks and is often competitive with a fully supervised method without the need\nfor any task-specific training. For instance, we significantly outperform the\nF1 score of the supervised open information extraction without needing to use\nits training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Curriculum Learning in Unsupervised Neural Machine Translation. (arXiv:2109.11177v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11177","description":"<p>Back-translation (BT) has become one of the de facto components in\nunsupervised neural machine translation (UNMT), and it explicitly makes UNMT\nhave translation ability. However, all the pseudo bi-texts generated by BT are\ntreated equally as clean data during optimization without considering the\nquality diversity, leading to slow convergence and limited translation\nperformance. To address this problem, we propose a curriculum learning method\nto gradually utilize pseudo bi-texts based on their quality from multiple\ngranularities. Specifically, we first apply cross-lingual word embedding to\ncalculate the potential translation difficulty (quality) for the monolingual\nsentences. Then, the sentences are fed into UNMT from easy to hard batch by\nbatch. Furthermore, considering the quality of sentences/tokens in a particular\nbatch are also diverse, we further adopt the model itself to calculate the\nfine-grained quality scores, which are served as learning factors to balance\nthe contributions of different parts when computing loss and encourage the UNMT\nmodel to focus on pseudo data with higher quality. Experimental results on WMT\n14 En-Fr, WMT 16 En-De, WMT 16 En-Ro, and LDC En-Zh translation tasks\ndemonstrate that the proposed method achieves consistent improvements with\nfaster convergence speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinliang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization. (arXiv:2109.11199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11199","description":"<p>Within natural language processing tasks, linguistic knowledge can always\nserve an important role in assisting the model to learn excel representations\nand better guide the natural language generation. In this work, we develop a\nneural network based abstractive multi-document summarization (MDS) model which\nleverages dependency parsing to capture cross-positional dependencies and\ngrammatical structures. More concretely, we process the dependency information\ninto the linguistic-guided attention mechanism and further fuse it with the\nmulti-head attention for better feature representation. With the help of\nlinguistic signals, sentence-level relations can be correctly captured, thus\nimproving MDS performance. Our model has two versions based on Flat-Transformer\nand Hierarchical Transformer respectively. Empirical studies on both versions\ndemonstrate that this simple but effective method outperforms existing works on\nthe benchmark dataset. Extensive analyses examine different settings and\nconfigurations of the proposed model which provide a good reference to the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Congbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Emma Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shubham Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mingyu Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuzzy Generalised Quantifiers for Natural Language in Categorical Compositional Distributional Semantics. (arXiv:2109.11227v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11227","description":"<p>Recent work on compositional distributional models shows that bialgebras over\nfinite dimensional vector spaces can be applied to treat generalised\nquantifiers for natural language. That technique requires one to construct the\nvector space over powersets, and therefore is computationally costly. In this\npaper, we overcome this problem by considering fuzzy versions of quantifiers\nalong the lines of Zadeh, within the category of many valued relations. We show\nthat this category is a concrete instantiation of the compositional\ndistributional model. We show that the semantics obtained in this model is\nequivalent to the semantics of the fuzzy quantifiers of Zadeh. As a result, we\nare now able to treat fuzzy quantification without requiring a powerset\nconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dostal_M/0/1/0/all/0/1\">Matej Dostal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijnholds_G/0/1/0/all/0/1\">Gijs Wijnholds</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pregroup Grammars, their Syntax and Semantics. (arXiv:2109.11237v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11237","description":"<p>Pregroup grammars were developed in 1999 and stayed Lambek's preferred\nalgebraic model of grammar. The set-theoretic semantics of pregroups, however,\nfaces an ambiguity problem. In his latest book, Lambek suggests that this\nproblem might be overcome using finite dimensional vector spaces rather than\nsets. What is the right notion of composition in this setting, direct sum or\ntensor product of spaces?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sadrzadeh_M/0/1/0/all/0/1\">Mehrnoosh Sadrzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21. (arXiv:2109.11247v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11247","description":"<p>This paper describes the Volctrans' submission to the WMT21 news translation\nshared task for German-&gt;English translation. We build a parallel (i.e.,\nnon-autoregressive) translation system using the Glancing Transformer, which\nenables fast and accurate parallel decoding in contrast to the currently\nprevailing autoregressive models. To the best of our knowledge, this is the\nfirst parallel translation system that can be scaled to such a practical\nscenario like WMT competition. More importantly, our parallel translation\nsystem achieves the best BLEU score (35.0) on German-&gt;English translation task,\noutperforming all strong autoregressive counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1\">Lihua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zaixiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Question Generation Debias Question Answering Models? A Case Study on Question-Context Lexical Overlap. (arXiv:2109.11256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11256","description":"<p>Question answering (QA) models for reading comprehension have been\ndemonstrated to exploit unintended dataset biases such as question-context\nlexical overlap. This hinders QA models from generalizing to under-represented\nsamples such as questions with low lexical overlap. Question generation (QG), a\nmethod for augmenting QA datasets, can be a solution for such performance\ndegradation if QG can properly debias QA datasets. However, we discover that\nrecent neural QG models are biased towards generating questions with high\nlexical overlap, which can amplify the dataset bias. Moreover, our analysis\nreveals that data augmentation with these QG models frequently impairs the\nperformance on questions with low lexical overlap, while improving that on\nquestions with high lexical overlap. To address this problem, we use a synonym\nreplacement-based approach to augment questions with low lexical overlap. We\ndemonstrate that the proposed data augmentation approach is simple yet\neffective to mitigate the degradation problem with only 70k synthetic examples.\nOur data is publicly available at\nhttps://github.com/KazutoshiShinoda/Synonym-Replacement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1\">Kazutoshi Shinoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1\">Saku Sugawara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System. (arXiv:2109.11292v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11292","description":"<p>Consistency Identification has obtained remarkable success on open-domain\ndialogue, which can be used for preventing inconsistent response generation.\nHowever, in contrast to the rapid development in open-domain dialogue, few\nefforts have been made to the task-oriented dialogue direction. In this paper,\nwe argue that consistency problem is more urgent in task-oriented domain. To\nfacilitate the research, we introduce CI-ToD, a novel dataset for Consistency\nIdentification in Task-oriented Dialog system. In addition, we not only\nannotate the single label to enable the model to judge whether the system\nresponse is contradictory, but also provide more fine-grained labels (i.e.,\nDialogue History Inconsistency, User Query Inconsistency and Knowledge Base\nInconsistency) to encourage model to know what inconsistent sources lead to it.\nEmpirical results show that state-of-the-art methods only achieve 51.3%, which\nis far behind the human performance of 93.2%, indicating that there is ample\nroom for improving consistency identification ability. Finally, we conduct\nexhaustive experiments and qualitative analysis to comprehend key challenges\nand provide guidance for future directions. All datasets and models are\npublicly available at \\url{https://github.com/yizhen20133868/CI-ToD}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianbao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shijue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiguang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Knowledge Distillation for Pre-trained Language Models. (arXiv:2109.11295v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11295","description":"<p>Knowledge distillation~(KD) has been proved effective for compressing\nlarge-scale pre-trained language models. However, existing methods conduct KD\nstatically, e.g., the student model aligns its output distribution to that of a\nselected teacher model on the pre-defined training dataset. In this paper, we\nexplore whether a dynamic knowledge distillation that empowers the student to\nadjust the learning procedure according to its competency, regarding the\nstudent performance and learning efficiency. We explore the dynamical\nadjustments on three aspects: teacher model adoption, data selection, and KD\nobjective adaptation. Experimental results show that (1) proper selection of\nteacher model can boost the performance of student model; (2) conducting KD\nwith 10% informative instances achieves comparable performance while greatly\naccelerates the training; (3) the student performance can be boosted by\nadjusting the supervision contribution of different alignment objective. We\nfind dynamic knowledge distillation is promising and provide discussions on\npotential future directions towards more efficient KD methods. Our code is\navailable at https://github.com/lancopku/DynamicKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11308","description":"<p>Both generic and domain-specific BERT models are widely used for natural\nlanguage processing (NLP) tasks. In this paper we investigate the vulnerability\nof BERT models to variation in input data for Named Entity Recognition (NER)\nthrough adversarial attack. Experimental results show that the original as well\nas the domain-specific BERT models are highly vulnerable to entity replacement:\nThey can be fooled in 89.2 to 99.4% of the cases to mislabel previously correct\nentities. BERT models are also vulnerable to variation in the entity context\nwith 20.2 to 45.0% of entities predicted completely wrong and another 29.3 to\n53.3% of entities predicted wrong partially. Often a single change is\nsufficient to fool the model. BERT models seem most vulnerable to changes in\nthe local context of entities. Of the two domain-specific BERT models, the\nvulnerability of BioBERT is comparable to the original BERT model whereas\nSciBERT is even more vulnerable. Our results chart the vulnerabilities of BERT\nmodels for NER and emphasize the importance of further research into uncovering\nand reducing these weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dirkson_A/0/1/0/all/0/1\">Anne Dirkson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verberne_S/0/1/0/all/0/1\">Suzan Verberne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraaij_W/0/1/0/all/0/1\">Wessel Kraaij</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParaShoot: A Hebrew Question Answering Dataset. (arXiv:2109.11314v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11314","description":"<p>NLP research in Hebrew has largely focused on morphology and syntax, where\nrich annotated datasets in the spirit of Universal Dependencies are available.\nSemantic datasets, however, are in short supply, hindering crucial advances in\nthe development of NLP technology in Hebrew. In this work, we present\nParaShoot, the first question answering dataset in modern Hebrew. The dataset\nfollows the format and crowdsourcing methodology of SQuAD, and contains\napproximately 3000 annotated examples, similar to other question-answering\ndatasets in low-resource languages. We provide the first baseline results using\nrecently-released BERT-style models for Hebrew, showing that there is\nsignificant room for improvement on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keren_O/0/1/0/all/0/1\">Omri Keren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Argument Strength Estimation. (arXiv:2109.11319v1 [cs.LG])","link":"http://arxiv.org/abs/2109.11319","description":"<p>High-quality arguments are an essential part of decision-making.\nAutomatically predicting the quality of an argument is a complex task that\nrecently got much attention in argument mining. However, the annotation effort\nfor this task is exceptionally high. Therefore, we test uncertainty-based\nactive learning (AL) methods on two popular argument-strength data sets to\nestimate whether sample-efficient learning can be enabled. Our extensive\nempirical evaluation shows that uncertainty-based acquisition functions can not\nsurpass the accuracy reached with the random acquisition on these data sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kees_N/0/1/0/all/0/1\">Nataliia Kees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fromm_M/0/1/0/all/0/1\">Michael Fromm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faerman_E/0/1/0/all/0/1\">Evgeniy Faerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1\">Thomas Seidl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11321","description":"<p>Large language models are known to suffer from the hallucination problem in\nthat they are prone to output statements that are false or inconsistent,\nindicating a lack of knowledge. A proposed solution to this is to provide the\nmodel with additional data modalities that complements the knowledge obtained\nthrough text. We investigate the use of visual data to complement the knowledge\nof large language models by proposing a method for evaluating visual knowledge\ntransfer to text for uni- or multimodal language models. The method is based on\ntwo steps, 1) a novel task querying for knowledge of memory colors, i.e.\ntypical colors of well-known objects, and 2) filtering of model training data\nto clearly separate knowledge contributions. Additionally, we introduce a model\narchitecture that involves a visual imagination step and evaluate it with our\nproposed method. We find that our method can successfully be used to measure\nvisual knowledge transfer capabilities in models and that our novel model\narchitecture shows promising results for leveraging multimodal knowledge in a\nunimodal setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1\">Tobias Norlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagstrom_L/0/1/0/all/0/1\">Lovisa Hagstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johanssom_R/0/1/0/all/0/1\">Richard Johanssom</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Current State of Finnish NLP. (arXiv:2109.11326v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11326","description":"<p>There are a lot of tools and resources available for processing Finnish. In\nthis paper, we survey recent papers focusing on Finnish NLP related to many\ndifferent subcategories of NLP such as parsing, generation, semantics and\nspeech. NLP research is conducted in many different research groups in Finland,\nand it is frequently the case that NLP tools and models resulting from academic\nresearch are made available for others to use on platforms such as Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Pattern- and Fact-based Fake News Detection via Model Preference Learning. (arXiv:2109.11333v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11333","description":"<p>To defend against fake news, researchers have developed various methods based\non texts. These methods can be grouped as 1) pattern-based methods, which focus\non shared patterns among fake news posts rather than the claim itself; and 2)\nfact-based methods, which retrieve from external sources to verify the claim's\nveracity without considering patterns. The two groups of methods, which have\ndifferent preferences of textual clues, actually play complementary roles in\ndetecting fake news. However, few works consider their integration. In this\npaper, we study the problem of integrating pattern- and fact-based models into\none framework via modeling their preference differences, i.e., making the\npattern- and fact-based models focus on respective preferred parts in a post\nand mitigate interference from non-preferred parts as possible. To this end, we\nbuild a Preference-aware Fake News Detection Framework (Pref-FEND), which\nlearns the respective preferences of pattern- and fact-based models for joint\ndetection. We first design a heterogeneous dynamic graph convolutional network\nto generate the respective preference maps, and then use these maps to guide\nthe joint learning of pattern- and fact-based models for final prediction.\nExperiments on two real-world datasets show that Pref-FEND effectively captures\nmodel preferences and improves the performance of models based on patterns,\nfacts, or both.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1\">Qiang Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xueyao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Juan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Lei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Second Pandemic? Analysis of Fake News About COVID-19 Vaccines in Qatar. (arXiv:2109.11372v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11372","description":"<p>While COVID-19 vaccines are finally becoming widely available, a second\npandemic that revolves around the circulation of anti-vaxxer fake news may\nhinder efforts to recover from the first one. With this in mind, we performed\nan extensive analysis of Arabic and English tweets about COVID-19 vaccines,\nwith focus on messages originating from Qatar. We found that Arabic tweets\ncontain a lot of false information and rumors, while English tweets are mostly\nfactual. However, English tweets are much more propagandistic than Arabic ones.\nIn terms of propaganda techniques, about half of the Arabic tweets express\ndoubt, and 1/5 use loaded language, while English tweets are abundant in loaded\nlanguage, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in\nterms of framing, Arabic tweets adopt a health and safety perspective, while in\nEnglish economic concerns dominate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v1 [cs.LG])","link":"http://arxiv.org/abs/2109.11377","description":"<p>Recent \\emph{Weak Supervision (WS)} approaches have had widespread success in\neasing the bottleneck of labeling training data for machine learning by\nsynthesizing labels from multiple potentially noisy supervision sources.\nHowever, proper measurement and analysis of these approaches remain a\nchallenge. First, datasets used in existing works are often private and/or\ncustom, limiting standardization. Second, WS datasets with the same name and\nbase data often vary in terms of the labels and weak supervision sources used,\na significant \"hidden\" source of evaluation variance. Finally, WS studies often\ndiverge in terms of the evaluation protocol and ablations used. To address\nthese problems, we introduce a benchmark platform, \\benchmark, for a thorough\nand standardized evaluation of WS approaches. It consists of 22 varied\nreal-world datasets for classification and sequence tagging; a range of real,\nsynthetic, and procedurally-generated weak supervision sources; and a modular,\nextensible framework for WS evaluation, including implementations for popular\nWS methods. We use \\benchmark to conduct extensive comparisons over more than\n100 method variants to demonstrate its efficacy as a benchmark platform. The\ncode is available at \\url{https://github.com/JieyuZ2/wrench}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jieyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1\">Alexander Ratner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-based Mention Typing for Named Entity Disambiguation. (arXiv:2109.11389v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11389","description":"<p>An entity mention in text such as \"Washington\" may correspond to many\ndifferent named entities such as the city \"Washington D.C.\" or the newspaper\n\"Washington Post.\" The goal of named entity disambiguation is to identify the\nmentioned named entity correctly among all possible candidates. If the type\n(e.g. location or person) of a mentioned entity can be correctly predicted from\nthe context, it may increase the chance of selecting the right candidate by\nassigning low probability to the unlikely ones. This paper proposes\ncluster-based mention typing for named entity disambiguation. The aim of\nmention typing is to predict the type of a given mention based on its context.\nGenerally, manually curated type taxonomies such as Wikipedia categories are\nused. We introduce cluster-based mention typing, where named entities are\nclustered based on their contextual similarities and the cluster ids are\nassigned as types. The hyperlinked mentions and their context in Wikipedia are\nused in order to obtain these cluster-based types. Then, mention typing models\nare trained on these mentions, which have been labeled with their cluster-based\ntypes through distant supervision. At the named entity disambiguation phase,\nfirst the cluster-based types of a given mention are predicted and then, these\ntypes are used as features in a ranking model to select the best entity among\nthe candidates. We represent entities at multiple contextual levels and obtain\ndifferent clusterings (and thus typing models) based on each level. As each\nclustering breaks the entity space differently, mention typing based on each\nclustering discriminates the mention differently. When predictions from all\ntyping models are used together, our system achieves better or comparable\nresults based on randomization tests with respect to the state-of-the-art\nlevels on four defacto test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Celebi_A/0/1/0/all/0/1\">Arda &#xc7;elebi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Named Entity Recognition and Classification on Historical Documents: A Survey. (arXiv:2109.11406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11406","description":"<p>After decades of massive digitisation, an unprecedented amount of historical\ndocuments is available in digital format, along with their machine-readable\ntexts. While this represents a major step forward with respect to preservation\nand accessibility, it also opens up new opportunities in terms of content\nmining and the next fundamental challenge is to develop appropriate\ntechnologies to efficiently search, retrieve and explore information from this\n'big data of the past'. Among semantic indexing opportunities, the recognition\nand classification of named entities are in great demand among humanities\nscholars. Yet, named entity recognition (NER) systems are heavily challenged\nwith diverse, historical and noisy inputs. In this survey, we present the array\nof challenges posed by historical documents to NER, inventory existing\nresources, describe the main approaches deployed so far, and identify key\npriorities for future developments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ehrmann_M/0/1/0/all/0/1\">Maud Ehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamdi_A/0/1/0/all/0/1\">Ahmed Hamdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pontes_E/0/1/0/all/0/1\">Elvys Linhares Pontes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romanello_M/0/1/0/all/0/1\">Matteo Romanello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Antoine Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Algorithm for Generating Gap-Fill Multiple Choice Questions of an Expert System. (arXiv:2109.11421v1 [cs.AI])","link":"http://arxiv.org/abs/2109.11421","description":"<p>This research is aimed to propose an artificial intelligence algorithm\ncomprising an ontology-based design, text mining, and natural language\nprocessing for automatically generating gap-fill multiple choice questions\n(MCQs). The simulation of this research demonstrated an application of the\nalgorithm in generating gap-fill MCQs about software testing. The simulation\nresults revealed that by using 103 online documents as inputs, the algorithm\ncould automatically produce more than 16 thousand valid gap-fill MCQs covering\na variety of topics in the software testing domain. Finally, in the discussion\nsection of this paper we suggest how the proposed algorithm should be applied\nto produce gap-fill MCQs being collected in a question pool used by a knowledge\nexpert system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirithumgul_P/0/1/0/all/0/1\">Pornpat Sirithumgul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasertsilp_P/0/1/0/all/0/1\">Pimpaka Prasertsilp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olfman_L/0/1/0/all/0/1\">Lorne Olfman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Fact-Checking: A Survey. (arXiv:2109.11427v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11427","description":"<p>As online false information continues to grow, automated fact-checking has\ngained an increasing amount of attention in recent years. Researchers in the\nfield of Natural Language Processing (NLP) have contributed to the task by\nbuilding fact-checking datasets, devising automated fact-checking pipelines and\nproposing NLP methods to further research in the development of different\ncomponents. This paper reviews relevant research on automated fact-checking\ncovering both the claim detection and claim validation components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xia Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abumansour_A/0/1/0/all/0/1\">Amani S. Abumansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corpus and Models for Lemmatisation and POS-tagging of Old French. (arXiv:2109.11442v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11442","description":"<p>Old French is a typical example of an under-resourced historic languages,\nthat furtherly displays animportant amount of linguistic variation. In this\npaper, we present the current results of a long going project (2015-...) and\ndescribe how we broached the difficult question of providing lemmatisation\nandPOS models for Old French with the help of neural taggers and the\nprogressive constitution of dedicated corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Camps_J/0/1/0/all/0/1\">Jean-Baptiste Camps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clerice_T/0/1/0/all/0/1\">Thibault Cl&#xe9;rice</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duval_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Duval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ing_L/0/1/0/all/0/1\">Lucence Ing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanaoka_N/0/1/0/all/0/1\">Naomi Kanaoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinche_A/0/1/0/all/0/1\">Ariane Pinche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. (arXiv:2109.11491v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11491","description":"<p>We present a method for exploring regions around individual points in a\ncontextualized vector space (particularly, BERT space), as a way to investigate\nhow these regions correspond to word senses. By inducing a contextualized\n\"pseudoword\" as a stand-in for a static embedding in the input layer, and then\nperforming masked prediction of a word in the sentence, we are able to\ninvestigate the geometry of the BERT-space in a controlled manner around\nindividual instances. Using our method on a set of carefully constructed\nsentences targeting ambiguous English words, we find substantial regularity in\nthe contextualized space, with regions that correspond to distinct word senses;\nbut between these regions there are occasionally \"sense voids\" -- regions that\ndo not correspond to any intelligible sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karidi_T/0/1/0/all/0/1\">Taelin Karidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding a Balanced Degree of Automation for Summary Evaluation. (arXiv:2109.11503v1 [cs.CL])","link":"http://arxiv.org/abs/2109.11503","description":"<p>Human evaluation for summarization tasks is reliable but brings in issues of\nreproducibility and high costs. Automatic metrics are cheap and reproducible\nbut sometimes poorly correlated with human judgment. In this work, we propose\nflexible semiautomatic to automatic summary evaluation metrics, following the\nPyramid human evaluation method. Semi-automatic Lite2Pyramid retains the\nreusable human-labeled Summary Content Units (SCUs) for reference(s) but\nreplaces the manual work of judging SCUs' presence in system summaries with a\nnatural language inference (NLI) model. Fully automatic Lite3Pyramid further\nsubstitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via\na semantic role labeling (SRL) model. Finally, we propose in-between metrics,\nLite2.xPyramid, where we use a simple regressor to predict how well the STUs\ncan simulate SCUs and retain SCUs that are more difficult to simulate, which\nprovides a smooth transition and balance between automation and manual\nevaluation. Comparing to 15 existing metrics, we evaluate human-metric\ncorrelations on 3 existing meta-evaluation datasets and our newly-collected\nPyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid\nconsistently has the best summary-level correlations; Lite3Pyramid works better\nthan or comparable to other automatic metrics; Lite2.xPyramid trades off small\ncorrelation drops for larger manual effort reduction, which can reduce costs\nfor future data collection. Our code and data are publicly available at:\nhttps://github.com/ZhangShiyue/Lite2-3Pyramid\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MARMOT: A Deep Learning Framework for Constructing Multimodal Representations for Vision-and-Language Tasks. (arXiv:2109.11526v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11526","description":"<p>Political activity on social media presents a data-rich window into political\nbehavior, but the vast amount of data means that almost all content analyses of\nsocial media require a data labeling step. However, most automated machine\nclassification methods ignore the multimodality of posted content, focusing\neither on text or images. State-of-the-art vision-and-language models are\nunusable for most political science research: they require all observations to\nhave both image and text and require computationally expensive pretraining.\nThis paper proposes a novel vision-and-language framework called multimodal\nrepresentations using modality translation (MARMOT). MARMOT presents two\nmethodological contributions: it can construct representations for observations\nmissing image or text, and it replaces the computationally expensive\npretraining with modality translation. MARMOT outperforms an ensemble text-only\nclassifier in 19 of 20 categories in multilabel classifications of tweets\nreporting election incidents during the 2016 U.S. general election. Moreover,\nMARMOT shows significant improvements over the results of benchmark multimodal\nmodels on the Hateful Memes dataset, improving the best result set by\nVisualBERT in terms of accuracy from 0.6473 to 0.6760 and area under the\nreceiver operating characteristic curve (AUC) from 0.7141 to 0.7530.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mebane_W/0/1/0/all/0/1\">Walter R. Mebane Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.06438","description":"<p>Online advertising is an important revenue source for many IT companies. In\nthe search advertising scenario, advertisement text that meets the need of the\nsearch query would be more attractive to the user. However, the manual creation\nof query-variant advertisement texts for massive items is expensive.\nTraditional text generation methods tend to focus on the general searching\nneeds with high frequency while ignoring the diverse personalized searching\nneeds with low frequency. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisement texts for different web search queries with various needs based\non queries and item keywords. To solve the problem of ignoring low-frequency\nneeds, we propose a dynamic association mechanism to expand the receptive field\nbased on external knowledge, which can obtain associated words to be added to\nthe input. These associated words can serve as bridges to transfer the ability\nof the model from the familiar high-frequency words to the unfamiliar\nlow-frequency words. With association, the model can make use of various\npersonalized needs in queries and generate query-variant advertisement texts.\nBoth automatic and human evaluations show that our model can generate more\nattractive advertisement text than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Siyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_C/0/1/0/all/0/1\">Cai Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yancheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summary-Source Proposition-level Alignment: Task, Datasets and Supervised Baseline. (arXiv:2009.00590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.00590","description":"<p>Aligning sentences in a reference summary with their counterparts in source\ndocuments was shown as a useful auxiliary summarization task, notably for\ngenerating training data for salience detection. Despite its assessed utility,\nthe alignment step was mostly approached with heuristic unsupervised methods,\ntypically ROUGE-based, and was never independently optimized or evaluated. In\nthis paper, we propose establishing summary-source alignment as an explicit\ntask, while introducing two major novelties: (1) applying it at the more\naccurate proposition span level, and (2) approaching it as a supervised\nclassification task. To that end, we created a novel training dataset for\nproposition-level alignment, derived automatically from available summarization\nevaluation data. In addition, we crowdsourced dev and test datasets, enabling\nmodel development and proper evaluation. Utilizing these data, we present a\nsupervised proposition alignment baseline model, showing improved\nalignment-quality over the unsupervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ernst_O/0/1/0/all/0/1\">Ori Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1\">Ori Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepioshkin_M/0/1/0/all/0/1\">Michael Lepioshkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1\">Jacob Goldberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"N-LTP: An Open-source Neural Language Technology Platform for Chinese. (arXiv:2009.11616v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.11616","description":"<p>We introduce \\texttt{N-LTP}, an open-source neural language technology\nplatform supporting six fundamental Chinese NLP tasks: {lexical analysis}\n(Chinese word segmentation, part-of-speech tagging, and named entity\nrecognition), {syntactic parsing} (dependency parsing), and {semantic parsing}\n(semantic dependency parsing and semantic role labeling). Unlike the existing\nstate-of-the-art toolkits, such as \\texttt{Stanza}, that adopt an independent\nmodel for each task, \\texttt{N-LTP} adopts the multi-task framework by using a\nshared pre-trained model, which has the advantage of capturing the shared\nknowledge across relevant Chinese tasks. In addition, a knowledge distillation\nmethod \\cite{DBLP:journals/corr/abs-1907-04829} where the single-task model\nteaches the multi-task model is further introduced to encourage the multi-task\nmodel to surpass its single-task teacher. Finally, we provide a collection of\neasy-to-use APIs and a visualization tool to make users to use and view the\nprocessing results more easily and directly. To the best of our knowledge, this\nis the first toolkit to support six Chinese NLP fundamental tasks. Source code,\ndocumentation, and pre-trained models are available at\n\\url{https://github.com/HIT-SCIR/ltp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yunlong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1\">Libo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings. (arXiv:2010.11247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11247","description":"<p>Simultaneous translation is vastly different from full-sentence translation,\nin the sense that it starts translation before the source sentence ends, with\nonly a few words delay. However, due to the lack of large-scale, high-quality\nsimultaneous translation datasets, most such systems are still trained on\nconventional full-sentence bitexts. This is far from ideal for the simultaneous\nscenario due to the abundance of unnecessary long-distance reorderings in those\nbitexts. We propose a novel method that rewrites the target side of existing\nfull-sentence corpora into simultaneous-style translation. Experiments on\nZh-&gt;En and Ja-&gt;En simultaneous translation show substantial improvements (up to\n+2.7 BLEU) with the addition of these generated pseudo-references.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kita_A/0/1/0/all/0/1\">Atsuhito Kita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Validating Label Consistency in NER Data Annotation. (arXiv:2101.08698v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.08698","description":"<p>Data annotation plays a crucial role in ensuring your named entity\nrecognition (NER) projects are trained with the right information to learn\nfrom. Producing the most accurate labels is a challenge due to the complexity\ninvolved with annotation. Label inconsistency between multiple subsets of data\nannotation (e.g., training set and test set, or multiple training subsets) is\nan indicator of label mistakes. In this work, we present an empirical method to\nexplore the relationship between label (in-)consistency and NER model\nperformance. It can be used to validate the label consistency (or catches the\ninconsistency) in multiple sets of NER data annotation. In experiments, our\nmethod identified the label inconsistency of test data in SCIERC and CoNLL03\ndatasets (with 26.7% and 5.4% label mistakes). It validated the consistency in\nthe corrected version of both datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1\">Qingkai Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mengxia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenhao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Meng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking. (arXiv:2104.04466v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04466","description":"<p>Dialogue State Tracking is central to multi-domain task-oriented dialogue\nsystems, responsible for extracting information from user utterances. We\npresent a novel hybrid architecture that augments GPT-2 with representations\nderived from Graph Attention Networks in such a way to allow causal, sequential\nprediction of slot values. The model architecture captures inter-slot\nrelationships and dependencies across domains that otherwise can be lost in\nsequential prediction. We report improvements in state tracking performance in\nMultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified\nsparse training scenario in which DST models are trained only on session-level\nannotations but evaluated at the turn level. We further report detailed\nanalyses to demonstrate the effectiveness of graph models in DST by showing\nthat the proposed graph modules capture inter-slot dependencies and improve the\npredictions of values that are common to multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weizhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsiang Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models. (arXiv:2104.08066v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08066","description":"<p>A method for creating a vision-and-language (V&amp;L) model is to extend a\nlanguage model through structural modifications and V&amp;L pre-training. Such an\nextension aims to make a V&amp;L model inherit the capability of natural language\nunderstanding (NLU) from the original language model. To see how well this is\nachieved, we propose to evaluate V&amp;L models using an NLU benchmark (GLUE). We\ncompare five V&amp;L models, including single-stream and dual-stream models,\ntrained with the same pre-training. Dual-stream models, with their higher\nmodality independence achieved by approximately doubling the number of\nparameters, are expected to preserve the NLU capability better. Our main\nfinding is that the dual-stream scores are not much different than the\nsingle-stream scores, contrary to expectation. Further analysis shows that\npre-training causes the performance drop in NLU tasks with few exceptions.\nThese results suggest that adopting a single-stream structure and devising the\npre-training could be an effective method for improving the maintenance of\nlanguage knowledge in V&amp;L extensions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iki_T/0/1/0/all/0/1\">Taichi Iki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers: \"The End of History\" for NLP?. (arXiv:2105.00813v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00813","description":"<p>Recent advances in neural architectures, such as the Transformer, coupled\nwith the emergence of large-scale pre-trained models such as BERT, have\nrevolutionized the field of Natural Language Processing (NLP), pushing the\nstate of the art for a number of NLP tasks. A rich family of variations of\nthese models has been proposed, such as RoBERTa, ALBERT, and XLNet, but\nfundamentally, they all remain limited in their ability to model certain kinds\nof information, and they cannot cope with certain information sources, which\nwas easy for pre-existing models. Thus, here we aim to shed light on some\nimportant theoretical limitations of pre-trained BERT-style models that are\ninherent in the general Transformer architecture. First, we demonstrate in\npractice on two general types of tasks -- segmentation and segment labeling --\nand on four datasets that these limitations are indeed harmful and that\naddressing them, even in some very simple and naive ways, can yield sizable\nimprovements over vanilla RoBERTa and XLNet models. Then, we offer a more\ngeneral discussion on desiderata for future additions to the Transformer\narchitecture that would increase its expressiveness, which we hope could help\nin the design of the next generation of deep NLP architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chernyavskiy_A/0/1/0/all/0/1\">Anton Chernyavskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilvovsky_D/0/1/0/all/0/1\">Dmitry Ilvovsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors. (arXiv:2107.01545v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2107.01545","description":"<p>Attractor-based end-to-end diarization is achieving comparable accuracy to\nthe carefully tuned conventional clustering-based methods on challenging\ndatasets. However, the main drawback is that it cannot deal with the case where\nthe number of speakers is larger than the one observed during training. This is\nbecause its speaker counting relies on supervised learning. In this work, we\nintroduce an unsupervised clustering process embedded in the attractor-based\nend-to-end diarization. We first split a sequence of frame-wise embeddings into\nshort subsequences and then perform attractor-based diarization for each\nsubsequence. Given subsequence-wise diarization results, inter-subsequence\nspeaker correspondence is obtained by unsupervised clustering of the vectors\ncomputed from the attractors from all the subsequences. This makes it possible\nto produce diarization results of a large number of speakers for the whole\nrecording even if the number of output speakers for each subsequence is\nlimited. Experimental results showed that our method could produce accurate\ndiarization results of an unseen number of speakers. Our method achieved 11.84\n%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,\nrespectively, each of which is better than the conventional end-to-end\ndiarization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Horiguchi_S/0/1/0/all/0/1\">Shota Horiguchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1\">Paola Garcia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_Y/0/1/0/all/0/1\">Yawen Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Takashima_Y/0/1/0/all/0/1\">Yuki Takashima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawaguchi_Y/0/1/0/all/0/1\">Yohei Kawaguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.09099","description":"<p>Punctuation is critical in understanding natural language text. Currently,\nmost automatic speech recognition (ASR) systems do not generate punctuation,\nwhich affects the performance of downstream tasks, such as intent detection and\nslot filling. This gives rise to the need for punctuation restoration. Recent\nwork in punctuation restoration heavily utilizes pre-trained language models\nwithout considering data imbalance when predicting punctuation classes. In this\nwork, we address this problem by proposing a token-level supervised contrastive\nlearning method that aims at maximizing the distance of representation of\ndifferent punctuation marks in the embedding space. The result shows that\ntraining with token-level supervised contrastive learning obtains up to 3.2%\nabsolute F1 improvement on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">H Lilian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bo Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02446","description":"<p>Text variational autoencoders (VAEs) are notorious for posterior collapse, a\nphenomenon where the model's decoder learns to ignore signals from the encoder.\nBecause posterior collapse is known to be exacerbated by expressive decoders,\nTransformers have seen limited adoption as components of text VAEs. Existing\nstudies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et\nal., 2021) mitigate posterior collapse using massive pretraining, a technique\nunavailable to most of the research community without extensive computing\nresources. We present a simple two-phase training scheme to convert a\nsequence-to-sequence Transformer into a VAE with just finetuning. The resulting\nlanguage model is competitive with massively pretrained Transformer-based VAEs\nin some internal metrics while falling short on others. To facilitate training\nwe comprehensively explore the impact of common posterior collapse alleviation\ntechniques in the literature. We release our code for reproducability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihwa Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues. (arXiv:2109.00185v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00185","description":"<p>We present an effective system adapted from the end-to-end neural coreference\nresolution model, targeting on the task of anaphora resolution in dialogues.\nThree aspects are specifically addressed in our approach, including the support\nof singletons, encoding speakers and turns throughout dialogue interactions,\nand knowledge transfer utilizing existing resources. Despite the simplicity of\nour adaptation strategies, they are shown to bring significant impact to the\nfinal performance, with up to 27 F1 improvement over the baseline. Our final\nsystem ranks the 1st place on the leaderboard of the anaphora resolution track\nin the CRAC 2021 shared task, and achieves the best evaluation results on all\nfour datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation. (arXiv:2109.00194v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00194","description":"<p>Recent multilingual pre-trained language models have achieved remarkable\nzero-shot performance, where the model is only finetuned on one source language\nand directly evaluated on target languages. In this work, we propose a\nself-learning framework that further utilizes unlabeled data of target\nlanguages, combined with uncertainty estimation in the process to select\nhigh-quality silver labels. Three different uncertainties are adapted and\nanalyzed specifically for the cross lingual transfer: Language\nHeteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty\n(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks\nincluding Named Entity Recognition (NER) and Natural Language Inference (NLI)\ncovering 40 languages in total, which outperforms the baselines significantly\nby 10 F1 on average for NER and 2.5 accuracy score for NLI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Bias in NLP -- Application to Hate Speech Classification. (arXiv:2109.09725v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09725","description":"<p>This document sums up our results forthe NLP lecture at ETH in the spring\nsemester 2021. In this work, a BERT based neural network model (Devlin et\nal.,2018) is applied to the JIGSAW dataset (Jigsaw/Conversation AI, 2019) in\norder to create a model identifying hateful and toxic comments (strictly\nseperated from offensive language) in online social platforms (English\nlanguage), inthis case Twitter. Three other neural network architectures and a\nGPT-2 (Radfordet al., 2019) model are also applied on the provided data set in\norder to compare these different models. The trained BERT model is then applied\non two different data sets to evaluate its generalisation power, namely on\nanother Twitter data set (Tom Davidson, 2017) (Davidsonet al., 2017) and the\ndata set HASOC 2019 (Thomas Mandl, 2019) (Mandl et al.,2019) which includes\nTwitter and also Facebook comments; we focus on the English HASOC 2019 data. In\naddition, it can be shown that by fine-tuning the trained BERT model on these\ntwo datasets by applying different transfer learning scenarios via retraining\npartial or all layers the predictive scores improve compared to simply applying\nthe model pre-trained on the JIGSAW data set. Withour results, we get\nprecisions from 64% to around 90% while still achieving acceptable recall\nvalues of at least lower 60s%, proving that BERT is suitable for real usecases\nin social platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patoulidis_G/0/1/0/all/0/1\">Georgios Patoulidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zagidullina_A/0/1/0/all/0/1\">Aygul Zagidullina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning. (arXiv:2109.10510v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10510","description":"<p>Despite the success of neural dialogue systems in achieving high performance\non the leader-board, they cannot meet users' requirements in practice, due to\ntheir poor reasoning skills. The underlying reason is that most neural dialogue\nmodels only capture the syntactic and semantic information, but fail to model\nthe logical consistency between the dialogue history and the generated\nresponse. Recently, a new multi-turn dialogue reasoning task has been proposed,\nto facilitate dialogue reasoning research. However, this task is challenging,\nbecause there are only slight differences between the illogical response and\nthe dialogue history. How to effectively solve this challenge is still worth\nexploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle\nthis problem. Inspired by human's behavior in reading comprehension, a\ncomparison mechanism is proposed to focus on the fine-grained differences in\nthe representation of each response candidate. Specifically, each candidate\nrepresentation is compared with the whole history to obtain a history\nconsistency representation. Furthermore, the consistency signals between each\ncandidate and the speaker's own history are considered to drive a model to\nprefer a candidate that is logically consistent with the speaker's history\nlogic. Finally, the above consistency representations are employed to output a\nranking list of the candidate responses for multi-turn dialogue reasoning.\nExperimental results on two public dialogue datasets show that our method\nobtains higher ranking scores than the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.10847","description":"<p>Recent progress in the Natural Language Processing domain has given us\nseveral State-of-the-Art (SOTA) pretrained models which can be finetuned for\nspecific tasks. These large models with billions of parameters trained on\nnumerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In\nthis paper, we discuss the need for a benchmark for cost and time effective\nsmaller models trained on a single GPU. This will enable researchers with\nresource constraints experiment with novel and innovative ideas on\ntokenization, pretraining tasks, architecture, fine tuning methods etc. We set\nup Small-Bench NLP, a benchmark for small efficient neural language models\ntrained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks\non the publicly available GLUE datasets and a leaderboard to track the progress\nof the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture\nachieves an average score of 81.53 which is comparable to that of BERT-Base's\n82.20 (110M parameters). Our models, code and leaderboard are available at\nhttps://github.com/smallbenchnlp\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanakarajan_K/0/1/0/all/0/1\">Kamal Raj Kanakarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kundumani_B/0/1/0/all/0/1\">Bhuvana Kundumani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankarasubbu_M/0/1/0/all/0/1\">Malaikannan Sankarasubbu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Mixed-supervised segmentation: Confidence maximization helps knowledge distillation. (arXiv:2109.10902v1 [eess.IV])","link":"http://arxiv.org/abs/2109.10902","description":"<p>Despite achieving promising results in a breadth of medical image\nsegmentation tasks, deep neural networks require large training datasets with\npixel-wise annotations. Obtaining these curated datasets is a cumbersome\nprocess which limits the application in scenarios where annotated images are\nscarce. Mixed supervision is an appealing alternative for mitigating this\nobstacle, where only a small fraction of the data contains complete pixel-wise\nannotations and other images have a weaker form of supervision. In this work,\nwe propose a dual-branch architecture, where the upper branch (teacher)\nreceives strong annotations, while the bottom one (student) is driven by\nlimited supervision and guided by the upper branch. Combined with a standard\ncross-entropy loss over the labeled pixels, our novel formulation integrates\ntwo important terms: (i) a Shannon entropy loss defined over the\nless-supervised images, which encourages confident student predictions in the\nbottom branch; and (ii) a Kullback-Leibler (KL) divergence term, which\ntransfers the knowledge of the strongly supervised branch to the\nless-supervised branch and guides the entropy (student-confidence) term to\navoid trivial solutions. We show that the synergy between the entropy and KL\ndivergence yields substantial improvements in performance. We also discuss an\ninteresting link between Shannon-entropy minimization and standard pseudo-mask\ngeneration, and argue that the former should be preferred over the latter for\nleveraging information from unlabeled pixels. Quantitative and qualitative\nresults on two publicly available datasets demonstrate that our method\nsignificantly outperforms other strategies for semantic segmentation within a\nmixed-supervision framework, as well as recent semi-supervised approaches.\nMoreover, we show that the branch trained with reduced supervision and guided\nby the top branch largely outperforms the latter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The CAMELS Multifield Dataset: Learning the Universe's Fundamental Parameters with Artificial Intelligence. (arXiv:2109.10915v1 [cs.LG])","link":"http://arxiv.org/abs/2109.10915","description":"<p>We present the Cosmology and Astrophysics with MachinE Learning Simulations\n(CAMELS) Multifield Dataset, CMD, a collection of hundreds of thousands of 2D\nmaps and 3D grids containing many different properties of cosmic gas, dark\nmatter, and stars from 2,000 distinct simulated universes at several cosmic\ntimes. The 2D maps and 3D grids represent cosmic regions that span $\\sim$100\nmillion light years and have been generated from thousands of state-of-the-art\nhydrodynamic and gravity-only N-body simulations from the CAMELS project.\nDesigned to train machine learning models, CMD is the largest dataset of its\nkind containing more than 70 Terabytes of data. In this paper we describe CMD\nin detail and outline a few of its applications. We focus our attention on one\nsuch task, parameter inference, formulating the problems we face as a challenge\nto the community. We release all data and provide further technical details at\nhttps://camels-multifield-dataset.readthedocs.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1\">Francisco Villaescusa-Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genel_S/0/1/0/all/0/1\">Shy Genel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angles_Alcazar_D/0/1/0/all/0/1\">Daniel Angles-Alcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiele_L/0/1/0/all/0/1\">Leander Thiele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_R/0/1/0/all/0/1\">Romeel Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_D/0/1/0/all/0/1\">Desika Narayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicola_A/0/1/0/all/0/1\">Andrina Nicola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villanueva_Domingo_P/0/1/0/all/0/1\">Pablo Villanueva-Domingo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandelt_B/0/1/0/all/0/1\">Benjamin Wandelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spergel_D/0/1/0/all/0/1\">David N. Spergel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somerville_R/0/1/0/all/0/1\">Rachel S. Somerville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matilla_J/0/1/0/all/0/1\">Jose Manuel Zorrilla Matilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_F/0/1/0/all/0/1\">Faizan G. Mohammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Sultan Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1\">Helen Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadekar_D/0/1/0/all/0/1\">Digvijay Wadekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickenberg_M/0/1/0/all/0/1\">Michael Eickenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kaze W.K. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contardo_G/0/1/0/all/0/1\">Gabriella Contardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1\">Yongseok Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moser_E/0/1/0/all/0/1\">Emily Moser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1\">Erwin T. Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_L/0/1/0/all/0/1\">Luis Fernando Machado Poletti Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_L/0/1/0/all/0/1\">Lucia A. Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagai_D/0/1/0/all/0/1\">Daisuke Nagai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_N/0/1/0/all/0/1\">Nicholas Battaglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelsberger_M/0/1/0/all/0/1\">Mark Vogelsberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T6D-Direct: Transformers for Multi-Object 6D Pose Direct Regression. (arXiv:2109.10948v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10948","description":"<p>6D pose estimation is the task of predicting the translation and orientation\nof objects in a given input image, which is a crucial prerequisite for many\nrobotics and augmented reality applications. Lately, the Transformer Network\narchitecture, equipped with a multi-head self-attention mechanism, is emerging\nto achieve state-of-the-art results in many computer vision tasks. DETR, a\nTransformer-based model, formulated object detection as a set prediction\nproblem and achieved impressive results without standard components like region\nof interest pooling, non-maximal suppression, and bounding box proposals. In\nthis work, we propose T6D-Direct, a real-time single-stage direct method with a\ntransformer-based architecture built on DETR to perform 6D multi-object pose\ndirect estimation. We evaluate the performance of our method on the YCB-Video\ndataset. Our method achieves the fastest inference time, and the pose\nestimation accuracy is comparable to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Arash Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Periyasamy_A/0/1/0/all/0/1\">Arul Selvam Periyasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Contrastive Representation for Semantic Correspondence. (arXiv:2109.10967v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10967","description":"<p>Dense correspondence across semantically related images has been extensively\nstudied, but still faces two challenges: 1) large variations in appearance,\nscale and pose exist even for objects from the same category, and 2) labeling\npixel-level dense correspondences is labor intensive and infeasible to scale.\nMost existing approaches focus on designing various matching approaches with\nfully-supervised ImageNet pretrained networks. On the other hand, while a\nvariety of self-supervised approaches are proposed to explicitly measure\nimage-level similarities, correspondence matching the pixel level remains\nunder-explored. In this work, we propose a multi-level contrastive learning\napproach for semantic matching, which does not rely on any ImageNet pretrained\nmodel. We show that image-level contrastive learning is a key component to\nencourage the convolutional features to find correspondence between similar\nobjects, while the performance can be further enhanced by regularizing\ncross-instance cycle-consistency at intermediate feature levels. Experimental\nresults on the PF-PASCAL, PF-WILLOW, and SPair-71k benchmark datasets\ndemonstrate that our method performs favorably against the state-of-the-art\napproaches. The source code and trained models will be made available to the\npublic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Taihong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient and Scalable Collection of Fly-inspired Voting Units for Visual Place Recognition in Changing Environments. (arXiv:2109.10986v1 [cs.CV])","link":"http://arxiv.org/abs/2109.10986","description":"<p>State-of-the-art visual place recognition performance is currently being\nachieved utilizing deep learning based approaches. Despite the recent efforts\nin designing lightweight convolutional neural network based models, these can\nstill be too expensive for the most hardware restricted robot applications.\nLow-overhead VPR techniques would not only enable platforms equipped with\nlow-end, cheap hardware but also reduce computation on more powerful systems,\nallowing these resources to be allocated for other navigation tasks. In this\nwork, our goal is to provide an algorithm of extreme compactness and efficiency\nwhile achieving state-of-the-art robustness to appearance changes and small\npoint-of-view variations. Our first contribution is DrosoNet, an exceptionally\ncompact model inspired by the odor processing abilities of the fruit fly,\nDrosophyla melanogaster. Our second and main contribution is a voting mechanism\nthat leverages multiple small and efficient classifiers to achieve more robust\nand consistent VPR compared to a single one. We use DrosoNet as the baseline\nclassifier for the voting mechanism and evaluate our models on five benchmark\ndatasets, assessing moderate to extreme appearance changes and small to\nmoderate viewpoint variations. We then compare the proposed algorithms to\nstate-of-the-art methods, both in terms of precision-recall AUC results and\ncomputational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arcanjo_B/0/1/0/all/0/1\">Bruno Arcanjo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrarini_B/0/1/0/all/0/1\">Bruno Ferrarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1\">Klaus D. McDonald-Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1\">Shoaib Ehsan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark Comparison of Visual Place Recognition Techniques for Resource-Constrained Embedded Platforms. (arXiv:2109.11002v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11002","description":"<p>Visual Place Recognition (VPR) has been a subject of significant research\nover the last 15 to 20 years. VPR is a fundamental task for autonomous\nnavigation as it enables self-localization within an environment. Although\nrobots are often equipped with resource-constrained hardware, the computational\nrequirements of and effects on VPR techniques have received little attention.\nIn this work, we present a hardware-focused benchmark evaluation of a number of\nstate-of-the-art VPR techniques on public datasets. We consider popular single\nboard computers, including ODroid, UP and Raspberry Pi 3, in addition to a\ncommodity desktop and laptop for reference. We present our analysis based on\nseveral key metrics, including place-matching accuracy, image encoding time,\ndescriptor matching time and memory needs. Key questions addressed include: (1)\nHow does the performance accuracy of a VPR technique change with processor\narchitecture? (2) How does power consumption vary for different VPR techniques\nand embedded platforms? (3) How much does descriptor size matter in comparison\nto today's embedded platforms' storage? (4) How does the performance of a\nhigh-end platform relate to an on-board low-end embedded platform for VPR? The\nextensive analysis and results in this work serve not only as a benchmark for\nthe VPR community, but also provide useful insights for real-world adoption of\nVPR applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Power_R/0/1/0/all/0/1\">Rose Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaffar_M/0/1/0/all/0/1\">Mubariz Zaffar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrarini_B/0/1/0/all/0/1\">Bruno Ferrarini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonald_Maier_K/0/1/0/all/0/1\">Klaus McDonald-Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsan_S/0/1/0/all/0/1\">Shoaib Ehsan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Coherence for Text-to-Image Retrieval. (arXiv:2109.11047v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11047","description":"<p>Common image-text joint understanding techniques presume that images and the\nassociated text can universally be characterized by a single implicit model.\nHowever, co-occurring images and text can be related in qualitatively different\nways, and explicitly modeling it could improve the performance of current joint\nunderstanding models. In this paper, we train a Cross-Modal Coherence Modelfor\ntext-to-image retrieval task. Our analysis shows that models trained with\nimage--text coherence relations can retrieve images originally paired with\ntarget text more often than coherence-agnostic models. We also show via human\nevaluation that images retrieved by the proposed coherence-aware model are\npreferred over a coherence-agnostic baseline by a huge margin. Our findings\nprovide insights into the ways that different modalities communicate and the\nrole of coherence relations in capturing commonsense inferences in text and\nimagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1\">Fangda Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_H/0/1/0/all/0/1\">Hareesh Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1\">Mubbasir Kapadia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1\">Vladimir Pavlovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards practical object detection for weed spraying in precision agriculture. (arXiv:2109.11048v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11048","description":"<p>The evolution of smaller, faster processors and cheaper digital storage\nmechanisms across the last 4-5 decades has vastly increased the opportunity to\nintegrate intelligent technologies in a wide range of practical environments to\naddress a broad spectrum of tasks. One exciting application domain for such\ntechnologies is precision agriculture, where the ability to integrate on-board\nmachine vision with data-driven actuation means that farmers can make decisions\nabout crop care and harvesting at the level of the individual plant rather than\nthe whole field. This makes sense both economically and environmentally.\nHowever, the key driver for this capability is fast and robust machine vision\n-- typically driven by machine learning (ML) solutions and dependent on\naccurate modelling. One critical challenge is that the bulk of ML-based vision\nresearch considers only metrics that evaluate the accuracy of object detection\nand do not assess practical factors. This paper introduces three metrics that\nhighlight different aspects relevant for real-world deployment of precision\nweeding and demonstrates their utility through experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salazar_Gomez_A/0/1/0/all/0/1\">Adrian Salazar-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darbyshire_M/0/1/0/all/0/1\">Madeleine Darbyshire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sklar_E/0/1/0/all/0/1\">Elizabeth I Sklar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parsons_S/0/1/0/all/0/1\">Simon Parsons</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A two-step machine learning approach for crop disease detection: an application of GAN and UAV technology. (arXiv:2109.11066v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11066","description":"<p>Automated plant diagnosis is a technology that promises large increases in\ncost-efficiency for agriculture. However, multiple problems reduce the\neffectiveness of drones, including the inverse relationship between resolution\nand speed and the lack of adequate labeled training data. This paper presents a\ntwo-step machine learning approach that analyzes low-fidelity and high-fidelity\nimages in sequence, preserving efficiency as well as accuracy. Two\ndata-generators are also used to minimize class imbalance in the high-fidelity\ndataset and to produce low-fidelity data that is representative of UAV images.\nThe analysis of applications and methods is conducted on a database of\nhigh-fidelity apple tree images which are corrupted with class imbalance. The\napplication begins by generating high-fidelity data using generative networks\nand then uses this novel data alongside the original high-fidelity data to\nproduce low-fidelity images. A machine-learning identifier identifies plants\nand labels them as potentially diseased or not. A machine learning classifier\nis then given the potentially diseased plant images and returns actual\ndiagnoses for these plants. The results show an accuracy of 96.3% for the\nhigh-fidelity system and a 75.5% confidence level for our low-fidelity system.\nOur drone technology shows promising results in accuracy when compared to\nlabor-based methods of diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Aaditya Prasad</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_N/0/1/0/all/0/1\">Nikhil Mehta</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Horak_M/0/1/0/all/0/1\">Matthew Horak</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bae_W/0/1/0/all/0/1\">Wan D. Bae</a> (3) ((1) Tesla STEM High School, (2) Lockheed Martin Corporation, (3) Seattle University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Downsample for Segmentation of Ultra-High Resolution Images. (arXiv:2109.11071v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11071","description":"<p>Segmentation of ultra-high resolution images with deep learning is\nchallenging because of their enormous size, often millions or even billions of\npixels. Typical solutions drastically downsample the image uniformly to meet\nmemory constraints, implicitly assuming all pixels equally important by\nsampling at the same density at all spatial locations. However this assumption\nis not true and compromises the performance of deep learning techniques that\nhave proved powerful on standard-sized images. For example with uniform\ndownsampling, see green boxed region in Fig.1, the rider and bike do not have\nenough corresponding samples while the trees and buildings are oversampled, and\nlead to a negative effect on the segmentation prediction from the\nlow-resolution downsampled image. In this work we show that learning the\nspatially varying downsampling strategy jointly with segmentation offers\nadvantages in segmenting large images with limited computational budget. Fig.1\nshows that our method adapts the sampling density over different locations so\nthat more samples are collected from the small important regions and less from\nthe others, which in turn leads to better segmentation accuracy. We show on two\npublic and one local high-resolution datasets that our method consistently\nlearns sampling locations preserving more information and boosting segmentation\naccuracy over baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1\">Ryutaro Tanno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mertzanidou_T/0/1/0/all/0/1\">Thomy Mertzanidou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagiotaki_E/0/1/0/all/0/1\">Eleftheria Panagiotaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Factor Graph-Based Optimization Technique for Stereo Correspondence Estimation. (arXiv:2109.11077v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11077","description":"<p>Dense disparities among multiple views is essential for estimating the 3D\narchitecture of a scene based on the geometrical relationship among the scene\nand the views or cameras. Scenes with larger extents of heterogeneous textures,\ndiffering scene illumination among the multiple views and with occluding\nobjects affect the accuracy of the estimated disparities. Markov random fields\n(MRF) based methods for disparity estimation address these limitations using\nspatial dependencies among the observations and among the disparity estimates.\nThese methods, however, are limited by spatially fixed and smaller neighborhood\nsystems or cliques. In this work, we present a new factor graph-based\nprobabilistic graphical model for disparity estimation that allows a larger and\na spatially variable neighborhood structure determined based on the local scene\ncharacteristics. We evaluated our method using the Middlebury benchmark stereo\ndatasets and the Middlebury evaluation dataset version 3.0 and compared its\nperformance with recent state-of-the-art disparity estimation algorithms. The\nnew factor graph-based method provided disparity estimates with higher accuracy\nwhen compared to the recent non-learning- and learning-based disparity\nestimation algorithms. In addition to disparity estimation, our factor graph\nformulation can be useful for obtaining maximum a posteriori solution to\noptimization problems with complex and variable dependency structures as well\nas for other dense estimation problems such as optical flow estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shabanian_H/0/1/0/all/0/1\">Hanieh Shabanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_M/0/1/0/all/0/1\">Madhusudhanan Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unseen Object Amodal Instance Segmentation via Hierarchical Occlusion Modeling. (arXiv:2109.11103v1 [cs.RO])","link":"http://arxiv.org/abs/2109.11103","description":"<p>Instance-aware segmentation of unseen objects is essential for a robotic\nsystem in an unstructured environment. Although previous works achieved\nencouraging results, they were limited to segmenting the only visible regions\nof unseen objects. For robotic manipulation in a cluttered scene, amodal\nperception is required to handle the occluded objects behind others. This paper\naddresses Unseen Object Amodal Instance Segmentation (UOAIS) to detect 1)\nvisible masks, 2) amodal masks, and 3) occlusions on unseen object instances.\nFor this, we propose a Hierarchical Occlusion Modeling (HOM) scheme designed to\nreason about the occlusion by assigning a hierarchy to a feature fusion and\nprediction order. We evaluated our method on three benchmarks (tabletop,\nindoors, and bin environments) and achieved state-of-the-art (SOTA)\nperformance. Robot demos for picking up occluded objects, codes, and datasets\nare available at https://sites.google.com/view/uoais\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Back_S/0/1/0/all/0/1\">Seunghyeok Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1\">Sangjun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_R/0/1/0/all/0/1\">Raeyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_S/0/1/0/all/0/1\">Seongho Bak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoobin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rational Polynomial Camera Model Warping for Deep Learning Based Satellite Multi-View Stereo Matching. (arXiv:2109.11121v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11121","description":"<p>Satellite multi-view stereo (MVS) imagery is particularly suited for\nlarge-scale Earth surface reconstruction. Differing from the perspective camera\nmodel (pin-hole model) that is commonly used for close-range and aerial\ncameras, the cubic rational polynomial camera (RPC) model is the mainstream\nmodel for push-broom linear-array satellite cameras. However, the homography\nwarping used in the prevailing learning based MVS methods is only applicable to\npin-hole cameras. In order to apply the SOTA learning based MVS technology to\nthe satellite MVS task for large-scale Earth surface reconstruction, RPC\nwarping should be considered. In this work, we propose, for the first time, a\nrigorous RPC warping module. The rational polynomial coefficients are recorded\nas a tensor, and the RPC warping is formulated as a series of tensor\ntransformations. Based on the RPC warping, we propose the deep learning based\nsatellite MVS (SatMVS) framework for large-scale and wide depth range Earth\nsurface reconstruction. We also introduce a large-scale satellite image dataset\nconsisting of 519 5120${\\times}$5120 images, which we call the TLC SatMVS\ndataset. The satellite images were acquired from a three-line camera (TLC) that\ncatches triple-view images simultaneously, forming a valuable supplement to the\nexisting open-source WorldView-3 datasets with single-scanline images.\nExperiments show that the proposed RPC warping module and the SatMVS framework\ncan achieve a superior reconstruction accuracy compared to the pin-hole fitting\nmethod and conventional MVS methods. Code and data are available at\nhttps://github.com/WHU-GPCV/SatMVS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_J/0/1/0/all/0/1\">Jian Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_S/0/1/0/all/0/1\">Shunping Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Transfer Attacks With Unknown Data and Class Overlap. (arXiv:2109.11125v1 [cs.LG])","link":"http://arxiv.org/abs/2109.11125","description":"<p>The ability to transfer adversarial attacks from one model (the surrogate) to\nanother model (the victim) has been an issue of concern within the machine\nlearning (ML) community. The ability to successfully evade unseen models\nrepresents an uncomfortable level of ease toward implementing attacks. In this\nwork we note that as studied, current transfer attack research has an\nunrealistic advantage for the attacker: the attacker has the exact same\ntraining data as the victim. We present the first study of transferring\nadversarial attacks focusing on the data available to attacker and victim under\nimperfect settings without querying the victim, where there is some variable\nlevel of overlap in the exact data used or in the classes learned by each\nmodel. This threat model is relevant to applications in medicine, malware, and\nothers. Under this new threat model attack success rate is not correlated with\ndata or class overlap in the way one would expect, and varies with dataset.\nThis makes it difficult for attacker and defender to reason about each other\nand contributes to the broader study of model robustness and security. We\nremedy this by developing a masked version of Projected Gradient Descent that\nsimulates class disparity, which enables the attacker to reliably estimate a\nlower-bound on their attack's success.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richards_L/0/1/0/all/0/1\">Luke E. Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Andr&#xe9; Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capps_R/0/1/0/all/0/1\">Ryan Capps</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsythe_S/0/1/0/all/0/1\">Steven Forsythe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matuszek_C/0/1/0/all/0/1\">Cynthia Matuszek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OH-Former: Omni-Relational High-Order Transformer for Person Re-Identification. (arXiv:2109.11159v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11159","description":"<p>Transformers have shown preferable performance on many vision tasks. However,\nfor the task of person re-identification (ReID), vanilla transformers leave the\nrich contexts on high-order feature relations under-exploited and deteriorate\nlocal feature details, which are insufficient due to the dramatic variations of\npedestrians. In this work, we propose an Omni-Relational High-Order Transformer\n(OH-Former) to model omni-relational features for ReID. First, to strengthen\nthe capacity of visual representation, instead of obtaining the attention\nmatrix based on pairs of queries and isolated keys at each spatial location, we\ntake a step further to model high-order statistics information for the\nnon-local mechanism. We share the attention weights in the corresponding layer\nof each order with a prior mixing mechanism to reduce the computation cost.\nThen, a convolution-based local relation perception module is proposed to\nextract the local relations and 2D position information. The experimental\nresults of our model are superior promising, which show state-of-the-art\nperformance on Market-1501, DukeMTMC, MSMT17 and Occluded-Duke datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xianing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jialang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiale Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shenghua Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clustering performance analysis using new correlation based cluster validity indices. (arXiv:2109.11172v1 [stat.ML])","link":"http://arxiv.org/abs/2109.11172","description":"<p>There are various cluster validity measures used for evaluating clustering\nresults. One of the main objective of using these measures is to seek the\noptimal unknown number of clusters. Some measures work well for clusters with\ndifferent densities, sizes and shapes. Yet, one of the weakness that those\nvalidity measures share is that they sometimes provide only one clear optimal\nnumber of clusters. That number is actually unknown and there might be more\nthan one potential sub-optimal options that a user may wish to choose based on\ndifferent applications. We develop two new cluster validity indices based on a\ncorrelation between an actual distance between a pair of data points and a\ncentroid distance of clusters that the two points locate in. Our proposed\nindices constantly yield several peaks at different numbers of clusters which\novercome the weakness previously stated. Furthermore, the introduced\ncorrelation can also be used for evaluating the quality of a selected\nclustering result. Several experiments in different scenarios including the\nwell-known iris data set and a real-world marketing application have been\nconducted in order to compare the proposed validity indices with several\nwell-known ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Wiroonsri_N/0/1/0/all/0/1\">Nathakhun Wiroonsri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting the Timing of Camera Movements From the Kinematics of Instruments in Robotic-Assisted Surgery Using Artificial Neural Networks. (arXiv:2109.11192v1 [cs.LG])","link":"http://arxiv.org/abs/2109.11192","description":"<p>Robotic-assisted surgeries benefit both surgeons and patients, however,\nsurgeons frequently need to adjust the endoscopic camera to achieve good\nviewpoints. Simultaneously controlling the camera and the surgical instruments\nis impossible, and consequentially, these camera adjustments repeatedly\ninterrupt the surgery. Autonomous camera control could help overcome this\nchallenge, but most existing systems are reactive, e.g., by having the camera\nfollow the surgical instruments. We propose a predictive approach for\nanticipating when camera movements will occur using artificial neural networks.\nWe used the kinematic data of the surgical instruments, which were recorded\nduring robotic-assisted surgical training on porcine models. We split the data\ninto segments, and labeled each either as a segment that immediately precedes a\ncamera movement, or one that does not. Due to the large class imbalance, we\ntrained an ensemble of networks, each on a balanced sub-set of the training\ndata. We found that the instruments' kinematic data can be used to predict when\ncamera movements will occur, and evaluated the performance on different segment\ndurations and ensemble sizes. We also studied how much in advance an upcoming\ncamera movement can be predicted, and found that predicting a camera movement\n0.25, 0.5, and 1 second before they occurred achieved 98%, 94%, and 84%\naccuracy relative to the prediction of an imminent camera movement. This\nindicates that camera movement events can be predicted early enough to leave\ntime for computing and executing an autonomous camera movement and suggests\nthat an autonomous camera controller for RAMIS may one day be feasible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kossowsky_H/0/1/0/all/0/1\">Hanna Kossowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nisky_I/0/1/0/all/0/1\">Ilana Nisky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-grained 3D Face Dense Registration: An Optimal Dividing and Diffusing Method. (arXiv:2109.11204v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11204","description":"<p>Dense vertex-to-vertex correspondence between 3D faces is a fundamental and\nchallenging issue for 3D&amp;2D face analysis. While the sparse landmarks have\nanatomically ground-truth correspondence, the dense vertex correspondences on\nmost facial regions are unknown. In this view, the current literatures commonly\nresult in reasonable but diverse solutions, which deviate from the optimum to\nthe 3D face dense registration problem. In this paper, we revisit dense\nregistration by a dimension-degraded problem, i.e. proportional segmentation of\na line, and employ an iterative dividing and diffusing method to reach the\nfinal solution uniquely. This method is then extended to 3D surface by\nformulating a local registration problem for dividing and a linear least-square\nproblem for diffusing, with constraints on fixed features. On this basis, we\nfurther propose a multi-resolution algorithm to accelerate the computational\nprocess. The proposed method is linked to a novel local scaling metric, where\nwe illustrate the physical meaning as smooth rearrangement for local cells of\n3D facial shapes. Extensive experiments on public datasets demonstrate the\neffectiveness of the proposed method in various aspects. Generally, the\nproposed method leads to coherent local registrations and elegant mesh grid\nroutines for fine-grained 3D face dense registrations, which benefits many\ndownstream applications significantly. It can also be applied to dense\ncorrespondence for other format of data which are not limited to face. The core\ncode will be publicly available at\nhttps://github.com/NaughtyZZ/3D_face_dense_registration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhenfeng Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Silong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shihong Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Emotional Relationship Recognition in Drama Videos: Dataset and Benchmark. (arXiv:2109.11243v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11243","description":"<p>Recognizing the emotional state of people is a basic but challenging task in\nvideo understanding. In this paper, we propose a new task in this field, named\nPairwise Emotional Relationship Recognition (PERR). This task aims to recognize\nthe emotional relationship between the two interactive characters in a given\nvideo clip. It is different from the traditional emotion and social relation\nrecognition task. Varieties of information, consisting of character appearance,\nbehaviors, facial emotions, dialogues, background music as well as subtitles\ncontribute differently to the final results, which makes the task more\nchallenging but meaningful in developing more advanced multi-modal models. To\nfacilitate the task, we develop a new dataset called Emotional RelAtionship of\ninTeractiOn (ERATO) based on dramas and movies. ERATO is a large-scale\nmulti-modal dataset for PERR task, which has 31,182 video clips, lasting about\n203 video hours. Different from the existing datasets, ERATO contains\ninteraction-centric videos with multi-shots, varied video length, and multiple\nmodalities including visual, audio and text. As a minor contribution, we\npropose a baseline model composed of Synchronous Modal-Temporal Attention\n(SMTA) unit to fuse the multi-modal information for the PERR task. In contrast\nto other prevailing attention mechanisms, our proposed SMTA can steadily\nimprove the performance by about 1\\%. We expect the ERATO as well as our\nproposed SMTA to open up a new way for PERR task in video understanding and\nfurther improve the research of multi-modal fusion methodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Longjun Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Dense Video Grounding via Parallel Regression. (arXiv:2109.11265v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11265","description":"<p>Video grounding aims to localize the corresponding video moment in an\nuntrimmed video given a language query. Existing methods often address this\ntask in an indirect way, by casting it as a proposal-and-match or\nfusion-and-detection problem. Solving these surrogate problems often requires\nsophisticated label assignment during training and hand-crafted removal of\nnear-duplicate results. Meanwhile, existing works typically focus on sparse\nvideo grounding with a single sentence as input, which could result in\nambiguous localization due to its unclear description. In this paper, we tackle\na new problem of dense video grounding, by simultaneously localizing multiple\nmoments with a paragraph as input. From a perspective on video grounding as\nlanguage conditioned regression, we present an end-to-end parallel decoding\nparadigm by re-purposing a Transformer-alike architecture (PRVG). The key\ndesign in our PRVG is to use languages as queries, and directly regress the\nmoment boundaries based on language-modulated visual representations. Thanks to\nits simplicity in design, our PRVG framework can be applied in different\ntesting schemes (sparse or dense grounding) and allows for efficient inference\nwithout any post-processing technique. In addition, we devise a robust\nproposal-level attention loss to guide the training of PRVG, which is invariant\nto moment duration and contributes to model convergence. We perform experiments\non two video grounding benchmarks of ActivityNet Captions and TACoS,\ndemonstrating that our PRVG can significantly outperform previous methods. We\nalso perform in-depth studies to investigate the effectiveness of parallel\nregression paradigm on video grounding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1\">Fengyuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weilin Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Strategies for Industrial Surface Defect Detection Systems. (arXiv:2109.11304v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11304","description":"<p>Deep learning methods have proven to outperform traditional computer vision\nmethods in various areas of image processing. However, the application of deep\nlearning in industrial surface defect detection systems is challenging due to\nthe insufficient amount of training data, the expensive data generation\nprocess, the small size, and the rare occurrence of surface defects. From\nliterature and a polymer products manufacturing use case, we identify design\nrequirements which reflect the aforementioned challenges. Addressing these, we\nconceptualize design principles and features informed by deep learning\nresearch. Finally, we instantiate and evaluate the gained design knowledge in\nthe form of actionable guidelines and strategies based on an industrial surface\ndefect detection use case. This article, therefore, contributes to academia as\nwell as practice by (1) systematically identifying challenges for the\nindustrial application of deep learning-based surface defect detection, (2)\nstrategies to overcome these, and (3) an experimental case study assessing the\nstrategies' applicability and usefulness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_D/0/1/0/all/0/1\">Dominik Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinzel_S/0/1/0/all/0/1\">Simon Heinzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischhoffshausen_J/0/1/0/all/0/1\">Johannes Kunze von Bischhoffshausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1\">Niklas K&#xfc;hl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-resolution deep learning pipeline for dense large scale point clouds. (arXiv:2109.11311v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11311","description":"<p>Recent development of 3D sensors allows the acquisition of extremely dense 3D\npoint clouds of large-scale scenes. The main challenge of processing such large\npoint clouds remains in the size of the data, which induce expensive\ncomputational and memory cost. In this context, the full resolution cloud is\nparticularly hard to process, and details it brings are rarely exploited.\nAlthough fine-grained details are important for detection of small objects,\nthey can alter the local geometry of large structural parts and mislead deep\nlearning networks. In this paper, we introduce a new generic deep learning\npipeline to exploit the full precision of large scale point clouds, but only\nfor objects that require details. The core idea of our approach is to split up\nthe process into multiple sub-networks which operate on different resolutions\nand with each their specific classes to retrieve. Thus, the pipeline allows\neach class to benefit either from noise and memory cost reduction of a\nsub-sampling or from fine-grained details.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richard_T/0/1/0/all/0/1\">Thomas Richard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupont_F/0/1/0/all/0/1\">Florent Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavoue_G/0/1/0/all/0/1\">Guillaume Lavoue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Hilti SLAM Challenge Dataset. (arXiv:2109.11316v1 [cs.RO])","link":"http://arxiv.org/abs/2109.11316","description":"<p>Accurate and robust pose estimation is a fundamental capability for\nautonomous systems to navigate, map and perform tasks. Particularly,\nconstruction environments pose challenging problem to Simultaneous Localization\nand Mapping (SLAM) algorithms due to sparsity, varying illumination conditions,\nand dynamic objects. Current academic research in SLAM is focused on developing\nmore accurate and robust algorithms for example by fusing different sensor\nmodalities. To help this research, we propose a new dataset, the Hilti SLAM\nChallenge Dataset. The sensor platform used to collect this dataset contains a\nnumber of visual, lidar and inertial sensors which have all been rigorously\ncalibrated. All data is temporally aligned to support precise multi-sensor\nfusion. Each dataset includes accurate ground truth to allow direct testing of\nSLAM results. Raw data as well as intrinsic and extrinsic sensor calibration\ndata from twelve datasets in various environments is provided. Each environment\nrepresents common scenarios found in building construction sites in various\nstages of completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Helmberger_M/0/1/0/all/0/1\">Michael Helmberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morin_K/0/1/0/all/0/1\">Kristian Morin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nitish Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yufeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cioffi_G/0/1/0/all/0/1\">Giovanni Cioffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Generalized and Incremental Few-Shot Object Detection. (arXiv:2109.11336v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11336","description":"<p>Real-world object detection is highly desired to be equipped with the\nlearning expandability that can enlarge its detection classes incrementally.\nMoreover, such learning from only few annotated training samples further adds\nthe flexibility for the object detector, which is highly expected in many\napplications such as autonomous driving, robotics, etc. However, such\nsequential learning scenario with few-shot training samples generally causes\ncatastrophic forgetting and dramatic overfitting. In this paper, to address the\nabove incremental few-shot learning issues, a novel Incremental Few-Shot Object\nDetection (iFSOD) method is proposed to enable the effective continual learning\nfrom few-shot samples. Specifically, a Double-Branch Framework (DBF) is\nproposed to decouple the feature representation of base and novel (few-shot)\nclass, which facilitates both the old-knowledge retention and new-class\nadaption simultaneously. Furthermore, a progressive model updating rule is\ncarried out to preserve the long-term memory on old classes effectively when\nadapt to sequential new classes. Moreover, an inter-task class separation loss\nis proposed to extend the decision region of new-coming classes for better\nfeature discrimination. We conduct experiments on both Pascal VOC and MS-COCO,\nwhich demonstrate that our method can effectively solve the problem of\nincremental few-shot detection and significantly improve the detection accuracy\non both base and novel classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiting Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haiyue Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teo_C/0/1/0/all/0/1\">Chek Sing Teo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Cheng Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadakkepat_P/0/1/0/all/0/1\">Prahlad Vadakkepat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Tong Heng Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRANet: Point Cloud Registration with an Artificial Agent. (arXiv:2109.11349v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11349","description":"<p>Point cloud registration plays a critical role in a multitude of computer\nvision tasks, such as pose estimation and 3D localization. Recently, a plethora\nof deep learning methods were formulated that aim to tackle this problem. Most\nof these approaches find point or feature correspondences, from which the\ntransformations are computed. We give a different perspective and frame the\nregistration problem as a Markov Decision Process. Instead of directly\nsearching for the transformation, the problem becomes one of finding a sequence\nof translation and rotation actions that is equivalent to this transformation.\nTo this end, we propose an artificial agent trained end-to-end using deep\nsupervised learning. In contrast to conventional reinforcement learning\ntechniques, the observations are sampled i.i.d. and thus no experience replay\nbuffer is required, resulting in a more streamlined training process.\nExperiments on ModelNet40 show results comparable or superior to the state of\nthe art in the case of clean, noisy and partially visible datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tse_L/0/1/0/all/0/1\">Lisa Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amadou_A/0/1/0/all/0/1\">Abdoul Aziz Amadou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georget_A/0/1/0/all/0/1\">Axen Georget</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuysuzoglu_A/0/1/0/all/0/1\">Ahmet Tuysuzoglu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances of Continual Learning in Computer Vision: An Overview. (arXiv:2109.11369v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11369","description":"<p>In contrast to batch learning where all training data is available at once,\ncontinual learning represents a family of methods that accumulate knowledge and\nlearn continuously with data available in sequential order. Similar to the\nhuman learning process with the ability of learning, fusing, and accumulating\nnew knowledge coming at different time steps, continual learning is considered\nto have high practical significance. Hence, continual learning has been studied\nin various artificial intelligence tasks. In this paper, we present a\ncomprehensive review of the recent progress of continual learning in computer\nvision. In particular, the works are grouped by their representative\ntechniques, including regularization, knowledge distillation, memory,\ngenerative replay, parameter isolation, and a combination of the above\ntechniques. For each category of these techniques, both its characteristics and\napplications in computer vision are presented. At the end of this overview,\nseveral subareas, where continuous knowledge accumulation is potentially\nhelpful while continual learning has not been well studied, are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Haoxuan Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Li Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Attention-guided Dense Network for Images Fusion. (arXiv:2109.11393v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11393","description":"<p>In recent years, various applications in computer vision have achieved\nsubstantial progress based on deep learning, which has been widely used for\nimage fusion and shown to achieve adequate performance. However, suffering from\nlimited ability in modelling the spatial correspondence of different source\nimages, it still remains a great challenge for existing unsupervised image\nfusion models to extract appropriate feature and achieves adaptive and balanced\nfusion. In this paper, we propose a novel cross attention-guided image fusion\nnetwork, which is a unified and unsupervised framework for multi-modal image\nfusion, multi-exposure image fusion, and multi-focus image fusion. Different\nfrom the existing self-attention module, our cross attention module focus on\nmodelling the cross-correlation between different source images. Using the\nproposed cross attention module as core block, a densely connected cross\nattention-guided network is built to dynamically learn the spatial\ncorrespondence to derive better alignment of important details from different\ninput images. Meanwhile, an auxiliary branch is also designed to model the\nlong-range information, and a merging network is attached to finally\nreconstruct the fusion image. Extensive experiments have been carried out on\npublicly available datasets, and the results demonstrate that the proposed\nmodel outperforms the state-of-the-art quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhengwen Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zaiyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yulian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiangyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Graph Generation for Better Image Captioning?. (arXiv:2109.11398v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11398","description":"<p>We investigate the incorporation of visual relationships into the task of\nsupervised image caption generation by proposing a model that leverages\ndetected objects and auto-generated visual relationships to describe images in\nnatural language. To do so, we first generate a scene graph from raw image\npixels by identifying individual objects and visual relationships between them.\nThis scene graph then serves as input to our graph-to-text model, which\ngenerates the final caption. In contrast to previous approaches, our model thus\nexplicitly models the detection of objects and visual relationships in the\nimage. For our experiments we construct a new dataset from the intersection of\nVisual Genome and MS COCO, consisting of images with both a corresponding gold\nscene graph and human-authored caption. Our results show that our methods\noutperform existing state-of-the-art end-to-end models that generate image\ndescriptions directly from raw input pixels when compared in terms of the BLEU\nand METEOR evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozes_M/0/1/0/all/0/1\">Maximilian Mozes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golkov_V/0/1/0/all/0/1\">Vladimir Golkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Skeleton-Driven Neural Occupancy Representation for Articulated Hands. (arXiv:2109.11399v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11399","description":"<p>We present Hand ArticuLated Occupancy (HALO), a novel representation of\narticulated hands that bridges the advantages of 3D keypoints and neural\nimplicit surfaces and can be used in end-to-end trainable architectures. Unlike\nexisting statistical parametric hand models (e.g.~MANO), HALO directly\nleverages 3D joint skeleton as input and produces a neural occupancy volume\nrepresenting the posed hand surface. The key benefits of HALO are (1) it is\ndriven by 3D key points, which have benefits in terms of accuracy and are\neasier to learn for neural networks than the latent hand-model parameters; (2)\nit provides a differentiable volumetric occupancy representation of the posed\nhand; (3) it can be trained end-to-end, allowing the formulation of losses on\nthe hand surface that benefit the learning of 3D keypoints. We demonstrate the\napplicability of HALO to the task of conditional generation of hands that grasp\n3D objects. The differentiable nature of HALO is shown to improve the quality\nof the synthesized hands both in terms of physical plausibility and user\npreference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karunratanakul_K/0/1/0/all/0/1\">Korrawe Karunratanakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zicong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Memory Matching Network for Video Object Segmentation. (arXiv:2109.11404v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11404","description":"<p>We present Hierarchical Memory Matching Network (HMMN) for semi-supervised\nvideo object segmentation. Based on a recent memory-based method [33], we\npropose two advanced memory read modules that enable us to perform memory\nreading in multiple scales while exploiting temporal smoothness. We first\npropose a kernel guided memory matching module that replaces the non-local\ndense memory read, commonly adopted in previous memory-based methods. The\nmodule imposes the temporal smoothness constraint in the memory read, leading\nto accurate memory retrieval. More importantly, we introduce a hierarchical\nmemory matching scheme and propose a top-k guided memory matching module in\nwhich memory read on a fine-scale is guided by that on a coarse-scale. With the\nmodule, we perform memory read in multiple scales efficiently and leverage both\nhigh-level semantic and low-level fine-grained memory features to predict\ndetailed object masks. Our network achieves state-of-the-art performance on the\nvalidation sets of DAVIS 2016/2017 (90.8% and 84.7%) and YouTube-VOS 2018/2019\n(82.6% and 82.5%), and test-dev set of DAVIS 2017 (78.6%). The source code and\nmodel are available online: https://github.com/Hongje/HMMN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seong_H/0/1/0/all/0/1\">Hongje Seong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seoung Wug Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seongwon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suhyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Euntai Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Layered Neural Atlases for Consistent Video Editing. (arXiv:2109.11418v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11418","description":"<p>We present a method that decomposes, or \"unwraps\", an input video into a set\nof layered 2D atlases, each providing a unified representation of the\nappearance of an object (or background) over the video. For each pixel in the\nvideo, our method estimates its corresponding 2D coordinate in each of the\natlases, giving us a consistent parameterization of the video, along with an\nassociated alpha (opacity) value. Importantly, we design our atlases to be\ninterpretable and semantic, which facilitates easy and intuitive editing in the\natlas domain, with minimal manual work required. Edits applied to a single 2D\natlas (or input video frame) are automatically and consistently mapped back to\nthe original video frames, while preserving occlusions, deformation, and other\ncomplex scene effects such as shadows and reflections. Our method employs a\ncoordinate-based Multilayer Perceptron (MLP) representation for mappings,\natlases, and alphas, which are jointly optimized on a per-video basis, using a\ncombination of video reconstruction and regularization losses. By operating\npurely in 2D, our method does not require any prior 3D knowledge about scene\ngeometry or camera poses, and can handle complex dynamic real world videos. We\ndemonstrate various video editing applications, including texture mapping,\nvideo style transfer, image-to-video texture transfer, and\nsegmentation/labeling propagation, all automatically produced by editing a\nsingle 2D atlas image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1\">Yoni Kasten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofri_D/0/1/0/all/0/1\">Dolev Ofri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_O/0/1/0/all/0/1\">Oliver Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1\">Tali Dekel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepRare: Generic Unsupervised Visual Attention Models. (arXiv:2109.11439v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11439","description":"<p>Human visual system is modeled in engineering field providing\nfeature-engineered methods which detect contrasted/surprising/unusual data into\nimages. This data is \"interesting\" for humans and leads to numerous\napplications. Deep learning (DNNs) drastically improved the algorithms\nefficiency on the main benchmark datasets. However, DNN-based models are\ncounter-intuitive: surprising or unusual data is by definition difficult to\nlearn because of its low occurrence probability. In reality, DNN-based models\nmainly learn top-down features such as faces, text, people, or animals which\nusually attract human attention, but they have low efficiency in extracting\nsurprising or unusual data in the images. In this paper, we propose a new\nvisual attention model called DeepRare2021 (DR21) which uses the power of DNNs\nfeature extraction and the genericity of feature-engineered algorithms. This\nalgorithm is an evolution of a previous version called DeepRare2019 (DR19)\nbased on a common framework. DR21 1) does not need any training and uses the\ndefault ImageNet training, 2) is fast even on CPU, 3) is tested on four very\ndifferent eye-tracking datasets showing that the DR21 is generic and is always\nin the within the top models on all datasets and metrics while no other model\nexhibits such a regularity and genericity. Finally DR21 4) is tested with\nseveral network architectures such as VGG16 (V16), VGG19 (V19) and MobileNetV2\n(MN2) and 5) it provides explanation and transparency on which parts of the\nimage are the most surprising at different levels despite the use of a\nDNN-based feature extractor. DeepRare2021 code can be found at\nhttps://github.com/numediart/VisualAttention-RareFamil}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_P/0/1/0/all/0/1\">Phutphalla Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancas_M/0/1/0/all/0/1\">Matei Mancas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gosselin_B/0/1/0/all/0/1\">Bernard Gosselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_K/0/1/0/all/0/1\">Kimtho Po</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisit Geophysical Imaging in A New View of Physics-informed Generative Adversarial Learning. (arXiv:2109.11452v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11452","description":"<p>Seismic full waveform inversion (FWI) is a powerful geophysical imaging\ntechnique that produces high-resolution subsurface models by iteratively\nminimizing the misfit between the simulated and observed seismograms.\nUnfortunately, conventional FWI with least-squares function suffers from many\ndrawbacks such as the local-minima problem and computation of explicit\ngradient. It is particularly challenging with the contaminated measurements or\npoor starting models. Recent works relying on partial differential equations\nand neural networks show promising performance for two-dimensional FWI.\nInspired by the competitive learning of generative adversarial networks, we\nproposed an unsupervised learning paradigm that integrates wave equation with a\ndiscriminate network to accurately estimate the physically consistent models in\na distribution sense. Our framework needs no labelled training data nor\npretraining of the network, is flexible to achieve multi-parameters inversion\nwith minimal user interaction. The proposed method faithfully recovers the\nwell-known synthetic models that outperforms the classical algorithms.\nFurthermore, our work paves the way to sidestep the local-minima issue via\nreducing the sensitivity to initial models and noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fangshu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1\">Jianwei Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Segmentation-assisted Scene Completion for LiDAR Point Clouds. (arXiv:2109.11453v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11453","description":"<p>Outdoor scene completion is a challenging issue in 3D scene understanding,\nwhich plays an important role in intelligent robotics and autonomous driving.\nDue to the sparsity of LiDAR acquisition, it is far more complex for 3D scene\ncompletion and semantic segmentation. Since semantic features can provide\nconstraints and semantic priors for completion tasks, the relationship between\nthem is worth exploring. Therefore, we propose an end-to-end semantic\nsegmentation-assisted scene completion network, including a 2D completion\nbranch and a 3D semantic segmentation branch. Specifically, the network takes a\nraw point cloud as input, and merges the features from the segmentation branch\ninto the completion branch hierarchically to provide semantic information. By\nadopting BEV representation and 3D sparse convolution, we can benefit from the\nlower operand while maintaining effective expression. Besides, the decoder of\nthe segmentation branch is used as an auxiliary, which can be discarded in the\ninference stage to save computational consumption. Extensive experiments\ndemonstrate that our method achieves competitive performance on SemanticKITTI\ndataset with low latency. Code and models will be released at\nhttps://github.com/jokester-zzz/SSA-SC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuemeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Hao Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Feng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Learning for Semi-supervised Temporal Language Grounding. (arXiv:2109.11475v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11475","description":"<p>Given a text description, Temporal Language Grounding (TLG) aims to localize\ntemporal boundaries of the segments that contain the specified semantics in an\nuntrimmed video. TLG is inherently a challenging task, as it requires to have\ncomprehensive understanding of both video contents and text sentences. Previous\nworks either tackle this task in a fully-supervised setting that requires a\nlarge amount of manual annotations or in a weakly supervised setting that\ncannot achieve satisfactory performance. To achieve good performance with\nlimited annotations, we tackle this task in a semi-supervised way and propose a\nunified Semi-supervised Temporal Language Grounding (STLG) framework. STLG\nconsists of two parts: (1) A pseudo label generation module that produces\nadaptive instant pseudo labels for unlabeled data based on predictions from a\nteacher model; (2) A self-supervised feature learning module with two\nsequential perturbations, i.e., time lagging and time scaling, for improving\nthe video representation by inter-modal and intra-modal contrastive learning.\nWe conduct experiments on the ActivityNet-CD-OOD and Charades-CD-OOD datasets\nand the results demonstrate that our proposed STLG framework achieve\ncompetitive performance compared to fully-supervised state-of-the-art methods\nwith only a small portion of temporal annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Fan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Tuberculosis (TB) Prediction using Synthetically Generated Computed Tomography (CT) Images. (arXiv:2109.11480v1 [eess.IV])","link":"http://arxiv.org/abs/2109.11480","description":"<p>The evaluation of infectious disease processes on radiologic images is an\nimportant and challenging task in medical image analysis. Pulmonary infections\ncan often be best imaged and evaluated through computed tomography (CT) scans,\nwhich are often not available in low-resource environments and difficult to\nobtain for critically ill patients. On the other hand, X-ray, a different type\nof imaging procedure, is inexpensive, often available at the bedside and more\nwidely available, but offers a simpler, two dimensional image. We show that by\nrelying on a model that learns to generate CT images from X-rays synthetically,\nwe can improve the automatic disease classification accuracy and provide\nclinicians with a different look at the pulmonary disease process.\nSpecifically, we investigate Tuberculosis (TB), a deadly bacterial infectious\ndisease that predominantly affects the lungs, but also other organ systems. We\nshow that relying on synthetically generated CT improves TB identification by\n7.50% and distinguishes TB properties up to 12.16% better than the X-ray\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lewis_A/0/1/0/all/0/1\">Ashia Lewis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmoodi_E/0/1/0/all/0/1\">Evanjelin Mahmoodi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyue Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Coffee_M/0/1/0/all/0/1\">Megan Coffee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sizikova_E/0/1/0/all/0/1\">Elena Sizikova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGD: Label-guided Self-distillation for Object Detection. (arXiv:2109.11496v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11496","description":"<p>In this paper, we propose the first self-distillation framework for general\nobject detection, termed LGD (Label-Guided self-Distillation). Previous studies\nrely on a strong pretrained teacher to provide instructive knowledge for\ndistillation. However, this could be unavailable in real-world scenarios.\nInstead, we generate an instructive knowledge by inter-and-intra relation\nmodeling among objects, requiring only student representations and regular\nlabels. In detail, our framework involves sparse label-appearance encoding,\ninter-object relation adaptation and intra-object knowledge mapping to obtain\nthe instructive knowledge. Modules in LGD are trained end-to-end with student\ndetector and are discarded in inference. Empirically, LGD obtains decent\nresults on various detectors, datasets, and extensive task like instance\nsegmentation. For example in MS-COCO dataset, LGD improves RetinaNet with\nResNet-50 under 2x single-scale training from 36.2% to 39.0% mAP (+ 2.8%). For\nmuch stronger detectors like FCOS with ResNeXt-101 DCN v2 under 2x multi-scale\ntraining (46.1%), LGD achieves 47.9% (+ 1.8%). For pedestrian detection in\nCrowdHuman dataset, LGD boosts mMR by 2.3% for Faster R-CNN with ResNet-50.\nCompared with a classical teacher-based method FGFI, LGD not only performs\nbetter without requiring pretrained teacher but also with 51% lower training\ncost beyond inherent student learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zijian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging distributed contact force measurements for slip detection: a physics-based approach enabled by a data-driven tactile sensor. (arXiv:2109.11504v1 [cs.RO])","link":"http://arxiv.org/abs/2109.11504","description":"<p>Grasping objects whose physical properties are unknown is still a great\nchallenge in robotics. Most solutions rely entirely on visual data to plan the\nbest grasping strategy. However, to match human abilities and be able to\nreliably pick and hold unknown objects, the integration of an artificial sense\nof touch in robotic systems is pivotal. This paper describes a novel\nmodel-based slip detection pipeline that can predict possibly failing grasps in\nreal-time and signal a necessary increase in grip force. As such, the slip\ndetector does not rely on manually collected data, but exploits physics to\ngeneralize across different tasks. To evaluate the approach, a state-of-the-art\nvision-based tactile sensor that accurately estimates distributed forces was\nintegrated into a grasping setup composed of a six degrees-of-freedom cobot and\na two-finger gripper. Results show that the system can reliably predict slip\nwhile manipulating objects of different shapes, materials, and weights. The\nsensor can detect both translational and rotational slip in various scenarios,\nmaking it suitable to improve the stability of a grasp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Griffa_P/0/1/0/all/0/1\">Pietro Griffa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sferrazza_C/0/1/0/all/0/1\">Carmelo Sferrazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAndrea_R/0/1/0/all/0/1\">Raffaello D&#x27;Andrea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How much \"human-like\" visual experience do current self-supervised learning algorithms need to achieve human-level object recognition?. (arXiv:2109.11523v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11523","description":"<p>This paper addresses a fundamental question: how good are our current\nself-supervised visual representation learning algorithms relative to humans?\nMore concretely, how much \"human-like\", natural visual experience would these\nalgorithms need in order to reach human-level performance in a complex,\nrealistic visual object recognition task such as ImageNet? Using a scaling\nexperiment, here we estimate that the answer is on the order of a million years\nof natural visual experience, in other words several orders of magnitude longer\nthan a human lifetime. However, this estimate is quite sensitive to some\nunderlying assumptions, underscoring the need to run carefully controlled human\nexperiments. We discuss the main caveats surrounding our estimate and the\nimplications of this rather surprising result.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orhan_A/0/1/0/all/0/1\">A. Emin Orhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End AI-based MRI Reconstruction and Lesion Detection Pipeline for Evaluation of Deep Learning Image Reconstruction. (arXiv:2109.11524v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11524","description":"<p>Deep learning techniques have emerged as a promising approach to highly\naccelerated MRI. However, recent reconstruction challenges have shown several\ndrawbacks in current deep learning approaches, including the loss of fine image\ndetails even using models that perform well in terms of global quality metrics.\nIn this study, we propose an end-to-end deep learning framework for image\nreconstruction and pathology detection, which enables a clinically aware\nevaluation of deep learning reconstruction quality. The solution is\ndemonstrated for a use case in detecting meniscal tears on knee MRI studies,\nultimately finding a loss of fine image details with common reconstruction\nmethods expressed as a reduced ability to detect important pathology like\nmeniscal tears. Despite the common practice of quantitative reconstruction\nmethodology evaluation with metrics such as SSIM, impaired pathology detection\nas an automated pathology-based reconstruction evaluation approach suggests\nexisting quantitative methods do not capture clinically important\nreconstruction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_M/0/1/0/all/0/1\">Michael S. Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MARMOT: A Deep Learning Framework for Constructing Multimodal Representations for Vision-and-Language Tasks. (arXiv:2109.11526v1 [cs.CV])","link":"http://arxiv.org/abs/2109.11526","description":"<p>Political activity on social media presents a data-rich window into political\nbehavior, but the vast amount of data means that almost all content analyses of\nsocial media require a data labeling step. However, most automated machine\nclassification methods ignore the multimodality of posted content, focusing\neither on text or images. State-of-the-art vision-and-language models are\nunusable for most political science research: they require all observations to\nhave both image and text and require computationally expensive pretraining.\nThis paper proposes a novel vision-and-language framework called multimodal\nrepresentations using modality translation (MARMOT). MARMOT presents two\nmethodological contributions: it can construct representations for observations\nmissing image or text, and it replaces the computationally expensive\npretraining with modality translation. MARMOT outperforms an ensemble text-only\nclassifier in 19 of 20 categories in multilabel classifications of tweets\nreporting election incidents during the 2016 U.S. general election. Moreover,\nMARMOT shows significant improvements over the results of benchmark multimodal\nmodels on the Hateful Memes dataset, improving the best result set by\nVisualBERT in terms of accuracy from 0.6473 to 0.6760 and area under the\nreceiver operating characteristic curve (AUC) from 0.7141 to 0.7530.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Patrick Y. Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mebane_W/0/1/0/all/0/1\">Walter R. Mebane Jr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation. (arXiv:1907.10326v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1907.10326","description":"<p>Estimating accurate depth from a single image is challenging because it is an\nill-posed problem as infinitely many 3D scenes can be projected to the same 2D\nscene. However, recent works based on deep convolutional neural networks show\ngreat progress with plausible results. The convolutional neural networks are\ngenerally composed of two parts: an encoder for dense feature extraction and a\ndecoder for predicting the desired depth. In the encoder-decoder schemes,\nrepeated strided convolution and spatial pooling layers lower the spatial\nresolution of transitional outputs, and several techniques such as skip\nconnections or multi-layer deconvolutional networks are adopted to recover the\noriginal resolution for effective dense prediction. In this paper, for more\neffective guidance of densely encoded features to the desired depth prediction,\nwe propose a network architecture that utilizes novel local planar guidance\nlayers located at multiple stages in the decoding phase. We show that the\nproposed method outperforms the state-of-the-art works with significant margin\nevaluating on challenging benchmarks. We also provide results from an ablation\nstudy to validate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jin Han Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Myung-Kyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_D/0/1/0/all/0/1\">Dong Wook Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_I/0/1/0/all/0/1\">Il Hong Suh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Pruning for Model Compression. (arXiv:1911.09817v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.09817","description":"<p>Previous AutoML pruning works utilized individual layer features to\nautomatically prune filters. We analyze the correlation for two layers from the\ndifferent blocks which have a short-cut structure. It shows that, in one block,\nthe deeper layer has many redundant filters which can be represented by filters\nin the former layer. So, it is necessary to take information from other layers\ninto consideration in pruning. In this paper, a novel pruning method, named\nGraphPruning, is proposed. Any series of the network is viewed as a graph. To\nautomatically aggregate neighboring features for each node, a graph aggregator\nbased on graph convolution networks(GCN) is designed. In the training stage, a\nPruningNet that is given aggregated node features generates reasonable weights\nfor any size of the sub-network. Subsequently, the best configuration of the\nPruned Network is searched by reinforcement learning. Different from previous\nwork, we take the node features from a well-trained graph aggregator instead of\nthe hand-craft features, as the states in reinforcement learning. Compared with\nother AutoML pruning works, our method has achieved the state-of-the-art under\nthe same conditions on ImageNet-2012.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xinyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rong_J/0/1/0/all/0/1\">Jingtao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_L/0/1/0/all/0/1\">Linlin Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Angular Triplet Loss-based Camera Network for ReID. (arXiv:2005.05740v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.05740","description":"<p>Person re-identification (ReID) is a challenging crosscamera retrieval task\nto identify pedestrians. Many complex network structures are proposed recently\nand many of them concentrate on multi-branch features to achieve high\nperformance. However, they are too heavy-weight to deploy in realworld\napplications. Additionally, pedestrian images are often captured by different\nsurveillance cameras, so the varied lights, perspectives and resolutions result\nin inevitable multi-camera domain gaps for ReID. To address these issues, this\npaper proposes ATCN, a simple but effective angular triplet loss-based camera\nnetwork, which is able to achieve compelling performance with only global\nfeatures. In ATCN, a novel angular distance is introduced to learn a more\ndiscriminative feature representation in the embedding space. Meanwhile, a\nlightweight camera network is designed to transfer global features to more\ndiscriminative features. ATCN is designed to be simple and flexible so it can\nbe easily deployed in practice. The experiment results on various benchmark\ndatasets show that ATCN outperforms many SOTA approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1\">Ruini Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mengmeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Neural Network for emotion recognition to assist psychiatrists and psychologists during the COVID-19 pandemic: experts opinion. (arXiv:2005.07649v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.07649","description":"<p>A web application with real-time emotion recognition for psychologists and\npsychiatrists is presented. Mental health effects during COVID-19 quarantine\nneed to be handled because society is being emotionally impacted. The human\nmicro-expressions can describe genuine emotions that can be captured by\nConvolutional Neural Networks (CNN) models. But the challenge is to implement\nit under the poor performance of a part of society computers and the low speed\nof internet connection, i.e., improve the computational efficiency and reduce\nthe data transfer. To validate the computational efficiency premise, we compare\nCNN architectures results, collecting the floating-point operations per second\n(FLOPS), the Number of Parameters (NP) and accuracy from the MobileNet,\nPeleeNet, Extended Deep Neural Network (EDNN), Inception- Based Deep Neural\nNetwork (IDNN) and our proposed Residual mobile-based Network model (ResmoNet).\nAlso, we compare the trained models results in terms of Main Memory Utilization\n(MMU) and Response Time to complete the Emotion (RTE) recognition. Besides, we\ndesign a data transfer that includes the raw data of emotions and the basic\npatient information. The web application was evaluated with the System\nUsability Scale (SUS) and a utility questionnaire by psychologists and\npsychiatrists. ResmoNet model generated the most reduced NP, FLOPS, and MMU\nresults, only EDNN overcomes ResmoNet in 0.01sec in RTE. The optimizations to\nour model impacted the accuracy, therefore IDNN and EDNN are 0.02 and 0.05 more\naccurate than our model respectively. Finally, according to psychologists and\npsychiatrists, the web application has good usability (73.8 of 100) and utility\n(3.94 of 5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitre_Hernandez_H/0/1/0/all/0/1\">Hugo Mitre-Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferro_Perez_R/0/1/0/all/0/1\">Rodolfo Ferro-Perez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Hernandez_F/0/1/0/all/0/1\">Francisco Gonzalez-Hernandez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Co-Training with Task Decomposition for Semi-Supervised Domain Adaptation. (arXiv:2007.12684v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.12684","description":"<p>Semi-supervised domain adaptation (SSDA) aims to adapt models trained from a\nlabeled source domain to a different but related target domain, from which\nunlabeled data and a small set of labeled data are provided. Current methods\nthat treat source and target supervision without distinction overlook their\ninherent discrepancy, resulting in a source-dominated model that has not\neffectively used the target supervision. In this paper, we argue that the\nlabeled target data needs to be distinguished for effective SSDA, and propose\nto explicitly decompose the SSDA task into two sub-tasks: a semi-supervised\nlearning (SSL) task in the target domain and an unsupervised domain adaptation\n(UDA) task across domains. By doing so, the two sub-tasks can better leverage\nthe corresponding supervision and thus yield very different classifiers. To\nintegrate the strengths of the two classifiers, we apply the well-established\nco-training framework, in which the two classifiers exchange their high\nconfident predictions to iteratively \"teach each other\" so that both\nclassifiers can excel in the target domain. We call our approach Deep\nCo-training with Task decomposition (DeCoTa). DeCoTa requires no adversarial\ntraining and is easy to implement. Moreover, DeCoTa is well-founded on the\ntheoretical condition of when co-training would succeed. As a result, DeCoTa\nachieves state-of-the-art results on several SSDA datasets, outperforming the\nprior art by a notable 4% margin on DomainNet. Code is available at\nhttps://github.com/LoyoYang/DeCoTa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Luyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingfei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Gradient Flow Framework For Analyzing Network Pruning. (arXiv:2009.11839v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.11839","description":"<p>Recent network pruning methods focus on pruning models early-on in training.\nTo estimate the impact of removing a parameter, these methods use importance\nmeasures that were originally designed to prune trained models. Despite lacking\njustification for their use early-on in training, such measures result in\nsurprisingly low accuracy loss. To better explain this behavior, we develop a\ngeneral framework that uses gradient flow to unify state-of-the-art importance\nmeasures through the norm of model parameters. We use this framework to\ndetermine the relationship between pruning measures and evolution of model\nparameters, establishing several results related to pruning models early-on in\ntraining: (i) magnitude-based pruning removes parameters that contribute least\nto reduction in loss, resulting in models that converge faster than\nmagnitude-agnostic methods; (ii) loss-preservation based pruning preserves\nfirst-order model evolution dynamics and is therefore appropriate for pruning\nminimally trained models; and (iii) gradient-norm based pruning affects\nsecond-order model evolution dynamics, such that increasing gradient norm via\npruning can produce poorly performing models. We validate our claims on several\nVGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100. Code\navailable at https://github.com/EkdeepSLubana/flowandprune.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation-efficient deep learning for automatic medical image segmentation. (arXiv:2012.04885v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2012.04885","description":"<p>Automatic medical image segmentation plays a critical role in scientific\nresearch and medical care. Existing high-performance deep learning methods\ntypically rely on large training datasets with high-quality manual annotations,\nwhich are difficult to obtain in many clinical applications. Here, we introduce\nAnnotation-effIcient Deep lEarning (AIDE), an open-source framework to handle\nimperfect training datasets. Methodological analyses and empirical evaluations\nare conducted, and we demonstrate that AIDE surpasses conventional\nfully-supervised models by presenting better performance on open datasets\npossessing scarce or noisy annotations. We further test AIDE in a real-life\ncase study for breast tumor segmentation. Three datasets containing 11,852\nbreast images from three medical centers are employed, and AIDE, utilizing 10%\ntraining annotations, consistently produces segmentation maps comparable to\nthose generated by fully-supervised counterparts or provided by independent\nradiologists. The 10-fold enhanced efficiency in utilizing expert labels has\nthe potential to promote a wide range of biomedical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Rongpin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Meiyun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_H/0/1/0/all/0/1\">Hongna Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yaping Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xinfeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hui Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Huihui Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hairong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Affective Computer Vision Models by Crowdsourcing Soft-Target Labels. (arXiv:2101.03477v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.03477","description":"<p>Emotion classifiers traditionally predict discrete emotions. However, emotion\nexpressions are often subjective, thus requiring a method to handle subjective\nlabels. We explore the use of crowdsourcing to acquire reliable soft-target\nlabels and evaluate an emotion detection classifier trained with these labels.\nWe center our study on the Child Affective Facial Expression (CAFE) dataset, a\ngold standard collection of images depicting pediatric facial expressions along\nwith 100 human labels per image. To test the feasibility of crowdsourcing to\ngenerate these labels, we used Microworkers to acquire labels for 207 CAFE\nimages. We evaluate both unfiltered workers as well as workers selected through\na short crowd filtration process. We then train two versions of a classifiers\non soft-target CAFE labels using the original 100 annotations provided with the\ndataset: (1) a classifier trained with traditional one-hot encoded labels, and\n(2) a classifier trained with vector labels representing the distribution of\nCAFE annotator responses. We compare the resulting softmax output distributions\nof the two classifiers with a 2-sample independent t-test of L1 distances\nbetween the classifier's output probability distribution and the distribution\nof human labels. While agreement with CAFE is weak for unfiltered crowd\nworkers, the filtered crowd agree with the CAFE labels 100% of the time for\nmany emotions. While the F1-score for a one-hot encoded classifier is much\nhigher (94.33% vs. 78.68%) with respect to the ground truth CAFE labels, the\noutput probability vector of the crowd-trained classifier more closely\nresembles the distribution of human labels (t=3.2827, p=0.0014). Reporting an\nemotion probability distribution that accounts for the subjectivity of human\ninterpretation. Crowdsourcing, including a sufficient filtering mechanism, is a\nfeasible solution for acquiring soft-target labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1\">Peter Washington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutlu_O/0/1/0/all/0/1\">Onur Cezmi Mutlu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leblanc_E/0/1/0/all/0/1\">Emilie Leblanc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kline_A/0/1/0/all/0/1\">Aaron Kline</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_C/0/1/0/all/0/1\">Cathy Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrisman_B/0/1/0/all/0/1\">Brianna Chrisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stockham_N/0/1/0/all/0/1\">Nate Stockham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paskov_K/0/1/0/all/0/1\">Kelley Paskov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voss_C/0/1/0/all/0/1\">Catalin Voss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_N/0/1/0/all/0/1\">Nick Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_D/0/1/0/all/0/1\">Dennis Wall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Puzzle-CAM: Improved localization via matching partial and full features. (arXiv:2101.11253v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11253","description":"<p>Weakly-supervised semantic segmentation (WSSS) is introduced to narrow the\ngap for semantic segmentation performance from pixel-level supervision to\nimage-level supervision. Most advanced approaches are based on class activation\nmaps (CAMs) to generate pseudo-labels to train the segmentation network. The\nmain limitation of WSSS is that the process of generating pseudo-labels from\nCAMs that use an image classifier is mainly focused on the most discriminative\nparts of the objects. To address this issue, we propose Puzzle-CAM, a process\nthat minimizes differences between the features from separate patches and the\nwhole image. Our method consists of a puzzle module and two regularization\nterms to discover the most integrated region in an object. Puzzle-CAM can\nactivate the overall region of an object using image-level supervision without\nrequiring extra parameters. % In experiments, Puzzle-CAM outperformed previous\nstate-of-the-art methods using the same labels for supervision on the PASCAL\nVOC 2012 test dataset. In experiments, Puzzle-CAM outperformed previous\nstate-of-the-art methods using the same labels for supervision on the PASCAL\nVOC 2012 dataset. Code associated with our experiments is available at\nhttps://github.com/OFRIN/PuzzleCAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Sanghyun Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1\">In-Jae Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Representation Learning via Maximization of Local Mutual Information. (arXiv:2103.04537v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04537","description":"<p>We propose and demonstrate a representation learning approach by maximizing\nthe mutual information between local features of images and text. The goal of\nthis approach is to learn useful image representations by taking advantage of\nthe rich information contained in the free text that describes the findings in\nthe image. Our method trains image and text encoders by encouraging the\nresulting representations to exhibit high local mutual information. We make use\nof recent advances in mutual information estimation with neural network\ndiscriminators. We argue that the sum of local mutual information is typically\na lower bound on the global mutual information. Our experimental results in the\ndownstream image classification tasks demonstrate the advantages of using local\nfeatures for image-text representation learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruizhi Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moyer_D/0/1/0/all/0/1\">Daniel Moyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quigley_K/0/1/0/all/0/1\">Keegan Quigley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkowitz_S/0/1/0/all/0/1\">Seth Berkowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horng_S/0/1/0/all/0/1\">Steven Horng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wells_W/0/1/0/all/0/1\">William M. Wells</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization of Transfer Learning Across Domains Using Spatio-Temporal Features in Autonomous Driving. (arXiv:2103.08116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.08116","description":"<p>Practical learning-based autonomous driving models must be capable of\ngeneralizing learned behaviors from simulated to real domains, and from\ntraining data to unseen domains with unusual image properties. In this paper,\nwe investigate transfer learning methods that achieve robustness to domain\nshifts by taking advantage of the invariance of spatio-temporal features across\ndomains. In this paper, we propose a transfer learning method to improve\ngeneralization across domains via transfer of spatio-temporal features and\nsalient data augmentation. Our model uses a CNN-LSTM network with Inception\nmodules for image feature extraction. Our method runs in two phases: Phase 1\ninvolves training on source domain data, while Phase 2 performs training on\ntarget domain data that has been supplemented by feature maps generated using\nthe Phase 1 model. Our model significantly improves performance in unseen test\ncases for both simulation-to-simulation transfer as well as simulation-to-real\ntransfer by up to +37.3\\% in test accuracy and up to +40.8\\% in steering angle\nprediction, compared to other SOTA methods across multiple datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akhauri_S/0/1/0/all/0/1\">Shivam Akhauri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Laura Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo CenterNet based 3D Object Detection for Autonomous Driving. (arXiv:2103.11071v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11071","description":"<p>Recently, three-dimensional (3D) detection based on stereo images has\nprogressed remarkably; however, most advanced methods adopt anchor-based\ntwo-dimensional (2D) detection or depth estimation to address this problem.\nNevertheless, high computational cost inhibits these methods from achieving\nreal-time performance. In this study, we propose a 3D object detection method,\nStereo CenterNet (SC), using geometric information in stereo imagery. SC\npredicts the four semantic key points of the 3D bounding box of the object in\nspace and utilizes 2D left and right boxes, 3D dimension, orientation, and key\npoints to restore the bounding box of the object in the 3D space. Subsequently,\nwe adopt an improved photometric alignment module to further optimize the\nposition of the 3D bounding box. Experiments conducted on the KITTI dataset\nindicate that the proposed SC exhibits the best speed-accuracy trade-off among\nadvanced methods without using extra data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuguang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_Z/0/1/0/all/0/1\">Zhenqiang Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-view analysis of unregistered medical images using cross-view transformers. (arXiv:2103.11390v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11390","description":"<p>Multi-view medical image analysis often depends on the combination of\ninformation from multiple views. However, differences in perspective or other\nforms of misalignment can make it difficult to combine views effectively, as\nregistration is not always possible. Without registration, views can only be\ncombined at a global feature level, by joining feature vectors after global\npooling. We present a novel cross-view transformer method to transfer\ninformation between unregistered views at the level of spatial feature maps. We\ndemonstrate this method on multi-view mammography and chest X-ray datasets. On\nboth datasets, we find that a cross-view transformer that links spatial feature\nmaps can outperform a baseline model that joins feature vectors after global\npooling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tulder_G/0/1/0/all/0/1\">Gijs van Tulder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yao Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchiori_E/0/1/0/all/0/1\">Elena Marchiori</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Logit Adjustment Loss for Long-Tailed Visual Recognition. (arXiv:2104.06094v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06094","description":"<p>Data in the real world tends to exhibit a long-tailed label distribution,\nwhich poses great challenges for the training of neural networks in visual\nrecognition. Existing methods tackle this problem mainly from the perspective\nof data quantity, i.e., the number of samples in each class. To be specific,\nthey pay more attention to tail classes, like applying larger adjustments to\nthe logit. However, in the training process, the quantity and difficulty of\ndata are two intertwined and equally crucial problems. For some tail classes,\nthe features of their instances are distinct and discriminative, which can also\nbring satisfactory accuracy; for some head classes, although with sufficient\nsamples, the high semantic similarity with other classes and lack of\ndiscriminative features will bring bad accuracy. Based on these observations,\nwe propose Adaptive Logit Adjustment Loss (ALA Loss) to apply an adaptive\nadjusting term to the logit. The adaptive adjusting term is composed of two\ncomplementary factors: 1) quantity factor, which pays more attention to tail\nclasses, and 2) difficulty factor, which adaptively pays more attention to hard\ninstances in the training process. The difficulty factor can alleviate the\nover-optimization on tail yet easy instances and under-optimization on head yet\nhard instances. The synergy of the two factors can not only advance the\nperformance on tail classes even further, but also promote the accuracy on head\nclasses. Unlike previous logit adjusting methods that only concerned about data\nquantity, ALA Loss tackles the long-tailed problem from a more comprehensive,\nfine-grained and adaptive perspective. Extensive experimental results show that\nour method achieves the state-of-the-art performance on challenging recognition\nbenchmarks, including ImageNet-LT, iNaturalist 2018, and Places-LT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weicong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informative and Representative Triplet Selection for Multi-Label Remote Sensing Image Retrieval. (arXiv:2105.03647v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.03647","description":"<p>Learning the similarity between remote sensing (RS) images forms the\nfoundation for content based RS image retrieval (CBIR). Recently, deep metric\nlearning approaches that map the semantic similarity of images into an\nembedding (metric) space have been found very popular in RS. A common approach\nfor learning the metric space relies on the selection of triplets of similar\n(positive) and dissimilar (negative) images to a reference image called as an\nanchor. Choosing triplets is a difficult task particularly for multi-label RS\nCBIR, where each training image is annotated by multiple class labels. To\naddress this problem, in this paper we propose a novel triplet sampling method\nin the framework of deep neural networks (DNNs) defined for multi-label RS CBIR\nproblems. The proposed method selects a small set of the most representative\nand informative triplets based on two main steps. In the first step, a set of\nanchors that are diverse to each other in the embedding space is selected from\nthe current mini-batch using an iterative algorithm. In the second step,\ndifferent sets of positive and negative images are chosen for each anchor by\nevaluating the relevancy, hardness and diversity of the images among each other\nbased on a novel strategy. Experimental results obtained on two multi-label\nbenchmark archives show that the selection of the most informative and\nrepresentative triplets in the context of DNNs results in: i) reducing the\ncomputational complexity of the training phase of the DNNs without any\nsignificant loss on the performance; and ii) an increase in learning speed\nsince informative triplets allow fast convergence. The code of the proposed\nmethod is publicly available at\nhttps://git.tu-berlin.de/rsim/image-retrieval-from-triplets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1\">Gencer Sumbul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion-Based Representation Learning. (arXiv:2105.14257v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14257","description":"<p>Score-based methods represented as stochastic differential equations on a\ncontinuous time domain have recently proven successful as a non-adversarial\ngenerative model. Training such models relies on denoising score matching,\nwhich can be seen as multi-scale denoising autoencoders. Here, we augment the\ndenoising score-matching framework to enable representation learning without\nany supervised signal. GANs and VAEs learn representations by directly\ntransforming latent codes to data samples. In contrast, the introduced\ndiffusion based representation learning relies on a new formulation of the\ndenoising score-matching objective and thus encodes information needed for\ndenoising. We illustrate how this difference allows for manual control of the\nlevel of details encoded in the representation. Using the same approach, we\npropose to learn an infinite-dimensional latent code which achieves\nimprovements of state-of-the-art models on semi-supervised image\nclassification. As a side contribution, we show how adversarial training in\nscore-based models can improve sample quality and improve sampling speed using\na new approximation of the prior at smaller noise scales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abstreiter_K/0/1/0/all/0/1\">Korbinian Abstreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehrjou_A/0/1/0/all/0/1\">Arash Mehrjou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Action Localization without Knowing Boundaries. (arXiv:2106.04150v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04150","description":"<p>Learning to localize actions in long, cluttered, and untrimmed videos is a\nhard task, that in the literature has typically been addressed assuming the\navailability of large amounts of annotated training samples for each class --\neither in a fully-supervised setting, where action boundaries are known, or in\na weakly-supervised setting, where only class labels are known for each video.\nIn this paper, we go a step further and show that it is possible to learn to\nlocalize actions in untrimmed videos when a) only one/few trimmed examples of\nthe target action are available at test time, and b) when a large collection of\nvideos with only class label annotation (some trimmed and some weakly annotated\nuntrimmed ones) are available for training; with no overlap between the classes\nused during training and testing. To do so, we propose a network that learns to\nestimate Temporal Similarity Matrices (TSMs) that model a fine-grained\nsimilarity pattern between pairs of videos (trimmed or untrimmed), and uses\nthem to generate Temporal Class Activation Maps (TCAMs) for seen or unseen\nclasses. The TCAMs serve as temporal attention mechanisms to extract\nvideo-level representations of untrimmed videos, and to temporally localize\nactions at test time. To the best of our knowledge, we are the first to propose\na weakly-supervised, one/few-shot action localization network that can be\ntrained in an end-to-end fashion. Experimental results on THUMOS14 and\nActivityNet1.2 datasets, show that our method achieves performance comparable\nor better to state-of-the-art fully-supervised, few-shot learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Ting-Ting Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzelepis_C/0/1/0/all/0/1\">Christos Tzelepis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1\">Fan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patras_I/0/1/0/all/0/1\">Ioannis Patras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond BatchNorm: Towards a General Understanding of Normalization in Deep Learning. (arXiv:2106.05956v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.05956","description":"<p>Inspired by BatchNorm, there has been an explosion of normalization layers\nfor deep neural networks (DNNs). However, these alternative normalization\nlayers have seen minimal use, partially due to a lack of guiding principles\nthat can help identify when these layers can serve as a replacement for\nBatchNorm. To address this problem, we take a theoretical approach,\ngeneralizing the known beneficial mechanisms of BatchNorm to several recently\nproposed normalization techniques. Our generalized theory leads to the\nfollowing set of principles: (i) similar to BatchNorm, activations-based\nnormalization layers can prevent exponential growth of activations in ResNets,\nbut parametric layers require explicit remedies; (ii) use of GroupNorm can\nensure informative forward propagation, with different samples being assigned\ndissimilar activations, but increasing group size results in increasingly\nindistinguishable activations for different samples, explaining slow\nconvergence speed in models with LayerNorm; (iii) small group sizes result in\nlarge gradient norm in earlier layers, hence explaining training instability\nissues in Instance Normalization and illustrating a speed-stability tradeoff in\nGroupNorm. Overall, our analysis reveals a unified set of mechanisms that\nunderpin the success of normalization methods in deep learning, providing us\nwith a compass to systematically explore the vast design space of DNN\nnormalization layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1\">Ekdeep Singh Lubana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1\">Hidenori Tanaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-guided Machine Learning for Remotely Sensed In-Season Crop Growth Estimation. (arXiv:2106.13323v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13323","description":"<p>Advanced machine learning techniques have been used in remote sensing (RS)\napplications such as crop mapping and yield prediction, but remain\nunder-utilized for tracking crop progress. In this study, we demonstrate the\nuse of agronomic knowledge of crop growth drivers in a Long Short-Term\nMemory-based, domain-guided neural network (DgNN) for in-season crop progress\nestimation. The DgNN uses a branched structure and attention to separate\nindependent crop growth drivers and capture their varying importance throughout\nthe growing season. The DgNN is implemented for corn, using RS data in Iowa for\nthe period 2003-2019, with USDA crop progress reports used as ground truth.\nState-wide DgNN performance shows significant improvement over sequential and\ndense-only NN structures, and a widely-used Hidden Markov Model method. The\nDgNN had a 4.0% higher Nash-Sutfliffe efficiency over all growth stages and 39%\nmore weeks with highest cosine similarity than the next best NN during test\nyears. The DgNN and Sequential NN were more robust during periods of abnormal\ncrop progress, though estimating the Silking-Grainfill transition was difficult\nfor all methods. Finally, Uniform Manifold Approximation and Projection\nvisualizations of layer activations showed how LSTM-based NNs separate crop\ngrowth time-series differently from a dense-only structure. Results from this\nstudy exhibit both the viability of NNs in crop growth stage estimation (CGSE)\nand the benefits of using domain knowledge. The DgNN methodology presented here\ncan be extended to provide near-real time CGSE of other crops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Worrall_G/0/1/0/all/0/1\">George Worrall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1\">Anand Rangarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Judge_J/0/1/0/all/0/1\">Jasmeet Judge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weaving Attention U-net: A Novel Hybrid CNN and Attention-based Method for Organs-at-risk Segmentation in Head and Neck CT Images. (arXiv:2107.04847v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.04847","description":"<p>In radiotherapy planning, manual contouring is labor-intensive and\ntime-consuming. Accurate and robust automated segmentation models improve the\nefficiency and treatment outcome. We aim to develop a novel hybrid deep\nlearning approach, combining convolutional neural networks (CNNs) and the\nself-attention mechanism, for rapid and accurate multi-organ segmentation on\nhead and neck computed tomography (CT) images. Head and neck CT images with\nmanual contours of 115 patients were retrospectively collected and used. We set\nthe training/validation/testing ratio to 81/9/25 and used the 10-fold\ncross-validation strategy to select the best model parameters. The proposed\nhybrid model segmented ten organs-at-risk (OARs) altogether for each case. The\nperformance of the model was evaluated by three metrics, i.e., the Dice\nSimilarity Coefficient (DSC), Hausdorff distance 95% (HD95), and mean surface\ndistance (MSD). We also tested the performance of the model on the Head and\nNeck 2015 challenge dataset and compared it against several state-of-the-art\nautomated segmentation algorithms. The proposed method generated contours that\nclosely resemble the ground truth for ten OARs. Our results of the new Weaving\nAttention U-net demonstrate superior or similar performance on the segmentation\nof head and neck CT images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuangzhuang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gay_H/0/1/0/all/0/1\">Hiram Gay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Weixiong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_B/0/1/0/all/0/1\">Baozhou Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones. (arXiv:2108.00335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00335","description":"<p>Stereo matching has recently witnessed remarkable progress using Deep Neural\nNetworks (DNNs). But, how robust are they? Although it has been well-known that\nDNNs often suffer from adversarial vulnerability with a catastrophic drop in\nperformance, the situation is even worse in stereo matching. This paper first\nshows that a type of weak white-box attacks can overwhelm state-of-the-art\nmethods. The attack is learned by a proposed stereo-constrained projected\ngradient descent (PGD) method in stereo matching. This observation raises\nserious concerns for the deployment of DNN-based stereo matching. Parallel to\nthe adversarial vulnerability, DNN-based stereo matching is typically trained\nunder the so-called simulation to reality pipeline, and thus domain\ngeneralizability is an important problem. This paper proposes to rethink the\nlearnable DNN-based feature backbone towards adversarially-robust and domain\ngeneralizable stereo matching by completely removing it for matching. In\nexperiments, the proposed method is tested in the SceneFlow dataset and the\nKITTI2015 benchmark, with promising results. We compute the matching cost\nvolume using the classic multi-scale census transform (i.e., local binary\npattern) of the raw input stereo images, followed by a stacked Hourglass head\nsub-network solving the matching problem. It significantly improves the\nadversarial robustness, while retaining accuracy performance comparable to\nstate-of-the-art methods. It also shows better generalizability from simulation\n(SceneFlow) to real (KITTI) datasets when no fine-tuning is used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kelvin Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Healey_C/0/1/0/all/0/1\">Christopher Healey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianfu Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting the Generalization Capability in Cross-Domain Few-shot Learning via Noise-enhanced Supervised Autoencoder. (arXiv:2108.05028v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.05028","description":"<p>State of the art (SOTA) few-shot learning (FSL) methods suffer significant\nperformance drop in the presence of domain differences between source and\ntarget datasets. The strong discrimination ability on the source dataset does\nnot necessarily translate to high classification accuracy on the target\ndataset. In this work, we address this cross-domain few-shot learning (CDFSL)\nproblem by boosting the generalization capability of the model. Specifically,\nwe teach the model to capture broader variations of the feature distributions\nwith a novel noise-enhanced supervised autoencoder (NSAE). NSAE trains the\nmodel by jointly reconstructing inputs and predicting the labels of inputs as\nwell as their reconstructed pairs. Theoretical analysis based on intra-class\ncorrelation (ICC) shows that the feature embeddings learned from NSAE have\nstronger discrimination and generalization abilities in the target domain. We\nalso take advantage of NSAE structure and propose a two-step fine-tuning\nprocedure that achieves better adaption and improves classification performance\nin the target domain. Extensive experiments and ablation studies are conducted\nto demonstrate the effectiveness of the proposed method. Experimental results\nshow that our proposed method consistently outperforms SOTA methods under\nvarious conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanwen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1\">Peng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Juwei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-DARTS: Distributed Differentiable Architecture Search. (arXiv:2108.09306v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.09306","description":"<p>Differentiable ARchiTecture Search (DARTS) is one of the most trending Neural\nArchitecture Search (NAS) methods, drastically reducing search cost by\nresorting to Stochastic Gradient Descent (SGD) and weight-sharing. However, it\nalso greatly reduces the search space, thus excluding potential promising\narchitectures from being discovered. In this paper, we propose D-DARTS, a novel\nsolution that addresses this problem by nesting several neural networks at\ncell-level instead of using weight-sharing to produce more diversified and\nspecialized architectures. Moreover, we introduce a novel algorithm which can\nderive deeper architectures from a few trained cells, increasing performance\nand saving computation time. Our solution is able to provide state-of-the-art\nresults on CIFAR-10, CIFAR-100 and ImageNet while using significantly less\nparameters than previous baselines, resulting in more hardware-efficient neural\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heuillet_A/0/1/0/all/0/1\">Alexandre Heuillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabia_H/0/1/0/all/0/1\">Hedi Tabia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arioui_H/0/1/0/all/0/1\">Hichem Arioui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Youcef_Toumi_K/0/1/0/all/0/1\">Kamal Youcef-Toumi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultrafast Focus Detection for Automated Microscopy. (arXiv:2108.12050v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.12050","description":"<p>Recent advances in scientific instruments have resulted in dramatic increase\nin the volumes and velocities of data being generated in every-day\nlaboratories. Scanning electron microscopy is one such example where\ntechnological advancements are now overwhelming scientists with critical data\nfor montaging, alignment, and image segmentation -- key practices for many\nscientific domains, including, for example, neuroscience, where they are used\nto derive the anatomical relationships of the brain. These instruments now\nnecessitate equally advanced computing resources and techniques to realize\ntheir full potential. Here we present a fast out-of-focus detection algorithm\nfor electron microscopy images collected serially and demonstrate that it can\nbe used to provide near-real time quality control for neurology research. Our\ntechnique, Multi-scale Histologic Feature Detection, adapts classical computer\nvision techniques and is based on detecting various fine-grained histologic\nfeatures. We further exploit the inherent parallelism in the technique by\nemploying GPGPU primitives in order to accelerate characterization. Tests are\nperformed that demonstrate near-real-time detection of out-of-focus conditions.\nWe deploy these capabilities as a funcX function and show that it can be\napplied as data are collected using an automated pipeline . We discuss\nextensions that enable scaling out to support multi-beam microscopes and\nintegration with existing focus systems for purposes of implementing\nauto-focus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Levental_M/0/1/0/all/0/1\">Maksim Levental</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chard_R/0/1/0/all/0/1\">Ryan Chard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wildenberg_G/0/1/0/all/0/1\">Gregg A. Wildenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransforMesh: A Transformer Network for Longitudinal modeling of Anatomical Meshes. (arXiv:2109.00532v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00532","description":"<p>The longitudinal modeling of neuroanatomical changes related to Alzheimer's\ndisease (AD) is crucial for studying the progression of the disease. To this\nend, we introduce TransforMesh, a spatio-temporal network based on transformers\nthat models longitudinal shape changes on 3D anatomical meshes. While\ntransformer and mesh networks have recently shown impressive performances in\nnatural language processing and computer vision, their application to medical\nimage analysis has been very limited. To the best of our knowledge, this is the\nfirst work that combines transformer and mesh networks. Our results show that\nTransforMesh can model shape trajectories better than other baseline\narchitectures that do not capture temporal dependencies. Moreover, we also\nexplore the capabilities of TransforMesh in detecting structural anomalies of\nthe hippocampus in patients developing AD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarasua_I/0/1/0/all/0/1\">Ignacio Sarasua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Neural Radiance Fields:Quantifying Uncertainty in Implicit 3D Representations. (arXiv:2109.02123v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02123","description":"<p>Neural Radiance Fields (NeRF) has become a popular framework for learning\nimplicit 3D representations and addressing different tasks such as novel-view\nsynthesis or depth-map estimation. However, in downstream applications where\ndecisions need to be made based on automatic predictions, it is critical to\nleverage the confidence associated with the model estimations. Whereas\nuncertainty quantification is a long-standing problem in Machine Learning, it\nhas been largely overlooked in the recent NeRF literature. In this context, we\npropose Stochastic Neural Radiance Fields (S-NeRF), a generalization of\nstandard NeRF that learns a probability distribution over all the possible\nradiance fields modeling the scene. This distribution allows to quantify the\nuncertainty associated with the scene information provided by the model. S-NeRF\noptimization is posed as a Bayesian learning problem which is efficiently\naddressed using the Variational Inference framework. Exhaustive experiments\nover benchmark datasets demonstrate that S-NeRF is able to provide more\nreliable predictions and confidence values than generic approaches previously\nproposed for uncertainty estimation in other domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jianxiong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1\">Adria Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agudo_A/0/1/0/all/0/1\">Antonio Agudo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1\">Francesc Moreno-Noguer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WiSoSuper: Benchmarking Super-Resolution Methods on Wind and Solar Data. (arXiv:2109.08770v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08770","description":"<p>The transition to green energy grids depends on detailed wind and solar\nforecasts to optimize the siting and scheduling of renewable energy generation.\nOperational forecasts from numerical weather prediction models, however, only\nhave a spatial resolution of 10 to 20-km, which leads to sub-optimal usage and\ndevelopment of renewable energy farms. Weather scientists have been developing\nsuper-resolution methods to increase the resolution, but often rely on simple\ninterpolation techniques or computationally expensive differential\nequation-based models. Recently, machine learning-based models, specifically\nthe physics-informed resolution-enhancing generative adversarial network\n(PhIREGAN), have outperformed traditional downscaling methods. We provide a\nthorough and extensible benchmark of leading deep learning-based\nsuper-resolution techniques, including the enhanced super-resolution generative\nadversarial network (ESRGAN) and an enhanced deep super-resolution (EDSR)\nnetwork, on wind and solar data. We accompany the benchmark with a novel\npublic, processed, and machine learning-ready dataset for benchmarking\nsuper-resolution methods on wind and solar data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kurinchi_Vendhan_R/0/1/0/all/0/1\">Rupa Kurinchi-Vendhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lutjens_B/0/1/0/all/0/1\">Bj&#xf6;rn L&#xfc;tjens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Ritwik Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werner_L/0/1/0/all/0/1\">Lucien Werner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Newman_D/0/1/0/all/0/1\">Dava Newman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Gesture Recognition. (arXiv:2109.09396v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09396","description":"<p>The Human-Machine Interaction (HMI) research field is an important topic in\nmachine learning that has been deeply investigated thanks to the rise of\ncomputing power in the last years. The first time, it is possible to use\nmachine learning to classify images and/or videos instead of the traditional\ncomputer vision algorithms. The aim of this project is to builda symbiosis\nbetween a convolutional neural network (CNN)[1] and a recurrent neural network\n(RNN) [2] to recognize cultural/anthropological Italian sign language gestures\nfrom videos. The CNN extracts important features that later areused by the RNN.\nWith RNNs we are able to store temporal information inside the model to provide\ncontextual information from previous frames to enhance the prediction accuracy.\nOur novel approach uses different data augmentation techniquesand\nregularization methods from only RGB frames to avoid overfitting and provide a\nsmall generalization error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bokstaller_J/0/1/0/all/0/1\">Jonas Bokstaller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Improta_C/0/1/0/all/0/1\">Costanza Maria Improta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09818","description":"<p>Convolutional Neural Networks have demonstrated dermatologist-level\nperformance in the classification of melanoma and other skin lesions, but\nprediction irregularities due to biases seen within the training data are an\nissue that should be addressed before widespread deployment is possible. In\nthis work, we robustly remove bias and spurious variation from an automated\nmelanoma classification pipeline using two leading bias unlearning techniques.\nWe show that the biases introduced by surgical markings and rulers presented in\nprevious studies can be reasonably mitigated using these bias removal methods.\nWe also demonstrate the generalisation benefits of unlearning spurious\nvariation relating to the imaging instrument used to capture lesion images.\nContributions of this work include the application of different debiasing\ntechniques for artefact bias removal and the concept of instrument bias\nunlearning for domain generalisation in melanoma detection. Our experimental\nresults provide evidence that the effects of each of the aforementioned biases\nare notably reduced, with different debiasing techniques excelling at different\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rapid detection and recognition of whole brain activity in a freely behaving Caenorhabditis elegans. (arXiv:2109.10474v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2109.10474","description":"<p>Advanced volumetric imaging methods and genetically encoded activity\nindicators have permitted a comprehensive characterization of whole brain\nactivity at single neuron resolution in \\textit{Caenorhabditis elegans}. The\nconstant motion and deformation of the mollusc nervous system, however, impose\na great challenge for a consistent identification of densely packed neurons in\na behaving animal. Here, we propose a cascade solution for long-term and rapid\nrecognition of head ganglion neurons in a freely moving \\textit{C. elegans}.\nFirst, potential neuronal regions from a stack of fluorescence images are\ndetected by a deep learning algorithm. Second, 2 dimensional neuronal regions\nare fused into 3 dimensional neuron entities. Third, by exploiting the neuronal\ndensity distribution surrounding a neuron and relative positional information\nbetween neurons, a multi-class artificial neural network transforms engineered\nneuronal feature vectors into digital neuronal identities. Under the constraint\nof a small number (20-40 volumes) of training samples, our bottom-up approach\nis able to process each volume - $1024 \\times 1024 \\times 18$ in voxels - in\nless than 1 second and achieves an accuracy of $91\\%$ in neuronal detection and\n$74\\%$ in neuronal recognition. Our work represents an important development\ntowards a rapid and fully automated algorithm for decoding whole brain activity\nunderlying natural animal behaviors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxiang Wu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_S/0/1/0/all/0/1\">Shang Wu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lang_C/0/1/0/all/0/1\">Chengtian Lang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1\">Quanshi Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wen_Q/0/1/0/all/0/1\">Quan Wen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_T/0/1/0/all/0/1\">Tianqi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving 360 Monocular Depth Estimation via Non-local Dense Prediction Transformer and Joint Supervised and Self-supervised Learning. (arXiv:2109.10563v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10563","description":"<p>Due to difficulties in acquiring ground truth depth of equirectangular (360)\nimages, the quality and quantity of equirectangular depth data today is\ninsufficient to represent the various scenes in the world. Therefore, 360 depth\nestimation studies, which relied solely on supervised learning, are destined to\nproduce unsatisfactory results. Although self-supervised learning methods\nfocusing on equirectangular images (EIs) are introduced, they often have\nincorrect or non-unique solutions, causing unstable performance. In this paper,\nwe propose 360 monocular depth estimation methods which improve on the areas\nthat limited previous studies. First, we introduce a self-supervised 360 depth\nlearning method that only utilizes gravity-aligned videos, which has the\npotential to eliminate the needs for depth data during the training procedure.\nSecond, we propose a joint learning scheme realized by combining supervised and\nself-supervised learning. The weakness of each learning is compensated, thus\nleading to more accurate depth estimation. Third, we propose a non-local fusion\nblock, which retains global information encoded by vision transformer when\nreconstructing the depths. With the proposed methods, we successfully apply the\ntransformer to 360 depth estimations, to the best of our knowledge, which has\nnot been tried before. On several benchmarks, our approach achieves significant\nimprovements over previous works and establishes a state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_I/0/1/0/all/0/1\">Ilwi Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyuk-Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1\">Chae Eun Rhee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}