{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-25T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction. (arXiv:2204.10360v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10360","description":"<p>Relation extraction is a core problem for natural language processing in the\nbiomedical domain. Recent research on relation extraction showed that\nprompt-based learning improves the performance on both fine-tuning on full\ntraining set and few-shot training. However, less effort has been made on\ndomain-specific tasks where good prompt design can be even harder. In this\npaper, we investigate prompting for biomedical relation extraction, with\nexperiments on the ChemProt dataset. We present a simple yet effective method\nto systematically generate comprehensive prompts that reformulate the relation\nextraction task as a cloze-test task under a simple prompt formulation. In\nparticular, we experiment with different ranking scores for prompt selection.\nWith BioMed-RoBERTa-base, our results show that prompting-based fine-tuning\nobtains gains by 14.21 F1 over its regular fine-tuning baseline, and 1.14 F1\nover SciFive-Large, the current state-of-the-art on ChemProt. Besides, we find\nprompt-based learning requires fewer training examples to make reasonable\npredictions. The results demonstrate the potential of our methods in such a\ndomain-specific relation extraction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_H/0/1/0/all/0/1\">Hui-Syuan Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavergne_T/0/1/0/all/0/1\">Thomas Lavergne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zweigenbaum_P/0/1/0/all/0/1\">Pierre Zweigenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards an Enhanced Understanding of Bias in Pre-trained Neural Language Models: A Survey with Special Emphasis on Affective Bias. (arXiv:2204.10365v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10365","description":"<p>The remarkable progress in Natural Language Processing (NLP) brought about by\ndeep learning, particularly with the recent advent of large pre-trained neural\nlanguage models, is brought into scrutiny as several studies began to discuss\nand report potential biases in NLP applications. Bias in NLP is found to\noriginate from latent historical biases encoded by humans into textual data\nwhich gets perpetuated or even amplified by NLP algorithm. We present a survey\nto comprehend bias in large pre-trained language models, analyze the stages at\nwhich they occur in these models, and various ways in which these biases could\nbe quantified and mitigated. Considering wide applicability of textual\naffective computing based downstream tasks in real-world systems such as\nbusiness, healthcare, education, etc., we give a special emphasis on\ninvestigating bias in the context of affect (emotion) i.e., Affective Bias, in\nlarge pre-trained language models. We present a summary of various bias\nevaluation corpora that help to aid future research and discuss challenges in\nthe research on bias in pre-trained language models. We believe that our\nattempt to draw a comprehensive view of bias in pre-trained language models,\nand especially the exploration of affective bias will be highly beneficial to\nresearchers interested in this evolving field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+K%2E_A/0/1/0/all/0/1\">Anoop K.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangan_M/0/1/0/all/0/1\">Manjary P. Gangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P%2E_D/0/1/0/all/0/1\">Deepak P.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_L/0/1/0/all/0/1\">Lajish V. L</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICDBigBird: A Contextual Embedding Model for ICD Code Classification. (arXiv:2204.10408v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10408","description":"<p>The International Classification of Diseases (ICD) system is the\ninternational standard for classifying diseases and procedures during a\nhealthcare encounter and is widely used for healthcare reporting and management\npurposes. Assigning correct codes for clinical procedures is important for\nclinical, operational, and financial decision-making in healthcare. Contextual\nword embedding models have achieved state-of-the-art results in multiple NLP\ntasks. However, these models have yet to achieve state-of-the-art results in\nthe ICD classification task since one of their main disadvantages is that they\ncan only process documents that contain a small number of tokens which is\nrarely the case with real patient notes. In this paper, we introduce ICDBigBird\na BigBird-based model which can integrate a Graph Convolutional Network (GCN),\nthat takes advantage of the relations between ICD codes in order to create\n'enriched' representations of their embeddings, with a BigBird contextual model\nthat can process larger documents. Our experiments on a real-world clinical\ndataset demonstrate the effectiveness of our BigBird-based model on the ICD\nclassification task as it outperforms the previous state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Michalopoulos_G/0/1/0/all/0/1\">George Michalopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malyska_M/0/1/0/all/0/1\">Michal Malyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahar_N/0/1/0/all/0/1\">Nicola Sahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Helen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"German Parliamentary Corpus (GerParCor). (arXiv:2204.10422v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10422","description":"<p>Parliamentary debates represent a large and partly unexploited treasure trove\nof publicly accessible texts. In the German-speaking area, there is a certain\ndeficit of uniformly accessible and annotated corpora covering all\nGerman-speaking parliaments at the national and federal level. To address this\ngap, we introduce the German Parliament Corpus (GerParCor). GerParCor is a\ngenre-specific corpus of (predominantly historical) German-language\nparliamentary protocols from three centuries and four countries, including\nstate and federal level data. In addition, GerParCor contains conversions of\nscanned protocols and, in particular, of protocols in Fraktur converted via an\nOCR process based on Tesseract. All protocols were preprocessed by means of the\nNLP pipeline of spaCy3 and automatically annotated with metadata regarding\ntheir session date. GerParCor is made available in the XMI format of the UIMA\nproject. In this way, GerParCor can be used as a large corpus of historical\ntexts in the field of political communication for various tasks in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abrami_G/0/1/0/all/0/1\">Giuseppe Abrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagci_M/0/1/0/all/0/1\">Mevl&#xfc;t Bagci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammerla_L/0/1/0/all/0/1\">Leon Hammerla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehler_A/0/1/0/all/0/1\">Alexander Mehler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Generalizability of Depression Detection by Leveraging Clinical Questionnaires. (arXiv:2204.10432v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10432","description":"<p>Automated methods have been widely used to identify and analyze mental health\nconditions (e.g., depression) from various sources of information, including\nsocial media. Yet, deployment of such models in real-world healthcare\napplications faces challenges including poor out-of-domain generalization and\nlack of trust in black box models. In this work, we propose approaches for\ndepression detection that are constrained to different degrees by the presence\nof symptoms described in PHQ9, a questionnaire used by clinicians in the\ndepression screening process. In dataset-transfer experiments on three social\nmedia datasets, we find that grounding the model in PHQ9's symptoms\nsubstantially improves its ability to generalize to out-of-distribution data\ncompared to a standard BERT-based approach. Furthermore, this approach can\nstill perform competitively on in-domain data. These results and our\nqualitative analyses suggest that grounding model predictions in\nclinically-relevant symptoms can improve generalizability while producing a\nmodel that is easier to inspect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Andrew Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zirikly_A/0/1/0/all/0/1\">Ayah Zirikly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desmet_B/0/1/0/all/0/1\">Bart Desmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering. (arXiv:2204.10448v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10448","description":"<p>Knowledge-based visual question answering (QA) aims to answer a question\nwhich requires visually-grounded external knowledge beyond image content\nitself. Answering complex questions that require multi-hop reasoning under weak\nsupervision is considered as a challenging problem since i) no supervision is\ngiven to the reasoning process and ii) high-order semantics of multi-hop\nknowledge facts need to be captured. In this paper, we introduce a concept of\nhypergraph to encode high-level semantics of a question and a knowledge base,\nand to learn high-order associations between them. The proposed model,\nHypergraph Transformer, constructs a question hypergraph and a query-aware\nknowledge hypergraph, and infers an answer by encoding inter-associations\nbetween two hypergraphs and intra-associations in both hypergraph itself.\nExtensive experiments on two knowledge-based visual QA and two knowledge-based\ntextual QA demonstrate the effectiveness of our method, especially for\nmulti-hop reasoning problem. Our source code is available at\nhttps://github.com/yujungheo/kbvqa-public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1\">Yu-Jung Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eun-Sol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woo Suk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WaBERT: A Low-resource End-to-end Model for Spoken Language Understanding and Speech-to-BERT Alignment. (arXiv:2204.10461v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10461","description":"<p>Historically lower-level tasks such as automatic speech recognition (ASR) and\nspeaker identification are the main focus in the speech field. Interest has\nbeen growing in higher-level spoken language understanding (SLU) tasks\nrecently, like sentiment analysis (SA). However, improving performances on SLU\ntasks remains a big challenge. Basically, there are two main methods for SLU\ntasks: (1) Two-stage method, which uses a speech model to transfer speech to\ntext, then uses a language model to get the results of downstream tasks; (2)\nOne-stage method, which just fine-tunes a pre-trained speech model to fit in\nthe downstream tasks. The first method loses emotional cues such as intonation,\nand causes recognition errors during ASR process, and the second one lacks\nnecessary language knowledge. In this paper, we propose the Wave BERT (WaBERT),\na novel end-to-end model combining the speech model and the language model for\nSLU tasks. WaBERT is based on the pre-trained speech and language model, hence\ntraining from scratch is not needed. We also set most parameters of WaBERT\nfrozen during training. By introducing WaBERT, audio-specific information and\nlanguage knowledge are integrated in the short-time and low-resource training\nprocess to improve results on the dev dataset of SLUE SA tasks by 1.15% of\nrecall score and 0.82% of F1 score. Additionally, we modify the serial\nContinuous Integrate-and-Fire (CIF) mechanism to achieve the monotonic\nalignment between the speech and text modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lin Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jianfei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruizhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yingfang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zijian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yafeng Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Contrastive Clustering: Fully Unsupervised Bias Reduction for Sentiment Classification. (arXiv:2204.10467v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10467","description":"<p>Background: Neural networks produce biased classification results due to\ncorrelation bias (they learn correlations between their inputs and outputs to\nclassify samples, even when those correlations do not represent\ncause-and-effect relationships).\n</p>\n<p>Objective: This study introduces a fully unsupervised method of mitigating\ncorrelation bias, demonstrated with sentiment classification on COVID-19 social\nmedia data.\n</p>\n<p>Methods: Correlation bias in sentiment classification often arises in\nconversations about controversial topics. Therefore, this study uses\nadversarial learning to contrast clusters based on sentiment classification\nlabels, with clusters produced by unsupervised topic modeling. This discourages\nthe neural network from learning topic-related features that produce biased\nclassification results.\n</p>\n<p>Results: Compared to a baseline classifier, neural contrastive clustering\napproximately doubles accuracy on bias-prone sentences for human-labeled\nCOVID-19 social media data, without adversely affecting the classifier's\noverall F1 score. Despite being a fully unsupervised approach, neural\ncontrastive clustering achieves a larger improvement in accuracy on bias-prone\nsentences than a supervised masking approach.\n</p>\n<p>Conclusions: Neural contrastive clustering reduces correlation bias in\nsentiment text classification. Further research is needed to explore\ngeneralizing this technique to other neural network architectures and\napplication domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mowery_J/0/1/0/all/0/1\">Jared Mowery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP Based Anomaly Detection for Categorical Time Series. (arXiv:2204.10483v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10483","description":"<p>Identifying anomalies in large multi-dimensional time series is a crucial and\ndifficult task across multiple domains. Few methods exist in the literature\nthat address this task when some of the variables are categorical in nature. We\nformalize an analogy between categorical time series and classical Natural\nLanguage Processing and demonstrate the strength of this analogy for anomaly\ndetection and root cause investigation by implementing and testing three\ndifferent machine learning anomaly detection and root cause investigation\nmodels based upon it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horak_M/0/1/0/all/0/1\">Matthew Horak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_S/0/1/0/all/0/1\">Sowmya Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tobar_G/0/1/0/all/0/1\">Giovanni Tobar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10496","description":"<p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with\ncarefully curated vision-language datasets. While these datasets reach an order\nof 10 million samples, the labor cost is prohibitive to scale further.\nConversely, unimodal encoders are pretrained with simpler annotations that are\nless cost-prohibitive, achieving scales of hundreds of millions to billions. As\na result, unimodal encoders have achieved state-of-art (SOTA) on many\ndownstream tasks. However, challenges remain when applying to VL tasks. The\npretraining data is not optimal for cross-modal architectures and requires\nheavy computational resources. In addition, unimodal architectures lack\ncross-modal interactions that have demonstrated significant benefits for VL\ntasks. Therefore, how to best leverage pretrained unimodal encoders for VL\ntasks is still an area of active research. In this work, we propose a method to\nleverage unimodal vision and text encoders for VL tasks that augment existing\nVL approaches while conserving computational complexity. Specifically, we\npropose Multimodal Adaptive Distillation (MAD), which adaptively distills\nuseful knowledge from pretrained encoders to cross-modal VL encoders. Second,\nto better capture nuanced impacts on VL task performance, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata constraints and conditions of domain shift. Experiments demonstrate that\nMAD leads to consistent gains in the low-shot, domain-shifted, and\nfully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA\nperformance on VCR compared to other single models pretrained with image-text\ndata. Finally, MAD outperforms concurrent works utilizing pretrained vision\nencoder from CLIP. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Multi-Turn Empathetic Dialogs with Positive Emotion Elicitation. (arXiv:2204.10509v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10509","description":"<p>Emotional support is a crucial skill for many real-world scenarios, including\ncaring for the elderly, mental health support, and customer service chats. This\npaper presents a novel task of empathetic dialog generation with positive\nemotion elicitation to promote users' positive emotions, similar to that of\nemotional support between humans. In this task, the agent conducts empathetic\nresponses along with the target of eliciting the user's positive emotions in\nthe multi-turn dialog. To facilitate the study of this task, we collect a\nlarge-scale emotional dialog dataset with positive emotion elicitation, called\nPosEmoDial (about 820k dialogs, 3M utterances). In these dialogs, the agent\ntries to guide the user from any possible initial emotional state, e.g.,\nsadness, to a positive emotional state. Then we present a\npositive-emotion-guided dialog generation model with a novel loss function\ndesign. This loss function encourages the dialog model to not only elicit\npositive emotions from users but also ensure smooth emotional transitions along\nwith the whole dialog. Finally, we establish benchmark results on PosEmoDial,\nand we will release this dataset and related source code to facilitate future\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shihang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinchao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenquan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1\">Zheng-Yu Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Taygete at SemEval-2022 Task 4: RoBERTa based models for detecting Patronising and Condescending Language. (arXiv:2204.10519v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10519","description":"<p>This work describes the development of different models to detect patronising\nand condescending language within extracts of news articles as part of the\nSemEval 2022 competition (Task-4). This work explores different models based on\nthe pre-trained RoBERTa language model coupled with LSTM and CNN layers. The\nbest models achieved 15$^{th}$ rank with an F1-score of 0.5924 for subtask-A\nand 12$^{th}$ in subtask-B with a macro-F1 score of 0.3763.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chhillar_J/0/1/0/all/0/1\">Jayant Chhillar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Offensive Text Detection as a Multi-Hop Reasoning Problem. (arXiv:2204.10521v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10521","description":"<p>We introduce the task of implicit offensive text detection in dialogues,\nwhere a statement may have either an offensive or non-offensive interpretation,\ndepending on the listener and context. We argue that reasoning is crucial for\nunderstanding this broader class of offensive utterances and release SLIGHT, a\ndataset to support research on this task. Experiments using the data show that\nstate-of-the-art methods of offense detection perform poorly when asked to\ndetect implicitly offensive statements, achieving only ${\\sim} 11\\%$ accuracy.\n</p>\n<p>In contrast to existing offensive text detection datasets, SLIGHT features\nhuman-annotated chains of reasoning which describe the mental process by which\nan offensive interpretation can be reached from each ambiguous statement. We\nexplore the potential for a multi-hop reasoning approach by utilizing existing\nentailment models to score the probability of these chains and show that even\nnaive reasoning models can yield improved performance in most situations.\nFurthermore, analysis of the chains provides insight into the human\ninterpretation process and emphasizes the importance of incorporating\nadditional commonsense knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naradowsky_J/0/1/0/all/0/1\">Jason Naradowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyao_Y/0/1/0/all/0/1\">Yusuke Miyao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero and Few-shot Learning for Author Profiling. (arXiv:2204.10543v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10543","description":"<p>Author profiling classifies author characteristics by analyzing how language\nis shared among people. In this work, we study that task from a low-resource\nviewpoint: using little or no training data. We explore different zero and\nfew-shot models based on entailment and evaluate our systems on several\nprofiling tasks in Spanish and English. In addition, we study the effect of\nboth the entailment hypothesis and the size of the few-shot training sample. We\nfind that entailment-based models out-perform supervised text classifiers based\non roberta-XLM and that we can reach 80% of the accuracy of previous approaches\nusing less than 50\\% of the training data on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chinea_Rios_M/0/1/0/all/0/1\">Mara Chinea-Rios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1\">Thomas M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarracen_G/0/1/0/all/0/1\">Gretel Liz De la Pe&#xf1;a Sarrac&#xe9;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangel_F/0/1/0/all/0/1\">Francisco Rangel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franco_Salvador_M/0/1/0/all/0/1\">Marc Franco-Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KALA: Knowledge-Augmented Language Model Adaptation. (arXiv:2204.10555v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10555","description":"<p>Pre-trained language models (PLMs) have achieved remarkable success on\nvarious natural language understanding tasks. Simple fine-tuning of PLMs, on\nthe other hand, might be suboptimal for domain-specific tasks because they\ncannot possibly cover knowledge from all domains. While adaptive pre-training\nof PLMs can help them obtain domain-specific knowledge, it requires a large\ntraining cost. Moreover, adaptive pre-training can harm the PLM's performance\non the downstream task by causing catastrophic forgetting of its general\nknowledge. To overcome such limitations of adaptive pre-training for PLM\nadaption, we propose a novel domain adaption framework for PLMs coined as\nKnowledge-Augmented Language model Adaptation (KALA), which modulates the\nintermediate hidden representations of PLMs with domain knowledge, consisting\nof entities and their relational facts. We validate the performance of our KALA\non question answering and named entity recognition tasks on multiple datasets\nacross various domains. The results show that, despite being computationally\nefficient, our KALA largely outperforms adaptive pre-training. Code is\navailable at: https://github.com/Nardien/KALA/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse and Dense Approaches for the Full-rank Retrieval of Responses for Dialogues. (arXiv:2204.10558v1 [cs.IR])","link":"http://arxiv.org/abs/2204.10558","description":"<p>Ranking responses for a given dialogue context is a popular benchmark in\nwhich the setup is to re-rank the ground-truth response over a limited set of\n$n$ responses, where $n$ is typically 10. The predominance of this setup in\nconversation response ranking has lead to a great deal of attention to building\nneural re-rankers, while the first-stage retrieval step has been overlooked.\nSince the correct answer is always available in the candidate list of $n$\nresponses, this artificial evaluation setup assumes that there is a first-stage\nretrieval step which is always able to rank the correct response in its top-$n$\nlist. In this paper we focus on the more realistic task of full-rank retrieval\nof responses, where $n$ can be up to millions of responses. We investigate both\ndialogue context and response expansion techniques for sparse retrieval, as\nwell as zero-shot and fine-tuned dense retrieval approaches. Our findings based\non three different information-seeking dialogue datasets reveal that a learned\nresponse expansion technique is a solid baseline for sparse retrieval. We find\nthe best performing method overall to be dense retrieval with intermediate\ntraining, i.e. a step after the language model pre-training where sentence\nrepresentations are learned, followed by fine-tuning on the target\nconversational data. We also investigate the intriguing phenomena that harder\nnegatives sampling techniques lead to worse results for the fine-tuned dense\nretrieval models. The code and datasets are available at\nhttps://github.com/Guzpenha/transformer_rankers/tree/full_rank_retrieval_dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Penha_G/0/1/0/all/0/1\">Gustavo Penha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauff_C/0/1/0/all/0/1\">Claudia Hauff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Training of Neural Transducer for Speech Recognition. (arXiv:2204.10586v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10586","description":"<p>As one of the most popular sequence-to-sequence modeling approaches for\nspeech recognition, the RNN-Transducer has achieved evolving performance with\nmore and more sophisticated neural network models of growing size and\nincreasing training epochs. While strong computation resources seem to be the\nprerequisite of training superior models, we try to overcome it by carefully\ndesigning a more efficient training pipeline. In this work, we propose an\nefficient 3-stage progressive training pipeline to build highly-performing\nneural transducer models from scratch with very limited computation resources\nin a reasonable short time period. The effectiveness of each stage is\nexperimentally verified on both Librispeech and Switchboard corpora. The\nproposed pipeline is able to train transducer models approaching\nstate-of-the-art performance with a single GPU in just 2-3 weeks. Our best\nconformer transducer achieves 4.1% WER on Librispeech test-other with only 35\nepochs of training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michel_W/0/1/0/all/0/1\">Wilfried Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emergent Communication for Understanding Human Language Evolution: What's Missing?. (arXiv:2204.10590v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10590","description":"<p>Emergent communication protocols among humans and artificial neural network\nagents do not yet share the same properties and show some critical mismatches\nin results. We describe three important phenomena with respect to the emergence\nand benefits of compositionality: ease-of-learning, generalization, and group\nsize effects (i.e., larger groups create more systematic languages). The latter\ntwo are not fully replicated with neural agents, which hinders the use of\nneural emergent communication for language evolution research. We argue that\none possible reason for these mismatches is that key cognitive and\ncommunicative constraints of humans are not yet integrated. Specifically, in\nhumans, memory constraints and the alternation between the roles of speaker and\nlistener underlie the emergence of linguistic structure, yet these constraints\nare typically absent in neural simulations. We suggest that introducing such\ncommunicative and cognitive constraints would promote more linguistically\nplausible behaviors with neural agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_Y/0/1/0/all/0/1\">Yoav Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raviv_L/0/1/0/all/0/1\">Limor Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SalesBot: Transitioning from Chit-Chat to Task-Oriented Dialogues. (arXiv:2204.10591v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10591","description":"<p>Dialogue systems are usually categorized into two types, open-domain and\ntask-oriented. The first one focuses on chatting with users and making them\nengage in the conversations, where selecting a proper topic to fit the dialogue\ncontext is essential for a successful dialogue. The other one focuses on a\nspecific task instead of casual talks, e.g., finding a movie on Friday night,\nor playing a song. These two directions have been studied separately due to\ntheir different purposes. However, how smoothly transitioning from social\nchatting to task-oriented dialogues is important for triggering business\nopportunities, and there is no public data focusing on such scenarios. Hence,\nthis paper focuses on investigating the conversations starting from open-domain\nsocial chatting and then gradually transitioning to task-oriented purposes, and\nreleases a large-scale dataset with detailed annotations for encouraging this\nresearch direction. To achieve this goal, this paper proposes a framework to\nautomatically generate many dialogues without human involvement, in which any\npowerful open-domain dialogue generation model can be easily leveraged. The\nhuman evaluation shows that our generated dialogue data has a natural flow at a\nreasonable quality, showing that our released data has a great potential of\nguiding future research directions and commercial activities. Furthermore, the\nreleased models allow researchers to automatically generate unlimited dialogues\nin the target scenarios, which can greatly benefit semi-supervised and\nunsupervised approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Ssu Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maolin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LibriS2S: A German-English Speech-to-Speech Translation Corpus. (arXiv:2204.10593v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10593","description":"<p>Recently, we have seen an increasing interest in the area of speech-to-text\ntranslation. This has led to astonishing improvements in this area. In\ncontrast, the activities in the area of speech-to-speech translation is still\nlimited, although it is essential to overcome the language barrier. We believe\nthat one of the limiting factors is the availability of appropriate training\ndata. We address this issue by creating LibriS2S, to our knowledge the first\npublicly available speech-to-speech training corpus between German and English.\nFor this corpus, we used independently created audio for German and English\nleading to an unbiased pronunciation of the text in both languages. This allows\nthe creation of a new text-to-speech and speech-to-speech translation model\nthat directly learns to generate the speech signal based on the pronunciation\nof the source language. Using this created corpus, we propose Text-to-Speech\nmodels based on the example of the recently proposed FastSpeech 2 model that\nintegrates source language information. We do this by adapting the model to\ntake information such as the pitch, energy or transcript from the source speech\nas additional input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jeuris_P/0/1/0/all/0/1\">Pedro Jeuris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Quantifiers as a Source of Error in Multilingual NLU Benchmarks. (arXiv:2204.10615v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10615","description":"<p>Logical approaches to representing language have developed and evaluated\ncomputational models of quantifier words since the 19th century, but today's\nNLU models still struggle to capture their semantics. We rely on Generalized\nQuantifier Theory for language-independent representations of the semantics of\nquantifier words, to quantify their contribution to the errors of NLU models.\nWe find that quantifiers are pervasive in NLU benchmarks, and their occurrence\nat test time is associated with performance drops. Multilingual models also\nexhibit unsatisfying quantifier reasoning abilities, but not necessarily worse\nfor non-English languages. To facilitate directly-targeted probing, we present\nan adversarial generalized quantifier NLI task (GQNLI) and show that\npre-trained language models have a clear lack of robustness in generalized\nquantifier reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Out-of-Domain Evaluation of Finnish Dependency Parsing. (arXiv:2204.10621v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10621","description":"<p>The prevailing practice in the academia is to evaluate the model performance\non in-domain evaluation data typically set aside from the training corpus.\nHowever, in many real world applications the data on which the model is applied\nmay very substantially differ from the characteristics of the training data. In\nthis paper, we focus on Finnish out-of-domain parsing by introducing a novel UD\nFinnish-OOD out-of-domain treebank including five very distinct data sources\n(web documents, clinical, online discussions, tweets, and poetry), and a total\nof 19,382 syntactic words in 2,122 sentences released under the Universal\nDependencies framework. Together with the new treebank, we present extensive\nout-of-domain parsing evaluation utilizing the available section-level\ninformation from three different Finnish UD treebanks (TDT, PUD, OOD). Compared\nto the previously existing treebanks, the new Finnish-OOD is shown include\nsections more challenging for the general parser, creating an interesting\nevaluation setting and yielding valuable information for those applying the\nparser outside of its training domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanerva_J/0/1/0/all/0/1\">Jenna Kanerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Functional Distributional Semantics with Visual Data. (arXiv:2204.10624v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10624","description":"<p>Functional Distributional Semantics is a recently proposed framework for\nlearning distributional semantics that provides linguistic interpretability. It\nmodels the meaning of a word as a binary classifier rather than a numerical\nvector. In this work, we propose a method to train a Functional Distributional\nSemantics model with grounded visual data. We train it on the Visual Genome\ndataset, which is closer to the kind of data encountered in human language\nacquisition than a large text corpus. On four external evaluation datasets, our\nmodel outperforms previous work on learning semantics from Visual Genome.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emerson_G/0/1/0/all/0/1\">Guy Emerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autoregressive Search Engines: Generating Substrings as Document Identifiers. (arXiv:2204.10628v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10628","description":"<p>Knowledge-intensive language tasks require NLP systems to both provide the\ncorrect answer and retrieve supporting evidence for it in a given corpus.\nAutoregressive language models are emerging as the de-facto standard for\ngenerating answers, with newer and more powerful systems emerging at an\nastonishing pace. In this paper we argue that all this (and future) progress\ncan be directly applied to the retrieval problem with minimal intervention to\nthe models' architecture. Previous work has explored ways to partition the\nsearch space into hierarchical structures and retrieve documents by\nautoregressively generating their unique identifier. In this work we propose an\nalternative that doesn't force any structure in the search space: using all\nngrams in a passage as its possible identifiers. This setup allows us to use an\nautoregressive model to generate and score distinctive ngrams, that are then\nmapped to full passages through an efficient data structure. Empirically, we\nshow this not only outperforms prior autoregressive approaches but also leads\nto an average improvement of at least 10 points over more established retrieval\nsolutions for passage-level retrieval on the KILT benchmark, establishing new\nstate-of-the-art downstream performance on some datasets, while using a\nconsiderably lighter memory footprint than competing systems. Code and\npre-trained models at https://github.com/facebookresearch/SEAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevilacqua_M/0/1/0/all/0/1\">Michele Bevilacqua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ottaviano_G/0/1/0/all/0/1\">Giuseppe Ottaviano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1\">Patrick Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1\">Wen-tau Yih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petroni_F/0/1/0/all/0/1\">Fabio Petroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering. (arXiv:2204.10629v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10629","description":"<p>Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG\nembedding contains concise data used in NLP tasks requiring implicit\ninformation about the real world. Furthermore, the size of KGs that may be\nuseful in actual NLP assignments is enormous, and creating embedding over it\nhas memory cost issues. We represent KG as a 3rd-order binary tensor and move\nbeyond the standard CP decomposition by using a data-specific generalized\nversion of it. The generalization of the standard CP-ALS algorithm allows\nobtaining optimization gradients without a backpropagation mechanism. It\nreduces the memory needed in training while providing computational benefits.\nWe propose a MEKER, a memory-efficient KG embedding model, which yields\nSOTA-comparable performance on link prediction tasks and KG-based Question\nAnswering.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chekalina_V/0/1/0/all/0/1\">Viktoriia Chekalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1\">Anton Razzhigaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sayapin_A/0/1/0/all/0/1\">Albert Sayapin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persona-Guided Planning for Controlling the Protagonist's Persona in Story Generation. (arXiv:2204.10703v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10703","description":"<p>Endowing the protagonist with a specific personality is essential for writing\nan engaging story. In this paper, we aim to control the protagonist's persona\nin story generation, i.e., generating a story from a leading context and a\npersona description, where the protagonist should exhibit the specified\npersonality through a coherent event sequence. Considering that personas are\nusually embodied implicitly and sparsely in stories, we propose a\nplanning-based generation model named CONPER to explicitly model the\nrelationship between personas and events. CONPER first plans events of the\nprotagonist's behavior which are motivated by the specified persona through\npredicting one target sentence, then plans the plot as a sequence of keywords\nwith the guidance of the predicted persona-related events and commonsense\nknowledge, and finally generates the whole story. Both automatic and manual\nevaluation results demonstrate that CONPER outperforms state-of-the-art\nbaselines for generating more coherent and persona-controllable stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhexin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiaxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Chinese Opinion Expressions with Extremely-Noisy Crowdsourcing Annotations. (arXiv:2204.10714v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10714","description":"<p>Recent works of opinion expression identification (OEI) rely heavily on the\nquality and scale of the manually-constructed training corpus, which could be\nextremely difficult to satisfy. Crowdsourcing is one practical solution for\nthis problem, aiming to create a large-scale but quality-unguaranteed corpus.\nIn this work, we investigate Chinese OEI with extremely-noisy crowdsourcing\nannotations, constructing a dataset at a very low cost. Following zhang et al.\n(2021), we train the annotator-adapter model by regarding all annotations as\ngold-standard in terms of crowd annotators, and test the model by using a\nsynthetic expert, which is a mixture of all annotators. As this\nannotator-mixture for testing is never modeled explicitly in the training\nphase, we propose to generate synthetic training samples by a pertinent mixup\nstrategy to make the training and testing highly consistent. The simulation\nexperiments on our constructed dataset show that crowdsourcing is highly\npromising for OEI, and our proposed annotator-mixup can further enhance the\ncrowdsourcing modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yueheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding. (arXiv:2204.10716v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10716","description":"<p>International Classification of Diseases (ICD) coding plays an important role\nin systematically classifying morbidity and mortality data. In this study, we\npropose a hierarchical label-wise attention Transformer model (HiLAT) for the\nexplainable prediction of ICD codes from clinical documents. HiLAT firstly\nfine-tunes a pretrained Transformer model to represent the tokens of clinical\ndocuments. We subsequently employ a two-level hierarchical label-wise attention\nmechanism that creates label-specific document representations. These\nrepresentations are in turn used by a feed-forward neural network to predict\nwhether a specific ICD code is assigned to the input clinical document of\ninterest. We evaluate HiLAT using hospital discharge summaries and their\ncorresponding ICD-9 codes from the MIMIC-III database. To investigate the\nperformance of different types of Transformer models, we develop\nClinicalplusXLNet, which conducts continual pretraining from XLNet-Base using\nall the MIMIC-III clinical notes. The experiment results show that the F1\nscores of the HiLAT+ClinicalplusXLNet outperform the previous state-of-the-art\nmodels for the top-50 most frequent ICD-9 codes from MIMIC-III. Visualisations\nof attention weights present a potential explainability tool for checking the\nface validity of ICD code predictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Leibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Concha_O/0/1/0/all/0/1\">Oscar Perez-Concha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anthony Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_V/0/1/0/all/0/1\">Vicki Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jorm_L/0/1/0/all/0/1\">Louisa Jorm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Summary of the ALQAC 2021 Competition. (arXiv:2204.10717v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10717","description":"<p>We summarize the evaluation of the first Automated Legal Question Answering\nCompetition (ALQAC 2021). The competition this year contains three tasks, which\naims at processing the statute law document, which are Legal Text Information\nRetrieval (Task 1), Legal Text Entailment Prediction (Task 2), and Legal Text\nQuestion Answering (Task 3). The final goal of these tasks is to build a system\nthat can automatically determine whether a particular statement is lawful.\nThere is no limit to the approaches of the participating teams. This year,\nthere are 5 teams participating in Task 1, 6 teams participating in Task 2, and\n5 teams participating in Task 3. There are in total 36 runs submitted to the\norganizer. In this paper, we summarize each team's approaches, official\nresults, and some discussion about the competition. Only results of the teams\nwho successfully submit their approach description paper are reported in this\npaper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thanh_N/0/1/0/all/0/1\">Nguyen Ha Thanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_B/0/1/0/all/0/1\">Bui Minh Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Tung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phuong_N/0/1/0/all/0/1\">Nguyen Minh Phuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binh_D/0/1/0/all/0/1\">Dang Tran Binh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_V/0/1/0/all/0/1\">Vuong Thi Hai Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Racharak_T/0/1/0/all/0/1\">Teeradaj Racharak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minh_N/0/1/0/all/0/1\">Nguyen Le Minh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tran Duc Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anh_P/0/1/0/all/0/1\">Phan Viet Anh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_N/0/1/0/all/0/1\">Nguyen Truong Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Huy Tien Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butr_indr_B/0/1/0/all/0/1\">Bhumindr Butr-indr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vateekul_P/0/1/0/all/0/1\">Peerapon Vateekul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boonkwan_P/0/1/0/all/0/1\">Prachya Boonkwan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pathways through Conspiracy: The Evolution of Conspiracy Radicalization through Engagement in Online Conspiracy Discussions. (arXiv:2204.10729v1 [cs.CY])","link":"http://arxiv.org/abs/2204.10729","description":"<p>The disruptive offline mobilization of participants in online conspiracy\ntheory (CT) discussions has highlighted the importance of understanding how\nonline users may form radicalized conspiracy beliefs. While prior work\nresearched the factors leading up to joining online CT discussions and provided\ntheories of how conspiracy beliefs form, we have little understanding of how\nconspiracy radicalization evolves after users join CT discussion communities.\nIn this paper, we provide the empirical modeling of various radicalization\nphases in online CT discussion participants. To unpack how conspiracy\nengagement is related to radicalization, we first characterize the users'\njourney through CT discussions via conspiracy engagement pathways.\nSpecifically, by studying 36K Reddit users through their 169M contributions, we\nuncover four distinct pathways of conspiracy engagement: steady high,\nincreasing, decreasing, and steady low. We further model three successive\nstages of radicalization guided by prior theoretical works. Specific\nsub-populations of users, namely those on steady high and increasing conspiracy\nengagement pathways, progress successively through various radicalization\nstages. In contrast, users on the decreasing engagement pathway show distinct\nbehavior: they limit their CT discussions to specialized topics, participate in\ndiverse discussion groups, and show reduced conformity with conspiracy\nsubreddits. By examining users who disengage from online CT discussions, this\npaper provides promising insights about conspiracy recovery process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phadke_S/0/1/0/all/0/1\">Shruti Phadke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samory_M/0/1/0/all/0/1\">Mattia Samory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1\">Tanushree Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E2E Segmenter: Joint Segmenting and Decoding for Long-Form ASR. (arXiv:2204.10749v1 [cs.SD])","link":"http://arxiv.org/abs/2204.10749","description":"<p>Improving the performance of end-to-end ASR models on long utterances ranging\nfrom minutes to hours in length is an ongoing challenge in speech recognition.\nA common solution is to segment the audio in advance using a separate voice\nactivity detector (VAD) that decides segment boundary locations based purely on\nacoustic speech/non-speech information. VAD segmenters, however, may be\nsub-optimal for real-world speech where, e.g., a complete sentence that should\nbe taken as a whole may contain hesitations in the middle (\"set an alarm for...\n5 o'clock\").\n</p>\n<p>We propose to replace the VAD with an end-to-end ASR model capable of\npredicting segment boundaries in a streaming fashion, allowing the segmentation\ndecision to be conditioned not only on better acoustic features but also on\nsemantic features from the decoded text with negligible extra computation. In\nexperiments on real world long-form audio (YouTube) with lengths of up to 30\nminutes, we demonstrate 8.5% relative WER improvement and 250 ms reduction in\nmedian end-of-segment latency compared to the VAD segmenter baseline on a\nstate-of-the-art Conformer RNN-T model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuo-yiin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1\">David Rybach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allauzen_C/0/1/0/all/0/1\">Cyril Allauzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyun Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaithDial: A Faithful Benchmark for Information-Seeking Dialogue. (arXiv:2204.10757v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10757","description":"<p>The goal of information-seeking dialogue is to respond to seeker queries with\nnatural language utterances that are grounded on knowledge sources. However,\ndialogue systems often produce unsupported utterances, a phenomenon known as\nhallucination. Dziri et al. (2022)'s investigation of hallucinations has\nrevealed that existing knowledge-grounded benchmarks are contaminated with\nhallucinated responses at an alarming level (&gt;60% of the responses) and models\ntrained on this data amplify hallucinations even further (&gt;80% of the\nresponses). To mitigate this behavior, we adopt a data-centric solution and\ncreate FaithDial, a new benchmark for hallucination-free dialogues, by editing\nhallucinated responses in the Wizard of Wikipedia (WoW) benchmark. We observe\nthat FaithDial is more faithful than WoW while also maintaining engaging\nconversations. We show that FaithDial can serve as a training signal for: i) a\nhallucination critic, which discriminates whether an utterance is faithful or\nnot, and boosts the performance by 21.1 F1 score on the BEGIN benchmark\ncompared to existing datasets for dialogue coherence; ii) high-quality dialogue\ngeneration. We benchmark a series of state-of-the-art models and propose an\nauxiliary contrastive objective that achieves the highest level of faithfulness\nand abstractiveness based on several automated metrics. Further, we find that\nthe benefits of FaithDial generalize to zero-shot transfer on other datasets,\nsuch as CMU-Dog and TopicalChat. Finally, human evaluation reveals that\nresponses generated by models trained on FaithDial are perceived as more\ninterpretable, cooperative, and engaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamalloo_E/0/1/0/all/0/1\">Ehsan Kamalloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milton_S/0/1/0/all/0/1\">Sivan Milton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo M. Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revise and Resubmit: An Intertextual Model of Text-based Collaboration in Peer Review. (arXiv:2204.10805v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10805","description":"<p>Peer review is a key component of the publishing process in most fields of\nscience. The increasing submission rates put a strain on reviewing quality and\nefficiency, motivating the development of applications to support the reviewing\nand editorial work. While existing NLP studies focus on the analysis of\nindividual texts, editorial assistance often requires modeling interactions\nbetween pairs of texts -- yet general frameworks and datasets to support this\nscenario are missing. Relationships between texts are the core object of the\nintertextuality theory -- a family of approaches in literary studies not yet\noperationalized in NLP. Inspired by prior theoretical work, we propose the\nfirst intertextual model of text-based collaboration, which encompasses three\nmajor phenomena that make up a full iteration of the review-revise-and-resubmit\ncycle: pragmatic tagging, linking and long-document version alignment. While\npeer review is used across the fields of science and publication formats,\nexisting datasets solely focus on conference-style review in computer science.\nAddressing this, we instantiate our proposed model in the first annotated\nmulti-domain corpus in journal-style post-publication open peer review, and\nprovide detailed insights into the practical aspects of intertextual\nannotation. Our resource is a major step towards multi-domain, fine-grained\napplications of NLP in editorial support for peer review, and our intertextual\nframework paves the path for general-purpose modeling of text-based\ncollaboration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuznetsov_I/0/1/0/all/0/1\">Ilia Kuznetsov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchmann_J/0/1/0/all/0/1\">Jan Buchmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eichler_M/0/1/0/all/0/1\">Max Eichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Scaffold: Optimizing Model Explanations for Teaching. (arXiv:2204.10810v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10810","description":"<p>Modern machine learning models are opaque, and as a result there is a\nburgeoning academic subfield on methods that explain these models' behavior.\nHowever, what is the precise goal of providing such explanations, and how can\nwe demonstrate that explanations achieve this goal? Some research argues that\nexplanations should help teach a student (either human or machine) to simulate\nthe model being explained, and that the quality of explanations can be measured\nby the simulation accuracy of students on unexplained examples. In this work,\nleveraging meta-learning techniques, we extend this idea to improve the quality\nof the explanations themselves, specifically by optimizing explanations such\nthat student models more effectively learn to simulate the original model. We\ntrain models on three natural language processing and computer vision tasks,\nand find that students trained with explanations extracted with our framework\nare able to simulate the teacher significantly more effectively than ones\nproduced with previous methods. Through human annotations and a user study, we\nfurther find that these learned explanations more closely align with how humans\nwould explain the required decisions in these tasks. Our code is available at\nhttps://github.com/coderpat/learning-scaffold\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Vocabulary-Free Multilingual Neural Tokenizer for End-to-End Task Learning. (arXiv:2204.10815v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10815","description":"<p>Subword tokenization is a commonly used input pre-processing step in most\nrecent NLP models. However, it limits the models' ability to leverage\nend-to-end task learning. Its frequency-based vocabulary creation compromises\ntokenization in low-resource languages, leading models to produce suboptimal\nrepresentations. Additionally, the dependency on a fixed vocabulary limits the\nsubword models' adaptability across languages and domains. In this work, we\npropose a vocabulary-free neural tokenizer by distilling segmentation\ninformation from heuristic-based subword tokenization. We pre-train our\ncharacter-based tokenizer by processing unique words from multilingual corpus,\nthereby extensively increasing word diversity across languages. Unlike the\npredefined and fixed vocabularies in subword methods, our tokenizer allows\nend-to-end task learning, resulting in optimal task-specific tokenization. The\nexperimental results show that replacing the subword tokenizer with our neural\ntokenizer consistently improves performance on multilingual (NLI) and\ncode-switching (sentiment analysis) tasks, with larger gains in low-resource\nlanguages. Additionally, our neural tokenizer exhibits a robust performance on\ndownstream tasks when adversarial noise is present (typos and misspelling),\nfurther increasing the initial improvements over statistical subword\ntokenizers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Mofijul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguilar_G/0/1/0/all/0/1\">Gustavo Aguilar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnusamy_P/0/1/0/all/0/1\">Pragaash Ponnusamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathialagan_C/0/1/0/all/0/1\">Clint Solomon Mathialagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chenlei Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meet Your Favorite Character: Open-domain Chatbot Mimicking Fictional Characters with only a Few Utterances. (arXiv:2204.10825v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10825","description":"<p>In this paper, we consider mimicking fictional characters as a promising\ndirection for building engaging conversation models. To this end, we present a\nnew practical task where only a few utterances of each fictional character are\navailable to generate responses mimicking them. Furthermore, we propose a new\nmethod named Pseudo Dialog Prompting (PDP) that generates responses by\nleveraging the power of large-scale language models with prompts containing the\ntarget character's utterances. To better reflect the style of the character,\nPDP builds the prompts in the form of dialog that includes the character's\nutterances as dialog history. Since only utterances of the characters are\navailable in the proposed task, PDP matches each utterance with an appropriate\npseudo-context from a predefined set of context candidates using a retrieval\nmodel. Through human and automatic evaluation, we show that PDP generates\nresponses that better reflect the style of fictional characters than baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seokjun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sangbum Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdenee_E/0/1/0/all/0/1\">Enkhbayar Erdenee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting early signs of depression in the conversational domain: The role of transfer learning in low-resource scenarios. (arXiv:2204.10841v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10841","description":"<p>The high prevalence of depression in society has given rise to the need for\nnew digital tools to assist in its early detection. To this end, existing\nresearch has mainly focused on detecting depression in the domain of social\nmedia, where there is a sufficient amount of data. However, with the rise of\nconversational agents like Siri or Alexa, the conversational domain is becoming\nmore critical. Unfortunately, there is a lack of data in the conversational\ndomain. We perform a study focusing on domain adaptation from social media to\nthe conversational domain. Our approach mainly exploits the linguistic\ninformation preserved in the vector representation of text. We describe\ntransfer learning techniques to classify users who suffer from early signs of\ndepression with high recall. We achieve state-of-the-art results on a commonly\nused conversational dataset, and we highlight how the method can easily be used\nin conversational agents. We publicly release all source code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenc_P/0/1/0/all/0/1\">Petr Lorenc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uban_A/0/1/0/all/0/1\">Ana-Sabina Uban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosso_P/0/1/0/all/0/1\">Paolo Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedivy_J/0/1/0/all/0/1\">Jan &#x160;ediv&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metric Learning and Adaptive Boundary for Out-of-Domain Detection. (arXiv:2204.10849v1 [cs.CL])","link":"http://arxiv.org/abs/2204.10849","description":"<p>Conversational agents are usually designed for closed-world environments.\nUnfortunately, users can behave unexpectedly. Based on the open-world\nenvironment, we often encounter the situation that the training and test data\nare sampled from different distributions. Then, data from different\ndistributions are called out-of-domain (OOD). A robust conversational agent\nneeds to react to these OOD utterances adequately. Thus, the importance of\nrobust OOD detection is emphasized. Unfortunately, collecting OOD data is a\nchallenging task. We have designed an OOD detection algorithm independent of\nOOD data that outperforms a wide range of current state-of-the-art algorithms\non publicly available datasets. Our algorithm is based on a simple but\nefficient approach of combining metric learning with adaptive decision\nboundary. Furthermore, compared to other algorithms, we have found that our\nproposed algorithm has significantly improved OOD performance in a scenario\nwith a lower number of classes while preserving the accuracy for in-domain\n(IND) classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lorenc_P/0/1/0/all/0/1\">Petr Lorenc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gargiani_T/0/1/0/all/0/1\">Tommaso Gargiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pichl_J/0/1/0/all/0/1\">Jan Pichl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konrad_J/0/1/0/all/0/1\">Jakub Konr&#xe1;d</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marek_P/0/1/0/all/0/1\">Petr Marek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobza_O/0/1/0/all/0/1\">Ond&#x159;ej Kobza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedivy_J/0/1/0/all/0/1\">Jan &#x160;ediv&#xfd;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study into Patient Similarity through Representation Learning from Medical Records. (arXiv:2108.10682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10682","description":"<p>Patient similarity assessment, which identifies patients similar to a given\npatient, can help improve medical care. The assessment can be performed using\nElectronic Medical Records (EMRs). Patient similarity measurement requires\nconverting heterogeneous EMRs into comparable formats to calculate their\ndistance. While versatile document representation learning methods have been\ndeveloped in recent years, it is still unclear how complex EMR data should be\nprocessed to create the most useful patient representations. This study\npresents a new data representation method for EMRs that takes the information\nin clinical narratives into account. To address the limitations of previous\napproaches in handling complex parts of EMR data, an unsupervised method is\nproposed for building a patient representation, which integrates unstructured\ndata with structured data extracted from patients' EMRs. In order to model the\nextracted data, we employed a tree structure that captures the temporal\nrelations of multiple medical events from EMR. We processed clinical notes to\nextract symptoms, signs, and diseases using different tools such as medspaCy,\nMetaMap, and scispaCy and mapped entities to the Unified Medical Language\nSystem (UMLS). After creating a tree data structure, we utilized two novel\nrelabeling methods for the non-leaf nodes of the tree to capture two temporal\naspects of the extracted events. By traversing the tree, we generated a\nsequence that could create an embedding vector for each patient. The\ncomprehensive evaluation of the proposed method for patient similarity and\nmortality prediction tasks demonstrated that our proposed model leads to lower\nmean squared error (MSE), higher precision, and normalized discounted\ncumulative gain (NDCG) relative to baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalafi_S/0/1/0/all/0/1\">Sahar Khalafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadiri_N/0/1/0/all/0/1\">Nasser Ghadiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regionalized models for Spanish language variations based on Twitter. (arXiv:2110.06128v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06128","description":"<p>Spanish is one of the most spoken languages in the globe, but not necessarily\nSpanish is written and spoken in the same way in different countries.\nUnderstanding local language variations can help to improve model performances\non regional tasks, both understanding local structures and also improving the\nmessage's content. For instance, think about a machine learning engineer who\nautomatizes some language classification task on a particular region or a\nsocial scientist trying to understand a regional event with echoes on social\nmedia; both can take advantage of dialect-based language models to understand\nwhat is happening with more contextual information hence more precision.\n</p>\n<p>This manuscript presents and describes a set of regionalized resources for\nthe Spanish language built on four-year Twitter public messages geotagged in 26\nSpanish-speaking countries. We introduce word embeddings based on FastText,\nlanguage models based on BERT, and per-region sample corpora. We also provide a\nbroad comparison among regions covering lexical and semantical similarities; as\nwell as examples of using regional resources on message classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1\">Eric S. Tellez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1\">Daniela Moctezuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_S/0/1/0/all/0/1\">Sabino Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1\">Mario Graff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_G/0/1/0/all/0/1\">Guillermo Ruiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Reason Deductively: Math Word Problem Solving as Complex Relation Extraction. (arXiv:2203.10316v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10316","description":"<p>Solving math word problems requires deductive reasoning over the quantities\nin the text. Various recent research efforts mostly relied on\nsequence-to-sequence or sequence-to-tree models to generate mathematical\nexpressions without explicitly performing relational reasoning between\nquantities in the given context. While empirically effective, such approaches\ntypically do not provide explanations for the generated expressions. In this\nwork, we view the task as a complex relation extraction problem, proposing a\nnovel approach that presents explainable deductive reasoning steps to\niteratively construct target expressions, where each step involves a primitive\noperation over two quantities defining their relation. Through extensive\nexperiments on four benchmark datasets, we show that the proposed model\nsignificantly outperforms existing strong baselines. We further demonstrate\nthat the deductive procedure not only presents more explainable steps but also\nenables us to make more accurate predictions on questions that require more\ncomplex reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jierui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model. (arXiv:2204.03905v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.03905","description":"<p>Pretrained language models have served as important backbones for natural\nlanguage processing. Recently, in-domain pretraining has been shown to benefit\nvarious domain-specific downstream tasks. In the biomedical domain, natural\nlanguage generation (NLG) tasks are of critical importance, while understudied.\nApproaching natural language understanding (NLU) tasks as NLG achieves\nsatisfying performance in the general domain through constrained language\ngeneration or language prompting. We emphasize the lack of in-domain generative\nlanguage models and the unsystematic generative downstream benchmarks in the\nbiomedical domain, hindering the development of the research community. In this\nwork, we introduce the generative language model BioBART that adapts BART to\nthe biomedical domain. We collate various biomedical language generation tasks\nincluding dialogue, summarization, entity linking, and named entity\nrecognition. BioBART pretrained on PubMed abstracts has enhanced performance\ncompared to BART and set strong baselines on several tasks. Furthermore, we\nconduct ablation studies on the pretraining tasks for BioBART and find that\nsentence permutation has negative effects on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_R/0/1/0/all/0/1\">Ruyi Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GERE: Generative Evidence Retrieval for Fact Verification. (arXiv:2204.05511v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05511","description":"<p>Fact verification (FV) is a challenging task which aims to verify a claim\nusing multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.\nMost existing approaches follow a three-step pipeline framework, including\ndocument retrieval, sentence retrieval and claim verification. High-quality\nevidences provided by the first two steps are the foundation of the effective\nreasoning in the last step. Despite being important, high-quality evidences are\nrarely studied by existing works for FV, which often adopt the off-the-shelf\nmodels to retrieve relevant documents and sentences in an\n\"index-retrieve-then-rank\" fashion. This classical approach has clear drawbacks\nas follows: i) a large document index as well as a complicated search process\nis required, leading to considerable memory and computational overhead; ii)\nindependent scoring paradigms fail to capture the interactions among documents\nand sentences in ranking; iii) a fixed number of sentences are selected to form\nthe final evidence set. In this work, we propose GERE, the first system that\nretrieves evidences in a generative fashion, i.e., generating the document\ntitles as well as evidence sentence identifiers. This enables us to mitigate\nthe aforementioned technical issues since: i) the memory and computational cost\nis greatly reduced because the document index is eliminated and the heavy\nranking process is replaced by a light generative process; ii) the dependency\nbetween documents and that between sentences could be captured via sequential\ngeneration process; iii) the generative formulation allows us to dynamically\nselect a precise set of relevant evidences for each claim. The experimental\nresults on the FEVER dataset show that GERE achieves significant improvements\nover the state-of-the-art baselines, with both time-efficiency and\nmemory-efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imagination-Augmented Natural Language Understanding. (arXiv:2204.08535v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08535","description":"<p>Human brains integrate linguistic and perceptual information simultaneously\nto understand natural language, and hold the critical ability to render\nimaginations. Such abilities enable us to construct new abstract concepts or\nconcrete objects, and are essential in involving practical knowledge to solve\nproblems in low-resource scenarios. However, most existing methods for Natural\nLanguage Understanding (NLU) are mainly focused on textual signals. They do not\nsimulate human visual imagination ability, which hinders models from inferring\nand learning efficiently from limited data samples. Therefore, we introduce an\nImagination-Augmented Cross-modal Encoder (iACE) to solve natural language\nunderstanding tasks from a novel learning perspective -- imagination-augmented\ncross-modal understanding. iACE enables visual imagination with external\nknowledge transferred from the powerful generative and pre-trained\nvision-and-language models. Extensive experiments on GLUE and SWAG show that\niACE achieves consistent improvement over visually-supervised pre-trained\nmodels. More importantly, results in extreme and normal few-shot settings\nvalidate the effectiveness of iACE in low-resource natural language\nunderstanding circumstances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situational Perception Guided Image Matting. (arXiv:2204.09276v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09276","description":"<p>Most automatic matting methods try to separate the salient foreground from\nthe background. However, the insufficient quantity and subjective bias of the\ncurrent existing matting datasets make it difficult to fully explore the\nsemantic association between object-to-object and object-to-environment in a\ngiven image. In this paper, we propose a Situational Perception Guided Image\nMatting (SPG-IM) method that mitigates subjective bias of matting annotations\nand captures sufficient situational perception information for better global\nsaliency distilled from the visual-to-textual task. SPG-IM can better associate\ninter-objects and object-to-environment saliency, and compensate the subjective\nnature of image matting and its expensive annotation. We also introduce a\ntextual Semantic Transformation (TST) module that can effectively transform and\nintegrate the semantic feature stream to guide the visual representations. In\naddition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed\nto adaptively switch multi-scale receptive fields and focal points to enhance\nboth global and local details. Extensive experiments demonstrate the\neffectiveness of situational perception guidance from the visual-to-textual\ntasks on image matting, and our model outperforms the state-of-the-art methods.\nWe also analyze the significance of different components in our model. The code\nwill be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-Aware Language Modeling for Goal-Oriented Dialogue Systems. (arXiv:2204.10198v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.10198","description":"<p>Goal-oriented dialogue systems face a trade-off between fluent language\ngeneration and task-specific control. While supervised learning with large\nlanguage models is capable of producing realistic text, how to steer such\nresponses towards completing a specific task without sacrificing language\nquality remains an open question. In this work, we formulate goal-oriented\ndialogue as a partially observed Markov decision process, interpreting the\nlanguage model as a representation of both the dynamics and the policy. This\nview allows us to extend techniques from learning-based control, such as task\nrelabeling, to derive a simple and effective method to finetune language models\nin a goal-aware way, leading to significantly improved task performance. We\nadditionally introduce a number of training strategies that serve to better\nfocus the model on the task at hand. We evaluate our method, Context-Aware\nLanguage Models (CALM), on a practical flight-booking task using AirDialogue.\nEmpirically, CALM outperforms the state-of-the-art method by 7% in terms of\ntask success, matching human-level task performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mengjiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Justin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Topic Modeling of Psychotherapy Sessions. (arXiv:2204.10189v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2204.10189","description":"<p>In this work, we compare different neural topic modeling methods in learning\nthe topical propensities of different psychiatric conditions from the\npsychotherapy session transcripts parsed from speech recordings. We also\nincorporate temporal modeling to put this additional interpretability to action\nby parsing out topic similarities as a time series in a turn-level resolution.\nWe believe this topic modeling framework can offer interpretable insights for\nthe therapist to optimally decide his or her strategy and improve the\npsychotherapy effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cecchi_G/0/1/0/all/0/1\">Guillermo Cecchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejwani_R/0/1/0/all/0/1\">Ravi Tejwani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Interactive Segmentation and Visualization for Tiny Objects in Multi-megapixel Images. (arXiv:2204.10356v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10356","description":"<p>We introduce an interactive image segmentation and visualization framework\nfor identifying, inspecting, and editing tiny objects (just a few pixels wide)\nin large multi-megapixel high-dynamic-range (HDR) images. Detecting cosmic rays\n(CRs) in astronomical observations is a cumbersome workflow that requires\nmultiple tools, so we developed an interactive toolkit that unifies model\ninference, HDR image visualization, segmentation mask inspection and editing\ninto a single graphical user interface. The feature set, initially designed for\nastronomical data, makes this work a useful research-supporting tool for\nhuman-in-the-loop tiny-object segmentation in scientific areas like\nbiomedicine, materials science, remote sensing, etc., as well as computer\nvision. Our interface features mouse-controlled, synchronized, dual-window\nvisualization of the image and the segmentation mask, a critical feature for\nlocating tiny objects in multi-megapixel images. The browser-based tool can be\nreadily hosted on the web to provide multi-user access and GPU acceleration for\nany device. The toolkit can also be used as a high-precision annotation tool,\nor adapted as the frontend for an interactive machine learning framework. Our\nopen-source dataset, CR detection model, and visualization toolkit are\navailable at https://github.com/cy-xu/cosmic-conn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Boning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stier_N/0/1/0/all/0/1\">Noah Stier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCully_C/0/1/0/all/0/1\">Curtis McCully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howell_D/0/1/0/all/0/1\">D. Andrew Howell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1\">Pradeep Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1\">Tobias H&#xf6;llerer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Test-Time Adaptation. (arXiv:2204.10377v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10377","description":"<p>Test-time adaptation is a special setting of unsupervised domain adaptation\nwhere a trained model on the source domain has to adapt to the target domain\nwithout accessing source data. We propose a novel way to leverage\nself-supervised contrastive learning to facilitate target feature learning,\nalong with an online pseudo labeling scheme with refinement that significantly\ndenoises pseudo labels. The contrastive learning task is applied jointly with\npseudo labeling, contrasting positive and negative pairs constructed similarly\nas MoCo but with source-initialized encoder, and excluding same-class negative\npairs indicated by pseudo labels. Meanwhile, we produce pseudo labels online\nand refine them via soft voting among their nearest neighbors in the target\nfeature space, enabled by maintaining a memory queue. Our method, AdaContrast,\nachieves state-of-the-art performance on major benchmarks while having several\ndesirable properties compared to existing works, including memory efficiency,\ninsensitivity to hyper-parameters, and better model calibration. Project page:\nsites.google.com/view/adacontrast.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_S/0/1/0/all/0/1\">Sayna Ebrahimi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The 6th AI City Challenge. (arXiv:2204.10380v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10380","description":"<p>The 6th edition of the AI City Challenge specifically focuses on problems in\ntwo domains where there is tremendous unlocked potential at the intersection of\ncomputer vision and artificial intelligence: Intelligent Traffic Systems (ITS),\nand brick and mortar retail businesses. The four challenge tracks of the 2022\nAI City Challenge received participation requests from 254 teams across 27\ncountries. Track 1 addressed city-scale multi-target multi-camera (MTMC)\nvehicle tracking. Track 2 addressed natural-language-based vehicle track\nretrieval. Track 3 was a brand new track for naturalistic driving analysis,\nwhere the data were captured by several cameras mounted inside the vehicle\nfocusing on driver safety, and the task was to classify driver actions. Track 4\nwas another new track aiming to achieve retail store automated checkout using\nonly a single view camera. We released two leader boards for submissions based\non different methods, including a public leader board for the contest, where no\nuse of external data is allowed, and a general leader board for all submitted\nresults. The top performance of participating teams established strong\nbaselines and even outperformed the state-of-the-art in the proposed challenge\ntracks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naphade_M/0/1/0/all/0/1\">Milind Naphade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1\">David C. Anastasiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yue Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Mohammed Shaiqur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatachalapathy_A/0/1/0/all/0/1\">Archana Venkatachalapathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1\">Vitaly Ablavsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sclaroff_S/0/1/0/all/0/1\">Stan Sclaroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1\">Pranamesh Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Alice Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shangru Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular Depth Estimation Using Cues Inspired by Biological Vision Systems. (arXiv:2204.10384v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10384","description":"<p>Monocular depth estimation (MDE) aims to transform an RGB image of a scene\ninto a pixelwise depth map from the same camera view. It is fundamentally\nill-posed due to missing information: any single image can have been taken from\nmany possible 3D scenes. Part of the MDE task is, therefore, to learn which\nvisual cues in the image can be used for depth estimation, and how. With\ntraining data limited by cost of annotation or network capacity limited by\ncomputational power, this is challenging. In this work we demonstrate that\nexplicitly injecting visual cue information into the model is beneficial for\ndepth estimation. Following research into biological vision systems, we focus\non semantic information and prior knowledge of object sizes and their\nrelations, to emulate the biological cues of relative size, familiar size, and\nabsolute size. We use state-of-the-art semantic and instance segmentation\nmodels to provide external information, and exploit language embeddings to\nencode relational information between classes. We also provide a prior on the\naverage real-world size of objects. This external information overcomes the\nlimitation in data availability, and ensures that the limited capacity of a\ngiven network is focused on known-helpful cues, therefore improving\nperformance. We experimentally validate our hypothesis and evaluate the\nproposed model on the widely used NYUD2 indoor depth estimation benchmark. The\nresults show improvements in depth prediction when the semantic information,\nsize prior and instance size are explicitly provided along with the RGB images,\nand our method can be easily adapted to any depth estimation system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Auty_D/0/1/0/all/0/1\">Dylan Auty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Sequential Latent Variable Models from Multimodal Time Series Data. (arXiv:2204.10419v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10419","description":"<p>Sequential modelling of high-dimensional data is an important problem that\nappears in many domains including model-based reinforcement learning and\ndynamics identification for control. Latent variable models applied to\nsequential data (i.e., latent dynamics models) have been shown to be a\nparticularly effective probabilistic approach to solve this problem, especially\nwhen dealing with images. However, in many application areas (e.g., robotics),\ninformation from multiple sensing modalities is available -- existing latent\ndynamics methods have not yet been extended to effectively make use of such\nmultimodal sequential data. Multimodal sensor streams can be correlated in a\nuseful manner and often contain complementary information across modalities. In\nthis work, we present a self-supervised generative modelling framework to\njointly learn a probabilistic latent state representation of multimodal data\nand the respective dynamics. Using synthetic and real-world datasets from a\nmultimodal robotic planar pushing task, we demonstrate that our approach leads\nto significant improvements in prediction and representation quality.\nFurthermore, we compare to the common learning baseline of concatenating each\nmodality in the latent space and show that our principled probabilistic\nformulation performs better. Finally, despite being fully self-supervised, we\ndemonstrate that our method is nearly as effective as an existing supervised\napproach that relies on ground truth labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Limoyo_O/0/1/0/all/0/1\">Oliver Limoyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ablett_T/0/1/0/all/0/1\">Trevor Ablett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1\">Jonathan Kelly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreTraM: Self-Supervised Pre-training via Connecting Trajectory and Map. (arXiv:2204.10435v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10435","description":"<p>Deep learning has recently achieved significant progress in trajectory\nforecasting. However, the scarcity of trajectory data inhibits the data-hungry\ndeep-learning models from learning good representations. While mature\nrepresentation learning methods exist in computer vision and natural language\nprocessing, these pre-training methods require large-scale data. It is hard to\nreplicate these approaches in trajectory forecasting due to the lack of\nadequate trajectory data (e.g., 34K samples in the nuScenes dataset). To work\naround the scarcity of trajectory data, we resort to another data modality\nclosely related to trajectories-HD-maps, which is abundantly provided in\nexisting datasets. In this paper, we propose PreTraM, a self-supervised\npre-training scheme via connecting trajectories and maps for trajectory\nforecasting. Specifically, PreTraM consists of two parts: 1) Trajectory-Map\nContrastive Learning, where we project trajectories and maps to a shared\nembedding space with cross-modal contrastive learning, and 2) Map Contrastive\nLearning, where we enhance map representation with contrastive learning on\nlarge quantities of HD-maps. On top of popular baselines such as AgentFormer\nand Trajectron++, PreTraM boosts their performance by 5.5% and 6.9% relatively\nin FDE-10 on the challenging nuScenes dataset. We show that PreTraM improves\ndata efficiency and scales well with model size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenfeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chen Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_A/0/1/0/all/0/1\">Alireza Fathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1\">Wei Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction. (arXiv:2204.10436v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10436","description":"<p>Unrolled neural networks have enabled state-of-the-art reconstruction\nperformance and fast inference times for the accelerated magnetic resonance\nimaging (MRI) reconstruction task. However, these approaches depend on\nfully-sampled scans as ground truth data which is either costly or not possible\nto acquire in many clinical medical imaging applications; hence, reducing\ndependence on data is desirable. In this work, we propose modeling the proximal\noperators of unrolled neural networks with scale-equivariant convolutional\nneural networks in order to improve the data-efficiency and robustness to\ndrifts in scale of the images that might stem from the variability of patient\nanatomies or change in field-of-view across different MRI scanners. Our\napproach demonstrates strong improvements over the state-of-the-art unrolled\nneural networks under the same memory constraints both with and without data\naugmentations on both in-distribution and out-of-distribution scaled images\nwithout significantly increasing the train or inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gunel_B/0/1/0/all/0/1\">Beliz Gunel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sahiner_A/0/1/0/all/0/1\">Arda Sahiner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Desai_A/0/1/0/all/0/1\">Arjun D. Desai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1\">Akshay S. Chaudhari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vasanawala_S/0/1/0/all/0/1\">Shreyas Vasanawala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pilanci_M/0/1/0/all/0/1\">Mert Pilanci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pauly_J/0/1/0/all/0/1\">John Pauly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiRA: Discriminative, Restorative, and Adversarial Learning for Self-supervised Medical Image Analysis. (arXiv:2204.10437v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10437","description":"<p>Discriminative learning, restorative learning, and adversarial learning have\nproven beneficial for self-supervised learning schemes in computer vision and\nmedical imaging. Existing efforts, however, omit their synergistic effects on\neach other in a ternary setup, which, we envision, can significantly benefit\ndeep semantic representation learning. To realize this vision, we have\ndeveloped DiRA, the first framework that unites discriminative, restorative,\nand adversarial learning in a unified manner to collaboratively glean\ncomplementary visual information from unlabeled medical images for fine-grained\nsemantic representation learning. Our extensive experiments demonstrate that\nDiRA (1) encourages collaborative learning among three learning ingredients,\nresulting in more generalizable representation across organs, diseases, and\nmodalities; (2) outperforms fully supervised ImageNet models and increases\nrobustness in small data regimes, reducing annotation cost across multiple\nmedical imaging applications; (3) learns fine-grained semantic representation,\nfacilitating accurate lesion localization with only image-level annotation; and\n(4) enhances state-of-the-art restorative approaches, revealing that DiRA is a\ngeneral mechanism for united representation learning. All code and pre-trained\nmodels are available at https: //github.com/JLiangLab/DiRA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haghighi_F/0/1/0/all/0/1\">Fatemeh Haghighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taher_M/0/1/0/all/0/1\">Mohammad Reza Hosseinzadeh Taher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotway_M/0/1/0/all/0/1\">Michael B. Gotway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jianming Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypergraph Transformer: Weakly-supervised Multi-hop Reasoning for Knowledge-based Visual Question Answering. (arXiv:2204.10448v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10448","description":"<p>Knowledge-based visual question answering (QA) aims to answer a question\nwhich requires visually-grounded external knowledge beyond image content\nitself. Answering complex questions that require multi-hop reasoning under weak\nsupervision is considered as a challenging problem since i) no supervision is\ngiven to the reasoning process and ii) high-order semantics of multi-hop\nknowledge facts need to be captured. In this paper, we introduce a concept of\nhypergraph to encode high-level semantics of a question and a knowledge base,\nand to learn high-order associations between them. The proposed model,\nHypergraph Transformer, constructs a question hypergraph and a query-aware\nknowledge hypergraph, and infers an answer by encoding inter-associations\nbetween two hypergraphs and intra-associations in both hypergraph itself.\nExtensive experiments on two knowledge-based visual QA and two knowledge-based\ntextual QA demonstrate the effectiveness of our method, especially for\nmulti-hop reasoning problem. Our source code is available at\nhttps://github.com/yujungheo/kbvqa-public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1\">Yu-Jung Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eun-Sol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_W/0/1/0/all/0/1\">Woo Suk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic View Synthesis With Few RGBD Cameras. (arXiv:2204.10477v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10477","description":"<p>There have been significant advancements in dynamic novel view synthesis in\nrecent years. However, current deep learning models often require (1) prior\nmodels (e.g., SMPL human models), (2) heavy pre-processing, or (3) per-scene\noptimization. We propose to utilize RGBD cameras to remove these limitations\nand synthesize free-viewpoint videos of dynamic indoor scenes. We generate\nfeature point clouds from RGBD frames and then render them into free-viewpoint\nvideos via a neural renderer. However, the inaccurate, unstable, and incomplete\ndepth measurements induce severe distortions, flickering, and ghosting\nartifacts. We enforce spatial-temporal consistency via the proposed Cycle\nReconstruction Consistency and Temporal Stabilization module to reduce these\nartifacts. We introduce a simple Regional Depth-Inpainting module that\nadaptively inpaints missing depth values to render complete novel views.\nAdditionally, we present a Human-Things Interactions dataset to validate our\napproach and facilitate future research. The dataset consists of 43 multi-view\nRGBD video sequences of everyday activities, capturing complex interactions\nbetween human subjects and their surroundings. Experiments on the HTI dataset\nshow that our method outperforms the baseline per-frame image fidelity and\nspatial-temporal consistency. We will release our code, and the dataset on the\nwebsite soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shengze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">YoungJoong Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+State_A/0/1/0/all/0/1\">Andrei State</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jia-Bin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_H/0/1/0/all/0/1\">Henry Fuchs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Affine Transformation for Text-to-image Synthesis. (arXiv:2204.10482v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10482","description":"<p>Text-to-image synthesis aims to generate natural images conditioned on text\ndescriptions. The main difficulty of this task lies in effectively fusing text\ninformation into the image synthesis process. Existing methods usually\nadaptively fuse suitable text information into the synthesis process with\nmultiple isolated fusion blocks (e.g., Conditional\n</p>\n<p>Batch Normalization and Instance Normalization). However, isolated fusion\nblocks not only conflict with each other but also increase the difficulty of\ntraining (see first page of the supplementary). To address these issues, we\npropose a Recurrent Affine Transformation (RAT) for Generative Adversarial\nNetworks that connects all the fusion blocks with a recurrent neural network to\nmodel their long-term dependency. Besides, to improve semantic consistency\nbetween texts and synthesized images, we incorporate a spatial attention model\nin the discriminator. Being aware of matching image regions, text descriptions\nsupervise the generator to synthesize more relevant image contents. Extensive\nexperiments on the CUB, Oxford-102 and COCO datasets demonstrate the\nsuperiority of the proposed model in comparison to state-of-the-art models\n\\footnote{https://github.com/senmaoy/Recurrent-Affine-Transformation-for-Text-to-image-Synthesis.git}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Senmao Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Minkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SE-GAN: Skeleton Enhanced GAN-based Model for Brush Handwriting Font Generation. (arXiv:2204.10484v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10484","description":"<p>Previous works on font generation mainly focus on the standard print fonts\nwhere character's shape is stable and strokes are clearly separated. There is\nrare research on brush handwriting font generation, which involves holistic\nstructure changes and complex strokes transfer. To address this issue, we\npropose a novel GAN-based image translation model by integrating the skeleton\ninformation. We first extract the skeleton from training images, then design an\nimage encoder and a skeleton encoder to extract corresponding features. A\nself-attentive refined attention module is devised to guide the model to learn\ndistinctive features between different domains. A skeleton discriminator is\ninvolved to first synthesize the skeleton image from the generated image with a\npre-trained generator, then to judge its realness to the target one. We also\ncontribute a large-scale brush handwriting font image dataset with six styles\nand 15,000 high-resolution images. Both quantitative and qualitative\nexperimental results demonstrate the competitiveness of our proposed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shaozu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruixue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Meng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhijie Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentions Help CNNs See Better: Attention-based Hybrid Image Quality Assessment Network. (arXiv:2204.10485v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10485","description":"<p>Image quality assessment (IQA) algorithm aims to quantify the human\nperception of image quality. Unfortunately, there is a performance drop when\nassessing the distortion images generated by generative adversarial network\n(GAN) with seemingly realistic texture. In this work, we conjecture that this\nmaladaptation lies in the backbone of IQA models, where patch-level prediction\nmethods use independent image patches as input to calculate their scores\nseparately, but lack spatial relationship modeling among image patches.\nTherefore, we propose an Attention-based Hybrid Image Quality Assessment\nNetwork (AHIQ) to deal with the challenge and get better performance on the\nGAN-based IQA task. Firstly, we adopt a two-branch architecture, including a\nvision transformer (ViT) branch and a convolutional neural network (CNN) branch\nfor feature extraction. The hybrid architecture combines interaction\ninformation among image patches captured by ViT and local texture details from\nCNN. To make the features from shallow CNN more focused on the visually salient\nregion, a deformable convolution is applied with the help of semantic\ninformation from the ViT branch. Finally, we use a patch-wise score prediction\nmodule to obtain the final score. The experiments show that our model\noutperforms the state-of-the-art methods on four standard IQA datasets and AHIQ\nranked first on the Full Reference (FR) track of the NTIRE 2022 Perceptual\nImage Quality Assessment Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lao_S/0/1/0/all/0/1\">Shanshan Lao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sidi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Adaptive Distillation for Leveraging Unimodal Encoders for Vision-Language Tasks. (arXiv:2204.10496v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10496","description":"<p>Cross-modal encoders for vision-language (VL) tasks are often pretrained with\ncarefully curated vision-language datasets. While these datasets reach an order\nof 10 million samples, the labor cost is prohibitive to scale further.\nConversely, unimodal encoders are pretrained with simpler annotations that are\nless cost-prohibitive, achieving scales of hundreds of millions to billions. As\na result, unimodal encoders have achieved state-of-art (SOTA) on many\ndownstream tasks. However, challenges remain when applying to VL tasks. The\npretraining data is not optimal for cross-modal architectures and requires\nheavy computational resources. In addition, unimodal architectures lack\ncross-modal interactions that have demonstrated significant benefits for VL\ntasks. Therefore, how to best leverage pretrained unimodal encoders for VL\ntasks is still an area of active research. In this work, we propose a method to\nleverage unimodal vision and text encoders for VL tasks that augment existing\nVL approaches while conserving computational complexity. Specifically, we\npropose Multimodal Adaptive Distillation (MAD), which adaptively distills\nuseful knowledge from pretrained encoders to cross-modal VL encoders. Second,\nto better capture nuanced impacts on VL task performance, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata constraints and conditions of domain shift. Experiments demonstrate that\nMAD leads to consistent gains in the low-shot, domain-shifted, and\nfully-supervised conditions on VCR, SNLI-VE, and VQA, achieving SOTA\nperformance on VCR compared to other single models pretrained with image-text\ndata. Finally, MAD outperforms concurrent works utilizing pretrained vision\nencoder from CLIP. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhecan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1\">Noel Codella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yen-Chun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Luowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_H/0/1/0/all/0/1\">Haoxuan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-fu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoint based Sign Language Translation without Glosses. (arXiv:2204.10511v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10511","description":"<p>Sign Language Translation (SLT) is a task that has not been studied\nrelatively much compared to the study of Sign Language Recognition (SLR).\nHowever, the SLR is a study that recognizes the unique grammar of sign\nlanguage, which is different from the spoken language and has a problem that\nnon-disabled people cannot easily interpret. So, we're going to solve the\nproblem of translating directly spoken language in sign language video. To this\nend, we propose a new keypoint normalization method for performing translation\nbased on the skeleton point of the signer and robustly normalizing these points\nin sign language translation. It contributed to performance improvement by a\ncustomized normalization method depending on the body parts. In addition, we\npropose a stochastic frame selection method that enables frame augmentation and\nsampling at the same time. Finally, it is translated into the spoken language\nthrough an Attention-based translation model. Our method can be applied to\nvarious datasets in a way that can be applied to datasets without glosses. In\naddition, quantitative experimental evaluation proved the excellence of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngmin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_M/0/1/0/all/0/1\">Minji Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeongeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_H/0/1/0/all/0/1\">Hyeongboo Baek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIPR:Automatic Annotation of Medical Images with Pixel Rearrangement. (arXiv:2204.10513v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10513","description":"<p>Most of the state-of-the-art semantic segmentation reported in recent years\nis based on fully supervised deep learning in the medical domain. How?ever, the\nhigh-quality annotated datasets require intense labor and domain knowledge,\nconsuming enormous time and cost. Previous works that adopt semi?supervised and\nunsupervised learning are proposed to address the lack of anno?tated data\nthrough assisted training with unlabeled data and achieve good perfor?mance.\nStill, these methods can not directly get the image annotation as doctors do.\nIn this paper, inspired by self-training of semi-supervised learning, we\npro?pose a novel approach to solve the lack of annotated data from another\nangle, called medical image pixel rearrangement (short in MIPR). The MIPR\ncombines image-editing and pseudo-label technology to obtain labeled data. As\nthe number of iterations increases, the edited image is similar to the original\nimage, and the labeled result is similar to the doctor annotation. Therefore,\nthe MIPR is to get labeled pairs of data directly from amounts of unlabled data\nwith pixel rearrange?ment, which is implemented with a designed conditional\nGenerative Adversarial Networks and a segmentation network. Experiments on the\nISIC18 show that the effect of the data annotated by our method for\nsegmentation task is is equal to or even better than that of doctors\nannotations\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dai_P/0/1/0/all/0/1\">Pingping Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1\">Haiming Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1\">Shuang Ge</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Ruihan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_X/0/1/0/all/0/1\">Xiang Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Object Detection with Proposal Balance Refinement. (arXiv:2204.10527v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10527","description":"<p>Few-shot object detection has gained significant attention in recent years as\nit has the potential to greatly reduce the reliance on large amounts of\nmanually annotated bounding boxes. While most existing few-shot object\ndetection literature primarily focuses on bounding box classification by\nobtaining as discriminative feature embeddings as possible, we emphasize the\nnecessity of handling the lack of intersection-over-union (IoU) variations\ninduced by a biased distribution of novel samples. In this paper, we analyze\nthe IoU imbalance that is caused by the relatively high number of low-quality\nregion proposals, and reveal that it plays a critical role in improving\nfew-shot learning capabilities. The well-known two stage fine-tuning technique\ncauses insufficient quality and quantity of the novel positive samples, which\nhinders the effective object detection of unseen novel classes. To alleviate\nthis issue, we present a few-shot object detection model with proposal balance\nrefinement, a simple yet effective approach in learning object proposals using\nan auxiliary sequential bounding box refinement process. This process enables\nthe detector to be optimized on the various IoU scores through additional novel\nclass samples. To fully exploit our sequential stage architecture, we revise\nthe fine-tuning strategy and expose the Region Proposal Network to the novel\nclasses in order to provide increased learning opportunities for the\nregion-of-interest (RoI) classifiers and regressors. Our extensive assessments\non PASCAL VOC and COCO demonstrate that our framework substantially outperforms\nother existing few-shot object detection approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sueyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_W/0/1/0/all/0/1\">Woo-Jeoung Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seong-Whan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fourier Imager Network (FIN): A deep neural network for hologram reconstruction with superior external generalization. (arXiv:2204.10533v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10533","description":"<p>Deep learning-based image reconstruction methods have achieved remarkable\nsuccess in phase recovery and holographic imaging. However, the generalization\nof their image reconstruction performance to new types of samples never seen by\nthe network remains a challenge. Here we introduce a deep learning framework,\ntermed Fourier Imager Network (FIN), that can perform end-to-end phase recovery\nand image reconstruction from raw holograms of new types of samples, exhibiting\nunprecedented success in external generalization. FIN architecture is based on\nspatial Fourier transform modules that process the spatial frequencies of its\ninputs using learnable filters and a global receptive field. Compared with\nexisting convolutional deep neural networks used for hologram reconstruction,\nFIN exhibits superior generalization to new types of samples, while also being\nmuch faster in its image inference speed, completing the hologram\nreconstruction task in ~0.04 s per 1 mm^2 of the sample area. We experimentally\nvalidated the performance of FIN by training it using human lung tissue samples\nand blindly testing it on human prostate, salivary gland tissue and Pap smear\nsamples, proving its superior external generalization and image reconstruction\nspeed. Beyond holographic microscopy and quantitative phase imaging, FIN and\nthe underlying neural network architecture might open up various new\nopportunities to design broadly generalizable deep learning models in\ncomputational imaging and machine vision fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Luzhe Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Representational Shift for Continual Fine-tuning. (arXiv:2204.10535v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10535","description":"<p>We study a practical setting of continual learning: fine-tuning on a\npre-trained model continually. Previous work has found that, when training on\nnew tasks, the features (penultimate layer representations) of previous data\nwill change, called representational shift. Besides the shift of features, we\nreveal that the intermediate layers' representational shift (IRS) also matters\nsince it disrupts batch normalization, which is another crucial cause of\ncatastrophic forgetting. Motivated by this, we propose ConFiT, a fine-tuning\nmethod incorporating two components, cross-convolution batch normalization\n(Xconv BN) and hierarchical fine-tuning. Xconv BN maintains pre-convolution\nrunning means instead of post-convolution, and recovers post-convolution ones\nbefore testing, which corrects the inaccurate estimates of means under IRS.\nHierarchical fine-tuning leverages a multi-stage strategy to fine-tune the\npre-trained network, preventing massive changes in Conv layers and thus\nalleviating IRS. Experimental results on four datasets show that our method\nremarkably outperforms several state-of-the-art methods with lower storage\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jie_S/0/1/0/all/0/1\">Shibo Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi-Hong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth Pruning with Auxiliary Networks for TinyML. (arXiv:2204.10546v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10546","description":"<p>Pruning is a neural network optimization technique that sacrifices accuracy\nin exchange for lower computational requirements. Pruning has been useful when\nworking with extremely constrained environments in tinyML. Unfortunately,\nspecial hardware requirements and limited study on its effectiveness on already\ncompact models prevent its wider adoption. Depth pruning is a form of pruning\nthat requires no specialized hardware but suffers from a large accuracy\nfalloff. To improve this, we propose a modification that utilizes a highly\nefficient auxiliary network as an effective interpreter of intermediate feature\nmaps. Our results show a parameter reduction of 93% on the MLPerfTiny Visual\nWakewords (VWW) task and 28% on the Keyword Spotting (KWS) task with accuracy\ncost of 0.65% and 1.06% respectively. When evaluated on a Cortex-M0\nmicrocontroller, our proposed method reduces the VWW model size by 4.7x and\nlatency by 1.6x while counter intuitively gaining 1% accuracy. KWS model size\non Cortex-M0 was also reduced by 1.2x and latency by 1.2x at the cost of 2.21%\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leon_J/0/1/0/all/0/1\">Josen Daniel De Leon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atienza_R/0/1/0/all/0/1\">Rowel Atienza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JIFF: Jointly-aligned Implicit Face Function for High Quality Single View Clothed Human Reconstruction. (arXiv:2204.10549v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10549","description":"<p>This paper addresses the problem of single view 3D human reconstruction.\nRecent implicit function based methods have shown impressive results, but they\nfail to recover fine face details in their reconstructions. This largely\ndegrades user experience in applications like 3D telepresence. In this paper,\nwe focus on improving the quality of face in the reconstruction and propose a\nnovel Jointly-aligned Implicit Face Function (JIFF) that combines the merits of\nthe implicit function based approach and model based approach. We employ a 3D\nmorphable face model as our shape prior and compute space-aligned 3D features\nthat capture detailed face geometry information. Such space-aligned 3D features\nare combined with pixel-aligned 2D features to jointly predict an implicit face\nfunction for high quality face reconstruction. We further extend our pipeline\nand introduce a coarse-to-fine architecture to predict high quality texture for\nour detailed face model. Extensive evaluations have been carried out on public\ndatasets and our proposed JIFF has demonstrates superior performance (both\nquantitatively and qualitatively) over existing state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yukang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guanying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenqi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kwan-Yee K. Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of an algorithm for medical image segmentation of bone tissue in interaction with metallic implants. (arXiv:2204.10560v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10560","description":"<p>This preliminary study focuses on the development of a medical image\nsegmentation algorithm based on artificial intelligence for calculating bone\ngrowth in contact with metallic implants. %as a result of the problem of\nestimating the growth of new bone tissue due to artifacts. %the presence of\nvarious types of distortions and errors, known as artifacts.\n</p>\n<p>Two databases consisting of computerized microtomography images have been\nused throughout this work: 100 images for training and 196 images for testing.\nBoth bone and implant tissue were manually segmented in the training data set.\nThe type of network constructed follows the U-Net architecture, a convolutional\nneural network explicitly used for medical image segmentation.\n</p>\n<p>In terms of network accuracy, the model reached around 98\\%. Once the\nprediction was obtained from the new data set (test set), the total number of\npixels belonging to bone tissue was calculated. This volume is around 15\\% of\nthe volume estimated by conventional techniques, which are usually\noverestimated. This method has shown its good performance and results, although\nit has a wide margin for improvement, modifying various parameters of the\nnetworks or using larger databases to improve training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_Torres_F/0/1/0/all/0/1\">Fernando Garc&#xed;a-Torres</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Minguez_Porter_C/0/1/0/all/0/1\">Carmen M&#xed;nguez-Porter</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomas_Chenoll_J/0/1/0/all/0/1\">Julia Tom&#xe1;s-Chenoll</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iranzo_Egea_S/0/1/0/all/0/1\">Sof&#xed;a Iranzo-Egea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belda_Lois_J/0/1/0/all/0/1\">Juan-Manuel Belda-Lois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Clustering as an Emergent Consensus of Autonomous Agents. (arXiv:2204.10585v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10585","description":"<p>We present a data segmentation method based on a first-order density-induced\nconsensus protocol. We provide a mathematically rigorous analysis of the\nconsensus model leading to the stopping criteria of the data segmentation\nalgorithm. To illustrate our method, the algorithm is applied to\ntwo-dimensional shape datasets and selected images from Berkeley Segmentation\nDataset. The method can be seen as an augmentation of classical clustering\ntechniques for multimodal feature space, such as DBSCAN. It showcases a curious\nconnection between data clustering and collective behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minakowski_P/0/1/0/all/0/1\">Piotr Minakowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peszek_J/0/1/0/all/0/1\">Jan Peszek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Note on the Regularity of Images Generated by Convolutional Neural Networks. (arXiv:2204.10588v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10588","description":"<p>The regularity of images generated by convolutional neural networks, such as\nthe U-net, generative adversarial networks, or the deep image prior, is\nanalyzed. In a resolution-independent, infinite dimensional setting, it is\nshown that such images, represented as functions, are always continuous and, in\nsome circumstances, even continuously differentiable, contradicting the widely\naccepted modeling of sharp edges in images via jump discontinuities. While such\nstatements require an infinite dimensional setting, the connection to\n(discretized) neural networks used in practice is made by considering the limit\nas the resolution approaches infinity. As practical consequence, the results of\nthis paper suggest to refrain from basic L2 regularization of network weights\nin case of images being the network output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habring_A/0/1/0/all/0/1\">Andreas Habring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holler_M/0/1/0/all/0/1\">Martin Holler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spacing Loss for Discovering Novel Categories. (arXiv:2204.10595v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10595","description":"<p>Novel Class Discovery (NCD) is a learning paradigm, where a machine learning\nmodel is tasked to semantically group instances from unlabeled data, by\nutilizing labeled instances from a disjoint set of classes. In this work, we\nfirst characterize existing NCD approaches into single-stage and two-stage\nmethods based on whether they require access to labeled and unlabeled data\ntogether while discovering new classes. Next, we devise a simple yet powerful\nloss function that enforces separability in the latent space using cues from\nmulti-dimensional scaling, which we refer to as Spacing Loss. Our proposed\nformulation can either operate as a standalone method or can be plugged into\nexisting methods to enhance them. We validate the efficacy of Spacing Loss with\nthorough experimental evaluation across multiple settings on CIFAR-10 and\nCIFAR-100 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joseph_K/0/1/0/all/0/1\">K J Joseph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sujoy Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_G/0/1/0/all/0/1\">Gaurav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Soma Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Expert Utilization in Mixture-of-Experts Layers Embedded in CNNs. (arXiv:2204.10598v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10598","description":"<p>This work addresses the problem of unbalanced expert utilization in\nsparsely-gated Mixture of Expert (MoE) layers, embedded directly into\nconvolutional neural networks. To enable a stable training process, we present\nboth soft and hard constraint-based approaches. With hard constraints, the\nweights of certain experts are allowed to become zero, while soft constraints\nbalance the contribution of experts with an additional auxiliary loss. As a\nresult, soft constraints handle expert utilization better and support the\nexpert specialization process, hard constraints mostly maintain generalized\nexperts and increase the model performance for many applications. Our findings\ndemonstrate that even with a single dataset and end-to-end training, experts\ncan implicitly focus on individual sub-domains of the input space. Experts in\nthe proposed models with MoE embeddings implicitly focus on distinct domains,\neven without suitable predefined datasets. As an example, experts trained for\nCIFAR-100 image classification specialize in recognizing different domains such\nas sea animals or flowers without previous data clustering. Experiments with\nRetinaNet and the COCO dataset further indicate that object detection experts\ncan also specialize in detecting objects of distinct sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavlitskaya_S/0/1/0/all/0/1\">Svetlana Pavlitskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubschneider_C/0/1/0/all/0/1\">Christian Hubschneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Surfaces for Sparse Point Clouds with On-Surface Priors. (arXiv:2204.10603v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10603","description":"<p>It is an important task to reconstruct surfaces from 3D point clouds. Current\nmethods are able to reconstruct surfaces by learning Signed Distance Functions\n(SDFs) from single point clouds without ground truth signed distances or point\nnormals. However, they require the point clouds to be dense, which dramatically\nlimits their performance in real applications. To resolve this issue, we\npropose to reconstruct highly accurate surfaces from sparse point clouds with\nan on-surface prior. We train a neural network to learn SDFs via projecting\nqueries onto the surface represented by the sparse point cloud. Our key idea is\nto infer signed distances by pushing both the query projections to be on the\nsurface and the projection distance to be the minimum. To achieve this, we\ntrain a neural network to capture the on-surface prior to determine whether a\npoint is on a sparse point cloud or not, and then leverage it as a\ndifferentiable function to learn SDFs from unseen sparse point cloud. Our\nmethod can learn SDFs from a single sparse point cloud without ground truth\nsigned distances or point normals. Our numerical evaluation under widely used\nbenchmarks demonstrates that our method achieves state-of-the-art\nreconstruction accuracy, especially for sparse point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Baorui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Transferability via Feature-Momentum Adversarial Attack. (arXiv:2204.10606v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10606","description":"<p>Transferable adversarial attack has drawn increasing attention due to their\npractical threaten to real-world applications. In particular, the feature-level\nadversarial attack is one recent branch that can enhance the transferability\nvia disturbing the intermediate features. The existing methods usually create a\nguidance map for features, where the value indicates the importance of the\ncorresponding feature element and then employs an iterative algorithm to\ndisrupt the features accordingly. However, the guidance map is fixed in\nexisting methods, which can not consistently reflect the behavior of networks\nas the image is changed during iteration. In this paper, we describe a new\nmethod called Feature-Momentum Adversarial Attack (FMAA) to further improve\ntransferability. The key idea of our method is that we estimate a guidance map\ndynamically at each iteration using momentum to effectively disturb the\ncategory-relevant features. Extensive experiments demonstrate that our method\nsignificantly outperforms other state-of-the-art methods by a large margin on\ndifferent target models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xianglong/0/1/0/all/0/1\">Xianglong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuezun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1\">Haipeng Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Junyu Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time HOG+SVM based object detection using SoC FPGA for a UHD video stream. (arXiv:2204.10619v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10619","description":"<p>Object detection is an essential component of many vision systems. For\nexample, pedestrian detection is used in advanced driver assistance systems\n(ADAS) and advanced video surveillance systems (AVSS). Currently, most\ndetectors use deep convolutional neural networks (e.g., the YOLO -- You Only\nLook Once -- family), which, however, due to their high computational\ncomplexity, are not able to process a very high-resolution video stream in\nreal-time, especially within a limited energy budget. In this paper we present\na hardware implementation of the well-known pedestrian detector with HOG\n(Histogram of Oriented Gradients) feature extraction and SVM (Support Vector\nMachine) classification. Our system running on AMD Xilinx Zynq UltraScale+\nMPSoC (Multiprocessor System on Chip) device allows real-time processing of 4K\nresolution (UHD -- Ultra High Definition, 3840 x 2160 pixels) video for 60\nframes per second. The system is capable of detecting a pedestrian in a single\nscale. The results obtained confirm the high suitability of reprogrammable\ndevices in the real-time implementation of embedded vision systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wasala_M/0/1/0/all/0/1\">Mateusz Wasala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1\">Tomasz Kryjak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Prototype Convolution Network for Few-Shot Semantic Segmentation. (arXiv:2204.10638v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10638","description":"<p>The key challenge for few-shot semantic segmentation (FSS) is how to tailor a\ndesirable interaction among support and query features and/or their prototypes,\nunder the episodic training scenario. Most existing FSS methods implement such\nsupport-query interactions by solely leveraging plain operations - e.g., cosine\nsimilarity and feature concatenation - for segmenting the query objects.\nHowever, these interaction approaches usually cannot well capture the intrinsic\nobject details in the query images that are widely encountered in FSS, e.g., if\nthe query object to be segmented has holes and slots, inaccurate segmentation\nalmost always happens. To this end, we propose a dynamic prototype convolution\nnetwork (DPCN) to fully capture the aforementioned intrinsic details for\naccurate FSS. Specifically, in DPCN, a dynamic convolution module (DCM) is\nfirstly proposed to generate dynamic kernels from support foreground, then\ninformation interaction is achieved by convolution operations over query\nfeatures using these kernels. Moreover, we equip DPCN with a support activation\nmodule (SAM) and a feature filtering module (FFM) to generate pseudo mask and\nfilter out background information for the query images, respectively. SAM and\nFFM together can mine enriched context information from the query features. Our\nDPCN is also flexible and efficient under the k-shot FSS setting. Extensive\nexperiments on PASCAL-5i and COCO-20i show that DPCN yields superior\nperformances under both 1-shot and 5-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yanqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Huan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonke_J/0/1/0/all/0/1\">Jan-Jakob Sonke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposure Correction Model to Enhance Image Quality. (arXiv:2204.10648v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10648","description":"<p>Exposure errors in an image cause a degradation in the contrast and low\nvisibility in the content. In this paper, we address this problem and propose\nan end-to-end exposure correction model in order to handle both under- and\noverexposure errors with a single model. Our model contains an image encoder,\nconsecutive residual blocks, and image decoder to synthesize the corrected\nimage. We utilize perceptual loss, feature matching loss, and multi-scale\ndiscriminator to increase the quality of the generated image as well as to make\nthe training more stable. The experimental results indicate the effectiveness\nof proposed model. We achieve the state-of-the-art result on a large-scale\nexposure dataset. Besides, we investigate the effect of exposure setting of the\nimage on the portrait matting task. We find that under- and overexposed images\ncause severe degradation in the performance of the portrait matting models. We\nshow that after applying exposure correction with the proposed model, the\nportrait matting quality increases significantly.\nhttps://github.com/yamand16/ExposureCorrection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eyiokur_F/0/1/0/all/0/1\">Fevziye Irem Eyiokur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DFAM-DETR: Deformable feature based attention mechanism DETR on slender object detection. (arXiv:2204.10667v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10667","description":"<p>Object detection is one of the most significant aspects of computer vision,\nand it has achieved substantial results in a variety of domains. It is worth\nnoting that there are few studies focusing on slender object detection. CNNs\nare widely employed in object detection, however it performs poorly on slender\nobject detection due to the fixed geometric structure and sampling points. In\ncomparison, Deformable DETR has the ability to obtain global to specific\nfeatures. Even though it outperforms the CNNs in slender objects detection\naccuracy and efficiency, the results are still not satisfactory. Therefore, we\npropose Deformable Feature based Attention Mechanism (DFAM) to increase the\nslender object detection accuracy and efficiency of Deformable DETR. The DFAM\nhas adaptive sampling points of deformable convolution and attention mechanism\nthat aggregate information from the entire input sequence in the backbone\nnetwork. This improved detector is named as Deformable Feature based Attention\nMechanism DETR (DFAM- DETR). Results indicate that DFAM-DETR achieves\noutstanding detection performance on slender objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_W/0/1/0/all/0/1\">Wang Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiaojie_H/0/1/0/all/0/1\">Hu Xiaojie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unknown Face Presentation Attack Detection via Localised Learning of Multiple Kernels. (arXiv:2204.10675v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10675","description":"<p>The paper studies face spoofing, a.k.a. presentation attack detection (PAD)\nin the demanding scenarios of unknown types of attack. While earlier studies\nhave revealed the benefits of ensemble methods, and in particular, a multiple\nkernel learning approach to the problem, one limitation of such techniques is\nthat they typically treat the entire observation space similarly and ignore any\nvariability and local structure inherent to the data. This work studies this\naspect of the face presentation attack detection problem in relation to\nmultiple kernel learning in a one-class setting to benefit from intrinsic local\nstructure in bona fide face samples. More concretely, inspired by the success\nof the one-class Fisher null formalism, we formulate a convex localised\nmultiple kernel learning algorithm by imposing a joint matrix-norm constraint\non the collection of local kernel weights and infer locally adaptive weights\nfor zero-shot one-class unseen attack detection.\n</p>\n<p>We present a theoretical study of the proposed localised MKL algorithm using\nRademacher complexities to characterise its generalisation capability and\ndemonstrate the advantages of the proposed technique over some other options.\nAn assessment of the proposed approach on general object image datasets\nillustrates its efficacy for abnormality and novelty detection while the\nresults of the experiments on face PAD datasets verifies its potential in\ndetecting unknown/unseen face presentation attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arashloo_S/0/1/0/all/0/1\">Shervin Rahimzadeh Arashloo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving tracking with a tracklet associator. (arXiv:2204.10677v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10677","description":"<p>Multiple object tracking (MOT) is a task in computer vision that aims to\ndetect the position of various objects in videos and to associate them to a\nunique identity. We propose an approach based on Constraint Programming (CP)\nwhose goal is to be grafted to any existing tracker in order to improve its\nobject association results. We developed a modular algorithm divided into three\nindependent phases. The first phase consists in recovering the tracklets\nprovided by a base tracker and to cut them at the places where uncertain\nassociations are spotted, for example, when tracklets overlap, which may cause\nidentity switches. In the second phase, we associate the previously constructed\ntracklets using a Belief Propagation Constraint Programming algorithm, where we\npropose various constraints that assign scores to each of the tracklets based\non multiple characteristics, such as their dynamics or the distance between\nthem in time and space. Finally, the third phase is a rudimentary interpolation\nmodel to fill in the remaining holes in the trajectories we built. Experiments\nshow that our model leads to improvements in the results for all three of the\nstate-of-the-art trackers on which we tested it (3 to 4 points gained on HOTA\nand IDF1).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nahon_R/0/1/0/all/0/1\">R&#xe9;mi Nahon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pesant_G/0/1/0/all/0/1\">Gilles Pesant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiality-guided Transformer for 3D Dense Captioning on Point Clouds. (arXiv:2204.10688v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10688","description":"<p>Dense captioning in 3D point clouds is an emerging vision-and-language task\ninvolving object-level 3D scene understanding. Apart from coarse semantic class\nprediction and bounding box regression as in traditional 3D object detection,\n3D dense captioning aims at producing a further and finer instance-level label\nof natural language description on visual appearance and spatial relations for\neach scene object of interest. To detect and describe objects in a scene,\nfollowing the spirit of neural machine translation, we propose a\ntransformer-based encoder-decoder architecture, namely SpaCap3D, to transform\nobjects into descriptions, where we especially investigate the relative\nspatiality of objects in 3D scenes and design a spatiality-guided encoder via a\ntoken-to-token spatial relation learning objective and an object-centric\ndecoder for precise and spatiality-enhanced object caption generation.\nEvaluated on two benchmark datasets, ScanRefer and ReferIt3D, our proposed\nSpaCap3D outperforms the baseline method Scan2Cap by 4.94% and 9.61% in\nCIDEr@0.5IoU, respectively. Our project page with source code and supplementary\nfiles is available at https://SpaCap3D.github.io/ .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianhui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcing Generated Images via Meta-learning for One-Shot Fine-Grained Visual Recognition. (arXiv:2204.10689v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10689","description":"<p>One-shot fine-grained visual recognition often suffers from the problem of\nhaving few training examples for new fine-grained classes. To alleviate this\nproblem, off-the-shelf image generation techniques based on Generative\nAdversarial Networks (GANs) can potentially create additional training images.\nHowever, these GAN-generated images are often not helpful for actually\nimproving the accuracy of one-shot fine-grained recognition. In this paper, we\npropose a meta-learning framework to combine generated images with original\nimages, so that the resulting \"hybrid\" training images improve one-shot\nlearning. Specifically, the generic image generator is updated by a few\ntraining instances of novel classes, and a Meta Image Reinforcing Network\n(MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as\nimage reinforcement. Our experiments demonstrate consistent improvement over\nbaselines on one-shot fine-grained image classification benchmarks.\nFurthermore, our analysis shows that the reinforced images have more diversity\ncompared to the original and GAN-generated images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsutsui_S/0/1/0/all/0/1\">Satoshi Tsutsui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUES-200: A Multi-height Multi-scene Cross-view Image Benchmark Across Drone and Satellite. (arXiv:2204.10704v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10704","description":"<p>The purpose of cross-view image matching is to match images acquired from the\ndifferent platforms of the same target scene and then help positioning system\nto infer the location of the target scene. With the rapid development of drone\ntechnology, how to help Drone positioning or navigation through cross-view\nmatching technology has become a challenging research topic. However, the\naccuracy of current cross-view matching models is still low, mainly because the\nexisting public datasets do not include the differences in images obtained by\ndrones at different heights, and the types of scenes are relatively\nhomogeneous, which makes the models unable to adapt to complex and changing\nscenes. We propose a new cross-view dataset, SUES-200, to address these\nissues.SUES-200 contains images acquired by the drone at four flight heights\nand the corresponding satellite view images under the same target scene. To our\nknowledge, SUES-200 is the first dataset that considers the differences\ngenerated by aerial photography of drones at different flight heights. In\naddition, we build a pipeline for efficient training testing and evaluation of\ncross-view matching models. Then, we comprehensively evaluate the performance\nof feature extractors with different CNN architectures on SUES-200 through an\nevaluation system for cross-view matching models and propose a robust baseline\nmodel. The experimental results show that SUES-200 can help the model learn\nfeatures with high discrimination at different heights. Evaluating indicators\nof the matching system improves as the drone flight height gets higher because\nthe drone camera pose and the surrounding environment have less influence on\naerial photography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Runzhe Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmbedTrack -- Simultaneous Cell Segmentation and Tracking Through Learning Offsets and Clustering Bandwidths. (arXiv:2204.10713v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10713","description":"<p>A systematic analysis of the cell behavior requires automated approaches for\ncell segmentation and tracking. While deep learning has been successfully\napplied for the task of cell segmentation, there are few approaches for\nsimultaneous cell segmentation and tracking using deep learning. Here, we\npresent EmbedTrack, a single convolutional neural network for simultaneous cell\nsegmentation and tracking which predicts easy to interpret embeddings. As\nembeddings, offsets of cell pixels to their cell center and bandwidths are\nlearned. We benchmark our approach on nine 2D data sets from the Cell Tracking\nChallenge, where our approach performs on seven out of nine data sets within\nthe top 3 contestants including three top 1 performances. The source code is\npublicly available at https://git.scc.kit.edu/kit-loe-ge/embedtrack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loffler_K/0/1/0/all/0/1\">Katharina L&#xf6;ffler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikut_R/0/1/0/all/0/1\">Ralf Mikut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Instance Discovery: Vision-Transformer for Instance-Aware Multi-Label Image Recognition. (arXiv:2204.10731v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10731","description":"<p>Previous works on multi-label image recognition (MLIR) usually use CNNs as a\nstarting point for research. In this paper, we take pure Vision Transformer\n(ViT) as the research base and make full use of the advantages of Transformer\nwith long-range dependency modeling to circumvent the disadvantages of CNNs\nlimited to local receptive field. However, for multi-label images containing\nmultiple objects from different categories, scales, and spatial relations, it\nis not optimal to use global information alone. Our goal is to leverage ViT's\npatch tokens and self-attention mechanism to mine rich instances in multi-label\nimages, named diverse instance discovery (DiD). To this end, we propose a\nsemantic category-aware module and a spatial relationship-aware module,\nrespectively, and then combine the two by a re-constraint strategy to obtain\ninstance-aware attention maps. Finally, we propose a weakly supervised object\nlocalization-based approach to extract multi-scale local features, to form a\nmulti-view pipeline. Our method requires only weakly supervised information at\nthe label level, no additional knowledge injection or other strongly supervised\ninformation is required. Experiments on three benchmark datasets show that our\nmethod significantly outperforms previous works and achieves state-of-the-art\nresults under fair experimental comparisons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yunqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xuan Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haiwen Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_F/0/1/0/all/0/1\">Feihu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic Images in Facial Capture Pipelines. (arXiv:2204.10746v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10746","description":"<p>We propose an end-to-end pipeline for both building and tracking 3D facial\nmodels from personalized in-the-wild (cellphone, webcam, youtube clips, etc.)\nvideo data. First, we present a method for automatic data curation and\nretrieval based on a hierarchical clustering framework typical of collision\ndetection algorithms in traditional computer graphics pipelines. Subsequently,\nwe utilize synthetic turntables and leverage deepfake technology in order to\nbuild a synthetic multi-view stereo pipeline for appearance capture that is\nrobust to imperfect synthetic geometry and image misalignment. The resulting\nmodel is fit with an animation rig, which is then used to track facial\nperformances. Notably, our novel use of deepfake technology enables us to\nperform robust tracking of in-the-wild data using differentiable renderers\ndespite a significant synthetic-to-real domain gap. Finally, we outline how we\ntrain a motion capture regressor, leveraging the aforementioned techniques to\navoid the need for real-world ground truth data and/or a high-end calibrated\ncamera capture setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Winnie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Demi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedkiw_R/0/1/0/all/0/1\">Ron Fedkiw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PU-EVA: An Edge Vector based Approximation Solution for Flexible-scale Point Cloud Upsampling. (arXiv:2204.10750v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10750","description":"<p>High-quality point clouds have practical significance for point-based\nrendering, semantic understanding, and surface reconstruction. Upsampling\nsparse, noisy and nonuniform point clouds for a denser and more regular\napproximation of target objects is a desirable but challenging task. Most\nexisting methods duplicate point features for upsampling, constraining the\nupsampling scales at a fixed rate. In this work, the flexible upsampling rates\nare achieved via edge vector based affine combinations, and a novel design of\nEdge Vector based Approximation for Flexible-scale Point clouds Upsampling\n(PU-EVA) is proposed. The edge vector based approximation encodes the\nneighboring connectivity via affine combinations based on edge vectors, and\nrestricts the approximation error within the second-order term of Taylor's\nExpansion. The EVA upsampling decouples the upsampling scales with network\narchitecture, achieving the flexible upsampling rates in one-time training.\nQualitative and quantitative evaluations demonstrate that the proposed PU-EVA\noutperforms the state-of-the-art in terms of proximity-to-surface, distribution\nuniformity, and geometric details preservation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Luqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Lulu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wanyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shizheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhi-Xin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iCAR: Bridging Image Classification and Image-text Alignment for Visual Recognition. (arXiv:2204.10760v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10760","description":"<p>Image classification, which classifies images by pre-defined categories, has\nbeen the dominant approach to visual representation learning over the last\ndecade. Visual learning through image-text alignment, however, has emerged to\nshow promising performance, especially for zero-shot recognition. We believe\nthat these two learning tasks are complementary, and suggest combining them for\nbetter visual learning. We propose a deep fusion method with three adaptations\nthat effectively bridge two learning tasks, rather than shallow fusion through\nnaive multi-task learning. First, we modify the previous common practice in\nimage classification, a linear classifier, with a cosine classifier which shows\ncomparable performance. Second, we convert the image classification problem\nfrom learning parametric category classifier weights to learning a text encoder\nas a meta network to generate category classifier weights. The learnt text\nencoder is shared between image classification and image-text alignment. Third,\nwe enrich each class name with a description to avoid confusion between classes\nand make the classification method closer to the image-text alignment. We prove\nthat this deep fusion approach performs better on a variety of visual\nrecognition tasks and setups than the individual learning or shallow fusion\napproach, from zero-shot/few-shot image classification, such as the Kornblith\n12-dataset benchmark, to downstream tasks of action recognition, semantic\nsegmentation, and object detection in fine-tuning and open-vocabulary settings.\nThe code will be available at https://github.com/weiyx16/iCAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuliang Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation. (arXiv:2204.10762v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10762","description":"<p>A high-resolution network exhibits remarkable capability in extracting\nmulti-scale features for human pose estimation, but fails to capture long-range\ninteractions between joints and has high computational complexity. To address\nthese problems, we present a Dynamic lightweight High-Resolution Network\n(Dite-HRNet), which can efficiently extract multi-scale contextual information\nand model long-range spatial dependency for human pose estimation.\nSpecifically, we propose two methods, dynamic split convolution and adaptive\ncontext modeling, and embed them into two novel lightweight blocks, which are\nnamed dynamic multi-scale context block and dynamic global context block. These\ntwo blocks, as the basic component units of our Dite-HRNet, are specially\ndesigned for the high-resolution networks to make full use of the parallel\nmulti-resolution architecture. Experimental results show that the proposed\nnetwork achieves superior performance on both COCO and MPII human pose\nestimation datasets, surpassing the state-of-the-art lightweight networks. Code\nis available at: \\url{https://github.com/ZiyiZhang27/Dite-HRNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tag-Based Attention Guided Bottom-Up Approach for Video Instance Segmentation. (arXiv:2204.10765v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10765","description":"<p>Video Instance Segmentation is a fundamental computer vision task that deals\nwith segmenting and tracking object instances across a video sequence. Most\nexisting methods typically accomplish this task by employing a multi-stage\ntop-down approach that usually involves separate networks to detect and segment\nobjects in each frame, followed by associating these detections in consecutive\nframes using a learned tracking head. In this work, however, we introduce a\nsimple end-to-end trainable bottom-up approach to achieve instance mask\npredictions at the pixel-level granularity, instead of the typical\nregion-proposals-based approach. Unlike contemporary frame-based models, our\nnetwork pipeline processes an input video clip as a single 3D volume to\nincorporate temporal information. The central idea of our formulation is to\nsolve the video instance segmentation task as a tag assignment problem, such\nthat generating distinct tag values essentially separates individual object\ninstances across the video sequence (here each tag could be any arbitrary value\nbetween 0 and 1). To this end, we propose a novel spatio-temporal tagging loss\nthat allows for sufficient separation of different objects as well as necessary\nidentification of different instances of the same object. Furthermore, we\npresent a tag-based attention module that improves instance tags, while\nconcurrently learning instance propagation within a video. Evaluations\ndemonstrate that our method provides competitive results on YouTube-VIS and\nDAVIS-19 datasets, and has minimum run-time compared to other state-of-the-art\nperformance methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kini_J/0/1/0/all/0/1\">Jyoti Kini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising of Three-Dimensional Fast Spin Echo Magnetic Resonance Images of Knee Joints using Spatial-Variant Noise-Relevant Residual Learning of Convolution Neural Network. (arXiv:2204.10773v1 [eess.IV])","link":"http://arxiv.org/abs/2204.10773","description":"<p>Two-dimensional (2D) fast spin echo (FSE) techniques play a central role in\nthe clinical magnetic resonance imaging (MRI) of knee joints. Moreover,\nthree-dimensional (3D) FSE provides high-isotropic-resolution magnetic\nresonance (MR) images of knee joints, but it has a reduced signal-to-noise\nratio compared to 2D FSE. Deep-learning denoising methods are a promising\napproach for denoising MR images, but they are often trained using synthetic\nnoise due to challenges in obtaining true noise distributions for MR images. In\nthis study, inherent true noise information from 2-NEX acquisition was used to\ndevelop a deep-learning model based on residual learning of convolutional\nneural network (CNN), and this model was used to suppress the noise in 3D FSE\nMR images of knee joints. The proposed CNN used two-step residual learning over\nparallel transporting and residual blocks and was designed to comprehensively\nlearn real noise features from 2-NEX training data. The results of an ablation\nstudy validated the network design. The new method achieved improved denoising\nperformance of 3D FSE knee MR images compared with current state-of-the-art\nmethods, based on the peak signal-to-noise ratio and structural similarity\nindex measure. The improved image quality after denoising using the new method\nwas verified by radiological evaluation. A deep CNN using the inherent\nspatial-varying noise information in 2-NEX acquisitions was developed. This\nmethod showed promise for clinical MRI assessments of the knee, and has\npotential applications for the assessment of other anatomical structures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_S/0/1/0/all/0/1\">Shutian Zhao</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Cahill_D/0/1/0/all/0/1\">Donal G. Cahill</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Siyue Li</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_F/0/1/0/all/0/1\">Fan Xiao</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Blu_T/0/1/0/all/0/1\">Thierry Blu</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Griffith_J/0/1/0/all/0/1\">James F Griffith</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weitian Chen</a> (1) ((1) Department of Imaging and Interventional Radiology, the Chinese University of Hong Kong, (2) Department of Electrical Engineering, the Chinese University of Hong Kong)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gen6D: Generalizable Model-Free 6-DoF Object Pose Estimation from RGB Images. (arXiv:2204.10776v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10776","description":"<p>In this paper, we present a generalizable model-free 6-DoF object pose\nestimator called Gen6D. Existing generalizable pose estimators either need\nhigh-quality object models or require additional depth maps or object masks in\ntest time, which significantly limits their application scope. In contrast, our\npose estimator only requires some posed images of the unseen object and is able\nto accurately predict the poses of the object in arbitrary environments. Gen6D\nconsists of an object detector, a viewpoint selector and a pose refiner, all of\nwhich do not require the 3D object model and can generalize to unseen objects.\nExperiments show that Gen6D achieves state-of-the-art results on two model-free\ndatasets: the MOPED dataset and a new GenMOP dataset collected by us. In\naddition, on the LINEMOD dataset, Gen6D achieves competitive results compared\nwith instance-specific pose estimators. Project page:\nhttps://liuyuan-pal.github.io/Gen6D/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yilin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Cheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiaoxiao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1\">Taku Komura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ParkPredict+: Multimodal Intent and Motion Prediction for Vehicles in Parking Lots with CNN and Transformer. (arXiv:2204.10777v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10777","description":"<p>The problem of multimodal intent and trajectory prediction for human-driven\nvehicles in parking lots is addressed in this paper. Using models designed with\nCNN and Transformer networks, we extract temporal-spatial and contextual\ninformation from trajectory history and local bird's eye view (BEV) semantic\nimages, and generate predictions about intent distribution and future\ntrajectory sequences. Our methods outperforms existing models in accuracy,\nwhile allowing an arbitrary number of modes, encoding complex multi-agent\nscenarios, and adapting to different parking maps. In addition, we present the\nfirst public human driving dataset in parking lot with high resolution and rich\ntraffic scenarios for relevant research in this field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacayo_M/0/1/0/all/0/1\">Matthew Lacayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guggilla_N/0/1/0/all/0/1\">Nidhir Guggilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borrelli_F/0/1/0/all/0/1\">Francesco Borrelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Centralized Adversarial Learning for Robust Deep Hashing. (arXiv:2204.10779v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10779","description":"<p>Deep hashing has been extensively utilized in massive image retrieval because\nof its efficiency and effectiveness. Recently, it becomes a hot issue to study\nadversarial examples which poses a security challenge to deep hashing models.\nHowever, there is still a critical bottleneck: how to find a superior and exact\nsemantic representative as the guide to further enhance the adversarial attack\nand defense in deep hashing based retrieval. We, for the first time, attempt to\ndesign an effective adversarial learning with the min-max paradigm to improve\nthe robustness of hashing networks by using the generated adversarial samples.\nSpecifically, we obtain the optimal solution (called center code) through a\nproved Continuous Hash Center Method (CHCM), which preserves the semantic\nsimilarity with positive samples and dissimilarity with negative samples. On\none hand, we propose the Deep Hashing Central Attack (DHCA) for efficient\nattack on hashing retrieval by maximizing the Hamming distance between the hash\ncode of adversarial example and the center code. On the other hand, we present\nthe Deep Hashing Central Adversarial Training (DHCAT) to optimize the hashing\nnetworks for defense, by minimizing the Hamming distance to the center code.\nExtensive experiments on the benchmark datasets verify that our attack method\ncan achieve better performance than the state-of-the-arts, and our defense\nalgorithm can effectively mitigate the effects of adversarial perturbations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xunguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pay \"Attention\" to Adverse Weather: Weather-aware Attention-based Object Detection. (arXiv:2204.10803v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10803","description":"<p>Despite the recent advances of deep neural networks, object detection for\nadverse weather remains challenging due to the poor perception of some sensors\nin adverse weather. Instead of relying on one single sensor, multimodal fusion\nhas been one promising approach to provide redundant detection information\nbased on multiple sensors. However, most existing multimodal fusion approaches\nare ineffective in adjusting the focus of different sensors under varying\ndetection environments in dynamic adverse weather conditions. Moreover, it is\ncritical to simultaneously observe local and global information under complex\nweather conditions, which has been neglected in most early or late-stage\nmultimodal fusion works. In view of these, this paper proposes a Global-Local\nAttention (GLA) framework to adaptively fuse the multi-modality sensing\nstreams, i.e., camera, gated camera, and lidar data, at two fusion stages.\nSpecifically, GLA integrates an early-stage fusion via a local attention\nnetwork and a late-stage fusion via a global attention network to deal with\nboth local and global information, which automatically allocates higher weights\nto the modality with better detection features at the late-stage fusion to cope\nwith the specific weather condition adaptively. Experimental results\ndemonstrate the superior performance of the proposed GLA compared with\nstate-of-the-art fusion approaches under various adverse weather conditions,\nsuch as light fog, dense fog, and snow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Saket S. Chaturvedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaoyong Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Scaffold: Optimizing Model Explanations for Teaching. (arXiv:2204.10810v1 [cs.LG])","link":"http://arxiv.org/abs/2204.10810","description":"<p>Modern machine learning models are opaque, and as a result there is a\nburgeoning academic subfield on methods that explain these models' behavior.\nHowever, what is the precise goal of providing such explanations, and how can\nwe demonstrate that explanations achieve this goal? Some research argues that\nexplanations should help teach a student (either human or machine) to simulate\nthe model being explained, and that the quality of explanations can be measured\nby the simulation accuracy of students on unexplained examples. In this work,\nleveraging meta-learning techniques, we extend this idea to improve the quality\nof the explanations themselves, specifically by optimizing explanations such\nthat student models more effectively learn to simulate the original model. We\ntrain models on three natural language processing and computer vision tasks,\nand find that students trained with explanations extracted with our framework\nare able to simulate the teacher significantly more effectively than ones\nproduced with previous methods. Through human annotations and a user study, we\nfurther find that these learned explanations more closely align with how humans\nwould explain the required decisions in these tasks. Our code is available at\nhttps://github.com/coderpat/learning-scaffold\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1\">Patrick Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1\">Marcos Treviso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1\">Danish Pruthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Video Object Segmentation via Cutout Prediction and Tagging. (arXiv:2204.10846v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10846","description":"<p>We propose a novel self-supervised Video Object Segmentation (VOS) approach\nthat strives to achieve better object-background discriminability for accurate\nobject segmentation. Distinct from previous self-supervised VOS methods, our\napproach is based on a discriminative learning loss formulation that takes into\naccount both object and background information to ensure object-background\ndiscriminability, rather than using only object appearance. The discriminative\nlearning loss comprises cutout-based reconstruction (cutout region represents\npart of a frame, whose pixels are replaced with some constant values) and tag\nprediction loss terms. The cutout-based reconstruction term utilizes a simple\ncutout scheme to learn the pixel-wise correspondence between the current and\nprevious frames in order to reconstruct the original current frame with added\ncutout region in it. The introduced cutout patch guides the model to focus as\nmuch on the significant features of the object of interest as the less\nsignificant ones, thereby implicitly equipping the model to address\nocclusion-based scenarios. Next, the tag prediction term encourages\nobject-background separability by grouping tags of all pixels in the cutout\nregion that are similar, while separating them from the tags of the rest of the\nreconstructed frame pixels. Additionally, we introduce a zoom-in scheme that\naddresses the problem of small object segmentation by capturing fine structural\ninformation at multiple scales. Our proposed approach, termed CT-VOS, achieves\nstate-of-the-art results on two challenging benchmarks: DAVIS-2017 and\nYoutube-VOS. A detailed ablation showcases the importance of the proposed loss\nformulation to effectively capture object-background discriminability and the\nimpact of our zoom-in scheme to accurately segment small-sized objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kini_J/0/1/0/all/0/1\">Jyoti Kini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Control-NeRF: Editable Feature Volumes for Scene Rendering and Manipulation. (arXiv:2204.10850v1 [cs.CV])","link":"http://arxiv.org/abs/2204.10850","description":"<p>We present a novel method for performing flexible, 3D-aware image content\nmanipulation while enabling high-quality novel view synthesis. While NeRF-based\napproaches are effective for novel view synthesis, such models memorize the\nradiance for every point in a scene within a neural network. Since these models\nare scene-specific and lack a 3D scene representation, classical editing such\nas shape manipulation, or combining scenes is not possible. Hence, editing and\ncombining NeRF-based scenes has not been demonstrated. With the aim of\nobtaining interpretable and controllable scene representations, our model\ncouples learnt scene-specific feature volumes with a scene agnostic neural\nrendering network. With this hybrid representation, we decouple neural\nrendering from scene-specific geometry and appearance. We can generalize to\nnovel scenes by optimizing only the scene-specific 3D feature representation,\nwhile keeping the parameters of the rendering network fixed. The rendering\nfunction learnt during the initial training stage can thus be easily applied to\nnew scenes, making our approach more flexible. More importantly, since the\nfeature volumes are independent of the rendering model, we can manipulate and\ncombine scenes by editing their corresponding feature volumes. The edited\nvolume can then be plugged into the rendering model to synthesize high-quality\nnovel views. We demonstrate various scene manipulations, including mixing\nscenes, deforming objects and inserting objects into scenes, while still\nproducing photo-realistic results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazova_V/0/1/0/all/0/1\">Verica Lazova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzov_V/0/1/0/all/0/1\">Vladimir Guzov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olszewski_K/0/1/0/all/0/1\">Kyle Olszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Dual Formulation of Boosting Algorithms. (arXiv:0901.3590v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/0901.3590","description":"<p>We study boosting algorithms from a new perspective. We show that the\nLagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with\ngeneralized hinge loss are all entropy maximization problems. By looking at the\ndual problems of these boosting algorithms, we show that the success of\nboosting algorithms can be understood in terms of maintaining a better margin\ndistribution by maximizing margins and at the same time controlling the margin\nvariance.We also theoretically prove that, approximately, AdaBoost maximizes\nthe average margin, instead of the minimum margin. The duality formulation also\nenables us to develop column generation based optimization algorithms, which\nare totally corrective. We show that they exhibit almost identical\nclassification results to that of standard stage-wise additive boosting\nalgorithms but with much faster convergence rates. Therefore fewer weak\nclassifiers are needed to build the ensemble using our proposed optimization\ntechnique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanxi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Land-Cover Classification with High-Resolution Remote Sensing Images Using Transferable Deep Models. (arXiv:1807.05713v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1807.05713","description":"<p>In recent years, large amount of high spatial-resolution remote sensing\n(HRRS) images are available for land-cover mapping. However, due to the complex\ninformation brought by the increased spatial resolution and the data\ndisturbances caused by different conditions of image acquisition, it is often\ndifficult to find an efficient method for achieving accurate land-cover\nclassification with high-resolution and heterogeneous remote sensing images. In\nthis paper, we propose a scheme to apply deep model obtained from labeled\nland-cover dataset to classify unlabeled HRRS images. The main idea is to rely\non deep neural networks for presenting the contextual information contained in\ndifferent types of land-covers and propose a pseudo-labeling and sample\nselection scheme for improving the transferability of deep models. More\nprecisely, a deep Convolutional Neural Networks is first pre-trained with a\nwell-annotated land-cover dataset, referred to as the source data. Then, given\na target image with no labels, the pre-trained CNN model is utilized to\nclassify the image in a patch-wise manner. The patches with high confidence are\nassigned with pseudo-labels and employed as the queries to retrieve related\nsamples from the source data. The pseudo-labels confirmed with the retrieved\nresults are regarded as supervised information for fine-tuning the pre-trained\ndeep model. To obtain a pixel-wise land-cover classification with the target\nimage, we rely on the fine-tuned CNN and develop a hybrid classification by\ncombining patch-wise classification and hierarchical segmentation. In addition,\nwe create a large-scale land-cover dataset containing 150 Gaofen-2 satellite\nimages for CNN pre-training. Experiments on multi-source HRRS images show\nencouraging results and demonstrate the applicability of the proposed scheme to\nland-cover classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin-Yi Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qikai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huanfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shengyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1\">Shucheng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Detectors for Digital and Physical Adversarial Inputs to Perception Systems. (arXiv:2002.09792v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.09792","description":"<p>Deep neural network (DNN) models have proven to be vulnerable to adversarial\ndigital and physical attacks. In this paper, we propose a novel attack- and\ndataset-agnostic and real-time detector for both types of adversarial inputs to\nDNN-based perception systems. In particular, the proposed detector relies on\nthe observation that adversarial images are sensitive to certain\nlabel-invariant transformations. Specifically, to determine if an image has\nbeen adversarially manipulated, the proposed detector checks if the output of\nthe target classifier on a given input image changes significantly after\nfeeding it a transformed version of the image under investigation. Moreover, we\nshow that the proposed detector is computationally-light both at runtime and\ndesign-time which makes it suitable for real-time applications that may also\ninvolve large-scale image domains. To highlight this, we demonstrate the\nefficiency of the proposed detector on ImageNet, a task that is computationally\nchallenging for the majority of relevant defenses, and on physically attacked\ntraffic signs that may be encountered in real-time autonomy applications.\nFinally, we propose the first adversarial dataset, called AdvNet that includes\nboth clean and physical traffic sign images. Our extensive comparative\nexperiments on the MNIST, CIFAR10, ImageNet, and AdvNet datasets show that\nVisionGuard outperforms existing defenses in terms of scalability and detection\nperformance. We have also evaluated the proposed detector on field test data\nobtained on a moving vehicle equipped with a perception-based DNN being under\nattack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kantaros_Y/0/1/0/all/0/1\">Yiannis Kantaros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpenter_T/0/1/0/all/0/1\">Taylor Carpenter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1\">Kaustubh Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yahan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Insup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weimer_J/0/1/0/all/0/1\">James Weimer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LandCover.ai: Dataset for Automatic Mapping of Buildings, Woodlands, Water and Roads from Aerial Imagery. (arXiv:2005.02264v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.02264","description":"<p>Monitoring of land cover and land use is crucial in natural resources\nmanagement. Automatic visual mapping can carry enormous economic value for\nagriculture, forestry, or public administration. Satellite or aerial images\ncombined with computer vision and deep learning enable precise assessment and\ncan significantly speed up change detection. Aerial imagery usually provides\nimages with much higher pixel resolution than satellite data allowing more\ndetailed mapping. However, there is still a lack of aerial datasets made for\nthe segmentation, covering rural areas with a resolution of tens centimeters\nper pixel, manual fine labels, and highly publicly important environmental\ninstances like buildings, woods, water, or roads.\n</p>\n<p>Here we introduce LandCover.ai (Land Cover from Aerial Imagery) dataset for\nsemantic segmentation. We collected images of 216.27 sq. km rural areas across\nPoland, a country in Central Europe, 39.51 sq. km with resolution 50 cm per\npixel and 176.76 sq. km with resolution 25 cm per pixel and manually fine\nannotated four following classes of objects: buildings, woodlands, water, and\nroads. Additionally, we report simple benchmark results, achieving 85.56% of\nmean intersection over union on the test set. It proves that the automatic\nmapping of land cover is possible with a relatively small, cost-efficient,\nRGB-only dataset. The dataset is publicly available at\nhttps://landcover.ai.linuxpolska.com/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boguszewski_A/0/1/0/all/0/1\">Adrian Boguszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batorski_D/0/1/0/all/0/1\">Dominik Batorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziemba_Jankowska_N/0/1/0/all/0/1\">Natalia Ziemba-Jankowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dziedzic_T/0/1/0/all/0/1\">Tomasz Dziedzic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zambrzycka_A/0/1/0/all/0/1\">Anna Zambrzycka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency. (arXiv:2106.14336v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14336","description":"<p>Many deep learning based methods are designed to remove non-uniform\n(spatially variant) motion blur caused by object motion and camera shake\nwithout knowing the blur kernel. Some methods directly output the latent sharp\nimage in one stage, while others utilize a multi-stage strategy (\\eg\nmulti-scale, multi-patch, or multi-temporal) to gradually restore the sharp\nimage. However, these methods have the following two main issues: 1) The\ncomputational cost of multi-stage is high; 2) The same convolution kernel is\napplied in different regions, which is not an ideal choice for non-uniform\nblur. Hence, non-uniform motion deblurring is still a challenging and open\nproblem. In this paper, we propose a new architecture which consists of\nmultiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to\ndeblur an image end-to-end with more flexibility. Multiple ASPDC modules\nimplicitly learn the pixel-specific motion with different dilation rates in the\nsame layer to handle movements of different magnitude. To improve the training,\nwe also propose a reblurring network to map the deblurred output back to the\nblurred input, which constrains the solution space. Our experimental results\nshow that the proposed method outperforms state-of-the-art methods on the\nbenchmark datasets. The code is available at https://github.com/Dong-Huo/ASPDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoumzadeh_A/0/1/0/all/0/1\">Abbas Masoumzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InsPose: Instance-Aware Networks for Single-Stage Multi-Person Pose Estimation. (arXiv:2107.08982v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08982","description":"<p>Multi-person pose estimation is an attractive and challenging task. Existing\nmethods are mostly based on two-stage frameworks, which include top-down and\nbottom-up methods. Two-stage methods either suffer from high computational\nredundancy for additional person detectors or they need to group keypoints\nheuristically after predicting all the instance-agnostic keypoints. The\nsingle-stage paradigm aims to simplify the multi-person pose estimation\npipeline and receives a lot of attention. However, recent single-stage methods\nhave the limitation of low performance due to the difficulty of regressing\nvarious full-body poses from a single feature vector. Different from previous\nsolutions that involve complex heuristic designs, we present a simple yet\neffective solution by employing instance-aware dynamic networks. Specifically,\nwe propose an instance-aware module to adaptively adjust (part of) the network\nparameters for each instance. Our solution can significantly increase the\ncapacity and adaptive-ability of the network for recognizing various poses,\nwhile maintaining a compact end-to-end trainable pipeline. Extensive\nexperiments on the MS-COCO dataset demonstrate that our method achieves\nsignificant improvement over existing single-stage methods, and makes a better\nbalance of accuracy and efficiency compared to the state-of-the-art two-stage\napproaches. The code and models are available at\n\\url{https://github.com/hikvision-research/opera}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dahu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Ye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer for Single Image Super-Resolution. (arXiv:2108.11084v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11084","description":"<p>Single image super-resolution (SISR) has witnessed great strides with the\ndevelopment of deep learning. However, most existing studies focus on building\nmore complex networks with a massive number of layers. Recently, more and more\nresearchers start to explore the application of Transformer in computer vision\ntasks. However, the heavy computational cost and high GPU memory occupation of\nthe vision Transformer cannot be ignored. In this paper, we propose a novel\nEfficient Super-Resolution Transformer (ESRT) for SISR. ESRT is a hybrid model,\nwhich consists of a Lightweight CNN Backbone (LCB) and a Lightweight\nTransformer Backbone (LTB). Among them, LCB can dynamically adjust the size of\nthe feature map to extract deep features with a low computational cost. LTB is\ncomposed of a series of Efficient Transformers (ET), which occupies a small GPU\nmemory occupation, thanks to the specially designed Efficient Multi-Head\nAttention (EMHA). Extensive experiments show that ESRT achieves competitive\nresults with low computational costs. Compared with the original Transformer\nwhich occupies 16,057M GPU memory, ESRT only occupies 4,191M GPU memory. All\ncodes are available at https://github.com/luissen/ESRT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhisheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tieyong Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator. (arXiv:2109.05820v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05820","description":"<p>As designers of artificial intelligence try to outwit hackers, both sides\ncontinue to hone in on AI's inherent vulnerabilities. Designed and trained from\ncertain statistical distributions of data, AI's deep neural networks (DNNs)\nremain vulnerable to deceptive inputs that violate a DNN's statistical,\npredictive assumptions. Before being fed into a neural network, however, most\nexisting adversarial examples cannot maintain malicious functionality when\napplied to an affine transformation. For practical purposes, maintaining that\nmalicious functionality serves as an important measure of the robustness of\nadversarial attacks. To help DNNs learn to defend themselves more thoroughly\nagainst attacks, we propose an affine-invariant adversarial attack, which can\nconsistently produce more robust adversarial examples over affine\ntransformations. For efficiency, we propose to disentangle current\naffine-transformation strategies from the Euclidean geometry coordinate plane\nwith its geometric translations, rotations and dilations; we reformulate the\nlatter two in polar coordinates. Afterwards, we construct an affine-invariant\ngradient estimator by convolving the gradient at the original image with\nderived kernels, which can be integrated with any gradient-based attack\nmethods. Extensive experiments on ImageNet, including some experiments under\nphysical condition, demonstrate that our method can significantly improve the\naffine invariance of adversarial examples and, as a byproduct, improve the\ntransferability of adversarial examples, compared with alternative\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1\">Wenzhao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shibao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Attribute and Structure Subspace Clustering Network. (arXiv:2109.13742v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.13742","description":"<p>Deep self-expressiveness-based subspace clustering methods have demonstrated\neffectiveness. However, existing works only consider the attribute information\nto conduct the self-expressiveness, which may limit the clustering performance.\nIn this paper, we propose a novel adaptive attribute and structure subspace\nclustering network (AASSC-Net) to simultaneously consider the attribute and\nstructure information in an adaptive graph fusion manner. Specifically, we\nfirst exploit an auto-encoder to represent input data samples with latent\nfeatures for the construction of an attribute matrix. We also construct a mixed\nsigned and symmetric structure matrix to capture the local geometric structure\nunderlying data samples. Then, we perform self-expressiveness on the\nconstructed attribute and structure matrices to learn their affinity graphs\nseparately. Finally, we design a novel attention-based fusion module to\nadaptively leverage these two affinity graphs to construct a more\ndiscriminative affinity graph. Extensive experimental results on commonly used\nbenchmark datasets demonstrate that our AASSC-Net significantly outperforms\nstate-of-the-art methods. In addition, we conduct comprehensive ablation\nstudies to discuss the effectiveness of the designed modules. The code will be\npublicly available at https://github.com/ZhihaoPENG-CityU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhihao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Proxy Anchor Loss and Effectiveness of Deep Metric Learning Performance Metrics. (arXiv:2110.03997v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03997","description":"<p>Deep metric learning (DML) learns the mapping, which maps into embedding\nspace in which similar data is near and dissimilar data is far. However,\nconventional proxy-based losses for DML have two problems: gradient problems\nand applying the real-world dataset with multiple local centers. Besides, DML\nperformance metrics also have some issues have stability and flexibility. This\npaper proposes multi-proxies anchor (MPA) loss and normalized discounted\ncumulative gain (nDCG@k) metric. This study contributes three following: (1)\nMPA loss is able to learn the real-world dataset with multi-local centers. (2)\nMPA loss improves the training capacity of a neural network owing to solving\nthe gradient issues. (3) nDCG@k metric encourages complete evaluation for\nvarious datasets. Finally, we demonstrate MPA loss's effectiveness, and MPA\nloss achieves higher accuracy on two datasets for fine-grained images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeki_S/0/1/0/all/0/1\">Shozo Saeki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_M/0/1/0/all/0/1\">Minoru Kawahara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aman_H/0/1/0/all/0/1\">Hirohisa Aman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset Condensation with Distribution Matching. (arXiv:2110.04181v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.04181","description":"<p>Computational cost of training state-of-the-art deep models in many learning\nproblems is rapidly increasing due to more sophisticated models and larger\ndatasets. A recent promising direction for reducing training cost is dataset\ncondensation that aims to replace the original large training set with a\nsignificantly smaller learned synthetic set while preserving the original\ninformation. While training deep models on the small set of condensed images\ncan be extremely fast, their synthesis remains computationally expensive due to\nthe complex bi-level optimization and second-order derivative computation. In\nthis work, we propose a simple yet effective method that synthesizes condensed\nimages by matching feature distributions of the synthetic and original training\nimages in many sampled embedding spaces. Our method significantly reduces the\nsynthesis cost while achieving comparable or better performance. Thanks to its\nefficiency, we apply our method to more realistic and larger datasets with\nsophisticated neural architectures and obtain a significant performance boost.\nWe also show promising practical benefits of our method in continual learning\nand neural architecture search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auxiliary Loss Reweighting for Image Inpainting. (arXiv:2111.07279v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.07279","description":"<p>Image Inpainting is a task that aims to fill in missing regions of corrupted\nimages with plausible contents. Recent inpainting methods have introduced\nperceptual and style losses as auxiliary losses to guide the learning of\ninpainting generators. Perceptual and style losses help improve the perceptual\nquality of inpainted results by supervising deep features of generated regions.\nHowever, two challenges have emerged with the usage of auxiliary losses: (i)\nthe time-consuming grid search is required to decide weights for perceptual and\nstyle losses to properly perform, and (ii) loss terms with different auxiliary\nabilities are equally weighted by perceptual and style losses. To meet these\ntwo challenges, we propose a novel framework that independently weights\nauxiliary loss terms and adaptively adjusts their weights within a single\ntraining process, without a time-consuming grid search. Specifically, to\nrelease the auxiliary potential of perceptual and style losses, we propose two\nauxiliary losses, Tunable Perceptual Loss (TPL) and Tunable Style Loss (TSL) by\nusing different tunable weights to consider the contributions of different loss\nterms. TPL and TSL are supersets of perceptual and style losses and release the\nauxiliary potential of standard perceptual and style losses. We further propose\nthe Auxiliary Weights Adaptation (AWA) algorithm, which efficiently reweights\nTPL and TSL in a single training process. AWA is based on the principle that\nthe best auxiliary weights would lead to the most improvement in inpainting\nperformance. We conduct experiments on publically available datasets and find\nthat our framework helps current SOTA methods achieve better results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hui_S/0/1/0/all/0/1\">Siqi Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Sanping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Ye Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinjun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViCE: Visual Concept Embedding Discovery and Superpixelization. (arXiv:2111.12460v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12460","description":"<p>Recent self-supervised computer vision methods have demonstrated equal or\nbetter performance to supervised methods, opening for AI systems to learn\nvisual representations from practically unlimited data. However, these methods\nare classification-based and thus ineffective for learning dense feature maps\nrequired for unsupervised semantic segmentation. This work presents a method to\neffectively learn dense semantically rich visual concept embeddings applicable\nto high-resolution images. We introduce superpixelization as a means to\ndecompose images into a small set of visually coherent regions, allowing\nefficient learning of dense semantics by swapped prediction. The expressiveness\nof our dense embeddings is demonstrated by significantly improving the SOTA\nrepresentation quality benchmarks on COCO (+16.27 mIoU) and Cityscapes (+19.24\nmIoU) for both low- and high-resolution images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karlsson_R/0/1/0/all/0/1\">Robin Karlsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_T/0/1/0/all/0/1\">Tomoki Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1\">Keisuke Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carballo_A/0/1/0/all/0/1\">Alexander Carballo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohtani_K/0/1/0/all/0/1\">Kento Ohtani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Kazuya Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Camera LiDAR Inertial Extension to the Newer College Dataset. (arXiv:2112.08854v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.08854","description":"<p>We present a multi-camera LiDAR inertial dataset of 4.5 km walking distance\nas an expansion of the Newer College Dataset. The global shutter multi-camera\ndevice is hardware synchronized with both the IMU and LiDAR. This dataset also\nprovides six Degrees of Freedom (DoF) ground truth poses at LiDAR frequency (10\nHz). Three data collections are described and an example use case of\nmulti-camera visual-inertial odometry is demonstrated. This expansion dataset\ncontains small and narrow passages, large scale open spaces, as well as\nvegetated areas, to test localization and mapping systems. Furthermore, some\nsequences present challenging situations such as abrupt lighting change,\ntextureless surfaces, and aggressive motion. The dataset is available at:\nhttps: //ori-drs.github.io/newer-college-dataset/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lintong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camurri_M/0/1/0/all/0/1\">Marco Camurri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wisth_D/0/1/0/all/0/1\">David Wisth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallon_M/0/1/0/all/0/1\">Maurice Fallon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MobilePhys: Personalized Mobile Camera-Based Contactless Physiological Sensing. (arXiv:2201.04039v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04039","description":"<p>Camera-based contactless photoplethysmography refers to a set of popular\ntechniques for contactless physiological measurement. The current\nstate-of-the-art neural models are typically trained in a supervised manner\nusing videos accompanied by gold standard physiological measurements. However,\nthey often generalize poorly out-of-domain examples (i.e., videos that are\nunlike those in the training set). Personalizing models can help improve model\ngeneralizability, but many personalization techniques still require some gold\nstandard data. To help alleviate this dependency, in this paper, we present a\nnovel mobile sensing system called MobilePhys, the first mobile personalized\nremote physiological sensing system, that leverages both front and rear cameras\non a smartphone to generate high-quality self-supervised labels for training\npersonalized contactless camera-based PPG models. To evaluate the robustness of\nMobilePhys, we conducted a user study with 39 participants who completed a set\nof tasks under different mobile devices, lighting conditions/intensities,\nmotion tasks, and skin types. Our results show that MobilePhys significantly\noutperforms the state-of-the-art on-device supervised training and few-shot\nadaptation methods. Through extensive user studies, we further examine how does\nMobilePhys perform in complex real-world settings. We envision that calibrated\nor personalized camera-based contactless PPG models generated from our proposed\ndual-camera mobile sensing system will open the door for numerous future\napplications such as smart mirrors, fitness and mobile health applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuntao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sinan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zixian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shwetak Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards 3D Scene Reconstruction from Locally Scale-Aligned Monocular Video Depth. (arXiv:2202.01470v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.01470","description":"<p>Existing monocular depth estimation methods have achieved excellent\nrobustness in diverse scenes, but they can only retrieve affine-invariant\ndepth, up to an unknown scale and shift. However, in some video-based scenarios\nsuch as video depth estimation and 3D scene reconstruction from a video, the\nunknown scale and shift residing in per-frame prediction may cause the depth\ninconsistency. To solve this problem, we propose a locally weighted linear\nregression method to recover the scale and shift with very sparse anchor\npoints, which ensures the scale consistency along consecutive frames. Extensive\nexperiments show that our method can boost the performance of existing\nstate-of-the-art approaches by 50% at most over several zero-shot benchmarks.\nBesides, we merge over 6.3 million RGBD images to train strong and robust depth\nmodels. Our produced ResNet50-backbone model even outperforms the\nstate-of-the-art DPT ViT-Large model. Combining with geometry-based\nreconstruction methods, we formulate a new dense 3D scene reconstruction\npipeline, which benefits from both the scale consistency of sparse points and\nthe robustness of monocular methods. By performing the simple per-frame\nprediction over a video, the accurate 3D scene shape can be recovered.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kai Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Feng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PSSNet: Planarity-sensible Semantic Segmentation of Large-scale Urban Meshes. (arXiv:2202.03209v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03209","description":"<p>We introduce a novel deep learning-based framework to interpret 3D urban\nscenes represented as textured meshes. Based on the observation that object\nboundaries typically align with the boundaries of planar regions, our framework\nachieves semantic segmentation in two steps: planarity-sensible\nover-segmentation followed by semantic classification. The over-segmentation\nstep generates an initial set of mesh segments that capture the planar and\nnon-planar regions of urban scenes. In the subsequent classification step, we\nconstruct a graph that encodes the geometric and photometric features of the\nsegments in its nodes and the multi-scale contextual features in its edges. The\nfinal semantic segmentation is obtained by classifying the segments using a\ngraph convolutional network. Experiments and comparisons on two semantic urban\nmesh benchmarks demonstrate that our approach outperforms the state-of-the-art\nmethods in terms of boundary quality, mean IoU (intersection over union), and\ngeneralization ability. We also introduce several new metrics for evaluating\nmesh over-segmentation methods dedicated to semantic segmentation, and our\nproposed over-segmentation approach outperforms state-of-the-art methods on all\nmetrics. Our source code is available at\n\\url{https://github.com/WeixiaoGao/PSSNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Weixiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boom_B/0/1/0/all/0/1\">Bas Boom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Modeling for Out-of-Distribution Generalization. (arXiv:2202.03958v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03958","description":"<p>Though remarkable progress has been achieved in various vision tasks, deep\nneural networks still suffer obvious performance degradation when tested in\nout-of-distribution scenarios. We argue that the feature statistics (mean and\nstandard deviation), which carry the domain characteristics of the training\ndata, can be properly manipulated to improve the generalization ability of deep\nlearning models. Common methods often consider the feature statistics as\ndeterministic values measured from the learned features and do not explicitly\nconsider the uncertain statistics discrepancy caused by potential domain shifts\nduring testing. In this paper, we improve the network generalization ability by\nmodeling the uncertainty of domain shifts with synthesized feature statistics\nduring training. Specifically, we hypothesize that the feature statistic, after\nconsidering the potential uncertainties, follows a multivariate Gaussian\ndistribution. Hence, each feature statistic is no longer a deterministic value,\nbut a probabilistic point with diverse distribution possibilities. With the\nuncertain feature statistics, the models can be trained to alleviate the domain\nperturbations and achieve better robustness against potential domain shifts.\nOur method can be readily integrated into networks without additional\nparameters. Extensive experiments demonstrate that our proposed method\nconsistently improves the network generalization ability on multiple vision\ntasks, including image classification, semantic segmentation, and instance\nretrieval. The code can be available at https://github.com/lixiaotong97/DSU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yongxing Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-Means for Noise-Insensitive Multi-Dimensional Feature Learning. (arXiv:2202.07754v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07754","description":"<p>Many measurement modalities which perform imaging by probing an object\npixel-by-pixel, such as via Photoacoustic Microscopy, produce a\nmulti-dimensional feature (typically a time-domain signal) at each pixel. In\nprinciple, the many degrees of freedom in the time-domain signal would admit\nthe possibility of significant multi-modal information being implicitly\npresent, much more than a single scalar \"brightness\", regarding the underlying\ntargets being observed. However, the measured signal is neither a weighted-sum\nof basis functions (such as principal components) nor one of a set of\nprototypes (K-means), which has motivated the novel clustering method proposed\nhere. Signals are clustered based on their shape, but not amplitude, via\nangular distance and centroids are calculated as the direction of maximal\nintra-cluster variance, resulting in a clustering algorithm capable of learning\ncentroids (signal shapes) that are related to the underlying, albeit unknown,\ntarget characteristics in a scalable and noise-robust manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pellegrino_N/0/1/0/all/0/1\">Nicholas Pellegrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fieguth_P/0/1/0/all/0/1\">Paul Fieguth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reza_P/0/1/0/all/0/1\">Parsin Haji Reza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSDN: Mutually Semantic Distillation Network for Zero-Shot Learning. (arXiv:2203.03137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03137","description":"<p>The key challenge of zero-shot learning (ZSL) is how to infer the latent\nsemantic knowledge between visual and attribute features on seen classes, and\nthus achieving a desirable knowledge transfer to unseen classes. Prior works\neither simply align the global features of an image with its associated class\nsemantic vector or utilize unidirectional attention to learn the limited latent\nsemantic representations, which could not effectively discover the intrinsic\nsemantic knowledge e.g., attribute semantics) between visual and attribute\nfeatures. To solve the above dilemma, we propose a Mutually Semantic\nDistillation Network (MSDN), which progressively distills the intrinsic\nsemantic representations between visual and attribute features for ZSL. MSDN\nincorporates an attribute$\\rightarrow$visual attention sub-net that learns\nattribute-based visual features, and a visual$\\rightarrow$attribute attention\nsub-net that learns visual-based attribute features. By further introducing a\nsemantic distillation loss, the two mutual attention sub-nets are capable of\nlearning collaboratively and teaching each other throughout the training\nprocess. The proposed MSDN yields significant improvements over the strong\nbaselines, leading to new state-of-the-art performances on three popular\nchallenging benchmarks, i.e., CUB, SUN, and AWA2. Our codes have been available\nat: \\url{https://github.com/shiming-chen/MSDN}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Ziming Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Polyp Segmentation Network. (arXiv:2203.04118v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.04118","description":"<p>Cancer is a disease that occurs as a result of the uncontrolled division and\nproliferation of cells. Colon cancer is one of the most common types of cancer\nin the world. Polyps that can be seen in the large intestine can cause cancer\nif not removed with early intervention. Deep learning and image segmentation\ntechniques are used to minimize the number of polyps that goes unnoticed by the\nexperts during these interventions. Although these techniques perform well in\nterms of accuracy, they require too many parameters. We propose a new model to\naddress this problem. Our proposed model requires fewer parameters as well as\noutperforms the state-of-the-art models. We use EfficientNetB0 for the encoder\npart, as it performs well in various tasks while requiring fewer parameters. We\nuse partial decoder, which is used to reduce the number of parameters while\nachieving high accuracy in segmentation. Since polyps have variable appearances\nand sizes, we use an asymmetric convolution block instead of a classic\nconvolution block. Then, we weight each feature map using a squeeze and\nexcitation block to improve our segmentation results. We used different splits\nof Kvasir and CVC-ClinicDB datasets for training, validation, and testing,\nwhile we use CVC- ColonDB, ETIS, and Endoscene datasets for testing. Our model\noutperforms state-of-art models with a Dice metric of %71.8 on the ColonDB test\ndataset, %89.3 on the EndoScene test dataset, and %74.8 on the ETIS test\ndataset while requiring fewer parameters. Our model requires 2.626.337\nparameters in total while the closest model in the state-of-the-art is U-Net++\nwith 9.042.177 parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Erol_T/0/1/0/all/0/1\">Tugberk Erol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarikaya_D/0/1/0/all/0/1\">Duygu Sarikaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Black-box Skeleton-based Human Activity Classifiers. (arXiv:2203.04713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04713","description":"<p>Deep learning has been regarded as the `go to' solution for many tasks today,\nbut its intrinsic vulnerability to malicious attacks has become a major\nconcern. The vulnerability is affected by a variety of factors including\nmodels, tasks, data, and attackers. Consequently, methods such as Adversarial\nTraining and Randomized Smoothing have been proposed to tackle the problem in a\nwide range of applications. In this paper, we investigate skeleton-based Human\nActivity Recognition, which is an important type of time-series data but\nunder-explored in defense against attacks. Our method is featured by (1) a new\nBayesian Energy-based formulation of robust discriminative classifiers, (2) a\nnew parameterization of the adversarial sample manifold of actions, and (3) a\nnew post-train Bayesian treatment on both the adversarial samples and the\nclassifier. We name our framework Bayesian Energy-based Adversarial Training or\nBEAT. BEAT is straightforward but elegant, which turns vulnerable black-box\nclassifiers into robust ones without sacrificing accuracy. It demonstrates\nsurprising and universal effectiveness across a wide range of action\nclassifiers and datasets, under various attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1\">Yunfeng Diao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HunYuan_tvr for Text-Video Retrieval. (arXiv:2204.03382v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03382","description":"<p>Text-Video Retrieval plays an important role in multi-modal understanding and\nhas attracted increasing attention in recent years. Most existing methods focus\non constructing contrastive pairs between whole videos and complete caption\nsentences, while ignoring fine-grained cross-modal relationships, e.g., short\nclips and phrases or single frame and word. In this paper, we propose a novel\nmethod, named HunYuan\\_tvr, to explore hierarchical cross-modal interactions by\nsimultaneously exploring video-sentence, clip-phrase, and frame-word\nrelationships. Considering intrinsic semantic relations between frames,\nHunYuan\\_tvr first performs self-attention to explore frame-wise correlations\nand adaptively clusters correlated frames into clip-level representations.\nThen, the clip-wise correlation is explored to aggregate clip representations\ninto a compact one to describe the video globally. In this way, we can\nconstruct hierarchical video representations for frame-clip-video\ngranularities, and also explore word-wise correlations to form\nword-phrase-sentence embeddings for the text modality. Finally, hierarchical\ncontrastive learning is designed to explore cross-modal\nrelationships,~\\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which\nenables HunYuan\\_tvr to achieve a comprehensive multi-modal understanding.\nFurther boosted by adaptive label denosing and marginal sample enhancement,\nHunYuan\\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,\nRank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,\nDiDemo, and ActivityNet respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Shaobo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rong-Cheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. As the core of CNNs, the image\nconvolution operation helps CNNs to get good performance on image-related\ntasks. However, the image convolution is hard to be implemented and\nparallelized. This paper proposes a novel neural network model, namely CEMNet,\nwhich can be trained in the frequency domain. The most important motivation of\nthis research is that we can use the straightforward element-wise\nmultiplication operation to replace the image convolution in the frequency\ndomain based on the Cross-Correlation Theorem. We further introduce a Weight\nFixation mechanism to alleviate the problem of over-fitting, and analyze the\nworking behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases. To the best of our knowledge,\nCEMNet is the first model trained in Fourier Domain that achieves more than\n70\\% validation accuracy on CIFAR-10 database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-Inertial Odometry with Online Calibration of Velocity-Control Based Kinematic Motion Models. (arXiv:2204.06776v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06776","description":"<p>Visual-inertial odometry (VIO) is an important technology for autonomous\nrobots with power and payload constraints. In this paper, we propose a novel\napproach for VIO with stereo cameras which integrates and calibrates the\nvelocity-control based kinematic motion model of wheeled mobile robots online.\nIncluding such a motion model can help to improve the accuracy of VIO. Compared\nto several previous approaches proposed to integrate wheel odometer\nmeasurements for this purpose, our method does not require wheel encoders and\ncan be applied when the robot motion can be modeled with velocity-control based\nkinematic motion model. We use radial basis function (RBF) kernels to\ncompensate for the time delay and deviations between control commands and\nactual robot motion. The motion model is calibrated online by the VIO system\nand can be used as a forward model for motion control and planning. We evaluate\nour approach with data obtained in variously sized indoor environments,\ndemonstrate improvements over a pure VIO method, and evaluate the prediction\naccuracy of the online calibrated model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haolong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1\">Joerg Stueckler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep CardioSound-An Ensembled Deep Learning Model for Heart Sound MultiLabelling. (arXiv:2204.07420v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.07420","description":"<p>Heart sound diagnosis and classification play an essential role in detecting\ncardiovascular disorders, especially when the remote diagnosis becomes standard\nclinical practice. Most of the current work is designed for single category\nbased heard sound classification tasks. To further extend the landscape of the\nautomatic heart sound diagnosis landscape, this work proposes a deep multilabel\nlearning model that can automatically annotate heart sound recordings with\nlabels from different label groups, including murmur's timing, pitch, grading,\nquality, and shape. Our experiment results show that the proposed method has\nachieved outstanding performance on the holdout data for the multi-labelling\ntask with sensitivity=0.990, specificity=0.999, F1=0.990 at the segments level,\nand an overall accuracy=0.969 at the patient's recording level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Li Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davenport_S/0/1/0/all/0/1\">Steven Davenport</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yonghong Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Linear Attention for Fast and Accurate Keypoint Matching. (arXiv:2204.07731v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07731","description":"<p>Recently Transformers have provided state-of-the-art performance in sparse\nmatching, crucial to realize high-performance 3D vision applications. Yet,\nthese Transformers lack efficiency due to the quadratic computational\ncomplexity of their attention mechanism. To solve this problem, we employ an\nefficient linear attention for the linear computational complexity. Then, we\npropose a new attentional aggregation that achieves high accuracy by\naggregating both the global and local information from sparse keypoints. To\nfurther improve the efficiency, we propose the joint learning of feature\nmatching and description. Our learning enables simpler and faster matching than\nSinkhorn, often used in matching the learned descriptors from Transformers. Our\nmethod achieves competitive performance with only 0.84M learnable parameters\nagainst the bigger SOTAs, SuperGlue (12M parameters) and SGMNet (30M\nparameters), on three benchmarks, HPatch, ETH, and Aachen Day-Night.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suwanwimolkul_S/0/1/0/all/0/1\">Suwichaya Suwanwimolkul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1\">Satoshi Komorita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BSRT: Improving Burst Super-Resolution with Swin Transformer and Flow-Guided Deformable Alignment. (arXiv:2204.08332v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08332","description":"<p>This work addresses the Burst Super-Resolution (BurstSR) task using a new\narchitecture, which requires restoring a high-quality image from a sequence of\nnoisy, misaligned, and low-resolution RAW bursts. To overcome the challenges in\nBurstSR, we propose a Burst Super-Resolution Transformer (BSRT), which can\nsignificantly improve the capability of extracting inter-frame information and\nreconstruction. To achieve this goal, we propose a Pyramid Flow-Guided\nDeformable Convolution Network (Pyramid FG-DCN) and incorporate Swin\nTransformer Blocks and Groups as our main backbone. More specifically, we\ncombine optical flows and deformable convolutions, hence our BSRT can handle\nmisalignment and aggregate the potential texture information in multi-frames\nmore efficiently. In addition, our Transformer-based structure can capture\nlong-range dependency to further improve the performance. The evaluation on\nboth synthetic and real-world tracks demonstrates that our approach achieves a\nnew state-of-the-art in BurstSR task. Further, our BSRT wins the championship\nin the NTIRE2022 Burst Super-Resolution Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziwei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Youwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shen Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zhihong Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interventional Multi-Instance Learning with Deconfounded Instance-Level Prediction. (arXiv:2204.09204v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09204","description":"<p>When applying multi-instance learning (MIL) to make predictions for bags of\ninstances, the prediction accuracy of an instance often depends on not only the\ninstance itself but also its context in the corresponding bag. From the\nviewpoint of causal inference, such bag contextual prior works as a confounder\nand may result in model robustness and interpretability issues. Focusing on\nthis problem, we propose a novel interventional multi-instance learning (IMIL)\nframework to achieve deconfounded instance-level prediction. Unlike traditional\nlikelihood-based strategies, we design an Expectation-Maximization (EM)\nalgorithm based on causal intervention, providing a robust instance selection\nin the training phase and suppressing the bias caused by the bag contextual\nprior. Experiments on pathological image analysis demonstrate that our IMIL\nmethod substantially reduces false positives and outperforms state-of-the-art\nMIL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tiancheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongteng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Canqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving The Long-Tailed Problem via Intra- and Inter-Category Balance. (arXiv:2204.09234v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09234","description":"<p>Benchmark datasets for visual recognition assume that data is uniformly\ndistributed, while real-world datasets obey long-tailed distribution. Current\napproaches handle the long-tailed problem to transform the long-tailed dataset\nto uniform distribution by re-sampling or re-weighting strategies. These\napproaches emphasize the tail classes but ignore the hard examples in head\nclasses, which result in performance degradation. In this paper, we propose a\nnovel gradient harmonized mechanism with category-wise adaptive precision to\ndecouple the difficulty and sample size imbalance in the long-tailed problem,\nwhich are correspondingly solved via intra- and inter-category balance\nstrategies. Specifically, intra-category balance focuses on the hard examples\nin each category to optimize the decision boundary, while inter-category\nbalance aims to correct the shift of decision boundary by taking each category\nas a unit. Extensive experiments demonstrate that the proposed method\nconsistently outperforms other approaches on all the datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tiancheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Situational Perception Guided Image Matting. (arXiv:2204.09276v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09276","description":"<p>Most automatic matting methods try to separate the salient foreground from\nthe background. However, the insufficient quantity and subjective bias of the\ncurrent existing matting datasets make it difficult to fully explore the\nsemantic association between object-to-object and object-to-environment in a\ngiven image. In this paper, we propose a Situational Perception Guided Image\nMatting (SPG-IM) method that mitigates subjective bias of matting annotations\nand captures sufficient situational perception information for better global\nsaliency distilled from the visual-to-textual task. SPG-IM can better associate\ninter-objects and object-to-environment saliency, and compensate the subjective\nnature of image matting and its expensive annotation. We also introduce a\ntextual Semantic Transformation (TST) module that can effectively transform and\nintegrate the semantic feature stream to guide the visual representations. In\naddition, an Adaptive Focal Transformation (AFT) Refinement Network is proposed\nto adaptively switch multi-scale receptive fields and focal points to enhance\nboth global and local details. Extensive experiments demonstrate the\neffectiveness of situational perception guidance from the visual-to-textual\ntasks on image matting, and our model outperforms the state-of-the-art methods.\nWe also analyze the significance of different components in our model. The code\nwill be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complete identification of complex salt-geometries from inaccurate migrated images using Deep Learning. (arXiv:2204.09710v2 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2204.09710","description":"<p>Delimiting salt inclusions from migrated images is a time-consuming activity\nthat relies on highly human-curated analysis and is subject to interpretation\nerrors or limitations of the methods available. We propose to use migrated\nimages produced from an inaccurate velocity model (with a reasonable\napproximation of sediment velocity, but without salt inclusions) to predict the\ncorrect salt inclusions shape using a Convolutional Neural Network (CNN). Our\napproach relies on subsurface Common Image Gathers to focus the sediments'\nreflections around the zero offset and to spread the energy of salt reflections\nover large offsets. Using synthetic data, we trained a U-Net to use\ncommon-offset subsurface images as input channels for the CNN and the correct\nsalt-masks as network output. The network learned to predict the salt\ninclusions masks with high accuracy; moreover, it also performed well when\napplied to synthetic benchmark data sets that were not previously introduced.\nOur training process tuned the U-Net to successfully learn the shape of complex\nsalt bodies from partially focused subsurface offset images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Muller_A/0/1/0/all/0/1\">Ana Paula O. Muller</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Costa_J/0/1/0/all/0/1\">Jesse C. Costa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bom_C/0/1/0/all/0/1\">Clecio R. Bom</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Faria_E/0/1/0/all/0/1\">Elisangela L. Faria</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Klatt_M/0/1/0/all/0/1\">Matheus Klatt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Teixeira_G/0/1/0/all/0/1\">Gabriel Teixeira</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Albuquerque_M/0/1/0/all/0/1\">Marcelo P. de Albuquerque</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Albuquerque_M/0/1/0/all/0/1\">Marcio P. de Albuquerque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-24T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}