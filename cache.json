{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-02T01:30:00Z","channels":[{"title":"cs.AI updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.AI","description":"Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Working Memory Connections for LSTM. (arXiv:2109.00020v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00020","description":"<p>Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of\ngating mechanisms to mitigate exploding and vanishing gradients when learning\nlong-term dependencies. For this reason, LSTMs and other gated RNNs are widely\nadopted, being the standard de facto for many sequence modeling tasks. Although\nthe memory cell inside the LSTM contains essential information, it is not\nallowed to influence the gating mechanism directly. In this work, we improve\nthe gate potential by including information coming from the internal cell\nstate. The proposed modification, named Working Memory Connection, consists in\nadding a learnable nonlinear projection of the cell content into the network\ngates. This modification can fit into the classical LSTM gates without any\nassumption on the underlying task, being particularly effective when dealing\nwith longer sequences. Previous research effort in this direction, which goes\nback to the early 2000s, could not bring a consistent improvement over vanilla\nLSTM. As part of this paper, we identify a key issue tied to previous\nconnections that heavily limits their effectiveness, hence preventing a\nsuccessful integration of the knowledge coming from the internal cell state. We\nshow through extensive experimental evaluation that Working Memory Connections\nconstantly improve the performance of LSTMs on a variety of tasks. Numerical\nresults suggest that the cell state contains useful information that is worth\nincluding in the gate structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep DNA Storage: Scalable and Robust DNA Storage via Coding Theory and Deep Learning. (arXiv:2109.00031v1 [cs.IT])","link":"http://arxiv.org/abs/2109.00031","description":"<p>The concept of DNA storage was first suggested in 1959 by Richard Feynman who\nshared his vision regarding nanotechnology in the talk \"There is plenty of room\nat the bottom\". Later, towards the end of the 20-th century, the interest in\nstorage solutions based on DNA molecules was increased as a result of the human\ngenome project which in turn led to a significant progress in sequencing and\nassembly methods. DNA storage enjoys major advantages over the well-established\nmagnetic and optical storage solutions. As opposed to magnetic solutions, DNA\nstorage does not require electrical supply to maintain data integrity and is\nsuperior to other storage solutions in both density and durability. Given the\ntrends in cost decreases of DNA synthesis and sequencing, it is now\nacknowledged that within the next 10-15 years DNA storage may become a highly\ncompetitive archiving technology and probably later the main such technology.\nWith that said, the current implementations of DNA based storage systems are\nvery limited and are not fully optimized to address the unique pattern of\nerrors which characterize the synthesis and sequencing processes. In this work,\nwe propose a robust, efficient and scalable solution to implement DNA-based\nstorage systems. Our method deploys Deep Neural Networks (DNN) which\nreconstruct a sequence of letters based on imperfect cluster of copies\ngenerated by the synthesis and sequencing processes. A tailor-made\nError-Correcting Code (ECC) is utilized to combat patterns of errors which\noccur during this process. Since our reconstruction method is adapted to\nimperfect clusters, our method overcomes the time bottleneck of the noisy DNA\ncopies clustering process by allowing the use of a rapid and scalable\npseudo-clustering instead. Our architecture combines between convolutions and\ntransformers blocks and is trained using synthetic data modelled after real\ndata statistics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bar_Lev_D/0/1/0/all/0/1\">Daniella Bar-Lev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orr_I/0/1/0/all/0/1\">Itai Orr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabary_O/0/1/0/all/0/1\">Omer Sabary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etzion_T/0/1/0/all/0/1\">Tuvi Etzion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaakobi_E/0/1/0/all/0/1\">Eitan Yaakobi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Informing Autonomous Deception Systems with Cyber Expert Performance Data. (arXiv:2109.00066v1 [cs.CR])","link":"http://arxiv.org/abs/2109.00066","description":"<p>The performance of artificial intelligence (AI) algorithms in practice\ndepends on the realism and correctness of the data, models, and feedback\n(labels or rewards) provided to the algorithm. This paper discusses methods for\nimproving the realism and ecological validity of AI used for autonomous cyber\ndefense by exploring the potential to use Inverse Reinforcement Learning (IRL)\nto gain insight into attacker actions, utilities of those actions, and\nultimately decision points which cyber deception could thwart. The Tularosa\nstudy, as one example, provides experimental data of real-world techniques and\ntools commonly used by attackers, from which core data vectors can be leveraged\nto inform an autonomous cyber defense system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Major_M/0/1/0/all/0/1\">Maxine Major</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_B/0/1/0/all/0/1\">Brian Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DiVita_J/0/1/0/all/0/1\">Joseph DiVita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferguson_Walter_K/0/1/0/all/0/1\">Kimberly Ferguson-Walter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proceedings of KDD 2021 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning. (arXiv:2109.00100v1 [cs.CY])","link":"http://arxiv.org/abs/2109.00100","description":"<p>Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 20211 .\nDespite these growing perils, there remains a notable paucity of data science\nresearch to scientifically inform equitable public policy decisions for\nimproving the livelihood of at-risk populations. Scattered data science efforts\nexist to address these challenges, but they remain isolated from practice and\nprone to algorithmic harms concerning lack of privacy, fairness,\ninterpretability, accountability, transparency, and ethics. Biases in\ndata-driven methods carry the risk of amplifying inequalities in high-stakes\npolicy decisions that impact the livelihood of millions of people.\nConsequently, proclaimed benefits of data-driven innovations remain\ninaccessible to policymakers, practitioners, and marginalized communities at\nthe core of humanitarian actions and global development. To help fill this gap,\nwe propose the Data-driven Humanitarian Mapping Research Program, which focuses\non developing novel data science methodologies that harness human-machine\nintelligence for high-stakes public policy and resilience planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snehalkumar/0/1/0/all/0/1\">Snehalkumar</a> (Neil) <a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">S. Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Shankar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lunga_D/0/1/0/all/0/1\">Dalton Lunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondi_E/0/1/0/all/0/1\">Elizabeth Bondi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic non-invasive Cough Detection based on Accelerometer and Audio Signals. (arXiv:2109.00103v1 [cs.SD])","link":"http://arxiv.org/abs/2109.00103","description":"<p>We present an automatic non-invasive way of detecting cough events based on\nboth accelerometer and audio signals.\n</p>\n<p>The acceleration signals are captured by a smartphone firmly attached to the\npatient's bed, using its integrated accelerometer.\n</p>\n<p>The audio signals are captured simultaneously by the same smartphone using an\nexternal microphone.\n</p>\n<p>We have compiled a manually-annotated dataset containing such\nsimultaneously-captured acceleration and audio signals for approximately 6000\ncough and 68000 non-cough events from 14 adult male patients in a tuberculosis\nclinic.\n</p>\n<p>LR, SVM and MLP are evaluated as baseline classifiers and compared with deep\narchitectures such as CNN, LSTM, and Resnet50 using a leave-one-out\ncross-validation scheme.\n</p>\n<p>We find that the studied classifiers can use either acceleration or audio\nsignals to distinguish between coughing and other activities including\nsneezing, throat-clearing, and movement on the bed with high accuracy.\n</p>\n<p>However, in all cases, the deep neural networks outperform the shallow\nclassifiers by a clear margin and the Resnet50 offers the best performance by\nachieving an AUC exceeding 0.98 and 0.99 for acceleration and audio signals\nrespectively.\n</p>\n<p>While audio-based classification consistently offers a better performance\nthan acceleration-based classification, we observe that the difference is very\nsmall for the best systems.\n</p>\n<p>Since the acceleration signal requires less processing power, and since the\nneed to record audio is sidestepped and thus privacy is inherently secured, and\nsince the recording device is attached to the bed and not worn, an\naccelerometer-based highly accurate non-invasive cough detector may represent a\nmore convenient and readily accepted method in long-term cough monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pahar_M/0/1/0/all/0/1\">Madhurananda Pahar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miranda_I/0/1/0/all/0/1\">Igor Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diacon_A/0/1/0/all/0/1\">Andreas Diacon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niesler_T/0/1/0/all/0/1\">Thomas Niesler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics. (arXiv:2109.00110v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00110","description":"<p>We present miniF2F, a dataset of formal Olympiad-level mathematics problems\nstatements intended to provide a unified cross-system benchmark for neural\ntheorem proving. The miniF2F benchmark currently targets Metamath, Lean, and\nIsabelle and consists of 488 problem statements drawn from the AIME, AMC, and\nthe International Mathematical Olympiad (IMO), as well as material from\nhigh-school and undergraduate mathematics courses. We report baseline results\nusing GPT-f, a neural theorem prover based on GPT-3 and provide an analysis of\nits performance. We intend for miniF2F to be a community-driven effort and hope\nthat our benchmark will help spur advances in neural theorem proving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kunhao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jesse Michael Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polu_S/0/1/0/all/0/1\">Stanislas Polu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cognitive science as a source of forward and inverse models of human decisions for robotics and control. (arXiv:2109.00127v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00127","description":"<p>Those designing autonomous systems that interact with humans will invariably\nface questions about how humans think and make decisions. Fortunately,\ncomputational cognitive science offers insight into human decision-making using\ntools that will be familiar to those with backgrounds in optimization and\ncontrol (e.g., probability theory, statistical machine learning, and\nreinforcement learning). Here, we review some of this work, focusing on how\ncognitive science can provide forward models of human decision-making and\ninverse models of how humans think about others' decision-making. We highlight\nrelevant recent developments, including approaches that synthesize blackbox and\ntheory-driven modeling, accounts that recast heuristics and biases as forms of\nbounded optimality, and models that characterize human theory of mind and\ncommunication in decision-theoretic terms. In doing so, we aim to provide\nreaders with a glimpse of the range of frameworks, methodologies, and\nactionable insights that lie at the intersection of cognitive science and\ncontrol research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ho_M/0/1/0/all/0/1\">Mark K. Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Exploration Methods in Reinforcement Learning. (arXiv:2109.00157v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00157","description":"<p>Exploration is an essential component of reinforcement learning algorithms,\nwhere agents need to learn how to predict and control unknown and often\nstochastic environments. Reinforcement learning agents depend crucially on\nexploration to obtain informative data for the learning process as the lack of\nenough information could hinder effective learning. In this article, we provide\na survey of modern exploration methods in (Sequential) reinforcement learning,\nas well as a taxonomy of exploration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amin_S/0/1/0/all/0/1\">Susan Amin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomrokchi_M/0/1/0/all/0/1\">Maziar Gomrokchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satija_H/0/1/0/all/0/1\">Harsh Satija</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoof_H/0/1/0/all/0/1\">Herke van Hoof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1\">Doina Precup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Problem Learning: Towards the Free Will of Machines. (arXiv:2109.00177v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00177","description":"<p>A machine intelligence pipeline usually consists of six components: problem,\nrepresentation, model, loss, optimizer and metric. Researchers have worked hard\ntrying to automate many components of the pipeline. However, one key component\nof the pipeline--problem definition--is still left mostly unexplored in terms\nof automation. Usually, it requires extensive efforts from domain experts to\nidentify, define and formulate important problems in an area. However,\nautomatically discovering research or application problems for an area is\nbeneficial since it helps to identify valid and potentially important problems\nhidden in data that are unknown to domain experts, expand the scope of tasks\nthat we can do in an area, and even inspire completely new findings.\n</p>\n<p>This paper describes Problem Learning, which aims at learning to discover and\ndefine valid and ethical problems from data or from the machine's interaction\nwith the environment. We formalize problem learning as the identification of\nvalid and ethical problems in a problem space and introduce several possible\napproaches to problem learning. In a broader sense, problem learning is an\napproach towards the free will of intelligent machines. Currently, machines are\nstill limited to solving the problems defined by humans, without the ability or\nflexibility to freely explore various possible problems that are even unknown\nto humans. Though many machine learning techniques have been developed and\nintegrated into intelligent systems, they still focus on the means rather than\nthe purpose in that machines are still solving human defined problems. However,\nproposing good problems is sometimes even more important than solving problems,\nbecause a good problem can help to inspire new ideas and gain deeper\nunderstandings. The paper also discusses the ethical implications of problem\nlearning under the background of Responsible AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations. (arXiv:2109.00181v1 [cs.SD])","link":"http://arxiv.org/abs/2109.00181","description":"<p>Existing audio-language task-specific predictive approaches focus on building\ncomplicated late-fusion mechanisms. However, these models are facing challenges\nof overfitting with limited labels and low model generalization abilities. In\nthis paper, we present a Cross-modal Transformer for Audio-and-Language, i.e.,\nCTAL, which aims to learn the intra-modality and inter-modality connections\nbetween audio and language through two proxy tasks on a large amount of\naudio-and-language pairs: masked language modeling and masked cross-modal\nacoustic modeling. After fine-tuning our pre-trained model on multiple\ndownstream audio-and-language tasks, we observe significant improvements across\nvarious tasks, such as, emotion classification, sentiment analysis, and speaker\nverification. On this basis, we further propose a specially-designed fusion\nmechanism that can be used in fine-tuning phase, which allows our pre-trained\nmodel to achieve better performance. Lastly, we demonstrate detailed ablation\nstudies to prove that both our novel cross-modality fusion component and\naudio-language pre-training methods significantly contribute to the promising\nresults.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wenbiao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zitao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep $\\mathcal{L}^1$ Stochastic Optimal Control Policies for Planetary Soft-landing. (arXiv:2109.00183v1 [eess.SY])","link":"http://arxiv.org/abs/2109.00183","description":"<p>In this paper, we introduce a novel deep learning based solution to the\nPowered-Descent Guidance (PDG) problem, grounded in principles of nonlinear\nStochastic Optimal Control (SOC) and Feynman-Kac theory. Our algorithm solves\nthe PDG problem by framing it as an $\\mathcal{L}^1$ SOC problem for minimum\nfuel consumption. Additionally, it can handle practically useful control\nconstraints, nonlinear dynamics and enforces state constraints as\nsoft-constraints. This is achieved by building off of recent work on deep\nForward-Backward Stochastic Differential Equations (FBSDEs) and differentiable\nnon-convex optimization neural-network layers based on stochastic search. In\ncontrast to previous approaches, our algorithm does not require convexification\nof the constraints or linearization of the dynamics and is empirically shown to\nbe robust to stochastic disturbances and the initial position of the\nspacecraft. After training offline, our controller can be activated once the\nspacecraft is within a pre-specified radius of the landing zone and at a\npre-specified altitude i.e., the base of an inverted cone with the tip at the\nlanding zone. We demonstrate empirically that our controller can successfully\nand safely land all trajectories initialized at the base of this cone while\nminimizing fuel consumption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pereira_M/0/1/0/all/0/1\">Marcus A. Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duarte_C/0/1/0/all/0/1\">Camilo A. Duarte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Exarchos_I/0/1/0/all/0/1\">Ioannis Exarchos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Theodorou_E/0/1/0/all/0/1\">Evangelos A. Theodorou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on the Joint Impact of Feature Selection and Data Resampling on Imbalance Classification. (arXiv:2109.00201v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00201","description":"<p>Real-world datasets often present different degrees of imbalanced (i.e.,\nlong-tailed or skewed) distributions. While the majority (a.k.a., head or\nfrequent) classes have sufficient samples, the minority (a.k.a., tail or rare)\nclasses can be under-represented by a rather limited number of samples. On one\nhand, data resampling is a common approach to tackling class imbalance. On the\nother hand, dimension reduction, which reduces the feature space, is a\nconventional machine learning technique for building stronger classification\nmodels on a dataset. However, the possible synergy between feature selection\nand data resampling for high-performance imbalance classification has rarely\nbeen investigated before. To address this issue, this paper carries out a\ncomprehensive empirical study on the joint influence of feature selection and\nresampling on two-class imbalance classification. Specifically, we study the\nperformance of two opposite pipelines for imbalance classification, i.e.,\napplying feature selection before or after data resampling. We conduct a large\namount of experiments (a total of 9225 experiments) on 52 publicly available\ndatasets, using 9 feature selection methods, 6 resampling approaches for class\nimbalance learning, and 3 well-known classification algorithms. Experimental\nresults show that there is no constant winner between the two pipelines, thus\nboth of them should be considered to derive the best performing model for\nimbalance classification. We also find that the performance of an imbalance\nclassification model depends on the classifier adopted, the ratio between the\nnumber of majority and minority samples (IR), as well as on the ratio between\nthe number of samples and features (SFR). Overall, this study should provide\nnew reference value for researchers and practitioners in imbalance learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1\">Paolo Soda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jingjun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1\">Gaojuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almpanidis_G/0/1/0/all/0/1\">George Almpanidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Learning: Issues in Medical Application. (arXiv:2109.00202v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00202","description":"<p>Since the federated learning, which makes AI learning possible without moving\nlocal data around, was introduced by google in 2017 it has been actively\nstudied particularly in the field of medicine. In fact, the idea of machine\nlearning in AI without collecting data from local clients is very attractive\nbecause data remain in local sites. However, federated learning techniques\nstill have various open issues due to its own characteristics such as non\nidentical distribution, client participation management, and vulnerable\nenvironments. In this presentation, the current issues to make federated\nlearning flawlessly useful in the real world will be briefly overviewed. They\nare related to data/system heterogeneity, client management, traceability, and\nsecurity. Also, we introduce the modularized federated learning framework, we\ncurrently develop, to experiment various techniques and protocols to find\nsolutions for aforementioned issues. The framework will be open to public after\ndevelopment completes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Joo Hun Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hyejun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaehyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1\">Tai-Myoung Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sample based Contrastive Loss for Top-k Recommendation. (arXiv:2109.00217v1 [cs.IR])","link":"http://arxiv.org/abs/2109.00217","description":"<p>The top-k recommendation is a fundamental task in recommendation systems\nwhich is generally learned by comparing positive and negative pairs. The\nContrastive Loss (CL) is the key in contrastive learning that has received more\nattention recently and we find it is well suited for top-k recommendations.\nHowever, it is a problem that CL treats the importance of the positive and\nnegative samples as the same. On the one hand, CL faces the imbalance problem\nof one positive sample and many negative samples. On the other hand, positive\nitems are so few in sparser datasets that their importance should be\nemphasized. Moreover, the other important issue is that the sparse positive\nitems are still not sufficiently utilized in recommendations. So we propose a\nnew data augmentation method by using multiple positive items (or samples)\nsimultaneously with the CL loss function. Therefore, we propose a Multi-Sample\nbased Contrastive Loss (MSCL) function which solves the two problems by\nbalancing the importance of positive and negative samples and data\naugmentation. And based on the graph convolution network (GCN) method,\nexperimental results demonstrate the state-of-the-art performance of MSCL. The\nproposed MSCL is simple and can be applied in many methods. We will release our\ncode on GitHub upon the acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoshuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting all Aspect-polarity Pairs Jointly in a Text with Relation Extraction Approach. (arXiv:2109.00256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00256","description":"<p>Extracting aspect-polarity pairs from texts is an important task of\nfine-grained sentiment analysis. While the existing approaches to this task\nhave gained many progresses, they are limited at capturing relationships among\naspect-polarity pairs in a text, thus degrading the extraction performance.\nMoreover, the existing state-of-the-art approaches, namely token-based\nse-quence tagging and span-based classification, have their own defects such as\npolarity inconsistency resulted from separately tagging tokens in the former\nand the heterogeneous categorization in the latter where aspect-related and\npolarity-related labels are mixed. In order to remedy the above defects,\nin-spiring from the recent advancements in relation extraction, we propose to\ngenerate aspect-polarity pairs directly from a text with relation extraction\ntechnology, regarding aspect-pairs as unary relations where aspects are\nenti-ties and the corresponding polarities are relations. Based on the\nperspective, we present a position- and aspect-aware sequence2sequence model\nfor joint extraction of aspect-polarity pairs. The model is characterized with\nits ability to capture not only relationships among aspect-polarity pairs in a\ntext through the sequence decoding, but also correlations between an aspect and\nits polarity through the position- and aspect-aware attentions. The\nexperi-ments performed on three benchmark datasets demonstrate that our model\noutperforms the existing state-of-the-art approaches, making significant\nim-provement over them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1\">Lingmei Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongmei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhonghua Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complex Event Forecasting with Prediction Suffix Trees: Extended Technical Report. (arXiv:2109.00287v1 [cs.DB])","link":"http://arxiv.org/abs/2109.00287","description":"<p>Complex Event Recognition (CER) systems have become popular in the past two\ndecades due to their ability to \"instantly\" detect patterns on real-time\nstreams of events. However, there is a lack of methods for forecasting when a\npattern might occur before such an occurrence is actually detected by a CER\nengine. We present a formal framework that attempts to address the issue of\nComplex Event Forecasting (CEF). Our framework combines two formalisms: a)\nsymbolic automata which are used to encode complex event patterns; and b)\nprediction suffix trees which can provide a succinct probabilistic description\nof an automaton's behavior. We compare our proposed approach against\nstate-of-the-art methods and show its advantage in terms of accuracy and\nefficiency. In particular, prediction suffix trees, being variable-order Markov\nmodels, have the ability to capture long-term dependencies in a stream by\nremembering only those past sequences that are informative enough. Our\nexperimental results demonstrate the benefits, in terms of accuracy, of being\nable to capture such long-term dependencies. This is achieved by increasing the\norder of our model beyond what is possible with full-order Markov models that\nneed to perform an exhaustive enumeration of all possible past sequences of a\ngiven order. We also discuss extensively how CEF solutions should be best\nevaluated on the quality of their forecasts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alevizos_E/0/1/0/all/0/1\">Elias Alevizos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artikis_A/0/1/0/all/0/1\">Alexander Artikis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Argument Strength in Structured Argumentation: a Principled Approach. (arXiv:2109.00318v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00318","description":"<p>Abstract argumentation provides us with methods such as gradual and Dung\nsemantics with which to evaluate arguments after potential attacks by other\narguments. Some of these methods can take intrinsic strengths of arguments as\ninput, with which to modulate the effects of attacks between arguments. Coming\nfrom abstract argumentation, these methods look only at the relations between\narguments and not at the structure of the arguments themselves. In structured\nargumentation the way an argument is constructed, by chaining inference rules\nstarting from premises, is taken into consideration. In this paper we study\nmethods for assigning an argument its intrinsic strength, based on the\nstrengths of the premises and inference rules used to form said argument. We\nfirst define a set of principles, which are properties that strength assigning\nmethods might satisfy. We then propose two such methods and analyse which\nprinciples they satisfy. Finally, we present a generalised system for creating\nnovel strength assigning methods and speak to the properties of this system\nregarding the proposed principles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spaans_J/0/1/0/all/0/1\">Jeroen Paul Spaans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building a Legal Dialogue System: Development Process, Challenges and Opportunities. (arXiv:2109.00381v1 [cs.HC])","link":"http://arxiv.org/abs/2109.00381","description":"<p>This paper presents key principles and solutions to the challenges faced in\ndesigning a domain-specific conversational agent for the legal domain. It\nincludes issues of scope, platform, architecture and preparation of input data.\nIt provides functionality in answering user queries and recording user\ninformation including contact details and case-related information. It utilises\ndeep learning technology built upon Amazon Web Services (AWS) LEX in\ncombination with AWS Lambda. Due to lack of publicly available data, we\nidentified two methods including crowdsourcing experiments and archived\nenquiries to develop a number of linguistic resources. This includes a training\ndataset, set of predetermined responses for the conversational agent, a set of\nregression test cases and a further conversation test set. We propose a\nhierarchical bot structure that facilitates multi-level delegation and report\nmodel accuracy on the regression test set. Additionally, we highlight features\nthat are added to the bot to improve the conversation flow and overall user\nexperience.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1\">Mudita Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russell_Rose_T/0/1/0/all/0/1\">Tony Russell-Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barakat_L/0/1/0/all/0/1\">Lina Barakat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_A/0/1/0/all/0/1\">Akitaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boolean proportions. (arXiv:2109.00388v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00388","description":"<p>Analogy-making is at the core of human intelligence and creativity with\napplications to such diverse tasks as commonsense reasoning, learning, language\nacquisition, and story telling. This paper studies analogical proportions\nbetween booleans of the form `$a$ is to $b$ what $c$ is to $d$' called boolean\nproportions. Technically, we instantiate an abstract algebraic framework of\nanalogical proportions -- recently introduced by the author -- in the boolean\ndomain consisting of the truth values true and false together with boolean\nfunctions. It turns out that our notion of boolean proportions has appealing\nmathematical properties and that it coincides with a prominent model of boolean\nproportions in the general case. In a broader sense, this paper is a further\nstep towards a theory of analogical reasoning and learning systems with\npotential applications to fundamental AI-problems like commonsense reasoning\nand computational learning and creativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1\">Christian Anti&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00412","description":"<p>In multimodal sentiment analysis (MSA), the performance of a model highly\ndepends on the quality of synthesized embeddings. These embeddings are\ngenerated from the upstream process called multimodal fusion, which aims to\nextract and combine the input unimodal raw data to produce a richer multimodal\nrepresentation. Previous work either back-propagates the task loss or\nmanipulates the geometric property of feature spaces to produce favorable\nfusion results, which neglects the preservation of critical task-related\ninformation that flows from input to the fusion results. In this work, we\npropose a framework named MultiModal InfoMax (MMIM), which hierarchically\nmaximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)\nand between multimodal fusion result and unimodal input in order to maintain\ntask-related information through multimodal fusion. The framework is jointly\ntrained with the main task (MSA) to improve the performance of the downstream\nMSA task. To address the intractable issue of MI bounds, we further formulate a\nset of computationally simple parametric and non-parametric methods to\napproximate their truth value. Experimental results on the two widely used\ndatasets demonstrate the efficacy of our approach. The implementation of this\nwork is publicly available at\nhttps://github.com/declare-lab/Multimodal-Infomax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Performance and Human Autonomy with Implicit Guidance Agent. (arXiv:2109.00414v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00414","description":"<p>The human-agent team, which is a problem in which humans and autonomous\nagents collaborate to achieve one task, is typical in human-AI collaboration.\nFor effective collaboration, humans want to have an effective plan, but in\nrealistic situations, they might have difficulty calculating the best plan due\nto cognitive limitations. In this case, guidance from an agent that has many\ncomputational resources may be useful. However, if an agent guides the human\nbehavior explicitly, the human may feel that they have lost autonomy and are\nbeing controlled by the agent. We therefore investigated implicit guidance\noffered by means of an agent's behavior. With this type of guidance, the agent\nacts in a way that makes it easy for the human to find an effective plan for a\ncollaborative task, and the human can then improve the plan. Since the human\nimproves their plan voluntarily, he or she maintains autonomy. We modeled a\ncollaborative agent with implicit guidance by integrating the Bayesian Theory\nof Mind into existing collaborative-planning algorithms and demonstrated\nthrough a behavioral experiment that implicit guidance is effective for\nenabling humans to maintain a balance between improving their plans and\nretaining autonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakahashi_R/0/1/0/all/0/1\">Ryo Nakahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_S/0/1/0/all/0/1\">Seiji Yamada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Adversarial Generation for Neural Machine Translation. (arXiv:2109.00417v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00417","description":"<p>Attacking Neural Machine Translation models is an inherently combinatorial\ntask on discrete sequences, solved with approximate heuristics. Most methods\nuse the gradient to attack the model on each sample independently. Instead of\nmechanically applying the gradient, could we learn to produce meaningful\nadversarial attacks ? In contrast to existing approaches, we learn to attack a\nmodel by training an adversarial generator based on a language model. We\npropose the Masked Adversarial Generation (MAG) model, that learns to perturb\nthe translation model throughout the training process. The experiments show\nthat it improves the robustness of machine translation models, while being\nfaster than competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Idrissi_B/0/1/0/all/0/1\">Badr Youbi Idrissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00430","description":"<p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proceedings of KDD 2020 Workshop on Data-driven Humanitarian Mapping: Harnessing Human-Machine Intelligence for High-Stake Public Policy and Resilience Planning. (arXiv:2109.00435v1 [cs.CY])","link":"http://arxiv.org/abs/2109.00435","description":"<p>Humanitarian challenges, including natural disasters, food insecurity,\nclimate change, racial and gender violence, environmental crises, the COVID-19\ncoronavirus pandemic, human rights violations, and forced displacements,\ndisproportionately impact vulnerable communities worldwide. According to UN\nOCHA, 235 million people will require humanitarian assistance in 20211 .\nDespite these growing perils, there remains a notable paucity of data science\nresearch to scientifically inform equitable public policy decisions for\nimproving the livelihood of at-risk populations. Scattered data science efforts\nexist to address these challenges, but they remain isolated from practice and\nprone to algorithmic harms concerning lack of privacy, fairness,\ninterpretability, accountability, transparency, and ethics. Biases in\ndata-driven methods carry the risk of amplifying inequalities in high-stakes\npolicy decisions that impact the livelihood of millions of people.\nConsequently, proclaimed benefits of data-driven innovations remain\ninaccessible to policymakers, practitioners, and marginalized communities at\nthe core of humanitarian actions and global development. To help fill this gap,\nwe propose the Data-driven Humanitarian Mapping Research Program, which focuses\non developing novel data science methodologies that harness human-machine\nintelligence for high-stakes public policy and resilience planning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Snehalkumar/0/1/0/all/0/1\">Snehalkumar</a> (Neil) <a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">S. Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1\">Shankar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lunga_D/0/1/0/all/0/1\">Dalton Lunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yu-Ru Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Planning from video game descriptions. (arXiv:2109.00449v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00449","description":"<p>This project proposes a methodology for the automatic generation of action\nmodels from video game dynamics descriptions, as well as its integration with a\nplanning agent for the execution and monitoring of the plans. Planners use\nthese action models to get the deliberative behaviour for an agent in many\ndifferent video games and, combined with a reactive module, solve deterministic\nand no-deterministic levels. Experimental results validate the methodology and\nprove that the effort put by a knowledge engineer can be greatly reduced in the\ndefinition of such complex domains. Furthermore, benchmarks of the domains has\nbeen produced that can be of interest to the international planning community\nto evaluate planners in international planning competitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vellido_I/0/1/0/all/0/1\">Ignacio Vellido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunez_Molina_C/0/1/0/all/0/1\">Carlos N&#xfa;&#xf1;ez-Molina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_V/0/1/0/all/0/1\">Vladislav Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fdez_Olivares_J/0/1/0/all/0/1\">Juan Fdez-Olivares</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Movement Kinematics to Object Properties: Online Recognition of Human Carefulness. (arXiv:2109.00460v1 [cs.RO])","link":"http://arxiv.org/abs/2109.00460","description":"<p>When manipulating objects, humans finely adapt their motions to the\ncharacteristics of what they are handling. Thus, an attentive observer can\nforesee hidden properties of the manipulated object, such as its weight,\ntemperature, and even whether it requires special care in manipulation. This\nstudy is a step towards endowing a humanoid robot with this last capability.\nSpecifically, we study how a robot can infer online, from vision alone, whether\nor not the human partner is careful when moving an object. We demonstrated that\na humanoid robot could perform this inference with high accuracy (up to 81.3%)\neven with a low-resolution camera. Only for short movements without obstacles,\ncarefulness recognition was insufficient. The prompt recognition of movement\ncarefulness from observing the partner's action will allow robots to adapt\ntheir actions on the object to show the same degree of care as their human\npartners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lastrico_L/0/1/0/all/0/1\">Linda Lastrico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carfi_A/0/1/0/all/0/1\">Alessandro Carf&#xec;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rea_F/0/1/0/all/0/1\">Francesco Rea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sciutti_A/0/1/0/all/0/1\">Alessandra Sciutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mastrogiovanni_F/0/1/0/all/0/1\">Fulvio Mastrogiovanni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impossibility Results in AI: A Survey. (arXiv:2109.00484v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00484","description":"<p>An impossibility theorem demonstrates that a particular problem or set of\nproblems cannot be solved as described in the claim. Such theorems put limits\non what is possible to do concerning artificial intelligence, especially the\nsuper-intelligent one. As such, these results serve as guidelines, reminders,\nand warnings to AI safety, AI policy, and governance researchers. These might\nenable solutions to some long-standing questions in the form of formalizing\ntheories in the framework of constraint satisfaction without committing to one\noption. In this paper, we have categorized impossibility theorems applicable to\nthe domain of AI into five categories: deduction, indistinguishability,\ninduction, tradeoffs, and intractability. We found that certain theorems are\ntoo specific or have implicit assumptions that limit application. Also, we\nadded a new result (theorem) about the unfairness of explainability, the first\nexplainability-related result in the induction category. We concluded that\ndeductive impossibilities deny 100%-guarantees for security. In the end, we\ngive some ideas that hold potential in explainability, controllability, value\nalignment, ethics, and group decision-making. They can be deepened by further\ninvestigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brcic_M/0/1/0/all/0/1\">Mario Brcic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yampolskiy_R/0/1/0/all/0/1\">Roman V. Yampolskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchically Structured Concepts. (arXiv:1909.04559v5 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/1909.04559","description":"<p>We study the question of how concepts that have structure get represented in\nthe brain. Specifically, we introduce a model for hierarchically structured\nconcepts and we show how a biologically plausible neural network can recognize\nthese concepts, and how it can learn them in the first place. Our main goal is\nto introduce a general framework for these tasks and prove formally how both\n(recognition and learning) can be achieved.\n</p>\n<p>We show that both tasks can be accomplished even in presence of noise. For\nlearning, we analyze Oja's rule formally, a well-known biologically-plausible\nrule for adjusting the weights of synapses. We complement the learning results\nwith lower bounds asserting that, in order to recognize concepts of a certain\nhierarchical depth, neural networks must have a corresponding number of layers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lynch_N/0/1/0/all/0/1\">Nancy Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallmann_Trenn_F/0/1/0/all/0/1\">Frederik Mallmann-Trenn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2009.04695","description":"<p>Multi-objective gradient methods are becoming the standard for solving\nmulti-objective problems. Among others, they show promising results in\ndeveloping multi-objective recommender systems with both correlated and\nconflicting objectives. Classic multi-gradient~descent usually relies on the\ncombination of the gradients, not including the computation of first and second\nmoments of the gradients. This leads to a brittle behavior and misses important\nareas in the solution space. In this work, we create a multi-objective\nmodel-agnostic Adamize method that leverages the benefits of the Adam optimizer\nin single-objective problems. This corrects and stabilizes~the~gradients of\nevery objective before calculating a common gradient descent vector that\noptimizes all the objectives simultaneously. We evaluate the benefits of\nMulti-objective Adamize on two multi-objective recommender systems and for\nthree different objective combinations, both correlated or conflicting. We\nreport significant improvements, measured with three different Pareto front\nmetrics: hypervolume, coverage, and spacing. Finally, we show that the\n\\textit{Adamized} Pareto front strictly dominates the previous one on multiple\nobjective pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1\">Blagoj Mitrevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1\">Milena Filipovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1\">Diego Antognini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1\">Emma Lejal Glaude</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1\">Boi Faltings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1\">Claudiu Musat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta Automatic Curriculum Learning. (arXiv:2011.08463v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.08463","description":"<p>A major challenge in the Deep RL (DRL) community is to train agents able to\ngeneralize their control policy over situations never seen in training.\nTraining on diverse tasks has been identified as a key ingredient for good\ngeneralization, which pushed researchers towards using rich procedural task\ngeneration systems controlled through complex continuous parameter spaces. In\nsuch complex task spaces, it is essential to rely on some form of Automatic\nCurriculum Learning (ACL) to adapt the task sampling distribution to a given\nlearning agent, instead of randomly sampling tasks, as many could end up being\neither trivial or unfeasible. Since it is hard to get prior knowledge on such\ntask spaces, many ACL algorithms explore the task space to detect progress\nniches over time, a costly tabula-rasa process that needs to be performed for\neach new learning agents, although they might have similarities in their\ncapabilities profiles. To address this limitation, we introduce the concept of\nMeta-ACL, and formalize it in the context of black-box RL learners, i.e.\nalgorithms seeking to generalize curriculum generation to an (unknown)\ndistribution of learners. In this work, we present AGAIN, a first instantiation\nof Meta-ACL, and showcase its benefits for curriculum generation over classical\nACL in multiple simulated environments including procedurally generated parkour\nenvironments with learners of varying morphologies. Videos and code are\navailable at https://sites.google.com/view/meta-acl .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romac_C/0/1/0/all/0/1\">Cl&#xe9;ment Romac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning. (arXiv:2011.11261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11261","description":"<p>We present a novel technique for self-supervised video representation\nlearning by: (a) decoupling the learning objective into two contrastive\nsubtasks respectively emphasizing spatial and temporal features, and (b)\nperforming it hierarchically to encourage multi-scale understanding. Motivated\nby their effectiveness in supervised learning, we first introduce\nspatial-temporal feature learning decoupling and hierarchical learning to the\ncontext of unsupervised video learning. We show by experiments that\naugmentations can be manipulated as regularization to guide the network to\nlearn desired semantics in contrastive learning, and we propose a way for the\nmodel to separately capture spatial and temporal features at multiple scales.\nWe also introduce an approach to overcome the problem of divergent levels of\ninstance invariance at different hierarchies by modeling the invariance as loss\nweights for objective re-weighting. Experiments on downstream action\nrecognition benchmarks on UCF101 and HMDB51 show that our proposed\nHierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial\nimprovements over directly learning spatial-temporal features as a whole and\nachieves competitive performance when compared with other state-of-the-art\nunsupervised methods. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zehua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Width-Based Planning and Learning. (arXiv:2101.06177v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2101.06177","description":"<p>Width-based search methods have demonstrated state-of-the-art performance in\na wide range of testbeds, from classical planning problems to image-based\nsimulators such as Atari games. These methods scale independently of the size\nof the state-space, but exponentially in the problem width. In practice,\nrunning the algorithm with a width larger than 1 is computationally\nintractable, prohibiting IW from solving higher width problems. In this paper,\nwe present a hierarchical algorithm that plans at two levels of abstraction. A\nhigh-level planner uses abstract features that are incrementally discovered\nfrom low-level pruning decisions. We illustrate this algorithm in classical\nplanning PDDL domains as well as in pixel-based simulator domains. In classical\nplanning, we show how IW(1) at two levels of abstraction can solve problems of\nwidth 2. For pixel-based domains, we show how in combination with a learned\npolicy and a learned value function, the proposed hierarchical IW can\noutperform current flat IW-based planners in Atari games with sparse rewards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Junyent_M/0/1/0/all/0/1\">Miquel Junyent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1\">Anders Jonsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v14 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.03479","description":"<p>Many complex multi-agent systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a popular MARL algorithm base on the monotonicity\nconstraint, has been used as a baseline for the benchmark environments, e.g.,\nStarcraft Multi-Agent Challenge (SMAC), Predator-Prey (PP). Recent variants of\nQMIX target relaxing the monotonicity constraint of QMIX to improve the\nexpressive power of QMIX, allowing for performance improvement in SMAC.\nHowever, we find that such performance improvements of the variants are\nsignificantly affected by various implementation tricks. In this paper, we\nrevisit the monotonicity constraint of QMIX, (1) we design a novel model RMC to\nfurther investigate the monotonicity constraint; the results show that\nmonotonicity constraint can improve sample efficiency in some purely\ncooperative tasks. (2) we then re-evaluate the performance of QMIX and these\nvariants by a grid hyperparameter search for the tricks; the results show QMIX\nachieves the best performance among them; (3) we analyze the monotonic mixing\nnetwork from a theoretical perspective and show that it can represent any tasks\nwhich can be interpreted as purely cooperative. These analyses demonstrate that\nrelaxing the monotonicity constraint of the mixing network will not always\nimprove the performance of QMIX, which breaks our previous impressions of the\nmonotonicity constraints. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1\">Seth Austin Harding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shih-wei Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01834","description":"<p>Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte (for workflow infrastructure and NLP\nfunction processors) and Stave (for user interaction, visualization, and\nannotation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanxiong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukkittu_A/0/1/0/all/0/1\">Avinash Bukkittu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mansi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Atif Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhavi_S/0/1/0/all/0/1\">Swapnil Singhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zecong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoran Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering. (arXiv:2104.03149v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03149","description":"<p>We introduce an evaluation methodology for visual question answering (VQA) to\nbetter diagnose cases of shortcut learning. These cases happen when a model\nexploits spurious statistical regularities to produce correct answers but does\nnot actually deploy the desired behavior. There is a need to identify possible\nshortcuts in a dataset and assess their use before deploying a model in the\nreal world. The research community in VQA has focused exclusively on\nquestion-based shortcuts, where a model might, for example, answer \"What is the\ncolor of the sky\" with \"blue\" by relying mostly on the question-conditional\ntraining prior and give little weight to visual evidence. We go a step further\nand consider multimodal shortcuts that involve both questions and images. We\nfirst identify potential shortcuts in the popular VQA v2 training set by mining\ntrivial predictive rules such as co-occurrences of words and visual elements.\nWe then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on\nour subset of CounterExamples i.e. image-question-answer triplets where our\nrules lead to incorrect answers. We use this new evaluation in a large-scale\nstudy of existing approaches for VQA. We demonstrate that even state-of-the-art\nmodels perform poorly and that existing techniques to reduce biases are largely\nineffective in this context. Our findings suggest that past work on\nquestion-based biases in VQA has only addressed one facet of a complex issue.\nThe code for our method is available at\nhttps://github.com/cdancette/detect-shortcuts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">Remi Cadene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07464","description":"<p>Recently, considerable literature has grown up around the theme of few-shot\nnamed entity recognition (NER), but little published benchmark data\nspecifically focused on the practical and challenging task. Current approaches\ncollect existing supervised NER datasets and re-organize them to the few-shot\nsetting for empirical study. These strategies conventionally aim to recognize\ncoarse-grained entity types with few examples, while in practice, most unseen\nentity types are fine-grained. In this paper, we present Few-NERD, a\nlarge-scale human-annotated few-shot NER dataset with a hierarchy of 8\ncoarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238\nsentences from Wikipedia, 4,601,160 words are included and each is annotated as\ncontext or a part of a two-level entity type. To the best of our knowledge,\nthis is the first few-shot NER dataset and the largest human-crafted NER\ndataset. We construct benchmark tasks with different emphases to\ncomprehensively assess the generalization capability of models. Extensive\nempirical results and analysis show that Few-NERD is challenging and the\nproblem requires further research. We make Few-NERD public at\nhttps://ningding97.github.io/fewnerd/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Domain Active Learning: A Comparative Study. (arXiv:2106.13516v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.13516","description":"<p>Building classifiers on multiple domains is a practical problem in the real\nlife. Instead of building classifiers one by one, multi-domain learning (MDL)\nsimultaneously builds classifiers on all the domains. MDL utilizes the\ninformation shared among the domains to improve the performance. As a\nsupervised learning problem, the labeling effort is still high in MDL problems.\nUsually, this high labeling cost issue could be relieved by using active\nlearning. Thus, it is natural to utilize active learning to reduce the labeling\neffort in MDL, and we refer this setting as multi-domain active learning\n(MDAL). However, there are only few works which are built on this setting. And\nwhen the researchers have to face this problem, there is no off-the-shelf\nsolution. Under this circumstance, combining the current multi-domain learning\nmodels and single-domain active learning strategies might be a preliminary\nsolution for MDAL problem. To find out the potential of this preliminary\nsolution, a comparative study over 5 models and 4 active learning strategies is\nmade in this paper. To the best of our knowledge, this is the first work\nprovides the formal definition of MDAL. Besides, this is the first comparative\nwork for MDAL problem. From the results, the Multinomial Adversarial Networks\n(MAN) model with a simple best vs second best (BvSB) uncertainty strategy shows\nits superiority in most cases. We take this combination as our off-the-shelf\nrecommendation for the MDAL problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Rui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1\">Ke Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00956","description":"<p>Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Driven Biophysical Computational Model of Parkinson's Disease based on Marmoset Monkeys. (arXiv:2107.12536v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2107.12536","description":"<p>In this work we propose a new biophysical computational model of brain\nregions relevant to Parkinson's Disease based on local field potential data\ncollected from the brain of marmoset monkeys. Parkinson's disease is a\nneurodegenerative disorder, linked to the death of dopaminergic neurons at the\nsubstantia nigra pars compacta, which affects the normal dynamics of the basal\nganglia-thalamus-cortex neuronal circuit of the brain. Although there are\nmultiple mechanisms underlying the disease, a complete description of those\nmechanisms and molecular pathogenesis are still missing, and there is still no\ncure. To address this gap, computational models that resemble neurobiological\naspects found in animal models have been proposed. In our model, we performed a\ndata-driven approach in which a set of biologically constrained parameters is\noptimised using differential evolution. Evolved models successfully resembled\nsingle-neuron mean firing rates and spectral signatures of local field\npotentials from healthy and parkinsonian marmoset brain data. As far as we are\nconcerned, this is the first computational model of Parkinson's Disease based\non simultaneous electrophysiological recordings from seven brain regions of\nMarmoset monkeys. Results show that the proposed model could facilitate the\ninvestigation of the mechanisms of PD and support the development of techniques\nthat can indicate new therapies. It could also be applied to other\ncomputational neuroscience problems in which biological data could be used to\nfit multi-scale models of brain circuits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ranieri_C/0/1/0/all/0/1\">Caetano M. Ranieri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pimentel_J/0/1/0/all/0/1\">Jhielson M. Pimentel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Romano_M/0/1/0/all/0/1\">Marcelo R. Romano</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Elias_L/0/1/0/all/0/1\">Leonardo A. Elias</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Romero_R/0/1/0/all/0/1\">Roseli A. F. Romero</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lones_M/0/1/0/all/0/1\">Michael A. Lones</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Araujo_M/0/1/0/all/0/1\">Mariana F. P. Araujo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Vargas_P/0/1/0/all/0/1\">Patricia A. Vargas</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Moioli_R/0/1/0/all/0/1\">Renan C. Moioli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Creating Powerful and Interpretable Models with Regression Networks. (arXiv:2107.14417v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.14417","description":"<p>As the discipline has evolved, research in machine learning has been focused\nmore and more on creating more powerful neural networks, without regard for the\ninterpretability of these networks. Such \"black-box models\" yield\nstate-of-the-art results, but we cannot understand why they make a particular\ndecision or prediction. Sometimes this is acceptable, but often it is not. We\npropose a novel architecture, Regression Networks, which combines the power of\nneural networks with the understandability of regression analysis. While some\nmethods for combining these exist in the literature, our architecture\ngeneralizes these approaches by taking interactions into account, offering the\npower of a dense neural network without forsaking interpretability. We\ndemonstrate that the models exceed the state-of-the-art performance of\ninterpretable models on several benchmark datasets, matching the power of a\ndense neural network. Finally, we discuss how these techniques can be\ngeneralized to other neural architectures, such as convolutional and recurrent\nneural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ONeill_L/0/1/0/all/0/1\">Lachlan O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angus_S/0/1/0/all/0/1\">Simon Angus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgohain_S/0/1/0/all/0/1\">Satya Borgohain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chmait_N/0/1/0/all/0/1\">Nader Chmait</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dowe_D/0/1/0/all/0/1\">David L. Dowe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Adversarial Examples\" for Proof-of-Learning. (arXiv:2108.09454v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2108.09454","description":"<p>In S&amp;P '21, Jia et al. proposed a new concept/mechanism named\nproof-of-learning (PoL), which allows a prover to demonstrate ownership of a\nmachine learning model by proving integrity of the training procedure. It\nguarantees that an adversary cannot construct a valid proof with less cost (in\nboth computation and storage) than that made by the prover in generating the\nproof. A PoL proof includes a set of intermediate models recorded during\ntraining, together with the corresponding data points used to obtain each\nrecorded model. Jia et al. claimed that an adversary merely knowing the final\nmodel and training dataset cannot efficiently find a set of intermediate models\nwith correct data points. In this paper, however, we show that PoL is\nvulnerable to \"adversarial examples\"! Specifically, in a similar way as\noptimizing an adversarial example, we could make an arbitrarily-chosen data\npoint \"generate\" a given model, hence efficiently generating intermediate\nmodels with correct data points. We demonstrate, both theoretically and\nempirically, that we are able to generate a valid proof with significantly less\ncost than generating a proof by the prover, thereby we successfully break PoL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qingbiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1\">Kui Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS-DARTS: Mean-Shift Based Differentiable Architecture Search. (arXiv:2108.09996v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2108.09996","description":"<p>Differentiable Architecture Search (DARTS) is an effective continuous\nrelaxation-based network architecture search (NAS) method with low search cost.\nIt has attracted significant attentions in Auto-ML research and becomes one of\nthe most useful paradigms in NAS. Although DARTS can produce superior\nefficiency over traditional NAS approaches with better control of complex\nparameters, oftentimes it suffers from stabilization issues in producing\ndeteriorating architectures when discretizing the continuous architecture. We\nobserved considerable loss of validity causing dramatic decline in performance\nat this final discretization step of DARTS. To address this issue, we propose a\nMean-Shift based DARTS (MS-DARTS) to improve stability based on sampling and\nperturbation. Our approach can improve bot the stability and accuracy of DARTS,\nby smoothing the loss landscape and sampling architecture parameters within a\nsuitable bandwidth. We investigate the convergence of our mean-shift approach,\ntogether with the effects of bandwidth selection that affects stability and\naccuracy. Evaluations performed on CIFAR-10, CIFAR-100, and ImageNet show that\nMS-DARTS archives higher performance over other state-of-the-art NAS methods\nwith reduced search cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_J/0/1/0/all/0/1\">Jun-Wei Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Ping-Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santra_S/0/1/0/all/0/1\">Santanu Santra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chou_C/0/1/0/all/0/1\">Cheng-Han Chou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chih-Sheng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"E-Commerce Promotions Personalization via Online Multiple-Choice Knapsack with Uplift Modeling. (arXiv:2108.13298v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.13298","description":"<p>Promotions and discounts are essential components of modern e-commerce\nplatforms, where they are often used to incentivize customers towards purchase\ncompletion. Promotions also affect revenue and may incur a monetary loss that\nis often limited by a dedicated promotional budget. We study the Online\nConstrained Multiple-Choice Promotions Personalization Problem, where the\noptimization goal is to select for each customer which promotion to present in\norder to maximize purchase completions, while also complying with global budget\nlimitations. Our work formalizes the problem as an Online Multiple Choice\nKnapsack Problem and extends the existent literature by addressing cases with\nnegative weights and values. We provide a real-time adaptive method that\nguarantees budget constraints compliance and achieves above 99.7% of the\noptimal promotional impact on various datasets. Our method is evaluated on a\nlarge-scale experimental study at one of the leading online travel platforms in\nthe world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albert_J/0/1/0/all/0/1\">Javier Albert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldenberg_D/0/1/0/all/0/1\">Dmitri Goldenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13679","description":"<p>In this paper, we propose to formulate the task-oriented dialogue system as\nthe purely natural language generation task, so as to fully leverage the\nlarge-scale pre-trained models like GPT-2 and simplify complicated\ndelexicalization prepossessing. However, directly applying this method heavily\nsuffers from the dialogue entity inconsistency caused by the removal of\ndelexicalized tokens, as well as the catastrophic forgetting problem of the\npre-trained model during fine-tuning, leading to unsatisfactory performance. To\nalleviate these problems, we design a novel GPT-Adapter-CopyNet network, which\nincorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve\nbetter performance on transfer learning and dialogue entity generation.\nExperimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ\ndataset demonstrate that our proposed approach significantly outperforms\nbaseline models with a remarkable performance on automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.13844","description":"<p>Fiducial markers are commonly used in navigation assisted minimally invasive\nspine surgery (MISS) and they help transfer image coordinates into real world\ncoordinates. In practice, these markers might be located outside the\nfield-of-view (FOV), due to the limited detector sizes of C-arm cone-beam\ncomputed tomography (CBCT) systems used in intraoperative surgeries. As a\nconsequence, reconstructed markers in CBCT volumes suffer from artifacts and\nhave distorted shapes, which sets an obstacle for navigation. In this work, we\npropose two fiducial marker detection methods: direct detection from distorted\nmarkers (direct method) and detection after marker recovery (recovery method).\nFor direct detection from distorted markers in reconstructed volumes, an\nefficient automatic marker detection method using two neural networks and a\nconventional circle detection algorithm is proposed. For marker recovery, a\ntask-specific learning strategy is proposed to recover markers from severely\ntruncated data. Afterwards, a conventional marker detection algorithm is\napplied for position detection. The two methods are evaluated on simulated data\nand real data, both achieving a marker registration error smaller than 0.2 mm.\nOur experiments demonstrate that the direct method is capable of detecting\ndistorted markers accurately and the recovery method with task-specific\nlearning has high robustness and generalizability on various data sets. In\naddition, the task-specific learning is able to reconstruct other structures of\ninterest accurately, e.g. ribs for image-guided needle biopsy, from severely\ntruncated data, which empowers CBCT systems with new potential applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_F/0/1/0/all/0/1\">Fuxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreher_B/0/1/0/all/0/1\">Bj&#xf6;rn Kreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keil_H/0/1/0/all/0/1\">Holger Keil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Subsampling-Based Method for Causal Discovery on Discrete Data. (arXiv:2108.13984v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2108.13984","description":"<p>Inferring causal directions on discrete and categorical data is an important\nyet challenging problem. Even though the additive noise models (ANMs) approach\ncan be adapted to the discrete data, the functional structure assumptions make\nit not applicable on categorical data. Inspired by the principle that the cause\nand mechanism are independent, various methods have been developed, leveraging\nindependence tests such as the distance correlation measure. In this work, we\ntake an alternative perspective and propose a subsampling-based method to test\nthe independence between the generating schemes of the cause and that of the\nmechanism. Our methodology works for both discrete and categorical data and\ndoes not imply any functional model on the data, making it a more flexible\napproach. To demonstrate the efficacy of our methodology, we compare it with\nexisting baselines over various synthetic data and real data experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Goddard_A/0/1/0/all/0/1\">Austin Goddard</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DASH: Modularized Human Manipulation Simulation with Vision and Language for Embodied AI. (arXiv:2108.12536v1 [cs.GR] CROSS LISTED)","link":"http://arxiv.org/abs/2108.12536","description":"<p>Creating virtual humans with embodied, human-like perceptual and actuation\nconstraints has the promise to provide an integrated simulation platform for\nmany scientific and engineering applications. We present Dynamic and Autonomous\nSimulated Human (DASH), an embodied virtual human that, given natural language\ncommands, performs grasp-and-stack tasks in a physically-simulated cluttered\nenvironment solely using its own visual perception, proprioception, and touch,\nwithout requiring human motion data. By factoring the DASH system into a vision\nmodule, a language module, and manipulation modules of two skill categories, we\ncan mix and match analytical and machine learning techniques for different\nmodules so that DASH is able to not only perform randomly arranged tasks with a\nhigh success rate, but also do so under anthropomorphic constraints and with\nfluid and diverse motions. The modular design also favors analysis and\nextensibility to more complex manipulation skills.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifeng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Michelle Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangshan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Exarchos_I/0/1/0/all/0/1\">Ioannis Exarchos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">C. Karen Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Artificial Intelligence"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Working Memory Connections for LSTM. (arXiv:2109.00020v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00020","description":"<p>Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of\ngating mechanisms to mitigate exploding and vanishing gradients when learning\nlong-term dependencies. For this reason, LSTMs and other gated RNNs are widely\nadopted, being the standard de facto for many sequence modeling tasks. Although\nthe memory cell inside the LSTM contains essential information, it is not\nallowed to influence the gating mechanism directly. In this work, we improve\nthe gate potential by including information coming from the internal cell\nstate. The proposed modification, named Working Memory Connection, consists in\nadding a learnable nonlinear projection of the cell content into the network\ngates. This modification can fit into the classical LSTM gates without any\nassumption on the underlying task, being particularly effective when dealing\nwith longer sequences. Previous research effort in this direction, which goes\nback to the early 2000s, could not bring a consistent improvement over vanilla\nLSTM. As part of this paper, we identify a key issue tied to previous\nconnections that heavily limits their effectiveness, hence preventing a\nsuccessful integration of the knowledge coming from the internal cell state. We\nshow through extensive experimental evaluation that Working Memory Connections\nconstantly improve the performance of LSTMs on a variety of tasks. Numerical\nresults suggest that the cell state contains useful information that is worth\nincluding in the gate structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine-Learning media bias. (arXiv:2109.00024v1 [cs.CY])","link":"http://arxiv.org/abs/2109.00024","description":"<p>We present an automated method for measuring media bias. Inferring which\nnewspaper published a given article, based only on the frequencies with which\nit uses different phrases, leads to a conditional probability distribution\nwhose analysis lets us automatically map newspapers and phrases into a bias\nspace. By analyzing roughly a million articles from roughly a hundred\nnewspapers for bias in dozens of news topics, our method maps newspapers into a\ntwo-dimensional bias landscape that agrees well with previous bias\nclassifications based on human judgement. One dimension can be interpreted as\ntraditional left-right bias, the other as establishment bias. This means that\nalthough news bias is inherently political, its measurement need not be.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DAlonzo_S/0/1/0/all/0/1\">Samantha D&#x27;Alonzo</a> (MIT), <a href=\"http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1\">Max Tegmark</a> (MIT)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sense representations for Portuguese: experiments with sense embeddings and deep neural language models. (arXiv:2109.00025v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00025","description":"<p>Sense representations have gone beyond word representations like Word2Vec,\nGloVe and FastText and achieved innovative performance on a wide range of\nnatural language processing tasks. Although very useful in many applications,\nthe traditional approaches for generating word embeddings have a strict\ndrawback: they produce a single vector representation for a given word ignoring\nthe fact that ambiguous words can assume different meanings. In this paper, we\nexplore unsupervised sense representations which, different from traditional\nword embeddings, are able to induce different senses of a word by analyzing its\ncontextual semantics in a text. The unsupervised sense representations\ninvestigated in this paper are: sense embeddings and deep neural language\nmodels. We present the first experiments carried out for generating sense\nembeddings for Portuguese. Our experiments show that the sense embedding model\n(Sense2vec) outperformed traditional word embeddings in syntactic and semantic\nanalogies task, proving that the language resource generated here can improve\nthe performance of NLP tasks in Portuguese. We also evaluated the performance\nof pre-trained deep neural language models (ELMo and BERT) in two transfer\nlearning approaches: feature based and fine-tuning, in the semantic textual\nsimilarity task. Our experiments indicate that the fine tuned Multilingual and\nPortuguese BERT language models were able to achieve better accuracy than the\nELMo model and baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1\">Jessica Rodrigues da Silva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caseli_H/0/1/0/all/0/1\">Helena de Medeiros Caseli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00055","description":"<p>Representation learning for text via pretraining a language model on a large\ncorpus has become a standard starting point for building NLP systems. This\napproach stands in contrast to autoencoders, also trained on raw text, but with\nthe objective of learning to encode each input as a vector that allows full\nreconstruction. Autoencoders are attractive because of their latent space\nstructure and generative properties. We therefore explore the construction of a\nsentence-level autoencoder from a pretrained, frozen transformer language\nmodel. We adapt the masked language modeling objective as a generative,\ndenoising one, while only training a sentence bottleneck and a single-layer\nmodified transformer decoder. We demonstrate that the sentence representations\ndiscovered by our model achieve better quality than previous methods that\nextract representations from pretrained transformers on text similarity tasks,\nstyle transfer (an example of controlled generation), and single-sentence\nclassification tasks in the GLUE benchmark, while using fewer parameters than\nlarge pretrained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1\">Ivan Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectiveness of Deep Networks in NLP using BiDAF as an example architecture. (arXiv:2109.00074v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00074","description":"<p>Question Answering with NLP has progressed through the evolution of advanced\nmodel architectures like BERT and BiDAF and earlier word, character, and\ncontext-based embeddings. As BERT has leapfrogged the accuracy of models, an\nelement of the next frontier can be the introduction of deep networks and an\neffective way to train them. In this context, I explored the effectiveness of\ndeep networks focussing on the model encoder layer of BiDAF. BiDAF with its\nheterogeneous layers provides the opportunity not only to explore the\neffectiveness of deep networks but also to evaluate whether the refinements\nmade in lower layers are additive to the refinements made in the upper layers\nof the model architecture. I believe the next greatest model in NLP will in\nfact fold in a solid language modeling like BERT with a composite architecture\nwhich will bring in refinements in addition to generic language modeling and\nwill have a more extensive layered architecture. I experimented with the Bypass\nnetwork, Residual Highway network, and DenseNet architectures. In addition, I\nevaluated the effectiveness of ensembling the last few layers of the network. I\nalso studied the difference character embeddings make in adding them to the\nword embeddings, and whether the effects are additive with deep networks. My\nstudies indicate that deep networks are in fact effective in giving a boost.\nAlso, the refinements in the lower layers like embeddings are passed on\nadditively to the gains made through deep networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumyendu Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Machine Comprehension with Dynamic Knowledge Graphs. (arXiv:2109.00077v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00077","description":"<p>Interactive machine reading comprehension (iMRC) is machine comprehension\ntasks where knowledge sources are partially observable. An agent must interact\nwith an environment sequentially to gather necessary knowledge in order to\nanswer a question. We hypothesize that graph representations are good inductive\nbiases, which can serve as an agent's memory mechanism in iMRC tasks. We\nexplore four different categories of graphs that can capture text information\nat various levels. We describe methods that dynamically build and update these\ngraphs during information gathering, as well as neural models to encode graph\nrepresentations in RL agents. Extensive experiments on iSQuAD suggest that\ngraph representations can result in significant performance improvements for RL\nagents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xingdi Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v1 [cs.SE])","link":"http://arxiv.org/abs/2109.00084","description":"<p>Collaborative software development is an integral part of the modern software\ndevelopment life cycle, essential to the success of large-scale software\nprojects. When multiple developers make concurrent changes around the same\nlines of code, a merge conflict may occur. Such conflicts stall pull requests\nand continuous integration pipelines for hours to several days, seriously\nhurting developer productivity.\n</p>\n<p>In this paper, we introduce MergeBERT, a novel neural program merge framework\nbased on the token-level three-way differencing and a transformer encoder\nmodel. Exploiting restricted nature of merge conflict resolutions, we\nreformulate the task of generating the resolution sequence as a classification\ntask over a set of primitive merge patterns extracted from real-world merge\ncommit data.\n</p>\n<p>Our model achieves 64--69% precision of merge resolution synthesis, yielding\nnearly a 2x performance improvement over existing structured and neural program\nmerge tools. Finally, we demonstrate versatility of our model, which is able to\nperform program merge in a multilingual setting with Java, JavaScript,\nTypeScript, and C# programming languages, generalizing zero-shot to unseen\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mytcowicz_T/0/1/0/all/0/1\">Todd Mytcowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Negar Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhoury_S/0/1/0/all/0/1\">Sarah Fakhoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinella_E/0/1/0/all/0/1\">Elizabeth Dinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_C/0/1/0/all/0/1\">Christian Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1\">Shuvendu Lahiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It's not Rocket Science : Interpreting Figurative Language in Narratives. (arXiv:2109.00087v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00087","description":"<p>Figurative language is ubiquitous in English. Yet, the vast majority of NLP\nresearch focuses on literal language. Existing text representations by design\nrely on compositionality, while figurative language is often non-compositional.\nIn this paper, we study the interpretation of two non-compositional figurative\nlanguages (idioms and similes). We collected datasets of fictional narratives\ncontaining a figurative expression along with crowd-sourced plausible and\nimplausible continuations relying on the correct interpretation of the\nexpression. We then trained models to choose or generate the plausible\ncontinuation. Our experiments show that models based solely on pre-trained\nlanguage models perform substantially worse than humans on these tasks. We\nadditionally propose knowledge-enhanced models, adopting human strategies for\ninterpreting figurative language: inferring meaning from the context and\nrelying on the constituent word's literal meanings. The knowledge-enhanced\nmodels improve the performance on both the discriminative and generative tasks,\nfurther bridging the gap from human performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00122","description":"<p>The sheer volume of financial statements makes it difficult for humans to\naccess and analyze a business's financials. Robust numerical reasoning likewise\nfaces unique challenges in this domain. In this work, we focus on answering\ndeep questions over financial data, aiming to automate the analysis of a large\ncorpus of financial documents. In contrast to existing tasks on general domain,\nthe finance domain includes complex numerical reasoning and understanding of\nheterogeneous representations. To facilitate analytical progress, we propose a\nnew large-scale dataset, FinQA, with Question-Answering pairs over Financial\nreports, written by financial experts. We also annotate the gold reasoning\nprograms to ensure full explainability. We further introduce baselines and\nconduct comprehensive experiments in our dataset. The results demonstrate that\npopular, large, pre-trained models fall far short of expert humans in acquiring\nfinance knowledge and in complex multi-step numerical reasoning on that\nknowledge. Our dataset -- the first of its kind -- should therefore enable\nsignificant, new community research into complex application domains. The\ndataset and code are publicly available\\url{https://github.com/czyssrs/FinQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smiley_C/0/1/0/all/0/1\">Charese Smiley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Sameena Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borova_I/0/1/0/all/0/1\">Iana Borova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langdon_D/0/1/0/all/0/1\">Dylan Langdon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussa_R/0/1/0/all/0/1\">Reema Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beane_M/0/1/0/all/0/1\">Matt Beane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Routledge_B/0/1/0/all/0/1\">Bryan Routledge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Unsupervised Method for Building Sentence Simplification Corpora in Multiple Languages. (arXiv:2109.00165v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00165","description":"<p>The availability of parallel sentence simplification (SS) is scarce for\nneural SS modelings. We propose an unsupervised method to build SS corpora from\nlarge-scale bilingual translation corpora, alleviating the need for SS\nsupervised corpora. Our method is motivated by the following two findings:\nneural machine translation model usually tends to generate more high-frequency\ntokens and the difference of text complexity levels exists between the source\nand target language of a translation corpus. By taking the pair of the source\nsentences of translation corpus and the translations of their references in a\nbridge language, we can construct large-scale pseudo parallel SS data. Then, we\nkeep these sentence pairs with a higher complexity difference as SS sentence\npairs. The building SS corpora with an unsupervised approach can satisfy the\nexpectations that the aligned sentences preserve the same meanings and have\ndifference in text complexity levels. Experimental results show that SS methods\ntrained by our corpora achieve the state-of-the-art results and significantly\noutperform the results on English benchmark WikiLarge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xinyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1\">Jipeng Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yunhao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yi Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Have Been Learned & What Should Be Learned? An Empirical Study of How to Selectively Augment Text for Classification. (arXiv:2109.00175v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00175","description":"<p>Text augmentation techniques are widely used in text classification problems\nto improve the performance of classifiers, especially in low-resource\nscenarios. Whilst lots of creative text augmentation methods have been\ndesigned, they augment the text in a non-selective manner, which means the less\nimportant or noisy words have the same chances to be augmented as the\ninformative words, and thereby limits the performance of augmentation. In this\nwork, we systematically summarize three kinds of role keywords, which have\ndifferent functions for text classification, and design effective methods to\nextract them from the text. Based on these extracted role keywords, we propose\nSTA (Selective Text Augmentation) to selectively augment the text, where the\ninformative, class-indicating words are emphasized but the irrelevant or noisy\nwords are diminished. Extensive experiments on four English and Chinese text\nclassification benchmark datasets demonstrate that STA can substantially\noutperform the non-selective text augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Biyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Sonqiao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hailiang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Problem Learning: Towards the Free Will of Machines. (arXiv:2109.00177v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00177","description":"<p>A machine intelligence pipeline usually consists of six components: problem,\nrepresentation, model, loss, optimizer and metric. Researchers have worked hard\ntrying to automate many components of the pipeline. However, one key component\nof the pipeline--problem definition--is still left mostly unexplored in terms\nof automation. Usually, it requires extensive efforts from domain experts to\nidentify, define and formulate important problems in an area. However,\nautomatically discovering research or application problems for an area is\nbeneficial since it helps to identify valid and potentially important problems\nhidden in data that are unknown to domain experts, expand the scope of tasks\nthat we can do in an area, and even inspire completely new findings.\n</p>\n<p>This paper describes Problem Learning, which aims at learning to discover and\ndefine valid and ethical problems from data or from the machine's interaction\nwith the environment. We formalize problem learning as the identification of\nvalid and ethical problems in a problem space and introduce several possible\napproaches to problem learning. In a broader sense, problem learning is an\napproach towards the free will of intelligent machines. Currently, machines are\nstill limited to solving the problems defined by humans, without the ability or\nflexibility to freely explore various possible problems that are even unknown\nto humans. Though many machine learning techniques have been developed and\nintegrated into intelligent systems, they still focus on the means rather than\nthe purpose in that machines are still solving human defined problems. However,\nproposing good problems is sometimes even more important than solving problems,\nbecause a good problem can help to inspire new ideas and gain deeper\nunderstandings. The paper also discusses the ethical implications of problem\nlearning under the background of Responsible AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues. (arXiv:2109.00185v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00185","description":"<p>We present an effective system adapted from the end-to-end neural coreference\nresolution model, targeting on the task of anaphora resolution in dialogues.\nThree aspects are specifically addressed in our approach, including the support\nof singletons, encoding speakers and turns throughout dialogue interactions,\nand knowledge transfer utilizing existing resources. Despite the simplicity of\nour adaptation strategies, they are shown to bring significant impact to the\nfinal performance, with up to 27 F1 improvement over the baseline. Our final\nsystem ranks the 1st place on the leaderboard of the anaphora resolution track\nin the CRAC 2021 shared task, and achieves the best evaluation results on all\nfour datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Predictive Uncertainty under Distributional Shift on Dialogue Dataset. (arXiv:2109.00186v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00186","description":"<p>In open-domain dialogues, predictive uncertainties are mainly evaluated in a\ndomain shift setting to cope with out-of-distribution inputs. However, in\nreal-world conversations, there could be more extensive distributional shifted\ninputs than the out-of-distribution. To evaluate this, we first propose two\nmethods, Unknown Word (UW) and Insufficient Context (IC), enabling gradual\ndistributional shifts by corruption on the dialogue dataset. We then\ninvestigate the effect of distributional shifts on accuracy and calibration.\nOur experiments show that the performance of existing uncertainty estimation\nmethods consistently degrades with intensifying the shift. The results suggest\nthat the proposed methods could be useful for evaluating the calibration of\ndialogue systems under distributional shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nyoungwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">ChaeHun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Ho-Jin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation. (arXiv:2109.00194v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00194","description":"<p>Recent multilingual pre-trained language models have achieved remarkable\nzero-shot performance, where the model is only finetuned on one source language\nand directly evaluated on target languages. In this work, we propose a\nself-learning framework that further utilizes unlabeled data of target\nlanguages, combined with uncertainty estimation in the process to select\nhigh-quality silver labels. Three different uncertainties are adapted and\nanalyzed specifically for the cross lingual transfer: Language\nHeteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty\n(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks\nincluding Named Entity Recognition (NER) and Natural Language Inference (NLI)\ncovering 40 languages in total, which outperforms the baselines significantly\nby 10 F1 on average for NER and 2.5 accuracy score for NLI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xujiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho D. Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v1 [cs.IR])","link":"http://arxiv.org/abs/2109.00199","description":"<p>We describe a rule-based approach for the automatic acquisition of scientific\nentities from scholarly article titles. Two observations motivated the\napproach: (i) noting the concentration of an article's contribution information\nin its title; and (ii) capturing information pattern regularities via a system\nof rules that alleviate the human annotation task in creating gold standards\nthat annotate single instances at a time. We identify a set of lexico-syntactic\npatterns that are easily recognizable, that occur frequently, and that\ngenerally indicates the scientific entity type of interest about the scholarly\ncontribution.\n</p>\n<p>A subset of the acquisition algorithm is implemented for article titles in\nthe Computational Linguistics (CL) scholarly domain. The tool called\nORKG-Title-Parser, in its first release, identifies the following six concept\ntypes of scientific terminology from the CL paper titles, viz. research\nproblem, solution, resource, language, tool, and method. It has been\nempirically evaluated on a collection of 50,237 titles that cover nearly all\narticles in the ACL Anthology. It has extracted 19,799 research problems;\n18,111 solutions; 20,033 resources; 1,059 languages; 6,878 tools; and 21,687\nmethods at an average extraction precision of 75%. The code and related data\nresources are publicly available at\nhttps://gitlab.com/TIBHannover/orkg/orkg-title-parser.\n</p>\n<p>Finally, in the article, we discuss extensions and applications to areas such\nas scholarly knowledge graph (SKG) creation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">Soeren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset for Identification of Homophobia and Transophobia in Multilingual YouTube Comments. (arXiv:2109.00227v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00227","description":"<p>The increased proliferation of abusive content on social media platforms has\na negative impact on online users. The dread, dislike, discomfort, or mistrust\nof lesbian, gay, transgender or bisexual persons is defined as\nhomophobia/transphobia. Homophobic/transphobic speech is a type of offensive\nlanguage that may be summarized as hate speech directed toward LGBT+ people,\nand it has been a growing concern in recent years. Online\nhomophobia/transphobia is a severe societal problem that can make online\nplatforms poisonous and unwelcome to LGBT+ people while also attempting to\neliminate equality, diversity, and inclusion. We provide a new hierarchical\ntaxonomy for online homophobia and transphobia, as well as an expert-labelled\ndataset that will allow homophobic/transphobic content to be automatically\nidentified. We educated annotators and supplied them with comprehensive\nannotation rules because this is a sensitive issue, and we previously\ndiscovered that untrained crowdsourcing annotators struggle with diagnosing\nhomophobia due to cultural and other prejudices. The dataset comprises 15,141\nannotated multilingual comments. This paper describes the process of building\nthe dataset, qualitative analysis of data, and inter-annotator agreement. In\naddition, we create baseline models for the dataset. To the best of our\nknowledge, our dataset is the first such dataset created. Warning: This paper\ncontains explicit statements of homophobia, transphobia, stereotypes which may\nbe distressing to some readers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponnusamy_R/0/1/0/all/0/1\">Rahul Ponnusamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumaresan_P/0/1/0/all/0/1\">Prasanna Kumar Kumaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_K/0/1/0/all/0/1\">Kayalvizhi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thenmozhi_D/0/1/0/all/0/1\">Durairaj Thenmozhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thangasamy_S/0/1/0/all/0/1\">Sathiyaraj Thangasamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallathambi_R/0/1/0/all/0/1\">Rajendran Nallathambi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1\">John Phillip McCrae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OptAGAN: Entropy-based finetuning on text VAE-GAN. (arXiv:2109.00239v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00239","description":"<p>Transfer learning through large pre-trained models has changed the landscape\nof current applications in natural language processing (NLP). Recently Optimus,\na variational autoencoder (VAE) which combines two pre-trained models, BERT and\nGPT-2, has been released, and its combination with generative adversial\nnetworks (GANs) has been shown to produce novel, yet very human-looking text.\nThe Optimus and GANs combination avoids the troublesome application of GANs to\nthe discrete domain of text, and prevents the exposure bias of standard maximum\nlikelihood methods. We combine the training of GANs in the latent space, with\nthe finetuning of the decoder of Optimus for single word generation. This\napproach lets us model both the high-level features of the sentences, and the\nlow-level word-by-word generation. We finetune using reinforcement learning\n(RL) by exploiting the structure of GPT-2 and by adding entropy-based\nintrinsically motivated rewards to balance between quality and diversity. We\nbenchmark the results of the VAE-GAN model, and show the improvements brought\nby our RL finetuning on three widely used datasets for text generation, with\nresults that greatly surpass the current state-of-the-art for the quality of\nthe generated texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tirotta_P/0/1/0/all/0/1\">Paolo Tirotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lodi_S/0/1/0/all/0/1\">Stefano Lodi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning Cross-lingual Sentence Representations with Dual Momentum Contrast. (arXiv:2109.00253v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00253","description":"<p>In this paper, we propose to align sentence representations from different\nlanguages into a unified embedding space, where semantic similarities (both\ncross-lingual and monolingual) can be computed with a simple dot product.\nPre-trained language models are fine-tuned with the translation ranking task.\nExisting work (Feng et al., 2020) uses sentences within the same batch as\nnegatives, which can suffer from the issue of easy negatives. We adapt MoCo (He\net al., 2020) to further improve the quality of alignment. As the experimental\nresults show, the sentence representations produced by our model achieve the\nnew state-of-the-art on several tasks, including Tatoeba en-zh similarity\nsearch (Artetxe and Schwenk, 2019b), BUCC en-zh bitext mining, and semantic\ntextual similarity on 7 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting all Aspect-polarity Pairs Jointly in a Text with Relation Extraction Approach. (arXiv:2109.00256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00256","description":"<p>Extracting aspect-polarity pairs from texts is an important task of\nfine-grained sentiment analysis. While the existing approaches to this task\nhave gained many progresses, they are limited at capturing relationships among\naspect-polarity pairs in a text, thus degrading the extraction performance.\nMoreover, the existing state-of-the-art approaches, namely token-based\nse-quence tagging and span-based classification, have their own defects such as\npolarity inconsistency resulted from separately tagging tokens in the former\nand the heterogeneous categorization in the latter where aspect-related and\npolarity-related labels are mixed. In order to remedy the above defects,\nin-spiring from the recent advancements in relation extraction, we propose to\ngenerate aspect-polarity pairs directly from a text with relation extraction\ntechnology, regarding aspect-pairs as unary relations where aspects are\nenti-ties and the corresponding polarities are relations. Based on the\nperspective, we present a position- and aspect-aware sequence2sequence model\nfor joint extraction of aspect-polarity pairs. The model is characterized with\nits ability to capture not only relationships among aspect-polarity pairs in a\ntext through the sequence decoding, but also correlations between an aspect and\nits polarity through the position- and aspect-aware attentions. The\nexperi-ments performed on three benchmark datasets demonstrate that our model\noutperforms the existing state-of-the-art approaches, making significant\nim-provement over them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1\">Lingmei Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongmei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhonghua Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Context and High-Coverage Grammar for Conversational Question Answering over Knowledge Graphs. (arXiv:2109.00269v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00269","description":"<p>We tackle the problem of weakly-supervised conversational Question Answering\nover large Knowledge Graphs using a neural semantic parsing approach. We\nintroduce a new Logical Form (LF) grammar that can model a wide range of\nqueries on the graph while remaining sufficiently simple to generate\nsupervision data efficiently. Our Transformer-based model takes a JSON-like\nstructure as input, allowing us to easily incorporate both Knowledge Graph and\nconversational contexts. This structured input is transformed to lists of\nembeddings and then fed to standard attention layers. We validate our approach,\nboth in terms of grammar coverage and LF execution accuracy, on two publicly\navailable datasets, CSQA and ConvQuestions, both grounded in Wikidata. On CSQA,\nour approach increases the coverage from $80\\%$ to $96.2\\%$, and the LF\nexecution accuracy from $70.6\\%$ to $75.6\\%$, with respect to previous\nstate-of-the-art results. On ConvQuestions, we achieve competitive results with\nrespect to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marion_P/0/1/0/all/0/1\">Pierre Marion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowak_P/0/1/0/all/0/1\">Pawe&#x142; Krzysztof Nowak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccinno_F/0/1/0/all/0/1\">Francesco Piccinno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Representation Sprachbund For Multilingual Pre-Training. (arXiv:2109.00271v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00271","description":"<p>Multilingual pre-trained models have demonstrated their effectiveness in many\nmultilingual NLP tasks and enabled zero-shot or few-shot transfer from\nhigh-resource languages to low resource ones. However, due to significant\ntypological differences and contradictions between some languages, such models\nusually perform poorly on many languages and cross-lingual settings, which\nshows the difficulty of learning a single model to handle massive diverse\nlanguages well at the same time. To alleviate this issue, we present a new\nmultilingual pre-training pipeline. We propose to generate language\nrepresentation from multilingual pre-trained models and conduct linguistic\nanalysis to show that language representation similarity reflect linguistic\nsimilarity from multiple perspectives, including language family, geographical\nsprachbund, lexicostatistics and syntax. Then we cluster all the target\nlanguages into multiple groups and name each group as a representation\nsprachbund. Thus, languages in the same representation sprachbund are supposed\nto boost each other in both pre-training and fine-tuning as they share rich\nlinguistic similarity. We pre-train one multilingual model for each\nrepresentation sprachbund. Experiments are conducted on cross-lingual\nbenchmarks and significant improvements are achieved compared to strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yimin Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discourse Analysis of Covid-19 in Persian Twitter Social Networks Using Graph Mining and Natural Language Processing. (arXiv:2109.00298v1 [cs.SI])","link":"http://arxiv.org/abs/2109.00298","description":"<p>One of the new scientific ways of understanding discourse dynamics is\nanalyzing the public data of social networks. This research's aim is\nPost-structuralist Discourse Analysis (PDA) of Covid-19 phenomenon (inspired by\nLaclau and Mouffe's Discourse Theory) by using Intelligent Data Mining for\nPersian Society. The examined big data is five million tweets from 160,000\nusers of the Persian Twitter network to compare two discourses. Besides\nanalyzing the tweet texts individually, a social network graph database has\nbeen created based on retweets relationships. We use the VoteRank algorithm to\nintroduce and rank people whose posts become word of mouth, provided that the\ntotal information spreading scope is maximized over the network. These users\nare also clustered according to their word usage pattern (the Gaussian Mixture\nModel is used). The constructed discourse of influential spreaders is compared\nto the most active users. This analysis is done based on Covid-related posts\nover eight episodes. Also, by relying on the statistical content analysis and\npolarity of tweet words, discourse analysis is done for the whole mentioned\nsubpopulations, especially for the top individuals. The most important result\nof this research is that the Twitter subjects' discourse construction is\ngovernment-based rather than community-based. The analyzed Iranian society does\nnot consider itself responsible for the Covid-19 wicked problem, does not\nbelieve in participation, and expects the government to solve all problems. The\nmost active and most influential users' similarity is that political, national,\nand critical discourse construction is the predominant one. In addition to the\nadvantages of its research methodology, it is necessary to pay attention to the\nstudy's limitations. Suggestion for future encounters of Iranian society with\nsimilar crises is given.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shokrollahi_O/0/1/0/all/0/1\">Omid Shokrollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_N/0/1/0/all/0/1\">Niloofar Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1\">Mohammad Dehghani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$\\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00301","description":"<p>Transformers struggle when attending to long contexts, since the amount of\ncomputation grows with the context length, and therefore they cannot model\nlong-term memories effectively. Several variations have been proposed to\nalleviate this problem, but they all have a finite memory capacity, being\nforced to drop old information. In this paper, we propose the $\\infty$-former,\nwhich extends the vanilla transformer with an unbounded long-term memory. By\nmaking use of a continuous-space attention mechanism to attend over the\nlong-term memory, the $\\infty$-former's attention complexity becomes\nindependent of the context length. Thus, it is able to model arbitrarily long\ncontexts and maintain \"sticky memories\" while keeping a fixed computation\nbudget. Experiments on a synthetic sorting task demonstrate the ability of the\n$\\infty$-former to retain information from long sequences. We also perform\nexperiments on language modeling, by training a model from scratch and by\nfine-tuning a pre-trained language model, which show benefits of unbounded\nlong-term memories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring deep learning methods for recognizing rare diseases and their clinical manifestations from texts. (arXiv:2109.00343v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00343","description":"<p>Although rare diseases are characterized by low prevalence, approximately 300\nmillion people are affected by a rare disease. The early and accurate diagnosis\nof these conditions is a major challenge for general practitioners, who do not\nhave enough knowledge to identify them. In addition to this, rare diseases\nusually show a wide variety of manifestations, which might make the diagnosis\neven more difficult. A delayed diagnosis can negatively affect the patient's\nlife. Therefore, there is an urgent need to increase the scientific and medical\nknowledge about rare diseases. Natural Language Processing (NLP) and Deep\nLearning can help to extract relevant information about rare diseases to\nfacilitate their diagnosis and treatments. The paper explores the use of\nseveral deep learning techniques such as Bidirectional Long Short Term Memory\n(BiLSTM) networks or deep contextualized word representations based on\nBidirectional Encoder Representations from Transformers (BERT) to recognize\nrare diseases and their clinical manifestations (signs and symptoms) in the\nRareDis corpus. This corpus contains more than 5,000 rare diseases and almost\n6,000 clinical manifestations. BioBERT, a domain-specific language\nrepresentation based on BERT and trained on biomedical corpora, obtains the\nbest results. In particular, this model obtains an F1-score of 85.2% for rare\ndiseases, outperforming all the other models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1\">Isabel Segura-Bedmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camino_Perdonas_D/0/1/0/all/0/1\">David Camino-Perdonas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_Aspizua_S/0/1/0/all/0/1\">Sara Guerrero-Aspizua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConRPG: Paraphrase Generation using Contexts as Regularizer. (arXiv:2109.00363v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00363","description":"<p>A long-standing issue with paraphrase generation is how to obtain reliable\nsupervision signals. In this paper, we propose an unsupervised paradigm for\nparaphrase generation based on the assumption that the probabilities of\ngenerating two sentences with the same meaning given the same context should be\nthe same. Inspired by this fundamental idea, we propose a pipelined system\nwhich consists of paraphrase candidate generation based on contextual language\nmodels, candidate filtering using scoring functions, and paraphrase model\ntraining based on the selected candidates. The proposed paradigm offers merits\nover existing paraphrase generation methods: (1) using the context regularizer\non meanings, the model is able to generate massive amounts of high-quality\nparaphrase pairs; and (2) using human-interpretable scoring functions to select\nparaphrase pairs from candidates, the proposed framework provides a channel for\ndevelopers to intervene with the data generation process, leading to a more\ncontrollable model. Experimental results across different tasks and datasets\ndemonstrate that the effectiveness of the proposed model in both supervised and\nunsupervised setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1\">Qinghong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+fan_C/0/1/0/all/0/1\">Chun fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chronic Pain and Language: A Topic Modelling Approach to Personal Pain Descriptions. (arXiv:2109.00402v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00402","description":"<p>Chronic pain is recognized as a major health problem, with impacts not only\nat the economic, but also at the social, and individual levels. Being a private\nand subjective experience, it is impossible to externally and impartially\nexperience, describe, and interpret chronic pain as a purely noxious stimulus\nthat would directly point to a causal agent and facilitate its mitigation,\ncontrary to acute pain, the assessment of which is usually straightforward.\nVerbal communication is, thus, key to convey relevant information to health\nprofessionals that would otherwise not be accessible to external entities,\nnamely, intrinsic qualities about the painful experience and the patient. We\npropose and discuss a topic modelling approach to recognize patterns in verbal\ndescriptions of chronic pain, and use these patterns to quantify and qualify\nexperiences of pain. Our approaches allow for the extraction of novel insights\non chronic pain experiences from the obtained topic models and latent spaces.\nWe argue that our results are clinically relevant for the assessment and\nmanagement of chronic pain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nunes_D/0/1/0/all/0/1\">Diogo A. P. Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matos_D/0/1/0/all/0/1\">David Martins de Matos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomes_J/0/1/0/all/0/1\">Joana Ferreira Gomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_F/0/1/0/all/0/1\">Fani Neto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis. (arXiv:2109.00412v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00412","description":"<p>In multimodal sentiment analysis (MSA), the performance of a model highly\ndepends on the quality of synthesized embeddings. These embeddings are\ngenerated from the upstream process called multimodal fusion, which aims to\nextract and combine the input unimodal raw data to produce a richer multimodal\nrepresentation. Previous work either back-propagates the task loss or\nmanipulates the geometric property of feature spaces to produce favorable\nfusion results, which neglects the preservation of critical task-related\ninformation that flows from input to the fusion results. In this work, we\npropose a framework named MultiModal InfoMax (MMIM), which hierarchically\nmaximizes the Mutual Information (MI) in unimodal input pairs (inter-modality)\nand between multimodal fusion result and unimodal input in order to maintain\ntask-related information through multimodal fusion. The framework is jointly\ntrained with the main task (MSA) to improve the performance of the downstream\nMSA task. To address the intractable issue of MI bounds, we further formulate a\nset of computationally simple parametric and non-parametric methods to\napproximate their truth value. Experimental results on the two widely used\ndatasets demonstrate the efficacy of our approach. The implementation of this\nwork is publicly available at\nhttps://github.com/declare-lab/Multimodal-Infomax.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Adversarial Generation for Neural Machine Translation. (arXiv:2109.00417v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00417","description":"<p>Attacking Neural Machine Translation models is an inherently combinatorial\ntask on discrete sequences, solved with approximate heuristics. Most methods\nuse the gradient to attack the model on each sample independently. Instead of\nmechanically applying the gradient, could we learn to produce meaningful\nadversarial attacks ? In contrast to existing approaches, we learn to attack a\nmodel by training an adversarial generator based on a language model. We\npropose the Masked Adversarial Generation (MAG) model, that learns to perturb\nthe translation model throughout the training process. The experiments show\nthat it improves the robustness of machine translation models, while being\nfaster than competing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Idrissi_B/0/1/0/all/0/1\">Badr Youbi Idrissi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00430","description":"<p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position Masking for Improved Layout-Aware Document Understanding. (arXiv:2109.00442v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00442","description":"<p>Natural language processing for document scans and PDFs has the potential to\nenormously improve the efficiency of business processes. Layout-aware word\nembeddings such as LayoutLM have shown promise for classification of and\ninformation extraction from such documents. This paper proposes a new\npre-training task called that can improve performance of layout-aware word\nembeddings that incorporate 2-D position embeddings. We compare models\npre-trained with only language masking against models pre-trained with both\nlanguage masking and position masking, and we find that position masking\nimproves performance by over 5% on a form understanding task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Anik Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finegan_Dollak_C/0/1/0/all/0/1\">Catherine Finegan-Dollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Ashish Verma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Stance Dynamics in Social Media: Open Challenges and Research Directions. (arXiv:2109.00475v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00475","description":"<p>Social media platforms provide a goldmine for mining public opinion on issues\nof wide societal interest. Opinion mining is a problem that can be\noperationalised by capturing and aggregating the stance of individual social\nmedia posts as supporting, opposing or being neutral towards the issue at hand.\nWhile most prior work in stance detection has investigated datasets with\nlimited time coverage, interest in investigating longitudinal datasets has\nrecently increased. Evolving dynamics in linguistic and behavioural patterns\nobserved in new data require in turn adapting stance detection systems to deal\nwith the changes. In this survey paper, we investigate the intersection between\ncomputational linguistics and the temporal evolution of human communication in\ndigital media. We perform a critical review in emerging research considering\ndynamics, exploring different semantic and pragmatic factors that impact\nlinguistic data in general, and stance particularly. We further discuss current\ndirections in capturing stance dynamics in social media. We organise the\nchallenges of dealing with stance dynamics, identify open challenges and\ndiscuss future directions in three key dimensions: utterance, context and\ninfluence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey of Low-Resource Machine Translation. (arXiv:2109.00486v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00486","description":"<p>We present a survey covering the state of the art in low-resource machine\ntranslation. There are currently around 7000 languages spoken in the world and\nalmost all language pairs lack significant resources for training machine\ntranslation models. There has been increasing interest in research addressing\nthe challenge of producing useful translation models when very little\ntranslated training data is available. We present a high level summary of this\ntopical field and provide an overview of best practices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bawden_R/0/1/0/all/0/1\">Rachel Bawden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barone_A/0/1/0/all/0/1\">Antonio Valerio Miceli Barone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helcl_J/0/1/0/all/0/1\">Jind&#x159;ich Helcl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Iterative Knowledge Transfer Network with Routing for Aspect-based Sentiment Analysis. (arXiv:2004.01935v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.01935","description":"<p>Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect\nterm extraction, opinion term extraction, and aspect-level sentiment\nclassification, which are typically handled in a separate or joint manner.\nHowever, previous approaches do not well exploit the interactive relations\namong three subtasks and do not pertinently leverage the easily available\ndocument-level labeled domain/sentiment knowledge, which restricts their\nperformances. To address these issues, we propose a novel Iterative\nMulti-Knowledge Transfer Network (IMKTN) for end-to-end ABSA. For one thing,\nthrough the interactive correlations between the ABSA subtasks, our IMKTN\ntransfers the task-specific knowledge from any two of the three subtasks to\nanother one at the token level by utilizing a well-designed routing algorithm,\nthat is, any two of the three subtasks will help the third one. For another,\nour IMKTN pertinently transfers the document-level knowledge, i.e.,\ndomain-specific and sentiment-related knowledge, to the aspect-level subtasks\nto further enhance the corresponding performance. Experimental results on three\nbenchmark datasets demonstrate the effectiveness and superiority of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker anonymisation using the McAdams coefficient. (arXiv:2011.01130v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2011.01130","description":"<p>Anonymisation has the goal of manipulating speech signals in order to degrade\nthe reliability of automatic approaches to speaker recognition, while\npreserving other aspects of speech, such as those relating to intelligibility\nand naturalness. This paper reports an approach to anonymisation that, unlike\nother current approaches, requires no training data, is based upon well-known\nsignal processing techniques and is both efficient and effective. The proposed\nsolution uses the McAdams coefficient to transform the spectral envelope of\nspeech signals. Results derived using common VoicePrivacy 2020 databases and\nprotocols show that random, optimised transformations can outperform competing\nsolutions in terms of anonymisation while causing only modest, additional\ndegradations to intelligibility, even in the case of a semi-informed privacy\nadversary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nautsch_A/0/1/0/all/0/1\">Andreas Nautsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Evolution of Word Order. (arXiv:2101.09579v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.09579","description":"<p>Most natural languages have a predominant or fixed word order. For example in\nEnglish the word order is usually Subject-Verb-Object. This work attempts to\nexplain this phenomenon as well as other typological findings regarding word\norder from a functional perspective. In particular, we examine whether fixed\nword order provides a functional advantage, explaining why these languages are\nprevalent. To this end, we consider an evolutionary model of language and\ndemonstrate, both theoretically and using genetic algorithms, that a language\nwith a fixed word order is optimal. We also show that adding information to the\nsentence, such as case markers and noun-verb distinction, reduces the need for\nfixed word order, in accordance with the typological findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rejwan_I/0/1/0/all/0/1\">Idan Rejwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1\">Avi Caciularu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emotion Dynamics in Movie Dialogues. (arXiv:2103.01345v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01345","description":"<p>Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hipson_W/0/1/0/all/0/1\">Will E. Hipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01834","description":"<p>Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte (for workflow infrastructure and NLP\nfunction processors) and Stave (for user interaction, visualization, and\nannotation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanxiong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukkittu_A/0/1/0/all/0/1\">Avinash Bukkittu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mansi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Atif Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhavi_S/0/1/0/all/0/1\">Swapnil Singhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zecong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoran Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08809","description":"<p>Determining coreference of concept mentions across multiple documents is a\nfundamental task in natural language understanding. Previous work on\ncross-document coreference resolution (CDCR) typically considers mentions of\nevents in the news, which seldom involve abstract technical concepts that are\nprevalent in science and technology. These complex concepts take diverse or\nambiguous forms and have many hierarchical levels of granularity (e.g., tasks\nand subtasks), posing challenges for CDCR. We present a new task of\nHierarchical CDCR (H-CDCR) with the goal of jointly inferring coreference\nclusters and hierarchy between them. We create SciCo, an expert-annotated\ndataset for H-CDCR in scientific papers, 3X larger than the prominent ECB+\nresource. We study strong baseline models that we customize for H-CDCR, and\nhighlight challenges for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-training for Spoken Language Understanding with Joint Textual and Phonetic Representation Learning. (arXiv:2104.10357v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.10357","description":"<p>In the traditional cascading architecture for spoken language understanding\n(SLU), it has been observed that automatic speech recognition errors could be\ndetrimental to the performance of natural language understanding. End-to-end\n(E2E) SLU models have been proposed to directly map speech input to desired\nsemantic frame with a single model, hence mitigating ASR error propagation.\nRecently, pre-training technologies have been explored for these E2E models. In\nthis paper, we propose a novel joint textual-phonetic pre-training approach for\nlearning spoken language representations, aiming at exploring the full\npotentials of phonetic information to improve SLU robustness to ASR errors. We\nexplore phoneme labels as high-level speech features, and design and compare\npre-training tasks based on conditional masked language model objectives and\ninter-sentence relation objectives. We also investigate the efficacy of\ncombining textual and phonetic information during fine-tuning. Experimental\nresults on spoken language understanding benchmarks, Fluent Speech Commands and\nSNIPS, show that the proposed approach significantly outperforms strong\nbaseline models and improves robustness of spoken language understanding to ASR\nerrors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qinglin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-NERD: A Few-Shot Named Entity Recognition Dataset. (arXiv:2105.07464v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07464","description":"<p>Recently, considerable literature has grown up around the theme of few-shot\nnamed entity recognition (NER), but little published benchmark data\nspecifically focused on the practical and challenging task. Current approaches\ncollect existing supervised NER datasets and re-organize them to the few-shot\nsetting for empirical study. These strategies conventionally aim to recognize\ncoarse-grained entity types with few examples, while in practice, most unseen\nentity types are fine-grained. In this paper, we present Few-NERD, a\nlarge-scale human-annotated few-shot NER dataset with a hierarchy of 8\ncoarse-grained and 66 fine-grained entity types. Few-NERD consists of 188,238\nsentences from Wikipedia, 4,601,160 words are included and each is annotated as\ncontext or a part of a two-level entity type. To the best of our knowledge,\nthis is the first few-shot NER dataset and the largest human-crafted NER\ndataset. We construct benchmark tasks with different emphases to\ncomprehensively assess the generalization capability of models. Extensive\nempirical results and analysis show that Few-NERD is challenging and the\nproblem requires further research. We make Few-NERD public at\nhttps://ningding97.github.io/fewnerd/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yulin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-Level Training for Non-Autoregressive Neural Machine Translation. (arXiv:2106.08122v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08122","description":"<p>In recent years, Neural Machine Translation (NMT) has achieved notable\nresults in various translation tasks. However, the word-by-word generation\nmanner determined by the autoregressive mechanism leads to high translation\nlatency of the NMT and restricts its low-latency applications.\nNon-Autoregressive Neural Machine Translation (NAT) removes the autoregressive\nmechanism and achieves significant decoding speedup through generating target\nwords independently and simultaneously. Nevertheless, NAT still takes the\nword-level cross-entropy loss as the training objective, which is not optimal\nbecause the output of NAT cannot be properly evaluated due to the multimodality\nproblem. In this article, we propose using sequence-level training objectives\nto train NAT models, which evaluate the NAT outputs as a whole and correlates\nwell with the real translation quality. Firstly, we propose training NAT models\nto optimize sequence-level evaluation metrics (e.g., BLEU) based on several\nnovel reinforcement algorithms customized for NAT, which outperforms the\nconventional method by reducing the variance of gradient estimation. Secondly,\nwe introduce a novel training objective for NAT models, which aims to minimize\nthe Bag-of-Ngrams (BoN) difference between the model output and the reference\nsentence. The BoN training objective is differentiable and can be calculated\nefficiently without doing any approximations. Finally, we apply a three-stage\ntraining strategy to combine these two methods to train the NAT model. We\nvalidate our approach on four translation tasks (WMT14 En$\\leftrightarrow$De,\nWMT16 En$\\leftrightarrow$Ro), which shows that our approach largely outperforms\nNAT baselines and achieves remarkable performance on all translation tasks. The\nsource code is available at https://github.com/ictnlp/Seq-NAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00956","description":"<p>Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Higher-Order Concurrency for Microcontrollers. (arXiv:2108.07805v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2108.07805","description":"<p>Programming microcontrollers involves low-level interfacing with hardware and\nperipherals that are concurrent and reactive. Such programs are typically\nwritten in a mixture of C and assembly using concurrent language extensions\n(like $\\texttt{FreeRTOS tasks}$ and $\\texttt{semaphores}$), resulting in\nunsafe, callback-driven, error-prone and difficult-to-maintain code.\n</p>\n<p>We address this challenge by introducing $\\texttt{SenseVM}$ - a\nbytecode-interpreted virtual machine that provides a message-passing based\n$\\textit{higher-order concurrency}$ model, originally introduced by Reppy, for\nmicrocontroller programming. This model treats synchronous operations as\nfirst-class values (called $\\texttt{Events}$) akin to the treatment of\nfirst-class functions in functional languages. This primarily allows the\nprogrammer to compose and tailor their own concurrency abstractions and,\nadditionally, abstracts away unsafe memory operations, common in shared-memory\nconcurrency models, thereby making microcontroller programs safer, composable\nand easier-to-maintain.\n</p>\n<p>Our VM is made portable via a low-level $\\textit{bridge}$ interface, built\natop the embedded OS - Zephyr. The bridge is implemented by all drivers and\ndesigned such that programming in response to a software message or a hardware\ninterrupt remains uniform and indistinguishable. In this paper we demonstrate\nthe features of our VM through an example, written in a Caml-like functional\nlanguage, running on the $\\texttt{nRF52840}$ and $\\texttt{STM32F4}$\nmicrocontrollers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1\">Abhiroop Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krook_R/0/1/0/all/0/1\">Robert Krook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_B/0/1/0/all/0/1\">Bo Joel Svensson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheeran_M/0/1/0/all/0/1\">Mary Sheeran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive fine-tuning of Transformers for Translation of low-resourced languages @LoResMT 2021. (arXiv:2108.08556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08556","description":"<p>This paper reports the Machine Translation (MT) systems submitted by the\nIIITT team for the English-&gt;Marathi and English-&gt;Irish language pairs LoResMT\n2021 shared task. The task focuses on getting exceptional translations for\nrather low-resourced languages like Irish and Marathi. We fine-tune IndicTrans,\na pretrained multilingual NMT model for English-&gt;Marathi, using external\nparallel corpus as input for additional training. We have used a pretrained\nHelsinki-NLP Opus MT English-&gt;Irish model for the latter language pair. Our\napproaches yield relatively promising results on the BLEU metrics. Under the\nteam name IIITT, our systems ranked 1, 1, and 2 in English-&gt;Marathi,\nIrish-&gt;English, and English-&gt;Irish, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puranik_K/0/1/0/all/0/1\">Karthik Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durairaj_T/0/1/0/all/0/1\">Thenmozhi Durairaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thamburaj_K/0/1/0/all/0/1\">Kingston Pal Thamburaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Wikipedia Article Quality in One Dimension by Extending ORES with Ordinal Regression. (arXiv:2108.10684v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10684","description":"<p>Organizing complex peer production projects and advancing scientific\nknowledge of open collaboration each depend on the ability to measure quality.\nArticle quality ratings on English language Wikipedia have been widely used by\nboth Wikipedia community members and academic researchers for purposes like\ntracking knowledge gaps and studying how political polarization shapes\ncollaboration. Even so, measuring quality presents many methodological\nchallenges. The most widely used systems use labels on discrete ordinal scales\nwhen assessing quality, but such labels can be inconvenient for statistics and\nmachine learning. Prior work handles this by assuming that different levels of\nquality are \"evenly spaced\" from one another. This assumption runs counter to\nintuitions about the relative degrees of effort needed to raise Wikipedia\nencyclopedia articles to different quality levels. Furthermore, models from\nprior work are fit to datasets that oversample high-quality articles. This\nlimits their accuracy for representative samples of articles or revisions. I\ndescribe a technique extending the Wikimedia Foundations' ORES article quality\nmodel to address these limitations. My method uses weighted ordinal regression\nmodels to construct one-dimensional continuous measures of quality. While\nscores from my technique and from prior approaches are correlated, my approach\nimproves accuracy for research datasets and provides evidence that the \"evenly\nspaced\" assumption is unfounded in practice on English Wikipedia. I conclude\nwith recommendations for using quality scores in future research and include\nthe full code, data, and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+TeBlunthuis_N/0/1/0/all/0/1\">Nathan TeBlunthuis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Why Intermediate-Task Fine-Tuning Works. (arXiv:2108.11696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11696","description":"<p>Supplementary Training on Intermediate Labeled-data Tasks (STILTs) is a\nwidely applied technique, which first fine-tunes the pretrained language models\non an intermediate task before on the target task of interest. While STILTs is\nable to further improve the performance of pretrained language models, it is\nstill unclear why and when it works. Previous research shows that those\nintermediate tasks involving complex inference, such as commonsense reasoning,\nwork especially well for RoBERTa. In this paper, we discover that the\nimprovement from an intermediate task could be orthogonal to it containing\nreasoning or other complex skills -- a simple real-fake discrimination task\nsynthesized by GPT2 can benefit diverse target tasks. We conduct extensive\nexperiments to study the impact of different factors on STILTs. These findings\nsuggest rethinking the role of intermediate fine-tuning in the STILTs pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Ting-Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chi-Jen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position-Invariant Truecasing with a Word-and-Character Hierarchical Recurrent Neural Network. (arXiv:2108.11943v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11943","description":"<p>Truecasing is the task of restoring the correct case (uppercase or lowercase)\nof noisy text generated either by an automatic system for speech recognition or\nmachine translation or by humans. It improves the performance of downstream NLP\ntasks such as named entity recognition and language modeling. We propose a\nfast, accurate and compact two-level hierarchical word-and-character-based\nrecurrent neural network model, the first of its kind for this problem. Using\nsequence distillation, we also address the problem of truecasing while ignoring\ntoken positions in the sentence, i.e. in a position-invariant manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">You-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13679","description":"<p>In this paper, we propose to formulate the task-oriented dialogue system as\nthe purely natural language generation task, so as to fully leverage the\nlarge-scale pre-trained models like GPT-2 and simplify complicated\ndelexicalization prepossessing. However, directly applying this method heavily\nsuffers from the dialogue entity inconsistency caused by the removal of\ndelexicalized tokens, as well as the catastrophic forgetting problem of the\npre-trained model during fine-tuning, leading to unsatisfactory performance. To\nalleviate these problems, we design a novel GPT-Adapter-CopyNet network, which\nincorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve\nbetter performance on transfer learning and dialogue entity generation.\nExperimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ\ndataset demonstrate that our proposed approach significantly outperforms\nbaseline models with a remarkable performance on automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Working Memory Connections for LSTM. (arXiv:2109.00020v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00020","description":"<p>Recurrent Neural Networks with Long Short-Term Memory (LSTM) make use of\ngating mechanisms to mitigate exploding and vanishing gradients when learning\nlong-term dependencies. For this reason, LSTMs and other gated RNNs are widely\nadopted, being the standard de facto for many sequence modeling tasks. Although\nthe memory cell inside the LSTM contains essential information, it is not\nallowed to influence the gating mechanism directly. In this work, we improve\nthe gate potential by including information coming from the internal cell\nstate. The proposed modification, named Working Memory Connection, consists in\nadding a learnable nonlinear projection of the cell content into the network\ngates. This modification can fit into the classical LSTM gates without any\nassumption on the underlying task, being particularly effective when dealing\nwith longer sequences. Previous research effort in this direction, which goes\nback to the early 2000s, could not bring a consistent improvement over vanilla\nLSTM. As part of this paper, we identify a key issue tied to previous\nconnections that heavily limits their effectiveness, hence preventing a\nsuccessful integration of the knowledge coming from the internal cell state. We\nshow through extensive experimental evaluation that Working Memory Connections\nconstantly improve the performance of LSTMs on a variety of tasks. Numerical\nresults suggest that the cell state contains useful information that is worth\nincluding in the gate structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DensePose 3D: Lifting Canonical Surface Maps of Articulated Objects to the Third Dimension. (arXiv:2109.00033v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00033","description":"<p>We tackle the problem of monocular 3D reconstruction of articulated objects\nlike humans and animals. We contribute DensePose 3D, a method that can learn\nsuch reconstructions in a weakly supervised fashion from 2D image annotations\nonly. This is in stark contrast with previous deformable reconstruction methods\nthat use parametric models such as SMPL pre-trained on a large dataset of 3D\nobject scans. Because it does not require 3D scans, DensePose 3D can be used\nfor learning a wide range of articulated categories such as different animal\nspecies. The method learns, in an end-to-end fashion, a soft partition of a\ngiven category-specific 3D template mesh into rigid parts together with a\nmonocular reconstruction network that predicts the part motions such that they\nreproject correctly onto 2D DensePose-like surface annotations of the object.\nThe decomposition of the object into parts is regularized by expressing part\nassignments as a combination of the smooth eigenfunctions of the\nLaplace-Beltrami operator. We show significant improvements compared to\nstate-of-the-art non-rigid structure-from-motion baselines on both synthetic\nand real data on categories of humans and animals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shapovalov_R/0/1/0/all/0/1\">Roman Shapovalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1\">Benjamin Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bio-inspired robot perception coupled with robot-modeled human perception. (arXiv:2109.00097v1 [cs.RO])","link":"http://arxiv.org/abs/2109.00097","description":"<p>My overarching research goal is to provide robots with perceptional abilities\nthat allow interactions with humans in a human-like manner. To develop these\nperceptional abilities, I believe that it is useful to study the principles of\nthe human visual system. I use these principles to develop new computer vision\nalgorithms and validate their effectiveness in intelligent robotic systems. I\nam enthusiastic about this approach as it offers the dual benefit of uncovering\nprinciples inherent in the human visual system, as well as applying these\nprinciples to its artificial counterpart. Fig. 1 contains a depiction of my\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1\">Tobias Fischer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-step Domain Adaptation for Mitosis Cell Detection in Histopathology Images. (arXiv:2109.00109v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00109","description":"<p>We propose a two-step domain shift-invariant mitosis cell detection method\nbased on Faster RCNN and a convolutional neural network (CNN). We generate\nvarious domain-shifted versions of existing histopathology images using a stain\naugmentation technique, enabling our method to effectively learn various stain\ndomains and achieve better generalization. The performance of our method is\nevaluated on the preliminary test data set of the MIDOG-2021 challenge. The\nexperimental results demonstrate that the proposed mitosis detection method can\nachieve promising performance for domain-shifted histopathology images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nateghi_R/0/1/0/all/0/1\">Ramin Nateghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pourakpour_F/0/1/0/all/0/1\">Fattaneh Pourakpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPFN: Cascaded Primitive Fitting Networks for High-Resolution Point Clouds. (arXiv:2109.00113v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00113","description":"<p>Representing human-made objects as a collection of base primitives has a long\nhistory in computer vision and reverse engineering. In the case of\nhigh-resolution point cloud scans, the challenge is to be able to detect both\nlarge primitives as well as those explaining the detailed parts. While the\nclassical RANSAC approach requires case-specific parameter tuning,\nstate-of-the-art networks are limited by memory consumption of their backbone\nmodules such as PointNet++, and hence fail to detect the fine-scale primitives.\nWe present Cascaded Primitive Fitting Networks (CPFN) that relies on an\nadaptive patch sampling network to assemble detection results of global and\nlocal primitive detection networks. As a key enabler, we present a merging\nformulation that dynamically aggregates the primitives across global and local\nscales. Our evaluation demonstrates that CPFN improves the state-of-the-art\nSPFN performance by 13-14% on high-resolution point cloud datasets and\nspecifically improves the detection of fine-scale primitives by 20-22%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_E/0/1/0/all/0/1\">Eric-Tuan L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceylan_D/0/1/0/all/0/1\">Duygu Ceylan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mech_R/0/1/0/all/0/1\">Radomir Mech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boubekeur_T/0/1/0/all/0/1\">Tamy Boubekeur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Quantified Deep Learning for Predicting Dice Coefficient of Digital Histopathology Image Segmentation. (arXiv:2109.00115v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00115","description":"<p>Deep learning models (DLMs) can achieve state of the art performance in\nmedical image segmentation and classification tasks. However, DLMs that do not\nprovide feedback for their predictions such as Dice coefficients (Dice) have\nlimited deployment potential in real world clinical settings. Uncertainty\nestimates can increase the trust of these automated systems by identifying\npredictions that need further review but remain computationally prohibitive to\ndeploy. In this study, we use a DLM with randomly initialized weights and Monte\nCarlo dropout (MCD) to segment tumors from microscopic Hematoxylin and Eosin\n(H&amp;E) dye stained prostate core biopsy RGB images. We devise a novel approach\nthat uses multiple clinical region based uncertainties from a single image\n(instead of the entire image) to predict Dice of the DLM model output by linear\nmodels. Image level uncertainty maps were generated and showed correspondence\nbetween imperfect model segmentation and high levels of uncertainty associated\nwith specific prostate tissue regions with or without tumors. Results from this\nstudy suggest that linear models can learn coefficients of uncertainty\nquantified deep learning and correlations ((Spearman's correlation (p&lt;0.05)) to\npredict Dice scores of specific regions of medical images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ghosal_S/0/1/0/all/0/1\">Sambuddha Ghosal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_A/0/1/0/all/0/1\">Audrey Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_P/0/1/0/all/0/1\">Pratik Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Multiview Coding with Electro-optics for SAR Semantic Segmentation. (arXiv:2109.00120v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00120","description":"<p>In the training of deep learning models, how the model parameters are\ninitialized greatly affects the model performance, sample efficiency, and\nconvergence speed. Representation learning for model initialization has\nrecently been actively studied in the remote sensing field. In particular, the\nappearance characteristics of the imagery obtained using the a synthetic\naperture radar (SAR) sensor are quite different from those of general\nelectro-optical (EO) images, and thus representation learning is even more\nimportant in remote sensing domain. Motivated from contrastive multiview\ncoding, we propose multi-modal representation learning for SAR semantic\nsegmentation. Unlike previous studies, our method jointly uses EO imagery, SAR\nimagery, and a label mask. Several experiments show that our approach is\nsuperior to the existing methods in model performance, sample efficiency, and\nconvergence speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_K/0/1/0/all/0/1\">Keumgang Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Junghoon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yeji Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPA: Learning Robust Physical Adversarial Camouflages for Object Detectors. (arXiv:2109.00124v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00124","description":"<p>Adversarial attacks are feasible in the real world for object detection.\nHowever, most of the previous works have tried to learn \"patches\" applied to an\nobject to fool detectors, which become less effective or even ineffective in\nsquint view angles. To address this issue, we propose the Dense Proposals\nAttack (DPA) to learn robust, physical and targeted adversarial camouflages for\ndetectors. The camouflages are robust because they remain adversarial when\nfilmed under arbitrary viewpoint and different illumination conditions,\nphysical because they function well both in the 3D virtual scene and the real\nworld, and targeted because they can cause detectors to misidentify an object\nas a specific target class. In order to make the generated camouflages robust\nin the physical world, we introduce a combination of viewpoint shifts, lighting\nand other natural transformations to model the physical phenomena. In addition,\nto improve the attacks, DPA substantially attacks all the classifications in\nthe fixed region proposals. Moreover, we build a virtual 3D scene using the\nUnity simulation engine to fairly and reproducibly evaluate different physical\nattacks. Extensive experiments demonstrate that DPA outperforms the\nstate-of-the-art methods significantly, and generalizes well to the real world,\nposing a potential threat to the security-critical computer vision systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yexin Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jialin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1\">Junhua Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhengyun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhisong Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Behavioral Cloning. (arXiv:2109.00137v1 [cs.RO])","link":"http://arxiv.org/abs/2109.00137","description":"<p>We find that across a wide range of robot policy learning scenarios, treating\nsupervised policy learning with an implicit model generally performs better, on\naverage, than commonly used explicit models. We present extensive experiments\non this finding, and we provide both intuitive insight and theoretical\narguments distinguishing the properties of implicit models compared to their\nexplicit counterparts, particularly with respect to approximating complex,\npotentially discontinuous and multi-valued (set-valued) functions. On robotic\npolicy learning tasks we show that implicit behavioral cloning policies with\nenergy-based models (EBM) often outperform common explicit (Mean Square Error,\nor Mixture Density) behavioral cloning policies, including on tasks with\nhigh-dimensional action spaces and visual image inputs. We find these policies\nprovide competitive results or outperform state-of-the-art offline\nreinforcement learning methods on the challenging human-expert tasks from the\nD4RL benchmark suite, despite using no reward information. In the real world,\nrobots with implicit policies can learn complex and remarkably subtle behaviors\non contact-rich tasks from human demonstrations, including tasks with high\ncombinatorial complexity and tasks requiring 1mm precision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lynch_C/0/1/0/all/0/1\">Corey Lynch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_O/0/1/0/all/0/1\">Oscar Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahid_A/0/1/0/all/0/1\">Ayzaan Wahid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downs_L/0/1/0/all/0/1\">Laura Downs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Adrian Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Johnny Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1\">Jonathan Tompson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eyes Tell All: Irregular Pupil Shapes Reveal GAN-generated Faces. (arXiv:2109.00162v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00162","description":"<p>Generative adversary network (GAN) generated high-realistic human faces have\nbeen used as profile images for fake social media accounts and are visually\nchallenging to discern from real ones. In this work, we show that GAN-generated\nfaces can be exposed via irregular pupil shapes. This phenomenon is caused by\nthe lack of physiological constraints in the GAN models. We demonstrate that\nsuch artifacts exist widely in high-quality GAN-generated faces and further\ndescribe an automatic method to extract the pupils from two eyes and analysis\ntheir shapes for exposing the GAN-generated faces. Qualitative and quantitative\nevaluations of our method suggest its simplicity and effectiveness in\ndistinguishing GAN-generated faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hui Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1\">Siwei Lyu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Architecture Aware Latency Constrained Sparse Neural Networks. (arXiv:2109.00170v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00170","description":"<p>Acceleration of deep neural networks to meet a specific latency constraint is\nessential for their deployment on mobile devices. In this paper, we design an\narchitecture aware latency constrained sparse (ALCS) framework to prune and\naccelerate CNN models. Taking modern mobile computation architectures into\nconsideration, we propose Single Instruction Multiple Data (SIMD)-structured\npruning, along with a novel sparse convolution algorithm for efficient\ncomputation. Besides, we propose to estimate the run time of sparse models with\npiece-wise linear interpolation. The whole latency constrained pruning task is\nformulated as a constrained optimization problem that can be efficiently solved\nwith Alternating Direction Method of Multipliers (ADMM). Extensive experiments\nshow that our system-algorithm co-design framework can achieve much better\nPareto frontier among network accuracy and latency on resource-constrained\nmobile devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianli Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qinghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weixiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_C/0/1/0/all/0/1\">Cong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Problem Learning: Towards the Free Will of Machines. (arXiv:2109.00177v1 [cs.AI])","link":"http://arxiv.org/abs/2109.00177","description":"<p>A machine intelligence pipeline usually consists of six components: problem,\nrepresentation, model, loss, optimizer and metric. Researchers have worked hard\ntrying to automate many components of the pipeline. However, one key component\nof the pipeline--problem definition--is still left mostly unexplored in terms\nof automation. Usually, it requires extensive efforts from domain experts to\nidentify, define and formulate important problems in an area. However,\nautomatically discovering research or application problems for an area is\nbeneficial since it helps to identify valid and potentially important problems\nhidden in data that are unknown to domain experts, expand the scope of tasks\nthat we can do in an area, and even inspire completely new findings.\n</p>\n<p>This paper describes Problem Learning, which aims at learning to discover and\ndefine valid and ethical problems from data or from the machine's interaction\nwith the environment. We formalize problem learning as the identification of\nvalid and ethical problems in a problem space and introduce several possible\napproaches to problem learning. In a broader sense, problem learning is an\napproach towards the free will of intelligent machines. Currently, machines are\nstill limited to solving the problems defined by humans, without the ability or\nflexibility to freely explore various possible problems that are even unknown\nto humans. Though many machine learning techniques have been developed and\nintegrated into intelligent systems, they still focus on the means rather than\nthe purpose in that machines are still solving human defined problems. However,\nproposing good problems is sometimes even more important than solving problems,\nbecause a good problem can help to inspire new ideas and gain deeper\nunderstandings. The paper also discusses the ethical implications of problem\nlearning under the background of Responsible AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongfeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-temporal Self-Supervised Representation Learning for 3D Point Clouds. (arXiv:2109.00179v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00179","description":"<p>To date, various 3D scene understanding tasks still lack practical and\ngeneralizable pre-trained models, primarily due to the intricate nature of 3D\nscene understanding tasks and their immense variations introduced by camera\nviews, lighting, occlusions, etc. In this paper, we tackle this challenge by\nintroducing a spatio-temporal representation learning (STRL) framework, capable\nof learning from unlabeled 3D point clouds in a self-supervised fashion.\nInspired by how infants learn from visual data in the wild, we explore the rich\nspatio-temporal cues derived from the 3D data. Specifically, STRL takes two\ntemporally-correlated frames from a 3D point cloud sequence as the input,\ntransforms it with the spatial data augmentation, and learns the invariant\nrepresentation self-supervisedly. To corroborate the efficacy of STRL, we\nconduct extensive experiments on three types (synthetic, indoor, and outdoor)\nof datasets. Experimental results demonstrate that, compared with supervised\nlearning methods, the learned self-supervised representation facilitates\nvarious models to attain comparable or even better performances while capable\nof generalizing pre-trained models to downstream tasks, including 3D shape\nclassification, 3D object detection, and 3D semantic segmentation. Moreover,\nthe spatio-temporal contextual cues embedded in 3D point clouds significantly\nimprove the learned representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yichen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptually Optimized Deep High-Dynamic-Range Image Tone Mapping. (arXiv:2109.00180v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00180","description":"<p>We describe a deep high-dynamic-range (HDR) image tone mapping operator that\nis computationally efficient and perceptually optimized. We first decompose an\nHDR image into a normalized {Laplacian} pyramid, and use two deep neural\nnetworks (DNNs) to estimate the {Laplacian} pyramid of the desired tone-mapped\nimage from the normalized representation. We then end-to-end optimize the\nentire method over a database of HDR images by minimizing the normalized\n{Laplacian} pyramid distance (NLPD), a recently proposed perceptual metric.\nQualitative and quantitative experiments demonstrate that our method produces\nimages with better visual quality, and runs the fastest among existing local\ntone mapping algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1\">Chenyang Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiebin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors. (arXiv:2109.00182v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00182","description":"<p>In this paper, we propose a novel local descriptor-based framework, called\nYou Only Hypothesize Once (YOHO), for the registration of two unaligned point\nclouds. In contrast to most existing local descriptors which rely on a fragile\nlocal reference frame to gain rotation invariance, the proposed descriptor\nachieves the rotation invariance by recent technologies of group equivariant\nfeature learning, which brings more robustness to point density and noise.\nMeanwhile, the descriptor in YOHO also has a rotation equivariant part, which\nenables us to estimate the registration from just one correspondence\nhypothesis. Such property reduces the searching space for feasible\ntransformations, thus greatly improves both the accuracy and the efficiency of\nYOHO. Extensive experiments show that YOHO achieves superior performances with\nmuch fewer needed RANSAC iterations on four widely-used datasets, the\n3DMatch/3DLoMatch datasets, the ETH dataset and the WHU-TLS dataset. More\ndetails are shown in our project page: https://hpwang-whu.github.io/YOHO/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bisheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study on the Joint Impact of Feature Selection and Data Resampling on Imbalance Classification. (arXiv:2109.00201v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00201","description":"<p>Real-world datasets often present different degrees of imbalanced (i.e.,\nlong-tailed or skewed) distributions. While the majority (a.k.a., head or\nfrequent) classes have sufficient samples, the minority (a.k.a., tail or rare)\nclasses can be under-represented by a rather limited number of samples. On one\nhand, data resampling is a common approach to tackling class imbalance. On the\nother hand, dimension reduction, which reduces the feature space, is a\nconventional machine learning technique for building stronger classification\nmodels on a dataset. However, the possible synergy between feature selection\nand data resampling for high-performance imbalance classification has rarely\nbeen investigated before. To address this issue, this paper carries out a\ncomprehensive empirical study on the joint influence of feature selection and\nresampling on two-class imbalance classification. Specifically, we study the\nperformance of two opposite pipelines for imbalance classification, i.e.,\napplying feature selection before or after data resampling. We conduct a large\namount of experiments (a total of 9225 experiments) on 52 publicly available\ndatasets, using 9 feature selection methods, 6 resampling approaches for class\nimbalance learning, and 3 well-known classification algorithms. Experimental\nresults show that there is no constant winner between the two pipelines, thus\nboth of them should be considered to derive the best performing model for\nimbalance classification. We also find that the performance of an imbalance\nclassification model depends on the classifier adopted, the ratio between the\nnumber of majority and minority samples (IR), as well as on the ratio between\nthe number of samples and features (SFR). Overall, this study should provide\nnew reference value for researchers and practitioners in imbalance learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soda_P/0/1/0/all/0/1\">Paolo Soda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jingjun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1\">Gaojuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almpanidis_G/0/1/0/all/0/1\">George Almpanidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1\">Salvador Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventPoint: Self-Supervised Local Descriptor Learning for Event Cameras. (arXiv:2109.00210v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00210","description":"<p>We proposes a method of extracting intrest points and descriptors using\nself-supervised learning method on frame-based event data, which is called\nEventPoint. Different from other feature extraction methods on event data, we\ntrain our model on real event-form driving dataset--DSEC with the\nself-supervised learning method we proposed, the training progress fully\nconsider the characteristics of event data.To verify the effectiveness of our\nwork,we conducted several complete evaluations: we emulated DART and carried\nout feature matching experiments on N-caltech101 dataset, the results shows\nthat the effect of EventPoint is better than DART; We use Vid2e tool provided\nby UZH to convert Oxford robotcar data into event-based format, and combined\nwith INS information provided to carry out the global pose estimation\nexperiment which is important in SLAM. As far as we know, this is the first\nwork to carry out this challenging task.Sufficient experimental data show that\nEventPoint can get better results while achieve real time on CPU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Ze Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Songzhi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Henry Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kevin Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Person Search: An Anchor-Free Approach. (arXiv:2109.00211v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00211","description":"<p>Person search aims to simultaneously localize and identify a query person\nfrom realistic, uncropped images. To achieve this goal, state-of-the-art models\ntypically add a re-id branch upon two-stage detectors like Faster R-CNN. Owing\nto the ROI-Align operation, this pipeline yields promising accuracy as re-id\nfeatures are explicitly aligned with the corresponding object regions, but in\nthe meantime, it introduces high computational overhead due to dense object\nanchors. In this work, we present an anchor-free approach to efficiently\ntackling this challenging task, by introducing the following dedicated designs.\nFirst, we select an anchor-free detector (i.e., FCOS) as the prototype of our\nframework. Due to the lack of dense object anchors, it exhibits significantly\nhigher efficiency compared with existing person search models. Second, when\ndirectly accommodating this anchor-free detector for person search, there exist\nseveral major challenges in learning robust re-id features, which we summarize\nas the misalignment issues in different levels (i.e., scale, region, and task).\nTo address these issues, we propose an aligned feature aggregation module to\ngenerate more discriminative and robust feature embeddings. Accordingly, we\nname our model as Feature-Aligned Person Search Network (AlignPS). Third, by\ninvestigating the advantages of both anchor-based and anchor-free models, we\nfurther augment AlignPS with an ROI-Align head, which significantly improves\nthe robustness of re-id features while still keeping our model highly\nefficient. Extensive experiments conducted on two challenging benchmarks (i.e.,\nCUHK-SYSU and PRW) demonstrate that our framework achieves state-of-the-art or\ncompetitive performance, while displaying higher efficiency. All the source\ncodes, data, and trained models are available at:\nhttps://github.com/daodaofr/alignps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yichao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diverse Sample Generation: Pushing the Limit of Data-free Quantization. (arXiv:2109.00212v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00212","description":"<p>Recently, generative data-free quantization emerges as a practical approach\nthat compresses the neural network to low bit-width without access to real\ndata. It generates data to quantize the network by utilizing the batch\nnormalization (BN) statistics of its full-precision counterpart. However, our\nstudy shows that in practice, the synthetic data completely constrained by BN\nstatistics suffers severe homogenization at distribution and sample level,\nwhich causes serious accuracy degradation of the quantized network. This paper\npresents a generic Diverse Sample Generation (DSG) scheme for the generative\ndata-free post-training quantization and quantization-aware training, to\nmitigate the detrimental homogenization. In our DSG, we first slack the\nstatistics alignment for features in the BN layer to relax the distribution\nconstraint. Then we strengthen the loss impact of the specific BN layer for\ndifferent samples and inhibit the correlation among samples in the generation\nprocess, to diversify samples from the statistical and spatial perspective,\nrespectively. Extensive experiments show that for large-scale image\nclassification tasks, our DSG can consistently outperform existing data-free\nquantization methods on various neural architectures, especially under\nultra-low bit-width (e.g., 22% gain under W4A4 setting). Moreover, data\ndiversifying caused by our DSG brings a general gain in various quantization\nmethods, demonstrating diversity is an important property of high-quality\nsynthetic data for data-free quantization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Haotong Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yifu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiakai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Perturbations for Video Attribution. (arXiv:2109.00222v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00222","description":"<p>The attribution method provides a direction for interpreting opaque neural\nnetworks in a visual way by identifying and visualizing the input\nregions/pixels that dominate the output of a network. Regarding the attribution\nmethod for visually explaining video understanding networks, it is challenging\nbecause of the unique spatiotemporal dependencies existing in video inputs and\nthe special 3D convolutional or recurrent structures of video understanding\nnetworks. However, most existing attribution methods focus on explaining\nnetworks taking a single image as input and a few works specifically devised\nfor video attribution come short of dealing with diversified structures of\nvideo understanding networks. In this paper, we investigate a generic\nperturbation-based attribution method that is compatible with diversified video\nunderstanding networks. Besides, we propose a novel regularization term to\nenhance the method by constraining the smoothness of its attribution results in\nboth spatial and temporal dimensions. In order to assess the effectiveness of\ndifferent video attribution methods without relying on manual judgement, we\nintroduce reliable objective metrics which are checked by a newly proposed\nreliability measurement. We verified the effectiveness of our method by both\nsubjective and objective evaluation and comparison with multiple significant\nattribution methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuoyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Protection Method of Trained CNN Model Using Feature Maps Transformed With Secret Key From Unauthorized Access. (arXiv:2109.00224v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00224","description":"<p>In this paper, we propose a model protection method for convolutional neural\nnetworks (CNNs) with a secret key so that authorized users get a high\nclassification accuracy, and unauthorized users get a low classification\naccuracy. The proposed method applies a block-wise transformation with a secret\nkey to feature maps in the network. Conventional key-based model protection\nmethods cannot maintain a high accuracy when a large key space is selected. In\ncontrast, the proposed method not only maintains almost the same accuracy as\nnon-protected accuracy, but also has a larger key space. Experiments were\ncarried out on the CIFAR-10 dataset, and results show that the proposed model\nprotection method outperformed the previous key-based model protection methods\nin terms of classification accuracy, key space, and robustness against key\nestimation attacks and fine-tuning attacks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+AprilPyone_M/0/1/0/all/0/1\">MaungMaung AprilPyone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Graph Learning and Matching for Semantic Feature Correspondence. (arXiv:2109.00240v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00240","description":"<p>In recent years, powered by the learned discriminative representation via\ngraph neural network (GNN) models, deep graph matching methods have made great\nprogresses in the task of matching semantic features. However, these methods\nusually rely on heuristically generated graph patterns, which may introduce\nunreliable relationships to hurt the matching performance. In this paper, we\npropose a joint \\emph{graph learning and matching} network, named GLAM, to\nexplore reliable graph structures for boosting graph matching. GLAM adopts a\npure attention-based framework for both graph learning and graph matching.\nSpecifically, it employs two types of attention mechanisms, self-attention and\ncross-attention for the task. The self-attention discovers the relationships\nbetween features and to further update feature representations over the learnt\nstructures; and the cross-attention computes cross-graph correlations between\nthe two feature sets to be matched for feature reconstruction. Moreover, the\nfinal matching solution is directly derived from the output of the\ncross-attention layer, without employing a specific matching decision module.\nThe proposed method is evaluated on three popular visual matching benchmarks\n(Pascal VOC, Willow Object and SPair-71k), and it outperforms previous\nstate-of-the-art graph matching methods by significant margins on all\nbenchmarks. Furthermore, the graph patterns learnt by our model are validated\nto be able to remarkably enhance previous deep graph matching methods by\nreplacing their handcrafted graph structures with the learnt ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">He Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Congyan Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeing Implicit Neural Representations as Fourier Series. (arXiv:2109.00249v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00249","description":"<p>Implicit Neural Representations (INR) use multilayer perceptrons to represent\nhigh-frequency functions in low-dimensional problem domains. Recently these\nrepresentations achieved state-of-the-art results on tasks related to complex\n3D objects and scenes. A core problem is the representation of highly detailed\nsignals, which is tackled using networks with periodic activation functions\n(SIRENs) or applying Fourier mappings to the input. This work analyzes the\nconnection between the two methods and shows that a Fourier mapped perceptron\nis structurally like one hidden layer SIREN. Furthermore, we identify the\nrelationship between the previously proposed Fourier mapping and the general\nd-dimensional Fourier series, leading to an integer lattice mapping. Moreover,\nwe modify a progressive training strategy to work on arbitrary Fourier mappings\nand show that it improves the generalization of the interpolation task. Lastly,\nwe compare the different mappings on the image regression and novel view\nsynthesis tasks. We confirm the previous finding that the main contributor to\nthe mapping performance is the size of the embedding and standard deviation of\nits elements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1\">Nuri Benbarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofer_T/0/1/0/all/0/1\">Timon H&#xf6;fer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riaz_H/0/1/0/all/0/1\">Hamd ul-moqeet Riaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BVMatch: Lidar-based Place Recognition Using Bird's-eye View Images. (arXiv:2109.00317v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00317","description":"<p>Recognizing places using Lidar in large-scale environments is challenging due\nto the sparse nature of point cloud data. In this paper we present BVMatch, a\nLidar-based frame-to-frame place recognition framework, that is capable of\nestimating 2D relative poses. Based on the assumption that the ground area can\nbe approximated as a plane, we uniformly discretize the ground area into grids\nand project 3D Lidar scans to bird's-eye view (BV) images. We further use a\nbank of Log-Gabor filters to build a maximum index map (MIM) that encodes the\norientation information of the structures in the images. We analyze the\norientation characteristics of MIM theoretically and introduce a novel\ndescriptor called bird's-eye view feature transform (BVFT). The proposed BVFT\nis insensitive to rotation and intensity variations of BV images. Leveraging\nthe BVFT descriptors, we unify the Lidar place recognition and pose estimation\ntasks into the BVMatch framework. The experiments conducted on three\nlarge-scale datasets show that BVMatch outperforms the state-of-the-art methods\nin terms of both recall rate of place recognition and pose estimation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Si-Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hui-Liang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category-Level Metric Scale Object Shape and Pose Estimation. (arXiv:2109.00326v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00326","description":"<p>Advances in deep learning recognition have led to accurate object detection\nwith 2D images. However, these 2D perception methods are insufficient for\ncomplete 3D world information. Concurrently, advanced 3D shape estimation\napproaches focus on the shape itself, without considering metric scale. These\nmethods cannot determine the accurate location and orientation of objects. To\ntackle this problem, we propose a framework that jointly estimates a metric\nscale shape and pose from a single RGB image. Our framework has two branches:\nthe Metric Scale Object Shape branch (MSOS) and the Normalized Object\nCoordinate Space branch (NOCS). The MSOS branch estimates the metric scale\nshape observed in the camera coordinates. The NOCS branch predicts the\nnormalized object coordinate space (NOCS) map and performs similarity\ntransformation with the rendered depth map from a predicted metric scale mesh\nto obtain 6d pose and size. Additionally, we introduce the Normalized Object\nCenter Estimation (NOCE) to estimate the geometrically aligned distance from\nthe camera to the object center. We validated our method on both synthetic and\nreal-world datasets to evaluate category-level object pose and shape.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1\">Taeyeop Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byeong-Uk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Myungchul Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Free Generative Replay For Class-Incremental Learning. (arXiv:2109.00328v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00328","description":"<p>Regularization-based methods are beneficial to alleviate the catastrophic\nforgetting problem in class-incremental learning. With the absence of old task\nimages, they often assume that old knowledge is well preserved if the\nclassifier produces similar output on new images. In this paper, we find that\ntheir effectiveness largely depends on the nature of old classes: they work\nwell on classes that are easily distinguishable between each other but may fail\non more fine-grained ones, e.g., boy and girl. In spirit, such methods project\nnew data onto the feature space spanned by the weight vectors in the fully\nconnected layer, corresponding to old classes. The resulting projections would\nbe similar on fine-grained old classes, and as a consequence the new classifier\nwill gradually lose the discriminative ability on these classes. To address\nthis issue, we propose a memory-free generative replay strategy to preserve the\nfine-grained old classes characteristics by generating representative old\nimages directly from the old classifier and combined with new data for new\nclassifier training. To solve the homogenization problem of the generated\nsamples, we also propose a diversity loss that maximizes Kullback Leibler (KL)\ndivergence between generated samples. Our method is best complemented by prior\nregularization-based methods proved to be effective for easily distinguishable\nold classes. We validate the above design and insights on CUB-200-2011,\nCaltech-101, CIFAR-100 and Tiny ImageNet and show that our strategy outperforms\nexisting memory-free methods with a clear margin. Code is available at\nhttps://github.com/xmengxin/MFGR\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1\">Xiaomeng Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1\">Yunzhong Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on IQA. (arXiv:2109.00347v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00347","description":"<p>Image quality assessment(IQA) is of increasing importance for image-based\napplications. Its purpose is to establish a model that can replace humans for\naccurately evaluating image quality. According to whether the reference image\nis complete and available, image quality evaluation can be divided into three\ncategories: full-reference(FR), reduced-reference(RR), and non-reference(NR)\nimage quality assessment. Due to the vigorous development of deep learning and\nthe widespread attention of researchers, several non-reference image quality\nassessment methods based on deep learning have been proposed in recent years,\nand some have exceeded the performance of reduced -reference or even\nfull-reference image quality assessment models. This article will review the\nconcepts and metrics of image quality assessment and also video quality\nassessment, briefly introduce some methods of full-reference and semi-reference\nimage quality assessment, and focus on the non-reference image quality\nassessment methods based on deep learning. Then introduce the commonly used\nsynthetic database and real-world database. Finally, summarize and present\nchallenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lanjiang.Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The University of California San Francisco Preoperative Diffuse Glioma (UCSF-PDGM) MRI Dataset. (arXiv:2109.00356v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00356","description":"<p>Here we present the University of California San Francisco Preoperative\nDiffuse Glioma MRI (UCSF-PDGM) dataset. The UCSF-PDGM dataset includes 500\nsubjects with histopathologically-proven diffuse gliomas who were imaged with a\nstandardized 3 Tesla preoperative brain tumor MRI protocol featuring\npredominantly 3D imaging, as well as advanced diffusion and perfusion imaging\ntechniques. The dataset also includes isocitrate dehydrogenase (IDH) mutation\nstatus for all cases and O6-methylguanine-DNA methyltransferase (MGMT) promotor\nmethylation status for World Health Organization (WHO) grade III and IV\ngliomas. The UCSF-PDGM has been made publicly available in the hopes that\nresearchers around the world will use these data to continue to push the\nboundaries of AI applications for diffuse gliomas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calabrese_E/0/1/0/all/0/1\">Evan Calabrese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villanueva_Meyer_J/0/1/0/all/0/1\">Javier Villanueva-Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudie_J/0/1/0/all/0/1\">Jeffrey Rudie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauschecker_A/0/1/0/all/0/1\">Andreas Rauschecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baid_U/0/1/0/all/0/1\">Ujjwal Baid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakas_S/0/1/0/all/0/1\">Spyridon Bakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mongan_J/0/1/0/all/0/1\">John Mongan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hess_C/0/1/0/all/0/1\">Christopher Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Soonmee Cha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory Based Video Scene Parsing. (arXiv:2109.00373v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00373","description":"<p>Video scene parsing is a long-standing challenging task in computer vision,\naiming to assign pre-defined semantic labels to pixels of all frames in a given\nvideo. Compared with image semantic segmentation, this task pays more attention\non studying how to adopt the temporal information to obtain higher predictive\naccuracy. In this report, we introduce our solution for the 1st Video Scene\nParsing in the Wild Challenge, which achieves a mIoU of 57.44 and obtained the\n2nd place (our team name is CharlesBLWX).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhenchao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1\">Kai Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zehuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageTBAD: A 3D Computed Tomography Angiography Image Dataset for Automatic Segmentation of Type-B Aortic Dissection. (arXiv:2109.00374v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00374","description":"<p>Type-B Aortic Dissection (TBAD) is one of the most serious cardiovascular\nevents characterized by a growing yearly incidence,and the severity of disease\nprognosis. Currently, computed tomography angiography (CTA) has been widely\nadopted for the diagnosis and prognosis of TBAD. Accurate segmentation of true\nlumen (TL), false lumen (FL), and false lumen thrombus (FLT) in CTA are crucial\nfor the precise quantification of anatomical features. However, existing works\nonly focus on only TL and FL without considering FLT. In this paper, we propose\nImageTBAD, the first 3D computed tomography angiography (CTA) image dataset of\nTBAD with annotation of TL, FL, and FLT. The proposed dataset contains 100 TBAD\nCTA images, which is of decent size compared with existing medical imaging\ndatasets. As FLT can appear almost anywhere along the aorta with irregular\nshapes, segmentation of FLT presents a wide class of segmentation problems\nwhere targets exist in a variety of positions with irregular shapes. We further\npropose a baseline method for automatic segmentation of TBAD. Results show that\nthe baseline method can achieve comparable results with existing works on aorta\nand TL segmentation. However, the segmentation accuracy of FLT is only 52%,\nwhich leaves large room for improvement and also shows the challenge of our\ndataset. To facilitate further research on this challenging problem, our\ndataset and codes are released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yao_Z/0/1/0/all/0/1\">Zeyang Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_H/0/1/0/all/0/1\">Hailong Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tianchen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_Y/0/1/0/all/0/1\">Yuhao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Integrated Framework for the Heterogeneous Spatio-Spectral-Temporal Fusion of Remote Sensing Images. (arXiv:2109.00400v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00400","description":"<p>Image fusion technology is widely used to fuse the complementary information\nbetween multi-source remote sensing images. Inspired by the frontier of deep\nlearning, this paper first proposes a heterogeneous-integrated framework based\non a novel deep residual cycle GAN. The proposed network consists of a forward\nfusion part and a backward degeneration feedback part. The forward part\ngenerates the desired fusion result from the various observations; the backward\ndegeneration feedback part considers the imaging degradation process and\nregenerates the observations inversely from the fusion result. The proposed\nnetwork can effectively fuse not only the homogeneous but also the\nheterogeneous information. In addition, for the first time, a\nheterogeneous-integrated fusion framework is proposed to simultaneously merge\nthe complementary heterogeneous spatial, spectral and temporal information of\nmulti-source heterogeneous observations. The proposed heterogeneous-integrated\nframework also provides a uniform mode that can complete various fusion tasks,\nincluding heterogeneous spatio-spectral fusion, spatio-temporal fusion, and\nheterogeneous spatio-spectral-temporal fusion. Experiments are conducted for\ntwo challenging scenarios of land cover changes and thick cloud coverage.\nImages from many remote sensing satellites, including MODIS, Landsat-8,\nSentinel-1, and Sentinel-2, are utilized in the experiments. Both qualitative\nand quantitative evaluations confirm the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Menghui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Huanfeng Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVReflex: Dense Time-to-Impact Prediction for Event-based Obstacle Avoidance. (arXiv:2109.00405v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00405","description":"<p>The broad scope of obstacle avoidance has led to many kinds of computer\nvision-based approaches. Despite its popularity, it is not a solved problem.\nTraditional computer vision techniques using cameras and depth sensors often\nfocus on static scenes, or rely on priors about the obstacles. Recent\ndevelopments in bio-inspired sensors present event cameras as a compelling\nchoice for dynamic scenes. Although these sensors have many advantages over\ntheir frame-based counterparts, such as high dynamic range and temporal\nresolution, event-based perception has largely remained in 2D. This often leads\nto solutions reliant on heuristics and specific to a particular task. We show\nthat the fusion of events and depth overcomes the failure cases of each\nindividual modality when performing obstacle avoidance. Our proposed approach\nunifies event camera and lidar streams to estimate metric time-to-impact\nwithout prior knowledge of the scene geometry or obstacles. In addition, we\nrelease an extensive event-based dataset with six visual streams spanning over\n700 scanned scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walters_C/0/1/0/all/0/1\">Celyn Walters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1\">Simon Hadfield</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Pre-training by Mixing and Disentangling. (arXiv:2109.00452v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00452","description":"<p>The annotation for large-scale point clouds is still time-consuming and\nunavailable for many real-world tasks. Point cloud pre-training is one\npotential solution for obtaining a scalable model for fast adaptation.\nTherefore, in this paper, we investigate a new self-supervised learning\napproach, called Mixing and Disentangling (MD), for point cloud pre-training.\nAs the name implies, we explore how to separate the original point cloud from\nthe mixed point cloud, and leverage this challenging task as a pretext\noptimization objective for model training. Considering the limited training\ndata in the original dataset, which is much less than prevailing ImageNet, the\nmixing process can efficiently generate more high-quality samples. We build one\nbaseline network to verify our intuition, which simply contains two modules,\nencoder and decoder. Given a mixed point cloud, the encoder is first\npre-trained to extract the semantic embedding. Then an instance-adaptive\ndecoder is harnessed to disentangle the point clouds according to the\nembedding. Albeit simple, the encoder is inherently able to capture the point\ncloud keypoints after training and can be fast adapted to downstream tasks\nincluding classification and segmentation by the pre-training and fine-tuning\nparadigm. Extensive experiments on two datasets show that the encoder + ours\n(MD) significantly surpasses that of the encoder trained from scratch and\nconverges quickly. In ablation studies, we further study the effect of each\ncomponent and discuss the advantages of the proposed self-supervised learning\nstrategy. We hope this self-supervised learning attempt on point clouds can\npave the way for reducing the deeply-learned model dependence on large-scale\nlabeled data and saving a lot of annotation costs in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly-Supervised Surface Crack Segmentation Method using Localisation with a Classifier and Thresholding. (arXiv:2109.00456v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00456","description":"<p>Surface cracks are a common sight on public infrastructure nowadays. Recent\nwork has been addressing this problem by supporting structural maintenance\nmeasures using machine learning methods which segment surface cracks from their\nbackground so that they are easy to localize. However, a common issue with\nthose methods is that to create a well functioning algorithm, the training data\nneeds to have detailed annotations of pixels that belong to cracks. Our work\nproposes a weakly supervised approach which leverages a CNN classifier to\ncreate surface crack segmentation maps. We use this classifier to create a\nrough crack localisation map by using its class activation maps and a patch\nbased classification approach and fuse this with a thresholding based approach\nto segment the mostly darker crack pixels. The classifier assists in\nsuppressing noise from the background regions, which commonly are incorrectly\nhighlighted as cracks by standard thresholding methods. We focus on the ease of\nimplementation of our method and it is shown to perform well on several surface\ncrack datasets, segmenting cracks efficiently even though the only data that\nwas used for training were simple classification labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konig_J/0/1/0/all/0/1\">Jacob K&#xf6;nig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenkins_M/0/1/0/all/0/1\">Mark Jenkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mannion_M/0/1/0/all/0/1\">Mike Mannion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrie_P/0/1/0/all/0/1\">Peter Barrie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morison_G/0/1/0/all/0/1\">Gordon Morison</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse to Dense Motion Transfer for Face Image Animation. (arXiv:2109.00471v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00471","description":"<p>Face image animation from a single image has achieved remarkable progress.\nHowever, it remains challenging when only sparse landmarks are available as the\ndriving signal. Given a source face image and a sequence of sparse face\nlandmarks, our goal is to generate a video of the face imitating the motion of\nlandmarks. We develop an efficient and effective method for motion transfer\nfrom sparse landmarks to the face image. We then combine global and local\nmotion estimation in a unified model to faithfully transfer the motion. The\nmodel can learn to segment the moving foreground from the background and\ngenerate not only global motion, such as rotation and translation of the face,\nbut also subtle local motion such as the gaze change. We further improve face\nlandmark detection on videos. With temporally better aligned landmark sequences\nfor training, our method can generate temporally coherent videos with higher\nvisual quality. Experiments suggest we achieve results comparable to the\nstate-of-the-art image driven method on the same identity testing and better\nresults on cross identity testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruiqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Learning a Vocabulary of Visual Concepts and Operators using Deep Neural Networks. (arXiv:2109.00479v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00479","description":"<p>Deep neural networks have become the default choice for many applications\nlike image and video recognition, segmentation and other image and video\nrelated tasks.However, a critical challenge with these models is the lack of\nexplainability.This requirement of generating explainable predictions has\nmotivated the research community to perform various analysis on trained\nmodels.In this study, we analyze the learned feature maps of trained models\nusing MNIST images for achieving more explainable predictions.Our study is\nfocused on deriving a set of primitive elements, here called visual concepts,\nthat can be used to generate any arbitrary sample from the data generating\ndistribution.We derive the primitive elements from the feature maps learned by\nthe model.We illustrate the idea by generating visual concepts from a\nVariational Autoencoder trained using MNIST images.We augment the training data\nof MNIST dataset by adding about 60,000 new images generated with visual\nconcepts chosen at random.With this we were able to reduce the reconstruction\nloss (mean square error) from an initial value of 120 without augmentation to\n60 with augmentation.Our approach is a first step towards the final goal of\nachieving trained deep neural network models whose predictions, features in\nhidden layers and the learned filters can be well explained.Such a model when\ndeployed in production can easily be modified to adapt to new data, whereas\nexisting deep learning models need a re training or fine tuning. This process\nagain needs a huge number of data samples that are not easy to generate unless\nthe model has good explainability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vengalil_S/0/1/0/all/0/1\">Sunil Kumar Vengalil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1\">Neelam Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking at the whole picture: constrained unsupervised anomaly segmentation. (arXiv:2109.00482v1 [eess.IV])","link":"http://arxiv.org/abs/2109.00482","description":"<p>Current unsupervised anomaly localization approaches rely on generative\nmodels to learn the distribution of normal images, which is later used to\nidentify potential anomalous regions derived from errors on the reconstructed\nimages. However, a main limitation of nearly all prior literature is the need\nof employing anomalous images to set a class-specific threshold to locate the\nanomalies. This limits their usability in realistic scenarios, where only\nnormal data is typically accessible. Despite this major drawback, only a\nhandful of works have addressed this limitation, by integrating supervision on\nattention maps during training. In this work, we propose a novel formulation\nthat does not require accessing images with abnormalities to define the\nthreshold. Furthermore, and in contrast to very recent work, the proposed\nconstraint is formulated in a more principled manner, leveraging well-known\nknowledge in constrained optimization. In particular, the equality constraint\non the attention maps in prior work is replaced by an inequality constraint,\nwhich allows more flexibility. In addition, to address the limitations of\npenalty-based functions we employ an extension of the popular log-barrier\nmethods to handle the constraint. Comprehensive experiments on the popular\nBRATS'19 dataset demonstrate that the proposed approach substantially\noutperforms relevant literature, establishing new state-of-the-art results for\nunsupervised lesion segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Silva_Rodriguez_J/0/1/0/all/0/1\">Julio Silva-Rodr&#xed;guez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naranjo_V/0/1/0/all/0/1\">Valery Naranjo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Common Objects in 3D: Large-Scale Learning and Evaluation of Real-life 3D Category Reconstruction. (arXiv:2109.00512v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00512","description":"<p>Traditional approaches for learning 3D object categories have been\npredominantly trained and evaluated on synthetic datasets due to the\nunavailability of real 3D-annotated category-centric data. Our main goal is to\nfacilitate advances in this field by collecting real-world data in a magnitude\nsimilar to the existing synthetic counterparts. The principal contribution of\nthis work is thus a large-scale dataset, called Common Objects in 3D, with real\nmulti-view images of object categories annotated with camera poses and ground\ntruth 3D point clouds. The dataset contains a total of 1.5 million frames from\nnearly 19,000 videos capturing objects from 50 MS-COCO categories and, as such,\nit is significantly larger than alternatives both in terms of the number of\ncategories and objects. We exploit this new dataset to conduct one of the first\nlarge-scale \"in-the-wild\" evaluations of several new-view-synthesis and\ncategory-centric 3D reconstruction methods. Finally, we contribute NerFormer -\na novel neural rendering method that leverages the powerful Transformer to\nreconstruct an object given a small number of its views. The CO3D dataset is\navailable at https://github.com/facebookresearch/co3d .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reizenstein_J/0/1/0/all/0/1\">Jeremy Reizenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapovalov_R/0/1/0/all/0/1\">Roman Shapovalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henzler_P/0/1/0/all/0/1\">Philipp Henzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbordone_L/0/1/0/all/0/1\">Luca Sbordone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labatut_P/0/1/0/all/0/1\">Patrick Labatut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Novotny_D/0/1/0/all/0/1\">David Novotny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RP2K: A Large-Scale Retail Product Dataset for Fine-Grained Image Classification. (arXiv:2006.12634v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.12634","description":"<p>We introduce RP2K, a new large-scale retail product dataset for fine-grained\nimage classification. Unlike previous datasets focusing on relatively few\nproducts, we collect more than 500,000 images of retail products on shelves\nbelonging to 2000 different products. Our dataset aims to advance the research\nin retail object recognition, which has massive applications such as automatic\nshelf auditing and image-based product information retrieval. Our dataset\nenjoys following properties: (1) It is by far the largest scale dataset in\nterms of product categories. (2) All images are captured manually in physical\nretail stores with natural lightings, matching the scenario of real\napplications. (3) We provide rich annotations to each object, including the\nsizes, shapes and flavors/scents. We believe our dataset could benefit both\ncomputer vision research and retail industry. Our dataset is publicly available\nat https://www.pinlandata.com/rp2k_dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jingtian Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yifan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Removal of Perspective Distortion of Elevator Button Images based on Corner Detection. (arXiv:2007.11806v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.11806","description":"<p>Elevator button recognition is a critical function to realize the autonomous\noperation of elevators. However, challenging image conditions and various image\ndistortions make it difficult to recognize buttons accurately. To fill this\ngap, we propose a novel deep learning-based approach, which aims to\nautonomously correct perspective distortions of elevator button images based on\nbutton corner detection results. First, we leverage a novel image segmentation\nmodel and the Hough Transform method to obtain button segmentation and button\ncorner detection results. Then, pixel coordinates of standard button corners\nare used as reference features to estimate camera motions for correcting\nperspective distortions. Fifteen elevator button images are captured from\ndifferent angles of view as the dataset. The experimental results demonstrate\nthat our proposed approach is capable of estimating camera motions and removing\nperspective distortions of elevator button images with high accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1\">Nachuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianbang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Delong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances. (arXiv:2009.13290v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.13290","description":"<p>Face recognition is one of the most popular and long-standing topics in\ncomputer vision. With the recent development of deep learning techniques and\nlarge-scale datasets, deep face recognition has made remarkable progress and\nbeen widely used in many real-world applications. Given a natural image or\nvideo frame as input, an end-to-end deep face recognition system outputs the\nface feature for recognition. To achieve this, a typical end-to-end system is\ngenerally built with three key elements: face detection, face alignment, and\nface representation. The face detection locates faces in the image or frame.\nThen, the face alignment is proceeded to calibrate the faces to a canonical\nview and crop them to a normalized pixel size. Finally, in the stage of face\nrepresentation, the discriminative features are extracted from the aligned face\nfor recognition. Nowadays, all of the three elements are fulfilled by the\ntechnique of deep convolutional neural network.In this survey article, we\npresent a comprehensive review about the recent advance of each element of the\nend-to-end deep face recognition, since the thriving deep learning techniques\nhave greatly improved the capability of them. To start with, we present an\noverview of the end-to-end deep face recognition. Then, we review the advance\nof each element, respectively, covering many aspects such as the to-date\nalgorithm designs, evaluation metrics, datasets, performance comparison,\nexisting challenges, and promising directions for future research. Through this\nsurvey, we wish to bring contributions in two aspects: first, readers can\nconveniently identify the methods which are quite strong-baseline style in the\nsubcategory for further exploration; second, one can also employ suitable\nmethods for establishing a state-of-the-art end-to-end face recognition system\nfrom scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring DeshuffleGANs in Self-Supervised Generative Adversarial Networks. (arXiv:2011.01730v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.01730","description":"<p>Generative Adversarial Networks (GANs) have become the most used networks\ntowards solving the problem of image generation. Self-supervised GANs are later\nproposed to avoid the catastrophic forgetting of the discriminator and to\nimprove the image generation quality without needing the class labels. However,\nthe generalizability of the self-supervision tasks on different GAN\narchitectures is not studied before. To that end, we extensively analyze the\ncontribution of a previously proposed self-supervision task, deshuffling of the\nDeshuffleGANs in the generalizability context. We assign the deshuffling task\nto two different GAN discriminators and study the effects of the task on both\narchitectures. We extend the evaluations compared to the previously proposed\nDeshuffleGANs on various datasets. We show that the DeshuffleGAN obtains the\nbest FID results for several datasets compared to the other self-supervised\nGANs. Furthermore, we compare the deshuffling with the rotation prediction that\nis firstly deployed to the GAN training and demonstrate that its contribution\nexceeds the rotation prediction. We design the conditional DeshuffleGAN called\ncDeshuffleGAN to evaluate the quality of the learnt representations. Lastly, we\nshow the contribution of the self-supervision tasks to the GAN training on the\nloss landscape and present that the effects of these tasks may not be\ncooperative to the adversarial training in some settings. Our code can be found\nat https://github.com/gulcinbaykal/DeshuffleGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baykal_G/0/1/0/all/0/1\">Gulcin Baykal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcelik_F/0/1/0/all/0/1\">Furkan Ozcelik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1\">Gozde Unal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchically Decoupled Spatial-Temporal Contrast for Self-supervised Video Representation Learning. (arXiv:2011.11261v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11261","description":"<p>We present a novel technique for self-supervised video representation\nlearning by: (a) decoupling the learning objective into two contrastive\nsubtasks respectively emphasizing spatial and temporal features, and (b)\nperforming it hierarchically to encourage multi-scale understanding. Motivated\nby their effectiveness in supervised learning, we first introduce\nspatial-temporal feature learning decoupling and hierarchical learning to the\ncontext of unsupervised video learning. We show by experiments that\naugmentations can be manipulated as regularization to guide the network to\nlearn desired semantics in contrastive learning, and we propose a way for the\nmodel to separately capture spatial and temporal features at multiple scales.\nWe also introduce an approach to overcome the problem of divergent levels of\ninstance invariance at different hierarchies by modeling the invariance as loss\nweights for objective re-weighting. Experiments on downstream action\nrecognition benchmarks on UCF101 and HMDB51 show that our proposed\nHierarchically Decoupled Spatial-Temporal Contrast (HDC) makes substantial\nimprovements over directly learning spatial-temporal features as a whole and\nachieves competitive performance when compared with other state-of-the-art\nunsupervised methods. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zehua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransPose: Keypoint Localization via Transformer. (arXiv:2012.14214v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14214","description":"<p>While CNN-based models have made remarkable progress on human pose\nestimation, what spatial dependencies they capture to localize keypoints\nremains unclear. In this work, we propose a model called \\textbf{TransPose},\nwhich introduces Transformer for human pose estimation. The attention layers\nbuilt in Transformer enable our model to capture long-range relationships\nefficiently and also can reveal what dependencies the predicted keypoints rely\non. To predict keypoint heatmaps, the last attention layer acts as an\naggregator, which collects contributions from image clues and forms maximum\npositions of keypoints. Such a heatmap-based localization approach via\nTransformer conforms to the principle of Activation\nMaximization~\\cite{erhan2009visualizing}. And the revealed dependencies are\nimage-specific and fine-grained, which also can provide evidence of how the\nmodel handles special cases, e.g., occlusion. The experiments show that\nTransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets,\nwhile being more lightweight and faster than mainstream CNN architectures. The\nTransPose model also transfers very well on MPII benchmark, achieving superior\nperformance on the test set when fine-tuned with small training costs. Code and\npre-trained models are publicly\navailable\\footnote{\\url{https://github.com/yangsenius/TransPose}}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Z/0/1/0/all/0/1\">Zhibin Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1\">Mu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wankou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-based Face Super-Resolution: A Survey. (arXiv:2101.03749v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.03749","description":"<p>Face super-resolution (FSR), also known as face hallucination, which is aimed\nat enhancing the resolution of low-resolution (LR) face images to generate\nhigh-resolution (HR) face images, is a domain-specific image super-resolution\nproblem. Recently, FSR has received considerable attention and witnessed\ndazzling advances with the development of deep learning techniques. To date,\nfew summaries of the studies on the deep learning-based FSR are available. In\nthis survey, we present a comprehensive review of deep learning-based FSR\nmethods in a systematic manner. First, we summarize the problem formulation of\nFSR and introduce popular assessment metrics and loss functions. Second, we\nelaborate on the facial characteristics and popular datasets used in FSR.\nThird, we roughly categorize existing methods according to the utilization of\nfacial characteristics. In each category, we start with a general description\nof design principles, then present an overview of representative approaches,\nand then discuss the pros and cons among them. Fourth, we evaluate the\nperformance of some state-of-the-art methods. Fifth, joint FSR and other tasks,\nand FSR-related applications are roughly introduced. Finally, we envision the\nprospects of further technological advancement in this field. A curated list of\npapers and resources to face super-resolution are available at\n\\url{https://github.com/junjun-jiang/Face-Hallucination-Benchmark}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiayi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Dual-resolution Networks for Real-time and Accurate Semantic Segmentation of Road Scenes. (arXiv:2101.06085v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.06085","description":"<p>Semantic segmentation is a key technology for autonomous vehicles to\nunderstand the surrounding scenes. The appealing performances of contemporary\nmodels usually come at the expense of heavy computations and lengthy inference\ntime, which is intolerable for self-driving. Using light-weight architectures\n(encoder-decoder or two-pathway) or reasoning on low-resolution images, recent\nmethods realize very fast scene parsing, even running at more than 100 FPS on a\nsingle 1080Ti GPU. However, there is still a significant gap in performance\nbetween these real-time methods and the models based on dilation backbones. To\ntackle this problem, we proposed a family of efficient backbones specially\ndesigned for real-time semantic segmentation. The proposed deep dual-resolution\nnetworks (DDRNets) are composed of two deep branches between which multiple\nbilateral fusions are performed. Additionally, we design a new contextual\ninformation extractor named Deep Aggregation Pyramid Pooling Module (DAPPM) to\nenlarge effective receptive fields and fuse multi-scale context based on\nlow-resolution feature maps. Our method achieves a new state-of-the-art\ntrade-off between accuracy and speed on both Cityscapes and CamVid dataset. In\nparticular, on a single 2080Ti GPU, DDRNet-23-slim yields 77.4% mIoU at 102 FPS\non Cityscapes test set and 74.7% mIoU at 230 FPS on CamVid test set. With\nwidely used test augmentation, our method is superior to most state-of-the-art\nmodels and requires much less computation. Codes and trained models are\navailable online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yuanduo Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Huihui Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yisong Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlowReg: Fast Deformable Unsupervised Medical Image Registration using Optical Flow. (arXiv:2101.09639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.09639","description":"<p>We propose FlowReg, a deep learning-based framework for unsupervised image\nregistration for neuroimaging applications. The system is composed of two\narchitectures that are trained sequentially: FlowReg-A which affinely corrects\nfor gross differences between moving and fixed volumes in 3D followed by\nFlowReg-O which performs pixel-wise deformations on a slice-by-slice basis for\nfine tuning in 2D. The affine network regresses the 3D affine matrix based on a\ncorrelation loss function that enforces global similarity. The deformable\nnetwork operates on 2D image slices based on the optical flow network\nFlowNet-Simple but with three loss components. The photometric loss minimizes\npixel intensity differences differences, the smoothness loss encourages similar\nmagnitudes between neighbouring vectors, and a correlation loss that is used to\nmaintain the intensity similarity between fixed and moving image slices. The\nproposed method is compared to four open source registration techniques ANTs,\nDemons, SE, and Voxelmorph. In total, 4643 FLAIR MR imaging volumes are used\nfrom dementia and vascular disease cohorts, acquired from over 60 international\ncentres with varying acquisition parameters. A battery of quantitative novel\nregistration validation metrics are proposed that focus on the structural\nintegrity of tissues, spatial alignment, and intensity similarity. Experimental\nresults show FlowReg (FlowReg-A+O) performs better than iterative-based\nregistration algorithms for intensity and spatial alignment metrics with a\nPixelwise Agreement of 0.65, correlation coefficient of 0.80, and Mutual\nInformation of 0.29. Among the deep learning frameworks, FlowReg-A or\nFlowReg-A+O provided the highest performance over all but one of the metrics.\nResults show that FlowReg is able to obtain high intensity and spatial\nsimilarity while maintaining the shape and structure of anatomy and pathology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_S/0/1/0/all/0/1\">Sergiu Mocanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moody_A/0/1/0/all/0/1\">Alan R. Moody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_A/0/1/0/all/0/1\">April Khademi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance and Panoptic Segmentation Using Conditional Convolutions. (arXiv:2102.03026v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03026","description":"<p>We propose a simple yet effective framework for instance and panoptic\nsegmentation, termed CondInst (conditional convolutions for instance and\npanoptic segmentation). In the literature, top-performing instance segmentation\nmethods typically follow the paradigm of Mask R-CNN and rely on ROI operations\n(typically ROIAlign) to attend to each instance. In contrast, we propose to\nattend to the instances with dynamic conditional convolutions. Instead of using\ninstance-wise ROIs as inputs to the instance mask head of fixed weights, we\ndesign dynamic instance-aware mask heads, conditioned on the instances to be\npredicted. CondInst enjoys three advantages: 1.) Instance and panoptic\nsegmentation are unified into a fully convolutional network, eliminating the\nneed for ROI cropping and feature alignment. 2.) The elimination of the ROI\ncropping also significantly improves the output instance mask resolution. 3.)\nDue to the much improved capacity of dynamically-generated conditional\nconvolutions, the mask head can be very compact (e.g., 3 conv. layers, each\nhaving only 8 channels), leading to significantly faster inference time per\ninstance and making the overall inference time almost constant, irrelevant to\nthe number of instances. We demonstrate a simpler method that can achieve\nimproved accuracy and inference speed on both instance and panoptic\nsegmentation tasks. On the COCO dataset, we outperform a few state-of-the-art\nmethods. We hope that CondInst can be a strong baseline for instance and\npanoptic segmentation. Code is available at: https://git.io/AdelaiDet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttributeNet: Attribute Enhanced Vehicle Re-Identification. (arXiv:2102.03898v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03898","description":"<p>Vehicle Re-Identification (V-ReID) is a critical task that associates the\nsame vehicle across images from different camera viewpoints. Many works explore\nattribute clues to enhance V-ReID; however, there is usually a lack of\neffective interaction between the attribute-related modules and final V-ReID\nobjective. In this work, we propose a new method to efficiently explore\ndiscriminative information from vehicle attributes (for instance, color and\ntype). We introduce AttributeNet (ANet) that jointly extracts identity-relevant\nfeatures and attribute features. We enable the interaction by distilling the\nReID-helpful attribute feature and adding it into the general ReID feature to\nincrease the discrimination power. Moreover, we propose a constraint, named\nAmelioration Constraint (AC), which encourages the feature after adding\nattribute features onto the general ReID feature to be more discriminative than\nthe original general ReID feature. We validate the effectiveness of our\nframework on three challenging datasets. Experimental results show that our\nmethod achieves the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quispe_R/0/1/0/all/0/1\">Rodolfo Quispe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrini_H/0/1/0/all/0/1\">Helio Pedrini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning for Unconstrained Space-Time Video Super-Resolution. (arXiv:2102.13011v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.13011","description":"<p>Recent years have seen considerable research activities devoted to video\nenhancement that simultaneously increases temporal frame rate and spatial\nresolution. However, the existing methods either fail to explore the intrinsic\nrelationship between temporal and spatial information or lack flexibility in\nthe choice of final temporal/spatial resolution. In this work, we propose an\nunconstrained space-time video super-resolution network, which can effectively\nexploit space-time correlation to boost performance. Moreover, it has complete\nfreedom in adjusting the temporal frame rate and spatial resolution through the\nuse of the optical flow technique and a generalized pixelshuffle operation. Our\nextensive experiments demonstrate that the proposed method not only outperforms\nthe state-of-the-art, but also requires far fewer parameters and less running\ntime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhihao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaohong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Linhui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davidson_T/0/1/0/all/0/1\">Timothy N. Davidson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Trust Region for Weakly Supervised Segmentation. (arXiv:2104.01948v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01948","description":"<p>Acquisition of training data for the standard semantic segmentation is\nexpensive if requiring that each pixel is labeled. Yet, current methods\nsignificantly deteriorate in weakly supervised settings, e.g. where a fraction\nof pixels is labeled or when only image-level tags are available. It has been\nshown that regularized losses - originally developed for unsupervised low-level\nsegmentation and representing geometric priors on pixel labels - can\nconsiderably improve the quality of weakly supervised training. However, many\ncommon priors require optimization stronger than gradient descent. Thus, such\nregularizers have limited applicability in deep learning. We propose a new\nrobust trust region approach for regularized losses improving the\nstate-of-the-art results. Our approach can be seen as a higher-order\ngeneralization of the classic chain rule. It allows neural network optimization\nto use strong low-level solvers for the corresponding regularizers, including\ndiscrete ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marin_D/0/1/0/all/0/1\">Dmitrii Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boykov_Y/0/1/0/all/0/1\">Yuri Boykov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering. (arXiv:2104.03149v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03149","description":"<p>We introduce an evaluation methodology for visual question answering (VQA) to\nbetter diagnose cases of shortcut learning. These cases happen when a model\nexploits spurious statistical regularities to produce correct answers but does\nnot actually deploy the desired behavior. There is a need to identify possible\nshortcuts in a dataset and assess their use before deploying a model in the\nreal world. The research community in VQA has focused exclusively on\nquestion-based shortcuts, where a model might, for example, answer \"What is the\ncolor of the sky\" with \"blue\" by relying mostly on the question-conditional\ntraining prior and give little weight to visual evidence. We go a step further\nand consider multimodal shortcuts that involve both questions and images. We\nfirst identify potential shortcuts in the popular VQA v2 training set by mining\ntrivial predictive rules such as co-occurrences of words and visual elements.\nWe then introduce VQA-CounterExamples (VQA-CE), an evaluation protocol based on\nour subset of CounterExamples i.e. image-question-answer triplets where our\nrules lead to incorrect answers. We use this new evaluation in a large-scale\nstudy of existing approaches for VQA. We demonstrate that even state-of-the-art\nmodels perform poorly and that existing techniques to reduce biases are largely\nineffective in this context. Our findings suggest that past work on\nquestion-based biases in VQA has only addressed one facet of a complex issue.\nThe code for our method is available at\nhttps://github.com/cdancette/detect-shortcuts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dancette_C/0/1/0/all/0/1\">Corentin Dancette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">Remi Cadene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STRUDEL: Self-Training with Uncertainty Dependent Label Refinement across Domains. (arXiv:2104.11596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11596","description":"<p>We propose an unsupervised domain adaptation (UDA) approach for white matter\nhyperintensity (WMH) segmentation, which uses Self-Training with Uncertainty\nDEpendent Label refinement (STRUDEL). Self-training has recently been\nintroduced as a highly effective method for UDA, which is based on\nself-generated pseudo labels. However, pseudo labels can be very noisy and\ntherefore deteriorate model performance. We propose to predict the uncertainty\nof pseudo labels and integrate it in the training process with an\nuncertainty-guided loss function to highlight labels with high certainty.\nSTRUDEL is further improved by incorporating the segmentation output of an\nexisting method in the pseudo label generation that showed high robustness for\nWMH segmentation. In our experiments, we evaluate STRUDEL with a standard U-Net\nand a modified network with a higher receptive field. Our results on WMH\nsegmentation across datasets demonstrate the significant improvement of STRUDEL\nwith respect to standard self-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Groger_F/0/1/0/all/0/1\">Fabian Gr&#xf6;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1\">Anne-Marie Rickmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visformer: The Vision-friendly Transformer. (arXiv:2104.12533v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12533","description":"<p>The past year has witnessed the rapid development of applying the Transformer\nmodule to vision problems. While some researchers have demonstrated that\nTransformer-based models enjoy a favorable ability of fitting data, there are\nstill growing number of evidences showing that these models suffer over-fitting\nespecially when the training data is limited. This paper offers an empirical\nstudy by performing step-by-step operations to gradually transit a\nTransformer-based model to a convolution-based model. The results we obtain\nduring the transition process deliver useful messages for improving visual\nrecognition. Based on these observations, we propose a new architecture named\nVisformer, which is abbreviated from the `Vision-friendly Transformer'. With\nthe same computational complexity, Visformer outperforms both the\nTransformer-based and convolution-based models in terms of ImageNet\nclassification accuracy, and the advantage becomes more significant when the\nmodel complexity is lower or the training set is smaller. The code is available\nat https://github.com/danczs/Visformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhengsu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuefeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining in Style: Training a GAN to explain a classifier in StyleSpace. (arXiv:2104.13369v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13369","description":"<p>Image classification models can depend on multiple different semantic\nattributes of the image. An explanation of the decision of the classifier needs\nto both discover and visualize these properties. Here we present StylEx, a\nmethod for doing this, by training a generative model to specifically explain\nmultiple attributes that underlie classifier decisions. A natural source for\nsuch attributes is the StyleSpace of StyleGAN, which is known to generate\nsemantically meaningful dimensions in the image. However, because standard GAN\ntraining is not dependent on the classifier, it may not represent these\nattributes which are important for the classifier decision, and the dimensions\nof StyleSpace may represent irrelevant attributes. To overcome this, we propose\na training procedure for a StyleGAN, which incorporates the classifier model,\nin order to learn a classifier-specific StyleSpace. Explanatory attributes are\nthen selected from this space. These can be used to visualize the effect of\nchanging multiple attributes per image, thus providing image-specific\nexplanations. We apply StylEx to multiple domains, including animals, leaves,\nfaces and retinal images. For these, we show how an image can be modified in\ndifferent ways to change its classifier output. Our results show that the\nmethod finds attributes that align well with semantic ones, generate meaningful\nimage-specific explanations, and are human-interpretable as measured in\nuser-studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_O/0/1/0/all/0/1\">Oran Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1\">Yossi Gandelsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1\">Michal Yarom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wald_Y/0/1/0/all/0/1\">Yoav Wald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elidan_G/0/1/0/all/0/1\">Gal Elidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1\">Avinatan Hassidim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosseri_I/0/1/0/all/0/1\">Inbar Mosseri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACDC: The Adverse Conditions Dataset with Correspondences for Semantic Driving Scene Understanding. (arXiv:2104.13395v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13395","description":"<p>Level 5 autonomy for self-driving cars requires a robust visual perception\nsystem that can parse input images under any visual condition. However,\nexisting semantic segmentation datasets are either dominated by images captured\nunder normal conditions or are small in scale. To address this, we introduce\nACDC, the Adverse Conditions Dataset with Correspondences for training and\ntesting semantic segmentation methods on adverse visual conditions. ACDC\nconsists of a large set of 4006 images which are equally distributed between\nfour common adverse conditions: fog, nighttime, rain, and snow. Each\nadverse-condition image comes with a high-quality fine pixel-level semantic\nannotation, a corresponding image of the same scene taken under normal\nconditions, and a binary mask that distinguishes between intra-image regions of\nclear and uncertain semantic content. Thus, ACDC supports both standard\nsemantic segmentation and the newly introduced uncertainty-aware semantic\nsegmentation. A detailed empirical study demonstrates the challenges that the\nadverse domains of ACDC pose to state-of-the-art supervised and unsupervised\napproaches and indicates the value of our dataset in steering future progress\nin the field. Our dataset and benchmark are publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous-discrete multiple target tracking with out-of-sequence measurements. (arXiv:2106.04898v2 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2106.04898","description":"<p>This paper derives the optimal Bayesian processing of an out-of-sequence\n(OOS) set of measurements in continuous-time for multiple target tracking. We\nconsider a multi-target system modelled in continuous time that is discretised\nat the time steps when we receive the measurements, which are distributed\naccording to the standard point target model. All information about this system\nat the sampled time steps is provided by the posterior density on the set of\nall trajectories. This density can be computed via the continuous-discrete\ntrajectory Poisson multi-Bernoulli mixture (TPMBM) filter. When we receive an\nOOS measurement, the optimal Bayesian processing performs a retrodiction step\nthat adds trajectory information at the OOS measurement time stamp followed by\nan update step. After the OOS measurement update, the posterior remains in\nTPMBM form. We also provide a computationally lighter alternative based on a\ntrajectory Poisson multi-Bernoulli filter. The effectiveness of the two\napproaches to handle OOS measurements is evaluated via simulations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_Fernandez_A/0/1/0/all/0/1\">&#xc1;ngel F. Garc&#xed;a-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yi_W/0/1/0/all/0/1\">Wei Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Video Classification Meets Incremental Classes. (arXiv:2106.15827v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15827","description":"<p>With the rapid development of social media, tremendous videos with new\nclasses are generated daily, which raise an urgent demand for video\nclassification methods that can continuously update new classes while\nmaintaining the knowledge of old videos with limited storage and computing\nresources. In this paper, we summarize this task as Class-Incremental Video\nClassification (CIVC) and propose a novel framework to address it. As a subarea\nof incremental learning tasks, the challenge of catastrophic forgetting is\nunavoidable in CIVC. To better alleviate it, we utilize some characteristics of\nvideos. First, we decompose the spatio-temporal knowledge before distillation\nrather than treating it as a whole in the knowledge transfer process;\ntrajectory is also used to refine the decomposition. Second, we propose a dual\ngranularity exemplar selection method to select and store representative video\ninstances of old classes and key-frames inside videos under a tight storage\nbudget. We benchmark our method and previous SOTA class-incremental learning\nmethods on Something-Something V2 and Kinetics datasets, and our method\noutperforms previous methods significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shihao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yongjian Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zibo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CI-Net: Contextual Information for Joint Semantic Segmentation and Depth Estimation. (arXiv:2107.13800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13800","description":"<p>Monocular depth estimation and semantic segmentation are two fundamental\ngoals of scene understanding. Due to the advantages of task interaction, many\nworks study the joint task learning algorithm. However, most existing methods\nfail to fully leverage the semantic labels, ignoring the provided context\nstructures and only using them to supervise the prediction of segmentation\nsplit, which limit the performance of both tasks. In this paper, we propose a\nnetwork injected with contextual information (CI-Net) to solve the problem.\nSpecifically, we introduce self-attention block in the encoder to generate\nattention map. With supervision from the ideal attention map created by\nsemantic label, the network is embedded with contextual information so that it\ncould understand scene better and utilize correlated features to make accurate\nprediction. Besides, a feature sharing module is constructed to make the\ntask-specific features deeply fused and a consistency loss is devised to make\nthe features mutually guided. We evaluate the proposed CI-Net on the\nNYU-Depth-v2 and SUN-RGBD datasets. The experimental results validate that our\nproposed CI-Net could effectively improve the accuracy of semantic segmentation\nand depth estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianxiao Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhongbin Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhun Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shane Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinmei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qiuda Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00394","description":"<p>Graph matching is an important problem that has received widespread\nattention, especially in the field of computer vision. Recently,\nstate-of-the-art methods seek to incorporate graph matching with deep learning.\nHowever, there is no research to explain what role the graph matching algorithm\nplays in the model. Therefore, we propose an approach integrating a MILP\nformulation of the graph matching problem. This formulation is solved to\noptimal and it provides inherent baseline. Meanwhile, similar approaches are\nderived by releasing the optimal guarantee of the graph matching solver and by\nintroducing a quality level. This quality level controls the quality of the\nsolutions provided by the graph matching solver. In addition, several\nrelaxations of the graph matching problem are put to the test. Our experimental\nevaluation gives several theoretical insights and guides the direction of deep\ngraph matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhoubo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Puqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1\">Romain Raveaux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huadong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Relighting against Face Recognition. (arXiv:2108.07920v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.07920","description":"<p>Deep face recognition (FR) has achieved significantly high accuracy on\nseveral challenging datasets and fosters successful real-world applications,\neven showing high robustness to the illumination variation that is usually\nregarded as a main threat to the FR system. However, in the real world,\nillumination variation caused by diverse lighting conditions cannot be fully\ncovered by the limited face dataset. In this paper, we study the threat of\nlighting against FR from a new angle, i.e., adversarial attack, and identify a\nnew task, i.e., adversarial relighting. Given a face image, adversarial\nrelighting aims to produce a naturally relighted counterpart while fooling the\nstate-of-the-art deep FR methods. To this end, we first propose the physical\nmodel-based adversarial relighting attack (ARA) denoted as\nalbedo-quotient-based adversarial relighting attack (AQ-ARA). It generates\nnatural adversarial light under the physical lighting model and guidance of FR\nsystems and synthesizes adversarially relighted face images. Moreover, we\npropose the auto-predictive adversarial relighting attack (AP-ARA) by training\nan adversarial relighting network (ARNet) to automatically predict the\nadversarial light in a one-step manner according to different input faces,\nallowing efficiency-sensitive applications. More importantly, we propose to\ntransfer the above digital attacks to physical ARA (Phy-ARA) through a precise\nrelighting device, making the estimated adversarial lighting condition\nreproducible in the real world. We validate our methods on three\nstate-of-the-art deep FR methods, i.e., FaceNet, ArcFace, and CosFace, on two\npublic datasets. The extensive and insightful results demonstrate our work can\ngenerate realistic adversarial relighted face images fooling FR easily,\nrevealing the threat of specific light directions and strengths.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruijun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juefei_Xu_F/0/1/0/all/0/1\">Felix Juefei-Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongkai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-and-Language Navigation: A Survey and Taxonomy. (arXiv:2108.11544v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11544","description":"<p>An agent that can understand natural-language instruction and carry out\ncorresponding actions in the visual world is one of the long-term challenges of\nArtificial Intelligent (AI). Due to multifarious instructions from humans, it\nrequires the agent can link natural language to vision and action in\nunstructured, previously unseen environments. If the instruction given by human\nis a navigation task, this challenge is called Visual-and-Language Navigation\n(VLN). It is a booming multi-disciplinary field of increasing importance and\nwith extraordinary practicality. Instead of focusing on the details of specific\nmethods, this paper provides a comprehensive survey on VLN tasks and makes a\nclassification carefully according the different characteristics of language\ninstructions in these tasks. According to when the instructions are given, the\ntasks can be divided into single-turn and multi-turn. For single-turn tasks, we\nfurther divided them into goal-orientation and route-orientation based on\nwhether the instructions contain a route. For multi-turn tasks, we divided them\ninto imperative task and interactive task based on whether the agent responses\nto the instructions. This taxonomy enable researchers to better grasp the key\npoint of a specific task and identify directions for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wansen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Tao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinmeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ThresholdNet: Pruning Tool for Densely Connected Convolutional Networks. (arXiv:2108.12604v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12604","description":"<p>Deep neural networks have made significant progress in the field of computer\nvision. Recent studies have shown that depth, width and shortcut connections of\nneural network architectures play a crucial role in their performance. One of\nthe most advanced neural network architectures, DenseNet, has achieved\nexcellent convergence rates through dense connections. However, it still has\nobvious shortcomings in the usage of amount of memory. In this paper, we\nintroduce a new type of pruning tool, threshold, which refers to the principle\nof the threshold voltage in MOSFET. This work employs this method to connect\nblocks of different depths in different ways to reduce the usage of memory. It\nis denoted as ThresholdNet. We evaluate ThresholdNet and other different\nnetworks on datasets of CIFAR10. Experiments show that HarDNet is twice as fast\nas DenseNet, and on this basis, ThresholdNet is 10% faster and 10% lower error\nrate than HarDNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Synthesis via Uncertainty-Driven Attribute Synchronization. (arXiv:2108.13499v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13499","description":"<p>Developing deep neural networks to generate 3D scenes is a fundamental\nproblem in neural synthesis with immediate applications in architectural CAD,\ncomputer graphics, as well as in generating virtual robot training\nenvironments. This task is challenging because 3D scenes exhibit diverse\npatterns, ranging from continuous ones, such as object sizes and the relative\nposes between pairs of shapes, to discrete patterns, such as occurrence and\nco-occurrence of objects with symmetrical relationships. This paper introduces\na novel neural scene synthesis approach that can capture diverse feature\npatterns of 3D scenes. Our method combines the strength of both neural\nnetwork-based and conventional scene synthesis approaches. We use the\nparametric prior distributions learned from training data, which provide\nuncertainties of object attributes and relative attributes, to regularize the\noutputs of feed-forward neural models. Moreover, instead of merely predicting a\nscene layout, our approach predicts an over-complete set of attributes. This\nmethodology allows us to utilize the underlying consistency constraints among\nthe predicted attributes to prune infeasible predictions. Experimental results\nshow that our approach outperforms existing methods considerably. The generated\n3D scenes interpolate the training data faithfully while preserving both\ncontinuous and discrete feature patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haitao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zaiwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Siming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fiducial marker recovery and detection from severely truncated data in navigation assisted spine surgery. (arXiv:2108.13844v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.13844","description":"<p>Fiducial markers are commonly used in navigation assisted minimally invasive\nspine surgery (MISS) and they help transfer image coordinates into real world\ncoordinates. In practice, these markers might be located outside the\nfield-of-view (FOV), due to the limited detector sizes of C-arm cone-beam\ncomputed tomography (CBCT) systems used in intraoperative surgeries. As a\nconsequence, reconstructed markers in CBCT volumes suffer from artifacts and\nhave distorted shapes, which sets an obstacle for navigation. In this work, we\npropose two fiducial marker detection methods: direct detection from distorted\nmarkers (direct method) and detection after marker recovery (recovery method).\nFor direct detection from distorted markers in reconstructed volumes, an\nefficient automatic marker detection method using two neural networks and a\nconventional circle detection algorithm is proposed. For marker recovery, a\ntask-specific learning strategy is proposed to recover markers from severely\ntruncated data. Afterwards, a conventional marker detection algorithm is\napplied for position detection. The two methods are evaluated on simulated data\nand real data, both achieving a marker registration error smaller than 0.2 mm.\nOur experiments demonstrate that the direct method is capable of detecting\ndistorted markers accurately and the recovery method with task-specific\nlearning has high robustness and generalizability on various data sets. In\naddition, the task-specific learning is able to reconstruct other structures of\ninterest accurately, e.g. ribs for image-guided needle biopsy, from severely\ntruncated data, which empowers CBCT systems with new potential applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fan_F/0/1/0/all/0/1\">Fuxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kreher_B/0/1/0/all/0/1\">Bj&#xf6;rn Kreher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Keil_H/0/1/0/all/0/1\">Holger Keil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Nuclear Instance and Layer Segmentation in Oral Epithelial Dysplasia. (arXiv:2108.13904v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.13904","description":"<p>Oral epithelial dysplasia (OED) is a pre-malignant histopathological\ndiagnosis given to lesions of the oral cavity. Predicting OED grade or whether\na case will transition to malignancy is critical for early detection and\nappropriate treatment. OED typically begins in the lower third of the\nepithelium before progressing upwards with grade severity, thus we have\nsuggested that segmenting intra-epithelial layers, in addition to individual\nnuclei, may enable researchers to evaluate important layer-specific\nmorphological features for grade/malignancy prediction. We present HoVer-Net+,\na deep learning framework to simultaneously segment (and classify) nuclei and\n(intra-)epithelial layers in H&amp;E stained slides from OED cases. The proposed\narchitecture consists of an encoder branch and four decoder branches for\nsimultaneous instance segmentation of nuclei and semantic segmentation of the\nepithelial layers. We show that the proposed model achieves the\nstate-of-the-art (SOTA) performance in both tasks, with no additional costs\nwhen compared to previous SOTA methods for each task. To the best of our\nknowledge, ours is the first method for simultaneous nuclear instance\nsegmentation and semantic tissue segmentation, with potential for use in\ncomputational pathology for other similar simultaneous tasks and for future\nstudies into malignancy prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1\">Adam J. Shephard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1\">Simon Graham</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bashir_R/0/1/0/all/0/1\">R.M. Saad Bashir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_H/0/1/0/all/0/1\">Hanya Mahmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1\">Syed Ali Khurram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir M. Rajpoot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}