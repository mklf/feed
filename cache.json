{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-11T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Sentence-Select: Large-Scale Language Model Data Selection for Rare-Word Speech Recognition. (arXiv:2203.05008v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05008","description":"<p>Language model fusion helps smart assistants recognize words which are rare\nin acoustic data but abundant in text-only corpora (typed search logs).\nHowever, such corpora have properties that hinder downstream performance,\nincluding being (1) too large, (2) beset with domain-mismatched content, and\n(3) heavy-headed rather than heavy-tailed (excessively many duplicate search\nqueries such as \"weather\"). We show that three simple strategies for selecting\nlanguage modeling data can dramatically improve rare-word recognition without\nharming overall performance. First, to address the heavy-headedness, we\ndownsample the data according to a soft log function, which tunably reduces\nhigh frequency (head) sentences. Second, to encourage rare-word exposure, we\nexplicitly filter for words rare in the acoustic data. Finally, we tackle\ndomain-mismatch via perplexity-based contrastive selection, filtering for\nexamples matched to the target domain. We down-select a large corpus of web\nsearch queries by a factor of 53x and achieve better LM perplexities than\nwithout down-selection. When shallow-fused with a state-of-the-art, production\nspeech engine, our LM achieves WER reductions of up to 24% relative on\nrare-word sentences (without changing overall WER) compared to a baseline LM\ntrained on the raw corpus. These gains are further validated through favorable\nside-by-side evaluations on live voice search traffic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1\">Cal Peyser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HealthPrompt: A Zero-shot Learning Paradigm for Clinical Natural Language Processing. (arXiv:2203.05061v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05061","description":"<p>Deep learning algorithms are dependent on the availability of large-scale\nannotated clinical text datasets. The lack of such publicly available datasets\nis the biggest bottleneck for the development of clinical Natural Language\nProcessing(NLP) systems. Zero-Shot Learning(ZSL) refers to the use of deep\nlearning models to classify instances from new classes of which no training\ndata have been seen before. Prompt-based learning is an emerging ZSL technique\nwhere we define task-based templates for NLP tasks. We developed a novel\nprompt-based clinical NLP framework called HealthPrompt and applied the\nparadigm of prompt-based learning on clinical texts. In this technique, rather\nthan fine-tuning a Pre-trained Language Model(PLM), the task definitions are\ntuned by defining a prompt template. We performed an in-depth analysis of\nHealthPrompt on six different PLMs in a no-data setting. Our experiments prove\nthat prompts effectively capture the context of clinical texts and perform\nremarkably well without any training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sivarajkumar_S/0/1/0/all/0/1\">Sonish Sivarajkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks. (arXiv:2203.05081v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05081","description":"<p>Natural language explanation (NLE) models aim at explaining the\ndecision-making process of a black box system via generating natural language\nsentences which are human-friendly, high-level and fine-grained. Current NLE\nmodels explain the decision-making process of a vision or vision-language model\n(a.k.a., task model), e.g., a VQA model, via a language model (a.k.a.,\nexplanation model), e.g., GPT. Other than the additional memory resources and\ninference time required by the task model, the task and explanation models are\ncompletely independent, which disassociates the explanation from the reasoning\nprocess made to predict the answer. We introduce NLX-GPT, a general, compact\nand faithful language model that can simultaneously predict an answer and\nexplain it. We first conduct pre-training on large scale data of image-caption\npairs for general understanding of images, and then formulate the answer as a\ntext prediction task along with the explanation. Without region proposals nor a\ntask model, our resulting overall framework attains better evaluation scores,\ncontains much less parameters and is 15$\\times$ faster than the current SoA\nmodel. We then address the problem of evaluating the explanations which can be\nin many times generic, data-biased and can come in several forms. We therefore\ndesign 2 new evaluation measures: (1) explain-predict and (2) retrieval-based\nattack, a self-evaluation framework that requires no labels. Code is at:\nhttps://github.com/fawazsammani/nlxgpt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sammani_F/0/1/0/all/0/1\">Fawaz Sammani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_T/0/1/0/all/0/1\">Tanmoy Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Librarian-in-the-Loop: A Natural Language Processing Paradigm for Detecting Informal Mentions of Research Data in Academic Literature. (arXiv:2203.05112v1 [cs.DL])","link":"http://arxiv.org/abs/2203.05112","description":"<p>Data citations provide a foundation for studying research data impact.\nCollecting and managing data citations is a new frontier in archival science\nand scholarly communication. However, the discovery and curation of research\ndata citations is labor intensive. Data citations that reference unique\nidentifiers (i.e. DOIs) are readily findable; however, informal mentions made\nto research data are more challenging to infer. We propose a natural language\nprocessing (NLP) paradigm to support the human task of identifying informal\nmentions made to research datasets. The work of discovering informal data\nmentions is currently performed by librarians and their staff in the\nInter-university Consortium for Political and Social Research (ICPSR), a large\nsocial science data archive that maintains a large bibliography of data-related\nliterature. The NLP model is bootstrapped from data citations actively\ncollected by librarians at ICPSR. The model combines pattern matching with\nmultiple iterations of human annotations to learn additional rules for\ndetecting informal data mentions. These examples are then used to train an NLP\npipeline. The librarian-in-the-loop paradigm is centered in the data work\nperformed by ICPSR librarians, supporting broader efforts to build a more\ncomprehensive bibliography of data-related literature that reflects the\nscholarly communities of research data users.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lizhou Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lafia_S/0/1/0/all/0/1\">Sara Lafia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bleckley_D/0/1/0/all/0/1\">David Bleckley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moss_E/0/1/0/all/0/1\">Elizabeth Moss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomer_A/0/1/0/all/0/1\">Andrea Thomer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hemphill_L/0/1/0/all/0/1\">Libby Hemphill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Internet-augmented language models through few-shot prompting for open-domain question answering. (arXiv:2203.05115v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05115","description":"<p>In this work, we aim to capitalize on the unique few-shot capabilities\noffered by large-scale language models to overcome some of their challenges\nwith respect to grounding to factual and up-to-date information. Motivated by\nsemi-parametric language models, which ground their decisions in external\nretrieved evidence, we use few-shot prompting to learn to condition language\nmodels on information returned from the web using Google Search, a broad and\nconstantly updated knowledge source. Our approach does not involve fine-tuning\nor learning additional parameters, thus making it applicable to any language\nmodel, offering like this a strong baseline. Indeed, we find that language\nmodels conditioned on the web surpass performance of closed-book models of\nsimilar, or even larger, model sizes in open-domain question answering.\nFinally, we find that increasing the inference-time compute of models, achieved\nvia using multiple retrieved evidences to generate multiple answers followed by\na reranking stage, alleviates generally decreased performance of smaller\nfew-shot language models. All in all, our findings suggest that it might be\nbeneficial to slow down the race towards the biggest model and instead shift\nthe attention towards finding more effective ways to use models, including but\nnot limited to better prompting or increasing inference-time compute.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribovskaya_E/0/1/0/all/0/1\">Elena Gribovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stokowiec_W/0/1/0/all/0/1\">Wojciech Stokowiec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigorev_N/0/1/0/all/0/1\">Nikolai Grigorev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compilable Neural Code Generation with Compiler Feedback. (arXiv:2203.05132v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05132","description":"<p>Automatically generating compilable programs with (or without) natural\nlanguage descriptions has always been a touchstone problem for computational\nlinguistics and automated software engineering. Existing deep-learning\napproaches model code generation as text generation, either constrained by\ngrammar structures in decoder, or driven by pre-trained language models on\nlarge-scale code corpus (e.g., CodeGPT, PLBART, and CodeT5). However, few of\nthem account for compilability of the generated programs. To improve\ncompilability of the generated programs, this paper proposes COMPCODER, a\nthree-stage pipeline utilizing compiler feedback for compilable code\ngeneration, including language model fine-tuning, compilability reinforcement,\nand compilability discrimination. Comprehensive experiments on two code\ngeneration tasks demonstrate the effectiveness of our proposed approach,\nimproving the success rate of compilation from 44.18 to 89.18 in code\ncompletion on average and from 70.3 to 96.2 in text-to-code generation,\nrespectively, when comparing with the state-of-the-art CodeGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yao Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pingyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speciesist Language and Nonhuman Animal Bias in English Masked Language Models. (arXiv:2203.05140v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05140","description":"<p>Various existing studies have analyzed what social biases are inherited by\nNLP models. These biases may directly or indirectly harm people, therefore\nprevious studies have focused only on human attributes. If the social biases in\nNLP models can be indirectly harmful to humans involved, then the models can\nalso indirectly harm nonhuman animals. However, until recently no research on\nsocial biases in NLP regarding nonhumans existed. In this paper, we analyze\nbiases to nonhuman animals, i.e. speciesist bias, inherent in English Masked\nLanguage Models. We analyze this bias using template-based and corpus-extracted\nsentences which contain speciesist (or non-speciesist) language, to show that\nthese models tend to associate harmful words with nonhuman animals. Our code\nfor reproducing the experiments will be made available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takeshita_M/0/1/0/all/0/1\">Masashi Takeshita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepka_R/0/1/0/all/0/1\">Rafal Rzepka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_K/0/1/0/all/0/1\">Kenji Araki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Accurate Unsupervised Method for Joint Entity Alignment and Dangling Entity Detection. (arXiv:2203.05147v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05147","description":"<p>Knowledge graph integration typically suffers from the widely existing\ndangling entities that cannot find alignment cross knowledge graphs (KGs). The\ndangling entity set is unavailable in most real-world scenarios, and manually\nmining the entity pairs that consist of entities with the same meaning is\nlabor-consuming. In this paper, we propose a novel accurate Unsupervised method\nfor joint Entity alignment (EA) and Dangling entity detection (DED), called\nUED. The UED mines the literal semantic information to generate pseudo entity\npairs and globally guided alignment information for EA and then utilizes the EA\nresults to assist the DED. We construct a medical cross-lingual knowledge graph\ndataset, MedED, providing data for both the EA and DED tasks. Extensive\nexperiments demonstrate that in the EA task, UED achieves EA results comparable\nto those of state-of-the-art supervised EA baselines and outperforms the\ncurrent state-of-the-art EA methods by combining supervised EA data. For the\nDED task, UED obtains high-quality results without supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TextConvoNet:A Convolutional Neural Network based Architecture for Text Classification. (arXiv:2203.05173v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05173","description":"<p>In recent years, deep learning-based models have significantly improved the\nNatural Language Processing (NLP) tasks. Specifically, the Convolutional Neural\nNetwork (CNN), initially used for computer vision, has shown remarkable\nperformance for text data in various NLP problems. Most of the existing\nCNN-based models use 1-dimensional convolving filters n-gram detectors), where\neach filter specialises in extracting n-grams features of a particular input\nword embedding. The input word embeddings, also called sentence matrix, is\ntreated as a matrix where each row is a word vector. Thus, it allows the model\nto apply one-dimensional convolution and only extract n-gram based features\nfrom a sentence matrix. These features can be termed as intra-sentence n-gram\nfeatures. To the extent of our knowledge, all the existing CNN models are based\non the aforementioned concept. In this paper, we present a CNN-based\narchitecture TextConvoNet that not only extracts the intra-sentence n-gram\nfeatures but also captures the inter-sentence n-gram features in input text\ndata. It uses an alternative approach for input matrix representation and\napplies a two-dimensional multi-scale convolutional operation on the input. To\nevaluate the performance of TextConvoNet, we perform an experimental study on\nfive text classification datasets. The results are evaluated by using various\nperformance metrics. The experimental results show that the presented\nTextConvoNet outperforms state-of-the-art machine learning and deep learning\nmodels for text classification purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_S/0/1/0/all/0/1\">Sanskar Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chouhan_S/0/1/0/all/0/1\">Satyendra Singh Chouhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathore_S/0/1/0/all/0/1\">Santosh Singh Rathore</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05203","description":"<p>3D dense captioning is a recently-proposed novel task, where point clouds\ncontain more geometric information than the 2D counterpart. However, it is also\nmore challenging due to the higher complexity and wider variety of inter-object\nrelations. Existing methods only treat such relations as by-products of object\nfeature learning in graphs without specifically encoding them, which leads to\nsub-optimal results. In this paper, aiming at improving 3D dense captioning via\ncapturing and utilizing the complex relations in the 3D scene, we propose MORE,\na Multi-Order RElation mining model, to support generating more descriptive and\ncomprehensive captions. Technically, our MORE encodes object relations in a\nprogressive manner since complex relations can be deduced from a limited number\nof basic ones. We first devise a novel Spatial Layout Graph Convolution (SLGC),\nwhich semantically encodes several first-order relations as edges of a graph\nconstructed over 3D object proposals. Next, from the resulting graph, we\nfurther extract multiple triplets which encapsulate basic first-order relations\nas the basic unit and construct several Object-centric Triplet Attention Graphs\n(OTAG) to infer multi-order relations for every target object. The updated node\nfeatures from OTAG are aggregated and fed into the caption decoder to provide\nabundant relational cues so that captions including diverse relations with\ncontext objects can be generated. Extensive experiments on the Scan2Cap dataset\nprove the effectiveness of our proposed MORE and its components, and we also\noutperform the current state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithfulness in Natural Language Generation: A Systematic Survey of Analysis, Evaluation and Optimization Methods. (arXiv:2203.05227v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05227","description":"<p>Natural Language Generation (NLG) has made great progress in recent years due\nto the development of deep learning techniques such as pre-trained language\nmodels. This advancement has resulted in more fluent, coherent and even\nproperties controllable (e.g. stylistic, sentiment, length etc.) generation,\nnaturally leading to development in downstream tasks such as abstractive\nsummarization, dialogue generation, machine translation, and data-to-text\ngeneration. However, the faithfulness problem that the generated text usually\ncontains unfaithful or non-factual information has become the biggest\nchallenge, which makes the performance of text generation unsatisfactory for\npractical applications in many real-world scenarios. Many studies on analysis,\nevaluation, and optimization methods for faithfulness problems have been\nproposed for various tasks, but have not been organized, compared and discussed\nin a combined manner. In this survey, we provide a systematic overview of the\nresearch progress on the faithfulness problem of NLG, including problem\nanalysis, evaluation metrics and optimization methods. We organize the\nevaluation and optimization methods for different tasks into a unified taxonomy\nto facilitate comparison and learning across tasks. Several research trends are\ndiscussed further.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Moye Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach. (arXiv:2203.05243v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05243","description":"<p>Temporal Sentence Grounding in Videos (TSGV), which aims to ground a natural\nlanguage sentence in an untrimmed video, has drawn widespread attention over\nthe past few years. However, recent studies have found that current benchmark\ndatasets may have obvious moment annotation biases, enabling several simple\nbaselines even without training to achieve SOTA performance. In this paper, we\ntake a closer look at existing evaluation protocols, and find both the\nprevailing dataset and evaluation metrics are the devils that lead to\nuntrustworthy benchmarking. Therefore, we propose to re-organize the two\nwidely-used datasets, making the ground-truth moment distributions different in\nthe training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile,\nwe introduce a new evaluation metric \"dR@n,IoU@m\" that discounts the basic\nrecall scores to alleviate the inflating evaluation caused by biased datasets.\nNew benchmarking results indicate that our proposed evaluation protocols can\nbetter monitor the research progress. Furthermore, we propose a novel\ncausality-based Multi-branch Deconfounding Debiasing (MDD) framework for\nunbiased moment prediction. Specifically, we design a multi-branch deconfounder\nto eliminate the effects caused by multiple confounders with causal\nintervention. In order to help the model better align the semantics between\nsentence queries and video moments, we enhance the representations during\nfeature encoding. Specifically, for textual information, the query is parsed\ninto several verb-centered phrases to obtain a more fine-grained textual\nfeature. For visual information, the positional information has been decomposed\nfrom moment features to enhance representations of moments with diverse\nlocations. Extensive experiments demonstrate that our proposed approach can\nachieve competitive results among existing SOTA approaches and outperform the\nbase model with great gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiaohan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yitian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Backward and Forward: Self-Knowledge Distillation with Bidirectional Decoder for Neural Machine Translation. (arXiv:2203.05248v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05248","description":"<p>Neural Machine Translation(NMT) models are usually trained via unidirectional\ndecoder which corresponds to optimizing one-step-ahead prediction. However,\nthis kind of unidirectional decoding framework may incline to focus on local\nstructure rather than global coherence. To alleviate this problem, we propose a\nnovel method, Self-Knowledge Distillation with Bidirectional Decoder for Neural\nMachine Translation(SBD-NMT). We deploy a backward decoder which can act as an\neffective regularization method to the forward decoder. By leveraging the\nbackward decoder's information about the longer-term future, distilling\nknowledge learned in the backward decoder can encourage auto-regressive NMT\nmodels to plan ahead. Experiments show that our method is significantly better\nthan the strong Transformer baselines on multiple machine translation data\nsets. Our codes will be released on github soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuanwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Libin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1\">Disheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yanjun Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting Neural Response measurements & Computational Models of language: a non-comprehensive guide. (arXiv:2203.05300v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05300","description":"<p>Understanding the neural basis of language comprehension in the brain has\nbeen a long-standing goal of various scientific research programs. Recent\nadvances in language modelling and in neuroimaging methodology promise\npotential improvements in both the investigation of language's neurobiology and\nin the building of better and more human-like language models. This survey\ntraces a line from early research linking Event Related Potentials and\ncomplexity measures derived from simple language models to contemporary studies\nemploying Artificial Neural Network models trained on large corpora in\ncombination with neural response recordings from multiple modalities using\nnaturalistic stimuli.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05321","description":"<p>We present StyleBabel, a unique open access dataset of natural language\ncaptions and free-form tags describing the artistic style of over 135K digital\nartworks, collected via a novel participatory method from experts studying at\nspecialist art and design schools. StyleBabel was collected via an iterative\nmethod, inspired by `Grounded Theory': a qualitative approach that enables\nannotation while co-evolving a shared language for fine-grained artistic style\nattribute description. We demonstrate several downstream tasks for StyleBabel,\nadapting the recent ALADIN architecture for fine-grained style similarity, to\ntrain cross-modal embeddings for: 1) free-form tag generation; 2) natural\nlanguage description of artistic style; 3) fine-grained text search of style.\nTo do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and\ncross-modal representation learning, achieving a state of the art accuracy in\nfine-grained style retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruta_D/0/1/0/all/0/1\">Dan Ruta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marri_N/0/1/0/all/0/1\">Naveen Marri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1\">Ajinkya Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briggs_J/0/1/0/all/0/1\">Jo Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speed_C/0/1/0/all/0/1\">Chris Speed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faieta_B/0/1/0/all/0/1\">Baldo Faieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipkowski_A/0/1/0/all/0/1\">Alex Filipkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AIFB-WebScience at SemEval-2022 Task 12: Relation Extraction First -- Using Relation Extraction to Identify Entities. (arXiv:2203.05325v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05325","description":"<p>In this paper, we present an end-to-end joint entity and relation extraction\napproach based on transformer-based language models. We apply the model to the\ntask of linking mathematical symbols to their descriptions in LaTeX documents.\nIn contrast to existing approaches, which perform entity and relation\nextraction in sequence, our system incorporates information from relation\nextraction into entity extraction. This means that the system can be trained\neven on data sets where only a subset of all valid entity spans is annotated.\nWe provide an extensive evaluation of the proposed system and its strengths and\nweaknesses. Our approach, which can be scaled dynamically in computational\ncomplexity at inference time, produces predictions with high precision and\nreaches 3rd place in the leaderboard of SemEval-2022 Task 12. For inputs in the\ndomain of physics and math, it achieves high relation extraction macro f1\nscores of 95.43% and 79.17%, respectively. The code used for training and\nevaluating our models is available at: https://github.com/nicpopovic/RE1st\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nicholas Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laurito_W/0/1/0/all/0/1\">Walter Laurito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_M/0/1/0/all/0/1\">Michael F&#xe4;rber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-enriched Attention Network with Group-wise Semantic for Visual Storytelling. (arXiv:2203.05346v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05346","description":"<p>As a technically challenging topic, visual storytelling aims at generating an\nimaginary and coherent story with narrative multi-sentences from a group of\nrelevant images. Existing methods often generate direct and rigid descriptions\nof apparent image-based contents, because they are not capable of exploring\nimplicit information beyond images. Hence, these schemes could not capture\nconsistent dependencies from holistic representation, impairing the generation\nof reasonable and fluent story. To address these problems, a novel\nknowledge-enriched attention network with group-wise semantic model is\nproposed. Three main novel components are designed and supported by substantial\nexperiments to reveal practical advantages. First, a knowledge-enriched\nattention network is designed to extract implicit concepts from external\nknowledge system, and these concepts are followed by a cascade cross-modal\nattention mechanism to characterize imaginative and concrete representations.\nSecond, a group-wise semantic module with second-order pooling is developed to\nexplore the globally consistent guidance. Third, a unified one-stage story\ngeneration model with encoder-decoder structure is proposed to simultaneously\ntrain and infer the knowledge-enriched attention network, group-wise semantic\nmodule and multi-modal story generation decoder in an end-to-end fashion.\nSubstantial experiments on the popular Visual Storytelling dataset with both\nobjective and subjective evaluation metrics demonstrate the superior\nperformance of the proposed scheme as compared with other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tengpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SATLab at SemEval-2022 Task 4: Trying to Detect Patronizing and Condescending Language with only Character and Word N-grams. (arXiv:2203.05355v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05355","description":"<p>A logistic regression model only fed with character and word n-grams is\nproposed for the SemEval-2022 Task 4 on Patronizing and Condescending Language\nDetection (PCL). It obtained an average level of performance, well above the\nperformance of a system that tries to guess without using any knowledge about\nthe task, but much lower than the best teams. As the proposed model is very\nsimilar to the one that performed well on a task requiring to automatically\nidentify hate speech and offensive content, this paper confirms the difficulty\nof PCL detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bestgen_Y/0/1/0/all/0/1\">Yves Bestgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KSoF: The Kassel State of Fluency Dataset -- A Therapy Centered Dataset of Stuttering. (arXiv:2203.05383v1 [eess.AS])","link":"http://arxiv.org/abs/2203.05383","description":"<p>Stuttering is a complex speech disorder that negatively affects an\nindividual's ability to communicate effectively. Persons who stutter (PWS)\noften suffer considerably under the condition and seek help through therapy.\nFluency shaping is a therapy approach where PWSs learn to modify their speech\nto help them to overcome their stutter. Mastering such speech techniques takes\ntime and practice, even after therapy. Shortly after therapy, success is\nevaluated highly, but relapse rates are high. To be able to monitor speech\nbehavior over a long time, the ability to detect stuttering events and\nmodifications in speech could help PWSs and speech pathologists to track the\nlevel of fluency. Monitoring could create the ability to intervene early by\ndetecting lapses in fluency. To the best of our knowledge, no public dataset is\navailable that contains speech from people who underwent stuttering therapy\nthat changed the style of speaking. This work introduces the Kassel State of\nFluency (KSoF), a therapy-based dataset containing over 5500 clips of PWSs. The\nclips were labeled with six stuttering-related event types: blocks,\nprolongations, sound repetitions, word repetitions, interjections, and -\nspecific to therapy - speech modifications. The audio was recorded during\ntherapy sessions at the Institut der Kasseler Stottertherapie. The data will be\nmade available for research purposes upon request.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bayerl_S/0/1/0/all/0/1\">Sebastian P. Bayerl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gudenberg_A/0/1/0/all/0/1\">Alexander Wolff von Gudenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honig_F/0/1/0/all/0/1\">Florian H&#xf6;nig</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Noth_E/0/1/0/all/0/1\">Elmar N&#xf6;th</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedhammer_K/0/1/0/all/0/1\">Korbinian Riedhammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faking Fake News for Real Fake News Detection: Propaganda-loaded Training Data Generation. (arXiv:2203.05386v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05386","description":"<p>While there has been a lot of research and many recent advances in neural\nfake news detection, defending against human-written disinformation remains\nunderexplored. Upon analyzing current approaches for fake news generation and\nhuman-crafted articles, we found that there is a gap between them, which can\nexplain the poor performance on detecting human-written fake news for detectors\ntrained on automatically generated data. To address this issue, we propose a\nnovel framework for generating articles closer to human-written ones.\nSpecifically, we perform self-critical sequence training with natural language\ninference to ensure the validity of the generated articles. We then explicitly\nincorporate propaganda techniques into the generated articles to mimic how\nhumans craft fake news. Eventually, we create a fake news detection training\ndataset, PropaNews, which includes 2,256 examples. Our experimental results\nshow that detectors trained on PropaNews are 7.3% to 12.0% more accurate for\ndetecting human-written disinformation than for counterparts trained on data\ngenerated by state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kung-Hsiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OneRel:Joint Entity and Relation Extraction with One Module in One Step. (arXiv:2203.05412v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05412","description":"<p>Joint entity and relation extraction is an essential task in natural language\nprocessing and knowledge graph construction. Existing approaches usually\ndecompose the joint extraction task into several basic modules or processing\nsteps to make it easy to conduct. However, such a paradigm ignores the fact\nthat the three elements of a triple are interdependent and indivisible.\nTherefore, previous joint methods suffer from the problems of cascading errors\nand redundant information. To address these issues, in this paper, we propose a\nnovel joint entity and relation extraction model, named OneRel, which casts\njoint extraction as a fine-grained triple classification problem. Specifically,\nour model consists of a scoring-based classifier and a relation-specific horns\ntagging strategy. The former evaluates whether a token pair and a relation\nbelong to a factual triple. The latter ensures a simple but effective decoding\nprocess. Extensive experimental results on two widely used datasets demonstrate\nthat the proposed method performs better than the state-of-the-art baselines,\nand delivers consistent performance gain on complex scenarios of various\noverlapping patterns and multiple triples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1\">Yu-Ming Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Norm Recognition and its application to Portuguese Law. (arXiv:2203.05425v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05425","description":"<p>Being able to clearly interpret legal texts and fully understanding our\nrights, obligations and other legal norms has become progressively more\nimportant in the digital society. However, simply giving citizens access to the\nlaws is not enough, as there is a need to provide meaningful information that\ncater to their specific queries and needs. For this, it is necessary to extract\nthe relevant semantic information present in legal texts. Thus, we introduce\nthe SNR (Semantic Norm Recognition) system, an automatic semantic information\nextraction system trained on a domain-specific (legal) text corpus taken from\nPortuguese Consumer Law. The SNR system uses the Portuguese Bert (BERTimbau)\nand was trained on a legislative Portuguese corpus. We demonstrate how our\nsystem achieved good results (81.44\\% F1-score) on this domain-specific corpus,\ndespite existing noise, and how it can be used to improve downstream tasks such\nas information retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duarte_M/0/1/0/all/0/1\">Maria Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_P/0/1/0/all/0/1\">Pedro A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_J/0/1/0/all/0/1\">Jo&#xe3;o Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baptista_J/0/1/0/all/0/1\">Jorge Baptista</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicNLG Suite: Multilingual Datasets for Diverse NLG Tasks in Indic Languages. (arXiv:2203.05437v1 [cs.CL])","link":"http://arxiv.org/abs/2203.05437","description":"<p>In this paper, we present the IndicNLG suite, a collection of datasets for\nbenchmarking Natural Language Generation (NLG) for 11 Indic languages. We focus\non five diverse tasks, namely, biography generation using Wikipedia infoboxes\n(WikiBio), news headline generation, sentence summarization, question\ngeneration and paraphrase generation. We describe the process of creating the\ndatasets and present statistics of the dataset, following which we train and\nreport a variety of strong monolingual and multilingual baselines that leverage\npre-trained sequence-to-sequence models and analyze the results to understand\nthe challenges involved in Indic language NLG. To the best of our knowledge,\nthis is the first NLG dataset for Indic languages and also the largest\nmultilingual NLG dataset. Our methods can also be easily applied to\nmodest-resource languages with reasonable monolingual and parallel corpora, as\nwell as corpora containing structured data like Wikipedia. We hope this dataset\nspurs research in NLG on diverse languages and tasks, particularly for Indic\nlanguages. The datasets and models are publicly available at\nhttps://indicnlp.ai4bharat.org/indicnlg-suite.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aman Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrotriya_H/0/1/0/all/0/1\">Himani Shrotriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahu_P/0/1/0/all/0/1\">Prachi Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puduppully_R/0/1/0/all/0/1\">Ratish Puduppully</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Amogh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval. (arXiv:2203.05465v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05465","description":"<p>Dual encoders and cross encoders have been widely used for image-text\nretrieval. Between the two, the dual encoder encodes the image and text\nindependently followed by a dot product, while the cross encoder jointly feeds\nimage and text as the input and performs dense multi-modal fusion. These two\narchitectures are typically modeled separately without interaction. In this\nwork, we propose LoopITR, which combines them in the same network for joint\nlearning. Specifically, we let the dual encoder provide hard negatives to the\ncross encoder, and use the more discriminative cross encoder to distill its\npredictions back to the dual encoder. Both steps are efficiently performed\ntogether in the same model. Our work centers on empirical analyses of this\ncombined architecture, putting the main focus on the design of the distillation\nobjective. Our experimental results highlight the benefits of training the two\nencoders in the same network, and demonstrate that distillation can be quite\neffective with just a few hard negative examples. Experiments on two standard\ndatasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual\nencoder performance when compared with approaches using a similar amount of\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengjiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v1 [cs.LG])","link":"http://arxiv.org/abs/2203.05482","description":"<p>The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. As a highlight, the resulting ViT-G model attains 90.94% top-1\naccuracy on ImageNet, a new state of the art. Furthermore, we show that the\nmodel soup approach extends to multiple image classification and natural\nlanguage processing tasks, improves out-of-distribution performance, and\nimproves zero-shot performance on new downstream tasks. Finally, we\nanalytically relate the performance similarity of weight-averaging and\nlogit-ensembling to flatness of the loss and confidence of the predictions, and\nvalidate this relation empirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Prompt Learning for Vision-Language Models. (arXiv:2203.05557v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05557","description":"<p>With the rise of powerful pre-trained vision-language models like CLIP, it\nbecomes essential to investigate ways to adapt these models to downstream\ndatasets. A recently proposed method named Context Optimization (CoOp)\nintroduces the concept of prompt learning -- a recent trend in NLP -- to the\nvision domain for adapting pre-trained vision-language models. Specifically,\nCoOp turns context words in a prompt into a set of learnable vectors and, with\nonly a few labeled images for learning, can achieve huge improvements over\nintensively-tuned manual prompts. In our study we identify a critical problem\nof CoOp: the learned context is not generalizable to wider unseen classes\nwithin the same dataset, suggesting that CoOp overfits base classes observed\nduring training. To address the problem, we propose Conditional Context\nOptimization (CoCoOp), which extends CoOp by further learning a lightweight\nneural network to generate for each image an input-conditional token (vector).\nCompared to CoOp's static prompts, our dynamic prompts adapt to each instance\nand are thus less sensitive to class shift. Extensive experiments show that\nCoCoOp generalizes much better than CoOp to unseen classes, even showing\npromising transferability beyond a single dataset; and yields stronger domain\ngeneralization performance as well. Code is available at\nhttps://github.com/KaiyangZhou/CoOp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base. (arXiv:2007.03875v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.03875","description":"<p>Complex question answering over knowledge base (Complex KBQA) is challenging\nbecause it requires various compositional reasoning capabilities, such as\nmulti-hop inference, attribute comparison, set operation. Existing benchmarks\nhave some shortcomings that limit the development of Complex KBQA: 1) they only\nprovide QA pairs without explicit reasoning processes; 2) questions are poor in\ndiversity or scale. To this end, we introduce KQA Pro, a dataset for Complex\nKBQA including ~120K diverse natural language questions. We introduce a\ncompositional and interpretable programming language KoPL to represent the\nreasoning process of complex questions. For each question, we provide the\ncorresponding KoPL program and SPARQL query, so that KQA Pro serves for both\nKBQA and semantic parsing tasks. Experimental results show that SOTA KBQA\nmethods cannot achieve promising results on KQA Pro as on current datasets,\nwhich suggests that KQA Pro is challenging and Complex KBQA requires further\nresearch efforts. We also treat KQA Pro as a diagnostic dataset for testing\nmultiple reasoning skills, conduct a thorough evaluation of existing models and\ndiscuss further directions for Complex KBQA. Our codes and datasets can be\nobtained from https://github.com/shijx12/KQAPro_Baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shulin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lunyiu Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yutong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Extreme Multi-label to Multi-class: A Hierarchical Approach for Automated ICD-10 Coding Using Phrase-level Attention. (arXiv:2102.09136v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.09136","description":"<p>Clinical coding is the task of assigning a set of alphanumeric codes,\nreferred to as ICD (International Classification of Diseases), to a medical\nevent based on the context captured in a clinical narrative. The latest version\nof ICD, ICD-10, includes more than 70,000 codes. As this is a labor-intensive\nand error-prone task, automatic ICD coding of medical reports using machine\nlearning has gained significant interest in the last decade. Existing\nliterature has modeled this problem as a multi-label task. Nevertheless, such\nmulti-label approach is challenging due to the extremely large label set size.\nFurthermore, the interpretability of the predictions is essential for the\nendusers (e.g., healthcare providers and insurance companies). In this paper,\nwe propose a novel approach for automatic ICD coding by reformulating the\nextreme multi-label problem into a simpler multi-class problem using a\nhierarchical solution. We made this approach viable through extensive data\ncollection to acquire phrase-level human coder annotations to supervise our\nmodels on learning the specific relations between the input text and predicted\nICD codes. Our approach employs two independently trained networks, the\nsentence tagger and the ICD classifier, stacked hierarchically to predict a\ncodeset for a medical report. The sentence tagger identifies focus sentences\ncontaining a medical event or concept relevant to an ICD coding. Using a\nsupervised attention mechanism, the ICD classifier then assigns each focus\nsentence with an ICD code. The proposed approach outperforms strong baselines\nby large margins of 23% in subset accuracy, 18% in micro-F1, and 15% in\ninstance based F-1. With our proposed approach, interpretability is achieved\nnot through implicitly learned attention scores but by attributing each\nprediction to a particular sentence and words selected by human coders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sen_C/0/1/0/all/0/1\">Cansu Sen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_B/0/1/0/all/0/1\">Bingyang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslam_J/0/1/0/all/0/1\">Javed Aslam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebi_A/0/1/0/all/0/1\">Amir Tahmasebi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummScreen: A Dataset for Abstractive Screenplay Summarization. (arXiv:2104.07091v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07091","description":"<p>We introduce SummScreen, a summarization dataset comprised of pairs of TV\nseries transcripts and human written recaps. The dataset provides a challenging\ntestbed for abstractive summarization for several reasons. Plot details are\noften expressed indirectly in character dialogues and may be scattered across\nthe entirety of the transcript. These details must be found and integrated to\nform the succinct plot descriptions in the recaps. Also, TV scripts contain\ncontent that does not directly pertain to the central plot but rather serves to\ndevelop characters or provide comic relief. This information is rarely\ncontained in recaps. Since characters are fundamental to TV series, we also\npropose two entity-centric evaluation metrics. Empirically, we characterize the\ndataset by evaluating several methods, including neural models and those based\non nearest neighbors. An oracle extractive approach outperforms all benchmarked\nmodels according to automatic metrics, showing that the neural models are\nunable to fully exploit the input transcripts. Human evaluation and qualitative\nanalysis reveal that our non-oracle models are competitive with their oracle\ncounterparts in terms of generating faithful plot events and can benefit from\nbetter content selectors. Both oracle and non-oracle models generate unfaithful\nfacts, suggesting future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingda Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1\">Zewei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1\">Sam Wiseman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Neurons in Pretrained Transformers. (arXiv:2104.08696v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08696","description":"<p>Large-scale pretrained language models are surprisingly good at recalling\nfactual knowledge presented in the training corpus. In this paper, we present\npreliminary studies on how factual knowledge is stored in pretrained\nTransformers by introducing the concept of knowledge neurons. Specifically, we\nexamine the fill-in-the-blank cloze task for BERT. Given a relational fact, we\npropose a knowledge attribution method to identify the neurons that express the\nfact. We find that the activation of such knowledge neurons is positively\ncorrelated to the expression of their corresponding facts. In our case studies,\nwe attempt to leverage knowledge neurons to edit (such as update, and erase)\nspecific factual knowledge without fine-tuning. Our results shed light on\nunderstanding the storage of knowledge within pretrained Transformers. The code\nis available at https://github.com/Hunter-DDM/knowledge-neurons.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yaru Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation Of Word Embeddings From Large-Scale French Web Content. (arXiv:2105.01990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.01990","description":"<p>Distributed word representations are popularly used in many tasks in natural\nlanguage processing. Adding that pretrained word vectors on huge text corpus\nachieved high performance in many different NLP tasks. This paper introduces\nmultiple high-quality word vectors for the French language where two of them\nare trained on massive crawled French data during this study and the others are\ntrained on an already existing French corpus. We also evaluate the quality of\nour proposed word vectors and the existing French word vectors on the French\nword analogy task. In addition, we do the evaluation on multiple real NLP tasks\nthat shows the important performance enhancement of the pre-trained word\nvectors compared to the existing and random ones. Finally, we created a demo\nweb application to test and visualize the obtained word embeddings. The\nproduced French word embeddings are available to the public, along with the\nfinetuning code on the NLU tasks and the demo code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdine_H/0/1/0/all/0/1\">Hadi Abdine</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xypolopoulos_C/0/1/0/all/0/1\">Christos Xypolopoulos</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Eddine_M/0/1/0/all/0/1\">Moussa Kamal Eddine</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1\">Michalis Vazirgiannis</a> (1 and 2) ((1) Ecole Polytechnique, (2) AUEB)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing to New Domains by Mapping Natural Language to Lifted LTL. (arXiv:2110.05603v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05603","description":"<p>Recent work on using natural language to specify commands to robots has\ngrounded that language to LTL. However, mapping natural language task\nspecifications to LTL task specifications using language models require\nprobability distributions over finite vocabulary. Existing state-of-the-art\nmethods have extended this finite vocabulary to include unseen terms from the\ninput sequence to improve output generalization. However, novel\nout-of-vocabulary atomic propositions cannot be generated using these methods.\nTo overcome this, we introduce an intermediate contextual query representation\nwhich can be learned from single positive task specification examples,\nassociating a contextual query with an LTL template. We demonstrate that this\nintermediate representation allows for generalization over unseen object\nreferences, assuming accurate groundings are available. We compare our method\nof mapping natural language task specifications to intermediate contextual\nqueries against state-of-the-art CopyNet models capable of translating natural\nlanguage to LTL, by evaluating whether correct LTL for manipulation and\nnavigation task specifications can be output, and show that our method\noutperforms the CopyNet model on unseen object references. We demonstrate that\nthe grounded LTL our method outputs can be used for planning in a simulated\nOO-MDP environment. Finally, we discuss some common failure modes encountered\nwhen translating natural language task specifications to grounded LTL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsiung_E/0/1/0/all/0/1\">Eric Hsiung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Hiloni Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_J/0/1/0/all/0/1\">Junchi Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_R/0/1/0/all/0/1\">Roma Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1\">Stefanie Tellex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1\">George Konidaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Curriculum Learning for AMR Parsing. (arXiv:2110.07855v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07855","description":"<p>Abstract Meaning Representation (AMR) parsing aims to translate sentences to\nsemantic representation with a hierarchical structure, and is recently\nempowered by pretrained sequence-to-sequence models. However, there exists a\ngap between their flat training objective (i.e., equally treats all output\ntokens) and the hierarchical AMR structure, which limits the model\ngeneralization. To bridge this gap, we propose a Hierarchical Curriculum\nLearning (HCL) framework with Structure-level (SC) and Instance-level Curricula\n(IC). SC switches progressively from core to detail AMR semantic elements while\nIC transits from structure-simple to -complex AMR instances during training.\nThrough these two warming-up processes, HCL reduces the difficulty of learning\ncomplex structures, thus the flat model can better adapt to the AMR hierarchy.\nExtensive experiments on AMR2.0, AMR3.0, structure-complex and\nout-of-distribution situations verify the effectiveness of HCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Damai Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Dangers of Underclaiming: Reasons for Caution When Reporting How NLP Systems Fail. (arXiv:2110.08300v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08300","description":"<p>Researchers in NLP often frame and discuss research results in ways that\nserve to deemphasize the field's successes, often in response to the field's\nwidespread hype. Though well-meaning, this has yielded many misleading or false\nclaims about the limits of our best technology. This is a problem, and it may\nbe more serious than it looks: It harms our credibility in ways that can make\nit harder to mitigate present-day harms, like those involving biased systems\nfor content moderation or resume screening. It also limits our ability to\nprepare for the potentially enormous impacts of more distant future advances.\nThis paper urges researchers to be careful about these claims and suggests some\nresearch directions and communication strategies that will make it easier to\navoid or rebut them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Seeking Patterns, Not just Memorizing Procedures: Contrastive Learning for Solving Math Word Problems. (arXiv:2110.08464v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08464","description":"<p>Math Word Problem (MWP) solving needs to discover the quantitative\nrelationships over natural language narratives. Recent work shows that existing\nmodels memorize procedures from context and rely on shallow heuristics to solve\nMWPs. In this paper, we look at this issue and argue that the cause is a lack\nof overall understanding of MWP patterns. We first investigate how a neural\nnetwork understands patterns only from semantics, and observe that, if the\nprototype equations are the same, most problems get closer representations and\nthose representations apart from them or close to other prototypes tend to\nproduce wrong solutions. Inspired by it, we propose a contrastive learning\napproach, where the neural network perceives the divergence of patterns. We\ncollect contrastive examples by converting the prototype equation into a tree\nand seeking similar tree structures. The solving model is trained with an\nauxiliary objective on the collected examples, resulting in the representations\nof problems with similar prototypes being pulled closer. We conduct experiments\non the Chinese dataset Math23k and the English dataset MathQA. Our method\ngreatly improves the performance in monolingual and multilingual settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhongli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Chao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongzhi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An unsupervised extractive summarization method based on multi-round computation. (arXiv:2112.03203v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03203","description":"<p>Text summarization methods have attracted much attention all the time. In\nrecent years, deep learning has been applied to text summarization, and it\nturned out to be pretty effective. Most of the current text summarization\nmethods based on deep learning are supervised methods which need large-scale\ndatasets. However, large-scale datasets are difficult to obtain in practical\napplications. In this paper, an unsupervised extractive text summarization\nmethod based on multi-round calculation is proposed. Based on the directed\ngraph algorithm, we change the common method which calculates the sentence\nranking at one time to multi-round calculation, and we dynamically optimize the\nrelation of sentences after each round of calculation to reduce the redundancy\nof summarization. Experiments are carried out on four data sets, each\nseparately containing Chinese, English, long and short texts. The experiment\nresults show that our method has better performance than other unsupervised\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dehao Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yingzhu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongliang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kevin Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Generating Counterfactuals for Relation Classification. (arXiv:2202.10668v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10668","description":"<p>The goal of relation classification (RC) is to extract the semantic relations\nbetween/among entities in the text. As a fundamental task in natural language\nprocessing, it is crucial to ensure the robustness of RC models. Despite the\nhigh accuracy current deep neural models have achieved in RC tasks, they are\neasily affected by spurious correlations. One solution to this problem is to\ntrain the model with counterfactually augmented data (CAD) such that it can\nlearn the causation rather than the confounding. However, no attempt has been\nmade on generating counterfactuals for RC tasks. In this paper, we formulate\nthe problem of automatically generating CAD for RC tasks from an entity-centric\nviewpoint, and develop a novel approach to derive contextual counterfactuals\nfor entities. Specifically, we exploit two elementary topological properties,\ni.e., the centrality and the shortest path, in syntactic and semantic\ndependency graphs, to first identify and then intervene on the contextual\ncausal features for entities. We conduct a comprehensive evaluation on four RC\ndatasets by combining our proposed approach with a variety of backbone RC\nmodels. The results demonstrate that our approach not only improves the\nperformance of the backbones, but also makes them more robust in the\nout-of-domain test.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Simultaneous to Streaming Machine Translation by Leveraging Streaming History. (arXiv:2203.02459v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02459","description":"<p>Simultaneous Machine Translation is the task of incrementally translating an\ninput sentence before it is fully available. Currently, simultaneous\ntranslation is carried out by translating each sentence independently of the\npreviously translated text. More generally, Streaming MT can be understood as\nan extension of Simultaneous MT to the incremental translation of a continuous\ninput text stream. In this work, a state-of-the-art simultaneous sentence-level\nMT system is extended to the streaming setup by leveraging the streaming\nhistory. Extensive empirical results are reported on IWSLT Translation Tasks,\nshowing that leveraging the streaming history leads to significant quality\ngains. In particular, the proposed system proves to compare favorably to the\nbest performing systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Jorge Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_A/0/1/0/all/0/1\">Alfons Juan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doctor Recommendation in Online Health Forums via Expertise Learning. (arXiv:2203.02932v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.02932","description":"<p>Huge volumes of patient queries are daily generated on online health forums,\nrendering manual doctor allocation a labor-intensive task. To better help\npatients, this paper studies a novel task of doctor recommendation to enable\nautomatic pairing of a patient to a doctor with relevant expertise. While most\nprior work in recommendation focuses on modeling target users from their past\nbehavior, we can only rely on the limited words in a query to infer a patient's\nneeds for privacy reasons. For doctor modeling, we study the joint effects of\ntheir profiles and previous dialogues with other patients and explore their\ninteractions via self-learning. The learned doctor embeddings are further\nemployed to estimate their capabilities of handling a patient query with a\nmulti-head attention mechanism. For experiments, a large-scale dataset is\ncollected from Chunyu Yisheng, a Chinese online health forum, where our model\nexhibits the state-of-the-art results, outperforming baselines only consider\nprofiles and past dialogues to characterize a doctor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoxin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yubo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_S/0/1/0/all/0/1\">Shi Zong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building and curating conversational corpora for diversity-aware language science and technology. (arXiv:2203.03399v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03399","description":"<p>We present a pipeline and tools to build a maximally natural data set of\nconversational interaction that covers 66 languages and varieties from 32\nphyla. We describe the curation and compilation process moving from diverse\nlanguage documentation corpora to a unified format and describe an open-source\ntool \"convo-parse\" to help in quality control and assessment of conversational\ndata. We conclude with two case studies of how diverse data sets can inform\ninteractional linguistics and speech recognition technology and thus contribute\nto broadening the empirical foundations of language sciences and technologies\nof the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liesenfeld_A/0/1/0/all/0/1\">Andreas Liesenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dingemanse_M/0/1/0/all/0/1\">Mark Dingemanse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fluid registration between lung CT and stationary chest tomosynthesis images. (arXiv:2203.04958v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04958","description":"<p>Registration is widely used in image-guided therapy and image-guided surgery\nto estimate spatial correspondences between organs of interest between planning\nand treatment images. However, while high-quality computed tomography (CT)\nimages are often available at planning time, limited angle acquisitions are\nfrequently used during treatment because of radiation concerns or imaging time\nconstraints. This requires algorithms to register CT images based on limited\nangle acquisitions. We, therefore, formulate a 3D/2D registration approach\nwhich infers a 3D deformation based on measured projections and digitally\nreconstructed radiographs of the CT. Most 3D/2D registration approaches use\nsimple transformation models or require complex mathematical derivations to\nformulate the underlying optimization problem. Instead, our approach entirely\nrelies on differentiable operations which can be combined with modern\ncomputational toolboxes supporting automatic differentiation. This then allows\nfor rapid prototyping, integration with deep neural networks, and to support a\nvariety of transformation models including fluid flow models. We demonstrate\nour approach for the registration between CT and stationary chest tomosynthesis\n(sDCT) images and show how it naturally leads to an iterative image\nreconstruction approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tian_L/0/1/0/all/0/1\">Lin Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puett_C/0/1/0/all/0/1\">Connor Puett</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1\">Peirong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Z/0/1/0/all/0/1\">Zhengyang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aylward_S/0/1/0/all/0/1\">Stephen R. Aylward</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1\">Yueh Z. Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities. (arXiv:2203.04959v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04959","description":"<p>Multiple Sclerosis (MS) is a chronic neuroinflammatory disease and\nmulti-modality MRIs are routinely used to monitor MS lesions. Many automatic MS\nlesion segmentation models have been developed and have reached human-level\nperformance. However, most established methods assume the MRI modalities used\nduring training are also available during testing, which is not guaranteed in\nclinical practice. A training strategy termed Modality Dropout (ModDrop) has\nbeen applied to MS lesion segmentation to achieve the state-of-the-art\nperformance for missing modality. We present a novel method dubbed ModDrop++ to\ntrain a unified network adaptive to an arbitrary number of input MRI sequences.\nMoreover, ModDrop++ can be easily applied to any existing model architectures.\nSpecifically, ModDrop++ upgrades the main idea of ModDrop in two key ways.\nFirst, we devise a plug-and-play dynamic head and adopt a filter scaling\nstrategy to improve the expressiveness of the network. Second, we design a\nco-training strategy to leverage the intra-subject relation between full\nmodality and missing modality. In particular, the intra-subject co-training\nstrategy aims to guide the dynamic head to generate similar feature\nrepresentations between the full- and missing-modality data from the same\nsubject. We use two public MS datasets to show the superiority of ModDrop++.\nSource code and trained models are available at\nhttps://github.com/han-liu/ModDropPlusPlus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Han Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiacheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dewei Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Ho Hin Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-augmented Deep Unfolding Network for Guided Image Super-resolution. (arXiv:2203.04960v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04960","description":"<p>Guided image super-resolution (GISR) aims to obtain a high-resolution (HR)\ntarget image by enhancing the spatial resolution of a low-resolution (LR)\ntarget image under the guidance of a HR image. However, previous model-based\nmethods mainly takes the entire image as a whole, and assume the prior\ndistribution between the HR target image and the HR guidance image, simply\nignoring many non-local common characteristics between them. To alleviate this\nissue, we firstly propose a maximal a posterior (MAP) estimation model for GISR\nwith two types of prior on the HR target image, i.e., local implicit prior and\nglobal implicit prior. The local implicit prior aims to model the complex\nrelationship between the HR target image and the HR guidance image from a local\nperspective, and the global implicit prior considers the non-local\nauto-regression property between the two images from a global perspective.\nSecondly, we design a novel alternating optimization algorithm to solve this\nmodel for GISR. The algorithm is in a concise framework that facilitates to be\nreplicated into commonly used deep network structures. Thirdly, to reduce the\ninformation loss across iterative stages, the persistent memory mechanism is\nintroduced to augment the information representation by exploiting the Long\nshort-term memory unit (LSTM) in the image and feature spaces. In this way, a\ndeep network with certain interpretation and high representation ability is\nbuilt. Extensive experimental results validate the superiority of our method on\na variety of GISR tasks, including Pan-sharpening, depth image\nsuper-resolution, and MR image super-resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Man Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_K/0/1/0/all/0/1\">Keyu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xie_Q/0/1/0/all/0/1\">Qi Xie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1\">Xiangyong Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sharing Generative Models Instead of Private Data: A Simulation Study on Mammography Patch Classification. (arXiv:2203.04961v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04961","description":"<p>Early detection of breast cancer in mammography screening via deep-learning\nbased computer-aided detection systems shows promising potential in improving\nthe curability and mortality rates of breast cancer. However, many clinical\ncentres are restricted in the amount and heterogeneity of available data to\ntrain such models to (i) achieve promising performance and to (ii) generalise\nwell across acquisition protocols and domains. As sharing data between centres\nis restricted due to patient privacy concerns, we propose a potential solution:\nsharing trained generative models between centres as substitute for real\npatient data. In this work, we use three well known mammography datasets to\nsimulate three different centres, where one centre receives the trained\ngenerator of Generative Adversarial Networks (GANs) from the two remaining\ncentres in order to augment the size and heterogeneity of its training dataset.\nWe evaluate the utility of this approach on mammography patch classification on\nthe test set of the GAN-receiving centre using two different classification\nmodels, (a) a convolutional neural network and (b) a transformer neural\nnetwork. Our experiments demonstrate that shared GANs notably increase the\nperformance of both transformer and convolutional classification models and\nhighlight this approach as a viable alternative to inter-centre data sharing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1\">Zuzanna Szafranowska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1\">Richard Osuala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breier_B/0/1/0/all/0/1\">Bennet Breier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1\">Kaisar Kushibar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1\">Karim Lekadir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1\">Oliver Diaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the Degradation Distribution for Blind Image Super-Resolution. (arXiv:2203.04962v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04962","description":"<p>Synthetic high-resolution (HR) \\&amp; low-resolution (LR) pairs are widely used\nin existing super-resolution (SR) methods. To avoid the domain gap between\nsynthetic and test images, most previous methods try to adaptively learn the\nsynthesizing (degrading) process via a deterministic model. However, some\ndegradations in real scenarios are stochastic and cannot be determined by the\ncontent of the image. These deterministic models may fail to model the random\nfactors and content-independent parts of degradations, which will limit the\nperformance of the following SR models. In this paper, we propose a\nprobabilistic degradation model (PDM), which studies the degradation\n$\\mathbf{D}$ as a random variable, and learns its distribution by modeling the\nmapping from a priori random variable $\\mathbf{z}$ to $\\mathbf{D}$. Compared\nwith previous deterministic degradation models, PDM could model more diverse\ndegradations and generate HR-LR pairs that may better cover the various\ndegradations of test images, and thus prevent the SR model from over-fitting to\nspecific ones. Extensive experiments have demonstrated that our degradation\nmodel can help the SR model achieve better performance on different datasets.\nThe source codes are released at \\url{git@github.com:greatlog/UnpairedSR.git}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_Z/0/1/0/all/0/1\">Zhengxiong Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yan Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Shang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_T/0/1/0/all/0/1\">Tieniu Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Data-Dependent Transform for Learned Image Compression. (arXiv:2203.04963v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04963","description":"<p>Learned image compression has achieved great success due to its excellent\nmodeling capacity, but seldom further considers the Rate-Distortion\nOptimization (RDO) of each input image. To explore this potential in the\nlearned codec, we make the first attempt to build a neural data-dependent\ntransform and introduce a continuous online mode decision mechanism to jointly\noptimize the coding efficiency for each individual image. Specifically, apart\nfrom the image content stream, we employ an additional model stream to generate\nthe transform parameters at the decoder side. The presence of a model stream\nenables our model to learn more abstract neural-syntax, which helps cluster the\nlatent representations of images more compactly. Beyond the transform stage, we\nalso adopt neural-syntax based post-processing for the scenarios that require\nhigher quality reconstructions regardless of extra decoding overhead. Moreover,\nthe involvement of the model stream further makes it possible to optimize both\nthe representation and the decoder in an online way, i.e. RDO at the testing\ntime. It is equivalent to a continuous online mode decision, like coding modes\nin the traditional codecs, to improve the coding efficiency based on the\nindividual input image. The experimental results show the effectiveness of the\nproposed neural-syntax design and the continuous online mode decision\nmechanism, demonstrating the superiority of our method in coding efficiency\ncompared to the latest conventional standard Versatile Video Coding (VVC) and\nother state-of-the-art learning-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dezhao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenhan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yueyu Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiaying Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metastatic Cancer Outcome Prediction with Injective Multiple Instance Pooling. (arXiv:2203.04964v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04964","description":"<p>Cancer stage is a large determinant of patient prognosis and management in\nmany cancer types, and is often assessed using medical imaging modalities, such\nas CT and MRI. These medical images contain rich information that can be\nexplored to stratify patients within each stage group to further improve\nprognostic algorithms. Although the majority of cancer deaths result from\nmetastatic and multifocal disease, building imaging biomarkers for patients\nwith multiple tumors has been a challenging task due to the lack of annotated\ndatasets and standard study framework. In this paper, we process two public\ndatasets to set up a benchmark cohort of 341 patient in total for studying\noutcome prediction of multifocal metastatic cancer. We identify the lack of\nexpressiveness in common multiple instance classification networks and propose\ntwo injective multiple instance pooling functions that are better suited to\noutcome prediction. Our results show that multiple instance learning with\ninjective pooling functions can achieve state-of-the-art performance in the\nnon-small-cell lung cancer CT and head and neck CT outcome prediction\nbenchmarking tasks. We will release the processed multifocal datasets, our code\nand the intermediate files i.e. extracted radiomic features to support further\ntransparent and reproducible research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1\">Jianan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Martel_A/0/1/0/all/0/1\">Anne L. Martel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNeXt: MLP-based Rapid Medical Image Segmentation Network. (arXiv:2203.04967v1 [eess.IV])","link":"http://arxiv.org/abs/2203.04967","description":"<p>UNet and its latest extensions like TransUNet have been the leading medical\nimage segmentation methods in recent years. However, these networks cannot be\neffectively adopted for rapid image segmentation in point-of-care applications\nas they are parameter-heavy, computationally complex and slow to use. To this\nend, we propose UNeXt which is a Convolutional multilayer perceptron (MLP)\nbased network for image segmentation. We design UNeXt in an effective way with\nan early convolutional stage and a MLP stage in the latent stage. We propose a\ntokenized MLP block where we efficiently tokenize and project the convolutional\nfeatures and use MLPs to model the representation. To further boost the\nperformance, we propose shifting the channels of the inputs while feeding in to\nMLPs so as to focus on learning local dependencies. Using tokenized MLPs in\nlatent space reduces the number of parameters and computational complexity\nwhile being able to result in a better representation to help segmentation. The\nnetwork also consists of skip connections between various levels of encoder and\ndecoder. We test UNeXt on multiple medical image segmentation datasets and show\nthat we reduce the number of parameters by 72x, decrease the computational\ncomplexity by 68x, and improve the inference speed by 10x while also obtaining\nbetter segmentation performance over the state-of-the-art medical image\nsegmentation architectures. Code is available at\nhttps://github.com/jeya-maria-jose/UNeXt-pytorch\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valanarasu_J/0/1/0/all/0/1\">Jeya Maria Jose Valanarasu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Resource-Efficient Invariant Networks: Exponential Gains by Unrolled Optimization. (arXiv:2203.05006v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05006","description":"<p>Achieving invariance to nuisance transformations is a fundamental challenge\nin the construction of robust and reliable vision systems. Existing approaches\nto invariance scale exponentially with the dimension of the family of\ntransformations, making them unable to cope with natural variabilities in\nvisual data such as changes in pose and perspective. We identify a common\nlimitation of these approaches--they rely on sampling to traverse the\nhigh-dimensional space of transformations--and propose a new computational\nprimitive for building invariant networks based instead on optimization, which\nin many scenarios provides a provably more efficient method for\nhigh-dimensional exploration than sampling. We provide empirical and\ntheoretical corroboration of the efficiency gains and soundness of our proposed\nmethod, and demonstrate its utility in constructing an efficient invariant\nnetwork for a simple hierarchical object detection task when combined with\nunrolled optimization. Code for our networks and experiments is available at\nhttps://github.com/sdbuch/refine.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchanan_S/0/1/0/all/0/1\">Sam Buchanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jingkai Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1\">Ellie Haber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">John Wright</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Instance Domain Adaptation. (arXiv:2203.05028v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05028","description":"<p>Most existing studies on unsupervised domain adaptation (UDA) assume that\neach domain's training samples come with domain labels (e.g., painting, photo).\nSamples from each domain are assumed to follow the same distribution and the\ndomain labels are exploited to learn domain-invariant features via feature\nalignment. However, such an assumption often does not hold true -- there often\nexist numerous finer-grained domains (e.g., dozens of modern painting styles\nhave been developed, each differing dramatically from those of the classic\nstyles). Therefore, forcing feature distribution alignment across each\nartificially-defined and coarse-grained domain can be ineffective. In this\npaper, we address both single-source and multi-source UDA from a completely\ndifferent perspective, which is to view each instance as a fine domain. Feature\nalignment across domains is thus redundant. Instead, we propose to perform\ndynamic instance domain adaptation (DIDA). Concretely, a dynamic neural network\nwith adaptive convolutional kernels is developed to generate instance-adaptive\nresiduals to adapt domain-agnostic deep features to each individual instance.\nThis enables a shared classifier to be applied to both source and target domain\ndata without relying on any domain annotation. Further, instead of imposing\nintricate feature alignment losses, we adopt a simple semi-supervised learning\nparadigm using only a cross-entropy loss for both labeled source and pseudo\nlabeled target data. Our model, dubbed DIDA-Net, achieves state-of-the-art\nperformance on several commonly used single-source and multi-source UDA\ndatasets including Digits, Office-Home, DomainNet, Digit-Five, and PACS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhongying Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Da Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Trajectory Prediction via Transferable GNN. (arXiv:2203.05046v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05046","description":"<p>Pedestrian trajectory prediction is an essential component in a wide range of\nAI applications such as autonomous driving and robotics. Existing methods\nusually assume the training and testing motions follow the same pattern while\nignoring the potential distribution differences (e.g., shopping mall and\nstreet). This issue results in inevitable performance decrease. To address this\nissue, we propose a novel Transferable Graph Neural Network (T-GNN) framework,\nwhich jointly conducts trajectory prediction as well as domain alignment in a\nunified framework. Specifically, a domain invariant GNN is proposed to explore\nthe structural motion knowledge where the domain specific knowledge is reduced.\nMoreover, an attention-based adaptive knowledge learning module is further\nproposed to explore fine-grained individual-level feature representation for\nknowledge transfer. By this way, disparities across different trajectory\ndomains will be better alleviated. More challenging while practical trajectory\nprediction experiments are designed, and the experimental results verify the\nsuperior performance of our proposed model. To the best of our knowledge, our\nwork is the pioneer which fills the gap in benchmarks and techniques for\npractical pedestrian trajectory prediction across different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lichen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Proposed Fairness Models for Face Recognition Algorithms. (arXiv:2203.05051v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05051","description":"<p>The development of face recognition algorithms by academic and commercial\norganizations is growing rapidly due to the onset of deep learning and the\nwidespread availability of training data. Though tests of face recognition\nalgorithm performance indicate yearly performance gains, error rates for many\nof these systems differ based on the demographic composition of the test set.\nThese \"demographic differentials\" in algorithm performance can contribute to\nunequal or unfair outcomes for certain groups of people, raising concerns with\nincreased worldwide adoption of face recognition systems. Consequently,\nregulatory bodies in both the United States and Europe have proposed new rules\nrequiring audits of biometric systems for \"discriminatory impacts\" (European\nUnion Artificial Intelligence Act) and \"fairness\" (U.S. Federal Trade\nCommission). However, no standard for measuring fairness in biometric systems\nyet exists. This paper characterizes two proposed measures of face recognition\nalgorithm fairness (fairness measures) from scientists in the U.S. and Europe.\nWe find that both proposed methods are challenging to interpret when applied to\ndisaggregated face recognition error rates as they are commonly experienced in\npractice. To address this, we propose a set of interpretability criteria,\ntermed the Functional Fairness Measure Criteria (FFMC), that outlines a set of\nproperties desirable in a face recognition algorithm fairness measure. We\nfurther develop a new fairness measure, the Gini Aggregation Rate for Biometric\nEquitability (GARBE), and show how, in conjunction with the Pareto\noptimization, this measure can be used to select among alternative algorithms\nbased on the accuracy/fairness trade-space. Finally, we have open-sourced our\ndataset of machine-readable, demographically disaggregated error rates. We\nbelieve this is currently the largest open-source dataset of its kind.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Howard_J/0/1/0/all/0/1\">John J. Howard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_E/0/1/0/all/0/1\">Eli J. Laird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sirotin_Y/0/1/0/all/0/1\">Yevgeniy B. Sirotin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_R/0/1/0/all/0/1\">Rebecca E. Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tipton_J/0/1/0/all/0/1\">Jerry L. Tipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemury_A/0/1/0/all/0/1\">Arun R. Vemury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optical Flow Training under Limited Label Budget via Active Learning. (arXiv:2203.05053v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05053","description":"<p>Supervised training of optical flow predictors generally yields better\naccuracy than unsupervised training. However, the improved performance comes at\nan often high annotation cost. Semi-supervised training trades off accuracy\nagainst annotation cost. We use a simple yet effective semi-supervised training\nmethod to show that even a small fraction of labels can improve flow accuracy\nby a significant margin over unsupervised training. In addition, we propose\nactive learning methods based on simple heuristics to further reduce the number\nof labels required to achieve the same target accuracy. Our experiments on both\nsynthetic and real optical flow datasets show that our semi-supervised networks\ngenerally need around 50% of the labels to achieve close to full-label\naccuracy, and only around 20% with active learning on Sintel. We also analyze\nand show insights on the factors that may influence our active learning\nperformance. Code will be made available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Shuai Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hannah Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shuzhi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomasi_C/0/1/0/all/0/1\">Carlo Tomasi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynWoodScape: Synthetic Surround-view Fisheye Camera Dataset for Autonomous Driving. (arXiv:2203.05056v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05056","description":"<p>Surround-view cameras are a primary sensor for automated driving, used for\nnear field perception. It is one of the most commonly used sensors in\ncommercial vehicles. Four fisheye cameras with a 190{\\deg} field of view cover\nthe 360{\\deg} around the vehicle. Due to its high radial distortion, the\nstandard algorithms do not extend easily. Previously, we released the first\npublic fisheye surround-view dataset named WoodScape. In this work, we release\na synthetic version of the surround-view dataset, covering many of its\nweaknesses and extending it. Firstly, it is not possible to obtain ground truth\nfor pixel-wise optical flow and depth. Secondly, WoodScape did not have all\nfour cameras simultaneously in order to sample diverse frames. However, this\nmeans that multi-camera algorithms cannot be designed, which is enabled in the\nnew dataset. We implemented surround-view fisheye geometric projections in\nCARLA Simulator matching WoodScape's configuration and created SynWoodScape. We\nrelease 80k images from the synthetic dataset with annotations for 10+ tasks.\nWe also release the baseline code and supporting scripts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sekkat_A/0/1/0/all/0/1\">Ahmed Rida Sekkat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupuis_Y/0/1/0/all/0/1\">Yohan Dupuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashed_H/0/1/0/all/0/1\">Hazem Rashed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasseur_P/0/1/0/all/0/1\">Pascal Vasseur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honeine_P/0/1/0/all/0/1\">Paul Honeine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v1 [cs.LG])","link":"http://arxiv.org/abs/2203.05074","description":"<p>Paradoxically, a Variational Autoencoder (VAE) could be pushed in two\nopposite directions, utilizing powerful decoder model for generating realistic\nimages but collapsing the learned representation, or increasing regularization\ncoefficient for disentangling representation but ultimately generating blurry\nexamples. Existing methods narrow the issues to the rate-distortion trade-off\nbetween compression and reconstruction. We argue that a good reconstruction\nmodel does learn high capacity latents that encode more details, however, its\nuse is hindered by two major issues: the prior is random noise which is\ncompletely detached from the posterior and allow no controllability in the\ngeneration; mean-field variational inference doesn't enforce hierarchy\nstructure which makes the task of recombining those units into plausible novel\noutput infeasible. As a result, we develop a system that learns a hierarchy of\ndisentangled representation together with a mechanism for recombining the\nlearned representation for generalization. This is achieved by introducing a\nminimal amount of inductive bias to learn controllable prior for the VAE. The\nidea is supported by here developed transitive information theory, that is, the\nmutual information between two target variables could alternately be maximized\nthrough the mutual information to the third variable, thus bypassing the\nrate-distortion bottleneck in VAE design. In particular, we show that our\nmodel, named SemafoVAE (inspired by the similar concept in computer science),\ncould generate high-quality examples in a controllable manner, perform smooth\ntraversals of the disentangled factors and intervention at a different level of\nrepresentation hierarchy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Trung Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinaniemi_M/0/1/0/all/0/1\">Merja Hein&#xe4;niemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLX-GPT: A Model for Natural Language Explanations in Vision and Vision-Language Tasks. (arXiv:2203.05081v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05081","description":"<p>Natural language explanation (NLE) models aim at explaining the\ndecision-making process of a black box system via generating natural language\nsentences which are human-friendly, high-level and fine-grained. Current NLE\nmodels explain the decision-making process of a vision or vision-language model\n(a.k.a., task model), e.g., a VQA model, via a language model (a.k.a.,\nexplanation model), e.g., GPT. Other than the additional memory resources and\ninference time required by the task model, the task and explanation models are\ncompletely independent, which disassociates the explanation from the reasoning\nprocess made to predict the answer. We introduce NLX-GPT, a general, compact\nand faithful language model that can simultaneously predict an answer and\nexplain it. We first conduct pre-training on large scale data of image-caption\npairs for general understanding of images, and then formulate the answer as a\ntext prediction task along with the explanation. Without region proposals nor a\ntask model, our resulting overall framework attains better evaluation scores,\ncontains much less parameters and is 15$\\times$ faster than the current SoA\nmodel. We then address the problem of evaluating the explanations which can be\nin many times generic, data-biased and can come in several forms. We therefore\ndesign 2 new evaluation measures: (1) explain-predict and (2) retrieval-based\nattack, a self-evaluation framework that requires no labels. Code is at:\nhttps://github.com/fawazsammani/nlxgpt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sammani_F/0/1/0/all/0/1\">Fawaz Sammani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_T/0/1/0/all/0/1\">Tanmoy Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deligiannis_N/0/1/0/all/0/1\">Nikos Deligiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tree-Structured Multi-Task Model Recommender. (arXiv:2203.05092v1 [cs.LG])","link":"http://arxiv.org/abs/2203.05092","description":"<p>Tree-structured multi-task architectures have been employed to jointly tackle\nmultiple vision tasks in the context of multi-task learning (MTL). The major\nchallenge is to determine where to branch out for each task given a backbone\nmodel to optimize for both task accuracy and computation efficiency. To address\nthe challenge, this paper proposes a recommender that, given a set of tasks and\na convolutional neural network-based backbone model, automatically suggests\ntree-structured multi-task architectures that could achieve a high task\nperformance while meeting a user-specified computation budget without\nperforming model training. Extensive evaluations on popular MTL benchmarks show\nthat the recommended architectures could achieve competitive task accuracy and\ncomputation efficiency compared with state-of-the-art MTL methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Hui Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Neural ODEs via Knowledge Distillation. (arXiv:2203.05103v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05103","description":"<p>Neural Ordinary Differential Equations (Neural ODEs) construct the continuous\ndynamics of hidden units using ordinary differential equations specified by a\nneural network, demonstrating promising results on many tasks. However, Neural\nODEs still do not perform well on image recognition tasks. The possible reason\nis that the one-hot encoding vector commonly used in Neural ODEs can not\nprovide enough supervised information. We propose a new training based on\nknowledge distillation to construct more powerful and robust Neural ODEs\nfitting image recognition tasks. Specially, we model the training of Neural\nODEs into a teacher-student learning process, in which we propose ResNets as\nthe teacher model to provide richer supervised information. The experimental\nresults show that the new training manner can improve the classification\naccuracy of Neural ODEs by 24% on CIFAR10 and 5% on SVHN. In addition, we also\nquantitatively discuss the effect of both knowledge distillation and time\nhorizon in Neural ODEs on robustness against adversarial examples. The\nexperimental analysis concludes that introducing the knowledge distillation and\nincreasing the time horizon can improve the robustness of Neural ODEs against\nadversarial examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Haoyu Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shikui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qiming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenTAL: Towards Open Set Temporal Action Localization. (arXiv:2203.05114v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05114","description":"<p>Temporal Action Localization (TAL) has experienced remarkable success under\nthe supervised learning paradigm. However, existing TAL methods are rooted in\nthe closed set assumption, which cannot handle the inevitable unknown actions\nin open-world scenarios. In this paper, we, for the first time, step toward the\nOpen Set TAL (OSTAL) problem and propose a general framework OpenTAL based on\nEvidential Deep Learning (EDL). Specifically, the OpenTAL consists of\nuncertainty-aware action classification, actionness prediction, and temporal\nlocation regression. With the proposed importance-balanced EDL method,\nclassification uncertainty is learned by collecting categorical evidence\nmajorly from important samples. To distinguish the unknown actions from\nbackground video frames, the actionness is learned by the positive-unlabeled\nlearning. The classification uncertainty is further calibrated by leveraging\nthe guidance from the temporal localization quality. The OpenTAL is general to\nenable existing TAL models for open set scenarios, and experimental results on\nTHUMOS14 and ActivityNet1.3 benchmarks show the effectiveness of our method.\nThe code and pre-trained models are released at\nhttps://www.rit.edu/actionlab/opental.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1\">Wentao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervision semantic segmentation with uncertainty-guided self cross supervision. (arXiv:2203.05118v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05118","description":"<p>As a powerful way of realizing semi-supervised segmentation, the cross\nsupervision method learns cross consistency based on independent ensemble\nmodels using abundant unlabeled images. However, the wrong pseudo labeling\ninformation generated by cross supervision would confuse the training process\nand negatively affect the effectiveness of the segmentation model. Besides, the\ntraining process of ensemble models in such methods also multiplies the cost of\ncomputation resources and decreases the training efficiency. To solve these\nproblems, we propose a novel cross supervision method, namely\nuncertainty-guided self cross supervision (USCS). In addition to ensemble\nmodels, we first design a multi-input multi-output (MIMO) segmentation model\nwhich can generate multiple outputs with shared model and consequently impose\nconsistency over the outputs, saving the cost on parameters and calculations.\nOn the other hand, we employ uncertainty as guided information to encourage the\nmodel to focus on the high confident regions of pseudo labels and mitigate the\neffects of wrong pseudo labeling in self cross supervision, improving the\nperformance of the segmentation model. Extensive experiments show that our\nmethod achieves state-of-the-art performance while saving 40.5% and 49.1% cost\non parameters and calculations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zhiqiang Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xiaohu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaoyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1\">Wen Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MetAug: Contrastive Learning via Meta Feature Augmentation. (arXiv:2203.05119v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05119","description":"<p>What matters for contrastive learning? We argue that contrastive learning\nheavily relies on informative features, or \"hard\" (positive or negative)\nfeatures. Early works include more informative features by applying complex\ndata augmentations and large batch size or memory bank, and recent works design\nelaborate sampling approaches to explore informative features. The key\nchallenge toward exploring such features is that the source multi-view data is\ngenerated by applying random data augmentations, making it infeasible to always\nadd useful information in the augmented data. Consequently, the informativeness\nof features learned from such augmented data is limited. In response, we\npropose to directly augment the features in latent space, thereby learning\ndiscriminative representations without a large amount of input data. We perform\na meta learning technique to build the augmentation generator that updates its\nnetwork parameters by considering the performance of the encoder. However,\ninsufficient input data may lead the encoder to learn collapsed features and\ntherefore malfunction the augmentation generator. A new margin-injected\nregularization is further added in the objective function to avoid the encoder\nlearning a degenerate mapping. To contrast all features in one gradient\nback-propagation step, we adopt the proposed optimization-driven unified\ncontrastive loss instead of the conventional contrastive loss. Empirically, our\nmethod achieves state-of-the-art results on several benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangmeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1\">Wenwen Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1\">Bing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Hui Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEER: Detection-agnostic End-to-End Recognizer for Scene Text Spotting. (arXiv:2203.05122v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05122","description":"<p>Recent end-to-end scene text spotters have achieved great improvement in\nrecognizing arbitrary-shaped text instances. Common approaches for text\nspotting use region of interest pooling or segmentation masks to restrict\nfeatures to single text instances. However, this makes it hard for the\nrecognizer to decode correct sequences when the detection is not accurate i.e.\none or more characters are cropped out. Considering that it is hard to\naccurately decide word boundaries with only the detector, we propose a novel\nDetection-agnostic End-to-End Recognizer, DEER, framework. The proposed method\nreduces the tight dependency between detection and recognition modules by\nbridging them with a single reference point for each text instance, instead of\nusing detected regions. The proposed method allows the decoder to recognize the\ntexts that are indicated by the reference point, with features from the whole\nimage. Since only a single point is required to recognize the text, the\nproposed method enables text spotting without an arbitrarily-shaped detector or\nbounding polygon annotations. Experimental results present that the proposed\nmethod achieves competitive results on regular and arbitrarily-shaped text\nspotting benchmarks. Further analysis shows that DEER is robust to the\ndetection errors. The code and dataset will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoonsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Han-Cheol Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kil_T/0/1/0/all/0/1\">Taeho Kil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surh_J/0/1/0/all/0/1\">Jaeheung Surh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bado Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_Y/0/1/0/all/0/1\">Youngmin Baek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Modeling in Quotient Space: Learning An Invariant Mapping with Decodability of Image Patches. (arXiv:2203.05134v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05134","description":"<p>This study proposes a framework for manifold learning of image patches using\nthe concept of equivalence classes: manifold modeling in quotient space (MMQS).\nIn MMQS, we do not consider a set of local patches of the image as it is, but\nrather the set of their canonical patches obtained by introducing the concept\nof equivalence classes and performing manifold learning on their canonical\npatches. Canonical patches represent equivalence classes, and their\nauto-encoder constructs a manifold in the quotient space. Based on this\nframework, we produce a novel manifold-based image model by introducing\nrotation-flip-equivalence relations. In addition, we formulate an image\nreconstruction problem by fitting the proposed image model to a corrupted\nobserved image and derive an algorithm to solve it. Our experiments show that\nthe proposed image model is effective for various self-supervised image\nreconstruction tasks, such as image inpainting, deblurring, super-resolution,\nand denoising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yokota_T/0/1/0/all/0/1\">Tatsuya Yokota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hontani_H/0/1/0/all/0/1\">Hidekata Hontani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Map Learning for Vision and Language Navigation. (arXiv:2203.05137v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05137","description":"<p>We consider the problem of Vision-and-Language Navigation (VLN). The majority\nof current methods for VLN are trained end-to-end using either unstructured\nmemory such as LSTM, or using cross-modal attention over the egocentric\nobservations of the agent. In contrast to other works, our key insight is that\nthe association between language and vision is stronger when it occurs in\nexplicit spatial representations. In this work, we propose a cross-modal map\nlearning model for vision-and-language navigation that first learns to predict\nthe top-down semantics on an egocentric map for both observed and unobserved\nregions, and then predicts a path towards the goal as a set of waypoints. In\nboth cases, the prediction is informed by the language through cross-modal\nattention mechanisms. We experimentally test the basic hypothesis that\nlanguage-driven navigation can be solved given a map, and then show competitive\nresults on the full VLN-CE benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanchoo_K/0/1/0/all/0/1\">Karan Wanchoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dan_S/0/1/0/all/0/1\">Soham Dan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltsakaki_E/0/1/0/all/0/1\">Eleni Miltsakaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intention-aware Feature Propagation Network for Interactive Segmentation. (arXiv:2203.05145v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05145","description":"<p>We aim to tackle the problem of point-based interactive segmentation, in\nwhich two key challenges are to infer user's intention correctly and to\npropagate the user-provided annotations to unlabeled regions efficiently. To\naddress those challenges, we propose a novel intention-aware feature\npropagation strategy that performs explicit user intention estimation and\nlearns an efficient click-augmented feature representation for high-resolution\nforeground segmentation. Specifically, we develop a coarse-to-fine sparse\npropagation network for each interactive segmentation step, which consists of a\ncoarse-level network for more effective tracking of user's interest, and a\nfine-level network for zooming to the target object and performing fine-level\nsegmentation. Moreover, we design a new sparse graph network module for both\nlevels to enable efficient long-range propagation of click information.\nExtensive experiments show that our method surpasses the previous\nstate-of-the-art methods on all popular benchmarks, demonstrating its efficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuanyang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Frequency-driven Imperceptible Adversarial Attack on Semantic Similarity. (arXiv:2203.05151v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05151","description":"<p>Current adversarial attack research reveals the vulnerability of\nlearning-based classifiers against carefully crafted perturbations. However,\nmost existing attack methods have inherent limitations in cross-dataset\ngeneralization as they rely on a classification layer with a closed set of\ncategories. Furthermore, the perturbations generated by these methods may\nappear in regions easily perceptible to the human visual system (HVS). To\ncircumvent the former problem, we propose a novel algorithm that attacks\nsemantic similarity on feature representations. In this way, we are able to\nfool classifiers without limiting attacks to a specific dataset. For\nimperceptibility, we introduce the low-frequency constraint to limit\nperturbations within high-frequency components, ensuring perceptual similarity\nbetween adversarial examples and originals. Extensive experiments on three\ndatasets (CIFAR-10, CIFAR-100, and ImageNet-1K) and three public online\nplatforms indicate that our attack can yield misleading and transferable\nadversarial examples across architectures and datasets. Additionally,\nvisualization results and quantitative performance (in terms of four different\nmetrics) show that the proposed algorithm generates more imperceptible\nperturbations than the state-of-the-art methods. Code is made available at.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Cheng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qinliang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weicheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Bizhu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Linlin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Evaluation of Adversarial Robustness via Adaptive Auto Attack. (arXiv:2203.05154v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05154","description":"<p>Defense models against adversarial attacks have grown significantly, but the\nlack of practical evaluation methods has hindered progress. Evaluation can be\ndefined as looking for defense models' lower bound of robustness given a budget\nnumber of iterations and a test dataset. A practical evaluation method should\nbe convenient (i.e., parameter-free), efficient (i.e., fewer iterations) and\nreliable (i.e., approaching the lower bound of robustness). Towards this\ntarget, we propose a parameter-free Adaptive Auto Attack (A$^3$) evaluation\nmethod which addresses the efficiency and reliability in a test-time-training\nfashion. Specifically, by observing that adversarial examples to a specific\ndefense model follow some regularities in their starting points, we design an\nAdaptive Direction Initialization strategy to speed up the evaluation.\nFurthermore, to approach the lower bound of robustness under the budget number\nof iterations, we propose an online statistics-based discarding strategy that\nautomatically identifies and abandons hard-to-attack images. Extensive\nexperiments demonstrate the effectiveness of our A$^3$. Particularly, we apply\nA$^3$ to nearly 50 widely-used defense models. By consuming much fewer\niterations than existing methods, i.e., $1/10$ on average (10$\\times$ speed\nup), we achieve lower robust accuracy in all cases. Notably, we won\n$\\textbf{first place}$ out of 1681 teams in CVPR 2021 White-box Adversarial\nAttacks on Defense Models competitions with this method. Code is available at:\n$\\href{https://github.com/liuye6666/adaptive_auto_attack}{https://github.com/liuye6666/adaptive\\_auto\\_attack}$\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Ye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianglong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Action Recognition with Transformer-based Video Semantic Embedding. (arXiv:2203.05156v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05156","description":"<p>While video action recognition has been an active area of research for\nseveral years, zero-shot action recognition has only recently started gaining\ntraction. However, there is a lack of a formal definition for the zero-shot\nlearning paradigm leading to uncertainty about classes that can be considered\nas previously unseen. In this work, we take a new comprehensive look at the\ninductive zero-shot action recognition problem from a realistic standpoint.\nSpecifically, we advocate for a concrete formulation for zero-shot action\nrecognition that avoids an exact overlap between the training and testing\nclasses and also limits the intra-class variance; and propose a novel\nend-to-end trained transformer model which is capable of capturing long range\nspatiotemporal dependencies efficiently, contrary to existing approaches which\nuse 3D-CNNs. The proposed approach outperforms the existing state-of-the-art\nalgorithms in many settings on all benchmark datasets by a wide margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doshi_K/0/1/0/all/0/1\">Keval Doshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_Y/0/1/0/all/0/1\">Yasin Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MVP: Multimodality-guided Visual Pre-training. (arXiv:2203.05175v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05175","description":"<p>Recently, masked image modeling (MIM) has become a promising direction for\nvisual pre-training. In the context of vision transformers, MIM learns\neffective visual representation by aligning the token-level features with a\npre-defined space (e.g., BEIT used a d-VAE trained on a large image corpus as\nthe tokenizer). In this paper, we go one step further by introducing guidance\nfrom other modalities and validating that such additional knowledge leads to\nimpressive gains for visual pre-training. The proposed approach is named\nMultimodality-guided Visual Pre-training (MVP), in which we replace the\ntokenizer with the vision branch of CLIP, a vision-language model pre-trained\non 400 million image-text pairs. We demonstrate the effectiveness of MVP by\nperforming standard experiments, i.e., pre-training the ViT models on ImageNet\nand fine-tuning them on a series of downstream visual recognition tasks. In\nparticular, pre-training ViT-Base/16 for 300 epochs, MVP reports a 52.4% mIoU\non ADE20K, surpassing BEIT (the baseline and previous state-of-the-art) with an\nimpressive margin of 6.8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Audio-Visual Attention Based Multimodal Network for Fake Talking Face Videos Detection. (arXiv:2203.05178v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05178","description":"<p>DeepFake based digital facial forgery is threatening the public media\nsecurity, especially when lip manipulation has been used in talking face\ngeneration, the difficulty of fake video detection is further improved. By only\nchanging lip shape to match the given speech, the facial features of identity\nis hard to be discriminated in such fake talking face videos. Together with the\nlack of attention on audio stream as the prior knowledge, the detection failure\nof fake talking face generation also becomes inevitable. Inspired by the\ndecision-making mechanism of human multisensory perception system, which\nenables the auditory information to enhance post-sensory visual evidence for\ninformed decisions output, in this study, a fake talking face detection\nframework FTFDNet is proposed by incorporating audio and visual representation\nto achieve more accurate fake talking face videos detection. Furthermore, an\naudio-visual attention mechanism (AVAM) is proposed to discover more\ninformative features, which can be seamlessly integrated into any audio-visual\nCNN architectures by modularization. With the additional AVAM, the proposed\nFTFDNet is able to achieve a better detection performance on the established\ndataset (FTFDD). The evaluation of the proposed work has shown an excellent\nperformance on the detection of fake talking face videos, which is able to\narrive at a detection rate above 97%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ganglai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1\">Yufei Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Open-Set Text Recognition via Label-to-Prototype Learning. (arXiv:2203.05179v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05179","description":"<p>Scene text recognition is a popular topic and can benefit various tasks.\nAlthough many methods have been proposed for the close-set text recognition\nchallenges, they cannot be directly applied to open-set scenarios, where the\nevaluation set contains novel characters not appearing in the training set.\nConventional methods require collecting new data and retraining the model to\nhandle these novel characters, which is an expensive and tedious process. In\nthis paper, we propose a label-to-prototype learning framework to handle novel\ncharacters without retraining the model. In the proposed framework, novel\ncharacters are effectively mapped to their corresponding prototypes with a\nlabel-to-prototype learning module. This module is trained on characters with\nseen labels and can be easily generalized to novel characters. Additionally,\nfeature-level rectification is conducted via topology-preserving\ntransformation, resulting in better alignments between visual features and\nconstructed prototypes while having a reasonably small impact on model speed. A\nlot of experiments show that our method achieves promising performance on a\nvariety of zero-shot, close-set, and open-set text recognition datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hai-Bo Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">JieBo Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Distillation as Efficient Pre-training: Faster Convergence, Higher Data-efficiency, and Better Transferability. (arXiv:2203.05180v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05180","description":"<p>Large-scale pre-training has been proven to be crucial for various computer\nvision tasks. However, with the increase of pre-training data amount, model\narchitecture amount, and the private/inaccessible data, it is not very\nefficient or possible to pre-train all the model architectures on large-scale\ndatasets. In this work, we investigate an alternative strategy for\npre-training, namely Knowledge Distillation as Efficient Pre-training (KDEP),\naiming to efficiently transfer the learned feature representation from existing\npre-trained models to new student models for future downstream tasks. We\nobserve that existing Knowledge Distillation (KD) methods are unsuitable\ntowards pre-training since they normally distill the logits that are going to\nbe discarded when transferred to downstream tasks. To resolve this problem, we\npropose a feature-based KD method with non-parametric feature dimension\naligning. Notably, our method performs comparably with supervised pre-training\ncounterparts in 3 downstream tasks and 9 downstream datasets requiring 10x less\ndata and 5x less pre-training time. Code is available at\nhttps://github.com/CVMI-Lab/KDEP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruifei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuyang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suspected Object Matters: Rethinking Model's Prediction for One-stage Visual Grounding. (arXiv:2203.05186v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05186","description":"<p>Recently, one-stage visual grounders attract high attention due to the\ncomparable accuracy but significantly higher efficiency than two-stage\ngrounders. However, inter-object relation modeling has not been well studied\nfor one-stage grounders. Inter-object relationship modeling, though important,\nis not necessarily performed among all the objects within the image, as only a\npart of them are related to the text query and may confuse the model. We call\nthese objects \"suspected objects\". However, exploring relationships among these\nsuspected objects in the one-stage visual grounding paradigm is non-trivial due\nto two core problems: (1) no object proposals are available as the basis on\nwhich to select suspected objects and perform relationship modeling; (2)\ncompared with those irrelevant to the text query, suspected objects are more\nconfusing, as they may share similar semantics, be entangled with certain\nrelationships, etc, and thereby more easily mislead the model's prediction. To\naddress the above issues, this paper proposes a Suspected Object Graph (SOG)\napproach to encourage the correct referred object selection among the suspected\nones in the one-stage visual grounding. Suspected objects are dynamically\nselected from a learned activation map as nodes to adapt to the current\ndiscrimination ability of the model during training. Afterward, on top of the\nsuspected objects, a Keyword-aware Node Representation module (KNR) and an\nExploration by Random Connection strategy (ERC) are concurrently proposed\nwithin the SOG to help the model rethink its initial prediction. Extensive\nablation studies and comparison with state-of-the-art approaches on prevalent\nvisual grounding benchmarks demonstrate the effectiveness of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluttered Food Grasping with Adaptive Fingers and Synthetic-Data Trained Object Detection. (arXiv:2203.05187v1 [cs.RO])","link":"http://arxiv.org/abs/2203.05187","description":"<p>The food packaging industry handles an immense variety of food products with\nwide-ranging shapes and sizes, even within one kind of food. Menus are also\ndiverse and change frequently, making automation of pick-and-place difficult. A\npopular approach to bin-picking is to first identify each piece of food in the\ntray by using an instance segmentation method. However, human annotations to\ntrain these methods are unreliable and error-prone since foods are packed close\ntogether with unclear boundaries and visual similarity making separation of\npieces difficult. To address this problem, we propose a method that trains\npurely on synthetic data and successfully transfers to the real world using\nsim2real methods by creating datasets of filled food trays using high-quality\n3d models of real pieces of food for the training instance segmentation models.\nAnother concern is that foods are easily damaged during grasping. We address\nthis by introducing two additional methods -- a novel adaptive finger mechanism\nto passively retract when a collision occurs, and a method to filter grasps\nthat are likely to cause damage to neighbouring pieces of food during a grasp.\nWe demonstrate the effectiveness of the proposed method on several kinds of\nreal foods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ummadisingu_A/0/1/0/all/0/1\">Avinash Ummadisingu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1\">Kuniyuki Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukaya_N/0/1/0/all/0/1\">Naoki Fukaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRFocus: Neural Radiance Field for 3D Synthetic Defocus. (arXiv:2203.05189v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05189","description":"<p>Neural radiance fields (NeRF) bring a new wave for 3D interactive\nexperiences. However, as an important part of the immersive experiences, the\ndefocus effects have not been fully explored within NeRF. Some recent\nNeRF-based methods generate 3D defocus effects in a post-process fashion by\nutilizing multiplane technology. Still, they are either time-consuming or\nmemory-consuming. This paper proposes a novel thin-lens-imaging-based NeRF\nframework that can directly render various 3D defocus effects, dubbed NeRFocus.\nUnlike the pinhole, the thin lens refracts rays of a scene point, so its\nimaging on the sensor plane is scattered as a circle of confusion (CoC). A\ndirect solution sampling enough rays to approximate this process is\ncomputationally expensive. Instead, we propose to inverse the thin lens imaging\nto explicitly model the beam path for each point on the sensor plane and\ngeneralize this paradigm to the beam path of each pixel, then use the\nfrustum-based volume rendering to render each pixel's beam path. We further\ndesign an efficient probabilistic training (p-training) strategy to simplify\nthe training process vastly. Extensive experiments demonstrate that our\nNeRFocus can achieve various 3D defocus effects with adjustable camera pose,\nfocus distance, and aperture size. Existing NeRF can be regarded as our special\ncase by setting aperture size as zero to render large depth-of-field images.\nDespite such merits, NeRFocus does not sacrifice NeRF's original performance\n(e.g., training and inference time, parameter consumption, rendering quality),\nwhich implies its great potential for broader application and further\nimprovement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yinhuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuzhou Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yujie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Background Matting Using Background Matching. (arXiv:2203.05193v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05193","description":"<p>Due to the difficulty of solving the matting problem, lots of methods use\nsome kinds of assistance to acquire high quality alpha matte. Green screen\nmatting methods rely on physical equipment. Trimap-based methods take manual\ninteractions as external input. Background-based methods require a\npre-captured, static background. The methods are not flexible and convenient\nenough to use widely. Trimap-free methods are flexible but not stable in\ncomplicated video applications. To be stable and flexible in real applications,\nwe propose an adaptive background matting method. The user first captures their\nvideos freely, moving the cameras. Then the user captures the background video\nafterwards, roughly covering the previous captured regions. We use dynamic\nbackground video instead of static background for accurate matting. The\nproposed method is convenient to use in any scenes as the static camera and\nbackground is no more the limitation. To achieve this goal, we use background\nmatching network to find the best-matched background frame by frame from\ndynamic backgrounds. Then, robust semantic estimation network is used to\nestimate the coarse alpha matte. Finally, we crop and zoom the target region\naccording to the coarse alpha matte, and estimate the final accurate alpha\nmatte. In experiments, the proposed method is able to perform comparably\nagainst the state-of-the-art matting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinlin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Screen-Shooting Resilient Document Image Watermarking Scheme using Deep Neural Network. (arXiv:2203.05198v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05198","description":"<p>With the advent of the screen-reading era, the confidential documents\ndisplayed on the screen can be easily captured by a camera without leaving any\ntraces. Thus, this paper proposes a novel screen-shooting resilient\nwatermarking scheme for document image using deep neural network. By applying\nthis scheme, when the watermarked image is displayed on the screen and captured\nby a camera, the watermark can be still extracted from the captured\nphotographs. Specifically, our scheme is an end-to-end neural network with an\nencoder to embed watermark and a decoder to extract watermark. During the\ntraining process, a distortion layer between encoder and decoder is added to\nsimulate the distortions introduced by screen-shooting process in real scenes,\nsuch as camera distortion, shooting distortion, light source distortion.\nBesides, an embedding strength adjustment strategy is designed to improve the\nvisual quality of the watermarked image with little loss of extraction\naccuracy. The experimental results show that the scheme has higher robustness\nand visual quality than other three recent state-of-the-arts. Specially, even\nif the shooting distances and angles are in extreme, our scheme can also obtain\nhigh extraction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Sulong Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhihua Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1\">Yao Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_J/0/1/0/all/0/1\">Jian Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral Imaging for cherry tomato. (arXiv:2203.05199v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05199","description":"<p>Cherry tomato (Solanum Lycopersicum) is popular with consumers over the world\ndue to its special flavor. Soluble solids content (SSC) and firmness are two\nkey metrics for evaluating the product qualities. In this work, we develop\nnon-destructive testing techniques for SSC and fruit firmness based on\nhyperspectral images and a corresponding deep learning regression model.\nHyperspectral reflectance images of over 200 tomato fruits are derived with\nspectrum ranging from 400 to 1000 nm. The acquired hyperspectral images are\ncorrected and the spectral information is extracted. A novel\none-dimensional(1D) convolutional ResNet (Con1dResNet) based regression model\nis prosed and compared with the state of art techniques. Experimental results\nshow that, with a relatively large number of samples our technique is 26.4\\%\nbetter than state of art technique for SSC and 33.7\\% for firmness. The results\nof this study indicate the application potential of hyperspectral imaging\ntechnique in the SSC and firmness detection, which provides a new option for\nnon-destructive testing of cherry tomato fruit quality in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yun Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhongjin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zuohui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guozhi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1\">Qi Xuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Deep Metric Learning via Mutual Distillation. (arXiv:2203.05201v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05201","description":"<p>Deep metric learning aims to transform input data into an embedding space,\nwhere similar samples are close while dissimilar samples are far apart from\neach other. In practice, samples of new categories arrive incrementally, which\nrequires the periodical augmentation of the learned model. The fine-tuning on\nthe new categories usually leads to poor performance on the old, which is known\nas \"catastrophic forgetting\". Existing solutions either retrain the model from\nscratch or require the replay of old samples during the training. In this\npaper, a complete online deep metric learning framework is proposed based on\nmutual distillation for both one-task and multi-task scenarios. Different from\nthe teacher-student framework, the proposed approach treats the old and new\nlearning tasks with equal importance. No preference over the old or new\nknowledge is caused. In addition, a novel virtual feature estimation approach\nis proposed to recover the features assumed to be extracted by the old models.\nIt allows the distillation between the new and the old models without the\nreplay of old training samples or the holding of old models during the\ntraining. A comprehensive study shows the superior performance of our approach\nwith the support of different backbones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gao-Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MORE: Multi-Order RElation Mining for Dense Captioning in 3D Scenes. (arXiv:2203.05203v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05203","description":"<p>3D dense captioning is a recently-proposed novel task, where point clouds\ncontain more geometric information than the 2D counterpart. However, it is also\nmore challenging due to the higher complexity and wider variety of inter-object\nrelations. Existing methods only treat such relations as by-products of object\nfeature learning in graphs without specifically encoding them, which leads to\nsub-optimal results. In this paper, aiming at improving 3D dense captioning via\ncapturing and utilizing the complex relations in the 3D scene, we propose MORE,\na Multi-Order RElation mining model, to support generating more descriptive and\ncomprehensive captions. Technically, our MORE encodes object relations in a\nprogressive manner since complex relations can be deduced from a limited number\nof basic ones. We first devise a novel Spatial Layout Graph Convolution (SLGC),\nwhich semantically encodes several first-order relations as edges of a graph\nconstructed over 3D object proposals. Next, from the resulting graph, we\nfurther extract multiple triplets which encapsulate basic first-order relations\nas the basic unit and construct several Object-centric Triplet Attention Graphs\n(OTAG) to infer multi-order relations for every target object. The updated node\nfeatures from OTAG are aggregated and fed into the caption decoder to provide\nabundant relational cues so that captions including diverse relations with\ncontext objects can be generated. Extensive experiments on the Scan2Cap dataset\nprove the effectiveness of our proposed MORE and its components, and we also\noutperform the current state-of-the-art method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yang Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoxiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zequn Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crowd Source Scene Change Detection and Local Map Update. (arXiv:2203.05205v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05205","description":"<p>As scene changes with time map descriptors become outdated, affecting VPS\nlocalization accuracy. In this work, we propose an approach to detect\nstructural and texture scene changes to be followed by map update. In our\nmethod - map includes 3D points with descriptors generated either via LiDAR or\nSFM. Common approaches suffer from shortcomings: 1) Direct comparison of the\ntwo point-clouds for change detection is slow due to the need to build new\npoint-cloud every time we want to compare; 2) Image based comparison requires\nto keep the map images adding substantial storage overhead. To circumvent this\nproblems, we propose an approach based on point-clouds descriptors comparison:\n1) Based on VPS poses select close query and map images pairs, 2) Registration\nof query images to map image descriptors, 3) Use segmentation to filter out\ndynamic or short term temporal changes, 4) Compare the descriptors between\ncorresponding segments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilf_I/0/1/0/all/0/1\">Itzik Wilf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniel_N/0/1/0/all/0/1\">Nati Daniel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manqing_L/0/1/0/all/0/1\">Lin Manqing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shama_F/0/1/0/all/0/1\">Firas Shama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asraf_O/0/1/0/all/0/1\">Omri Asraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wensen_F/0/1/0/all/0/1\">Feng Wensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruzel_O/0/1/0/all/0/1\">Ofer Kruzel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReF -- Rotation Equivariant Features for Local Feature Matching. (arXiv:2203.05206v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05206","description":"<p>Sparse local feature matching is pivotal for many computer vision and\nrobotics tasks. To improve their invariance to challenging appearance\nconditions and viewing angles, and hence their usefulness, existing\nlearning-based methods have primarily focused on data augmentation-based\ntraining. In this work, we propose an alternative, complementary approach that\ncenters on inducing bias in the model architecture itself to generate\n`rotation-specific' features using Steerable E2-CNNs, that are then\ngroup-pooled to achieve rotation-invariant local features. We demonstrate that\nthis high performance, rotation-specific coverage from the steerable CNNs can\nbe expanded to all rotation angles by combining it with augmentation-trained\nstandard CNNs which have broader coverage but are often inaccurate, thus\ncreating a state-of-the-art rotation-robust local feature matcher. We benchmark\nour proposed methods against existing techniques on HPatches and a newly\nproposed UrbanScenes3D-Air dataset for visual place recognition. Furthermore,\nwe present a detailed analysis of the performance effects of ensembling, robust\nestimation, network architecture variations, and the use of rotation priors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peri_A/0/1/0/all/0/1\">Abhishek Peri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_K/0/1/0/all/0/1\">Kinal Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1\">Avneesh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1\">Michael Milford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sourav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K. Madhava Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferring Dual Stochastic Graph Convolutional Network for Facial Micro-expression Recognition. (arXiv:2203.05208v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05208","description":"<p>Micro-expression recognition has drawn increasing attention due to its wide\napplication in lie detection, criminal detection and psychological\nconsultation. To improve the recognition performance of the small\nmicro-expression data, this paper presents a transferring dual stochastic Graph\nConvolutional Network (TDSGCN) model. We propose a stochastic graph\nconstruction method and dual graph convolutional network to extract more\ndiscriminative features from the micro-expression images. We use transfer\nlearning to pre-train SGCNs from macro expression data. Optical flow algorithm\nis also integrated to extract their temporal features. We fuse both spatial and\ntemporal features to improve the recognition performance. To the best of our\nknowledge, this is the first attempt to utilize the transferring learning and\ngraph convolutional network in micro-expression recognition task. In addition,\nto handle the class imbalance problem of dataset, we focus on the design of\nfocal loss function. Through extensive evaluation, our proposed method achieves\nstate-of-the-art performance on SAMM and recently released MMEW benchmarks. Our\ncode will be publicly available accompanying this paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Li Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wanli Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Membership Privacy Protection for Image Translation Models via Adversarial Knowledge Distillation. (arXiv:2203.05212v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05212","description":"<p>Image-to-image translation models are shown to be vulnerable to the\nMembership Inference Attack (MIA), in which the adversary's goal is to identify\nwhether a sample is used to train the model or not. With daily increasing\napplications based on image-to-image translation models, it is crucial to\nprotect the privacy of these models against MIAs.\n</p>\n<p>We propose adversarial knowledge distillation (AKD) as a defense method\nagainst MIAs for image-to-image translation models. The proposed method\nprotects the privacy of the training samples by improving the generalizability\nof the model. We conduct experiments on the image-to-image translation models\nand show that AKD achieves the state-of-the-art utility-privacy tradeoff by\nreducing the attack performance up to 38.9% compared with the regular training\nmodel at the cost of a slight drop in the quality of the generated output\nimages. The experimental results also indicate that the models trained by AKD\ngeneralize better than the regular training models. Furthermore, compared with\nexisting defense methods, the results show that at the same privacy protection\nlevel, image translation models trained by AKD generate outputs with higher\nquality; while at the same quality of outputs, AKD enhances the privacy\nprotection over 30%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alvar_S/0/1/0/all/0/1\">Saeed Ranjbar Alvar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jian Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement. (arXiv:2203.05238v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05238","description":"<p>In this paper, we propose a weakly-supervised approach for 3D object\ndetection, which makes it possible to train strong 3D detector with\nposition-level annotations (i.e. annotations of object centers). In order to\nremedy the information loss from box annotations to centers, our method, namely\nBack to Reality (BR), makes use of synthetic 3D shapes to convert the weak\nlabels into fully-annotated virtual scenes as stronger supervision, and in turn\nutilizes the perfect virtual labels to complement and refine the real labels.\nSpecifically, we first assemble 3D shapes into physically reasonable virtual\nscenes according to the coarse scene layout extracted from position-level\nannotations. Then we go back to reality by applying a virtual-to-real domain\nadaptation method, which refine the weak labels and additionally supervise the\ntraining of detector with the virtual scenes. Furthermore, we propose a more\nchallenging benckmark for indoor 3D object detection with more diversity in\nobject sizes to better show the potential of BR. With less than 5% of the\nlabeling labor, we achieve comparable detection performance with some popular\nfully-supervised approaches on the widely used ScanNet dataset. Code is\navailable at: https://github.com/xuxw98/BackToReality\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiuwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Debiased Temporal Sentence Grounding in Videos: Dataset, Metric, and Approach. (arXiv:2203.05243v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05243","description":"<p>Temporal Sentence Grounding in Videos (TSGV), which aims to ground a natural\nlanguage sentence in an untrimmed video, has drawn widespread attention over\nthe past few years. However, recent studies have found that current benchmark\ndatasets may have obvious moment annotation biases, enabling several simple\nbaselines even without training to achieve SOTA performance. In this paper, we\ntake a closer look at existing evaluation protocols, and find both the\nprevailing dataset and evaluation metrics are the devils that lead to\nuntrustworthy benchmarking. Therefore, we propose to re-organize the two\nwidely-used datasets, making the ground-truth moment distributions different in\nthe training and test splits, i.e., out-of-distribution (OOD) test. Meanwhile,\nwe introduce a new evaluation metric \"dR@n,IoU@m\" that discounts the basic\nrecall scores to alleviate the inflating evaluation caused by biased datasets.\nNew benchmarking results indicate that our proposed evaluation protocols can\nbetter monitor the research progress. Furthermore, we propose a novel\ncausality-based Multi-branch Deconfounding Debiasing (MDD) framework for\nunbiased moment prediction. Specifically, we design a multi-branch deconfounder\nto eliminate the effects caused by multiple confounders with causal\nintervention. In order to help the model better align the semantics between\nsentence queries and video moments, we enhance the representations during\nfeature encoding. Specifically, for textual information, the query is parsed\ninto several verb-centered phrases to obtain a more fine-grained textual\nfeature. For visual information, the positional information has been decomposed\nfrom moment features to enhance representations of moments with diverse\nlocations. Extensive experiments demonstrate that our proposed approach can\nachieve competitive results among existing SOTA approaches and outperform the\nbase model with great gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_X/0/1/0/all/0/1\">Xiaohan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yitian Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Scene Text Detection Based on Global Level and Word Level Features. (arXiv:2203.05251v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05251","description":"<p>It is an extremely challenging task to detect arbitrary shape text in natural\nscenes on high accuracy and efficiency. In this paper, we propose a scene text\ndetection framework, namely GWNet, which mainly includes two modules: Global\nmodule and RCNN module. Specifically, Global module improves the adaptive\nperformance of the DB (Differentiable Binarization) module by adding k\nsubmodule and shift submodule. Two submodules enhance the adaptability of\namplifying factor k, accelerate the convergence of models and help to produce\nmore accurate detection results. RCNN module fuses global-level and word-level\nfeatures. The word-level label is generated by obtaining the minimum\naxis-aligned rectangle boxes of the shrunk polygon. In the inference period,\nGWNet only uses global-level features to output simple polygon detections.\nExperiments on four benchmark datasets, including the MSRA-TD500, Total-Text,\nICDAR2015 and CTW-1500, demonstrate that our GWNet outperforms the\nstate-of-the-art detectors. Specifically, with a backbone of ResNet-50, we\nachieve an F-measure of 88.6% on MSRA- TD500, 87.9% on Total-Text, 89.2% on\nICDAR2015 and 87.5% on CTW-1500.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jionghua Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Enjun Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xue Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Boundary Learning for Point Cloud Segmentation. (arXiv:2203.05272v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05272","description":"<p>Point cloud segmentation is fundamental in understanding 3D environments.\nHowever, current 3D point cloud segmentation methods usually perform poorly on\nscene boundaries, which degenerates the overall segmentation performance. In\nthis paper, we focus on the segmentation of scene boundaries. Accordingly, we\nfirst explore metrics to evaluate the segmentation performance on scene\nboundaries. To address the unsatisfactory performance on boundaries, we then\npropose a novel contrastive boundary learning (CBL) framework for point cloud\nsegmentation. Specifically, the proposed CBL enhances feature discrimination\nbetween points across boundaries by contrasting their representations with the\nassistance of scene contexts at multiple scales. By applying CBL on three\ndifferent baseline methods, we experimentally show that CBL consistently\nimproves different baselines and assists them to achieve compelling performance\non boundaries, as well as the overall performance, eg in mIoU. The experimental\nresults demonstrate the effectiveness of our method and the importance of\nboundaries for 3D point cloud segmentation. Code and model will be made\npublicly available at https://github.com/LiyaoTang/contrastBoundary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalisation for Object Detection. (arXiv:2203.05294v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05294","description":"<p>Domain generalisation aims to promote the learning of domain-invariant\nfeatures while suppressing domain specific features, so that a model can\ngeneralise well on previously unseen target domains. This paper studies domain\ngeneralisation in the object detection setting. We propose new terms for\nhandling both the bounding box detector and domain belonging, and incorporate\nthem with consistency regularisation. This allows us to learn a domain agnostic\nfeature representation for object detection, applicable to the problem of\ndomain generalisation. The proposed approach is evaluated using four standard\nobject detection datasets with available domain metadata, namely GWHD,\nCityscapes, BDD100K, Sim10K and exhibits consistently superior generalisation\nperformance over baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seemakurthy_K/0/1/0/all/0/1\">Karthik Seemakurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_C/0/1/0/all/0/1\">Charles Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aptoula_E/0/1/0/all/0/1\">Erchan Aptoula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosilj_P/0/1/0/all/0/1\">Petra Bosilj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GrainSpace: A Large-scale Dataset for Fine-grained and Domain-adaptive Recognition of Cereal Grains. (arXiv:2203.05306v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05306","description":"<p>Cereal grains are a vital part of human diets and are important commodities\nfor people's livelihood and international trade. Grain Appearance Inspection\n(GAI) serves as one of the crucial steps for the determination of grain quality\nand grain stratification for proper circulation, storage and food processing,\netc. GAI is routinely performed manually by qualified inspectors with the aid\nof some hand tools. Automated GAI has the benefit of greatly assisting\ninspectors with their jobs but has been limited due to the lack of datasets and\nclear definitions of the tasks.\n</p>\n<p>In this paper we formulate GAI as three ubiquitous computer vision tasks:\nfine-grained recognition, domain adaptation and out-of-distribution\nrecognition. We present a large-scale and publicly available cereal grains\ndataset called GrainSpace. Specifically, we construct three types of device\nprototypes for data acquisition, and a total of 5.25 million images determined\nby professional inspectors. The grain samples including wheat, maize and rice\nare collected from five countries and more than 30 regions. We also develop a\ncomprehensive benchmark based on semi-supervised learning and self-supervised\nlearning techniques. To the best of our knowledge, GrainSpace is the first\npublicly released dataset for cereal grain inspection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dongdong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_D/0/1/0/all/0/1\">Donglin Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pagnucco_M/0/1/0/all/0/1\">Maurice Pagnucco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleBabel: Artistic Style Tagging and Captioning. (arXiv:2203.05321v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05321","description":"<p>We present StyleBabel, a unique open access dataset of natural language\ncaptions and free-form tags describing the artistic style of over 135K digital\nartworks, collected via a novel participatory method from experts studying at\nspecialist art and design schools. StyleBabel was collected via an iterative\nmethod, inspired by `Grounded Theory': a qualitative approach that enables\nannotation while co-evolving a shared language for fine-grained artistic style\nattribute description. We demonstrate several downstream tasks for StyleBabel,\nadapting the recent ALADIN architecture for fine-grained style similarity, to\ntrain cross-modal embeddings for: 1) free-form tag generation; 2) natural\nlanguage description of artistic style; 3) fine-grained text search of style.\nTo do so, we extend ALADIN with recent advances in Visual Transformer (ViT) and\ncross-modal representation learning, achieving a state of the art accuracy in\nfine-grained style retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruta_D/0/1/0/all/0/1\">Dan Ruta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_P/0/1/0/all/0/1\">Pranav Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marri_N/0/1/0/all/0/1\">Naveen Marri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kale_A/0/1/0/all/0/1\">Ajinkya Kale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briggs_J/0/1/0/all/0/1\">Jo Briggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Speed_C/0/1/0/all/0/1\">Chris Speed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faieta_B/0/1/0/all/0/1\">Baldo Faieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipkowski_A/0/1/0/all/0/1\">Alex Filipkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backbone is All Your Need: A Simplified Architecture for Visual Object Tracking. (arXiv:2203.05328v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05328","description":"<p>Exploiting a general-purpose neural architecture to replace hand-wired\ndesigns or inductive biases has recently drawn extensive interest. However,\nexisting tracking approaches rely on customized sub-modules and need prior\nknowledge for architecture selection, hindering the tracking development in a\nmore general system. This paper presents a Simplified Tracking architecture\n(SimTrack) by leveraging a transformer backbone for joint feature extraction\nand interaction. Unlike existing Siamese trackers, we serialize the input\nimages and concatenate them directly before the one-branch backbone. Feature\ninteraction in the backbone helps to remove well-designed interaction modules\nand produce a more efficient and effective framework. To reduce the information\nloss from down-sampling in vision transformers, we further propose a foveal\nwindow strategy, providing more diverse input patches with acceptable\ncomputational costs. Our SimTrack improves the baseline with 2.5%/2.6% AUC\ngains on LaSOT/TNL2K and gets results competitive with other specialized\ntracking algorithms without bells and whistles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peixia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Lei Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Q/0/1/0/all/0/1\">Qiuhong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1\">Weihao Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wanli Ouyang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfTune: Metrically Scaled Monocular Depth Estimation through Self-Supervised Learning. (arXiv:2203.05332v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05332","description":"<p>Monocular depth estimation in the wild inherently predicts depth up to an\nunknown scale. To resolve scale ambiguity issue, we present a learning\nalgorithm that leverages monocular simultaneous localization and mapping (SLAM)\nwith proprioceptive sensors. Such monocular SLAM systems can provide metrically\nscaled camera poses. Given these metric poses and monocular sequences, we\npropose a self-supervised learning method for the pre-trained supervised\nmonocular depth networks to enable metrically scaled depth estimation. Our\napproach is based on a teacher-student formulation which guides our network to\npredict high-quality depths. We demonstrate that our approach is useful for\nvarious applications such as mobile robot navigation and is applicable to\ndiverse environments. Our full system shows improvements over recent\nself-supervised depth estimation and completion methods on EuRoC, OpenLORIS,\nand ScanNet datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaehoon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1\">Dongki Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yonghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Deokhwa Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Donghwan Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Corresponding Geometry: Fusing Region and Depth for Highly Efficient 3D Tracking of Textureless Objects. (arXiv:2203.05334v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05334","description":"<p>Tracking objects in 3D space and predicting their 6DoF pose is an essential\ntask in computer vision. State-of-the-art approaches often rely on object\ntexture to tackle this problem. However, while they achieve impressive results,\nmany objects do not contain sufficient texture, violating the main underlying\nassumption. In the following, we thus propose ICG, a novel probabilistic\ntracker that fuses region and depth information and only requires the object\ngeometry. Our method deploys correspondence lines and points to iteratively\nrefine the pose. We also implement robust occlusion handling to improve\nperformance in real-world settings. Experiments on the YCB-Video, OPT, and Choi\ndatasets demonstrate that, even for textured objects, our approach outperforms\nthe current state of the art with respect to accuracy and robustness. At the\nsame time, ICG shows fast convergence and outstanding efficiency, requiring\nonly 1.3 ms per frame on a single CPU core. Finally, we analyze the influence\nof individual components and discuss our performance compared to deep\nlearning-based methods. The source code of our tracker is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stoiber_M/0/1/0/all/0/1\">Manuel Stoiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundermeyer_M/0/1/0/all/0/1\">Martin Sundermeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1\">Rudolph Triebel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-generative Generalized Zero-shot Learning via Task-correlated Disentanglement and Controllable Samples Synthesis. (arXiv:2203.05335v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05335","description":"<p>Synthesizing pseudo samples is currently the most effective way to solve the\nGeneralized Zero Shot Learning (GZSL) problem. Most models achieve competitive\nperformance but still suffer from two problems: (1) feature confounding, that\ntask-correlated and task-independent features are confounded in overall\nrepresentations, which is unreasonable to synthesize reliable pseudo samples;\nand (2) distribution uncertainty, that massive data is needed when existing\nmodels synthesize samples from the uncertain distribution, which causes poor\nperformance in limited samples of seen classes. In this paper, we propose a\nnon-generative model to address these problems correspondingly in two modules:\n(1) Task-correlated feature disentanglement, to exclude the task-correlated\nfeatures from task-independent ones by adversarial learning of domain adaption\ntowards reasonable synthesis; and (2) Controllable pseudo sample synthesis, to\nsynthesize edge-pseudo and center-pseudo samples with certain characteristics\ntowards more diversity generated and intuitive transfer. To describe the new\nscene that is the limit seen class samples in the training process, we further\nformulate a new ZSL task named the 'Few-shot Seen class and Zero-shot Unseen\nclass learning' (FSZU). Extensive experiments on four benchmarks verify that\nthe proposed method is competitive in the GZSL and the FSZU tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yaogong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaowen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Pengbo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jian Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1\">Jitao Sang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrueType Transformer: Character and Font Style Recognition in Outline Format. (arXiv:2203.05338v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05338","description":"<p>We propose TrueType Transformer (T3), which can perform character and font\nstyle recognition in an outline format. The outline format, such as TrueType,\nrepresents each character as a sequence of control points of stroke contours\nand is frequently used in born-digital documents. T3 is organized by a deep\nneural network, so-called Transformer. Transformer is originally proposed for\nsequential data, such as text, and therefore appropriate for handling the\noutline data. In other words, T3 directly accepts the outline data without\nconverting it into a bitmap image. Consequently, T3 realizes a\nresolution-independent classification. Moreover, since the locations of the\ncontrol points represent the fine and local structures of the font style, T3 is\nsuitable for font style classification, where such structures are very\nimportant. In this paper, we experimentally show the applicability of T3 in\ncharacter and font style recognition tasks, while observing how the individual\ncontrol points contribute to classification results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagata_Y/0/1/0/all/0/1\">Yusuke Nagata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otao_J/0/1/0/all/0/1\">Jinki Otao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1\">Daichi Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05340","description":"<p>With diverse presentation attacks emerging continually, generalizable face\nanti-spoofing (FAS) has drawn growing attention. Most existing methods\nimplement domain generalization (DG) on the complete representations. However,\ndifferent image statistics may have unique properties for the FAS tasks. In\nthis work, we separate the complete representation into content and style ones.\nA novel Shuffled Style Assembly Network (SSAN) is proposed to extract and\nreassemble different content and style features for a stylized feature space.\nThen, to obtain a generalized representation, a contrastive learning strategy\nis developed to emphasize liveness-related style information while suppress the\ndomain-specific one. Finally, the representations of the correct assemblies are\nused to distinguish between living and spoofing during the inferring. On the\nother hand, despite the decent performance, there still exists a gap between\nacademia and industry, due to the difference in data quantity and distribution.\nThus, a new large-scale benchmark for FAS is built up to further evaluate the\nperformance of algorithms in reality. Both qualitative and quantitative results\non existing and proposed benchmarks demonstrate the effectiveness of our\nmethods. The codes will be available at https://github.com/wangzhuo2019/SSAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Size Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EyeLoveGAN: Exploiting domain-shifts to boost network learning with cycleGANs. (arXiv:2203.05344v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05344","description":"<p>This paper presents our contribution to the REFUGE challenge 2020. The\nchallenge consisted of three tasks based on a dataset of retinal images:\nSegmentation of optic disc and cup, classification of glaucoma, and\nlocalization of fovea. We propose employing convolutional neural networks for\nall three tasks. Segmentation is performed using a U-Net, classification is\nperformed by a pre-trained InceptionV3 network, and fovea detection is\nperformed by employing stacked hour-glass for heatmap prediction. The challenge\ndataset contains images from three different data sources. To enhance\nperformance, cycleGANs were utilized to create a domain-shift between the data\nsources. These cycleGANs move images across domains, thus creating artificial\nimages which can be used for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sundgaard_J/0/1/0/all/0/1\">Josefine Vilsb&#xf8;ll Sundgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juhl_K/0/1/0/all/0/1\">Kristine Aavild Juhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slipsager_J/0/1/0/all/0/1\">Jakob M&#xf8;lkj&#xe6;r Slipsager</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-enriched Attention Network with Group-wise Semantic for Visual Storytelling. (arXiv:2203.05346v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05346","description":"<p>As a technically challenging topic, visual storytelling aims at generating an\nimaginary and coherent story with narrative multi-sentences from a group of\nrelevant images. Existing methods often generate direct and rigid descriptions\nof apparent image-based contents, because they are not capable of exploring\nimplicit information beyond images. Hence, these schemes could not capture\nconsistent dependencies from holistic representation, impairing the generation\nof reasonable and fluent story. To address these problems, a novel\nknowledge-enriched attention network with group-wise semantic model is\nproposed. Three main novel components are designed and supported by substantial\nexperiments to reveal practical advantages. First, a knowledge-enriched\nattention network is designed to extract implicit concepts from external\nknowledge system, and these concepts are followed by a cascade cross-modal\nattention mechanism to characterize imaginative and concrete representations.\nSecond, a group-wise semantic module with second-order pooling is developed to\nexplore the globally consistent guidance. Third, a unified one-stage story\ngeneration model with encoder-decoder structure is proposed to simultaneously\ntrain and infer the knowledge-enriched attention network, group-wise semantic\nmodule and multi-modal story generation decoder in an end-to-end fashion.\nSubstantial experiments on the popular Visual Storytelling dataset with both\nobjective and subjective evaluation metrics demonstrate the superior\nperformance of the proposed scheme as compared with other state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tengpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bin He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Wen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stream Hierarchical Similarity Reasoning for Image-text Matching. (arXiv:2203.05349v1 [cs.MM])","link":"http://arxiv.org/abs/2203.05349","description":"<p>Reasoning-based approaches have demonstrated their powerful ability for the\ntask of image-text matching. In this work, two issues are addressed for\nimage-text matching. First, for reasoning processing, conventional approaches\nhave no ability to find and use multi-level hierarchical similarity\ninformation. To solve this problem, a hierarchical similarity reasoning module\nis proposed to automatically extract context information, which is then\nco-exploited with local interaction information for efficient reasoning.\nSecond, previous approaches only consider learning single-stream similarity\nalignment (i.e., image-to-text level or text-to-image level), which is\ninadequate to fully use similarity information for image-text matching. To\naddress this issue, a two-stream architecture is developed to decompose\nimage-text matching into image-to-text level and text-to-image level similarity\ncomputation. These two issues are investigated by a unifying framework that is\ntrained in an end-to-end manner, namely two-stream hierarchical similarity\nreasoning network. The extensive experiments performed on the two benchmark\ndatasets of MSCOCO and Flickr30K show the superiority of the proposed approach\nas compared to existing state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Context for Robust Maritime Obstacle Detection. (arXiv:2203.05352v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05352","description":"<p>Robust maritime obstacle detection is essential for fully autonomous unmanned\nsurface vehicles (USVs). The currently widely adopted segmentation-based\nobstacle detection methods are prone to misclassification of object reflections\nand sun glitter as obstacles, producing many false positive detections,\neffectively rendering the methods impractical for USV navigation. However,\nwater-turbulence-induced temporal appearance changes on object reflections are\nvery distinctive from the appearance dynamics of true objects. We harness this\nproperty to design WaSR-T, a novel maritime obstacle detection network, that\nextracts the temporal context from a sequence of recent frames to reduce\nambiguity. By learning the local temporal characteristics of object reflection\non the water surface, WaSR-T substantially improves obstacle detection accuracy\nin the presence of reflections and glitter. Compared with existing single-frame\nmethods, WaSR-T reduces the number of false positive detections by 41% overall\nand by over 53% within the danger zone of the boat, while preserving a high\nrecall, and achieving new state-of-the-art performance on the challenging MODS\nmaritime obstacle detection benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zust_L/0/1/0/all/0/1\">Lojze &#x17d;ust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Commonsense Graph for Object Localisation in Partial Scenes. (arXiv:2203.05380v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05380","description":"<p>We solve object localisation in partial scenes, a new problem of estimating\nthe unknown position of an object (e.g. where is the bag?) given a partial 3D\nscan of a scene. The proposed solution is based on a novel scene graph model,\nthe Spatial Commonsense Graph (SCG), where objects are the nodes and edges\ndefine pairwise distances between them, enriched by concept nodes and\nrelationships from a commonsense knowledge base. This allows SCG to better\ngeneralise its spatial inference over unknown 3D scenes. The SCG is used to\nestimate the unknown position of the target object in two steps: first, we feed\nthe SCG into a novel Proximity Prediction Network, a graph neural network that\nuses attention to perform distance prediction between the node representing the\ntarget object and the nodes representing the observed objects in the SCG;\nsecond, we propose a Localisation Module based on circular intersection to\nestimate the object position using all the predicted pairwise distances in\norder to be independent of any reference system. We create a new dataset of\npartially reconstructed scenes to benchmark our method and baselines for object\nlocalisation in partial scenes, where our proposed method achieves the best\nlocalisation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giuliari_F/0/1/0/all/0/1\">Francesco Giuliari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiming Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1\">Alessio Del Bue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Annotation Efficient Person Re-Identification with Diverse Cluster-Based Pair Selection. (arXiv:2203.05395v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05395","description":"<p>Person Re-identification (Re-ID) has attracted great attention due to its\npromising real-world applications. However, in practice, it is always costly to\nannotate the training data to train a Re-ID model, and it still remains\nchallenging to reduce the annotation cost while maintaining the performance for\nthe Re-ID task. To solve this problem, we propose the Annotation Efficient\nPerson Re-Identification method to select image pairs from an alternative pair\nset according to the fallibility and diversity of pairs, and train the Re-ID\nmodel based on the annotation. Specifically, we design an annotation and\ntraining framework to firstly reduce the size of the alternative pair set by\nclustering all images considering the locality of features, secondly select\nimages pairs from intra-/inter-cluster samples for human to annotate, thirdly\nre-assign clusters according to the annotation, and finally train the model\nwith the re-assigned clusters. During the pair selection, we seek for valuable\npairs according to pairs' fallibility and diversity, which includes an\nintra-cluster criterion to construct image pairs with the most chaotic samples\nand the representative samples within clusters, an inter-cluster criterion to\nconstruct image pairs between clusters based on the second-order Wasserstein\ndistance, and a diversity criterion for clusterbased pair selection. Combining\nall criteria above, a greedy strategy is developed to solve the pair selection\nproblem. Finally, the above\nclustering-selecting-annotating-reassigning-training procedure will be repeated\nuntil the annotation budget is reached. Extensive experiments on three widely\nadopted Re-ID datasets show that we can greatly reduce the annotation cost\nwhile achieving better performance compared with state-of-the-art works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1\">Lantian Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yixiong Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1\">Peixi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tiejun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representation Compensation Networks for Continual Semantic Segmentation. (arXiv:2203.05402v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05402","description":"<p>In this work, we study the continual semantic segmentation problem, where the\ndeep neural networks are required to incorporate new classes continually\nwithout catastrophic forgetting. We propose to use a structural\nre-parameterization mechanism, named representation compensation (RC) module,\nto decouple the representation learning of both old and new knowledge. The RC\nmodule consists of two dynamically evolved branches with one frozen and one\ntrainable. Besides, we design a pooled cube knowledge distillation strategy on\nboth spatial and channel dimensions to further enhance the plasticity and\nstability of the model. We conduct experiments on two challenging continual\nsemantic segmentation scenarios, continual class segmentation and continual\ndomain segmentation. Without any extra computational overhead and parameters\nduring inference, our method outperforms state-of-the-art performance. The code\nis available at \\url{https://github.com/zhangchbin/RCIL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chang-Bin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jia-Wen Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying-Cong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoopITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval. (arXiv:2203.05465v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05465","description":"<p>Dual encoders and cross encoders have been widely used for image-text\nretrieval. Between the two, the dual encoder encodes the image and text\nindependently followed by a dot product, while the cross encoder jointly feeds\nimage and text as the input and performs dense multi-modal fusion. These two\narchitectures are typically modeled separately without interaction. In this\nwork, we propose LoopITR, which combines them in the same network for joint\nlearning. Specifically, we let the dual encoder provide hard negatives to the\ncross encoder, and use the more discriminative cross encoder to distill its\npredictions back to the dual encoder. Both steps are efficiently performed\ntogether in the same model. Our work centers on empirical analyses of this\ncombined architecture, putting the main focus on the design of the distillation\nobjective. Our experimental results highlight the benefits of training the two\nencoders in the same network, and demonstrate that distillation can be quite\neffective with just a few hard negative examples. Experiments on two standard\ndatasets (Flickr30K and COCO) show our approach achieves state-of-the-art dual\nencoder performance when compared with approaches using a similar amount of\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">Jie Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mengjiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1\">Tamara L. Berg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Licheng Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prediction-Guided Distillation for Dense Object Detection. (arXiv:2203.05469v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05469","description":"<p>Real-world object detection models should be cheap and accurate. Knowledge\ndistillation (KD) can boost the accuracy of a small, cheap detection model by\nleveraging useful information from a larger teacher model. However, a key\nchallenge is identifying the most informative features produced by the teacher\nfor distillation. In this work, we show that only a very small fraction of\nfeatures within a ground-truth bounding box are responsible for a teacher's\nhigh detection performance. Based on this, we propose Prediction-Guided\nDistillation (PGD), which focuses distillation on these key predictive regions\nof the teacher and yields considerable gains in performance over many existing\nKD baselines. In addition, we propose an adaptive weighting scheme over the key\nregions to smooth out their influence and achieve even better performance. Our\nproposed approach outperforms current state-of-the-art KD baselines on a\nvariety of advanced one-stage detection architectures. Specifically, on the\nCOCO dataset, our method achieves between +3.1% and +4.6% AP improvement using\nResNet-101 and ResNet-50 as the teacher and student backbones, respectively. On\nthe CrowdHuman dataset, we achieve +3.2% and +2.0% improvements in MR and AP,\nalso using these backbones. Our code is available at\nhttps://github.com/ChenhongyiYang/PGD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenhongyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ochal_M/0/1/0/all/0/1\">Mateusz Ochal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1\">Elliot J. Crowley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time. (arXiv:2203.05482v1 [cs.LG])","link":"http://arxiv.org/abs/2203.05482","description":"<p>The conventional recipe for maximizing model accuracy is to (1) train\nmultiple models with various hyperparameters and (2) pick the individual model\nwhich performs best on a held-out validation set, discarding the remainder. In\nthis paper, we revisit the second step of this procedure in the context of\nfine-tuning large pre-trained models, where fine-tuned models often appear to\nlie in a single low error basin. We show that averaging the weights of multiple\nmodels fine-tuned with different hyperparameter configurations often improves\naccuracy and robustness. Unlike a conventional ensemble, we may average many\nmodels without incurring any additional inference or memory costs -- we call\nthe results \"model soups.\" When fine-tuning large pre-trained models such as\nCLIP, ALIGN, and a ViT-G pre-trained on JFT, our soup recipe provides\nsignificant improvements over the best model in a hyperparameter sweep on\nImageNet. As a highlight, the resulting ViT-G model attains 90.94% top-1\naccuracy on ImageNet, a new state of the art. Furthermore, we show that the\nmodel soup approach extends to multiple image classification and natural\nlanguage processing tasks, improves out-of-distribution performance, and\nimproves zero-shot performance on new downstream tasks. Finally, we\nanalytically relate the performance similarity of weight-averaging and\nlogit-ensembling to flatness of the loss and confidence of the predictions, and\nvalidate this relation empirically.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wortsman_M/0/1/0/all/0/1\">Mitchell Wortsman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilharco_G/0/1/0/all/0/1\">Gabriel Ilharco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadre_S/0/1/0/all/0/1\">Samir Yitzhak Gadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gontijo_Lopes_R/0/1/0/all/0/1\">Raphael Gontijo-Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1\">Ari S. Morcos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1\">Hongseok Namkoong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1\">Ali Farhadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1\">Simon Kornblith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1\">Ludwig Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Less Constrained Macro-Neural Architecture Search. (arXiv:2203.05508v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05508","description":"<p>Networks found with Neural Architecture Search (NAS) achieve state-of-the-art\nperformance in a variety of tasks, out-performing human-designed networks.\nHowever, most NAS methods heavily rely on human-defined assumptions that\nconstrain the search: architecture's outer-skeletons, number of layers,\nparameter heuristics and search spaces. Additionally, common search spaces\nconsist of repeatable modules (cells) instead of fully exploring the\narchitecture's search space by designing entire architectures (macro-search).\nImposing such constraints requires deep human expertise and restricts the\nsearch to pre-defined settings. In this paper, we propose LCMNAS, a method that\npushes NAS to less constrained search spaces by performing macro-search without\nrelying on pre-defined heuristics or bounded search spaces. LCMNAS introduces\nthree components for the NAS pipeline: i) a method that leverages information\nabout well-known architectures to autonomously generate complex search spaces\nbased on Weighted Directed Graphs with hidden properties, ii) a evolutionary\nsearch strategy that generates complete architectures from scratch, and iii) a\nmixed-performance estimation approach that combines information about\narchitectures at initialization stage and lower fidelity estimates to infer\ntheir trainability and capacity to model complex functions. We present\nexperiments showing that LCMNAS generates state-of-the-art architectures from\nscratch with minimal GPU computation. We study the importance of different NAS\ncomponents on a macro-search setting. Code for reproducibility is public at\n\\url{https://github.com/VascoLopes/LCMNAS}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lopes_V/0/1/0/all/0/1\">Vasco Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexandre_L/0/1/0/all/0/1\">Lu&#xed;s A. Alexandre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGCN: Augmented Graph Convolutional Network for Lifelong Multi-label Image Recognition. (arXiv:2203.05534v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05534","description":"<p>The Lifelong Multi-Label (LML) image recognition builds an online\nclass-incremental classifier in a sequential multi-label image recognition data\nstream. The key challenges of LML image recognition are the construction of\nlabel relationships on Partial Labels of training data and the Catastrophic\nForgetting on old classes, resulting in poor generalization. To solve the\nproblems, the study proposes an Augmented Graph Convolutional Network (AGCN)\nmodel that can construct the label relationships across the sequential\nrecognition tasks and sustain the catastrophic forgetting. First, we build an\nAugmented Correlation Matrix (ACM) across all seen classes, where the\nintra-task relationships derive from the hard label statistics while the\ninter-task relationships leverage both hard and soft labels from data and a\nconstructed expert network. Then, based on the ACM, the proposed AGCN captures\nlabel dependencies with dynamic augmented structure and yields effective class\nrepresentations. Last, to suppress the forgetting of label dependencies across\nold tasks, we propose a relationship-preserving loss as a constraint to the\nconstruction of label relationships. The proposed method is evaluated using two\nmulti-label image benchmarks and the experimental results show that the\nproposed method is effective for LML image recognition and can build convincing\ncorrelation across tasks even if the labels of previous tasks are missing. Our\ncode is available at https://github.com/Kaile-Du/AGCN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kaile Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1\">Fan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fuyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fenglei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qiming Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Investigation of 3D Anomaly Detection and Segmentation. (arXiv:2203.05550v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05550","description":"<p>Anomaly detection and segmentation in images has made tremendous progress in\nrecent years while 3D information has often been ignored. The objective of this\npaper is to further understand the benefit and role of 3D as opposed to color\nin image anomaly detection. Our study begins by presenting a surprising\nfinding: standard color-only anomaly segmentation methods, when applied to 3D\ndatasets, significantly outperform all current methods. On the other hand, we\nobserve that color-only methods are insufficient for images containing\ngeometric anomalies where shape cannot be unambiguously inferred from 2D. This\nsuggests that better 3D methods are needed. We investigate different\nrepresentations for 3D anomaly detection and discover that handcrafted\norientation-invariant representations are unreasonably effective on this task.\nWe uncover a simple 3D-only method that outperforms all recent approaches while\nnot using deep learning, external pretraining datasets, or color information.\nAs the 3D-only method cannot detect color and texture anomalies, we combine it\nwith 2D color features, granting us the best current results by a large margin\n(Pixel-wise ROCAUC: 99.2%, PRO: 95.9% on MVTec 3D-AD). We conclude by\ndiscussing future challenges for 3D anomaly detection and segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horwitz_E/0/1/0/all/0/1\">Eliahu Horwitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transfer of Representations to Video Label Propagation: Implementation Factors Matter. (arXiv:2203.05553v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05553","description":"<p>This work studies feature representations for dense label propagation in\nvideo, with a focus on recently proposed methods that learn video\ncorrespondence using self-supervised signals such as colorization or temporal\ncycle consistency. In the literature, these methods have been evaluated with an\narray of inconsistent settings, making it difficult to discern trends or\ncompare performance fairly. Starting with a unified formulation of the label\npropagation algorithm that encompasses most existing variations, we\nsystematically study the impact of important implementation factors in feature\nextraction and label propagation. Along the way, we report the accuracies of\nproperly tuned supervised and unsupervised still image baselines, which are\nhigher than those found in previous works. We also demonstrate that augmenting\nvideo-based correspondence cues with still-image-based ones can further improve\nperformance. We then attempt a fair comparison of recent video-based methods on\nthe DAVIS benchmark, showing convergence of best methods to performance levels\nnear our strong ImageNet baseline, despite the usage of a variety of\nspecialized video-based losses and training particulars. Additional comparisons\non JHMDB and VIP datasets confirm the similar performance of current methods.\nWe hope that this study will help to improve evaluation practices and better\ninform future research directions in temporal correspondence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKee_D/0/1/0/all/0/1\">Daniel McKee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Z/0/1/0/all/0/1\">Zitong Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuai_B/0/1/0/all/0/1\">Bing Shuai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modolo_D/0/1/0/all/0/1\">Davide Modolo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1\">Svetlana Lazebnik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Prompt Learning for Vision-Language Models. (arXiv:2203.05557v1 [cs.CV])","link":"http://arxiv.org/abs/2203.05557","description":"<p>With the rise of powerful pre-trained vision-language models like CLIP, it\nbecomes essential to investigate ways to adapt these models to downstream\ndatasets. A recently proposed method named Context Optimization (CoOp)\nintroduces the concept of prompt learning -- a recent trend in NLP -- to the\nvision domain for adapting pre-trained vision-language models. Specifically,\nCoOp turns context words in a prompt into a set of learnable vectors and, with\nonly a few labeled images for learning, can achieve huge improvements over\nintensively-tuned manual prompts. In our study we identify a critical problem\nof CoOp: the learned context is not generalizable to wider unseen classes\nwithin the same dataset, suggesting that CoOp overfits base classes observed\nduring training. To address the problem, we propose Conditional Context\nOptimization (CoCoOp), which extends CoOp by further learning a lightweight\nneural network to generate for each image an input-conditional token (vector).\nCompared to CoOp's static prompts, our dynamic prompts adapt to each instance\nand are thus less sensitive to class shift. Extensive experiments show that\nCoCoOp generalizes much better than CoOp to unseen classes, even showing\npromising transferability beyond a single dataset; and yields stronger domain\ngeneralization performance as well. Code is available at\nhttps://github.com/KaiyangZhou/CoOp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Noisy Labels with Deep Neural Networks: A Survey. (arXiv:2007.08199v7 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2007.08199","description":"<p>Deep learning has achieved remarkable success in numerous domains with help\nfrom large amounts of big data. However, the quality of data labels is a\nconcern because of the lack of high-quality labels in many real-world\nscenarios. As noisy labels severely degrade the generalization performance of\ndeep neural networks, learning from noisy labels (robust training) is becoming\nan important task in modern deep learning applications. In this survey, we\nfirst describe the problem of learning with label noise from a supervised\nlearning perspective. Next, we provide a comprehensive review of 62\nstate-of-the-art robust training methods, all of which are categorized into\nfive groups according to their methodological difference, followed by a\nsystematic comparison of six properties used to evaluate their superiority.\nSubsequently, we perform an in-depth analysis of noise rate estimation and\nsummarize the typically used evaluation methodology, including public noisy\ndatasets and evaluation metrics. Finally, we present several promising research\ndirections that can serve as a guideline for future studies. All the contents\nwill be available at https://github.com/songhwanjun/Awesome-Noisy-Labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minseok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dongmin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yooju Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae-Gil Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptual Compression via Deep Structure and Texture Synthesis. (arXiv:2011.04976v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.04976","description":"<p>Existing compression methods typically focus on the removal of signal-level\nredundancies, while the potential and versatility of decomposing visual data\ninto compact conceptual components still lack further study. To this end, we\npropose a novel conceptual compression framework that encodes visual data into\ncompact structure and texture representations, then decodes in a deep synthesis\nfashion, aiming to achieve better visual reconstruction quality, flexible\ncontent manipulation, and potential support for various vision tasks. In\nparticular, we propose to compress images by a dual-layered model consisting of\ntwo complementary visual features: 1) structure layer represented by structural\nmaps and 2) texture layer characterized by low-dimensional deep\nrepresentations. At the encoder side, the structural maps and texture\nrepresentations are individually extracted and compressed, generating the\ncompact, interpretable, inter-operable bitstreams. During the decoding stage, a\nhierarchical fusion GAN (HF-GAN) is proposed to learn the synthesis paradigm\nwhere the textures are rendered into the decoded structural maps, leading to\nhigh-quality reconstruction with remarkable visual realism. Extensive\nexperiments on diverse images have demonstrated the superiority of our\nframework with lower bitrates, higher reconstruction quality, and increased\nversatility towards visual analysis and content manipulation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianhui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhenghui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chuanmin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingbo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1\">Qi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Volumetrized Deep Generative Models for Data-Efficient Contextual Learning of MR Image Recovery. (arXiv:2011.13913v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.13913","description":"<p>Magnetic resonance imaging (MRI) offers the flexibility to image a given\nanatomic volume under a multitude of tissue contrasts. Yet, scan time\nconsiderations put stringent limits on the quality and diversity of MRI data.\nThe gold-standard approach to alleviate this limitation is to recover\nhigh-quality images from data undersampled across various dimensions, most\ncommonly the Fourier domain or contrast sets. A primary distinction among\nrecovery methods is whether the anatomy is processed per volume or per\ncross-section. Volumetric models offer enhanced capture of global contextual\ninformation, but they can suffer from suboptimal learning due to elevated model\ncomplexity. Cross-sectional models with lower complexity offer improved\nlearning behavior, yet they ignore contextual information across the\nlongitudinal dimension of the volume. Here, we introduce a novel progressive\nvolumetrization strategy for generative models (ProvoGAN) that serially\ndecomposes complex volumetric image recovery tasks into successive\ncross-sectional mappings task-optimally ordered across individual rectilinear\ndimensions. ProvoGAN effectively captures global context and recovers\nfine-structural details across all dimensions, while maintaining low model\ncomplexity and improved learning behaviour. Comprehensive demonstrations on\nmainstream MRI reconstruction and synthesis tasks show that ProvoGAN yields\nsuperior performance to state-of-the-art volumetric and cross-sectional models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozbey_M/0/1/0/all/0/1\">Muzaffer &#xd6;zbey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dar_S/0/1/0/all/0/1\">Salman Ul Hassan Dar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinaz_B/0/1/0/all/0/1\">Berk T&#x131;naz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_K/0/1/0/all/0/1\">Kader Karl&#x131; O&#x11f;uz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HANA: A HAndwritten NAme Database for Offline Handwritten Text Recognition. (arXiv:2101.10862v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10862","description":"<p>Methods for linking individuals across historical data sets, typically in\ncombination with AI based transcription models, are developing rapidly.\nProbably the single most important identifier for linking is personal names.\nHowever, personal names are prone to enumeration and transcription errors and\nalthough modern linking methods are designed to handle such challenges, these\nsources of errors are critical and should be minimized. For this purpose,\nimproved transcription methods and large-scale databases are crucial\ncomponents. This paper describes and provides documentation for HANA, a newly\nconstructed large-scale database which consists of more than 3.3 million names.\nThe database contain more than 105 thousand unique names with a total of more\nthan 1.1 million images of personal names, which proves useful for transfer\nlearning to other settings. We provide three examples hereof, obtaining\nsignificantly improved transcription accuracy on both Danish and US census\ndata. In addition, we present benchmark results for deep learning models\nautomatically transcribing the personal names from the scanned documents.\nThrough making more challenging large-scale databases publicly available we\nhope to foster more sophisticated, accurate, and robust models for handwritten\ntext recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahl_C/0/1/0/all/0/1\">Christian M. Dahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johansen_T/0/1/0/all/0/1\">Torben Johansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorensen_E/0/1/0/all/0/1\">Emil N. S&#xf8;rensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wittrock_S/0/1/0/all/0/1\">Simon Wittrock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Compact Deep Neural Networks via Energy-Aware Pruning. (arXiv:2103.10858v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10858","description":"<p>Despite the remarkable performance, modern deep neural networks are\ninevitably accompanied by a significant amount of computational cost for\nlearning and deployment, which may be incompatible with their usage on edge\ndevices. Recent efforts to reduce these overheads involve pruning and\ndecomposing the parameters of various layers without performance deterioration.\nInspired by several decomposition studies, in this paper, we propose a novel\nenergy-aware pruning method that quantifies the importance of each filter in\nthe network using nuclear-norm (NN). Proposed energy-aware pruning leads to\nstate-of-the-art performance for Top-1 accuracy, FLOPs, and parameter reduction\nacross a wide range of scenarios with multiple network architectures on\nCIFAR-10 and ImageNet after fine-grained classification tasks. On toy\nexperiment, without fine-tuning, we can visually observe that NN has a minute\nchange in decision boundaries across classes and outperforms the previous\npopular criteria. We achieve competitive results with 40.4/49.8% of FLOPs and\n45.9/52.9% of parameter reduction with 94.13/94.61% in the Top-1 accuracy with\nResNet-56/110 on CIFAR-10, respectively. In addition, our observations are\nconsistent for a variety of different pruning setting in terms of data size as\nwell as data quality which can be emphasized in the stability of the\nacceleration and compression with negligible accuracy loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yeom_S/0/1/0/all/0/1\">Seul-Ki Yeom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_K/0/1/0/all/0/1\">Kyung-Hwan Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jee-Hyun Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AD-GAN: End-to-end Unsupervised Nuclei Segmentation with Aligned Disentangling Training. (arXiv:2107.11022v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.11022","description":"<p>We consider unsupervised cell nuclei segmentation in this paper. Exploiting\nthe recently-proposed unpaired image-to-image translation between cell nuclei\nimages and randomly synthetic masks, existing approaches, e.g., CycleGAN, have\nachieved encouraging results. However, these methods usually take a two-stage\npipeline and fail to learn end-to-end in cell nuclei images. More seriously,\nthey could lead to the lossy transformation problem, i.e., the content\ninconsistency between the original images and the corresponding segmentation\noutput. To address these limitations, we propose a novel end-to-end\nunsupervised framework called Aligned Disentangling Generative Adversarial\nNetwork (AD-GAN). Distinctively, AD-GAN introduces representation\ndisentanglement to separate content representation (the underling spatial\nstructure) from style representation (the rendering of the structure). With\nthis framework, spatial structure can be preserved explicitly, enabling a\nsignificant reduction of macro-level lossy transformation. We also propose a\nnovel training algorithm able to align the disentangled content in the latent\nspace to reduce micro-level lossy transformation. Evaluations on real-world 2D\nand 3D datasets show that AD-GAN substantially outperforms the other comparison\nmethods and the professional software both quantitatively and qualitatively.\nSpecifically, the proposed AD-GAN leads to significant improvement over the\ncurrent best unsupervised methods by an average 17.8% relatively (w.r.t. the\nmetric DICE) on four cell nuclei datasets. As an unsupervised method, AD-GAN\neven performs competitive with the best supervised models, taking a further\nleap towards end-to-end unsupervised nuclei segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yao_K/0/1/0/all/0/1\">Kai Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jie Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jude_C/0/1/0/all/0/1\">Curran Jude</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patchwork: Concentric Zone-based Region-wise Ground Segmentation with Ground Likelihood Estimation Using a 3D LiDAR Sensor. (arXiv:2108.05560v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.05560","description":"<p>Ground segmentation is crucial for terrestrial mobile platforms to perform\nnavigation or neighboring object recognition. Unfortunately, the ground is not\nflat, as it features steep slopes; bumpy roads; or objects, such as curbs,\nflower beds, and so forth. To tackle the problem, this paper presents a novel\nground segmentation method called \\textit{Patchwork}, which is robust for\naddressing the under-segmentation problem and operates at more than 40 Hz. In\nthis paper, a point cloud is encoded into a Concentric Zone Model-based\nrepresentation to assign an appropriate density of cloud points among bins in a\nway that is not computationally complex. This is followed by Region-wise Ground\nPlane Fitting, which is performed to estimate the partial ground for each bin.\nFinally, Ground Likelihood Estimation is introduced to dramatically reduce\nfalse positives. As experimentally verified on SemanticKITTI and rough terrain\ndatasets, our proposed method yields promising performance compared with the\nstate-of-the-art methods, showing faster speed compared with existing plane\nfitting--based methods. Code is available:\nhttps://github.com/LimHyungTae/patchwork\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyungtae Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_M/0/1/0/all/0/1\">Minho Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review and experimental evaluation of deep learning methods for MRI reconstruction. (arXiv:2109.08618v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.08618","description":"<p>Following the success of deep learning in a wide range of applications,\nneural network-based machine-learning techniques have received significant\ninterest for accelerating magnetic resonance imaging (MRI) acquisition and\nreconstruction strategies. A number of ideas inspired by deep learning\ntechniques for computer vision and image processing have been successfully\napplied to nonlinear image reconstruction in the spirit of compressed sensing\nfor accelerated MRI. Given the rapidly growing nature of the field, it is\nimperative to consolidate and summarize the large number of deep learning\nmethods that have been reported in the literature, to obtain a better\nunderstanding of the field in general. This article provides an overview of the\nrecent developments in neural-network based approaches that have been proposed\nspecifically for improving parallel imaging. A general background and\nintroduction to parallel MRI is also given from a classical view of k-space\nbased reconstruction methods. Image domain based techniques that introduce\nimproved regularizers are covered along with k-space based methods which focus\non better interpolation strategies using neural networks. While the field is\nrapidly evolving with plenty of papers published each year, in this review, we\nattempt to cover broad categories of methods that have shown good performance\non publicly available data sets. Limitations and open problems are also\ndiscussed and recent efforts for producing open data sets and benchmarks for\nthe community are examined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pal_A/0/1/0/all/0/1\">Arghya Pal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Contrastive Representation for Semantic Correspondence. (arXiv:2109.10967v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10967","description":"<p>Dense correspondence across semantically related images has been extensively\nstudied, but still faces two challenges: 1) large variations in appearance,\nscale and pose exist even for objects from the same category, and 2) labeling\npixel-level dense correspondences is labor intensive and infeasible to scale.\nMost existing approaches focus on designing various matching approaches with\nfully-supervised ImageNet pretrained networks. On the other hand, while a\nvariety of self-supervised approaches are proposed to explicitly measure\nimage-level similarities, correspondence matching the pixel level remains\nunder-explored. In this work, we propose a multi-level contrastive learning\napproach for semantic matching, which does not rely on any ImageNet pretrained\nmodel. We show that image-level contrastive learning is a key component to\nencourage the convolutional features to find correspondence between similar\nobjects, while the performance can be further enhanced by regularizing\ncross-instance cycle-consistency at intermediate feature levels. Experimental\nresults on the PF-PASCAL, PF-WILLOW, and SPair-71k benchmark datasets\ndemonstrate that our method performs favorably against the state-of-the-art\napproaches. The source code and trained models will be made available to the\npublic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Taihong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartoon Explanations of Image Classifiers. (arXiv:2110.03485v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2110.03485","description":"<p>We present {CartoonX} (Cartoon Explanation), a novel model-agnostic\nexplanation method tailored towards image classifiers and based on the\nrate-distortion explanation (RDE) framework. Natural images are roughly\npiece-wise smooth signals -- also called cartoon-like images -- and tend to be\nsparse in the wavelet domain. CartoonX is the first explanation method to\nexploit this by requiring its explanations to be sparse in the wavelet domain,\nthus extracting the {relevant piece-wise smooth} part of an image instead of\nrelevant pixel-sparse regions. We demonstrate that CartoonX can reveal novel\nvaluable explanatory information, particularly for misclassifications.\nMoreover, we show that CartoonX achieves a lower distortion with fewer\ncoefficients than other state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolek_S/0/1/0/all/0/1\">Stefan Kolek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duc Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levie_R/0/1/0/all/0/1\">Ron Levie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutyniok_G/0/1/0/all/0/1\">Gitta Kutyniok</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientPhys: Enabling Simple, Fast and Accurate Camera-Based Vitals Measurement. (arXiv:2110.04447v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04447","description":"<p>Camera-based physiological measurement is a growing field with neural models\nproviding state-the-art-performance. Prior research have explored various\n\"end-to-end\" models; however these methods still require several preprocessing\nsteps. These additional operations are often non-trivial to implement making\nreplication and deployment difficult and can even have a higher computational\nbudget than the \"core\" network itself. In this paper, we propose two novel and\nefficient neural models for camera-based physiological measurement called\nEfficientPhys that remove the need for face detection, segmentation,\nnormalization, color space transformation or any other preprocessing steps.\nUsing an input of raw video frames, our models achieve strong performance on\nthree public datasets. We show that this is the case whether using a\ntransformer or convolutional backbone. We further evaluate the latency of the\nproposed networks and show that our most light weight network also achieves a\n33% improvement in efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_B/0/1/0/all/0/1\">Brian L. Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Ziheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shwetak Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisit Dictionary Learning for Video Compressive Sensing under the Plug-and-Play Framework. (arXiv:2110.04966v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04966","description":"<p>Aiming at high-dimensional (HD) data acquisition and analysis, snapshot\ncompressive imaging (SCI) obtains the 2D compressed measurement of HD data with\noptical imaging systems and reconstructs HD data using compressive sensing\nalgorithms. While the Plug-and-Play (PnP) framework offers an emerging solution\nto SCI reconstruction, its intrinsic denoising process is still a challenging\nproblem. Unfortunately, existing denoisers in the PnP framework either suffer\nlimited performance or require extensive training data. In this paper, we\npropose an efficient and effective shallow-learning-based algorithm for video\nSCI reconstruction. Revisiting dictionary learning methods, we empower the PnP\nframework with a new denoiser, the kernel singular value decomposition (KSVD).\nBenefited from the advent of KSVD, our algorithm retains a good trade-off among\nquality, speed, and training difficulty. On a variety of datasets, both\nquantitative and qualitative evaluations of our simulation results demonstrate\nthe effectiveness of our proposed method. In comparison to a typical baseline\nusing total variation, our method achieves around $2$ dB improvement in PSNR\nand 0.2 in SSIM. We expect that our proposed PnP-KSVD algorithm can serve as a\nnew baseline for video SCI reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yaping Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFNet: Multi-class Few-shot Segmentation Network with Pixel-wise Metric Learning. (arXiv:2111.00232v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00232","description":"<p>In visual recognition tasks, few-shot learning requires the ability to learn\nobject categories with few support examples. Its re-popularity in light of the\ndeep learning development is mainly in image classification. This work focuses\non few-shot semantic segmentation, which is still a largely unexplored field. A\nfew recent advances are often restricted to single-class few-shot segmentation.\nIn this paper, we first present a novel multi-way (class) encoding and decoding\narchitecture which effectively fuses multi-scale query information and\nmulti-class support information into one query-support embedding. Multi-class\nsegmentation is directly decoded upon this embedding. For better feature\nfusion, a multi-level attention mechanism is proposed within the architecture,\nwhich includes the attention for support feature modulation and attention for\nmulti-scale combination. Last, to enhance the embedding space learning, an\nadditional pixel-wise metric learning module is introduced with triplet loss\nformulated on the pixel-level embedding of the input image. Extensive\nexperiments on standard benchmarks PASCAL-5i and COCO-20i show clear benefits\nof our method over the state of the art in few-shot segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Miaojing Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Li Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAC-ReconNet: A Multiple Acquisition Context based Convolutional Neural Network for MR Image Reconstruction using Dynamic Weight Prediction. (arXiv:2111.05055v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.05055","description":"<p>Convolutional Neural network-based MR reconstruction methods have shown to\nprovide fast and high quality reconstructions. A primary drawback with a\nCNN-based model is that it lacks flexibility and can effectively operate only\nfor a specific acquisition context limiting practical applicability. By\nacquisition context, we mean a specific combination of three input settings\nconsidered namely, the anatomy under study, undersampling mask pattern and\nacceleration factor for undersampling. The model could be trained jointly on\nimages combining multiple contexts. However the model does not meet the\nperformance of context specific models nor extensible to contexts unseen at\ntrain time. This necessitates a modification to the existing architecture in\ngenerating context specific weights so as to incorporate flexibility to\nmultiple contexts. We propose a multiple acquisition context based network,\ncalled MAC-ReconNet for MRI reconstruction, flexible to multiple acquisition\ncontexts and generalizable to unseen contexts for applicability in real\nscenarios. The proposed network has an MRI reconstruction module and a dynamic\nweight prediction (DWP) module. The DWP module takes the corresponding\nacquisition context information as input and learns the context-specific\nweights of the reconstruction module which changes dynamically with context at\nrun time. We show that the proposed approach can handle multiple contexts based\non cardiac and brain datasets, Gaussian and Cartesian undersampling patterns\nand five acceleration factors. The proposed network outperforms the naive\njointly trained model and gives competitive results with the context-specific\nmodels both quantitatively and qualitatively. We also demonstrate the\ngeneralizability of our model by testing on contexts unseen at train time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ramanarayanan_S/0/1/0/all/0/1\">Sriprabha Ramanarayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murugesan_B/0/1/0/all/0/1\">Balamurali Murugesan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ram_K/0/1/0/all/0/1\">Keerthi Ram</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sivaprakasam_M/0/1/0/all/0/1\">Mohanasankar Sivaprakasam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SequentialPointNet: A strong frame-level parallel point cloud sequence network for 3D action recognition. (arXiv:2111.08492v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08492","description":"<p>The point cloud sequence of 3D human actions consists of a set of ordered\npoint cloud frames. Compared to static point clouds, point cloud sequences have\nhuge data sizes proportional to the time dimension. Therefore, developing an\nefficient and lightweight point cloud sequence model is pivotal for 3D action\nrecognition. In this paper, we propose a strong frame-level parallel point\ncloud sequence network referred to as SequentialPointNet for 3D action\nrecognition. The key to our approach is to divide the main modeling operations\ninto frame-level units executed in parallel, which greatly improves the\nefficiency of modeling point cloud sequences.Moreover, we propose to flatten\nthe point cloud sequence into a new point data type named hyperpoint sequence\nthat preserves the complete spatial structure of each frame. Then, a novel\nHyperpoint-Mixer module is introduced to mix intra-frame spatial features and\ninter-frame temporal features of the hyperpoint sequence. By doing so,\nSequentialPointNet maximizes the appearance encoding ability and extracts\nsufficient motion information for effective human action recognition. Extensive\nexperiments show that SequentialPointNet achieves up to 10X faster than\nexisting point cloud sequence models. Additionally, our SequentialPointNet\nsurpasses state-of-the-art approaches for human action recognition on both\nlarge-scale datasets (i.e., NTU RGB+D 60 and NTU RGB+D 120) and small-scale\ndatasets (i.e., MSR Action3D and UTD-MHAD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhenjie Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianjin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TraSw: Tracklet-Switch Adversarial Attacks against Multi-Object Tracking. (arXiv:2111.08954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08954","description":"<p>Multi-Object Tracking (MOT) has achieved aggressive progress and derives many\nexcellent deep learning models. However, the robustness of the trackers is\nrarely studied, and it is challenging to attack the MOT system since its mature\nassociation algorithms are designed to be robust against errors during the\ntracking. In this work, we analyze the vulnerability of popular pedestrian MOT\ntrackers and propose a novel adversarial attack method called Tracklet-Switch\n(TraSw) against the complete tracking pipeline of MOT. TraSw can fool the\nadvanced deep trackers (i.e., FairMOT and ByteTrack) to fail to track the\ntargets in the subsequent frames by attacking very few frames. Experiments on\nthe MOT-Challenge datasets (i.e., 2DMOT15, MOT17, and MOT20) show that TraSw\ncan achieve an extraordinarily high success rate of over 95% by attacking only\nfour frames on average. To our knowledge, this is the first work on the\nadversarial attack against pedestrian MOT trackers. The code is available at\nhttps://github.com/DerryHub/FairMOT-attack .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Delv Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IntraQ: Learning Synthetic Images with Intra-Class Heterogeneity for Zero-Shot Network Quantization. (arXiv:2111.09136v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09136","description":"<p>Learning to synthesize data has emerged as a promising direction in zero-shot\nquantization (ZSQ), which represents neural networks by low-bit integer without\naccessing any of the real data. In this paper, we observe an interesting\nphenomenon of intra-class heterogeneity in real data and show that existing\nmethods fail to retain this property in their synthetic images, which causes a\nlimited performance increase. To address this issue, we propose a novel\nzero-shot quantization method referred to as IntraQ. First, we propose a local\nobject reinforcement that locates the target objects at different scales and\npositions of the synthetic images. Second, we introduce a marginal distance\nconstraint to form class-related features distributed in a coarse area. Lastly,\nwe devise a soft inception loss which injects a soft prior label to prevent the\nsynthetic images from being overfitting to a fixed object. Our IntraQ is\ndemonstrated to well retain the intra-class heterogeneity in the synthetic\nimages and also observed to perform state-of-the-art. For example, compared to\nthe advanced ZSQ, our IntraQ obtains 9.17\\% increase of the top-1 accuracy on\nImageNet when all layers of MobileNetV1 are quantized to 4-bit. Code is at\nhttps://github.com/zysxmu/IntraQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Gongrui Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jianzhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.11191","description":"<p>The paper presents a Convolutional Neural Networks (CNN) model for image\nclassification, aiming at increasing predictive performance for COVID-19\ndiagnosis while avoiding deeper and thus more complex alternatives. The\nproposed model includes four similar convolutional layers followed by a\nflattening and two dense layers. This work proposes a less complex solution\nbased on simply classifying 2D CT-Scan slices of images using their pixels via\na 2D CNN model. Despite the simplicity in architecture, the proposed model\nshowed improved quantitative results exceeding state-of-the-art on the same\ndataset of images, in terms of the macro f1 score. In this case study,\nextracting features from images, segmenting parts of the images, or other more\ncomplex techniques, ultimately aiming at images classification, do not yield\nbetter results. With that, this paper introduces a simple yet powerful deep\nlearning based solution for automated COVID-19 classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1\">Kenan Morani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1\">Devrim Unay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Modular Network for Video Captioning. (arXiv:2111.12476v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12476","description":"<p>Video captioning aims to generate natural language descriptions according to\nthe content, where representation learning plays a crucial role. Existing\nmethods are mainly developed within the supervised learning framework via\nword-by-word comparison of the generated caption against the ground-truth text\nwithout fully exploiting linguistic semantics. In this work, we propose a\nhierarchical modular network to bridge video representations and linguistic\nsemantics from three levels before generating captions. In particular, the\nhierarchy is composed of: (I) Entity level, which highlights objects that are\nmost likely to be mentioned in captions. (II) Predicate level, which learns the\nactions conditioned on highlighted objects and is supervised by the predicate\nin captions. (III) Sentence level, which learns the global semantic\nrepresentation and is supervised by the whole caption. Each level is\nimplemented by one module. Extensive experimental results show that the\nproposed method performs favorably against the state-of-the-art models on the\ntwo widely-used benchmarks: MSVD 104.0% and MSR-VTT 51.5% in CIDEr score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hanhua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guorong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Collaborative Graph Machines for Table Structure Recognition. (arXiv:2111.13359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13359","description":"<p>Recently, table structure recognition has achieved impressive progress with\nthe help of deep graph models. Most of them exploit single visual cues of\ntabular elements or simply combine visual cues with other modalities via early\nfusion to reason their graph relationships. However, neither early fusion nor\nindividually reasoning in terms of multiple modalities can be appropriate for\nall varieties of table structures with great diversity. Instead, different\nmodalities are expected to collaborate with each other in different patterns\nfor different table cases. In the community, the importance of intra-inter\nmodality interactions for table structure reasoning is still unexplored. In\nthis paper, we define it as heterogeneous table structure recognition\n(Hetero-TSR) problem. With the aim of filling this gap, we present a novel\nNeural Collaborative Graph Machines (NCGM) equipped with stacked collaborative\nblocks, which alternatively extracts intra-modality context and models\ninter-modality interactions in a hierarchical way. It can represent the\nintra-inter modality relationships of tabular elements more robustly, which\nsignificantly improves the recognition performance. We also show that the\nproposed NCGM can modulate collaborative pattern of different modalities\nconditioned on the context of intra-modality cues, which is vital for\ndiversified table cases. Experimental results on benchmarks demonstrate our\nproposed NCGM achieves state-of-the-art performance and beats other\ncontemporary methods by a large margin especially under challenging scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Deqiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinsong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bo Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.14820","description":"<p>Learning behavioral patterns from observational data has been a de-facto\napproach to motion forecasting. Yet, the current paradigm suffers from two\nshortcomings: brittle under distribution shifts and inefficient for knowledge\ntransfer. In this work, we propose to address these challenges from a causal\nrepresentation perspective. We first introduce a causal formalism of motion\nforecasting, which casts the problem as a dynamic process with three groups of\nlatent variables, namely invariant variables, style confounders, and spurious\nfeatures. We then introduce a learning framework that treats each group\nseparately: (i) unlike the common practice mixing datasets collected from\ndifferent locations, we exploit their subtle distinctions by means of an\ninvariance loss encouraging the model to suppress spurious correlations; (ii)\nwe devise a modular architecture that factorizes the representations of\ninvariant mechanisms and style confounders to approximate a sparse causal\ngraph; (iii) we introduce a style contrastive loss that not only enforces the\nstructure of style representations but also serves as a self-supervisory signal\nfor test-time refinement on the fly. Experiments on synthetic and real datasets\nshow that our proposed method improves the robustness and reusability of\nlearned motion representations, significantly outperforming prior\nstate-of-the-art motion forecasting models for out-of-distribution\ngeneralization and low-shot transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuejiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadei_R/0/1/0/all/0/1\">Riccardo Cadei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweizer_J/0/1/0/all/0/1\">Jonas Schweizer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1\">Sherwin Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Autoencoders: Toward a Meaningful and Decodable Representation. (arXiv:2111.15640v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15640","description":"<p>Diffusion probabilistic models (DPMs) have achieved remarkable quality in\nimage generation that rivals GANs'. But unlike GANs, DPMs use a set of latent\nvariables that lack semantic meaning and cannot serve as a useful\nrepresentation for other tasks. This paper explores the possibility of using\nDPMs for representation learning and seeks to extract a meaningful and\ndecodable representation of an input image via autoencoding. Our key idea is to\nuse a learnable encoder for discovering the high-level semantics, and a DPM as\nthe decoder for modeling the remaining stochastic variations. Our method can\nencode any image into a two-part latent code, where the first part is\nsemantically meaningful and linear, and the second part captures stochastic\ndetails, allowing near-exact reconstruction. This capability enables\nchallenging applications that currently foil GAN-based methods, such as\nattribute manipulation on real images. We also show that this two-level\nencoding improves denoising efficiency and naturally facilitates various\ndownstream tasks including few-shot conditional sampling. Please visit our\nproject page: https://Diff-AE.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Preechakul_K/0/1/0/all/0/1\">Konpat Preechakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatthee_N/0/1/0/all/0/1\">Nattanat Chatthee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wizadwongsa_S/0/1/0/all/0/1\">Suttisak Wizadwongsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suwajanakorn_S/0/1/0/all/0/1\">Supasorn Suwajanakorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Curvature-guided dynamic scale networks for Multi-view Stereo. (arXiv:2112.05999v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05999","description":"<p>Multi-view stereo (MVS) is a crucial task for precise 3D reconstruction. Most\nrecent studies tried to improve the performance of matching cost volume in MVS\nby designing aggregated 3D cost volumes and their regularization. This paper\nfocuses on learning a robust feature extraction network to enhance the\nperformance of matching costs without heavy computation in the other steps. In\nparticular, we present a dynamic scale feature extraction network, namely,\nCDSFNet. It is composed of multiple novel convolution layers, each of which can\nselect a proper patch scale for each pixel guided by the normal curvature of\nthe image surface. As a result, CDFSNet can estimate the optimal patch scales\nto learn discriminative features for accurate matching computation between\nreference and source images. By combining the robust extracted features with an\nappropriate cost formulation strategy, our resulting MVS architecture can\nestimate depth maps more precisely. Extensive experiments showed that the\nproposed method outperforms other state-of-the-art methods on complex outdoor\nscenes. It significantly improves the completeness of reconstructed models. As\na result, the method can process higher resolution inputs within faster\nrun-time and lower memory than other MVS methods. Our source code is available\nat url{https://github.com/TruongKhang/cds-mvsnet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giang_K/0/1/0/all/0/1\">Khang Truong Giang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Soohwan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Sungho Jo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking. (arXiv:2112.14016v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14016","description":"<p>Tracking visual objects from a single initial exemplar in the testing phase\nhas been broadly cast as a one-/few-shot problem, i.e., one-shot learning for\ninitial adaptation and few-shot learning for online adaptation. The recent\nfew-shot online adaptation methods incorporate the prior knowledge from large\namounts of annotated training data via complex meta-learning optimization in\nthe offline phase. This helps the online deep trackers to achieve fast\nadaptation and reduce overfitting risk in tracking. In this paper, we propose a\nsimple yet effective recursive least-squares estimator-aided online learning\napproach for few-shot online adaptation without requiring offline training. It\nallows an in-built memory retention mechanism for the model to remember the\nknowledge about the object seen before, and thus the seen data can be safely\nremoved from training. This also bears certain similarities to the emerging\ncontinual learning field in preventing catastrophic forgetting. This mechanism\nenables us to unveil the power of modern online deep trackers without incurring\ntoo much extra computational cost. We evaluate our approach based on two\nnetworks in the online learning families for tracking, i.e., multi-layer\nperceptrons in RT-MDNet and convolutional neural networks in DiMP. The\nconsistent improvements on several challenging tracking benchmarks demonstrate\nits effectiveness and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaojuan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_Y/0/1/0/all/0/1\">Yutong Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Depth Estimation from Multiple 360-degree Images Using Virtual Depth. (arXiv:2112.14931v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14931","description":"<p>In this paper, we propose a dense depth estimation pipeline for multiview\n360{\\deg} images. The proposed pipeline leverages a spherical camera model that\ncompensates for radial distortion in 360{\\deg} images. The key contribution of\nthis paper is the extension of a spherical camera model to multiview by\nintroducing a translation scaling scheme. Moreover, we propose an effective\ndense depth estimation method by setting virtual depth and minimizing photonic\nreprojection error. We validate the performance of the proposed pipeline using\nthe images of natural scenes as well as the synthesized dataset for quantitive\nevaluation. The experimental results verify that the proposed pipeline improves\nestimation accuracy compared to the current state-of-art dense depth estimation\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Seongyeop Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeejin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Non-Local Contrastive Attention for Image Super-Resolution. (arXiv:2201.03794v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.03794","description":"<p>Non-Local Attention (NLA) brings significant improvement for Single Image\nSuper-Resolution (SISR) by leveraging intrinsic feature correlation in natural\nimages. However, NLA gives noisy information large weights and consumes\nquadratic computation resources with respect to the input size, limiting its\nperformance and application. In this paper, we propose a novel Efficient\nNon-Local Contrastive Attention (ENLCA) to perform long-range visual modeling\nand leverage more relevant non-local features. Specifically, ENLCA consists of\ntwo parts, Efficient Non-Local Attention (ENLA) and Sparse Aggregation. ENLA\nadopts the kernel method to approximate exponential function and obtains linear\ncomputation complexity. For Sparse Aggregation, we multiply inputs by an\namplification factor to focus on informative features, yet the variance of\napproximation increases exponentially. Therefore, contrastive learning is\napplied to further separate relevant and irrelevant features. To demonstrate\nthe effectiveness of ENLCA, we build an architecture called Efficient Non-Local\nContrastive Network (ENLCN) by adding a few of our modules in a simple\nbackbone. Extensive experimental results show that ENLCN reaches superior\nperformance over state-of-the-art approaches on both quantitative and\nqualitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_Y/0/1/0/all/0/1\">Yucheng Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation for Reference-based Super-Resolution. (arXiv:2201.04358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.04358","description":"<p>Reference-based super-resolution (RefSR) has made significant progress in\nproducing realistic textures using an external reference (Ref) image. However,\nexisting RefSR methods obtain high-quality correspondence matchings consuming\nquadratic computation resources with respect to the input size, limiting its\napplication. Moreover, these approaches usually suffer from scale misalignments\nbetween the low-resolution (LR) image and Ref image. In this paper, we propose\nan Accelerated Multi-Scale Aggregation network (AMSA) for Reference-based\nSuper-Resolution, including Coarse-to-Fine Embedded PatchMatch (CFE-PatchMatch)\nand Multi-Scale Dynamic Aggregation (MSDA) module. To improve matching\nefficiency, we design a novel Embedded PatchMacth scheme with random samples\npropagation, which involves end-to-end training with asymptotic linear\ncomputational cost to the input size. To further reduce computational cost and\nspeed up convergence, we apply the coarse-to-fine strategy on Embedded\nPatchMacth constituting CFE-PatchMatch. To fully leverage reference information\nacross multiple scales and enhance robustness to scale misalignment, we develop\nthe MSDA module consisting of Dynamic Aggregation and Multi-Scale Aggregation.\nThe Dynamic Aggregation corrects minor scale misalignment by dynamically\naggregating features, and the Multi-Scale Aggregation brings robustness to\nlarge scale misalignment by fusing multi-scale information. Experimental\nresults show that the proposed AMSA achieves superior performance over\nstate-of-the-art approaches on both quantitative and qualitative evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Bin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yapeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hang_Y/0/1/0/all/0/1\">Yucheng Hang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1\">Qingmin Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Data-Driven STAP Radar. (arXiv:2201.10712v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10712","description":"<p>Using an amalgamation of techniques from classical radar, computer vision,\nand deep learning, we characterize our ongoing data-driven approach to\nspace-time adaptive processing (STAP) radar. We generate a rich example dataset\nof received radar signals by randomly placing targets of variable strengths in\na predetermined region using RFView, a site-specific radio frequency modeling\nand simulation tool developed by ISL Inc. For each data sample within this\nregion, we generate heatmap tensors in range, azimuth, and elevation of the\noutput power of a minimum variance distortionless response (MVDR) beamformer,\nwhich can be replaced with a desired test statistic. These heatmap tensors can\nbe thought of as stacked images, and in an airborne scenario, the moving radar\ncreates a sequence of these time-indexed image stacks, resembling a video. Our\ngoal is to use these images and videos to detect targets and estimate their\nlocations, a procedure reminiscent of computer vision algorithms for object\ndetection$-$namely, the Faster Region-Based Convolutional Neural Network\n(Faster R-CNN). The Faster R-CNN consists of a proposal generating network for\ndetermining regions of interest (ROI), a regression network for positioning\nanchor boxes around targets, and an object classification algorithm; it is\ndeveloped and optimized for natural images. Our ongoing research will develop\nanalogous tools for heatmap images of radar data. In this regard, we will\ngenerate a large, representative adaptive radar signal processing database for\ntraining and testing, analogous in spirit to the COCO dataset for natural\nimages. As a preliminary example, we present a regression network in this paper\nfor estimating target locations to demonstrate the feasibility of and\nsignificant improvements provided by our data-driven approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkatasubramanian_S/0/1/0/all/0/1\">Shyam Venkatasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wongkamthong_C/0/1/0/all/0/1\">Chayut Wongkamthong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1\">Mohammadreza Soltani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Bosung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gogineni_S/0/1/0/all/0/1\">Sandeep Gogineni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezeshki_A/0/1/0/all/0/1\">Ali Pezeshki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangaswamy_M/0/1/0/all/0/1\">Muralidhar Rangaswamy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1\">Vahid Tarokh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Access Control of Object Detection Models Using Encrypted Feature Maps. (arXiv:2202.00265v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00265","description":"<p>In this paper, we propose an access control method for object detection\nmodels. The use of encrypted images or encrypted feature maps has been\ndemonstrated to be effective in access control of models from unauthorized\naccess. However, the effectiveness of the approach has been confirmed in only\nimage classification models and semantic segmentation models, but not in object\ndetection models. In this paper, the use of encrypted feature maps is shown to\nbe effective in access control of object detection models for the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagamori_T/0/1/0/all/0/1\">Teru Nagamori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maung_A/0/1/0/all/0/1\">April Pyone Maung Maung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wukong: 100 Million Large-scale Chinese Cross-modal Pre-training Dataset and A Foundation Framework. (arXiv:2202.06767v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06767","description":"<p>Vision-Language Pre-training (VLP) models have shown remarkable performance\non various downstream tasks. Their success heavily relies on the scale of\npre-trained cross-modal datasets. However, the lack of large-scale datasets and\nbenchmarks in Chinese hinders the development of Chinese VLP models and broader\nmultilingual applications. In this work, we release a large-scale Chinese\ncross-modal dataset named Wukong, containing 100 million Chinese image-text\npairs from the web. Wukong aims to benchmark different multi-modal pre-training\nmethods to facilitate the VLP research and community development. Furthermore,\nwe release a group of models pre-trained with various image encoders\n(ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP\nsuch as locked-image text tuning, token-wise similarity in contrastive\nlearning, and reduced-token interaction. Extensive experiments and a deep\nbenchmarking of different downstream tasks are also provided. Experiments show\nthat Wukong can serve as a promising Chinese pre-training dataset and benchmark\nfor different cross-modal learning methods. For the zero-shot image\nclassification task on 10 datasets, our model achieves an average accuracy of\n73.03%. For the image-text retrieval task,our model achieves a mean recall of\n71.6% on AIC-ICC which is 12.9% higher than the result of WenLan 2.0. More\ninformation can refer to https://wukong-dataset.github.io/wukong-dataset/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiaxi Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xiaojun Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guansong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lu Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1\">Lewei Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Runhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When, Why, and Which Pretrained GANs Are Useful?. (arXiv:2202.08937v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.08937","description":"<p>The literature has proposed several methods to finetune pretrained GANs on\nnew datasets, which typically results in higher performance compared to\ntraining from scratch, especially in the limited-data regime. However, despite\nthe apparent empirical benefits of GAN pretraining, its inner mechanisms were\nnot analyzed in-depth, and understanding of its role is not entirely clear.\nMoreover, the essential practical details, e.g., selecting a proper pretrained\nGAN checkpoint, currently do not have rigorous grounding and are typically\ndetermined by trial and error.\n</p>\n<p>This work aims to dissect the process of GAN finetuning. First, we show that\ninitializing the GAN training process by a pretrained checkpoint primarily\naffects the model's coverage rather than the fidelity of individual samples.\nSecond, we explicitly describe how pretrained generators and discriminators\ncontribute to the finetuning process and explain the previous evidence on the\nimportance of pretraining both of them. Finally, as an immediate practical\nbenefit of our analysis, we describe a simple recipe to choose an appropriate\nGAN checkpoint that is the most suitable for finetuning to a particular target\ntask. Importantly, for most of the target tasks, Imagenet-pretrained GAN,\ndespite having poor visual quality, appears to be an excellent starting point\nfor finetuning, resembling the typical pretraining scenario of discriminative\ncomputer vision models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grigoryev_T/0/1/0/all/0/1\">Timofey Grigoryev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1\">Andrey Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Outlier-based Autism Detection using Longitudinal Structural MRI. (arXiv:2202.09988v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.09988","description":"<p>Diagnosis of Autism Spectrum Disorder (ASD) using clinical evaluation\n(cognitive tests) is challenging due to wide variations amongst individuals.\nSince no effective treatment exists, prompt and reliable ASD diagnosis can\nenable the effective preparation of treatment regimens. This paper proposes\nstructural Magnetic Resonance Imaging (sMRI)-based ASD diagnosis via an outlier\ndetection approach. To learn Spatio-temporal patterns in structural brain\nconnectivity, a Generative Adversarial Network (GAN) is trained exclusively\nwith sMRI scans of healthy subjects. Given a stack of three adjacent slices as\ninput, the GAN generator reconstructs the next three adjacent slices; the GAN\ndiscriminator then identifies ASD sMRI scan reconstructions as outliers. This\nmodel is compared against two other baselines -- a simpler UNet and a\nsophisticated Self-Attention GAN. Axial, Coronal, and Sagittal sMRI slices from\nthe multi-site ABIDE II dataset are used for evaluation. Extensive experiments\nreveal that our ASD detection framework performs comparably with the\nstate-of-the-art with far fewer training data. Furthermore, longitudinal data\n(two scans per subject over time) achieve 17-28% higher accuracy than\ncross-sectional data (one scan per subject). Among other findings, metrics\nemployed for model training as well as reconstruction loss computation impact\ndetection performance, and the coronal modality is found to best encode\nstructural information for ASD detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+K_D/0/1/0/all/0/1\">Devika K</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oruganti_V/0/1/0/all/0/1\">Venkata Ramana Murthy Oruganti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahapatra_D/0/1/0/all/0/1\">Dwarikanath Mahapatra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Subramanian_R/0/1/0/all/0/1\">Ramanathan Subramanian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern Based Multivariable Regression using Deep Learning (PBMR-DP). (arXiv:2202.13541v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.13541","description":"<p>We propose a deep learning methodology for multivariate regression that is\nbased on pattern recognition that triggers fast learning over sensor data. We\nused a conversion of sensors-to-image which enables us to take advantage of\nComputer Vision architectures and training processes. In addition to this data\npreparation methodology, we explore the use of state-of-the-art architectures\nto generate regression outputs to predict agricultural crop continuous yield\ninformation. Finally, we compare with some of the top models reported in\nMLCAS2021. We found that using a straightforward training process, we were able\nto accomplish an MAE of 4.394, RMSE of 5.945, and R^2 of 0.861.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jiztom Kavalakkatt Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_C/0/1/0/all/0/1\">Chandan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrera_Gerena_J/0/1/0/all/0/1\">Jansel Herrera-Gerena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Kundan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darr_M/0/1/0/all/0/1\">Matthew J Darr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Deep Learning for Image Classification with Distribution Mismatch: A Survey. (arXiv:2203.00190v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00190","description":"<p>Deep learning methodologies have been employed in several different fields,\nwith an outstanding success in image recognition applications, such as material\nquality control, medical imaging, autonomous driving, etc. Deep learning models\nrely on the abundance of labelled observations to train a prospective model.\nThese models are composed of millions of parameters to estimate, increasing the\nneed of more training observations. Frequently it is expensive to gather\nlabelled observations of data, making the usage of deep learning models not\nideal, as the model might over-fit data. In a semi-supervised setting,\nunlabelled data is used to improve the levels of accuracy and generalization of\na model with small labelled datasets. Nevertheless, in many situations\ndifferent unlabelled data sources might be available. This raises the risk of a\nsignificant distribution mismatch between the labelled and unlabelled datasets.\nSuch phenomena can cause a considerable performance hit to typical\nsemi-supervised deep learning frameworks, which often assume that both labelled\nand unlabelled datasets are drawn from similar distributions. Therefore, in\nthis paper we study the latest approaches for semi-supervised deep learning for\nimage recognition. Emphasis is made in semi-supervised deep learning models\ndesigned to deal with a distribution mismatch between the labelled and\nunlabelled datasets. We address open challenges with the aim to encourage the\ncommunity to tackle them, and overcome the high data demand of traditional deep\nlearning pipelines under real-world usage settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Calderon_Ramirez_S/0/1/0/all/0/1\">Saul Calderon-Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shengxiang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elizondo_D/0/1/0/all/0/1\">David Elizondo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recent Advances in Vision Transformer: A Survey and Outlook of Recent Work. (arXiv:2203.01536v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01536","description":"<p>Vision Transformers (ViTs) are becoming more popular and dominating technique\nfor various vision tasks, compare to Convolutional Neural Networks (CNNs). As a\ndemanding technique in computer vision, ViTs have been successfully solved\nvarious vision problems while focusing on long-range relationships. In this\npaper, we begin by introducing the fundamental concepts and background of the\nself-attention mechanism. Next, we provide a comprehensive overview of recent\ntop-performing ViT methods describing in terms of strength and weakness,\ncomputational cost as well as training and testing dataset. We thoroughly\ncompare the performance of various ViT algorithms and most representative CNN\nmethods on popular benchmark datasets. Finally, we explore some limitations\nwith insightful observations and provide further research direction. The\nproject page along with the collections of papers are available at\nhttps://github.com/khawar512/ViT-Survey\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_K/0/1/0/all/0/1\">Khawar Islam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Image Synthesis with Panoptic Layout Generation. (arXiv:2203.02104v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02104","description":"<p>Interactive image synthesis from user-guided input is a challenging task when\nusers wish to control the scene structure of a generated image with\nease.Although remarkable progress has been made on layout-based image synthesis\napproaches, in order to get realistic fake image in interactive scene, existing\nmethods require high-precision inputs, which probably need adjustment several\ntimes and are unfriendly to novice users. When placement of bounding boxes is\nsubject to perturbation, layout-based models suffer from \"missing regions\" in\nthe constructed semantic layouts and hence undesirable artifacts in the\ngenerated images. In this work, we propose Panoptic Layout Generative\nAdversarial Networks (PLGAN) to address this challenge. The PLGAN employs\npanoptic theory which distinguishes object categories between \"stuff\" with\namorphous boundaries and \"things\" with well-defined shapes, such that stuff and\ninstance layouts are constructed through separate branches and later fused into\npanoptic layouts. In particular, the stuff layouts can take amorphous shapes\nand fill up the missing regions left out by the instance layouts. We\nexperimentally compare our PLGAN with state-of-the-art layout-based models on\nthe COCO-Stuff, Visual Genome, and Landscape datasets. The advantages of PLGAN\nare not only visually demonstrated but quantitatively verified in terms of\ninception score, Fr\\'echet inception distance, classification accuracy score,\nand coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_P/0/1/0/all/0/1\">Peng Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperTransformer: A Textural and Spectral Feature Fusion Transformer for Pansharpening. (arXiv:2203.02503v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02503","description":"<p>Pansharpening aims to fuse a registered high-resolution panchromatic image\n(PAN) with a low-resolution hyperspectral image (LR-HSI) to generate an\nenhanced HSI with high spectral and spatial resolution. Existing pansharpening\napproaches neglect using an attention mechanism to transfer HR texture features\nfrom PAN to LR-HSI features, resulting in spatial and spectral distortions. In\nthis paper, we present a novel attention mechanism for pansharpening called\nHyperTransformer, in which features of LR-HSI and PAN are formulated as queries\nand keys in a transformer, respectively. HyperTransformer consists of three\nmain modules, namely two separate feature extractors for PAN and HSI, a\nmulti-head feature soft attention module, and a spatial-spectral feature fusion\nmodule. Such a network improves both spatial and spectral quality measures of\nthe pansharpened HSI by learning cross-feature space dependencies and\nlong-range details of PAN and LR-HSI. Furthermore, HyperTransformer can be\nutilized across multiple spatial scales at the backbone for obtaining improved\nperformance. Extensive experiments conducted on three widely used datasets\ndemonstrate that HyperTransformer achieves significant improvement over the\nstate-of-the-art methods on both spatial and spectral quality measures.\nImplementation code and pre-trained weights can be accessed at\nhttps://github.com/wgcban/HyperTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Temporal Action Localization via Representative Snippet Knowledge Propagation. (arXiv:2203.02925v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02925","description":"<p>Weakly supervised temporal action localization aims to localize temporal\nboundaries of actions and simultaneously identify their categories with only\nvideo-level category labels. Many existing methods seek to generate pseudo\nlabels for bridging the discrepancy between classification and localization,\nbut usually only make use of limited contextual information for pseudo label\ngeneration. To alleviate this problem, we propose a representative snippet\nsummarization and propagation framework. Our method seeks to mine the\nrepresentative snippets in each video for propagating information between video\nsnippets to generate better pseudo labels. For each video, its own\nrepresentative snippets and the representative snippets from a memory bank are\npropagated to update the input features in an intra- and inter-video manner.\nThe pseudo labels are generated from the temporal class activation maps of the\nupdated features to rectify the predictions of the main branch. Our method\nobtains superior performance in comparison to the existing methods on two\nbenchmarks, THUMOS14 and ActivityNet1.3, achieving gains as high as 1.2% in\nterms of average mAP on THUMOS14.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linjiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SingleSketch2Mesh : Generating 3D Mesh model from Sketch. (arXiv:2203.03157v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03157","description":"<p>Sketching is an important activity in any design process. Designers and\nstakeholders share their ideas through hand-drawn sketches. These sketches are\nfurther used to create 3D models. Current methods to generate 3D models from\nsketches are either manual or tightly coupled with 3D modeling platforms.\nTherefore, it requires users to have an experience of sketching on such\nplatform. Moreover, most of the existing approaches are based on geometric\nmanipulation and thus cannot be generalized. We propose a novel AI based\nensemble approach, SingleSketch2Mesh, for generating 3D models from hand-drawn\nsketches. Our approach is based on Generative Networks and Encoder-Decoder\nArchitecture to generate 3D mesh model from a hand-drawn sketch. We evaluate\nour solution with existing solutions. Our approach outperforms existing\napproaches on both - quantitative and qualitative evaluation criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhardwaj_N/0/1/0/all/0/1\">Nitish Bhardwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_D/0/1/0/all/0/1\">Dhornala Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Alpana Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Dual Trainable Bounds for Ultra-low Precision Super-Resolution Networks. (arXiv:2203.03844v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.03844","description":"<p>Light-weight super-resolution (SR) models have received considerable\nattention for their serviceability in mobile devices. Many efforts employ\nnetwork quantization to compress SR models. However, these methods suffer from\nsevere performance degradation when quantizing the SR models to ultra-low\nprecision (e.g., 2-bit and 3-bit) with the low-cost layer-wise quantizer. In\nthis paper, we identify that the performance drop comes from the contradiction\nbetween the layer-wise symmetric quantizer and the highly asymmetric activation\ndistribution in SR models. This discrepancy leads to either a waste on the\nquantization levels or detail loss in reconstructed images. Therefore, we\npropose a novel activation quantizer, referred to as Dynamic Dual Trainable\nBounds (DDTB), to accommodate the asymmetry of the activations. Specifically,\nDDTB innovates in: 1) A layer-wise quantizer with trainable upper and lower\nbounds to tackle the highly asymmetric activations. 2) A dynamic gate\ncontroller to adaptively adjust the upper and lower bounds at runtime to\novercome the drastically varying activation ranges over different samples.To\nreduce the extra overhead, the dynamic gate controller is quantized to 2-bit\nand applied to only part of the SR networks according to the introduced dynamic\nintensity. Extensive experiments demonstrate that our DDTB exhibits significant\nperformance improvements in ultra-low precision. For example, our DDTB achieves\na 0.70dB PSNR increase on Urban100 benchmark when quantizing EDSR to 2-bit and\nscaling up output images to x4. Code is at\n\\url{https://github.com/zysxmu/DDTB}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1\">Yunshan Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xunchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1\">Yunhang Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chao_F/0/1/0/all/0/1\">Fei Chao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Evaluation and Generation of Physical Adversarial Patch on Face Recognition. (arXiv:2203.04623v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04623","description":"<p>Recent studies have revealed the vulnerability of face recognition models\nagainst physical adversarial patches, which raises security concerns about the\ndeployed face recognition systems. However, it is still challenging to ensure\nthe reproducibility for most attack algorithms under complex physical\nconditions, which leads to the lack of a systematic evaluation of the existing\nmethods. It is therefore imperative to develop a framework that can enable a\ncomprehensive evaluation of the vulnerability of face recognition in the\nphysical world. To this end, we propose to simulate the complex transformations\nof faces in the physical world via 3D-face modeling, which serves as a digital\ncounterpart of physical faces. The generic framework allows us to control\ndifferent face variations and physical conditions to conduct reproducible\nevaluations comprehensively. With this digital simulator, we further propose a\nFace3DAdv method considering the 3D face transformations and realistic physical\nvariations. Extensive experiments validate that Face3DAdv can significantly\nimprove the effectiveness of diverse physically realizable adversarial patches\nin both simulated and physical environments, against various white-box and\nblack-box face recognition models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1\">Tianyu Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-DIAE: Degradation Invariant Autoencoders for Text Recognition and Document Enhancement. (arXiv:2203.04814v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04814","description":"<p>In this work, we propose Text-Degradation Invariant Auto Encoder (Text-DIAE)\naimed to solve two tasks, text recognition (handwritten or scene-text) and\ndocument image enhancement. We define three pretext tasks as learning\nobjectives to be optimized during pre-training without the usage of labelled\ndata. Each of the pre-text objectives is specifically tailored for the final\ndownstream tasks. We conduct several ablation experiments that show the\nimportance of each degradation for a specific domain. Exhaustive\nexperimentation shows that our method does not have limitations of previous\nstate-of-the-art based on contrastive losses while at the same time requiring\nessentially fewer data samples to converge. Finally, we demonstrate that our\nmethod surpasses the state-of-the-art significantly in existing supervised and\nself-supervised settings in handwritten and scene text recognition and document\nimage enhancement. Our code and trained models will be made publicly available\nat~\\url{ <a href=\"http://Upon_Acceptance\">this http URL</a>}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1\">Sanket Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mafla_A/0/1/0/all/0/1\">Andres Mafla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llados_J/0/1/0/all/0/1\">Josep Llad&#xf3;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_L/0/1/0/all/0/1\">Lluis Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1\">Dimosthenis Karatzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-10T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}