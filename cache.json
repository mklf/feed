{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-05-19T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Deploying self-supervised learning in the wild for hybrid automatic speech recognition. (arXiv:2205.08598v1 [cs.SD])","link":"http://arxiv.org/abs/2205.08598","description":"<p>Self-supervised learning (SSL) methods have proven to be very successful in\nautomatic speech recognition (ASR). These great improvements have been reported\nmostly based on highly curated datasets such as LibriSpeech for non-streaming\nEnd-to-End ASR models. However, the pivotal characteristics of SSL is to be\nutilized for any untranscribed audio data. In this paper, we provide a full\nexploration on how to utilize uncurated audio data in SSL from data\npre-processing to deploying an streaming hybrid ASR model. More specifically,\nwe present (1) the effect of Audio Event Detection (AED) model in data\npre-processing pipeline (2) analysis on choosing optimizer and learning rate\nscheduling (3) comparison of recently developed contrastive losses, (4)\ncomparison of various pre-training strategies such as utilization of in-domain\nversus out-domain pre-training data, monolingual versus multilingual\npre-training data, multi-head multilingual SSL versus single-head multilingual\nSSL and supervised pre-training versus SSL. The experimental results show that\nSSL pre-training with in-domain uncurated data can achieve better performance\nin comparison to all the alternative out-domain pre-training strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karimi_M/0/1/0/all/0/1\">Mostafa Karimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Changliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumatani_K/0/1/0/all/0/1\">Kenichi Kumatani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yao Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OneAligner: Zero-shot Cross-lingual Transfer with One Rich-Resource Language Pair for Low-Resource Sentence Retrieval. (arXiv:2205.08605v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08605","description":"<p>Aligning parallel sentences in multilingual corpora is essential to curating\ndata for downstream applications such as Machine Translation. In this work, we\npresent OneAligner, an alignment model specially designed for sentence\nretrieval tasks. This model is able to train on only one language pair and\ntransfers, in a cross-lingual fashion, to low-resource language pairs with\nnegligible degradation in performance. When trained with all language pairs of\na large-scale parallel multilingual corpus (OPUS-100), this model achieves the\nstate-of-the-art result on the Tateoba dataset, outperforming an equally-sized\nprevious model by 8.0 points in accuracy while using less than 0.6% of their\nparallel data. When finetuned on a single rich-resource language pair, be it\nEnglish-centered or not, our model is able to match the performance of the ones\nfinetuned on all language pairs under the same data budget with less than 2.0\npoints decrease in accuracy. Furthermore, with the same setup, scaling up the\nnumber of rich-resource language pairs monotonically improves the performance,\nreaching a minimum of 0.4 points discrepancy in accuracy, making it less\nmandatory to collect any low-resource parallel data. Finally, we conclude\nthrough empirical results and analyses that the performance of the sentence\nalignment task depends mostly on the monolingual and parallel data size, up to\na certain size threshold, rather than on what language pairs are used for\ntraining or evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geographical Distance Is The New Hyperparameter: A Case Study Of Finding The Optimal Pre-trained Language For English-isiZulu Machine Translation. (arXiv:2205.08621v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08621","description":"<p>Stemming from the limited availability of datasets and textual resources for\nlow-resource languages such as isiZulu, there is a significant need to be able\nto harness knowledge from pre-trained models to improve low resource machine\ntranslation. Moreover, a lack of techniques to handle the complexities of\nmorphologically rich languages has compounded the unequal development of\ntranslation models, with many widely spoken African languages being left\nbehind. This study explores the potential benefits of transfer learning in an\nEnglish-isiZulu translation framework. The results indicate the value of\ntransfer learning from closely related languages to enhance the performance of\nlow-resource translation models, thus providing a key strategy for low-resource\ntranslation going forward. We gathered results from 8 different language\ncorpora, including one multi-lingual corpus, and saw that isiXhosa-isiZulu\noutperformed all languages, with a BLEU score of 8.56 on the test set which was\nbetter from the multi-lingual corpora pre-trained model by 2.73. We also\nderived a new coefficient, Nasir's Geographical Distance Coefficient (NGDC)\nwhich provides an easy selection of languages for the pre-trained models. NGDC\nalso indicated that isiXhosa should be selected as the language for the\npre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1\">Muhammad Umair Nasir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mchechesi_I/0/1/0/all/0/1\">Innocent Amos Mchechesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic and Trend-aware Curriculum Learning for Relation Extraction in Graph Neural Networks. (arXiv:2205.08625v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08625","description":"<p>We present a generic and trend-aware curriculum learning approach for graph\nneural networks. It extends existing approaches by incorporating sample-level\nloss trends to better discriminate easier from harder samples and schedule them\nfor training. The model effectively integrates textual and structural\ninformation for relation extraction in text graphs. Experimental results show\nthat the model provides robust estimations of sample difficulty and shows\nsizable improvement over the state-of-the-art approaches across several\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vakil_N/0/1/0/all/0/1\">Nidhi Vakil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1\">Hadi Amiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Addressing Resource and Privacy Constraints in Semantic Parsing Through Data Augmentation. (arXiv:2205.08675v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08675","description":"<p>We introduce a novel setup for low-resource task-oriented semantic parsing\nwhich incorporates several constraints that may arise in real-world scenarios:\n(1) lack of similar datasets/models from a related domain, (2) inability to\nsample useful logical forms directly from a grammar, and (3) privacy\nrequirements for unlabeled natural utterances. Our goal is to improve a\nlow-resource semantic parser using utterances collected through user\ninteractions. In this highly challenging but realistic setting, we investigate\ndata augmentation approaches involving generating a set of structured canonical\nutterances corresponding to logical forms, before simulating corresponding\nnatural language and filtering the resulting pairs. We find that such\napproaches are effective despite our restrictive setup: in a low-resource\nsetting on the complex SMCalFlow calendaring dataset (Andreas et al., 2020), we\nobserve 33% relative improvement over a non-data-augmented baseline in top-1\nmatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kevin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_O/0/1/0/all/0/1\">Olivia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Charles Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_R/0/1/0/all/0/1\">Richard Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Subhro Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Solvability of Interpretability Evaluation Metrics. (arXiv:2205.08696v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08696","description":"<p>Feature attribution methods are popular for explaining neural network\npredictions, and they are often evaluated on metrics such as comprehensiveness\nand sufficiency, which are motivated by the principle that more important\nfeatures -- as judged by the explanation -- should have larger impacts on model\nprediction. In this paper, we highlight an intriguing property of these\nmetrics: their solvability. Concretely, we can define the problem of optimizing\nan explanation for a metric and solve it using beam search. This brings up the\nobvious question: given such solvability, why do we still develop other\nexplainers and then evaluate them on the metric? We present a series of\ninvestigations showing that this beam search explainer is generally comparable\nor favorable to current choices such as LIME and SHAP, suggest rethinking the\ngoals of model interpretability, and identify several directions towards better\nevaluations of new method proposals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yilun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation to Address Out-of-Vocabulary Problem in Low-Resource Sinhala-English Neural Machine Translation. (arXiv:2205.08722v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08722","description":"<p>Out-of-Vocabulary (OOV) is a problem for Neural Machine Translation (NMT).\nOOV refers to words with a low occurrence in the training data, or to those\nthat are absent from the training data. To alleviate this, word or phrase-based\nData Augmentation (DA) techniques have been used. However, existing DA\ntechniques have addressed only one of these OOV types and limit to considering\neither syntactic constraints or semantic constraints. We present a word and\nphrase replacement-based DA technique that consider both types of OOV, by\naugmenting (1) rare words in the existing parallel corpus, and (2) new words\nfrom a bilingual dictionary. During augmentation, we consider both syntactic\nand semantic properties of the words to guarantee fluency in the synthetic\nsentences. This technique was experimented with low resource Sinhala-English\nlanguage pair. We observe with only semantic constraints in the DA, the results\nare comparable with the scores obtained considering syntactic constraints, and\nis favourable for low-resourced languages that lacks linguistic tool support.\nAdditionally, results can be further improved by considering both syntactic and\nsemantic constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernando_A/0/1/0/all/0/1\">Aloka Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A reproducible experimental survey on biomedical sentence similarity: a string-based method sets the state of the art. (arXiv:2205.08740v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08740","description":"<p>This registered report introduces the largest, and for the first time,\nreproducible experimental survey on biomedical sentence similarity with the\nfollowing aims: (1) to elucidate the state of the art of the problem; (2) to\nsolve some reproducibility problems preventing the evaluation of most of\ncurrent methods; (3) to evaluate several unexplored sentence similarity\nmethods; (4) to evaluate an unexplored benchmark, called\nCorpus-Transcriptional-Regulation; (5) to carry out a study on the impact of\nthe pre-processing stages and Named Entity Recognition (NER) tools on the\nperformance of the sentence similarity methods; and finally, (6) to bridge the\nlack of reproducibility resources for methods and experiments in this line of\nresearch. Our experimental survey is based on a single software platform that\nis provided with a detailed reproducibility protocol and dataset as\nsupplementary material to allow the exact replication of all our experiments.\nIn addition, we introduce a new aggregated string-based sentence similarity\nmethod, called LiBlock, together with eight variants of current ontology-based\nmethods and a new pre-trained word embedding model trained on the full-text\narticles in the PMC-BioC corpus. Our experiments show that our novel\nstring-based measure sets the new state of the art on the sentence similarity\ntask in the biomedical domain and significantly outperforms all the methods\nevaluated herein, except one ontology-based method. Likewise, our experiments\nconfirm that the pre-processing stages, and the choice of the NER tool, have a\nsignificant impact on the performance of the sentence similarity methods. We\nalso detail some drawbacks and limitations of current methods, and warn on the\nneed of refining the current benchmarks. Finally, a noticeable finding is that\nour new string-based method significantly outperforms all state-of-the-art\nMachine Learning models evaluated herein.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lara_Clares_A/0/1/0/all/0/1\">Alicia Lara-Clares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lastra_Diaz_J/0/1/0/all/0/1\">Juan J. Lastra-D&#xed;az</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Serrano_A/0/1/0/all/0/1\">Ana Garcia-Serrano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Persian Natural Language Inference: A Meta-learning approach. (arXiv:2205.08755v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08755","description":"<p>Incorporating information from other languages can improve the results of\ntasks in low-resource languages. A powerful method of building functional\nnatural language processing systems for low-resource languages is to combine\nmultilingual pre-trained representations with cross-lingual transfer learning.\nIn general, however, shared representations are learned separately, either\nacross tasks or across languages. This paper proposes a meta-learning approach\nfor inferring natural language in Persian. Alternately, meta-learning uses\ndifferent task information (such as QA in Persian) or other language\ninformation (such as natural language inference in English). Also, we\ninvestigate the role of task augmentation strategy for forming additional\nhigh-quality tasks. We evaluate the proposed method using four languages and an\nauxiliary task. Compared to the baseline approach, the proposed model\nconsistently outperforms it, improving accuracy by roughly six percent. We also\nexamine the effect of finding appropriate initial parameters using zero-shot\nevaluation and CCA similarity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soudani_H/0/1/0/all/0/1\">Heydar Soudani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mojab_M/0/1/0/all/0/1\">Mohammad Hassan Mojab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beigy_H/0/1/0/all/0/1\">Hamid Beigy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction with Weighted Contrastive Pre-training on Distant Supervision. (arXiv:2205.08770v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08770","description":"<p>Contrastive pre-training on distant supervision has shown remarkable\neffectiveness for improving supervised relation extraction tasks. However, the\nexisting methods ignore the intrinsic noise of distant supervision during the\npre-training stage. In this paper, we propose a weighted contrastive learning\nmethod by leveraging the supervised data to estimate the reliability of\npre-training instances and explicitly reduce the effect of noise. Experimental\nresults on three supervised datasets demonstrate the advantages of our proposed\nweighted contrastive learning approach, compared to two state-of-the-art\nnon-weighted baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhen Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1\">Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qianying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhuoyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Haiyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurohashi_S/0/1/0/all/0/1\">Sadao Kurohashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Adaptive Semantic Transfer for Cross-domain Sentiment Classification. (arXiv:2205.08772v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08772","description":"<p>Cross-domain sentiment classification (CDSC) aims to use the transferable\nsemantics learned from the source domain to predict the sentiment of reviews in\nthe unlabeled target domain. Existing studies in this task attach more\nattention to the sequence modeling of sentences while largely ignoring the rich\ndomain-invariant semantics embedded in graph structures (i.e., the\npart-of-speech tags and dependency relations). As an important aspect of\nexploring characteristics of language comprehension, adaptive graph\nrepresentations have played an essential role in recent years. To this end, in\nthe paper, we aim to explore the possibility of learning invariant semantic\nfeatures from graph-like structures in CDSC. Specifically, we present Graph\nAdaptive Semantic Transfer (GAST) model, an adaptive syntactic graph embedding\nmethod that is able to learn domain-invariant semantics from both word\nsequences and syntactic graphs. More specifically, we first raise a\nPOS-Transformer module to extract sequential semantic features from the word\nsequences as well as the part-of-speech tags. Then, we design a Hybrid Graph\nAttention (HGAT) module to generate syntax-based semantic features by\nconsidering the transferable dependency relations. Finally, we devise an\nIntegrated aDaptive Strategy (IDS) to guide the joint learning process of both\nmodules. Extensive experiments on four public datasets indicate that GAST\nachieves comparable effectiveness to a range of state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhenya Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingyue Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengdi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LogiGAN: Learning Logical Reasoning via Adversarial Pre-training. (arXiv:2205.08794v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08794","description":"<p>We present LogiGAN, an unsupervised adversarial pre-training framework for\nimproving logical reasoning abilities of language models. Upon automatic\nidentifying logical reasoning phenomena in massive text corpus via detection\nheuristics, we train language models to predict the masked-out logical\nstatements. Inspired by the facilitation effect of reflective thinking in human\nlearning, we analogically simulate the learning-thinking process with an\nadversarial Generator-Verifier architecture to assist logic learning. LogiGAN\nimplements a novel sequential GAN approach that (a) circumvents the\nnon-differentiable challenge of the sequential GAN by leveraging the Generator\nas a sentence-level generative likelihood scorer with a learning objective of\nreaching scoring consensus with the Verifier; (b) is computationally feasible\nfor large-scale pre-training with arbitrary target length. Both base and large\nsize language models pre-trained with LogiGAN demonstrate obvious performance\nimprovement on 12 datasets requiring general reasoning abilities, revealing the\nfundamental role of logic in broad reasoning, as well as the effectiveness of\nLogiGAN. Ablation studies on LogiGAN components reveal the relative\northogonality between linguistic and logic abilities and suggest that\nreflective thinking's facilitation effect might also generalize to machine\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pi_X/0/1/0/all/0/1\">Xinyu Pi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1\">Wanjun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1\">Jian-Guang Lou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity Alignment with Reliable Path Reasoning and Relation-Aware Heterogeneous Graph Transformer. (arXiv:2205.08806v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08806","description":"<p>Entity Alignment (EA) has attracted widespread attention in both academia and\nindustry, which aims to seek entities with same meanings from different\nKnowledge Graphs (KGs). There are substantial multi-step relation paths between\nentities in KGs, indicating the semantic relations of entities. However,\nexisting methods rarely consider path information because not all natural paths\nfacilitate for EA judgment. In this paper, we propose a more effective entity\nalignment framework, RPR-RHGT, which integrates relation and path structure\ninformation, as well as the heterogeneous information in KGs. Impressively, an\ninitial reliable path reasoning algorithm is developed to generate the paths\nfavorable for EA task from the relation structures of KGs, which is the first\nalgorithm in the literature to successfully use unrestricted path information.\nIn addition, to efficiently capture heterogeneous features in entity\nneighborhoods, a relation-aware heterogeneous graph transformer is designed to\nmodel the relation and path structures of KGs. Extensive experiments on three\nwell-known datasets show RPR-RHGT significantly outperforms 11 state-of-the-art\nmethods, exceeding the best performing baseline up to 8.62% on Hits@1. We also\nshow its better performance than the baselines on different ratios of training\nset, and harder datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weishan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1\">Jieyu Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuncheng Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Transfer Learning for Polish with a Text-to-Text Model. (arXiv:2205.08808v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08808","description":"<p>We introduce a new benchmark for assessing the quality of text-to-text models\nfor Polish. The benchmark consists of diverse tasks and datasets: KLEJ\nbenchmark adapted for text-to-text, en-pl translation, summarization, and\nquestion answering. In particular, since summarization and question answering\nlack benchmark datasets for the Polish language, we describe their construction\nand make them publicly available. Additionally, we present plT5 - a\ngeneral-purpose text-to-text model for Polish that can be fine-tuned on various\nNatural Language Processing (NLP) tasks with a single training objective.\nUnsupervised denoising pre-training is performed efficiently by initializing\nthe model weights with a multi-lingual T5 (mT5) counterpart. We evaluate the\nperformance of plT5, mT5, Polish BART (plBART), and Polish GPT-2 (papuGaPT2).\nThe plT5 scores top on all of these tasks except summarization, where plBART is\nbest. In general (except for summarization), the larger the model, the better\nthe results. The encoder-decoder architectures prove to be better than the\ndecoder-only equivalent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrabrowa_A/0/1/0/all/0/1\">Aleksandra Chrabrowa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragan_L/0/1/0/all/0/1\">&#x141;ukasz Dragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorczyk_K/0/1/0/all/0/1\">Karol Grzegorczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajtoch_D/0/1/0/all/0/1\">Dariusz Kajtoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koszowski_M/0/1/0/all/0/1\">Miko&#x142;aj Koszowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mroczkowski_R/0/1/0/all/0/1\">Robert Mroczkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybak_P/0/1/0/all/0/1\">Piotr Rybak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regex in a Time of Deep Learning: The Role of an Old Technology in Age Discrimination Detection in Job Advertisements. (arXiv:2205.08813v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08813","description":"<p>Deep learning holds great promise for detecting discriminatory language in\nthe public sphere. However, for the detection of illegal age discrimination in\njob advertisements, regex approaches are still strong performers. In this\npaper, we investigate job advertisements in the Netherlands. We present a\nqualitative analysis of the benefits of the 'old' approach based on regexes and\ninvestigate how neural embeddings could address its limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pillar_A/0/1/0/all/0/1\">Anna Pillar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poelmans_K/0/1/0/all/0/1\">Kyrill Poelmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larson_M/0/1/0/all/0/1\">Martha Larson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting Social Media Content for Self-Supervised Style Transfer. (arXiv:2205.08814v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08814","description":"<p>Recent research on style transfer takes inspiration from unsupervised neural\nmachine translation (UNMT), learning from large amounts of non-parallel data by\nexploiting cycle consistency loss, back-translation, and denoising\nautoencoders. By contrast, the use of self-supervised NMT (SSNMT), which\nleverages (near) parallel instances hidden in non-parallel data more\nefficiently than UNMT, has not yet been explored for style transfer. In this\npaper we present a novel Self-Supervised Style Transfer (3ST) model, which\naugments SSNMT with UNMT methods in order to identify and efficiently exploit\nsupervisory signals in non-parallel social media posts. We compare 3ST with\nstate-of-the-art (SOTA) style transfer models across civil rephrasing,\nformality and polarity tasks. We show that 3ST is able to balance the three\nmajor objectives (fluency, content preservation, attribute transfer accuracy)\nthe best, outperforming SOTA models on averaged performance across their tested\ntasks in automatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruiter_D/0/1/0/all/0/1\">Dana Ruiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleinbauer_T/0/1/0/all/0/1\">Thomas Kleinbauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1\">Cristina Espa&#xf1;a-Bonet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPoeT-2: A GPT-2 Based Poem Generator. (arXiv:2205.08847v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08847","description":"<p>This project aims to produce the next volume of machine-generated poetry, a\ncomplex art form that can be structured and unstructured, and carries depth in\nthe meaning between the lines. GPoeT-2 is based on fine-tuning a state of the\nart natural language model (i.e. GPT-2) to generate limericks, typically\nhumorous structured poems consisting of five lines with a AABBA rhyming scheme.\nWith a two-stage generation system utilizing both forward and reverse language\nmodeling, GPoeT-2 is capable of freely generating limericks in diverse topics\nwhile following the rhyming structure without any seed phrase or a posteriori\nconstraints.Based on the automated generation process, we explore a wide\nvariety of evaluation metrics to quantify \"good poetry,\" including syntactical\ncorrectness, lexical diversity, and subject continuity. Finally, we present a\ncollection of 94 categorized limericks that rank highly on the explored \"good\npoetry\" metrics to provoke human creativity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kai-Ling Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariss_R/0/1/0/all/0/1\">Rami Ariss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_P/0/1/0/all/0/1\">Philipp Kurz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BFCAI at SemEval-2022 Task 6: Multi-Layer Perceptron for Sarcasm Detection in Arabic Texts. (arXiv:2205.08868v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08868","description":"<p>This paper describes the systems submitted to iSarcasm shared task. The aim\nof iSarcasm is to identify the sarcastic contents in Arabic and English text.\nOur team participated in iSarcasm for the Arabic language. A multi-Layer\nmachine learning based model has been submitted for Arabic sarcasm detection.\nIn this model, a vector space TF-IDF has been used as for feature\nrepresentation. The submitted system is simple and does not need any external\nresources. The test results show encouraging results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashraf_N/0/1/0/all/0/1\">Nsrin Ashraf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elkazaz_F/0/1/0/all/0/1\">Fathy Elkazaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taha_M/0/1/0/all/0/1\">Mohamed Taha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayel_H/0/1/0/all/0/1\">Hamada Nayel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elshishtawy_T/0/1/0/all/0/1\">Tarek Elshishtawy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scalable Workflow to Build Machine Learning Classifiers with Clinician-in-the-Loop to Identify Patients in Specific Diseases. (arXiv:2205.08891v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08891","description":"<p>Clinicians may rely on medical coding systems such as International\nClassification of Diseases (ICD) to identify patients with diseases from\nElectronic Health Records (EHRs). However, due to the lack of detail and\nspecificity as well as a probability of miscoding, recent studies suggest the\nICD codes often cannot characterise patients accurately for specific diseases\nin real clinical practice, and as a result, using them to find patients for\nstudies or trials can result in high failure rates and missing out on uncoded\npatients. Manual inspection of all patients at scale is not feasible as it is\nhighly costly and slow.\n</p>\n<p>This paper proposes a scalable workflow which leverages both structured data\nand unstructured textual notes from EHRs with techniques including NLP, AutoML\nand Clinician-in-the-Loop mechanism to build machine learning classifiers to\nidentify patients at scale with given diseases, especially those who might\ncurrently be miscoded or missed by ICD codes.\n</p>\n<p>Case studies in the MIMIC-III dataset were conducted where the proposed\nworkflow demonstrates a higher classification performance in terms of F1 scores\ncompared to simply using ICD codes on gold testing subset to identify patients\nwith Ovarian Cancer (0.901 vs 0.814), Lung Cancer (0.859 vs 0.828), Cancer\nCachexia (0.862 vs 0.650), and Lupus Nephritis (0.959 vs 0.855). Also, the\nproposed workflow that leverages unstructured notes consistently outperforms\nthe baseline that uses structured data only with an increase of F1 (Ovarian\nCancer 0.901 vs 0.719, Lung Cancer 0.859 vs 0.787, Cancer Cachexia 0.862 vs\n0.838 and Lupus Nephritis 0.959 vs 0.785). Experiments on the large testing set\nalso demonstrate the proposed workflow can find more patients who are miscoded\nor missed by ICD codes. Moreover, interpretability studies are also conducted\nto clinically validate the top impact features of the classifiers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Atri Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolanos_L/0/1/0/all/0/1\">Luis Bolanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanwar_A/0/1/0/all/0/1\">Ashwani Tanwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vibhor Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yike Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Features of Perceived Metaphoricity on the Discourse Level: Abstractness and Emotionality. (arXiv:2205.08939v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08939","description":"<p>Research on metaphorical language has shown ties between abstractness and\nemotionality with regard to metaphoricity; prior work is however limited to the\nword and sentence levels, and up to date there is no empirical study\nestablishing the extent to which this is also true on the discourse level. This\npaper explores which textual and perceptual features human annotators perceive\nas important for the metaphoricity of discourses and expressions, and addresses\ntwo research questions more specifically. First, is a metaphorically-perceived\ndiscourse more abstract and more emotional in comparison to a\nliterally-perceived discourse? Second, is a metaphorical expression preceded by\na more metaphorical/abstract/emotional context than a synonymous literal\nalternative? We used a dataset of 1,000 corpus-extracted discourses for which\ncrowdsourced annotators (1) provided judgements on whether they perceived the\ndiscourses as more metaphorical or more literal, and (2) systematically listed\nlexical terms which triggered their decisions in (1). Our results indicate that\nmetaphorical discourses are more emotional and to a certain extent more\nabstract than literal discourses. However, neither the metaphoricity nor the\nabstractness and emotionality of the preceding discourse seem to play a role in\ntriggering the choice between synonymous metaphorical vs. literal expressions.\nOur dataset is available at\nhttps://www.ims.uni-stuttgart.de/data/discourse-met-lit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Piccirilli_P/0/1/0/all/0/1\">Prisca Piccirilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1\">Sabine Schulte im Walde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREATER: CTR-driven Advertising Text Generation with Controlled Pre-Training and Contrastive Fine-Tuning. (arXiv:2205.08943v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08943","description":"<p>This paper focuses on automatically generating the text of an ad, and the\ngoal is that the generated text can capture user interest for achieving higher\nclick-through rate (CTR). We propose CREATER, a CTR-driven advertising text\ngeneration approach, to generate ad texts based on high-quality user reviews.\nTo incorporate CTR objective, our model learns from online A/B test data with\ncontrastive learning, which encourages the model to generate ad texts that\nobtain higher CTR. To alleviate the low-resource issue, we design a customized\nself-supervised objective reducing the gap between pre-training and\nfine-tuning. Experiments on industrial datasets show that CREATER significantly\noutperforms current approaches. It has been deployed online in a leading\nadvertising platform and brings uplift on core online metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_P/0/1/0/all/0/1\">Penghui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xuanhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaoguo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Pseudo-labeled Data to Improve Direct Speech-to-Speech Translation. (arXiv:2205.08993v1 [cs.CL])","link":"http://arxiv.org/abs/2205.08993","description":"<p>Direct Speech-to-speech translation (S2ST) has drawn more and more attention\nrecently. The task is very challenging due to data scarcity and complex\nspeech-to-speech mapping. In this paper, we report our recent achievements in\nS2ST. Firstly, we build a S2ST Transformer baseline which outperforms the\noriginal Translatotron. Secondly, we utilize the external data by\npseudo-labeling and obtain a new state-of-the-art result on the Fisher\nEnglish-to-Spanish test set. Indeed, we exploit the pseudo data with a\ncombination of popular techniques which are not trivial when applied to S2ST.\nMoreover, we evaluate our approach on both syntactically similar\n(Spanish-English) and distant (English-Chinese) language pairs. Our\nimplementation is available at\nhttps://github.com/fengpeng-yue/speech-to-speech-translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_F/0/1/0/all/0/1\">Fengpeng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_T/0/1/0/all/0/1\">Tom Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Qibing Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimising Biasing Word Errors for Contextual ASR with the Tree-Constrained Pointer Generator. (arXiv:2205.09058v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09058","description":"<p>Contextual knowledge is essential for reducing speech recognition errors on\nhigh-valued long-tail words. This paper proposes a novel tree-constrained\npointer generator (TCPGen) component that enables end-to-end ASR models to bias\ntowards a list of long-tail words obtained using external contextual\ninformation. With only a small overhead in memory use and computation cost,\nTCPGen can structure thousands of biasing words efficiently into a symbolic\nprefix-tree and creates a neural shortcut between the tree and the final ASR\noutput to facilitate the recognition of the biasing words. To enhance TCPGen,\nwe further propose a novel minimum biasing word error (MBWE) loss that directly\noptimises biasing word errors during training, along with a biasing-word-driven\nlanguage model discounting (BLMD) method during the test. All contextual ASR\nsystems were evaluated on the public Librispeech audiobook corpus and the data\nfrom the dialogue state tracking challenges (DSTC) with the biasing lists\nextracted from the dialogue-system ontology. Consistent word error rate (WER)\nreductions were achieved with TCPGen, which were particularly significant on\nthe biasing words with around 40\\% relative reductions in the recognition error\nrates. MBWE and BLMD further improved the effectiveness of TCPGen and achieved\nmore significant WER reductions on the biasing words. TCPGen also achieved\nzero-shot learning of words not in the audio training set with large WER\nreductions on the out-of-vocabulary words in the biasing list.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Induction for Efficient Semi-Supervised Learning. (arXiv:2205.09067v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09067","description":"<p>Semi-supervised learning has shown promise in allowing NLP models to\ngeneralize from small amounts of labeled data. Meanwhile, pretrained\ntransformer models act as black-box correlation engines that are difficult to\nexplain and sometimes behave unreliably. In this paper, we propose tackling\nboth of these challenges via Automatic Rule Induction (ARI), a simple and\ngeneral-purpose framework for the automatic discovery and integration of\nsymbolic rules into pretrained transformer models. First, we extract weak\nsymbolic rules from low-capacity machine learning models trained on small\namounts of labeled data. Next, we use an attention mechanism to integrate these\nrules into high-capacity pretrained transformer models. Last, the\nrule-augmented system becomes part of a self-training framework to boost\nsupervision signal on unlabeled data. These steps can be layered beneath a\nvariety of existing weak supervision and semi-supervised NLP algorithms in\norder to improve performance and interpretability. Experiments across nine\nsequence classification and relation extraction tasks suggest that ARI can\nimprove state-of-the-art methods with no manual effort and minimal\ncomputational overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialog Inpainting: Turning Documents into Dialogs. (arXiv:2205.09073v1 [cs.CL])","link":"http://arxiv.org/abs/2205.09073","description":"<p>Many important questions (e.g. \"How to eat healthier?\") require conversation\nto establish context and explore in depth. However, conversational question\nanswering (ConvQA) systems have long been stymied by scarce training data that\nis expensive to collect. To address this problem, we propose a new technique\nfor synthetically generating diverse and high-quality dialog data: dialog\ninpainting. Our approach takes the text of any document and transforms it into\na two-person dialog between the writer and an imagined reader: we treat\nsentences from the article as utterances spoken by the writer, and then use a\ndialog inpainter to predict what the imagined reader asked or said in between\neach of the writer's utterances. By applying this approach to passages from\nWikipedia and the web, we produce WikiDialog and WebDialog, two datasets\ntotalling 19 million diverse information-seeking dialogs -- 1,000x larger than\nthe largest existing ConvQA dataset. Furthermore, human raters judge the answer\nadequacy and conversationality of WikiDialog to be as good or better than\nexisting manually-collected datasets. Using our inpainted data to pre-train\nConvQA retrieval systems, we significantly advance state-of-the-art across\nthree benchmarks (QReCC, OR-QuAC, TREC CAsT) yielding up to 40% relative gains\non standard evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zhuyun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaganty_A/0/1/0/all/0/1\">Arun Tejasvi Chaganty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1\">Vincent Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Aida Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_Q/0/1/0/all/0/1\">Qazi Mamunur Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Green_M/0/1/0/all/0/1\">Mike Green</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guu_K/0/1/0/all/0/1\">Kelvin Guu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing the Transformer Decoder with Transition-based Syntax. (arXiv:2101.12640v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.12640","description":"<p>Notwithstanding recent advances, syntactic generalization remains a challenge\nfor text decoders. While some studies showed gains from incorporating\nsource-side symbolic syntactic and semantic structure into text generation\nTransformers, very little work addressed the decoding of such structure. We\npropose a general approach for tree decoding using a transition-based approach.\nExamining the challenging test case of incorporating Universal Dependencies\nsyntax into machine translation, we present substantial improvements on test\nsets that focus on syntactic generalization, while presenting improved or\ncomparable performance on standard MT benchmarks. Further qualitative analysis\naddresses cases where syntactic generalization in the vanilla Transformer\ndecoder is inadequate and demonstrates the advantages afforded by integrating\nsyntactic information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1\">Leshem Choshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1\">Omri Abend</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation. (arXiv:2103.06874v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.06874","description":"<p>Pipelined NLP systems have largely been superseded by end-to-end neural\nmodeling, yet nearly all commonly-used models still require an explicit\ntokenization step. While recent tokenization approaches based on data-derived\nsubword lexicons are less brittle than manually engineered tokenizers, these\ntechniques are not equally suited to all languages, and the use of any fixed\nvocabulary may limit a model's ability to adapt. In this paper, we present\nCANINE, a neural encoder that operates directly on character sequences, without\nexplicit tokenization or vocabulary, and a pre-training strategy that operates\neither directly on characters or optionally uses subwords as a soft inductive\nbias. To use its finer-grained input effectively and efficiently, CANINE\ncombines downsampling, which reduces the input sequence length, with a deep\ntransformer stack, which encodes context. CANINE outperforms a comparable mBERT\nmodel by 2.8 F1 on TyDi QA, a challenging multilingual benchmark, despite\nhaving 28% fewer model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1\">Jonathan H. Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garrette_D/0/1/0/all/0/1\">Dan Garrette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wieting_J/0/1/0/all/0/1\">John Wieting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Transformer-based End-to-end ASR using BERT. (arXiv:2104.04805v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04805","description":"<p>Transformer-based models have led to significant innovation in classical and\npractical subjects as varied as speech processing, natural language processing,\nand computer vision. On top of the Transformer, attention-based end-to-end\nautomatic speech recognition (ASR) models have recently become popular.\nSpecifically, non-autoregressive modeling, which boasts fast inference and\nperformance comparable to conventional autoregressive methods, is an emerging\nresearch topic. In the context of natural language processing, the\nbidirectional encoder representations from Transformers (BERT) model has\nreceived widespread attention, partially due to its ability to infer\ncontextualized word representations and to enable superior performance for\ndownstream tasks while needing only simple fine-tuning. Motivated by the\nsuccess, we intend to view speech recognition as a downstream task of BERT,\nthus an ASR system is expected to be deduced by performing fine-tuning.\nConsequently, to not only inherit the advantages of non-autoregressive ASR\nmodels but also enjoy the benefits of a pre-trained language model (e.g.,\nBERT), we propose a non-autoregressive Transformer-based end-to-end ASR model\nbased on BERT. We conduct a series of experiments on the AISHELL-1 dataset that\ndemonstrate competitive or superior results for the model when compared to\nstate-of-the-art ASR systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fu-Hao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kuan-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08821","description":"<p>This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation, and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework by using \"entailment\" pairs as\npositives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nthe previous best results. We also show -- both theoretically and empirically\n-- that the contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Word Segmentation from Discrete Speech Units in Low-Resource Settings. (arXiv:2106.04298v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04298","description":"<p>Documenting languages helps to prevent the extinction of endangered dialects,\nmany of which are otherwise expected to disappear by the end of the century.\nWhen documenting oral languages, unsupervised word segmentation (UWS) from\nspeech is a useful, yet challenging, task. It consists in producing time-stamps\nfor slicing utterances into smaller segments corresponding to words, being\nperformed from phonetic transcriptions, or in the absence of these, from the\noutput of unsupervised speech discretization models. These discretization\nmodels are trained using raw speech only, producing discrete speech units that\ncan be applied for downstream (text-based) tasks. In this paper we compare five\nof these models: three Bayesian and two neural approaches, with regards to the\nexploitability of the produced units for UWS. For the UWS task, we experiment\nwith two models, using as our target language the Mboshi (Bantu C25), an\nunwritten language from Congo-Brazzaville. Additionally, we report results for\nFinnish, Hungarian, Romanian and Russian in equally low-resource settings,\nusing only 4 hours of speech. Our results suggest that neural models for speech\ndiscretization are difficult to exploit in our setting, and that it might be\nnecessary to adapt them to limit sequence length. We obtain our best UWS\nresults by using Bayesian models that produce high quality, yet compressed,\ndiscrete representations of the input speech signal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boito_M/0/1/0/all/0/1\">Marcely Zanon Boito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yusuf_B/0/1/0/all/0/1\">Bolaji Yusuf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondel_L/0/1/0/all/0/1\">Lucas Ondel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villavicencio_A/0/1/0/all/0/1\">Aline Villavicencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1\">Laurent Besacier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translatotron 2: High-quality direct speech-to-speech translation with voice preservation. (arXiv:2107.08661v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.08661","description":"<p>We present Translatotron 2, a neural direct speech-to-speech translation\nmodel that can be trained end-to-end. Translatotron 2 consists of a speech\nencoder, a linguistic decoder, an acoustic synthesizer, and a single attention\nmodule that connects them together. Experimental results on three datasets\nconsistently show that Translatotron 2 outperforms the original Translatotron\nby a large margin on both translation quality (up to +15.5 BLEU) and speech\ngeneration quality, and approaches the same of cascade systems. In addition, we\npropose a simple method for preserving speakers' voices from the source speech\nto the translation speech in a different language. Unlike existing approaches,\nthe proposed method is able to preserve each speaker's voice on speaker turns\nwithout requiring for speaker segmentation. Furthermore, compared to existing\napproaches, it better preserves speaker's privacy and mitigates potential\nmisuse of voice cloning for creating spoofing audio artifacts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanovich_M/0/1/0/all/0/1\">Michelle Tadmor Ramanovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Remez_T/0/1/0/all/0/1\">Tal Remez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pomerantz_R/0/1/0/all/0/1\">Roi Pomerantz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making Document-Level Information Extraction Right for the Right Reasons. (arXiv:2110.07686v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07686","description":"<p>Document-level models for information extraction tasks like slot-filling are\nflexible: they can be applied to settings where information is not necessarily\nlocalized in a single sentence. For example, key features of a diagnosis in a\nradiology report may not be explicitly stated in one place, but nevertheless\ncan be inferred from parts of the report's text. However, these models can\neasily learn spurious correlations between labels and irrelevant information.\nThis work studies how to ensure that these models make correct inferences from\ncomplex text and make those inferences in an auditable way: beyond just being\nright, are these models \"right for the right reasons?\" We experiment with\npost-hoc evidence extraction in a predict-select-verify framework using feature\nattribution techniques. We show that regularization with small amounts of\nevidence supervision during training can substantially improve the quality of\nextracted evidence. We evaluate on two domains: a small-scale labeled dataset\nof brain MRI reports and a large-scale modified version of DocRED (Yao et al.,\n2019) and show that models' plausibility can be improved with no loss in\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Liyan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_D/0/1/0/all/0/1\">Dhruv Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Suyash Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pradhan_A/0/1/0/all/0/1\">Abhijeet Pradhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bryan_R/0/1/0/all/0/1\">R. Nick Bryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00656","description":"<p>Recently, by introducing large-scale dataset and strong transformer network,\nvideo-language pre-training has shown great success especially for retrieval.\nYet, existing video-language transformer models do not explicitly fine-grained\nsemantic align. In this work, we present Object-aware Transformers, an\nobject-centric approach that extends video-language transformer to incorporate\nobject representations. The key idea is to leverage the bounding boxes and\nobject tags to guide the training process. We evaluate our model on three\nstandard sub-tasks of video-text matching on four widely used benchmarks. We\nalso provide deep analysis and detailed ablation about the proposed method. We\nshow clear improvement in performance across all tasks and datasets considered,\ndemonstrating the value of a model that incorporates object representations\ninto a video-language architecture. The code will be released at\n\\url{https://github.com/FingerRec/OA-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Robustness of a BiLSTM-based Structural Story Classifier. (arXiv:2201.02733v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.02733","description":"<p>The growing prevalence of counterfeit stories on the internet has fostered\nsignificant interest towards fast and scalable detection of fake news in the\nmachine learning community. While several machine learning techniques for this\npurpose have emerged, we observe that there is a need to evaluate the impact of\nnoise on these techniques' performance, where noise constitutes news articles\nbeing mistakenly labeled as fake (or real). This work takes a step in that\ndirection, where we examine the impact of noise on a state-of-the-art,\nstructural model based on BiLSTM (Bidirectional Long-Short Term Model) for fake\nnews detection, Hierarchical Discourse-level Structure for Fake News Detection\nby Karimi and Tang (Reference no. 9).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Aftab Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nanduri_S/0/1/0/all/0/1\">Sai Durga Prasad Nanduri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seenuvasavarathan_S/0/1/0/all/0/1\">Sneha Seenuvasavarathan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Describing Differences between Text Distributions with Natural Language. (arXiv:2201.12323v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12323","description":"<p>How do two distributions of texts differ? Humans are slow at answering this,\nsince discovering patterns might require tediously reading through hundreds of\nsamples. We propose to automatically summarize the differences by \"learning a\nnatural language hypothesis\": given two distributions $D_{0}$ and $D_{1}$, we\nsearch for a description that is more often true for $D_{1}$, e.g., \"is\nmilitary-related.\" To tackle this problem, we fine-tune GPT-3 to propose\ndescriptions with the prompt: \"[samples of $D_{0}$] + [samples of $D_{1}$] +\nthe difference between them is_____.\" We then re-rank the descriptions by\nchecking how often they hold on a larger set of samples with a learned\nverifier. On a benchmark of 54 real-world binary classification tasks, while\nGPT-3 Curie (13B) only generates a description similar to human annotation 7%\nof the time, the performance reaches 61% with fine-tuning and re-ranking, and\nour best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to\ndescribe distribution shifts, debug dataset shortcuts, summarize unknown tasks,\nand label text clusters, and present analyses based on automatically generated\ndescriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Pretrained Models of Source Code. (arXiv:2202.08975v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2202.08975","description":"<p>Deep learning models are widely used for solving challenging code processing\ntasks, such as code generation or code summarization. Traditionally, a specific\nmodel architecture was carefully built to solve a particular code processing\ntask. However, recently general pretrained models such as CodeBERT or CodeT5\nhave been shown to outperform task-specific models in many applications. While\npretrained models are known to learn complex patterns from data, they may fail\nto understand some properties of source code. To test diverse aspects of code\nunderstanding, we introduce a set of diagnosting probing tasks. We show that\npretrained models of code indeed contain information about code syntactic\nstructure and correctness, the notions of identifiers, data flow and\nnamespaces, and natural language naming. We also investigate how probing\nresults are affected by using code-specific pretraining objectives, varying the\nmodel size, or finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Troshin_S/0/1/0/all/0/1\">Sergey Troshin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trigger-GNN: A Trigger-Based Graph Neural Network for Nested Named Entity Recognition. (arXiv:2204.05518v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05518","description":"<p>Nested named entity recognition (NER) aims to identify the entity boundaries\nand recognize categories of the named entities in a complex hierarchical\nsentence. Some works have been done using character-level, word-level, or\nlexicon-level based models. However, such researches ignore the role of the\ncomplementary annotations. In this paper, we propose a trigger-based graph\nneural network (Trigger-GNN) to leverage the nested NER. It obtains the\ncomplementary annotation embeddings through entity trigger encoding and\nsemantic matching, and tackle nested entity utilizing an efficient graph\nmessage passing architecture, aggregation-update mode. We posit that using\nentity triggers as external annotations can add in complementary supervision\nsignals on the whole sentences. It helps the model to learn and generalize more\nefficiently and cost-effectively. Experiments show that the Trigger-GNN\nconsistently outperforms the baselines on four public NER datasets, and it can\neffectively alleviate the nested NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1\">Yuan Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_F/0/1/0/all/0/1\">Fanyang Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yingting Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling. (arXiv:2204.08152v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08152","description":"<p>Multi-turn dialogue modeling as a challenging branch of natural language\nunderstanding (NLU), aims to build representations for machines to understand\nhuman dialogues, which provides a solid foundation for multiple downstream\ntasks. Recent studies of dialogue modeling commonly employ pre-trained language\nmodels (PrLMs) to encode the dialogue history as successive tokens, which is\ninsufficient in capturing the temporal characteristics of dialogues. Therefore,\nwe propose Bidirectional Information Decoupling Network (BiDeN) as a universal\ndialogue encoder, which explicitly incorporates both the past and future\ncontexts and can be generalized to a wide range of dialogue-related tasks.\nExperimental results on datasets of different downstream tasks demonstrate the\nuniversality and effectiveness of our BiDeN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Parallelize in a Shared-Memory Environment with Transformers. (arXiv:2204.12835v2 [cs.DC] UPDATED)","link":"http://arxiv.org/abs/2204.12835","description":"<p>In past years, the world has switched to many-core and multi-core shared\nmemory architectures. As a result, there is a growing need to utilize these\narchitectures by introducing shared memory parallelization schemes to software\napplications. OpenMP is the most comprehensive API that implements such\nschemes, characterized by a readable interface. Nevertheless, introducing\nOpenMP into code is challenging due to pervasive pitfalls in management of\nparallel shared memory. To facilitate the performance of this task, many\nsource-to-source (S2S) compilers have been created over the years, tasked with\ninserting OpenMP directives into code automatically. In addition to having\nlimited robustness to their input format, these compilers still do not achieve\nsatisfactory coverage and precision in locating parallelizable code and\ngenerating appropriate directives. In this work, we propose leveraging recent\nadvances in ML techniques, specifically in natural language processing (NLP),\nto replace S2S compilers altogether. We create a database (corpus), Open-OMP,\nspecifically for this goal. Open-OMP contains over 28,000 code snippets, half\nof which contain OpenMP directives while the other half do not need\nparallelization at all with high probability. We use the corpus to train\nsystems to automatically classify code segments in need of parallelization, as\nwell as suggest individual OpenMP clauses. We train several transformer models,\nnamed PragFormer, for these tasks, and show that they outperform\nstatistically-trained baselines and automatic S2S parallelization compilers in\nboth classifying the overall need for an OpenMP directive and the introduction\nof private and reduction clauses.\n</p>\n<p>Our source code and database are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/PragFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harel_R/0/1/0/all/0/1\">Re&#x27;em Harel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supplementary Material: Implementation and Experiments for GAU-based Model. (arXiv:2205.05842v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.05842","description":"<p>In February this year Google proposed a new Transformer variant called FLASH,\nwhich has a faster speed, lower VRAM footprint and better performance. This is\nachieved by designing a performant layer named GAU (Gated Attention Unit),\nwhich combines the Attention layer and FFN. In this paper, some implementation\ndetails are re-analyzed both theoretically and practically. We then propose a\nnovel GAU-based model and pre-train it on a Chinese corpus. Results of the CLUE\nbenchmark show that our model achieves a dev average score of 75.02, 1% higher\nthan RoFormerV1 and being 45% faster, which is also competitive with\nRoFormerV2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenjie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Hate Speech Data along Racial, Gender and Intersectional Axes. (arXiv:2205.06621v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.06621","description":"<p>To tackle the rising phenomenon of hate speech, efforts have been made\ntowards data curation and analysis. When it comes to analysis of bias, previous\nwork has focused predominantly on race. In our work, we further investigate\nbias in hate speech datasets along racial, gender and intersectional axes. We\nidentify strong bias against African American English (AAE), masculine and\nAAE+Masculine tweets, which are annotated as disproportionately more hateful\nand offensive than from other demographics. We provide evidence that BERT-based\nmodels propagate this bias and show that balancing the training data for these\nprotected attributes can lead to fairer models with regards to gender, but not\nrace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maronikolakis_A/0/1/0/all/0/1\">Antonis Maronikolakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_P/0/1/0/all/0/1\">Philip Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"What makes a question inquisitive?\" A Study on Type-Controlled Inquisitive Question Generation. (arXiv:2205.08056v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.08056","description":"<p>We propose a type-controlled framework for inquisitive question generation.\nWe annotate an inquisitive question dataset with question types, train question\ntype classifiers, and finetune models for type-controlled question generation.\nEmpirical results demonstrate that we can generate a variety of questions that\nadhere to specific types while drawing from the source texts. We also\ninvestigate strategies for selecting a single question from a generated set,\nconsidering both an informative vs.~inquisitive question classifier and a\npairwise ranker trained from a small set of expert annotations. Question\nselection using the pairwise ranker yields strong results in automatic and\nmanual evaluation. Our human evaluation assesses multiple aspects of the\ngenerated questions, finding that the ranker chooses questions with the best\nsyntax (4.59), semantics (4.37), and inquisitiveness (3.92) on a scale of 1-5,\neven rivaling the performance of human-written questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lingyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1\">Debanjan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1\">Kevin Gimpel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Text Detection & Recognition in the Wild for Robot Localization. (arXiv:2205.08565v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08565","description":"<p>Signage is everywhere and a robot should be able to take advantage of signs\nto help it localize (including Visual Place Recognition (VPR)) and map. Robust\ntext detection &amp; recognition in the wild is challenging due to such factors as\npose, irregular text, illumination, and occlusion. We propose an end-to-end\nscene text spotting model that simultaneously outputs the text string and\nbounding boxes. This model is more suitable for VPR. Our central contribution\nis introducing utilizing an end-to-end scene text spotting framework to\nadequately capture the irregular and occluded text regions in different\nchallenging places. To evaluate our proposed architecture's performance for\nVPR, we conducted several experiments on the challenging Self-Collected Text\nPlace (SCTP) benchmark dataset. The initial experimental results show that the\nproposed method outperforms the SOTA methods in terms of precision and recall\nwhen tested on this benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raisi_Z/0/1/0/all/0/1\">Zobeir Raisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelek_J/0/1/0/all/0/1\">John Zelek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Efficient Self-Supervised Federated Learning for Tackling Data Heterogeneity in Medical Imaging. (arXiv:2205.08576v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08576","description":"<p>The curation of large-scale medical datasets from multiple institutions\nnecessary for training deep learning models is challenged by the difficulty in\nsharing patient data with privacy-preserving. Federated learning (FL), a\nparadigm that enables privacy-protected collaborative learning among different\ninstitutions, is a promising solution to this challenge. However, FL generally\nsuffers from performance deterioration due to heterogeneous data distributions\nacross institutions and the lack of quality labeled data. In this paper, we\npresent a robust and label-efficient self-supervised FL framework for medical\nimage analysis. Specifically, we introduce a novel distributed self-supervised\npre-training paradigm into the existing FL pipeline (i.e., pre-training the\nmodels directly on the decentralized target task datasets). Built upon the\nrecent success of Vision Transformers, we employ masked image encoding tasks\nfor self-supervised pre-training, to facilitate more effective knowledge\ntransfer to downstream federated models. Extensive empirical results on\nsimulated and real-world medical imaging federated datasets show that\nself-supervised pre-training largely benefits the robustness of federated\nmodels against various degrees of data heterogeneity. Notably, under severe\ndata heterogeneity, our method, without relying on any additional pre-training\ndata, achieves an improvement of 5.06%, 1.53% and 4.58% in test accuracy on\nretinal, dermatology and chest X-ray classification compared with the\nsupervised baseline with ImageNet pre-training. Moreover, we show that our\nself-supervised FL algorithm generalizes well to out-of-distribution data and\nlearns federated models more effectively in limited label scenarios, surpassing\nthe supervised baseline by 10.36% and the semi-supervised FL method by 8.3% in\ntest accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1\">Qingyue Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Liyue Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Lei Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CV4Code: Sourcecode Understanding via Visual Code Representations. (arXiv:2205.08585v1 [cs.SE])","link":"http://arxiv.org/abs/2205.08585","description":"<p>We present CV4Code, a compact and effective computer vision method for\nsourcecode understanding. Our method leverages the contextual and the\nstructural information available from the code snippet by treating each snippet\nas a two-dimensional image, which naturally encodes the context and retains the\nunderlying structural information through an explicit spatial representation.\nTo codify snippets as images, we propose an ASCII codepoint-based image\nrepresentation that facilitates fast generation of sourcecode images and\neliminates redundancy in the encoding that would arise from an RGB pixel\nrepresentation. Furthermore, as sourcecode is treated as images, neither\nlexical analysis (tokenisation) nor syntax tree parsing is required, which\nmakes the proposed method agnostic to any particular programming language and\nlightweight from the application pipeline point of view. CV4Code can even\nfeaturise syntactically incorrect code which is not possible from methods that\ndepend on the Abstract Syntax Tree (AST). We demonstrate the effectiveness of\nCV4Code by learning Convolutional and Transformer networks to predict the\nfunctional task, i.e. the problem it solves, of the source code directly from\nits two-dimensional representation, and using an embedding from its latent\nspace to derive a similarity score of two code snippets in a retrieval setup.\nExperimental results show that our approach achieves state-of-the-art\nperformance in comparison to other methods with the same task and data\nconfigurations. For the first time we show the benefits of treating sourcecode\nunderstanding as a form of image processing task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1\">Ruibo Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1\">Lili Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphal_R/0/1/0/all/0/1\">Rohan Saphal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silavong_F/0/1/0/all/0/1\">Fran Silavong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moran_S/0/1/0/all/0/1\">Sean J. Moran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RARITYNet: Rarity Guided Affective Emotion Learning Framework. (arXiv:2205.08595v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08595","description":"<p>Inspired from the assets of handcrafted and deep learning approaches, we\nproposed a RARITYNet: RARITY guided affective emotion learning framework to\nlearn the appearance features and identify the emotion class of facial\nexpressions. The RARITYNet framework is designed by combining the shallow\n(RARITY) and deep (AffEmoNet) features to recognize the facial expressions from\nchallenging images as spontaneous expressions, pose variations, ethnicity\nchanges, and illumination conditions. The RARITY is proposed to encode the\ninter-radial transitional patterns in the local neighbourhood. The AffEmoNet:\naffective emotion learning network is proposed by incorporating three feature\nstreams: high boost edge filtering (HBSEF) stream, to extract the edge\ninformation of highly affected facial expressive regions, multi-scale\nsophisticated edge cumulative (MSSEC) stream is to learns the sophisticated\nedge information from multi-receptive fields and RARITY uplift complementary\ncontext feature (RUCCF) stream refines the RARITY-encoded features and aid the\nMSSEC stream features to enrich the learning ability of RARITYNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_M/0/1/0/all/0/1\">Monu Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vipparthi_S/0/1/0/all/0/1\">Santosh Kumar Vipparthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust Low Light Image Enhancement. (arXiv:2205.08615v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08615","description":"<p>In this paper, we study the problem of making brighter images from dark\nimages found in the wild. The images are dark because they are taken in dim\nenvironments. They suffer from color shifts caused by quantization and from\nsensor noise. We don't know the true camera reponse function for such images\nand they are not RAW. We use a supervised learning method, relying on a\nstraightforward simulation of an imaging pipeline to generate usable dataset\nfor training and testing. On a number of standard datasets, our approach\noutperforms the state of the art quantitatively. Qualitative comparisons\nsuggest strong improvements in reconstruction accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aghajanzadeh_S/0/1/0/all/0/1\">Sara Aghajanzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Accurate Super-Resolution Generative Adversarial Networks. (arXiv:2205.08659v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08659","description":"<p>This work addresses the problems of semantic segmentation and image\nsuper-resolution by jointly considering the performance of both in training a\nGenerative Adversarial Network (GAN). We propose a novel architecture and\ndomain-specific feature loss, allowing super-resolution to operate as a\npre-processing step to increase the performance of downstream computer vision\ntasks, specifically semantic segmentation. We demonstrate this approach using\nNearmap's aerial imagery dataset which covers hundreds of urban areas at 5-7 cm\nper pixel resolution. We show the proposed approach improves perceived image\nquality as well as quantitative segmentation accuracy across all prediction\nclasses, yielding an average accuracy improvement of 11.8% and 108% at 4x and\n32x super-resolution, compared with state-of-the art single-network methods.\nThis work demonstrates that jointly considering image-based and task-specific\nlosses can improve the performance of both, and advances the state-of-the-art\nin semantic-aware super-resolution of aerial imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frizza_T/0/1/0/all/0/1\">Tristan Frizza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dansereau_D/0/1/0/all/0/1\">Donald G. Dansereau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seresht_N/0/1/0/all/0/1\">Nagita Mehr Seresht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bewley_M/0/1/0/all/0/1\">Michael Bewley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Monocular Depth Estimation via Selective Distillation of Stereo Knowledge. (arXiv:2205.08668v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08668","description":"<p>Monocular depth estimation has been extensively explored based on deep\nlearning, yet its accuracy and generalization ability still lag far behind the\nstereo-based methods. To tackle this, a few recent studies have proposed to\nsupervise the monocular depth estimation network by distilling disparity maps\nas proxy ground-truths. However, these studies naively distill the stereo\nknowledge without considering the comparative advantages of stereo-based and\nmonocular depth estimation methods. In this paper, we propose to selectively\ndistill the disparity maps for more reliable proxy supervision. Specifically,\nwe first design a decoder (MaskDecoder) that learns two binary masks which are\ntrained to choose optimally between the proxy disparity maps and the estimated\ndepth maps for each pixel. The learned masks are then fed to another decoder\n(DepthDecoder) to enforce the estimated depths to learn from only the masked\narea in the proxy disparity maps. Additionally, a Teacher-Student module is\ndesigned to transfer the geometric knowledge of the StereoNet to the MonoNet.\nExtensive experiments validate our methods achieve state-of-the-art performance\nfor self- and proxy-supervised monocular depth estimation on the KITTI dataset,\neven surpassing some of the semi-supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kyeongseob Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"K-textures, a self supervised hard clustering deep learning algorithm for satellite images segmentation. (arXiv:2205.08671v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08671","description":"<p>Deep learning self supervised algorithms that can segment an image in a fixed\nnumber of hard labels such as the k-means algorithm and only relying only on\ndeep learning techniques are still lacking. Here, we introduce the k-textures\nalgorithm which provides self supervised segmentation of a 4-band image\n(RGB-NIR) for a $k$ number of classes. An example of its application on high\nresolution Planet satellite imagery is given. Our algorithm shows that discrete\nsearch is feasible using convolutional neural networks (CNN) and gradient\ndescent. The model detects $k$ hard clustering classes represented in the model\nas $k$ discrete binary masks and their associated $k$ independently generated\ntextures, that combined are a simulation of the original image. The similarity\nloss is the mean squared error between the features of the original and the\nsimulated image, both extracted from the penultimate convolutional block of\nKeras 'imagenet' pretrained VGG-16 model and a custom feature extractor made\nwith Planet data. The main advances of the k-textures model are: first, the $k$\ndiscrete binary masks are obtained inside the model using gradient descent. The\nmodel allows for the generation of discrete binary masks using a novel method\nusing a hard sigmoid activation function. Second, it provides hard clustering\nclasses -- each pixels has only one class. Finally, in comparison to k-means,\nwhere each pixel is considered independently, here, contextual information is\nalso considered and each class is not associated only to a similar values in\nthe color channels but to a texture. Our approach is designed to ease the\nproduction of training samples for satellite image segmentation. The model\ncodes and weights are available at https://doi.org/10.5281/zenodo.6359859\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wagner_F/0/1/0/all/0/1\">Fabien H. Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalagnol_R/0/1/0/all/0/1\">Ricardo Dalagnol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1\">Alber H. S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirye_M/0/1/0/all/0/1\">Mayumi C.M. Hirye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Favrichon_S/0/1/0/all/0/1\">Samuel Favrichon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jake H. Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauceri_S/0/1/0/all/0/1\">Steffen Mauceri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saatchi_S/0/1/0/all/0/1\">Sassan Saatchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning on rail profiles matching. (arXiv:2205.08687v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08687","description":"<p>Matching the rail cross-section profiles measured on site with the designed\nprofile is a must to evaluate the wear of the rail, which is very important for\ntrack maintenance and rail safety. So far, the measured rail profiles to be\nmatched usually have four features, that is, large amount of data, diverse\nsection shapes, hardware made errors, and human experience needs to be\nintroduced to solve the complex situation on site during matching process.\nHowever, traditional matching methods based on feature points or feature lines\ncould no longer meet the requirements. To this end, we first establish the rail\nprofiles matching dataset composed of 46386 pairs of professional manual\nmatched data, then propose a general high-precision method for rail profiles\nmatching using pre-trained convolutional neural network (CNN). This new method\nbased on deep learning is promising to be the dominant approach for this issue.\nSource code is at\nhttps://github.com/Kunqi1994/Deep-learning-on-rail-profile-matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kunqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperparameter Optimization with Neural Network Pruning. (arXiv:2205.08695v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08695","description":"<p>Since the deep learning model is highly dependent on hyperparameters,\nhyperparameter optimization is essential in developing deep learning\nmodel-based applications, even if it takes a long time. As service development\nusing deep learning models has gradually become competitive, many developers\nhighly demand rapid hyperparameter optimization algorithms. In order to keep\npace with the needs of faster hyperparameter optimization algorithms,\nresearchers are focusing on improving the speed of hyperparameter optimization\nalgorithm. However, the huge time consumption of hyperparameter optimization\ndue to the high computational cost of the deep learning model itself has not\nbeen dealt with in-depth. Like using surrogate model in Bayesian optimization,\nto solve this problem, it is necessary to consider proxy model for a neural\nnetwork (N_B) to be used for hyperparameter optimization. Inspired by the main\ngoal of neural network pruning, i.e., high computational cost reduction and\nperformance preservation, we presumed that the neural network (N_P) obtained\nthrough neural network pruning would be a good proxy model of N_B. In order to\nverify our idea, we performed extensive experiments by using CIFAR10, CFIAR100,\nand TinyImageNet datasets and three generally-used neural networks and three\nrepresentative hyperparameter optmization methods. Through these experiments,\nwe verified that N_P can be a good proxy model of N_B for rapid hyperparameter\noptimization. The proposed hyperparameter optimization framework can reduce the\namount of time up to 37%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kangil Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Junho Yim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemiCurv: Semi-Supervised Curvilinear Structure Segmentation. (arXiv:2205.08706v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08706","description":"<p>Recent work on curvilinear structure segmentation has mostly focused on\nbackbone network design and loss engineering. The challenge of collecting\nlabelled data, an expensive and labor intensive process, has been overlooked.\nWhile labelled data is expensive to obtain, unlabelled data is often readily\navailable. In this work, we propose SemiCurv, a semi-supervised learning (SSL)\nframework for curvilinear structure segmentation that is able to utilize such\nunlabelled data to reduce the labelling burden. Our framework addresses two key\nchallenges in formulating curvilinear segmentation in a semi-supervised manner.\nFirst, to fully exploit the power of consistency based SSL, we introduce a\ngeometric transformation as strong data augmentation and then align\nsegmentation predictions via a differentiable inverse transformation to enable\nthe computation of pixel-wise consistency. Second, the traditional mean square\nerror (MSE) on unlabelled data is prone to collapsed predictions and this issue\nexacerbates with severe class imbalance (significantly more background pixels).\nWe propose a N-pair consistency loss to avoid trivial predictions on unlabelled\ndata. We evaluate SemiCurv on six curvilinear segmentation datasets, and find\nthat with no more than 5% of the labelled data, it achieves close to 95% of the\nperformance relative to its fully supervised counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Manh Cuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazici_Y/0/1/0/all/0/1\">Yasin Yazici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kangkang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_H/0/1/0/all/0/1\">Hlaing Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1\">Chuan-Sheng Foo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It Isn't Sh!tposting, It's My CAT Posting. (arXiv:2205.08710v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08710","description":"<p>In this paper, we describe a novel architecture which can generate hilarious\ncaptions for a given input image. The architecture is split into two halves,\ni.e. image captioning and hilarious text conversion. The architecture starts\nwith a pre-trained CNN model, VGG16 in this implementation, and applies\nattention LSTM on it to generate normal caption. These normal captions then are\nfed forward to our hilarious text conversion transformer which converts this\ntext into something hilarious while maintaining the context of the input image.\nThe architecture can also be split into two halves and only the seq2seq\ntransformer can be used to generate hilarious caption by inputting a\nsentence.This paper aims to help everyday user to be more lazy and hilarious at\nthe same time by generating captions using CATNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rawat_P/0/1/0/all/0/1\">Parthsarthi Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sayan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aguirre_J/0/1/0/all/0/1\">Jorge Aguirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daphara_A/0/1/0/all/0/1\">Akhil Daphara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse MDOD: Training End-to-End Multi-Object Detector without Bipartite Matching. (arXiv:2205.08714v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08714","description":"<p>Recent end-to-end multi-object detectors simplify the inference pipeline by\nremoving the hand-crafted process such as the duplicate bounding box removal\nusing non-maximum suppression (NMS). However, in the training, they require\nbipartite matching to calculate the loss from the output of the detector.\nContrary to the directivity of the end-to-end method, the bipartite matching\nmakes the training of the end-to-end detector complex, heuristic, and reliant.\nIn this paper, we aim to propose a method to train the end-to-end multi-object\ndetector without bipartite matching. To this end, we approach end-to-end\nmulti-object detection as a density estimation using a mixture model. Our\nproposed detector, called Sparse Mixture Density Object Detector (Sparse MDOD)\nestimates the distribution of bounding boxes using a mixture model. Sparse MDOD\nis trained by minimizing the negative log-likelihood and our proposed\nregularization term, maximum component maximization (MCM) loss that prevents\nduplicated predictions. During training, no additional procedure such as\nbipartite matching is needed, and the loss is directly computed from the\nnetwork outputs. Moreover, our Sparse MDOD outperforms the existing detectors\non MS-COCO, a renowned multi-object detection benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jaeyoung Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hojun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seunghyeon Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_I/0/1/0/all/0/1\">Inseop Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RandomMix: A mixed sample data augmentation method with multiple mixed modes. (arXiv:2205.08728v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08728","description":"<p>Data augmentation is a very practical technique that can be used to improve\nthe generalization ability of neural networks and prevent overfitting.\nRecently, mixed sample data augmentation has received a lot of attention and\nachieved great success. In order to enhance the performance of mixed sample\ndata augmentation, a series of recent works are devoted to obtaining and\nanalyzing the salient regions of the image, and using the saliency area to\nguide the image mixing. However, obtaining the salient information of an image\nrequires a lot of extra calculations. Different from improving performance\nthrough saliency analysis, our proposed method RandomMix mainly increases the\ndiversity of the mixed sample to enhance the generalization ability and\nperformance of neural networks. Moreover, RandomMix can improve the robustness\nof the model, does not require too much additional calculation, and is easy to\ninsert into the training pipeline. Finally, experiments on the CIFAR-10/100,\nTiny-ImageNet, ImageNet, and Google Speech Commands datasets demonstrate that\nRandomMix achieves better performance than other state-of-the-art mixed sample\ndata augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_C/0/1/0/all/0/1\">Changhai Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TTAPS: Test-Time Adaption by Aligning Prototypes using Self-Supervision. (arXiv:2205.08731v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08731","description":"<p>Nowadays, deep neural networks outperform humans in many tasks. However, if\nthe input distribution drifts away from the one used in training, their\nperformance drops significantly. Recently published research has shown that\nadapting the model parameters to the test sample can mitigate this performance\ndegradation. In this paper, we therefore propose a novel modification of the\nself-supervised training algorithm SwAV that adds the ability to adapt to\nsingle test samples. Using the provided prototypes of SwAV and our derived\ntest-time loss, we align the representation of unseen test samples with the\nself-supervised learned prototypes. We show the success of our method on the\ncommon benchmark dataset CIFAR10-C.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bartler_A/0/1/0/all/0/1\">Alexander Bartler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bender_F/0/1/0/all/0/1\">Florian Bender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiewel_F/0/1/0/all/0/1\">Felix Wiewel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-learned orthogonal basis patterns for fast, noise-robust single-pixel imaging. (arXiv:2205.08736v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08736","description":"<p>Single-pixel imaging (SPI) is a novel, unconventional method that goes beyond\nthe notion of traditional cameras but can be computationally expensive and slow\nfor real-time applications. Deep learning has been proposed as an alternative\napproach for solving the SPI reconstruction problem, but a detailed analysis of\nits performance and generated basis patterns when used for SPI is limited. We\npresent a modified deep convolutional autoencoder network (DCAN) for SPI on\n64x64 pixel images with up to 6.25% compression ratio and apply binary and\northogonality regularizers during training. Training a DCAN with these\nregularizers allows it to learn multiple measurement bases that have\ncombinations of binary or non-binary, and orthogonal or non-orthogonal\npatterns. We compare the reconstruction quality, orthogonality of the patterns,\nand robustness to noise of the resulting DCAN models to traditional SPI\nreconstruction algorithms (such as Total Variation minimization and Fourier\nTransform). Our DCAN models can be trained to be robust to noise while still\nhaving fast enough reconstruction times (~3 ms per frame) to be viable for\nreal-time imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Aguilar_R/0/1/0/all/0/1\">Ritz Ann Aguilar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dailisan_D/0/1/0/all/0/1\">Damian Dailisan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Passive Defense Against 3D Adversarial Point Clouds Through the Lens of 3D Steganalysis. (arXiv:2205.08738v1 [cs.MM])","link":"http://arxiv.org/abs/2205.08738","description":"<p>Nowadays, 3D data plays an indelible role in the computer vision field.\nHowever, extensive studies have proved that deep neural networks (DNNs) fed\nwith 3D data, such as point clouds, are susceptible to adversarial examples,\nwhich aim to misguide DNNs and might bring immeasurable losses. Currently, 3D\nadversarial point clouds are chiefly generated in three fashions, i.e., point\nshifting, point adding, and point dropping. These point manipulations would\nmodify geometrical properties and local correlations of benign point clouds\nmore or less. Motivated by this basic fact, we propose to defend such\nadversarial examples with the aid of 3D steganalysis techniques. Specifically,\nwe first introduce an adversarial attack and defense model adapted from the\ncelebrated Prisoners' Problem in steganography to help us comprehend 3D\nadversarial attack and defense more generally. Then we rethink two significant\nbut vague concepts in the field of adversarial example, namely, active defense\nand passive defense, from the perspective of steganalysis. Most importantly, we\ndesign a 3D adversarial point cloud detector through the lens of 3D\nsteganalysis. Our detector is double-blind, that is to say, it does not rely on\nthe exact knowledge of the adversarial attack means and victim models. To\nenable the detector to effectively detect malicious point clouds, we craft a\n64-D discriminant feature set, including features related to first-order and\nsecond-order local descriptions of point clouds. To our knowledge, this work is\nthe first to apply 3D steganalysis to 3D adversarial example defense. Extensive\nexperimental results demonstrate that the proposed 3D adversarial point cloud\ndetector can achieve good detection performance on multiple types of 3D\nadversarial point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiahao Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Validation of a photogrammetric approach for the study of ancient bowed instruments. (arXiv:2205.08745v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08745","description":"<p>Some ancient violins have been reduced throughout their history. We propose\nan objective photogrammetric approach to differentiate between a reduced and an\nunreduced instrument, where a three-dimensional mesh is studied geometrically\nby examining 2D slices. First, we validate the accuracy of the photogrammetric\nmesh by the way of a comparison with reference images obtained with medical\nimaging. Then, we show how contour lines and channels of minima can be\nautomatically extracted from the photogrammetric meshes, allowing to\nsuccessfully highlight differences between instruments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beghin_P/0/1/0/all/0/1\">Phil&#xe9;mon Beghin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ceulemans_A/0/1/0/all/0/1\">Anne-Emmanuelle Ceulemans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisette_P/0/1/0/all/0/1\">Paul Fisette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glineur_F/0/1/0/all/0/1\">Fran&#xe7;ois Glineur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Attention-based Self-supervised Absolute Depth Estimation using Geometric Priors in Autonomous Driving. (arXiv:2205.08780v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08780","description":"<p>Although existing monocular depth estimation methods have made great\nprogress, predicting an accurate absolute depth map from a single image is\nstill challenging due to the limited modeling capacity of networks and the\nscale ambiguity issue. In this paper, we introduce a fully Visual\nAttention-based Depth (VADepth) network, where spatial attention and channel\nattention are applied to all stages. By continuously extracting the\ndependencies of features along the spatial and channel dimensions over a long\ndistance, VADepth network can effectively preserve important details and\nsuppress interfering features to better perceive the scene structure for more\naccurate depth estimates. In addition, we utilize geometric priors to form\nscale constraints for scale-aware model training. Specifically, we construct a\nnovel scale-aware loss using the distance between the camera and a plane fitted\nby the ground points corresponding to the pixels of the rectangular area in the\nbottom middle of the image. Experimental results on the KITTI dataset show that\nthis architecture achieves the state-of-the-art performance and our method can\ndirectly output absolute depth without post-processing. Moreover, our\nexperiments on the SeasonDepth dataset also demonstrate the robustness of our\nmodel to multiple unseen environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_J/0/1/0/all/0/1\">Jie Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Lifeng An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-subject Action Unit Detection with Meta Learning and Transformer-based Relation Modeling. (arXiv:2205.08787v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08787","description":"<p>Facial Action Unit (AU) detection is a crucial task for emotion analysis from\nfacial movements. The apparent differences of different subjects sometimes\nmislead changes brought by AUs, resulting in inaccurate results. However, most\nof the existing AU detection methods based on deep learning didn't consider the\nidentity information of different subjects. The paper proposes a\nmeta-learning-based cross-subject AU detection model to eliminate the\nidentity-caused differences. Besides, a transformer-based relation learning\nmodule is introduced to learn the latent relations of multiple AUs. To be\nspecific, our proposed work is composed of two sub-tasks. The first sub-task is\nmeta-learning-based AU local region representation learning, called MARL, which\nlearns discriminative representation of local AU regions that incorporates the\nshared information of multiple subjects and eliminates identity-caused\ndifferences. The second sub-task uses the local region representation of AU of\nthe first sub-task as input, then adds relationship learning based on the\ntransformer encoder architecture to capture AU relationships. The entire\ntraining process is cascaded. Ablation study and visualization show that our\nMARL can eliminate identity-caused differences, thus obtaining a robust and\ngeneralized AU discriminative embedding representation. Our results prove that\non the two public datasets BP4D and DISFA, our method is superior to the\nstate-of-the-art technology, and the F1 score is improved by 1.3% and 1.4%,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiyuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects. (arXiv:2205.08811v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08811","description":"<p>Object pose estimation is crucial for robotic applications and augmented\nreality. Beyond instance level 6D object pose estimation methods, estimating\ncategory-level pose and shape has become a promising trend. As such, a new\nresearch field needs to be supported by well-designed datasets. To provide a\nbenchmark with high-quality ground truth annotations to the community, we\nintroduce a multimodal dataset for category-level object pose estimation with\nphotometrically challenging objects termed PhoCaL. PhoCaL comprises 60 high\nquality 3D models of household objects over 8 categories including highly\nreflective, transparent and symmetric objects. We developed a novel\nrobot-supported multi-modal (RGB, depth, polarisation) data acquisition and\nannotation process. It ensures sub-millimeter accuracy of the pose for opaque\ntextured, shiny and transparent objects, no motion blur and perfect camera\nsynchronisation. To set a benchmark for our dataset, state-of-the-art RGB-D and\nmonocular RGB methods are evaluated on the challenging scenes of PhoCaL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">HyunJun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siyuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikanth_R/0/1/0/all/0/1\">Rahul Parthasarathy Srikanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garattoni_L/0/1/0/all/0/1\">Lorenzo Garattoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_S/0/1/0/all/0/1\">Sven Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomaly detection using prediction error with Spatio-Temporal Convolutional LSTM. (arXiv:2205.08812v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08812","description":"<p>In this paper, we propose a novel method for video anomaly detection\nmotivated by an existing architecture for sequence-to-sequence prediction and\nreconstruction using a spatio-temporal convolutional Long Short-Term Memory\n(convLSTM). As in previous work on anomaly detection, anomalies arise as\nspatially localised failures in reconstruction or prediction. In experiments\nwith five benchmark datasets, we show that using prediction gives superior\nperformance to using reconstruction. We also compare performance with different\nlength input/output sequences. Overall, our results using prediction are\ncomparable with the state of the art on the benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hanh Thi Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogg_D/0/1/0/all/0/1\">David Hogg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speckle Image Restoration without Clean Data. (arXiv:2205.08833v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08833","description":"<p>Speckle noise is an inherent disturbance in coherent imaging systems such as\ndigital holography, synthetic aperture radar, optical coherence tomography, or\nultrasound systems. These systems usually produce only single observation per\nview angle of the same interest object, imposing the difficulty to leverage the\nstatistic among observations. We propose a novel image restoration algorithm\nthat can perform speckle noise removal without clean data and does not require\nmultiple noisy observations in the same view angle. Our proposed method can\nalso be applied to the situation without knowing the noise distribution as\nprior. We demonstrate our method is especially well-suited for spectral images\nby first validating on the synthetic dataset, and also applied on real-world\ndigital holography samples. The results are superior in both quantitative\nmeasurement and visual inspection compared to several widely applied baselines.\nOur method even shows promising results across different speckle noise\nstrengths, without the clean data needed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tai_T/0/1/0/all/0/1\">Tsung-Ming Tai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jhang_Y/0/1/0/all/0/1\">Yun-Jie Jhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hwang_W/0/1/0/all/0/1\">Wen-Jyi Hwang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chau-Jern Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Neural Networks Learning from Scratch with Very Few Data and without Regularization. (arXiv:2205.08836v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08836","description":"<p>Recent findings have shown that Neural Networks generalize also in\nover-parametrized regimes with zero training error. This is surprising, since\nit is completely against traditional machine learning wisdom. In our empirical\nstudy we fortify these findings in the domain of fine-grained image\nclassification. We show that very large Convolutional Neural Networks with\nmillions of weights do learn with only a handful of training samples and\nwithout image augmentation, explicit regularization or pretraining. We train\nthe architectures ResNet018, ResNet101 and VGG19 on subsets of the difficult\nbenchmark datasets Caltech101, CUB_200_2011, FGVCAircraft, Flowers102 and\nStanfordCars with 100 classes and more, perform a comprehensive comparative\nstudy and draw implications for the practical application of CNNs. Finally, we\nshow that VGG19 with 140 million weights learns to distinguish airplanes and\nmotorbikes up to 95% accuracy with only 20 samples per class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Linse_C/0/1/0/all/0/1\">Christoph Linse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinetz_T/0/1/0/all/0/1\">Thomas Martinetz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Positional Information is All You Need: A Novel Pipeline for Self-Supervised SVDE from Videos. (arXiv:2205.08851v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08851","description":"<p>Recently, much attention has been drawn to learning the underlying 3D\nstructures of a scene from monocular videos in a fully self-supervised fashion.\nOne of the most challenging aspects of this task is handling the independently\nmoving objects as they break the rigid-scene assumption. For the first time, we\nshow that pixel positional information can be exploited to learn SVDE (Single\nView Depth Estimation) from videos. Our proposed moving object (MO) masks,\nwhich are induced by shifted positional information (SPI) and referred to as\n`SPIMO' masks, are very robust and consistently remove the independently moving\nobjects in the scenes, allowing for better learning of SVDE from videos.\nAdditionally, we introduce a new adaptive quantization scheme that assigns the\nbest per-pixel quantization curve for our depth discretization. Finally, we\nemploy existing boosting techniques in a new way to further self-supervise the\ndepth of the moving objects. With these features, our pipeline is robust\nagainst moving objects and generalizes well to high-resolution images, even\nwhen trained with small patches, yielding state-of-the-art (SOTA) results with\nalmost 8.5x fewer parameters than the previous works that learn from videos. We\npresent extensive experiments on KITTI and CityScapes that show the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bello_J/0/1/0/all/0/1\">Juan Luis Gonzalez Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jaeho Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Munchurl Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer based multiple instance learning for weakly supervised histopathology image segmentation. (arXiv:2205.08878v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08878","description":"<p>Hispathological image segmentation algorithms play a critical role in\ncomputer aided diagnosis technology. The development of weakly supervised\nsegmentation algorithm alleviates the problem of medical image annotation that\nit is time-consuming and labor-intensive. As a subset of weakly supervised\nlearning, Multiple Instance Learning (MIL) has been proven to be effective in\nsegmentation. However, there is a lack of related information between instances\nin MIL, which limits the further improvement of segmentation performance. In\nthis paper, we propose a novel weakly supervised method for pixel-level\nsegmentation in histopathology images, which introduces Transformer into the\nMIL framework to capture global or long-range dependencies. The multi-head\nself-attention in the Transformer establishes the relationship between\ninstances, which solves the shortcoming that instances are independent of each\nother in MIL. In addition, deep supervision is introduced to overcome the\nlimitation of annotations in weakly supervised methods and make the better\nutilization of hierarchical information. The state-of-the-art results on the\ncolon cancer dataset demonstrate the superiority of the proposed method\ncompared with other weakly supervised methods. It is worth believing that there\nis a potential of our approach for various applications in medical images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1\">Ziniu Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kailu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_M/0/1/0/all/0/1\">Maode Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Eric I-Chao Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_B/0/1/0/all/0/1\">Bingzheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Segmentation Guided Style-based Generative Adversarial Networks for PET Synthesis. (arXiv:2205.08887v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08887","description":"<p>Potential radioactive hazards in full-dose positron emission tomography (PET)\nimaging remain a concern, whereas the quality of low-dose images is never\ndesirable for clinical use. So it is of great interest to translate low-dose\nPET images into full-dose. Previous studies based on deep learning methods\nusually directly extract hierarchical features for reconstruction. We notice\nthat the importance of each feature is different and they should be weighted\ndissimilarly so that tiny information can be captured by the neural network.\nFurthermore, the synthesis on some regions of interest is important in some\napplications. Here we propose a novel segmentation guided style-based\ngenerative adversarial network (SGSGAN) for PET synthesis. (1) We put forward a\nstyle-based generator employing style modulation, which specifically controls\nthe hierarchical features in the translation process, to generate images with\nmore realistic textures. (2) We adopt a task-driven strategy that couples a\nsegmentation task with a generative adversarial network (GAN) framework to\nimprove the translation performance. Extensive experiments show the superiority\nof our overall framework in PET synthesis, especially on those regions of\ninterest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwen Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_E/0/1/0/all/0/1\">Eric I-Chao Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yubo Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Remote Sensing Novel View Synthesis with Implicit Multiplane Representations. (arXiv:2205.08908v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08908","description":"<p>Novel view synthesis of remote sensing scenes is of great significance for\nscene visualization, human-computer interaction, and various downstream\napplications. Despite the recent advances in computer graphics and\nphotogrammetry technology, generating novel views is still challenging\nparticularly for remote sensing images due to its high complexity, view\nsparsity and limited view-perspective variations. In this paper, we propose a\nnovel remote sensing view synthesis method by leveraging the recent advances in\nimplicit neural representations. Considering the overhead and far depth imaging\nof remote sensing images, we represent the 3D space by combining implicit\nmultiplane images (MPI) representation and deep neural networks. The 3D scene\nis reconstructed under a self-supervised optimization paradigm through a\ndifferentiable multiplane renderer with multi-view input constraints. Images\nfrom any novel views thus can be freely rendered on the basis of the\nreconstructed model. As a by-product, the depth maps corresponding to the given\nviewpoint can be generated along with the rendering output. We refer to our\nmethod as Implicit Multiplane Images (ImMPI). To further improve the view\nsynthesis under sparse-view inputs, we explore the learning-based\ninitialization of remote sensing 3D scenes and proposed a neural network based\nPrior extractor to accelerate the optimization process. In addition, we propose\na new dataset for remote sensing novel view synthesis with multi-view\nreal-world google earth images. Extensive experiments demonstrate the\nsuperiority of the ImMPI over previous state-of-the-art methods in terms of\nreconstruction accuracy, visual fidelity, and time efficiency. Ablation\nexperiments also suggest the effectiveness of our methodology design. Our\ndataset and code can be found at https://github.com/wyc-Chang/ImMPI\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongchang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1\">Zhengxia Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Financial Time Series Data Augmentation with Generative Adversarial Networks and extended Intertemporal Return Plots. (arXiv:2205.08924v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08924","description":"<p>Data augmentation is a key regularization method to support the forecast and\nclassification performance of highly parameterized models in computer vision.\nIn the time series domain however, regularization in terms of augmentation is\nnot equally common even though these methods have proven to mitigate effects\nfrom small sample size or non-stationarity. In this paper we apply state-of-the\nart image-based generative models for the task of data augmentation and\nintroduce the extended intertemporal return plot (XIRP), a new image\nrepresentation for time series. Multiple tests are conducted to assess the\nquality of the augmentation technique regarding its ability to synthesize time\nseries effectively and improve forecast results on a subset of the M4\ncompetition. We further investigate the relationship between data set\ncharacteristics and sampling results via Shapley values for feature attribution\non the performance metrics and the optimal ratio of augmented data. Over all\ndata sets, our approach proves to be effective in reducing the return forecast\nerror by 7% on 79% of the financial data sets with varying statistical\nproperties and frequencies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hellermann_J/0/1/0/all/0/1\">Justin Hellermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qinzhuan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Ankit Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COVID-Net UV: An End-to-End Spatio-Temporal Deep Neural Network Architecture for Automated Diagnosis of COVID-19 Infection from Ultrasound Videos. (arXiv:2205.08932v1 [eess.IV])","link":"http://arxiv.org/abs/2205.08932","description":"<p>Besides vaccination, as an effective way to mitigate the further spread of\nCOVID-19, fast and accurate screening of individuals to test for the disease is\nyet necessary to ensure public health safety. We propose COVID-Net UV, an\nend-to-end hybrid spatio-temporal deep neural network architecture, to detect\nCOVID-19 infection from lung point-of-care ultrasound videos captured by convex\ntransducers. COVID-Net UV comprises a convolutional neural network that\nextracts spatial features and a recurrent neural network that learns temporal\ndependence. After careful hyperparameter tuning, the network achieves an\naverage accuracy of 94.44% with no false-negative cases for COVID-19 cases. The\ngoal with COVID-Net UV is to assist front-line clinicians in the fight against\nCOVID-19 via accelerating the screening of lung point-of-care ultrasound videos\nand automatic detection of COVID-19 positive cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Azimi_H/0/1/0/all/0/1\">Hilda Azimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1\">Ashkan Ebadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jessy Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1\">Pengcheng Xi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Features for CBIR with Scarce Data using Hebbian Learning. (arXiv:2205.08935v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08935","description":"<p>Features extracted from Deep Neural Networks (DNNs) have proven to be very\neffective in the context of Content Based Image Retrieval (CBIR). In recent\nwork, biologically inspired \\textit{Hebbian} learning algorithms have shown\npromises for DNN training. In this contribution, we study the performance of\nsuch algorithms in the development of feature extractors for CBIR tasks.\nSpecifically, we consider a semi-supervised learning strategy in two steps:\nfirst, an unsupervised pre-training stage is performed using Hebbian learning\non the image dataset; second, the network is fine-tuned using supervised\nStochastic Gradient Descent (SGD) training. For the unsupervised pre-training\nstage, we explore the nonlinear Hebbian Principal Component Analysis (HPCA)\nlearning rule. For the supervised fine-tuning stage, we assume sample\nefficiency scenarios, in which the amount of labeled samples is just a small\nfraction of the whole dataset. Our experimental analysis, conducted on the\nCIFAR10 and CIFAR100 datasets shows that, when few labeled samples are\navailable, our Hebbian approach provides relevant improvements compared to\nvarious alternative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lagani_G/0/1/0/all/0/1\">Gabriele Lagani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1\">Fabrizio Falchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1\">Claudio Gennaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1\">Giuseppe Amato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A lightweight multi-scale context network for salient object detection in optical remote sensing images. (arXiv:2205.08959v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08959","description":"<p>Due to the more dramatic multi-scale variations and more complicated\nforegrounds and backgrounds in optical remote sensing images (RSIs), the\nsalient object detection (SOD) for optical RSIs becomes a huge challenge.\nHowever, different from natural scene images (NSIs), the discussion on the\noptical RSI SOD task still remains scarce. In this paper, we propose a\nmulti-scale context network, namely MSCNet, for SOD in optical RSIs.\nSpecifically, a multi-scale context extraction module is adopted to address the\nscale variation of salient objects by effectively learning multi-scale\ncontextual information. Meanwhile, in order to accurately detect complete\nsalient objects in complex backgrounds, we design an attention-based pyramid\nfeature aggregation mechanism for gradually aggregating and refining the\nsalient regions from the multi-scale context extraction module. Extensive\nexperiments on two benchmarks demonstrate that MSCNet achieves competitive\nperformance with only 3.26M parameters. The code will be available at\nhttps://github.com/NuaaYH/MSCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Han Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ningzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yetong Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1\">Jun Cen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DL4DS -- Deep Learning for empirical DownScaling. (arXiv:2205.08967v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08967","description":"<p>A common task in Earth Sciences is to infer climate information at local and\nregional scales from global climate models. Dynamical downscaling requires\nrunning expensive numerical models at high resolution which can be prohibitive\ndue to long model runtimes. On the other hand, statistical downscaling\ntechniques present an alternative approach for learning links between the\nlarge- and local-scale climate in a more efficient way. A large number of deep\nneural network-based approaches for statistical downscaling have been proposed\nin recent years, mostly based on convolutional architectures developed for\ncomputer vision and super-resolution tasks. This paper presents DL4DS, Deep\nLearning for empirical DownScaling, a python library that implements a wide\nvariety of state-of-the-art and novel algorithms for downscaling gridded Earth\nScience data with deep neural networks. DL4DS has been designed with the goal\nof providing a general framework for training convolutional neural networks\nwith configurable architectures and learning strategies to facilitate the\nconduction of comparative and ablation studies in a robust way. We showcase the\ncapabilities of DL4DS on air quality CAMS data over the western Mediterranean\narea. The DL4DS library can be found in this repository:\nhttps://github.com/carlos-gg/dl4ds\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1\">Carlos Alberto Gomez Gonzalez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trading Positional Complexity vs. Deepness in Coordinate Networks. (arXiv:2205.08987v1 [cs.CV])","link":"http://arxiv.org/abs/2205.08987","description":"<p>It is well noted that coordinate-based MLPs benefit -- in terms of preserving\nhigh-frequency information -- through the encoding of coordinate positions as\nan array of Fourier features. Hitherto, the rationale for the effectiveness of\nthese positional encodings has been mainly studied through a Fourier lens. In\nthis paper, we strive to broaden this understanding by showing that alternative\nnon-Fourier embedding functions can indeed be used for positional encoding.\nMoreover, we show that their performance is entirely determined by a trade-off\nbetween the stable rank of the embedded matrix and the distance preservation\nbetween embedded coordinates. We further establish that the now ubiquitous\nFourier feature mapping of position is a special case that fulfills these\nconditions. Consequently, we present a more general theory to analyze\npositional encoding in terms of shifted basis functions. In addition, we argue\nthat employing a more complex positional encoding -- that scales exponentially\nwith the number of modes -- requires only a linear (rather than deep)\ncoordinate function to achieve comparable performance. Counter-intuitively, we\ndemonstrate that trading positional embedding complexity for network deepness\nis orders of magnitude faster than current state-of-the-art; despite the\nadditional embedding complexity. To this end, we develop the necessary\ntheoretical formulae and empirically verify that our theoretical claims hold in\npractice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jianqiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1\">Sameera Ramasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xueqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constraining the Attack Space of Machine Learning Models with Distribution Clamping Preprocessing. (arXiv:2205.08989v1 [cs.LG])","link":"http://arxiv.org/abs/2205.08989","description":"<p>Preprocessing and outlier detection techniques have both been applied to\nneural networks to increase robustness with varying degrees of success. In this\npaper, we formalize the ideal preprocessor function as one that would take any\ninput and set it to the nearest in-distribution input. In other words, we\ndetect any anomalous pixels and set them such that the new input is\nin-distribution. We then illustrate a relaxed solution to this problem in the\ncontext of patch attacks. Specifically, we demonstrate that we can model\nconstraints on the patch attack that specify regions as out of distribution.\nWith these constraints, we are able to preprocess inputs successfully,\nincreasing robustness on CARLA object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Advocacy of Bio-inspired Models for Robust Image Recognition. (arXiv:2205.09037v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09037","description":"<p>Deep convolutional neural networks (DCNNs) have revolutionized computer\nvision and are often advocated as good models of the human visual system.\nHowever, there are currently many shortcomings of DCNNs, which preclude them as\na model of human vision. There are continuous attempts to use features of the\nhuman visual system to improve the robustness of neural networks to data\nperturbations. We provide a detailed analysis of such bio-inspired models and\ntheir properties. To this end, we benchmark the robustness of several\nbio-inspired models against their most comparable baseline DCNN models. We find\nthat bio-inspired models tend to be adversarially robust without requiring any\nspecial data augmentation. Additionally, we find that bio-inspired models beat\nadversarially trained models in the presence of more real-world common\ncorruptions. Interestingly, we also find that bio-inspired models tend to use\nboth low and mid-frequency information, in contrast to other DCNN models. We\nfind that this mix of frequency information makes them robust to both\nadversarial perturbations and common corruptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Machiraju_H/0/1/0/all/0/1\">Harshitha Machiraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choung_O/0/1/0/all/0/1\">Oh-Hyeon Choung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzog_M/0/1/0/all/0/1\">Michael H. Herzog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Contrast Masked Autoencoders Are Powerful Pathological Representation Learners. (arXiv:2205.09048v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09048","description":"<p>Based on digital whole slide scanning technique, artificial intelligence\nalgorithms represented by deep learning have achieved remarkable results in the\nfield of computational pathology. Compared with other medical images such as\nComputed Tomography (CT) or Magnetic Resonance Imaging (MRI), pathological\nimages are more difficult to annotate, thus there is an extreme lack of data\nsets that can be used for supervised learning. In this study, a self-supervised\nlearning (SSL) model, Global Contrast Masked Autoencoders (GCMAE), is proposed,\nwhich has the ability to represent both global and local domain-specific\nfeatures of whole slide image (WSI), as well as excellent cross-data transfer\nability. The Camelyon16 and NCTCRC datasets are used to evaluate the\nperformance of our model. When dealing with transfer learning tasks with\ndifferent data sets, the experimental results show that GCMAE has better linear\nclassification accuracy than MAE, which can reach 81.10% and 89.22%\nrespectively. Our method outperforms the previous state-of-the-art algorithm\nand even surpass supervised learning (improved by 3.86% on NCTCRC data sets).\nThe source code of this paper is publicly available at\nhttps://github.com/StarUniversus/gcmae\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_H/0/1/0/all/0/1\">Hao Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weixing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_M/0/1/0/all/0/1\">Mingchen Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruijie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tingting Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_R/0/1/0/all/0/1\">Ruiqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinghua Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaoyu Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VRAG: Region Attention Graphs for Content-Based Video Retrieval. (arXiv:2205.09068v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09068","description":"<p>Content-based Video Retrieval (CBVR) is used on media-sharing platforms for\napplications such as video recommendation and filtering. To manage databases\nthat scale to billions of videos, video-level approaches that use fixed-size\nembeddings are preferred due to their efficiency. In this paper, we introduce\nVideo Region Attention Graph Networks (VRAG) that improves the state-of-the-art\nof video-level methods. We represent videos at a finer granularity via\nregion-level features and encode video spatio-temporal dynamics through\nregion-level relations. Our VRAG captures the relationships between regions\nbased on their semantic content via self-attention and the permutation\ninvariant aggregation of Graph Convolution. In addition, we show that the\nperformance gap between video-level and frame-level methods can be reduced by\nsegmenting videos into shots and using shot embeddings for video retrieval. We\nevaluate our VRAG over several video retrieval tasks and achieve a new\nstate-of-the-art for video-level retrieval. Furthermore, our shot-level VRAG\nshows higher retrieval precision than other existing video-level methods, and\ncloser performance to frame-level methods at faster evaluation speeds. Finally,\nour code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1\">Kennard Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pluralistic Image Completion with Probabilistic Mixture-of-Experts. (arXiv:2205.09086v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09086","description":"<p>Pluralistic image completion focuses on generating both visually realistic\nand diverse results for image completion. Prior methods enjoy the empirical\nsuccesses of this task. However, their used constraints for pluralistic image\ncompletion are argued to be not well interpretable and unsatisfactory from two\naspects. First, the constraints for visual reality can be weakly correlated to\nthe objective of image completion or even redundant. Second, the constraints\nfor diversity are designed to be task-agnostic, which causes the constraints to\nnot work well. In this paper, to address the issues, we propose an end-to-end\nprobabilistic method. Specifically, we introduce a unified probabilistic graph\nmodel that represents the complex interactions in image completion. The entire\nprocedure of image completion is then mathematically divided into several\nsub-procedures, which helps efficient enforcement of constraints. The\nsub-procedure directly related to pluralistic results is identified, where the\ninteraction is established by a Gaussian mixture model (GMM). The inherent\nparameters of GMM are task-related, which are optimized adaptively during\ntraining, while the number of its primitives can control the diversity of\nresults conveniently. We formally establish the effectiveness of our method and\ndemonstrate it with comprehensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiaobo Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yewen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1\">Yibing Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BodyMap: Learning Full-Body Dense Correspondence Map. (arXiv:2205.09111v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09111","description":"<p>Dense correspondence between humans carries powerful semantic information\nthat can be utilized to solve fundamental problems for full-body understanding\nsuch as in-the-wild surface matching, tracking and reconstruction. In this\npaper we present BodyMap, a new framework for obtaining high-definition\nfull-body and continuous dense correspondence between in-the-wild images of\nclothed humans and the surface of a 3D template model. The correspondences\ncover fine details such as hands and hair, while capturing regions far from the\nbody surface, such as loose clothing. Prior methods for estimating such dense\nsurface correspondence i) cut a 3D body into parts which are unwrapped to a 2D\nUV space, producing discontinuities along part seams, or ii) use a single\nsurface for representing the whole body, but none handled body details. Here,\nwe introduce a novel network architecture with Vision Transformers that learn\nfine-level features on a continuous body surface. BodyMap outperforms prior\nwork on various metrics and datasets, including DensePose-COCO by a large\nmargin. Furthermore, we show various applications ranging from multi-layer\ndense cloth correspondence, neural rendering with novel-view synthesis and\nappearance swapping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ianina_A/0/1/0/all/0/1\">Anastasia Ianina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1\">Nikolaos Sarafianos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rocco_I/0/1/0/all/0/1\">Ignacio Rocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tung_T/0/1/0/all/0/1\">Tony Tung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Autoencoders As Spatiotemporal Learners. (arXiv:2205.09113v1 [cs.CV])","link":"http://arxiv.org/abs/2205.09113","description":"<p>This paper studies a conceptually simple extension of Masked Autoencoders\n(MAE) to spatiotemporal representation learning from videos. We randomly mask\nout spacetime patches in videos and learn an autoencoder to reconstruct them in\npixels. Interestingly, we show that our MAE method can learn strong\nrepresentations with almost no inductive bias on spacetime (only except for\npatch and positional embeddings), and spacetime-agnostic random masking\nperforms the best. We observe that the optimal masking ratio is as high as 90%\n(vs. 75% on images), supporting the hypothesis that this ratio is related to\ninformation redundancy of the data. A high masking ratio leads to a large\nspeedup, e.g., &gt; 4x in wall-clock time or even more. We report competitive\nresults on several challenging video datasets using vanilla Vision\nTransformers. We observe that MAE can outperform supervised pre-training by\nlarge margins. We further report encouraging results of training on real-world,\nuncurated Instagram data. Our study suggests that the general framework of\nmasked autoencoding (BERT, MAE, etc.) can be a unified methodology for\nrepresentation learning with minimal domain knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanghao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kaiming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Selective Sensor Fusion for States Estimation. (arXiv:1912.13077v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1912.13077","description":"<p>Autonomous vehicles and mobile robotic systems are typically equipped with\nmultiple sensors to provide redundancy. By integrating the observations from\ndifferent sensors, these mobile agents are able to perceive the environment and\nestimate system states, e.g. locations and orientations. Although deep learning\napproaches for multimodal odometry estimation and localization have gained\ntraction, they rarely focus on the issue of robust sensor fusion - a necessary\nconsideration to deal with noisy or incomplete sensor observations in the real\nworld. Moreover, current deep odometry models suffer from a lack of\ninterpretability. To this extent, we propose SelectFusion, an end-to-end\nselective sensor fusion module which can be applied to useful pairs of sensor\nmodalities such as monocular images and inertial measurements, depth images and\nLIDAR point clouds. Our model is a uniform framework that is not restricted to\nspecific modality or task. During prediction, the network is able to assess the\nreliability of the latent features from different sensor modalities and\nestimate trajectory both at scale and global pose. In particular, we propose\ntwo fusion modules - a deterministic soft fusion and a stochastic hard fusion,\nand offer a comprehensive study of the new strategies compared to trivial\ndirect fusion. We extensively evaluate all fusion strategies in both public\ndatasets and on progressively degraded datasets that present synthetic\nocclusions, noisy and missing data and time misalignment between sensors, and\nwe investigate the effectiveness of the different fusion strategies in\nattending the most reliable features, which in itself, provides insights into\nthe operation of the various models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_S/0/1/0/all/0/1\">Stefano Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chris Xiaoxuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1\">Andrew Markham</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.09147","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial noises. By adding\nadversarial noises to training samples, adversarial training can improve the\nmodel's robustness against adversarial noises. However, adversarial training\nsamples with excessive noises can harm standard accuracy, which may be\nunacceptable for many medical image analysis applications. This issue has been\ntermed the trade-off between standard accuracy and adversarial robustness. In\nthis paper, we hypothesize that this issue may be alleviated if the adversarial\nsamples for training are placed right on the decision boundaries. Based on this\nhypothesis, we design an adaptive adversarial training method, named IMA. For\neach individual training sample, IMA makes a sample-wise estimation of the\nupper bound of the adversarial perturbation. In the training process, each of\nthe sample-wise adversarial perturbations is gradually increased to match the\nmargin. Once an equilibrium state is reached, the adversarial perturbations\nwill stop increasing. IMA is evaluated on publicly available datasets under two\npopular adversarial attacks, PGD and IFGSM. The results show that: (1) IMA\nsignificantly improves adversarial robustness of DNN classifiers, which\nachieves the state-of-the-art performance; (2) IMA has a minimal reduction in\nclean accuracy among all competing defense methods; (3) IMA can be applied to\npretrained models to reduce time cost; (4) IMA can be applied to the\nstate-of-the-art medical image segmentation networks, with outstanding\nperformance. We hope our work may help to lift the trade-off between\nadversarial robustness and clean accuracy and facilitate the development of\nrobust applications in the medical field. The source code will be released when\nthis paper is published.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Linhai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1\">Liang Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Moderately Supervised Learning: Definition, Framework and Generality. (arXiv:2008.11945v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.11945","description":"<p>Learning with supervision has achieved remarkable success in numerous\nartificial intelligence (AI) applications. In the current literature, by\nreferring to the properties of the labels prepared for the training data set,\nlearning with supervision is categorized as supervised learning (SL) and weakly\nsupervised learning (WSL). SL concerns the situation where the training data\nset is assigned with ideal labels, while WSL concerns the situation where the\ntraining data set is assigned with non-ideal labels. However, without\nconsidering the properties of the transformation from the given labels to\nlearnable targets, the definition of SL is relatively abstract, which conceals\nsome details that can be critical to building the appropriate solutions for\nspecific SL tasks. Thus, it is desirable to reveal these details more\nconcretely. This article attempts to achieve this goal by expanding the\ncategorization of SL and investigating the sub-type that plays the central role\nin SL. More specifically, taking into consideration the properties of the\ntransformation from the given labels to learnable targets, we firstly\ncategorize SL into three narrower sub-types. Then we focus on the moderately\nsupervised learning (MSL) sub-type that concerns the situation where the given\nlabels are ideal, but due to the simplicity in annotation, careful designs are\nrequired to transform the given labels into learnable targets. From the\nperspectives of the definition, framework and generality, we comprehensively\nillustrate MSL and reveal what details are concealed by the abstractness of the\ndefinition of SL. At the meantime, the whole presentation of this paper as well\nestablishes a tutorial for AI application engineers to refer to viewing a\nproblem to be solved from the mathematicians' vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongquan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Deep Learning -- A systematic Meta-Review. (arXiv:2010.14881v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.14881","description":"<p>Deep learning (DL) has remarkably impacted several different scientific\ndisciplines over the last few years. E.g., in image processing and analysis, DL\nalgorithms were able to outperform other cutting-edge methods. Additionally, DL\nhas delivered state-of-the-art results in tasks like autonomous driving,\noutclassing previous attempts. There are even instances where DL outperformed\nhumans, for example with object recognition and gaming. DL is also showing vast\npotential in the medical domain. With the collection of large quantities of\npatient records and data, and a trend towards personalized treatments, there is\na great need for automated and reliable processing and analysis of health\ninformation. Patient data is not only collected in clinical centers, like\nhospitals and private practices, but also by mobile healthcare apps or online\nwebsites. The abundance of collected patient data and the recent growth in the\nDL field has resulted in a large increase in research efforts. In Q2/2020, the\nsearch engine PubMed returned already over 11,000 results for the search term\n'deep learning', and around 90% of these publications are from the last three\nyears. However, even though PubMed represents the largest search engine in the\nmedical field, it does not cover all medical-related publications. Hence, a\ncomplete overview of the field of 'medical deep learning' is almost impossible\nto obtain and acquiring a full overview of medical sub-fields is becoming\nincreasingly more difficult. Nevertheless, several review and survey articles\nabout medical DL have been published within the last few years. They focus, in\ngeneral, on specific medical scenarios, like the analysis of medical images\ncontaining specific pathologies. With these surveys as a foundation, the aim of\nthis article is to provide the first high-level, systematic meta-review of\nmedical DL surveys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1\">Jan Egger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gsaxner_C/0/1/0/all/0/1\">Christina Gsaxner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pepe_A/0/1/0/all/0/1\">Antonio Pepe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pomykala_K/0/1/0/all/0/1\">Kelsey L. Pomykala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jonske_F/0/1/0/all/0/1\">Frederic Jonske</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurz_M/0/1/0/all/0/1\">Manuel Kurz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jianning Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1\">Jens Kleesiek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PocketNet: A Smaller Neural Network for Medical Image Analysis. (arXiv:2104.10745v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.10745","description":"<p>Medical imaging deep learning models are often large and complex, requiring\nspecialized hardware to train and evaluate these models. To address such\nissues, we propose the PocketNet paradigm to reduce the size of deep learning\nmodels by throttling the growth of the number of channels in convolutional\nneural networks. We demonstrate that, for a range of segmentation and\nclassification tasks, PocketNet architectures produce results comparable to\nthat of conventional neural networks while reducing the number of parameters by\nmultiple orders of magnitude, using up to 90% less GPU memory, and speeding up\ntraining times by up to 40%, thereby allowing such models to be trained and\ndeployed in resource-constrained settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1\">Adrian Celaya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Actor_J/0/1/0/all/0/1\">Jonas A. Actor</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muthusivarajan_R/0/1/0/all/0/1\">Rajarajeswari Muthusivarajan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gates_E/0/1/0/all/0/1\">Evan Gates</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1\">Caroline Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schellingerhout_D/0/1/0/all/0/1\">Dawid Schellingerhout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1\">Beatrice Riviere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1\">David Fuentes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Operating Points for High Performance Lesion Detection and Segmentation Using Lesion Size Reweighting. (arXiv:2107.12978v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.12978","description":"<p>There are many clinical contexts which require accurate detection and\nsegmentation of all focal pathologies (e.g. lesions, tumours) in patient\nimages. In cases where there are a mix of small and large lesions, standard\nbinary cross entropy loss will result in better segmentation of large lesions\nat the expense of missing small ones. Adjusting the operating point to\naccurately detect all lesions generally leads to oversegmentation of large\nlesions. In this work, we propose a novel reweighing strategy to eliminate this\nperformance gap, increasing small pathology detection performance while\nmaintaining segmentation accuracy. We show that our reweighing strategy vastly\noutperforms competing strategies based on experiments on a large scale,\nmulti-scanner, multi-center dataset of Multiple Sclerosis patient images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Physics-informed Guided Disentanglement in Generative Networks. (arXiv:2107.14229v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.14229","description":"<p>Image-to-image translation (i2i) networks suffer from entanglement effects in\npresence of physics-related phenomena in target domain (such as occlusions,\nfog, etc), lowering altogether the translation quality, controllability and\nvariability. In this paper, we build upon collection of simple physics models\nand present a comprehensive method for disentangling visual traits in target\nimages, guiding the process with a physical model that renders some of the\ntarget traits, and learning the remaining ones. Because it allows explicit and\ninterpretable outputs, our physical models (optimally regressed on target)\nallows generating unseen scenarios in a controllable manner. We also extend our\nframework, showing versatility to neural-guided disentanglement. The results\nshow our disentanglement strategies dramatically increase performances\nqualitatively and quantitatively in several challenging scenarios for image\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pizzati_F/0/1/0/all/0/1\">Fabio Pizzati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cerri_P/0/1/0/all/0/1\">Pietro Cerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.00713","description":"<p>Many automatic machine learning models developed for focal pathology (e.g.\nlesions, tumours) detection and segmentation perform well, but do not\ngeneralize as well to new patient cohorts, impeding their widespread adoption\ninto real clinical contexts. One strategy to create a more diverse,\ngeneralizable training set is to naively pool datasets from different cohorts.\nSurprisingly, training on this \\it{big data} does not necessarily increase, and\nmay even reduce, overall performance and model generalizability, due to the\nexistence of cohort biases that affect label distributions. In this paper, we\npropose a generalized affine conditioning framework to learn and account for\ncohort biases across multi-source datasets, which we call Source-Conditioned\nInstance Normalization (SCIN). Through extensive experimentation on three\ndifferent, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)\nclinical trial MRI datasets, we show that our cohort bias adaptation method (1)\nimproves performance of the network on pooled datasets relative to naively\npooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the\ninstance normalization parameters, thus learning the new cohort bias with only\n10 labelled samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1\">Brennan Nichyporuk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1\">Jillian Cardinell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1\">Justin Szeto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1\">Raghav Mehta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios Tsaftaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1\">Douglas L. Arnold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1\">Tal Arbel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRI: General Reinforced Imitation and its Application to Vision-Based Autonomous Driving. (arXiv:2111.08575v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.08575","description":"<p>Deep reinforcement learning (DRL) has been demonstrated to be effective for\nseveral complex decision-making applications such as autonomous driving and\nrobotics. However, DRL is notoriously limited by its high sample complexity and\nits lack of stability. Prior knowledge, e.g. as expert demonstrations, is often\navailable but challenging to leverage to mitigate these issues. In this paper,\nwe propose General Reinforced Imitation (GRI), a novel method which combines\nbenefits from exploration and expert data and is straightforward to implement\nover any off-policy RL algorithm. We make one simplifying hypothesis: expert\ndemonstrations can be seen as perfect data whose underlying policy gets a\nconstant high reward. Based on this assumption, GRI introduces the notion of\noffline demonstration agents. This agent sends expert data which are processed\nboth concurrently and indistinguishably with the experiences coming from the\nonline RL exploration agent. We show that our approach enables major\nimprovements on vision-based autonomous driving in urban environments. We\nfurther validate the GRI method on Mujoco continuous control tasks with\ndifferent off-policy RL algorithms. Our method ranked first on the CARLA\nLeaderboard and outperforms World on Rails, the previous state-of-the-art, by\n17%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chekroun_R/0/1/0/all/0/1\">Raphael Chekroun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toromanoff_M/0/1/0/all/0/1\">Marin Toromanoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hornauer_S/0/1/0/all/0/1\">Sascha Hornauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moutarde_F/0/1/0/all/0/1\">Fabien Moutarde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIVeR: Real-time and Accurate Neural Radiance Fields with Deterministic Integration for Volume Rendering. (arXiv:2111.10427v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10427","description":"<p>DIVeR builds on the key ideas of NeRF and its variants -- density models and\nvolume rendering -- to learn 3D object models that can be rendered\nrealistically from small numbers of images. In contrast to all previous NeRF\nmethods, DIVeR uses deterministic rather than stochastic estimates of the\nvolume rendering integral. DIVeR's representation is a voxel based field of\nfeatures. To compute the volume rendering integral, a ray is broken into\nintervals, one per voxel; components of the volume rendering integral are\nestimated from the features for each interval using an MLP, and the components\nare aggregated. As a result, DIVeR can render thin translucent structures that\nare missed by other integrators. Furthermore, DIVeR's representation has\nsemantics that is relatively exposed compared to other such methods -- moving\nfeature vectors around in the voxel space results in natural edits. Extensive\nqualitative and quantitative comparisons to current state-of-the-art methods\nshow that DIVeR produces models that (1) render at or above state-of-the-art\nquality, (2) are very small without being baked, (3) render very fast without\nbeing baked, and (4) can be edited in natural ways.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Yong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1\">Anand Bhattad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1\">David Forsyth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MegLoc: A Robust and Accurate Visual Localization Pipeline. (arXiv:2111.13063v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13063","description":"<p>In this paper, we present a visual localization pipeline, namely MegLoc, for\nrobust and accurate 6-DoF pose estimation under varying scenarios, including\nindoor and outdoor scenes, different time across a day, different seasons\nacross a year, and even across years. MegLoc achieves state-of-the-art results\non a range of challenging datasets, including winning the Outdoor and Indoor\nVisual Localization Challenge of ICCV 2021 Workshop on Long-term Visual\nLocalization under Changing Conditions, as well as the Re-localization\nChallenge for Autonomous Driving of ICCV 2021 Workshop on Map-based\nLocalization for Autonomous Driving.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuxue Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haotian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Ran Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chuting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer. (arXiv:2111.13824v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13824","description":"<p>Network quantization significantly reduces model inference complexity and has\nbeen widely used in real-world deployments. However, most existing quantization\nmethods have been developed mainly on Convolutional Neural Networks (CNNs), and\nsuffer severe degradation when applied to fully quantized vision transformers.\nIn this work, we demonstrate that many of these difficulties arise because of\nserious inter-channel variation in LayerNorm inputs, and present, Power-of-Two\nFactor (PTF), a systematic method to reduce the performance degradation and\ninference complexity of fully quantized vision transformers. In addition,\nobserving an extreme non-uniform distribution in attention maps, we propose\nLog-Int-Softmax (LIS) to sustain that and simplify inference by using 4-bit\nquantization and the BitShift operator. Comprehensive experiments on various\ntransformer-based architectures and benchmarks show that our Fully Quantized\nVision Transformer (FQ-ViT) outperforms previous works while even using lower\nbit-width on attention maps. For instance, we reach 84.89% top-1 accuracy with\nViT-L on ImageNet and 50.8 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our\nknowledge, we are the first to achieve lossless accuracy degradation (~1%) on\nfully quantized vision transformers. The code is available at\nhttps://github.com/megvii-research/FQ-ViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peiqin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuchang Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaAfford: Learning to Adapt Manipulation Affordance for 3D Articulated Objects via Few-shot Interactions. (arXiv:2112.00246v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00246","description":"<p>Perceiving and interacting with 3D articulated objects, such as cabinets,\ndoors, and faucets, pose particular challenges for future home-assistant robots\nperforming daily tasks in human environments. Besides parsing the articulated\nparts and joint parameters, researchers recently advocate learning manipulation\naffordance over the input shape geometry which is more task-aware and\ngeometrically fine-grained. However, taking only passive observations as\ninputs, these methods ignore many hidden but important kinematic constraints\n(e.g., joint location and limits) and dynamic factors (e.g., joint friction and\nrestitution), therefore losing significant accuracy for test cases with such\nuncertainties. In this paper, we propose a novel framework, named AdaAfford,\nthat learns to perform very few test-time interactions for quickly adapting the\naffordance priors to more accurate instance-specific posteriors. We conduct\nlarge-scale experiments using the PartNet-Mobility dataset and prove that our\nsystem performs better than baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1\">Jiaqi Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object-aware Video-language Pre-training for Retrieval. (arXiv:2112.00656v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00656","description":"<p>Recently, by introducing large-scale dataset and strong transformer network,\nvideo-language pre-training has shown great success especially for retrieval.\nYet, existing video-language transformer models do not explicitly fine-grained\nsemantic align. In this work, we present Object-aware Transformers, an\nobject-centric approach that extends video-language transformer to incorporate\nobject representations. The key idea is to leverage the bounding boxes and\nobject tags to guide the training process. We evaluate our model on three\nstandard sub-tasks of video-text matching on four widely used benchmarks. We\nalso provide deep analysis and detailed ablation about the proposed method. We\nshow clear improvement in performance across all tasks and datasets considered,\ndemonstrating the value of a model that incorporates object representations\ninto a video-language architecture. The code will be released at\n\\url{https://github.com/FingerRec/OA-Transformer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alex Jinpeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_G/0/1/0/all/0/1\">Guanyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xudong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_X/0/1/0/all/0/1\">Xiaohu Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Interpretation of Public Scenes Using Computer Vision and Temporal Graphs to Identify Social Distancing Violations. (arXiv:2112.06428v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06428","description":"<p>The COVID-19 pandemic has caused an unprecedented global public health\ncrisis. Given its inherent nature, social distancing measures are proposed as\nthe primary strategies to curb the spread of this pandemic. Therefore,\nidentifying situations where these protocols are violated, has implications for\ncurtailing the spread of the disease and promoting a sustainable lifestyle.\nThis paper proposes a novel computer vision-based system to analyze CCTV\nfootage to provide a threat level assessment of COVID-19 spread. The system\nstrives to holistically capture and interpret the information content of CCTV\nfootage spanning multiple frames to recognize instances of various violations\nof social distancing protocols, across time and space, as well as\nidentification of group behaviors. This functionality is achieved primarily by\nutilizing a temporal graph-based structure to represent the information of the\nCCTV footage and a strategy to holistically interpret the graph and quantify\nthe threat level of the given scene. The individual components are tested and\nvalidated on a range of scenarios and the complete system is tested against\nhuman expert opinion. The results reflect the dependence of the threat level on\npeople, their physical proximity, interactions, protective clothing, and group\ndynamics. The system performance has an accuracy of 76%, thus enabling a\ndeployable threat monitoring system in cities, to permit normalcy and\nsustainability in the society.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jayatilaka_G/0/1/0/all/0/1\">Gihan Jayatilaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_J/0/1/0/all/0/1\">Jameel Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sritharan_S/0/1/0/all/0/1\">Suren Sritharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senananayaka_J/0/1/0/all/0/1\">Janith Bandara Senananayaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weligampola_H/0/1/0/all/0/1\">Harshana Weligampola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Godaliyadda_R/0/1/0/all/0/1\">Roshan Godaliyadda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_P/0/1/0/all/0/1\">Parakrama Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_V/0/1/0/all/0/1\">Vijitha Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekanayake_J/0/1/0/all/0/1\">Janaka Ekanayake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharmaratne_S/0/1/0/all/0/1\">Samath Dharmaratne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust photon-efficient imaging using a pixel-wise residual shrinkage network. (arXiv:2201.01453v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01453","description":"<p>Single-photon light detection and ranging (LiDAR) has been widely applied to\n3D imaging in challenging scenarios. However, limited signal photon counts and\nhigh noises in the collected data have posed great challenges for predicting\nthe depth image precisely. In this paper, we propose a pixel-wise residual\nshrinkage network for photon-efficient imaging from high-noise data, which\nadaptively generates the optimal thresholds for each pixel and denoises the\nintermediate features by soft thresholding. Besides, redefining the\noptimization target as pixel-wise classification provides a sharp advantage in\nproducing confident and accurate depth estimation when compared with existing\nresearch. Comprehensive experiments conducted on both simulated and real-world\ndatasets demonstrate that the proposed model outperforms the state-of-the-arts\nand maintains robust imaging performance under different signal-to-noise ratios\nincluding the extreme case of 1:100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yao_G/0/1/0/all/0/1\">Gongxin Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaomin Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Cut Once: Boosting Data Augmentation with a Single Cut. (arXiv:2201.12078v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12078","description":"<p>We present You Only Cut Once (YOCO) for performing data augmentations. YOCO\ncuts one image into two pieces and performs data augmentations individually\nwithin each piece. Applying YOCO improves the diversity of the augmentation per\nsample and encourages neural networks to recognize objects from partial\ninformation. YOCO enjoys the properties of parameter-free, easy usage, and\nboosting almost all augmentations for free. Thorough experiments are conducted\nto evaluate its effectiveness. We first demonstrate that YOCO can be seamlessly\napplied to varying data augmentations, neural network architectures, and brings\nperformance gains on CIFAR and ImageNet classification tasks, sometimes\nsurpassing conventional image-level augmentation by large margins. Moreover, we\nshow YOCO benefits contrastive pre-training toward a more powerful\nrepresentation that can be better transferred to multiple downstream tasks.\nFinally, we study a number of variants of YOCO and empirically analyze the\nperformance for respective settings. Code is available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UQGAN: A Unified Model for Uncertainty Quantification of Deep Classifiers trained via Conditional GANs. (arXiv:2201.13279v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13279","description":"<p>We present an approach to quantifying both aleatoric and epistemic\nuncertainty for deep neural networks in image classification, based on\ngenerative adversarial networks (GANs). While most works in the literature that\nuse GANs to generate out-of-distribution (OoD) examples only focus on the\nevaluation of OoD detection, we present a GAN based approach to learn a\nclassifier that produces proper uncertainties for OoD examples as well as for\nfalse positives (FPs). Instead of shielding the entire in-distribution data\nwith GAN generated OoD examples which is state-of-the-art, we shield each class\nseparately with out-of-class examples generated by a conditional GAN and\ncomplement this with a one-vs-all image classifier. In our experiments, in\nparticular on CIFAR10 and CIFAR100, we improve over the OoD detection and FP\ndetection performance of state-of-the-art GAN-training based classifiers.\nFurthermore, we also find that the generated GAN examples do not significantly\naffect the calibration error of our classifier and result in a significant gain\nin model accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oberdiek_P/0/1/0/all/0/1\">Philipp Oberdiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fink_G/0/1/0/all/0/1\">Gernot A. Fink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computing Multiple Image Reconstructions with a Single Hypernetwork. (arXiv:2202.11009v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11009","description":"<p>Deep learning based techniques achieve state-of-the-art results in a wide\nrange of image reconstruction tasks like compressed sensing. These methods\nalmost always have hyperparameters, such as the weight coefficients that\nbalance the different terms in the optimized loss function. The typical\napproach is to train the model for a hyperparameter setting determined with\nsome empirical or theoretical justification. Thus, at inference time, the model\ncan only compute reconstructions corresponding to the pre-determined\nhyperparameter values. In this work, we present a hypernetwork-based approach,\ncalled HyperRecon, to train reconstruction models that are agnostic to\nhyperparameter settings. At inference time, HyperRecon can efficiently produce\ndiverse reconstructions, which would each correspond to different\nhyperparameter values. In this framework, the user is empowered to select the\nmost useful output(s) based on their own judgement. We demonstrate our method\nin compressed sensing, super-resolution and denoising tasks, using two\nlarge-scale and publicly-available MRI datasets. Our code is available at\nhttps://github.com/alanqrwang/hyperrecon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mobile authentication of copy detection patterns. (arXiv:2203.02397v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2203.02397","description":"<p>In the recent years, the copy detection patterns (CDP) attracted a lot of\nattention as a link between the physical and digital worlds, which is of great\ninterest for the internet of things and brand protection applications. However,\nthe security of CDP in terms of their reproducibility by unauthorized parties\nor clonability remains largely unexplored. In this respect this paper addresses\na problem of anti-counterfeiting of physical objects and aims at investigating\nthe authentication aspects and the resistances to illegal copying of the modern\nCDP from machine learning perspectives. A special attention is paid to a\nreliable authentication under the real life verification conditions when the\ncodes are printed on an industrial printer and enrolled via modern mobile\nphones under regular light conditions. The theoretical and empirical\ninvestigation of authentication aspects of CDP is performed with respect to\nfour types of copy fakes from the point of view of (i) multi-class supervised\nclassification as a baseline approach and (ii) one-class classification as a\nreal-life application case. The obtained results show that the modern\nmachine-learning approaches and the technical capacities of modern mobile\nphones allow to reliably authenticate CDP on end-user mobile phones under the\nconsidered classes of fakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taran_O/0/1/0/all/0/1\">Olga Taran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tutt_J/0/1/0/all/0/1\">Joakim Tutt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holotyak_T/0/1/0/all/0/1\">Taras Holotyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaban_R/0/1/0/all/0/1\">Roman Chaban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonev_S/0/1/0/all/0/1\">Slavi Bonev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1\">Slava Voloshynovskiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection. (arXiv:2203.12121v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12121","description":"<p>Current polyp detection methods from colonoscopy videos use exclusively\nnormal (i.e., healthy) training images, which i) ignore the importance of\ntemporal information in consecutive video frames, and ii) lack knowledge about\nthe polyps. Consequently, they often have high detection errors, especially on\nchallenging polyp cases (e.g., small, flat, or partially visible polyps). In\nthis work, we formulate polyp detection as a weakly-supervised anomaly\ndetection task that uses video-level labelled training data to detect\nframe-level polyps. In particular, we propose a novel convolutional\ntransformer-based multiple instance learning method designed to identify\nabnormal frames (i.e., frames with polyps) from anomalous videos (i.e., videos\ncontaining at least one frame with polyp). In our method, local and global\ntemporal dependencies are seamlessly captured while we simultaneously optimise\nvideo and snippet-level anomaly scores. A contrastive snippet mining method is\nalso proposed to enable an effective modelling of the challenging polyp cases.\nThe resulting method achieves a detection accuracy that is substantially better\nthan current state-of-the-art approaches on a new large-scale colonoscopy video\ndataset introduced in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoTrack: Shuttle trajectory reconstruction from monocular badminton video. (arXiv:2204.01899v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01899","description":"<p>Trajectory estimation is a fundamental component of racket sport analytics,\nas the trajectory contains information not only about the winning and losing of\neach point, but also how it was won or lost. In sports such as badminton,\nplayers benefit from knowing the full 3D trajectory, as the height of\nshuttlecock or ball provides valuable tactical information. Unfortunately, 3D\nreconstruction is a notoriously hard problem, and standard trajectory\nestimators can only track 2D pixel coordinates. In this work, we present the\nfirst complete end-to-end system for the extraction and segmentation of 3D\nshuttle trajectories from monocular badminton videos. Our system integrates\nbadminton domain knowledge such as court dimension, shot placement, physical\nlaws of motion, along with vision-based features such as player poses and\nshuttle tracking. We find that significant engineering efforts and model\nimprovements are needed to make the overall system robust, and as a by-product\nof our work, improve state-of-the-art results on court recognition, 2D\ntrajectory estimation, and hit recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Paul Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jui-Hsien Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation. (arXiv:2204.10762v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10762","description":"<p>A high-resolution network exhibits remarkable capability in extracting\nmulti-scale features for human pose estimation, but fails to capture long-range\ninteractions between joints and has high computational complexity. To address\nthese problems, we present a Dynamic lightweight High-Resolution Network\n(Dite-HRNet), which can efficiently extract multi-scale contextual information\nand model long-range spatial dependency for human pose estimation.\nSpecifically, we propose two methods, dynamic split convolution and adaptive\ncontext modeling, and embed them into two novel lightweight blocks, which are\nnamed dynamic multi-scale context block and dynamic global context block. These\ntwo blocks, as the basic component units of our Dite-HRNet, are specially\ndesigned for the high-resolution networks to make full use of the parallel\nmulti-resolution architecture. Experimental results show that the proposed\nnetwork achieves superior performance on both COCO and MPII human pose\nestimation datasets, surpassing the state-of-the-art lightweight networks. Code\nis available at: \\url{https://github.com/ZiyiZhang27/Dite-HRNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_F/0/1/0/all/0/1\">Fu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhanu_B/0/1/0/all/0/1\">Bir Bhanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transferability for Covid 3D Localization Using CT SARS-CoV-2 segmentation models. (arXiv:2205.02152v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2205.02152","description":"<p>Recent studies indicate that detecting radiographic patterns on CT scans can\nyield high sensitivity and specificity for Covid-19 localization. In this\npaper, we investigate the appropriateness of deep learning models\ntransferability, for semantic segmentation of pneumonia-infected areas in CT\nimages. Transfer learning allows for the fast initialization/reutilization of\ndetection models, given that large volumes of training data are not available.\nOur work explores the efficacy of using pre-trained U-Net architectures, on a\nspecific CT data set, for identifying Covid-19 side-effects over images from\ndifferent datasets. Experimental results indicate improvement in the\nsegmentation accuracy of identifying Covid-19 infected regions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Maganaris_C/0/1/0/all/0/1\">Constantine Maganaris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakalos_N/0/1/0/all/0/1\">Nikolaos Bakalos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalogeras_D/0/1/0/all/0/1\">Dimitris Kalogeras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angeli_A/0/1/0/all/0/1\">Aikaterini Angeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural 3D Scene Reconstruction with the Manhattan-world Assumption. (arXiv:2205.02836v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.02836","description":"<p>This paper addresses the challenge of reconstructing 3D indoor scenes from\nmulti-view images. Many previous works have shown impressive reconstruction\nresults on textured objects, but they still have difficulty in handling\nlow-textured planar regions, which are common in indoor scenes. An approach to\nsolving this issue is to incorporate planer constraints into the depth map\nestimation in multi-view stereo-based methods, but the per-view plane\nestimation and depth optimization lack both efficiency and multi-view\nconsistency. In this work, we show that the planar constraints can be\nconveniently integrated into the recent implicit neural representation-based\nreconstruction methods. Specifically, we use an MLP network to represent the\nsigned distance function as the scene geometry. Based on the Manhattan-world\nassumption, planar constraints are employed to regularize the geometry in floor\nand wall regions predicted by a 2D semantic segmentation network. To resolve\nthe inaccurate segmentation, we encode the semantics of 3D points with another\nMLP and design a novel loss that jointly optimizes the scene geometry and\nsemantics in 3D space. Experiments on ScanNet and 7-Scenes datasets show that\nthe proposed method outperforms previous methods by a large margin on 3D\nreconstruction quality. The code is available at\nhttps://zju3dv.github.io/manhattan_sdf.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haoyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Sida Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haotong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qianqian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guofeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LatentKeypointGAN: Controlling Images via Latent Keypoints -- Extended Abstract. (arXiv:2205.03448v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.03448","description":"<p>Generative adversarial networks (GANs) can now generate photo-realistic\nimages. However, how to best control the image content remains an open\nchallenge. We introduce LatentKeypointGAN, a two-stage GAN internally\nconditioned on a set of keypoints and associated appearance embeddings\nproviding control of the position and style of the generated objects and their\nrespective parts. A major difficulty that we address is disentangling the image\ninto spatial and appearance factors with little domain knowledge and\nsupervision signals. We demonstrate in a user study and quantitative\nexperiments that LatentKeypointGAN provides an interpretable latent space that\ncan be used to re-arrange the generated images by re-positioning and exchanging\nkeypoint embeddings, such as generating portraits by combining the eyes, and\nmouth from different images. Notably, our method does not require labels as it\nis self-supervised and thereby applies to diverse application domains, such as\nediting portraits, indoor rooms, and full-body human poses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identical Image Retrieval using Deep Learning. (arXiv:2205.04883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.04883","description":"<p>In recent years, we know that the interaction with images has increased.\nImage similarity involves fetching similar-looking images abiding by a given\nreference image. The target is to find out whether the image searched as a\nquery can result in similar pictures. We are using the BigTransfer Model, which\nis a state-of-art model itself. BigTransfer(BiT) is essentially a ResNet but\npre-trained on a larger dataset like ImageNet and ImageNet-21k with additional\nmodifications. Using the fine-tuned pre-trained Convolution Neural Network\nModel, we extract the key features and train on the K-Nearest Neighbor model to\nobtain the nearest neighbor. The application of our model is to find similar\nimages, which are hard to achieve through text queries within a low inference\ntime. We analyse the benchmark of our model based on this application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1\">Sayan Nath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nikhil Nayak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender and Racial Bias in Visual Question Answering Datasets. (arXiv:2205.08148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08148","description":"<p>Vision-and-language tasks have increasingly drawn more attention as a means\nto evaluate human-like reasoning in machine learning models. A popular task in\nthe field is visual question answering (VQA), which aims to answer questions\nabout images. However, VQA models have been shown to exploit language bias by\nlearning the statistical correlations between questions and answers without\nlooking into the image content: e.g., questions about the color of a banana are\nanswered with yellow, even if the banana in the image is green. If societal\nbias (e.g., sexism, racism, ableism, etc.) is present in the training data,\nthis problem may be causing VQA models to learn harmful stereotypes. For this\nreason, we investigate gender and racial bias in five VQA datasets. In our\nanalysis, we find that the distribution of answers is highly different between\nquestions about women and men, as well as the existence of detrimental\ngender-stereotypical samples. Likewise, we identify that specific race-related\nattributes are underrepresented, whereas potentially discriminatory samples\nappear in the analyzed datasets. Our findings suggest that there are dangers\nassociated to using VQA datasets without considering and dealing with the\npotentially harmful stereotypes. We conclude the paper by proposing solutions\nto alleviate the problem before, during, and after the dataset collection\nprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1\">Yusuke Hirota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer Adapter for Dense Predictions. (arXiv:2205.08534v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.08534","description":"<p>This work investigates a simple yet powerful adapter for Vision Transformer\n(ViT). Unlike recent visual transformers that introduce vision-specific\ninductive biases into their architectures, ViT achieves inferior performance on\ndense prediction tasks due to lacking prior information of images. To solve\nthis issue, we propose a Vision Transformer Adapter (ViT-Adapter), which can\nremedy the defects of ViT and achieve comparable performance to vision-specific\nmodels by introducing inductive biases via an additional architecture.\nSpecifically, the backbone in our framework is a vanilla transformer that can\nbe pre-trained with multi-modal data. When fine-tuning on downstream tasks, a\nmodality-specific adapter is used to introduce the data and tasks' prior\ninformation into the model, making it suitable for these tasks. We verify the\neffectiveness of our ViT-Adapter on multiple downstream tasks, including object\ndetection, instance segmentation, and semantic segmentation. Notably, when\nusing HTC++, our ViT-Adapter-L yields 60.1 box AP and 52.1 mask AP on COCO\ntest-dev, surpassing Swin-L by 1.4 box AP and 1.0 mask AP. For semantic\nsegmentation, our ViT-Adapter-L establishes a new state-of-the-art of 60.5 mIoU\non ADE20K val, 0.6 points higher than SwinV2-G. We hope that the proposed\nViT-Adapter could serve as an alternative for vision-specific transformers and\nfacilitate future research. The code and models will be released at\nhttps://github.com/czczup/ViT-Adapter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuchen Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time semantic segmentation on FPGAs for autonomous vehicles with hls4ml. (arXiv:2205.07690v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2205.07690","description":"<p>In this paper, we investigate how field programmable gate arrays can serve as\nhardware accelerators for real-time semantic segmentation tasks relevant for\nautonomous driving. Considering compressed versions of the ENet convolutional\nneural network architecture, we demonstrate a fully-on-chip deployment with a\nlatency of 4.9 ms per image, using less than 30% of the available resources on\na Xilinx ZCU102 evaluation board. The latency is reduced to 3 ms per image when\nincreasing the batch size to ten, corresponding to the use case where the\nautonomous vehicle receives inputs from multiple cameras simultaneously. We\nshow, through aggressive filter reduction and heterogeneous quantization-aware\ntraining, and an optimized implementation of convolutional layers, that the\npower consumption and resource utilization can be significantly reduced while\nmaintaining accuracy on the Cityscapes dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghielmetti_N/0/1/0/all/0/1\">Nicol&#xf2; Ghielmetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loncar_V/0/1/0/all/0/1\">Vladimir Loncar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roed_M/0/1/0/all/0/1\">Marcel Roed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_S/0/1/0/all/0/1\">Sioni Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aarrestad_T/0/1/0/all/0/1\">Thea Aarrestad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linander_H/0/1/0/all/0/1\">Hampus Linander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngadiuba_J/0/1/0/all/0/1\">Jennifer Ngadiuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kelvin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harris_P/0/1/0/all/0/1\">Philip Harris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-05-18T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}