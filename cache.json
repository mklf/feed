{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Data Bootstrapping Approaches to Improve Low Resource Abusive Language Detection for Indic Languages. (arXiv:2204.12543v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12543","description":"<p>Abusive language is a growing concern in many social media platforms.\nRepeated exposure to abusive speech has created physiological effects on the\ntarget users. Thus, the problem of abusive language should be addressed in all\nforms for online peace and safety. While extensive research exists in abusive\nspeech detection, most studies focus on English. Recently, many smearing\nincidents have occurred in India, which provoked diverse forms of abusive\nspeech in online space in various languages based on the geographic location.\nTherefore it is essential to deal with such malicious content. In this paper,\nto bridge the gap, we demonstrate a large-scale analysis of multilingual\nabusive speech in Indic languages. We examine different interlingual transfer\nmechanisms and observe the performance of various multilingual models for\nabusive speech detection for eight different Indic languages. We also\nexperiment to show how robust these models are on adversarial attacks. Finally,\nwe conduct an in-depth error analysis by looking into the models' misclassified\nposts across various settings. We have made our code and models public for\nother researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Somnath Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parkinson's disease diagnostics using AI and natural language knowledge transfer. (arXiv:2204.12559v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12559","description":"<p>In this work, the issue of Parkinson's disease (PD) diagnostics using\nnon-invasive antemortem techniques was tackled. A deep learning approach for\nclassification of raw speech recordings in patients with diagnosed PD was\nproposed. The core of proposed method is an audio classifier using knowledge\ntransfer from a pretrained natural language model, namely \\textit{wav2vec 2.0}.\nMethod was tested on a group of 38 PD patients and 10 healthy persons above the\nage of 50. A dataset of speech recordings acquired using a smartphone recorder\nwas constructed and the recordings were label as PD/non-PD with severity of the\ndisease additionally rated using Hoehn-Yahr scale. The audio recordings were\ncut into 2141 samples that include sentences, syllables, vowels and sustained\nphonation. The classifier scores up to 97.92\\% of cross-validated accuracy.\nAdditionally, paper presents results of a human-level performance assessment\nquestionnaire, which was consulted with the neurology professionals\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chronowski_M/0/1/0/all/0/1\">Maurycy Chronowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klaczynski_M/0/1/0/all/0/1\">Maciej Klaczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dec_Cwiek_M/0/1/0/all/0/1\">Malgorzata Dec-Cwiek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porebska_K/0/1/0/all/0/1\">Karolina Porebska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for Tamil TrollMeme Classification. (arXiv:2204.12587v1 [cs.MM])","link":"http://arxiv.org/abs/2204.12587","description":"<p>Social media platforms often act as breeding grounds for various forms of\ntrolling or malicious content targeting users or communities. One way of\ntrolling users is by creating memes, which in most cases unites an image with a\nshort piece of text embedded on top of it. The situation is more complex for\nmultilingual(e.g., Tamil) memes due to the lack of benchmark datasets and\nmodels. We explore several models to detect Troll memes in Tamil based on the\nshared task, \"Troll Meme Classification in DravidianLangTech2022\" at ACL-2022.\nWe observe while the text-based model MURIL performs better for Non-troll meme\nclassification, the image-based model VGG16 performs better for Troll-meme\nclassification. Further fusing these two modalities help us achieve stable\noutcomes in both classes. Our fusion model achieved a 0.561 weighted average F1\nscore and ranked second in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Somnath Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Machine Translation Domain Adaptation. (arXiv:2204.12608v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12608","description":"<p>Machine translation models struggle when translating out-of-domain text,\nwhich makes domain adaptation a topic of critical importance. However, most\ndomain adaptation methods focus on fine-tuning or training the entire or part\nof the model on every new domain, which can be costly. On the other hand,\nsemi-parametric models have been shown to successfully perform domain\nadaptation by retrieving examples from an in-domain datastore (Khandelwal et\nal., 2021). A drawback of these retrieval-augmented models, however, is that\nthey tend to be substantially slower. In this paper, we explore several\napproaches to speed up nearest neighbor machine translation. We adapt the\nmethods recently proposed by He et al. (2021) for language modeling, and\nintroduce a simple but effective caching strategy that avoids performing\nretrieval when similar contexts have been seen before. Translation quality and\nruntimes for several domains show the effectiveness of the proposed solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1\">Pedro Henrique Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marinho_Z/0/1/0/all/0/1\">Zita Marinho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Testing the Ability of Language Models to Interpret Figurative Language. (arXiv:2204.12632v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12632","description":"<p>Figurative and metaphorical language are commonplace in discourse, and\nfigurative expressions play an important role in communication and cognition.\nHowever, figurative language has been a relatively under-studied area in NLP,\nand it remains an open question to what extent modern language models can\ninterpret nonliteral phrases. To address this question, we introduce Fig-QA, a\nWinograd-style nonliteral language understanding task consisting of correctly\ninterpreting paired figurative phrases with divergent meanings. We evaluate the\nperformance of several state-of-the-art language models on this task, and find\nthat although language models achieve performance significantly over chance,\nthey still fall short of human performance, particularly in zero- or few-shot\nsettings. This suggests that further work is needed to improve the nonliteral\nreasoning capabilities of language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1\">Emmy Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chen Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kenneth Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Developing Universal Dependency Treebanks for Magahi and Braj. (arXiv:2204.12633v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12633","description":"<p>In this paper, we discuss the development of treebanks for two low-resourced\nIndian languages - Magahi and Braj based on the Universal Dependencies\nframework. The Magahi treebank contains 945 sentences and Braj treebank around\n500 sentences marked with their lemmas, part-of-speech, morphological features\nand universal dependencies. This paper gives a description of the different\ndependency relationship found in the two languages and give some statistics of\nthe two treebanks. The dataset will be made publicly available on Universal\nDependency (UD) repository\n(https://github.com/UniversalDependencies/UD_Magahi-MGTB/tree/master) in the\nnext(v2.10) release.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raj_M/0/1/0/all/0/1\">Mohit Raj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratan_S/0/1/0/all/0/1\">Shyam Ratan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alok_D/0/1/0/all/0/1\">Deepak Alok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Ritesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ojha_A/0/1/0/all/0/1\">Atul Kr. Ojha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Executive Function: A Contrastive Value Policy for Resampling and Relabeling Perceptions via Hindsight Summarization?. (arXiv:2204.12639v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12639","description":"<p>We develop the few-shot continual learning task from first principles and\nhypothesize an evolutionary motivation and mechanism of action for executive\nfunction as a contrastive value policy which resamples and relabels perception\ndata via hindsight summarization to minimize attended prediction error, similar\nto an online prompt engineering problem. This is made feasible by the use of a\nmemory policy and a pretrained network with inductive biases for a grammar of\nlearning and is trained to maximize evolutionary survival. We show how this\nmodel of executive function can be used to implement hypothesis testing as a\nstream of consciousness and may explain observations of human few-shot learning\nand neuroanatomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lengerich_C/0/1/0/all/0/1\">Chris Lengerich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1\">Ben Lengerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Better Query Graph Selection for Knowledge Base Question Answering. (arXiv:2204.12662v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12662","description":"<p>This paper presents a novel approach based on semantic parsing to improve the\nperformance of Knowledge Base Question Answering (KBQA). Specifically, we focus\non how to select an optimal query graph from a candidate set so as to retrieve\nthe answer from knowledge base (KB). In our approach, we first propose to\nlinearize the query graph into a sequence, which is used to form a sequence\npair with the question. It allows us to use mature sequence modeling, such as\nBERT, to encode the sequence pair. Then we use a ranking method to sort\ncandidate query graphs. In contrast to the previous studies, our approach can\nefficiently model semantic interactions between the graph and the question as\nwell as rank the candidate graphs from a global view. The experimental results\nshow that our system achieves the top performance on ComplexQuestions and the\nsecond best performance on WebQuestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yonghui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptable Text Matching via Meta-Weight Regulator. (arXiv:2204.12668v1 [cs.IR])","link":"http://arxiv.org/abs/2204.12668","description":"<p>Neural text matching models have been used in a range of applications such as\nquestion answering and natural language inference, and have yielded a good\nperformance. However, these neural models are of a limited adaptability,\nresulting in a decline in performance when encountering test examples from a\ndifferent dataset or even a different task. The adaptability is particularly\nimportant in the few-shot setting: in many cases, there is only a limited\namount of labeled data available for a target dataset or task, while we may\nhave access to a richly labeled source dataset or task. However, adapting a\nmodel trained on the abundant source data to a few-shot target dataset or task\nis challenging. To tackle this challenge, we propose a Meta-Weight Regulator\n(MWR), which is a meta-learning approach that learns to assign weights to the\nsource examples based on their relevance to the target loss. Specifically, MWR\nfirst trains the model on the uniformly weighted source examples, and measures\nthe efficacy of the model on the target examples via a loss function. By\niteratively performing a (meta) gradient descent, high-order gradients are\npropagated to the source examples. These gradients are then used to update the\nweights of source examples, in a way that is relevant to the target\nperformance. As MWR is model-agnostic, it can be applied to any backbone neural\nmodel. Extensive experiments are conducted with various backbone text matching\nmodels, on four widely used datasets and two tasks. The results demonstrate\nthat our proposed approach significantly outperforms a number of existing\nadaptation methods and effectively improves the cross-dataset and cross-task\nadaptability of the neural text matching models in the few-shot setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawei Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Driven Adaptive Simultaneous Machine Translation. (arXiv:2204.12672v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12672","description":"<p>In simultaneous translation (SimulMT), the most widely used strategy is the\nwait-k policy thanks to its simplicity and effectiveness in balancing\ntranslation quality and latency. However, wait-k suffers from two major\nlimitations: (a) it is a fixed policy that can not adaptively adjust latency\ngiven context, and (b) its training is much slower than full-sentence\ntranslation. To alleviate these issues, we propose a novel and efficient\ntraining scheme for adaptive SimulMT by augmenting the training corpus with\nadaptive prefix-to-prefix pairs, while the training complexity remains the same\nas that of training full-sentence translation models. Experiments on two\nlanguage pairs show that our method outperforms all strong baselines in terms\nof translation quality and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xun_G/0/1/0/all/0/1\">Guangxu Xun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1\">Yuchen Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xingyu Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaji Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Jiahong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1\">Kenneth Church</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Span-level Bidirectional Cross-attention Framework for Aspect Sentiment Triplet Extraction. (arXiv:2204.12674v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12674","description":"<p>Aspect Sentiment Triplet Extraction (ASTE) is a new fine-grained sentiment\nanalysis task that aims to extract triplets of aspect terms, sentiments, and\nopinion terms from review sentences. Recently, span-level models achieve\ngratifying results on ASTE task by taking advantage of whole span predictions.\nHowever, all the spans generated by these methods inevitably share at least one\ntoken with some others, and these method suffer from the similarity of these\nspans due to their similar distributions. Moreover, since either the aspect\nterm or opinion term can trigger a sentiment triplet, it is challenging to make\nuse of the information more comprehensively and adequately. To address these\nconcerns, we propose a span-level bidirectional cross-attention framework.\nSpecifically, we design a similar span separation loss to detach the spans with\nshared tokens and a bidirectional cross-attention structure that consists of\naspect and opinion decoders to decode the span-level representations in both\naspect-to-opinion and opinion-to-aspect directions. With differentiated span\nrepresentations and bidirectional decoding structure, our model can extract\nsentiment triplets more precisely and efficiently. Experimental results show\nthat our framework significantly outperforms state-of-the-art methods,\nachieving better performance in predicting triplets with multi-token entities\nand extracting triplets in sentences with multi-triplets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Keming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zequn Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-Level Relation Extraction with Sentences Importance Estimation and Focusing. (arXiv:2204.12679v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12679","description":"<p>Document-level relation extraction (DocRE) aims to determine the relation\nbetween two entities from a document of multiple sentences. Recent studies\ntypically represent the entire document by sequence- or graph-based models to\npredict the relations of all entity pairs. However, we find that such a model\nis not robust and exhibits bizarre behaviors: it predicts correctly when an\nentire test document is fed as input, but errs when non-evidence sentences are\nremoved. To this end, we propose a Sentence Importance Estimation and Focusing\n(SIEF) framework for DocRE, where we design a sentence importance score and a\nsentence focusing loss, encouraging DocRE models to focus on evidence\nsentences. Experimental results on two domains show that our SIEF not only\nimproves overall performance, but also makes DocRE models more robust.\nMoreover, SIEF is a general framework, shown to be effective when combined with\na variety of base DocRE models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kehai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lili Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"$G^2$: Enhance Knowledge Grounded Dialogue via Ground Graph. (arXiv:2204.12681v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12681","description":"<p>Knowledge grounded dialogue system is designed to generate responses that\nconvey information from given knowledge documents. However, it's a challenge\nfor the current Seq2Seq model to acquire knowledge from complex documents and\nintegrate it to perform correct responses without the aid of an explicit\nsemantic structure. To address these issues, we present a novel graph\nstructure, Ground Graph ($G^2$), which models the semantic structure of both\ndialogue contexts and knowledge documents to facilitate knowledge selection and\nintegration for the task. Besides, a Ground Graph Aware Transformer ($G^2AT$)\nis proposed to enhance knowledge grounded response generation. Empirical\nresults show that our proposed model outperforms previous state-of-the-art\nmethods with more than 10\\% and 20\\% gains on response generation and factual\nconsistency. Furthermore, our structure-aware approach shows excellent\ngeneralization ability in resource-limited situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yizhe Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distant finetuning with discourse relations for stance classification. (arXiv:2204.12693v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12693","description":"<p>Approaches for the stance classification task, an important task for\nunderstanding argumentation in debates and detecting fake news, have been\nrelying on models which deal with individual debate topics. In this paper, in\norder to train a system independent from topics, we propose a new method to\nextract data with silver labels from raw text to finetune a model for stance\nclassification. The extraction relies on specific discourse relation\ninformation, which is shown as a reliable and accurate source for providing\nstance information. We also propose a 3-stage training framework where the\nnoisy level in the data used for finetuning decreases over different stages\ngoing from the most noisy to the least noisy. Detailed experiments show that\nthe automatically annotated dataset as well as the 3-stage training help\nimprove model performance in stance classification. Our approach ranks 1st\namong 26 competing teams in the stance classification track of the NLPCC 2021\nshared task Argumentative Text Understanding for AI Debater, which confirms the\neffectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lifeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Linfeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Limitations of Dataset Balancing: The Lost Battle Against Spurious Correlations. (arXiv:2204.12708v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12708","description":"<p>Recent work has shown that deep learning models in NLP are highly sensitive\nto low-level correlations between simple features and specific output labels,\nleading to overfitting and lack of generalization. To mitigate this problem, a\ncommon practice is to balance datasets by adding new instances or by filtering\nout \"easy\" instances (Sakaguchi et al., 2020), culminating in a recent proposal\nto eliminate single-word correlations altogether (Gardner et al., 2021). In\nthis opinion paper, we identify that despite these efforts,\nincreasingly-powerful models keep exploiting ever-smaller spurious\ncorrelations, and as a result even balancing all single-word features is\ninsufficient for mitigating all of these correlations. In parallel, a truly\nbalanced dataset may be bound to \"throw the baby out with the bathwater\" and\nmiss important signal encoding common sense and world knowledge. We highlight\nseveral alternatives to dataset balancing, focusing on enhancing datasets with\nricher contexts, allowing models to abstain and interact with users, and\nturning from large-scale fine-tuning to zero- or few-shot setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREER: A Large-Scale Corpus for Relation Extraction and Entity Recognition. (arXiv:2204.12710v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12710","description":"<p>We describe the design and use of the CREER dataset, a large corpus annotated\nwith rich English grammar and semantic attributes. The CREER dataset uses the\nStanford CoreNLP Annotator to capture rich language structures from Wikipedia\nplain text. This dataset follows widely used linguistic and semantic\nannotations so that it can be used for not only most natural language\nprocessing tasks but also scaling the dataset. This large supervised dataset\ncan serve as the basis for improving the performance of NLP tasks in the\nfuture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Siou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chung-Hsien Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UBERT: A Novel Language Model for Synonymy Prediction at Scale in the UMLS Metathesaurus. (arXiv:2204.12716v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12716","description":"<p>The UMLS Metathesaurus integrates more than 200 biomedical source\nvocabularies. During the Metathesaurus construction process, synonymous terms\nare clustered into concepts by human editors, assisted by lexical similarity\nalgorithms. This process is error-prone and time-consuming. Recently, a deep\nlearning model (LexLM) has been developed for the UMLS Vocabulary Alignment\n(UVA) task. This work introduces UBERT, a BERT-based language model, pretrained\non UMLS terms via a supervised Synonymy Prediction (SP) task replacing the\noriginal Next Sentence Prediction (NSP) task. The effectiveness of UBERT for\nUMLS Metathesaurus construction process is evaluated using the UMLS Vocabulary\nAlignment (UVA) task. We show that UBERT outperforms the LexLM, as well as\nbiomedical BERT-based models. Key to the performance of UBERT are the synonymy\nprediction task specifically developed for UBERT, the tight alignment of\ntraining data to the UVA task, and the similarity of the models used for\npretrained UBERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_G/0/1/0/all/0/1\">Goonmeet Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_H/0/1/0/all/0/1\">Hong Yung Yip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javangula_V/0/1/0/all/0/1\">Vishesh Javangula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuqing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_K/0/1/0/all/0/1\">Kin Wah Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit P. Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenreider_O/0/1/0/all/0/1\">Olivier Bodenreider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition. (arXiv:2204.12732v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12732","description":"<p>Nested named entity recognition (nested NER) is a fundamental task in natural\nlanguage processing. Various span-based methods have been proposed to detect\nnested entities with span representations. However, span-based methods do not\nconsider the relationship between a span and other entities or phrases, which\nis helpful in the NER task. Besides, span-based methods have trouble predicting\nlong entities due to limited span enumeration length. To mitigate these issues,\nwe present the Propose-and-Refine Network (PnRNet), a two-stage set prediction\nnetwork for nested NER. In the propose stage, we use a span-based predictor to\ngenerate some coarse entity predictions as entity proposals. In the refine\nstage, proposals interact with each other, and richer contextual information is\nincorporated into the proposal representations. The refined proposal\nrepresentations are used to re-predict entity boundaries and classes. In this\nway, errors in coarse proposals can be eliminated, and the boundary prediction\nis no longer constrained by the span enumeration length limitation.\nAdditionally, we build multi-scale sentence representations, which better model\nthe hierarchical structure of sentences and provide richer contextual\ninformation than token-level representations. Experiments show that PnRNet\nachieves state-of-the-art performance on four nested NER datasets and one flat\nNER dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuhui Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yongliang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zeqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Weiming Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Control Globally, Understand Locally: A Global-to-Local Hierarchical Graph Network for Emotional Support Conversation. (arXiv:2204.12749v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12749","description":"<p>Emotional support conversation aims at reducing the emotional distress of the\nhelp-seeker, which is a new and challenging task. It requires the system to\nexplore the cause of help-seeker's emotional distress and understand their\npsychological intention to provide supportive responses. However, existing\nmethods mainly focus on the sequential contextual information, ignoring the\nhierarchical relationships with the global cause and local psychological\nintention behind conversations, thus leads to a weak ability of emotional\nsupport. In this paper, we propose a Global-to-Local Hierarchical Graph Network\nto capture the multi-source information (global cause, local intentions and\ndialog history) and model hierarchical relationships between them, which\nconsists of a multi-source encoder, a hierarchical graph reasoner, and a\nglobal-guide decoder. Furthermore, a novel training objective is designed to\nmonitor semantic information of the global cause. Experimental results on the\nemotional support conversation dataset, ESConv, confirm that the proposed GLHG\nhas achieved the state-of-the-art performance on the automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Wei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1\">Luxi Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuqiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yajing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Understanding of Code-mixed Language Semantics using Hierarchical Transformer. (arXiv:2204.12753v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12753","description":"<p>Being a popular mode of text-based communication in multilingual communities,\ncode-mixing in online social media has became an important subject to study.\nLearning the semantics and morphology of code-mixed language remains a key\nchallenge, due to scarcity of data and unavailability of robust and\nlanguage-invariant representation learning technique. Any morphologically-rich\nlanguage can benefit from character, subword, and word-level embeddings, aiding\nin learning meaningful correlations. In this paper, we explore a hierarchical\ntransformer-based architecture (HIT) to learn the semantics of code-mixed\nlanguages. HIT consists of multi-headed self-attention and outer product\nattention components to simultaneously comprehend the semantic and syntactic\nstructures of code-mixed texts. We evaluate the proposed method across 6 Indian\nlanguages (Bengali, Gujarati, Hindi, Tamil, Telugu and Malayalam) and Spanish\nfor 9 NLP tasks on 17 datasets. The HIT model outperforms state-of-the-art\ncode-mixed representation learning and multilingual language models in all\ntasks. We further demonstrate the generalizability of the HIT architecture\nusing masked language modeling-based pre-training, zero-shot learning, and\ntransfer learning approaches. Our empirical results show that the pre-training\nobjectives significantly improve the performance on downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1\">Ayan Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_T/0/1/0/all/0/1\">Tharun Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Thorough Examination on Zero-shot Dense Retrieval. (arXiv:2204.12755v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12755","description":"<p>Recent years have witnessed the significant advance in dense retrieval (DR)\nbased on powerful pre-trained language models (PLM). DR models have achieved\nexcellent performance in several benchmark datasets, while they are shown to be\nnot as competitive as traditional sparse retrieval models (e.g., BM25) in a\nzero-shot retrieval setting. However, in the related literature, there still\nlacks a detailed and comprehensive study on zero-shot retrieval. In this paper,\nwe present the first thorough examination of the zero-shot capability of DR\nmodels. We aim to identify the key factors and analyze how they affect\nzero-shot retrieval performance. In particular, we discuss the effect of\nseveral key factors related to source training set, analyze the potential bias\nfrom the target dataset, and review and compare existing zero-shot DR models.\nOur findings provide important evidence to better understand and develop\nzero-shot DR models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1\">Ruiyang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yingqi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qifei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuchen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why does Self-Supervised Learning for Speech Recognition Benefit Speaker Recognition?. (arXiv:2204.12765v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12765","description":"<p>Recently, self-supervised learning (SSL) has demonstrated strong performance\nin speaker recognition, even if the pre-training objective is designed for\nspeech recognition. In this paper, we study which factor leads to the success\nof self-supervised learning on speaker-related tasks, e.g. speaker verification\n(SV), through a series of carefully designed experiments. Our empirical results\non the Voxceleb-1 dataset suggest that the benefit of SSL to SV task is from a\ncombination of mask speech prediction loss, data scale, and model size, while\nthe SSL quantizer has a minor impact. We further employ the integrated\ngradients attribution method and loss landscape visualization to understand the\neffectiveness of self-supervised learning for speaker recognition performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chengyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiangzhan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra Fast Speech Separation Model with Teacher Student Learning. (arXiv:2204.12777v1 [eess.AS])","link":"http://arxiv.org/abs/2204.12777","description":"<p>Transformer has been successfully applied to speech separation recently with\nits strong long-dependency modeling capacity using a self-attention mechanism.\nHowever, Transformer tends to have heavy run-time costs due to the deep encoder\nlayers, which hinders its deployment on edge devices. A small Transformer model\nwith fewer encoder layers is preferred for computational efficiency, but it is\nprone to performance degradation. In this paper, an ultra fast speech\nseparation Transformer model is proposed to achieve both better performance and\nefficiency with teacher student learning (T-S learning). We introduce\nlayer-wise T-S learning and objective shifting mechanisms to guide the small\nstudent model to learn intermediate representations from the large teacher\nmodel. Compared with the small Transformer model trained from scratch, the\nproposed T-S learning method reduces the word error rate (WER) by more than 5%\nfor both multi-channel and single-channel speech separation on LibriCSS\ndataset. Utilizing more unlabeled speech data, our ultra fast speech separation\nmodels achieve more than 10% relative WER reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1\">Sanyuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xiangzhan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks. (arXiv:2204.12784v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12784","description":"<p>Aspect-level sentiment analysis aims to determine the sentiment polarity\ntowards a specific target in a sentence. The main challenge of this task is to\neffectively model the relation between targets and sentiments so as to filter\nout noisy opinion words from irrelevant targets. Most recent efforts capture\nrelations through target-sentiment pairs or opinion spans from a word-level or\nphrase-level perspective. Based on the observation that targets and sentiments\nessentially establish relations following the grammatical hierarchy of\nphrase-clause-sentence structure, it is hopeful to exploit comprehensive\nsyntactic information for better guiding the learning process. Therefore, we\nintroduce the concept of Scope, which outlines a structural text region related\nto a specific target. To jointly learn structural Scope and predict the\nsentiment polarity, we propose a hybrid graph convolutional network (HGCN) to\nsynthesize information from constituency tree and dependency tree, exploring\nthe potential of linking two syntax parsing methods to enrich the\nrepresentation. Experimental results on four public datasets illustrate that\nour HGCN model outperforms current state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lvxiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_X/0/1/0/all/0/1\">Xiaoxuan Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianwang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1\">Ming Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiawei Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug-and-Play Adaptation for Continuously-updated QA. (arXiv:2204.12785v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12785","description":"<p>Language models (LMs) have shown great potential as implicit knowledge bases\n(KBs). And for their practical use, knowledge in LMs need to be updated\nperiodically. However, existing tasks to assess LMs' efficacy as KBs do not\nadequately consider multiple large-scale updates. To this end, we first propose\na novel task--Continuously-updated QA (CuQA)--in which multiple large-scale\nupdates are made to LMs, and the performance is measured with respect to the\nsuccess in adding and updating knowledge while retaining existing knowledge. We\nthen present LMs with plug-in modules that effectively handle the updates.\nExperiments conducted on zsRE QA and NQ datasets show that our method\noutperforms existing approaches. We find that our method is 4x more effective\nin terms of updates/forgets ratio, compared to a fine-tuning baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wookje Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seung-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Joonsuk Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern Baselines for SPARQL Semantic Parsing. (arXiv:2204.12793v1 [cs.IR])","link":"http://arxiv.org/abs/2204.12793","description":"<p>In this work, we focus on the task of generating SPARQL queries from natural\nlanguage questions, which can then be executed on Knowledge Graphs (KGs). We\nassume that gold entity and relations have been provided, and the remaining\ntask is to arrange them in the right order along with SPARQL vocabulary, and\ninput tokens to produce the correct SPARQL query. Pre-trained Language Models\n(PLMs) have not been explored in depth on this task so far, so we experiment\nwith BART, T5 and PGNs (Pointer Generator Networks) with BERT embeddings,\nlooking for new baselines in the PLM era for this task, on DBpedia and Wikidata\nKGs. We show that T5 requires special input tokenisation, but produces state of\nthe art performance on LC-QuAD 1.0 and LC-QuAD 2.0 datasets, and outperforms\ntask-specific models from previous works. Moreover, the methods enable semantic\nparsing for questions where a part of the input needs to be copied to the\noutput query, thus enabling a new paradigm in KG semantic parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_D/0/1/0/all/0/1\">Debayan Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_P/0/1/0/all/0/1\">Pranav Ajit Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_J/0/1/0/all/0/1\">Jivat Neet Kaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Simile Knowledge from Pre-trained Language Models. (arXiv:2204.12807v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12807","description":"<p>Simile interpretation (SI) and simile generation (SG) are challenging tasks\nfor NLP because models require adequate world knowledge to produce predictions.\nPrevious works have employed many hand-crafted resources to bring\nknowledge-related into models, which is time-consuming and labor-intensive. In\nrecent years, pre-trained language models (PLMs) based approaches have become\nthe de-facto standard in NLP since they learn generic knowledge from a large\ncorpus. The knowledge embedded in PLMs may be useful for SI and SG tasks.\nNevertheless, there are few works to explore it. In this paper, we probe simile\nknowledge from PLMs to solve the SI and SG tasks in the unified framework of\nsimile triple completion for the first time. The backbone of our framework is\nto construct masked sentences with manual patterns and then predict the\ncandidate words in the masked position. In this framework, we adopt a secondary\ntraining process (Adjective-Noun mask Training) with the masked language model\n(MLM) loss to enhance the prediction diversity of candidate words in the masked\nposition. Moreover, pattern ensemble (PE) and pattern search (PS) are applied\nto improve the quality of predicted words. Finally, automatic and human\nevaluations demonstrate the effectiveness of our framework in both SI and SG\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weijie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yongzhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_J/0/1/0/all/0/1\">Jiashu Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guandan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yijiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chang Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Method of Query Graph Reranking for Knowledge Base Question Answering. (arXiv:2204.12808v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12808","description":"<p>This paper presents a novel reranking method to better choose the optimal\nquery graph, a sub-graph of knowledge graph, to retrieve the answer for an\ninput question in Knowledge Base Question Answering (KBQA). Existing methods\nsuffer from a severe problem that there is a significant gap between top-1\nperformance and the oracle score of top-n results. To address this problem, our\nmethod divides the choosing procedure into two steps: query graph ranking and\nquery graph reranking. In the first step, we provide top-n query graphs for\neach question. Then we propose to rerank the top-n query graphs by combining\nwith the information of answer type. Experimental results on two widely used\ndatasets show that our proposed method achieves the best results on the\nWebQuestions dataset and the second best on the ComplexQuestions dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yonghui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenliang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillSpan: Hard and Soft Skill Extraction from English Job Postings. (arXiv:2204.12811v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12811","description":"<p>Skill Extraction (SE) is an important and widely-studied task useful to gain\ninsights into labor market dynamics. However, there is a lacuna of datasets and\nannotation guidelines; available datasets are few and contain crowd-sourced\nlabels on the span-level or labels from a predefined skill inventory. To\naddress this gap, we introduce SKILLSPAN, a novel SE dataset consisting of\n14.5K sentences and over 12.5K annotated spans. We release its respective\nguidelines created over three different sources annotated for hard and soft\nskills by domain experts. We introduce a BERT baseline (Devlin et al., 2019).\nTo improve upon this baseline, we experiment with language models that are\noptimized for long spans (Joshi et al., 2020; Beltagy et al., 2020), continuous\npre-training on the job posting domain (Han and Eisenstein, 2019; Gururangan et\nal., 2020), and multi-task learning (Caruana, 1997). Our results show that the\ndomain-adapted models significantly outperform their non-adapted counterparts,\nand single-task outperforms multi-task learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jensen_K/0/1/0/all/0/1\">Kristian N&#xf8;rgaard Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonniks_S/0/1/0/all/0/1\">Sif Dam Sonniks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LyS_ACoru\\~na at SemEval-2022 Task 10: Repurposing Off-the-Shelf Tools for Sentiment Analysis as Semantic Dependency Parsing. (arXiv:2204.12820v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12820","description":"<p>This paper addressed the problem of structured sentiment analysis using a\nbi-affine semantic dependency parser, large pre-trained language models, and\npublicly available translation models. For the monolingual setup, we\nconsidered: (i) training on a single treebank, and (ii) relaxing the setup by\ntraining on treebanks coming from different languages that can be adequately\nprocessed by cross-lingual language models. For the zero-shot setup and a given\ntarget treebank, we relied on: (i) a word-level translation of available\ntreebanks in other languages to get noisy, unlikely-grammatical, but annotated\ndata (we release as much of it as licenses allow), and (ii) merging those\ntranslated treebanks to obtain training data. In the post-evaluation phase, we\nalso trained cross-lingual models that simply merged all the English treebanks\nand did not use word-level translations, and yet obtained better results.\nAccording to the official results, we ranked 8th and 9th in the monolingual and\ncross-lingual setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alonso_Alonso_I/0/1/0/all/0/1\">Iago Alonso-Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vilares_D/0/1/0/all/0/1\">David Vilares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Parallelize in a Shared-Memory Environment with Transformers. (arXiv:2204.12835v1 [cs.DC])","link":"http://arxiv.org/abs/2204.12835","description":"<p>In past years, the world has switched to many-core and multi-core shared\nmemory architectures.\n</p>\n<p>As a result, there is a growing need to utilize these architectures by\nintroducing shared memory parallelization schemes to software applications.\nOpenMP is the most comprehensive API that implements such schemes,\ncharacterized by a readable interface. Nevertheless, introducing OpenMP into\ncode is challenging due to pervasive pitfalls in management of parallel shared\nmemory. To facilitate the performance of this task, many source-to-source (S2S)\ncompilers have been created over the years, tasked with inserting OpenMP\ndirectives into code automatically.\n</p>\n<p>In addition to having limited robustness to their input format, these\ncompilers still do not achieve satisfactory coverage and precision in locating\nparallelizable code and generating appropriate directives.\n</p>\n<p>In this work, we propose leveraging recent advances in ML techniques,\nspecifically in natural language processing (NLP), to replace S2S compilers\naltogether.\n</p>\n<p>We create a database (corpus), Open-OMP, specifically for this goal. Open-OMP\ncontains over 28,000 code snippets, half of which contain OpenMP directives\nwhile the other half do not need parallelization at all with high probability.\n</p>\n<p>We use the corpus to train systems to automatically classify code segments in\nneed of parallelization, as well as suggest individual OpenMP clauses.\n</p>\n<p>We train several transformer models, named PragFormer, for these tasks, and\nshow that they outperform statistically-trained baselines and automatic S2S\nparallelization compilers in both classifying the overall need for an OpenMP\ndirective and the introduction of private and reduction clauses.\n</p>\n<p>Our source code and database are available at:\nhttps://github.com/Scientific-Computing-Lab-NRCN/PragFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harel_R/0/1/0/all/0/1\">Re&#x27;em Harel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oren_G/0/1/0/all/0/1\">Gal Oren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query2Particles: Knowledge Graph Reasoning with Particle Embeddings. (arXiv:2204.12847v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12847","description":"<p>Answering complex logical queries on incomplete knowledge graphs (KGs) with\nmissing edges is a fundamental and important task for knowledge graph\nreasoning. The query embedding method is proposed to answer these queries by\njointly encoding queries and entities to the same embedding space. Then the\nanswer entities are selected according to the similarities between the entity\nembeddings and the query embedding. As the answers to a complex query are\nobtained from a combination of logical operations over sub-queries, the\nembeddings of the answer entities may not always follow a uni-modal\ndistribution in the embedding space. Thus, it is challenging to simultaneously\nretrieve a set of diverse answers from the embedding space using a single and\nconcentrated query representation such as a vector or a hyper-rectangle. To\nbetter cope with queries with diversified answers, we propose Query2Particles\n(Q2P), a complex KG query answering method. Q2P encodes each query into\nmultiple vectors, named particle embeddings. By doing so, the candidate answers\ncan be retrieved from different areas over the embedding space using the\nmaximal similarities between the entity embeddings and any of the particle\nembeddings. Meanwhile, the corresponding neural logic operations are defined to\nsupport its reasoning over arbitrary first-order logic queries. The experiments\nshow that Query2Particles achieves state-of-the-art performance on the complex\nquery answering tasks on FB15k, FB15K-237, and NELL knowledge graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jiaxin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaCoach: A Virtual Coach for Training Customer Service Agents. (arXiv:2204.12935v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12935","description":"<p>With the development of online business, customer service agents gradually\nplay a crucial role as an interface between the companies and their customers.\nMost companies spend a lot of time and effort on hiring and training customer\nservice agents. To this end, we propose AdaCoach: A Virtual Coach for Training\nCustomer Service Agents, to promote the ability of newly hired service agents\nbefore they get to work. AdaCoach is designed to simulate real customers who\nseek help and actively initiate the dialogue with the customer service agents.\nBesides, AdaCoach uses an automated dialogue evaluation model to score the\nperformance of the customer agent in the training process, which can provide\nnecessary assistance when the newly hired customer service agent encounters\nproblems. We apply recent NLP technologies to ensure efficient run-time\nperformance in the deployed system. To the best of our knowledge, this is the\nfirst system that trains the customer service agent through human-computer\ninteraction. Until now, the system has already supported more than 500,000\nsimulation training and cultivated over 1000 qualified customer service agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shuang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shuai Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Minghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhou Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_B/0/1/0/all/0/1\">Biao Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An End-to-End Dialogue Summarization System for Sales Calls. (arXiv:2204.12951v1 [cs.CL])","link":"http://arxiv.org/abs/2204.12951","description":"<p>Summarizing sales calls is a routine task performed manually by salespeople.\nWe present a production system which combines generative models fine-tuned for\ncustomer-agent setting, with a human-in-the-loop user experience for an\ninteractive summary curation process. We address challenging aspects of\ndialogue summarization task in a real-world setting including long input\ndialogues, content validation, lack of labeled data and quality evaluation. We\nshow how GPT-3 can be leveraged as an offline data labeler to handle training\ndata scarcity and accommodate privacy constraints in an industrial setting.\nExperiments show significant improvements by our models in tackling the\nsummarization and content validation tasks on public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asi_A/0/1/0/all/0/1\">Abedelkadir Asi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstadt_R/0/1/0/all/0/1\">Roy Eisenstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geckt_D/0/1/0/all/0/1\">Dean Geckt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuper_Y/0/1/0/all/0/1\">Yarin Kuper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronen_R/0/1/0/all/0/1\">Royi Ronen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extremal GloVe: Theoretically Accurate Distributed Word Embedding by Tail Inference. (arXiv:2204.13009v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13009","description":"<p>Distributed word embeddings such as Word2Vec and GloVe have been widely\nadopted in industrial context settings. Major technical applications of GloVe\ninclude recommender systems and natural language processing. The fundamental\ntheory behind GloVe relies on the selection of a weighting function in the\nweighted least squres formulation that computes the powered ratio of word\noccurrence count and the maximum word count in the corpus. However, the initial\nformulation of GloVe is not theoretically sound in two aspects, namely the\nselection of the weighting function and its power exponent is ad-hoc. In this\npaper, we utilize the theory of extreme value analysis and propose a\ntheoretically accurate version of GloVe. By reformulating the weighted least\nsquares loss function as the expected loss function and accurately choosing the\npower exponent, we create a theoretically accurate version of GloVe. We\ndemonstrate the competitiveness of our algorithm and show that the initial\nformulation of GloVe with the suggested optimal parameter can be viewed as a\nspecial case of our paradigm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLU++: A Multi-Label, Slot-Rich, Generalisable Dataset for Natural Language Understanding in Task-Oriented Dialogue. (arXiv:2204.13021v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13021","description":"<p>We present NLU++, a novel dataset for natural language understanding (NLU) in\ntask-oriented dialogue (ToD) systems, with the aim to provide a much more\nchallenging evaluation environment for dialogue NLU models, up to date with the\ncurrent application and industry requirements. NLU++ is divided into two\ndomains (BANKING and HOTELS) and brings several crucial improvements over\ncurrent commonly used NLU datasets. \\textbf{1)} NLU++ provides fine-grained\ndomain ontologies with a large set of challenging \\textit{multi-intent}\nsentences, introducing and validating the idea of \\textit{intent modules} that\ncan be combined into complex intents that convey complex user goals, combined\nwith finer-grained and thus more challenging slot sets. \\textbf{2)} The\nontology is divided into \\textit{domain-specific} and \\textit{generic} (i.e.,\ndomain-universal) intent modules that overlap across domains, promoting\ncross-domain reusability of annotated examples. \\textbf{3)} The dataset design\nhas been inspired by the problems observed in industrial ToD systems, and\n\\textbf{4)} it has been collected, filtered and carefully annotated by dialogue\nNLU experts, yielding high-quality annotated data. Finally, we benchmark a\nseries of current state-of-the-art NLU models on NLU++; the results demonstrate\nthe challenging nature of the dataset, especially in low-data regimes, the\nvalidity of `intent modularisation', and call for further research on ToD NLU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casanueva_I/0/1/0/all/0/1\">I&#xf1;igo Casanueva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spithourakis_G/0/1/0/all/0/1\">Georgios Spithourakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1\">Pawe&#x142; Budzianowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialogVED: A Pre-trained Latent Variable Encoder-Decoder Model for Dialog Response Generation. (arXiv:2204.13031v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13031","description":"<p>Dialog response generation in open domain is an important research topic\nwhere the main challenge is to generate relevant and diverse responses. In this\npaper, we propose a new dialog pre-training framework called DialogVED, which\nintroduces continuous latent variables into the enhanced encoder-decoder\npre-training framework to increase the relevance and diversity of responses.\nWith the help of a large dialog corpus (Reddit), we pre-train the model using\nthe following 4 tasks, used in training language models (LMs) and Variational\nAutoencoders (VAEs) literature: 1) masked language model; 2) response\ngeneration; 3) bag-of-words prediction; and 4) KL divergence reduction. We also\nadd additional parameters to model the turn structure in dialogs to improve the\nperformance of the pre-trained model. We conduct experiments on PersonaChat,\nDailyDialog, and DSTC7-AVSD benchmarks for response generation. Experimental\nresults show that our model achieves the new state-of-the-art results on all\nthese datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1\">Weizhen Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bartuer Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Biao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeBERT: Enhancing Pre-Trained Language Representations with Temporal Information. (arXiv:2204.13032v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13032","description":"<p>Time is an important aspect of text documents, which has been widely\nexploited in natural language processing and has strong influence, for example,\nin temporal information retrieval, where the temporal information of queries or\ndocuments need to be identified for relevance estimation. Event-related tasks\nlike event ordering, which aims to order events by their occurrence time, also\nneed to determine the temporal information of events. In this work, we\ninvestigate methods for incorporating temporal information during pre-training,\nto further improve the performance on time-related tasks. Compared with BERT\nwhich utilizes synchronic document collections (BooksCorpus and English\nWikipedia) as the training corpora, we use long-span temporal news collection\nfor building word representations, since temporal information constitutes one\nof the most significant features of news articles. We then introduce TimeBERT,\na novel language representation model trained on a temporal collection of news\narticles via two new pre-training tasks, which harness two distinct temporal\nsignals to construct time-aware language representation. The experimental\nresults show that TimeBERT consistently outperforms BERT and other existing\npre-trained models, with substantial gains on different downstream NLP tasks or\napplications for which time is of importance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Teachable Reasoning Systems. (arXiv:2204.13074v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13074","description":"<p>Our goal is a teachable reasoning system for question-answering (QA), where a\nuser can interact with faithful answer explanations, and correct errors so that\nthe system improves over time. Our approach is three-fold: First, generated\nchains of reasoning show how answers are implied by the system's own internal\nbeliefs. Second, users can interact with the explanations to identify erroneous\nmodel beliefs and provide corrections. Third, we augment the model with a\ndynamic memory of such corrections. Retrievals from memory are used as\nadditional context for QA, to help avoid previous mistakes in similar new\nsituations - a novel type of memory-based continuous learning. To our\nknowledge, this is the first system to generate chains that are both faithful\n(the answer follows from the reasoning) and truthful (the chain reflects the\nsystem's own beliefs, as ascertained by self-querying). In evaluation, users\njudge that a majority (65%+) of generated chains clearly show how an answer\nfollows from a set of facts - substantially better than a high-performance\nbaseline. We also find that using simulated feedback, our system (called\nEntailmentWriter) continually improves with time, requiring feedback on only\n25% of training examples to reach within 1% of the upper-bound (feedback on all\nexamples). We observe a similar trend with real users. This suggests new\nopportunities for using language models in an interactive setting where users\ncan inspect, debug, correct, and improve a system's performance over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_B/0/1/0/all/0/1\">Bhavana Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Borrow -- Relation Representation for Without-Mention Entity-Pairs for Knowledge Graph Completion. (arXiv:2204.13097v1 [cs.CL])","link":"http://arxiv.org/abs/2204.13097","description":"<p>Prior work on integrating text corpora with knowledge graphs (KGs) to improve\nKnowledge Graph Embedding (KGE) have obtained good performance for entities\nthat co-occur in sentences in text corpora. Such sentences (textual mentions of\nentity-pairs) are represented as Lexicalised Dependency Paths (LDPs) between\ntwo entities. However, it is not possible to represent relations between\nentities that do not co-occur in a single sentence using LDPs. In this paper,\nwe propose and evaluate several methods to address this problem, where we\nborrow LDPs from the entity pairs that co-occur in sentences in the corpus\n(i.e. with mention entity pairs) to represent entity pairs that do not co-occur\nin any sentence in the corpus (i.e. without mention entity pairs). We propose a\nsupervised borrowing method, SuperBorrow, that learns to score the suitability\nof an LDP to represent a without-mention entity pair using pre-trained entity\nembeddings and contextualised LDP representations. Experimental results show\nthat SuperBorrow improves the link prediction performance of multiple\nwidely-used prior KGE methods such as TransE, DistMult, ComplEx and RotatE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hakami_H/0/1/0/all/0/1\">Huda Hakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakami_M/0/1/0/all/0/1\">Mona Hakami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandya_A/0/1/0/all/0/1\">Angrosh Mandya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural String Edit Distance. (arXiv:2104.08388v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08388","description":"<p>We propose the neural string edit distance model for string-pair matching and\nstring transduction based on learnable string edit distance. We modify the\noriginal expectation-maximization learned edit distance algorithm into a\ndifferentiable loss function, allowing us to integrate it into a neural network\nproviding a contextual representation of the input. We evaluate on cognate\ndetection, transliteration, and grapheme-to-phoneme conversion, and show that\nwe can trade off between performance and interpretability in a single\nframework. Using contextual representations, which are difficult to interpret,\nwe match the performance of state-of-the-art string-pair matching models. Using\nstatic embeddings and a slightly different loss function, we force\ninterpretability, at the expense of an accuracy drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shades of confusion: Lexical uncertainty modulates ad hoc coordination in an interactive communication task. (arXiv:2105.06546v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06546","description":"<p>There is substantial variability in the expectations that communication\npartners bring into interactions, creating the potential for misunderstandings.\nTo directly probe these gaps and our ability to overcome them, we propose a\ncommunication task based on color-concept associations. In Experiment 1, we\nestablish several key properties of the mental representations of these\nexpectations, or lexical priors, based on recent probabilistic theories.\nAssociations are more variable for abstract concepts, variability is\nrepresented as uncertainty within each individual, and uncertainty enables\naccurate predictions about whether others are likely to share the same\nassociation. In Experiment 2, we then examine the downstream consequences of\nthese representations for communication. Accuracy is initially low when\ncommunicating about concepts with more variable associations, but rapidly\nincreases as participants form ad hoc conventions. Together, our findings\nsuggest that people cope with variability by maintaining well-calibrated\nuncertainty about their partner and appropriately adaptable representations of\ntheir own.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Murthy_S/0/1/0/all/0/1\">Sonia K. Murthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Pre-trained Auto-regressive Language Models for Named Entity Typing and Recognition. (arXiv:2108.11857v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11857","description":"<p>Despite impressive results of language models for named entity recognition\n(NER), their generalization to varied textual genres, a growing entity type\nset, and new entities remains a challenge. Collecting thousands of annotations\nin each new case for training or fine-tuning is expensive and time-consuming.\nIn contrast, humans can easily identify named entities given some simple\ninstructions. Inspired by this, we challenge the reliance on large datasets and\nstudy pre-trained language models for NER in a meta-learning setup. First, we\ntest named entity typing (NET) in a zero-shot transfer scenario. Then, we\nperform NER by giving few examples at inference. We propose a method to select\nseen and rare / unseen names when having access only to the pre-trained model\nand report results on these groups. The results show: auto-regressive language\nmodels as meta-learners can perform NET and NER fairly well especially for\nregular or seen names; name irregularity when often present for a certain\nentity type can become an effective exploitable cue; names with words foreign\nto the model have the most negative impact on results; the model seems to rely\nmore on name than context cues in few-shot NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Epure_E/0/1/0/all/0/1\">Elena V. Epure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Fine-to-Coarse Distillation for Coarse-grained Response Selection in Open-Domain Conversations. (arXiv:2109.13087v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13087","description":"<p>We study the problem of coarse-grained response selection in retrieval-based\ndialogue systems. The problem is equally important with fine-grained response\nselection, but is less explored in existing literature. In this paper, we\npropose a Contextual Fine-to-Coarse (CFC) distilled model for coarse-grained\nresponse selection in open-domain conversations. In our CFC model, dense\nrepresentations of query, candidate response and corresponding context is\nlearned based on the multi-tower architecture, and more expressive knowledge\nlearned from the one-tower architecture (fine-grained) is distilled into the\nmulti-tower architecture (coarse-grained) to enhance the performance of the\nretriever. To evaluate the performance of our proposed model, we construct two\nnew datasets based on the Reddit comments dump and Twitter corpus. Extensive\nexperimental results on the two datasets show that the proposed methods achieve\na significant improvement over all evaluation metrics compared with traditional\nbaseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bolun Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaowu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bartuer Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Biao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Why don't people use character-level machine translation?. (arXiv:2110.08191v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08191","description":"<p>We present a literature and empirical survey that critically assesses the\nstate of the art in character-level modeling for machine translation (MT).\nDespite evidence in the literature that character-level systems are comparable\nwith subword systems, they are virtually never used in competitive setups in\nWMT competitions. We empirically show that even with recent modeling\ninnovations in character-level natural language processing, character-level MT\nsystems still struggle to match their subword-based counterparts.\nCharacter-level MT systems show neither better domain robustness, nor better\nmorphological generalization, despite being often so motivated. However, we are\nable to show robustness towards source side noise and that translation quality\ndoes not degrade with increasing beam size at decoding time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1\">Helmut Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Cross-Utterance Language Modeling for Conversational Speech Recognition. (arXiv:2111.03333v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03333","description":"<p>Conversational speech normally is embodied with loose syntactic structures at\nthe utterance level but simultaneously exhibits topical coherence relations\nacross consecutive utterances. Prior work has shown that capturing longer\ncontext information with a recurrent neural network or long short-term memory\nlanguage model (LM) may suffer from the recent bias while excluding the\nlong-range context. In order to capture the long-term semantic interactions\namong words and across utterances, we put forward disparate conversation\nhistory fusion methods for language modeling in automatic speech recognition\n(ASR) of conversational speech. Furthermore, a novel audio-fusion mechanism is\nintroduced, which manages to fuse and utilize the acoustic embeddings of a\ncurrent utterance and the semantic content of its corresponding conversation\nhistory in a cooperative way. To flesh out our ideas, we frame the ASR N-best\nhypothesis rescoring task as a prediction problem, leveraging BERT, an iconic\npre-trained LM, as the ingredient vehicle to facilitate selection of the oracle\nhypothesis from a given N-best hypothesis list. Empirical experiments conducted\non the AMI benchmark dataset seem to demonstrate the feasibility and efficacy\nof our methods in relation to some current top-of-line methods. The proposed\nmethods not only achieve significant inference time reduction but also improve\nthe ASR performance for conversational speech.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Bi-Cheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1\">Shih-Hsuan Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Hsuan-Sheng Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Berlin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Transferability of Prompt Tuning for Natural Language Processing. (arXiv:2111.06719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.06719","description":"<p>Prompt tuning (PT) is a promising parameter-efficient method to utilize\nextremely large pre-trained language models (PLMs), which can achieve\ncomparable performance to full-parameter fine-tuning by only tuning a few soft\nprompts. However, PT requires much more training time than fine-tuning.\nIntuitively, knowledge transfer can help to improve the efficiency. To explore\nwhether we can improve PT via prompt transfer, we empirically investigate the\ntransferability of soft prompts across different downstream tasks and PLMs in\nthis work. We find that (1) in zero-shot setting, trained soft prompts can\neffectively transfer to similar tasks on the same PLM and also to other PLMs\nwith a cross-model projector trained on similar tasks; (2) when used as\ninitialization, trained soft prompts of similar tasks and projected prompts of\nother PLMs can significantly accelerate training and also improve the\nperformance of PT. Moreover, to explore what decides prompt transferability, we\ninvestigate various transferability indicators and find that the overlapping\nrate of activated neurons strongly reflects the transferability, which suggests\nhow the prompts stimulate PLMs is essential. Our findings show that prompt\ntransfer is promising for improving PT, and further research shall focus more\non prompts' stimulation to PLMs. The source code can be obtained from\nhttps://github.com/thunlp/Prompt-Transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yusheng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yujia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Chi-Min Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yankai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_K/0/1/0/all/0/1\">Kaiyue Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated scholarly paper review: Technologies and challenges. (arXiv:2111.07533v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2111.07533","description":"<p>Peer review is a widely accepted mechanism for research evaluation, playing a\npivotal role in scholarly publishing. However, criticisms have long been\nleveled on this mechanism, mostly because of its inefficiency and subjectivity.\nRecent years have seen the application of artificial intelligence (AI) in\nassisting the peer review process. Nonetheless, with the involvement of humans,\nsuch limitations remain inevitable. In this review paper, we propose the\nconcept and pipeline of automated scholarly paper review (ASPR) and review the\nrelevant literature and technologies of achieving a full-scale computerized\nreview process. On the basis of the review and discussion, we conclude that\nthere is already corresponding research and implementation at each stage of\nASPR. We further look into the challenges in ASPR with the existing\ntechnologies. The major difficulties lie in imperfect document parsing and\nrepresentation, inadequate data, defective human-computer interaction and\nflawed deep logical reasoning. Moreover, we discuss the possible moral &amp;\nethical issues and point out the future directions of ASPR. In the foreseeable\nfuture, ASPR and peer review will coexist in a reinforcing manner before ASPR\nis able to fully undertake the reviewing workload from humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jialiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhangping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yidong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaodong Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Triggerless Backdoor Attack for NLP Tasks with Clean Labels. (arXiv:2111.07970v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07970","description":"<p>Backdoor attacks pose a new threat to NLP models. A standard strategy to\nconstruct poisoned data in backdoor attacks is to insert triggers (e.g., rare\nwords) into selected sentences and alter the original label to a target label.\nThis strategy comes with a severe flaw of being easily detected from both the\ntrigger and the label perspectives: the trigger injected, which is usually a\nrare word, leads to an abnormal natural language expression, and thus can be\neasily detected by a defense model; the changed target label leads the example\nto be mistakenly labeled and thus can be easily detected by manual inspections.\nTo deal with this issue, in this paper, we propose a new strategy to perform\ntextual backdoor attacks which do not require an external trigger, and the\npoisoned samples are correctly labeled. The core idea of the proposed strategy\nis to construct clean-labeled examples, whose labels are correct but can lead\nto test label changes when fused with the training set. To generate poisoned\nclean-labeled examples, we propose a sentence generation model based on the\ngenetic algorithm to cater to the non-differentiable characteristic of text\ndata. Extensive experiments demonstrate that the proposed attacking strategy is\nnot only effective, but more importantly, hard to defend due to its triggerless\nand clean-labeled nature. Our work marks the first step towards developing\ntriggerless attacking strategies in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1\">Leilei Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shangwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chun Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Literature-Augmented Clinical Outcome Prediction. (arXiv:2111.08374v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08374","description":"<p>We present BEEP (Biomedical Evidence-Enhanced Predictions), a novel approach\nfor clinical outcome prediction that retrieves patient-specific medical\nliterature and incorporates it into predictive models. Based on each individual\npatient's clinical notes, we train language models (LMs) to find relevant\npapers and fuse them with information from notes to predict outcomes such as\nin-hospital mortality. We develop methods to retrieve literature based on\nnoisy, information-dense patient notes, and to augment existing outcome\nprediction models with retrieved papers in a manner that maximizes predictive\naccuracy. Our approach boosts predictive performance on three important\nclinical tasks in comparison to strong recent LM baselines, increasing F1 by up\nto 5 points and precision@Top-K by a large margin of over 25%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Aakanksha Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1\">Sergey Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Variational Learning for Unsupervised Knowledge Grounded Dialogs. (arXiv:2112.00653v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.00653","description":"<p>Recent methods for knowledge grounded dialogs generate responses by\nincorporating information from an external textual document. These methods do\nnot require the exact document to be known during training and rely on the use\nof a retrieval system to fetch relevant documents from a large index. The\ndocuments used to generate the responses are modeled as latent variables whose\nprior probabilities need to be estimated. Models such as RAG and REALM,\nmarginalize the document probabilities over the documents retrieved from the\nindex to define the log likelihood loss function which is optimized end-to-end.\n</p>\n<p>In this paper, we develop a variational approach to the above technique\nwherein, we instead maximize the Evidence Lower bound (ELBO). Using a\ncollection of three publicly available open-conversation datasets, we\ndemonstrate how the posterior distribution, that has information from the\nground-truth response, allows for a better approximation of the objective\nfunction during training. To overcome the challenges associated with sampling\nover a large knowledge collection, we develop an efficient approach to\napproximate the ELBO. To the best of our knowledge we are the first to apply\nvariational training for open-scale unsupervised knowledge grounded dialog\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_M/0/1/0/all/0/1\">Mayank Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madan_D/0/1/0/all/0/1\">Dhiraj Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_G/0/1/0/all/0/1\">Gaurav Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Contractor_D/0/1/0/all/0/1\">Danish Contractor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Text-to-SQL Parsing through Question Decomposition. (arXiv:2112.06311v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06311","description":"<p>Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query\nrelational data. Training such parsers, by contrast, generally requires\nexpertise in annotating natural language (NL) utterances with corresponding SQL\nqueries. In this work, we propose a weak supervision approach for training\ntext-to-SQL parsers. We take advantage of the recently proposed question\nmeaning representation called QDMR, an intermediate between NL and formal query\nlanguages. Given questions, their QDMR structures (annotated by non-experts or\nautomatically predicted), and the answers, we are able to automatically\nsynthesize SQL queries that are used to train text-to-SQL models. We test our\napproach by experimenting on five benchmark datasets. Our results show that the\nweakly supervised models perform competitively with those trained on annotated\nNL-SQL data. Overall, we effectively train text-to-SQL parsers, while using\nzero SQL annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolfson_T/0/1/0/all/0/1\">Tomer Wolfson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deutch_D/0/1/0/all/0/1\">Daniel Deutch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1\">Jonathan Berant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Neural Models for Query-Focused Summarization. (arXiv:2112.07637v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.07637","description":"<p>Query-focused summarization (QFS) aims to produce summaries that answer\nparticular questions of interest, enabling greater user control and\npersonalization. While recently released datasets, such as QMSum or AQuaMuSe,\nfacilitate research efforts in QFS, the field lacks a comprehensive study of\nthe broad space of applicable modeling methods. In this paper we conduct a\nsystematic exploration of neural approaches to QFS, considering two general\nclasses of methods: two-stage extractive-abstractive solutions and end-to-end\nmodels. Within those categories, we investigate existing models and explore\nstrategies for transfer learning. We also present two modeling extensions that\nachieve state-of-the-art performance on the QMSum dataset, up to a margin of\n3.38 ROUGE-1, 3.72 ROUGE2, and 3.28 ROUGE-L when combined with transfer\nlearning strategies. Results from human evaluation suggest that the best models\nproduce more comprehensive and factually consistent summaries compared to a\nbaseline model. Code and checkpoints are made publicly available:\nhttps://github.com/salesforce/query-focused-sum.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1\">Jesse Vig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1\">Alexander R. Fabbri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kry&#x15b;ci&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chien-Sheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenhao Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sublinear Time Approximation of Text Similarity Matrices. (arXiv:2112.09631v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.09631","description":"<p>We study algorithms for approximating pairwise similarity matrices that arise\nin natural language processing. Generally, computing a similarity matrix for\n$n$ data points requires $\\Omega(n^2)$ similarity computations. This quadratic\nscaling is a significant bottleneck, especially when similarities are computed\nvia expensive functions, e.g., via transformer models. Approximation methods\nreduce this quadratic complexity, often by using a small subset of exactly\ncomputed similarities to approximate the remainder of the complete pairwise\nsimilarity matrix.\n</p>\n<p>Significant work focuses on the efficient approximation of positive\nsemidefinite (PSD) similarity matrices, which arise e.g., in kernel methods.\nHowever, much less is understood about indefinite (non-PSD) similarity\nmatrices, which often arise in NLP. Motivated by the observation that many of\nthese matrices are still somewhat close to PSD, we introduce a generalization\nof the popular Nystr\\\"{o}m method to the indefinite setting. Our algorithm can\nbe applied to any similarity matrix and runs in sublinear time in the size of\nthe matrix, producing a rank-$s$ approximation with just $O(ns)$ similarity\ncomputations.\n</p>\n<p>We show that our method, along with a simple variant of CUR decomposition,\nperforms very well in approximating a variety of similarity matrices arising in\nNLP tasks. We demonstrate high accuracy of the approximated similarity matrices\nin the downstream tasks of document classification, sentence similarity, and\ncross-document coreference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Archan Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asyncval: A Toolkit for Asynchronously Validating Dense Retriever Checkpoints during Training. (arXiv:2202.12510v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2202.12510","description":"<p>The process of model checkpoint validation refers to the evaluation of the\nperformance of a model checkpoint executed on a held-out portion of the\ntraining data while learning the hyperparameters of the model, and is used to\navoid over-fitting and determine when the model has converged so as to stop\ntraining. A simple and efficient strategy to validate deep learning checkpoints\nis the addition of validation loops to execute during training. However, the\nvalidation of dense retrievers (DR) checkpoints is not as trivial -- and the\naddition of validation loops is not efficient. This is because, in order to\naccurately evaluate the performance of a DR checkpoint, the whole document\ncorpus needs to be encoded into vectors using the current checkpoint before any\nactual retrieval operation for checkpoint validation can be performed. This\ncorpus encoding process can be very time-consuming if the document corpus\ncontains millions of documents (e.g., 8.8m for MS MARCO and 21m for Natural\nQuestions). Thus, a naive use of validation loops during training will\nsignificantly increase training time. To address this issue, in this demo\npaper, we propose Asyncval: a Python-based toolkit for efficiently validating\nDR checkpoints during training. Instead of pausing the training loop for\nvalidating DR checkpoints, Asyncval decouples the validation loop from the\ntraining loop, uses another GPU to automatically validate new DR checkpoints\nand thus permits to perform validation asynchronously from training. Asyncval\nalso implements a range of different corpus subset sampling strategies for\nvalidating DR checkpoints; these strategies allow to further speed up the\nvalidation process. We provide an investigation of these methods in terms of\ntheir impact on validation time and validation fidelity. Asyncval is made\navailable as an open-source project at https://github.com/ielab/asyncval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Shengyao Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuccon_G/0/1/0/all/0/1\">Guido Zuccon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaPraDoR: Unsupervised Pretrained Dense Retriever for Zero-Shot Text Retrieval. (arXiv:2203.06169v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06169","description":"<p>In this paper, we propose LaPraDoR, a pretrained dual-tower dense retriever\nthat does not require any supervised data for training. Specifically, we first\npresent Iterative Contrastive Learning (ICoL) that iteratively trains the query\nand document encoders with a cache mechanism. ICoL not only enlarges the number\nof negative instances but also keeps representations of cached examples in the\nsame hidden space. We then propose Lexicon-Enhanced Dense Retrieval (LEDR) as a\nsimple yet effective way to enhance dense retrieval with lexical matching. We\nevaluate LaPraDoR on the recently proposed BEIR benchmark, including 18\ndatasets of 9 zero-shot text retrieval tasks. Experimental results show that\nLaPraDoR achieves state-of-the-art performance compared with supervised dense\nretrieval models, and further analysis reveals the effectiveness of our\ntraining strategy and objectives. Compared to re-ranking, our lexicon-enhanced\napproach can be run in milliseconds (22.5x faster) while achieving superior\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08075","description":"<p>Spatial commonsense, the knowledge about spatial position and relationship\nbetween objects (like the relative size of a lion and a girl, and the position\nof a boy relative to a bicycle when cycling), is an important part of\ncommonsense knowledge. Although pretrained language models (PLMs) succeed in\nmany NLP tasks, they are shown to be ineffective in spatial commonsense\nreasoning. Starting from the observation that images are more likely to exhibit\nspatial commonsense than texts, we explore whether models with visual signals\nlearn more spatial commonsense than text-based PLMs. We propose a spatial\ncommonsense benchmark that focuses on the relative scales of objects, and the\npositional relationship between people and objects under different actions. We\nprobe PLMs and models with visual signals, including vision-language pretrained\nmodels and image synthesis models, on this benchmark, and find that image\nsynthesis models are more capable of learning accurate and consistent spatial\nknowledge than other models. The spatial knowledge from image synthesis models\nalso helps in natural language understanding tasks that require spatial\ncommonsense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plagiarism Detection in the Bengali Language: A Text Similarity-Based Approach. (arXiv:2203.13430v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.13430","description":"<p>Plagiarism means taking another person's work and not giving any credit to\nthem for it. Plagiarism is one of the most serious problems in academia and\namong researchers. Even though there are multiple tools available to detect\nplagiarism in a document but most of them are domain-specific and designed to\nwork in English texts, but plagiarism is not limited to a single language only.\nBengali is the most widely spoken language of Bangladesh and the second most\nspoken language in India with 300 million native speakers and 37 million\nsecond-language speakers. Plagiarism detection requires a large corpus for\ncomparison. Bengali Literature has a history of 1300 years. Hence most Bengali\nLiterature books are not yet digitalized properly. As there was no such corpus\npresent for our purpose so we have collected Bengali Literature books from the\nNational Digital Library of India and with a comprehensive methodology\nextracted texts from it and constructed our corpus. Our experimental results\nfind out average accuracy between 72.10 % - 79.89 % in text extraction using\nOCR. Levenshtein Distance algorithm is used for determining Plagiarism. We have\nbuilt a web application for end-user and successfully tested it for Plagiarism\ndetection in Bengali texts. In future, we aim to construct a corpus with more\nbooks for more accurate detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Satyajit Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Aniruddha Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_B/0/1/0/all/0/1\">Bittaswer Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Abhishek Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Overview of Indian Language Datasets used for Text Summarization. (arXiv:2203.16127v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.16127","description":"<p>In this paper, we survey Text Summarization (TS) datasets in Indian Languages\n(ILs), which are also low-resource languages (LRLs). We seek to answer one\nprimary question: is the pool of Indian Language Text Summarization (ILTS)\ndataset growing or is there a resource poverty? To an-swer the primary\nquestion, we pose two sub-questions that we seek about ILTS datasets: first,\nwhat characteristics: format and domain do ILTS datasets have? Second, how\ndifferent are those characteristics of ILTS datasets from high-resource\nlanguages (HRLs) particularly English. We focus on datasets reported in\npublished ILTS research works during 2012-2022. The survey of ILTS and English\ndatasets reveals two similarities and one contrast. The two similarities are:\nfirst, the domain of dataset commonly is news (Hermann et al., 2015). The\nsecond similarity is the format of the dataset which is both extractive and\nabstractive. The contrast is in how the research in dataset development has\nprogressed. ILs face a slow speed of development and public release of datasets\nas compared with English. We argue that the relatively lower number of ILTS\ndatasets is because of two reasons: first, absence of a dedicated forum for\ndeveloping TS tools and resources; and second, lack of shareable standard\ndatasets in the public domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Shagun Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_G/0/1/0/all/0/1\">Girish Nath Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperBox: A Supervised Approach for Hypernym Discovery using Box Embeddings. (arXiv:2204.02058v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02058","description":"<p>Hypernymy plays a fundamental role in many AI tasks like taxonomy learning,\nontology learning, etc. This has motivated the development of many automatic\nidentification methods for extracting this relation, most of which rely on word\ndistribution. We present a novel model HyperBox to learn box embeddings for\nhypernym discovery. Given an input term, HyperBox retrieves its suitable\nhypernym from a target corpus. For this task, we use the dataset published for\nSemEval 2018 Shared Task on Hypernym Discovery. We compare the performance of\nour model on two specific domains of knowledge: medical and music.\nExperimentally, we show that our model outperforms existing methods on the\nmajority of the evaluation metrics. Moreover, our model generalize well over\nunseen hypernymy pairs using only a small set of training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Maulik Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayan_D/0/1/0/all/0/1\">Dr. Apurva Narayan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Weakly Supervised Propagation Model for Rumor Verification and Stance Detection with Multiple Instance Learning. (arXiv:2204.02626v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02626","description":"<p>The diffusion of rumors on microblogs generally follows a propagation tree\nstructure, that provides valuable clues on how an original message is\ntransmitted and responded by users over time. Recent studies reveal that rumor\ndetection and stance detection are two different but relevant tasks which can\njointly enhance each other, e.g., rumors can be debunked by cross-checking the\nstances conveyed by their relevant microblog posts, and stances are also\nconditioned on the nature of the rumor. However, most stance detection methods\nrequire enormous post-level stance labels for training, which are\nlabor-intensive given a large number of posts. Enlightened by Multiple Instance\nLearning (MIL) scheme, we first represent the diffusion of claims with\nbottom-up and top-down trees, then propose two tree-structured weakly\nsupervised frameworks to jointly classify rumors and stances, where only the\nbag-level labels concerning claim's veracity are needed. Specifically, we\nconvert the multi-class problem into a multiple MIL-based binary classification\nproblem where each binary model focuses on differentiating a target stance or\nrumor type and other types. Finally, we propose a hierarchical attention\nmechanism to aggregate the binary predictions, including (1) a bottom-up or\ntop-down tree attention layer to aggregate binary stances into binary veracity;\nand (2) a discriminative attention layer to aggregate the binary class into\nfiner-grained classes. Extensive experiments conducted on three Twitter-based\ndatasets demonstrate promising performance of our model on both claim-level\nrumor detection and post-level stance classification compared with\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tweet Emotion Dynamics: Emotion Word Usage in Tweets from US and Canada. (arXiv:2204.04862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04862","description":"<p>Over the last decade, Twitter has emerged as one of the most influential\nforums for social, political, and health discourse. In this paper, we introduce\na massive dataset of more than 45 million geo-located tweets posted between\n2015 and 2021 from US and Canada (TUSC), especially curated for natural\nlanguage analysis. We also introduce Tweet Emotion Dynamics (TED) -- metrics to\ncapture patterns of emotions associated with tweets over time. We use TED and\nTUSC to explore the use of emotion-associated words across US and Canada;\nacross 2019 (pre-pandemic), 2020 (the year the pandemic hit), and 2021 (the\nsecond year of the pandemic); and across individual tweeters. We show that\nCanadian tweets tend to have higher valence, lower arousal, and higher\ndominance than the US tweets. Further, we show that the COVID-19 pandemic had a\nmarked impact on the emotional signature of tweets posted in 2020, when\ncompared to the adjoining years. Finally, we determine metrics of TED for\n170,000 tweeters to benchmark characteristics of TED metrics at an aggregate\nlevel. TUSC and the metrics for TED will enable a wide variety of research on\nstudying how we use language to express ourselves, persuade, communicate, and\ninfluence, with particularly promising applications in public health, affective\nscience, social science, and psychology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishnubhotla_K/0/1/0/all/0/1\">Krishnapriya Vishnubhotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How does fake news use a thumbnail? CLIP-based Multimodal Detection on the Unrepresentative News Image. (arXiv:2204.05533v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05533","description":"<p>This study investigates how fake news uses a thumbnail for a news article\nwith a focus on whether a news article's thumbnail represents the news content\ncorrectly. A news article shared with an irrelevant thumbnail can mislead\nreaders into having a wrong impression of the issue, especially in social media\nenvironments where users are less likely to click the link and consume the\nentire content. We propose to capture the degree of semantic incongruity in the\nmultimodal relation by using the pretrained CLIP representation. From a\nsource-level analysis, we found that fake news employs a more incongruous image\nto the main content than general news. Going further, we attempted to detect\nnews articles with image-text incongruity. Evaluation experiments suggest that\nCLIP-based methods can successfully detect news articles in which the thumbnail\nis semantically irrelevant to news text. This study contributes to the research\nby providing a novel view on tackling online fake news and misinformation. Code\nand datasets are available at\nhttps://github.com/ssu-humane/fake-news-thumbnail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyewon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1\">Yejun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1\">Kunwoo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets. (arXiv:2204.08997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08997","description":"<p>In recent years, interest has arisen in using machine learning to improve the\nefficiency of automatic medical consultation and enhance patient experience. In\nthis paper, we propose two frameworks to support automatic medical\nconsultation, namely doctor-patient dialogue understanding and task-oriented\ninteraction. A new large medical dialogue dataset with multi-level fine-grained\nannotations is introduced and five independent tasks are established, including\nnamed entity recognition, dialogue act classification, symptom label inference,\nmedical report generation and diagnosis-oriented dialogue policy. We report a\nset of benchmark results for each task, which shows the usability of the\ndataset and sets a baseline for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hongyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jianye Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jiajie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Augmentation for Named Entity Recognition with Meta Reweighting. (arXiv:2204.11406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.11406","description":"<p>Self-augmentation has been received increasing research interest recently to\nimprove named entity recognition (NER) performance in low-resource scenarios.\nToken substitution and mixup are two feasible heterogeneous self-augmentation\ntechniques for NER that can achieve effective performance with certain\nspecialized efforts. Noticeably, self-augmentation may introduce potentially\nnoisy augmented data. Prior research has mainly resorted to heuristic rule\nbased constraints to reduce the noise for specific self-augmentation\nindividually. In this paper, we revisit the two self-augmentation methods for\nNER, and propose a unified meta-reweighting strategy for these heterogeneous\nmethods to achieve a natural integration. Our method is easily extensible,\nimposing little effort on a specific self-augmentation method. Experiments on\ndifferent Chinese and English NER benchmarks demonstrate that our token\nsubstitution and mixup method, as well as their integration, can obtain\neffective performance improvement. Based on the meta-reweighting mechanism, we\ncan enhance the advantages of the self-augmentation techniques without extra\nefforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Linzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chunping Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PolyLoss: A Polynomial Expansion Perspective of Classification Loss Functions. (arXiv:2204.12511v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12511","description":"<p>Cross-entropy loss and focal loss are the most common choices when training\ndeep neural networks for classification problems. Generally speaking, however,\na good loss function can take on much more flexible forms, and should be\ntailored for different tasks and datasets. Motivated by how functions can be\napproximated via Taylor expansion, we propose a simple framework, named\nPolyLoss, to view and design loss functions as a linear combination of\npolynomial functions. Our PolyLoss allows the importance of different\npolynomial bases to be easily adjusted depending on the targeting tasks and\ndatasets, while naturally subsuming the aforementioned cross-entropy loss and\nfocal loss as special cases. Extensive experimental results show that the\noptimal choice within the PolyLoss is indeed dependent on the task and dataset.\nSimply by introducing one extra hyperparameter and adding one line of code, our\nPoly-1 formulation outperforms the cross-entropy loss and focal loss on 2D\nimage classification, instance segmentation, object detection, and 3D object\ndetection tasks, sometimes by a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leng_Z/0/1/0/all/0/1\">Zhaoqi Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingxing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cubuk_E/0/1/0/all/0/1\">Ekin Dogus Cubuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiaojie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shuyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1\">Dragomir Anguelov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coupled Iterative Refinement for 6D Multi-Object Pose Estimation. (arXiv:2204.12516v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12516","description":"<p>We address the task of 6D multi-object pose: given a set of known 3D objects\nand an RGB or RGB-D input image, we detect and estimate the 6D pose of each\nobject. We propose a new approach to 6D object pose estimation which consists\nof an end-to-end differentiable architecture that makes use of geometric\nknowledge. Our approach iteratively refines both pose and correspondence in a\ntightly coupled manner, allowing us to dynamically remove outliers to improve\naccuracy. We use a novel differentiable layer to perform pose refinement by\nsolving an optimization problem we refer to as Bidirectional Depth-Augmented\nPerspective-N-Point (BD-PnP). Our method achieves state-of-the-art accuracy on\nstandard 6D Object Pose benchmarks. Code is available at\nhttps://github.com/princeton-vl/Coupled-Iterative-Refinement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lipson_L/0/1/0/all/0/1\">Lahav Lipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teed_Z/0/1/0/all/0/1\">Zachary Teed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1\">Ankit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Unlabeled Data for Sketch-based Understanding. (arXiv:2204.12522v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12522","description":"<p>Sketch-based understanding is a critical component of human cognitive\nlearning and is a primitive communication means between humans. This topic has\nrecently attracted the interest of the computer vision community as sketching\nrepresents a powerful tool to express static objects and dynamic scenes.\nUnfortunately, despite its broad application domains, the current sketch-based\nmodels strongly rely on labels for supervised training, ignoring knowledge from\nunlabeled data, thus limiting the underlying generalization and the\napplicability. Therefore, we present a study about the use of unlabeled data to\nimprove a sketch-based model. To this end, we evaluate variations of VAE and\nsemi-supervised VAE, and present an extension of BYOL to deal with sketches.\nOur results show the superiority of sketch-BYOL, which outperforms other\nself-supervised approaches increasing the retrieval performance for known and\nunknown categories. Furthermore, we show how other tasks can benefit from our\nproposal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morales_J/0/1/0/all/0/1\">Javier Morales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murrugarra_Llerena_N/0/1/0/all/0/1\">Nils Murrugarra-Llerena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saavedra_J/0/1/0/all/0/1\">Jose M. Saavedra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expanding the Latent Space of StyleGAN for Real Face Editing. (arXiv:2204.12530v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12530","description":"<p>Recently, a surge of face editing techniques have been proposed to employ the\npretrained StyleGAN for semantic manipulation. To successfully edit a real\nimage, one must first convert the input image into StyleGAN's latent variables.\nHowever, it is still challenging to find latent variables, which have the\ncapacity for preserving the appearance of the input subject (e.g., identity,\nlighting, hairstyles) as well as enabling meaningful manipulations. In this\npaper, we present a method to expand the latent space of StyleGAN with\nadditional content features to break down the trade-off between low-distortion\nand high-editability. Specifically, we proposed a two-branch model, where the\nstyle branch first tackles the entanglement issue by the sparse manipulation of\nlatent codes, and the content branch then mitigates the distortion issue by\nleveraging the content and appearance details from the input image. We confirm\nthe effectiveness of our method using extensive qualitative and quantitative\nexperiments on real face editing and reconstruction tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_G/0/1/0/all/0/1\">Ghasedi Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HsiangTao_W/0/1/0/all/0/1\">Wu HsiangTao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaolong_Y/0/1/0/all/0/1\">Yang Jiaolong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_T/0/1/0/all/0/1\">Tong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_F/0/1/0/all/0/1\">Fu Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AccMPEG: Optimizing Video Encoding for Video Analytics. (arXiv:2204.12534v1 [cs.NI])","link":"http://arxiv.org/abs/2204.12534","description":"<p>With more videos being recorded by edge sensors (cameras) and analyzed by\ncomputer-vision deep neural nets (DNNs), a new breed of video streaming systems\nhas emerged, with the goal to compress and stream videos to remote servers in\nreal time while preserving enough information to allow highly accurate\ninference by the server-side DNNs. An ideal design of the video streaming\nsystem should simultaneously meet three key requirements: (1) low latency of\nencoding and streaming, (2) high accuracy of server-side DNNs, and (3) low\ncompute overheads on the camera. Unfortunately, despite many recent efforts,\nsuch video streaming system has hitherto been elusive, especially when serving\nadvanced vision tasks such as object detection or semantic segmentation. This\npaper presents AccMPEG, a new video encoding and streaming system that meets\nall the three requirements. The key is to learn how much the encoding quality\nat each (16x16) macroblock can influence the server-side DNN accuracy, which we\ncall accuracy gradient. Our insight is that these macroblock-level accuracy\ngradient can be inferred with sufficient precision by feeding the video frames\nthrough a cheap model. AccMPEG provides a suite of techniques that, given a new\nserver-side DNN, can quickly create a cheap model to infer the accuracy\ngradient on any new frame in near realtime. Our extensive evaluation of AccMPEG\non two types of edge devices (one Intel Xeon Silver 4100 CPU or NVIDIA Jetson\nNano) and three vision tasks (six recent pre-trained DNNs) shows that AccMPEG\n(with the same camera-side compute resources) can reduce the end-to-end\ninference delay by 10-43% without hurting accuracy compared to the\nstate-of-the-art baselines\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kuntai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arapin_A/0/1/0/all/0/1\">Anton Arapin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haodong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Zhengxu Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junchen Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Change Detection using Multi-Temporal Airborne LiDAR Data. (arXiv:2204.12535v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12535","description":"<p>Building change detection is essential for monitoring urbanization, disaster\nassessment, urban planning and frequently updating the maps. 3D structure\ninformation from airborne light detection and ranging (LiDAR) is very effective\nfor detecting urban changes. But the 3D point cloud from airborne LiDAR(ALS)\nholds an enormous amount of unordered and irregularly sparse information.\nHandling such data is tricky and consumes large memory for processing. Most of\nthis information is not necessary when we are looking for a particular type of\nurban change. In this study, we propose an automatic method that reduces the 3D\npoint clouds into a much smaller representation without losing the necessary\ninformation required for detecting Building changes. The method utilizes the\nDeep Learning(DL) model U-Net for segmenting the buildings from the background.\nProduced segmentation maps are then processed further for detecting changes and\nthe results are refined using morphological methods. For the change detection\ntask, we used multi-temporal airborne LiDAR data. The data is acquired over\nStockholm in the years 2017 and 2019. The changes in buildings are classified\ninto four types: 'newly built', 'demolished', 'taller' and 'shorter'. The\ndetected changes are visualized in one map for better interpretation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Ritu Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nascetti_A/0/1/0/all/0/1\">Andrea Nascetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yifang Ban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi stain graph fusion for multimodal integration in pathology. (arXiv:2204.12541v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12541","description":"<p>In pathology, tissue samples are assessed using multiple staining techniques\nto enhance contrast in unique histologic features. In this paper, we introduce\na multimodal CNN-GNN based graph fusion approach that leverages complementary\ninformation from multiple non-registered histopathology images to predict\npathologic scores. We demonstrate this approach in nonalcoholic steatohepatitis\n(NASH) by predicting CRN fibrosis stage and NAFLD Activity Score (NAS). Primary\nassessment of NASH typically requires liver biopsy evaluation on two\nhistological stains: Trichrome (TC) and hematoxylin and eosin (H&amp;E). Our\nmultimodal approach learns to extract complementary information from TC and H&amp;E\ngraphs corresponding to each stain while simultaneously learning an optimal\npolicy to combine this information. We report up to 20% improvement in\npredicting fibrosis stage and NAS component grades over single-stain modeling\napproaches, measured by computing linearly weighted Cohen's kappa between\nmachine-derived vs. pathologist consensus scores. Broadly, this paper\ndemonstrates the value of leveraging diverse pathology images for improved\nML-powered histologic assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Dwivedi_C/0/1/0/all/0/1\">Chaitanya Dwivedi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nofallah_S/0/1/0/all/0/1\">Shima Nofallah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pouryahya_M/0/1/0/all/0/1\">Maryam Pouryahya</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iyer_J/0/1/0/all/0/1\">Janani Iyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leidal_K/0/1/0/all/0/1\">Kenneth Leidal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_C/0/1/0/all/0/1\">Chuhan Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watkins_T/0/1/0/all/0/1\">Timothy Watkins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Billin_A/0/1/0/all/0/1\">Andrew Billin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myers_R/0/1/0/all/0/1\">Robert Myers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Abel_J/0/1/0/all/0/1\">John Abel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Behrooz_A/0/1/0/all/0/1\">Ali Behrooz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for Tamil TrollMeme Classification. (arXiv:2204.12587v1 [cs.MM])","link":"http://arxiv.org/abs/2204.12587","description":"<p>Social media platforms often act as breeding grounds for various forms of\ntrolling or malicious content targeting users or communities. One way of\ntrolling users is by creating memes, which in most cases unites an image with a\nshort piece of text embedded on top of it. The situation is more complex for\nmultilingual(e.g., Tamil) memes due to the lack of benchmark datasets and\nmodels. We explore several models to detect Troll memes in Tamil based on the\nshared task, \"Troll Meme Classification in DravidianLangTech2022\" at ACL-2022.\nWe observe while the text-based model MURIL performs better for Non-troll meme\nclassification, the image-based model VGG16 performs better for Troll-meme\nclassification. Further fusing these two modalities help us achieve stable\noutcomes in both classes. Our fusion model achieved a 0.561 weighted average F1\nscore and ranked second in this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mithun Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1\">Somnath Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Animesh Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Influence of the Other-Race Effect on Susceptibility to Face Morphing Attacks. (arXiv:2204.12591v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12591","description":"<p>Facial morphs created between two identities resemble both of the faces used\nto create the morph. Consequently, humans and machines are prone to mistake\nmorphs made from two identities for either of the faces used to create the\nmorph. This vulnerability has been exploited in \"morph attacks\" in security\nscenarios. Here, we asked whether the \"other-race effect\" (ORE) -- the human\nadvantage for identifying own- vs. other-race faces -- exacerbates morph attack\nsusceptibility for humans. We also asked whether face-identification\nperformance in a deep convolutional neural network (DCNN) is affected by the\nrace of morphed faces. Caucasian (CA) and East-Asian (EA) participants\nperformed a face-identity matching task on pairs of CA and EA face images in\ntwo conditions. In the morph condition, different-identity pairs consisted of\nan image of identity \"A\" and a 50/50 morph between images of identity \"A\" and\n\"B\". In the baseline condition, morphs of different identities never appeared.\nAs expected, morphs were identified mistakenly more often than original face\nimages. Moreover, CA participants showed an advantage for CA faces in\ncomparison to EA faces (a partial ORE). Of primary interest, morph\nidentification was substantially worse for cross-race faces than for own-race\nfaces. Similar to humans, the DCNN performed more accurately for original face\nimages than for morphed image pairs. Notably, the deep network proved\nsubstantially more accurate than humans in both cases. The results point to the\npossibility that DCNNs might be useful for improving face identification\naccuracy when morphed faces are presented. They also indicate the significance\nof the ORE in morph attack susceptibility in applied settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mallick_S/0/1/0/all/0/1\">Snipta Mallick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeckeln_G/0/1/0/all/0/1\">Geraldine Jeckeln</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parde_C/0/1/0/all/0/1\">Connor J. Parde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_A/0/1/0/all/0/1\">Alice J. O&#x27;Toole</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Self-taught Learning-based Representations for Facial Emotion Recognition. (arXiv:2204.12624v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12624","description":"<p>This work describes different strategies to generate unsupervised\nrepresentations obtained through the concept of self-taught learning for facial\nemotion recognition (FER). The idea is to create complementary representations\npromoting diversity by varying the autoencoders' initialization, architecture,\nand training data. SVM, Bagging, Random Forest, and a dynamic ensemble\nselection method are evaluated as final classification methods. Experimental\nresults on Jaffe and Cohn-Kanade datasets using a leave-one-subject-out\nprotocol show that FER methods based on the proposed diverse representations\ncompare favorably against state-of-the-art approaches that also explore\nunsupervised feature learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Delazeri_B/0/1/0/all/0/1\">Bruna Delazeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veras_L/0/1/0/all/0/1\">Leonardo L. Veras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britto_A/0/1/0/all/0/1\">Alceu de S. Britto Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barddal_J/0/1/0/all/0/1\">Jean Paul Barddal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro L. Koerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCGC : Self-Supervised Contrastive Graph Clustering. (arXiv:2204.12656v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12656","description":"<p>Graph clustering discovers groups or communities within networks. Deep\nlearning methods such as autoencoders (AE) extract effective clustering and\ndownstream representations but cannot incorporate rich structural information.\nWhile Graph Neural Networks (GNN) have shown great success in encoding graph\nstructure, typical GNNs based on convolution or attention variants suffer from\nover-smoothing, noise, heterophily, are computationally expensive and typically\nrequire the complete graph being present. Instead, we propose Self-Supervised\nContrastive Graph Clustering (SCGC), which imposes graph-structure via\ncontrastive loss signals to learn discriminative node representations and\niteratively refined soft cluster labels. We also propose SCGC*, with a more\neffective, novel, Influence Augmented Contrastive (IAC) loss to fuse richer\nstructural information, and half the original model parameters. SCGC(*) is\nfaster with simple linear units, completely eliminate convolutions and\nattention of traditional GNNs, yet efficiently incorporates structure. It is\nimpervious to layer depth and robust to over-smoothing, incorrect edges and\nheterophily. It is scalable by batching, a limitation in many prior GNN models,\nand trivially parallelizable. We obtain significant improvements over\nstate-of-the-art on a wide range of benchmark graph datasets, including images,\nsensor data, text, and citation networks efficiently. Specifically, 20% on ARI\nand 18% on NMI for DBLP; overall 55% reduction in training time and overall,\n81% reduction on inference time. Our code is available at :\nhttps://github.com/gayanku/SCGC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulatilleke_G/0/1/0/all/0/1\">Gayan K. Kulatilleke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portmann_M/0/1/0/all/0/1\">Marius Portmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1\">Shekhar S. Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation. (arXiv:2204.12667v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12667","description":"<p>Test-time adaptation approaches have recently emerged as a practical solution\nfor handling domain shift without access to the source domain data. In this\npaper, we propose and explore a new multi-modal extension of test-time\nadaptation for 3D semantic segmentation. We find that directly applying\nexisting methods usually results in performance instability at test time\nbecause multi-modal input is not considered jointly. To design a framework that\ncan take full advantage of multi-modality, where each modality provides\nregularized self-supervisory signals to other modalities, we propose two\ncomplementary modules within and across the modalities. First, Intra-modal\nPseudolabel Generation (Intra-PG) is introduced to obtain reliable pseudo\nlabels within each modality by aggregating information from two models that are\nboth pre-trained on source data but updated with target data at different\npaces. Second, Inter-modal Pseudo-label Refinement (Inter-PR) adaptively\nselects more reliable pseudo labels from different modalities based on a\nproposed consistency scheme. Experiments demonstrate that our regularized\npseudo labels produce stable self-learning signals in numerous multi-modal\ntest-time adaptation scenarios for 3D semantic segmentation. Visit our project\nwebsite at https://www.nec-labs.com/~mas/MM-TTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1\">Inkyu Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bingbing Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulter_S/0/1/0/all/0/1\">Samuel Schulter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Buyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Sparsh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1\">Kuk-Jin Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimized latent-code selection for explainable conditional text-to-image GANs. (arXiv:2204.12678v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12678","description":"<p>The task of text-to-image generation has achieved remarkable progress due to\nthe advances in the conditional generative adversarial networks (GANs).\nHowever, existing conditional text-to-image GANs approaches mostly concentrate\non improving both image quality and semantic relevance but ignore the\nexplainability of the model which plays a vital role in real-world\napplications. In this paper, we present a variety of techniques to take a deep\nlook into the latent space and semantic space of the conditional text-to-image\nGANs model. We introduce pairwise linear interpolation of latent codes and\n`linguistic' linear interpolation to study what the model has learned within\nthe latent space and `linguistic' embeddings. Subsequently, we extend linear\ninterpolation to triangular interpolation conditioned on three corners to\nfurther analyze the model. After that, we build a Good/Bad data set containing\nunsuccessfully and successfully synthetic samples and corresponding latent\ncodes for the image-quality research. Based on this data set, we propose a\nframework for finding good latent codes by utilizing a linear SVM. Experimental\nresults on the recent DiverGAN generator trained on two benchmark data sets\nqualitatively prove the effectiveness of our presented techniques, with a\nbetter than 94\\% accuracy in predicting ${Good}$/${Bad}$ classes for latent\nvectors. The Good/Bad data set is publicly available at\nhttps://zenodo.org/record/5850224#.YeGMwP7MKUk.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Transferability of Adversarial Examples with Restructure Embedded Patches. (arXiv:2204.12680v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12680","description":"<p>Vision transformers (ViTs) have demonstrated impressive performance in\nvarious computer vision tasks. However, the adversarial examples generated by\nViTs are challenging to transfer to other networks with different structures.\nRecent attack methods do not consider the specificity of ViTs architecture and\nself-attention mechanism, which leads to poor transferability of the generated\nadversarial samples by ViTs. We attack the unique self-attention mechanism in\nViTs by restructuring the embedded patches of the input. The restructured\nembedded patches enable the self-attention mechanism to obtain more diverse\npatches connections and help ViTs keep regions of interest on the object.\nTherefore, we propose an attack method against the unique self-attention\nmechanism in ViTs, called Self-Attention Patches Restructure (SAPR). Our method\nis simple to implement yet efficient and applicable to any self-attention based\nnetwork and gradient transferability-based attack methods. We evaluate attack\ntransferability on black-box models with different structures. The result show\nthat our method generates adversarial examples on white-box ViTs with higher\ntransferability and higher image quality. Our research advances the development\nof black-box transfer attacks on ViTs and demonstrates the feasibility of using\nwhite-box ViTs to attack other black-box models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yu-an Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yajie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1\">Haoran Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shangbo Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanzhang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Density-preserving Deep Point Cloud Compression. (arXiv:2204.12684v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12684","description":"<p>Local density of point clouds is crucial for representing local details, but\nhas been overlooked by existing point cloud compression methods. To address\nthis, we propose a novel deep point cloud compression method that preserves\nlocal density information. Our method works in an auto-encoder fashion: the\nencoder downsamples the points and learns point-wise features, while the\ndecoder upsamples the points using these features. Specifically, we propose to\nencode local geometry and density with three embeddings: density embedding,\nlocal position embedding and ancestor embedding. During the decoding, we\nexplicitly predict the upsampling factor for each point, and the directions and\nscales of the upsampled points. To mitigate the clustered points issue in\nexisting methods, we design a novel sub-point convolution layer, and an\nupsampling block with adaptive scale. Furthermore, our method can also compress\npoint-wise attributes, such as normal. Extensive qualitative and quantitative\nresults on SemanticKITTI and ShapeNet demonstrate that our method achieves the\nstate-of-the-art rate-distortion trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xinlin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Danhang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Face Anti-Spoofing with Dual Probabilistic Modeling. (arXiv:2204.12685v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12685","description":"<p>The field of face anti-spoofing (FAS) has witnessed great progress with the\nsurge of deep learning. Due to its data-driven nature, existing FAS methods are\nsensitive to the noise in the dataset, which will hurdle the learning process.\nHowever, very few works consider noise modeling in FAS. In this work, we\nattempt to fill this gap by automatically addressing the noise problem from\nboth label and data perspectives in a probabilistic manner. Specifically, we\npropose a unified framework called Dual Probabilistic Modeling (DPM), with two\ndedicated modules, DPM-LQ (Label Quality aware learning) and DPM-DQ (Data\nQuality aware learning). Both modules are designed based on the assumption that\ndata and label should form coherent probabilistic distributions. DPM-LQ is able\nto produce robust feature representations without overfitting to the\ndistribution of noisy semantic labels. DPM-DQ can eliminate data noise from\n`False Reject' and `False Accept' during inference by correcting the prediction\nconfidence of noisy data based on its quality distribution. Both modules can be\nincorporated into existing deep networks seamlessly and efficiently.\nFurthermore, we propose the generalized DPM to address the noise problem in\npractical usage without the need of semantic annotations. Extensive experiments\ndemonstrate that this probabilistic modeling can 1) significantly improve the\naccuracy, and 2) make the model robust to the noise in real-world datasets.\nWithout bells and whistles, our proposed DPM achieves state-of-the-art\nperformance on multiple standard FAS benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yichao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grasping the Arrow of Time from the Singularity: Decoding Micromotion in Low-dimensional Latent Spaces from StyleGAN. (arXiv:2204.12696v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12696","description":"<p>The disentanglement of StyleGAN latent space has paved the way for realistic\nand controllable image editing, but does StyleGAN know anything about temporal\nmotion, as it was only trained on static images? To study the motion features\nin the latent space of StyleGAN, in this paper, we hypothesize and demonstrate\nthat a series of meaningful, natural, and versatile small, local movements\n(referred to as \"micromotion\", such as expression, head movement, and aging\neffect) can be represented in low-rank spaces extracted from the latent space\nof a conventionally pre-trained StyleGAN-v2 model for face generation, with the\nguidance of proper \"anchors\" in the form of either short text or video clips.\nStarting from one target face image, with the editing direction decoded from\nthe low-rank space, its micromotion features can be represented as simple as an\naffine transformation over its latent feature. Perhaps more surprisingly, such\nmicromotion subspace, even learned from just single target face, can be\npainlessly transferred to other unseen face images, even those from vastly\ndifferent domains (such as oil painting, cartoon, and sculpture faces). It\ndemonstrates that the local feature geometry corresponding to one type of\nmicromotion is aligned across different face subjects, and hence that\nStyleGAN-v2 is indeed \"secretly\" aware of the subject-disentangled feature\nvariations caused by that micromotion. We present various successful examples\nof applying our low-dimensional micromotion subspace technique to directly and\neffortlessly manipulate faces, showing high robustness, low computational\noverhead, and impressive domain transferability. Our codes are available at\nhttps://github.com/wuqiuche/micromotion-StyleGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiucheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yifan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junru Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping suburban bicycle lanes using street scene images and deep learning. (arXiv:2204.12701v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12701","description":"<p>On-road bicycle lanes improve safety for cyclists, and encourage\nparticipation in cycling for active transport and recreation. With many local\nauthorities responsible for portions of the infrastructure, official maps and\ndatasets of bicycle lanes may be out-of-date and incomplete. Even\n\"crowdsourced\" databases may have significant gaps, especially outside popular\nmetropolitan areas. This thesis presents a method to create a map of bicycle\nlanes in a survey area by taking sample street scene images from each road, and\nthen applying a deep learning model that has been trained to recognise bicycle\nlane symbols. The list of coordinates where bicycle lane markings are detected\nis then correlated to geospatial data about the road network to record bicycle\nlane routes. The method was applied to successfully build a map for a survey\narea in the outer suburbs of Melbourne. It was able to identify bicycle lanes\nnot previously recorded in the official state government dataset,\nOpenStreetMap, or the \"biking\" layer of Google Maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxton_T/0/1/0/all/0/1\">Tyler Saxton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dataset for Robust and Accurate Leading Vehicle Velocity Recognition. (arXiv:2204.12717v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12717","description":"<p>Recognition of the surrounding environment using a camera is an important\ntechnology in Advanced Driver-Assistance Systems and Autonomous Driving, and\nrecognition technology is often solved by machine learning approaches such as\ndeep learning in recent years. Machine learning requires datasets for learning\nand evaluation. To develop robust recognition technology in the real world, in\naddition to normal driving environment, data in environments that are difficult\nfor cameras such as rainy weather or nighttime are essential. We have\nconstructed a dataset that one can benchmark the technology, targeting the\nvelocity recognition of the leading vehicle. This task is an important one for\nthe Advanced Driver-Assistance Systems and Autonomous Driving. The dataset is\navailable at https://signate.jp/competitions/657\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ogawa_G/0/1/0/all/0/1\">Genya Ogawa</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Saito_T/0/1/0/all/0/1\">Toru Saito</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Aoi_N/0/1/0/all/0/1\">Noriyuki Aoi</a> (2) ((1) Subaru Corporation, (2) Signate Inc.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRE-NAS: Predictor-assisted Evolutionary Neural Architecture Search. (arXiv:2204.12726v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12726","description":"<p>Neural architecture search (NAS) aims to automate architecture engineering in\nneural networks. This often requires a high computational overhead to evaluate\na number of candidate networks from the set of all possible networks in the\nsearch space during the search. Prediction of the networks' performance can\nalleviate this high computational overhead by mitigating the need for\nevaluating every candidate network. Developing such a predictor typically\nrequires a large number of evaluated architectures which may be difficult to\nobtain. We address this challenge by proposing a novel evolutionary-based NAS\nstrategy, Predictor-assisted E-NAS (PRE-NAS), which can perform well even with\nan extremely small number of evaluated architectures. PRE-NAS leverages new\nevolutionary search strategies and integrates high-fidelity weight inheritance\nover generations. Unlike one-shot strategies, which may suffer from bias in the\nevaluation due to weight sharing, offspring candidates in PRE-NAS are\ntopologically homogeneous, which circumvents bias and leads to more accurate\npredictions. Extensive experiments on NAS-Bench-201 and DARTS search spaces\nshow that PRE-NAS can outperform state-of-the-art NAS methods. With only a\nsingle GPU searching for 0.6 days, competitive architecture can be found by\nPRE-NAS which achieves 2.40% and 24% test error rates on CIFAR-10 and ImageNet\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yameng Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_A/0/1/0/all/0/1\">Andy Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciesielski_V/0/1/0/all/0/1\">Vic Ciesielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayek_H/0/1/0/all/0/1\">Haytham M. Fayek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Centered Prior-Guided and Task-Dependent Multi-Task Representation Learning for Action Recognition Pre-Training. (arXiv:2204.12729v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12729","description":"<p>Recently, much progress has been made for self-supervised action recognition.\nMost existing approaches emphasize the contrastive relations among videos,\nincluding appearance and motion consistency. However, two main issues remain\nfor existing pre-training methods: 1) the learned representation is neutral and\nnot informative for a specific task; 2) multi-task learning-based pre-training\nsometimes leads to sub-optimal solutions due to inconsistent domains of\ndifferent tasks. To address the above issues, we propose a novel action\nrecognition pre-training framework, which exploits human-centered prior\nknowledge that generates more informative representation, and avoids the\nconflict between multiple tasks by using task-dependent representations.\nSpecifically, we distill knowledge from a human parsing model to enrich the\nsemantic capability of representation. In addition, we combine knowledge\ndistillation with contrastive learning to constitute a task-dependent\nmulti-task framework. We achieve state-of-the-art performance on two popular\nbenchmarks for action recognition task, i.e., UCF101 and HMDB51, verifying the\neffectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhanhao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gaoang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-Head Convolutional Neural Network With Multi-path Attention improves Image Denoising. (arXiv:2204.12736v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12736","description":"<p>Recently, convolutional neural networks (CNNs) and attention mechanisms have\nbeen widely used in image denoising and achieved satisfactory performance.\nHowever, the previous works mostly use a single head to receive the noisy\nimage, limiting the richness of extracted features. Therefore, a novel CNN with\nmultiple heads (MH) named MHCNN is proposed in this paper, whose heads will\nreceive the input images rotated by different rotation angles. MH makes MHCNN\nsimultaneously utilize features of rotated images to remove noise. We also\npresent a novel multi-path attention mechanism (MPA) to integrate these\nfeatures effectively. Unlike previous attention mechanisms that handle\npixel-level, channel-level, and patch-level features, MPA focuses on features\nat the image level. Experiments show MHCNN surpasses other state-of-the-art CNN\nmodels on additive white Gaussian noise (AWGN) denoising and real-world image\ndenoising. Its peak signal-to-noise ratio (PSNR) results are higher than other\nnetworks, such as DnCNN, BRDNet, RIDNet, PAN-Net, and CSANN. It is also\ndemonstrated that the proposed MH with MPA mechanism can be used as a pluggable\ncomponent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_M/0/1/0/all/0/1\">Meijun Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ye Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lihong Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Text Erasing with Controllable Image Synthesis. (arXiv:2204.12743v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12743","description":"<p>Recent efforts on scene text erasing have shown promising results. However,\nexisting methods require rich yet costly label annotations to obtain robust\nmodels, which limits the use for practical applications. To this end, we study\nan unsupervised scenario by proposing a novel Self-supervised Text Erasing\n(STE) framework that jointly learns to synthesize training images with erasure\nground-truth and accurately erase texts in the real world. We first design a\nstyle-aware image synthesis function to generate synthetic images with diverse\nstyled texts based on two synthetic mechanisms. To bridge the text style gap\nbetween the synthetic and real-world data, a policy network is constructed to\ncontrol the synthetic mechanisms by picking style parameters with the guidance\nof two specifically designed rewards. The synthetic training images with\nerasure ground-truth are then fed to train a coarse-to-fine erasing network. To\nproduce better erasing outputs, a triplet erasure loss is designed to enforce\nthe refinement stage to recover background textures. Moreover, we provide a new\ndataset (called PosterErase), which contains 60K high-resolution posters with\ntexts and is more challenging for the text erasing task. The proposed method\nhas been extensively evaluated with both PosterErase and the widely-used\nSCUT-Enstext dataset. Notably, on PosterErase, our unsupervised method achieves\n5.07 in terms of FID, with a relative performance of 20.9% over existing\nsupervised baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1\">Gangwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Ying Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Driving Car Steering Angle Prediction: Let Transformer Be a Car Again. (arXiv:2204.12748v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12748","description":"<p>Self-driving vehicles are expected to be a massive economic influence over\nthe coming decades. Udacity https://www.udacity.com/ has been working on a\ncompletely open-source self driving car. Thus, it regularly organizes various\ncompetitions, one of which was dedicated to steering angle prediction task. In\nthis work, we perform an extensive study on this particular task by exploring\nthe Udacity Self-driving Car Challenge 2. We provide insights on the previous\nteams' solutions. Moreover, we propose our new architecture that is inspired by\nsome of the teams. We report our performance and compare it with multiple\nbaseline architectures as well as other teams' solutions. We make our work\navailable on GitHub and hope it is useful for the Udacity community and brings\ninsights for future works https://github.com/chingisooinar/AI_self-driving-car\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oinar_C/0/1/0/all/0/1\">Chingis Oinar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Eunmin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Head Generation Driven by Speech-Related Facial Action Units and Audio- Based on Multimodal Representation Fusion. (arXiv:2204.12756v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12756","description":"<p>Talking head generation is to synthesize a lip-synchronized talking head\nvideo by inputting an arbitrary face image and corresponding audio clips.\nExisting methods ignore not only the interaction and relationship of\ncross-modal information, but also the local driving information of the mouth\nmuscles. In this study, we propose a novel generative framework that contains a\ndilated non-causal temporal convolutional self-attention network as a\nmultimodal fusion module to promote the relationship learning of cross-modal\nfeatures. In addition, our proposed method uses both audio- and speech-related\nfacial action units (AUs) as driving information. Speech-related AU information\ncan guide mouth movements more accurately. Because speech is highly correlated\nwith speech-related AUs, we propose an audio-to-AU module to predict\nspeech-related AU information. We utilize pre-trained AU classifier to ensure\nthat the generated images contain correct AU information. We verify the\neffectiveness of the proposed model on the GRID and TCD-TIMIT datasets. An\nablation study is also conducted to verify the contribution of each component.\nThe results of quantitative and qualitative experiments demonstrate that our\nmethod outperforms existing methods in terms of both image quality and lip-sync\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Longbiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Scalable Combinatorial Solver for Elastic Geometrically Consistent 3D Shape Matching. (arXiv:2204.12805v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12805","description":"<p>We present a scalable combinatorial algorithm for globally optimizing over\nthe space of geometrically consistent mappings between 3D shapes. We use the\nmathematically elegant formalism proposed by Windheuser et al. (ICCV 2011)\nwhere 3D shape matching was formulated as an integer linear program over the\nspace of orientation-preserving diffeomorphisms. Until now, the resulting\nformulation had limited practical applicability due to its complicated\nconstraint structure and its large size. We propose a novel primal heuristic\ncoupled with a Lagrange dual problem that is several orders of magnitudes\nfaster compared to previous solvers. This allows us to handle shapes with\nsubstantially more triangles than previously solvable. We demonstrate\ncompelling results on diverse datasets, and, even showcase that we can address\nthe challenging setting of matching two partial shapes without availability of\ncomplete shapes. Our code is publicly available at\n<a href=\"http://github.com/paul0noah/sm-comb\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roetzer_P/0/1/0/all/0/1\">Paul Roetzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The MeVer DeepFake Detection Service: Lessons Learnt from Developing and Deploying in the Wild. (arXiv:2204.12816v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12816","description":"<p>Enabled by recent improvements in generation methodologies, DeepFakes have\nbecome mainstream due to their increasingly better visual quality, the increase\nin easy-to-use generation tools and the rapid dissemination through social\nmedia. This fact poses a severe threat to our societies with the potential to\nerode social cohesion and influence our democracies. To mitigate the threat,\nnumerous DeepFake detection schemes have been introduced in the literature but\nvery few provide a web service that can be used in the wild. In this paper, we\nintroduce the MeVer DeepFake detection service, a web service detecting deep\nlearning manipulations in images and video. We present the design and\nimplementation of the proposed processing pipeline that involves a model\nensemble scheme, and we endow the service with a model card for transparency.\nExperimental results show that our service performs robustly on the three\nbenchmark datasets while being vulnerable to Adversarial Attacks. Finally, we\noutline our experience and lessons learned when deploying a research system\ninto production in the hopes that it will be useful to other academic and\nindustry teams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baxevanakis_S/0/1/0/all/0/1\">Spyridon Baxevanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordopatis_Zilos_G/0/1/0/all/0/1\">Giorgos Kordopatis-Zilos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galopoulos_P/0/1/0/all/0/1\">Panagiotis Galopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostolidis_L/0/1/0/all/0/1\">Lazaros Apostolidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levacher_K/0/1/0/all/0/1\">Killian Levacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1\">Ipek B. Schlicht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teyssou_D/0/1/0/all/0/1\">Denis Teyssou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kompatsiaris_I/0/1/0/all/0/1\">Ioannis Kompatsiaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1\">Symeon Papadopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CATrans: Context and Affinity Transformer for Few-Shot Segmentation. (arXiv:2204.12817v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12817","description":"<p>Few-shot segmentation (FSS) aims to segment novel categories given scarce\nannotated support images. The crux of FSS is how to aggregate dense\ncorrelations between support and query images for query segmentation while\nbeing robust to the large variations in appearance and context. To this end,\nprevious Transformer-based methods explore global consensus either on context\nsimilarity or affinity map between support-query pairs. In this work, we\neffectively integrate the context and affinity information via the proposed\nnovel Context and Affinity Transformer (CATrans) in a hierarchical\narchitecture. Specifically, the Relation-guided Context Transformer (RCT)\npropagates context information from support to query images conditioned on more\ninformative support features. Based on the observation that a huge feature\ndistinction between support and query pairs brings barriers for context\nknowledge transfer, the Relation-guided Affinity Transformer (RAT) measures\nattention-aware affinity as auxiliary information for FSS, in which the\nself-affinity is responsible for more reliable cross-affinity. We conduct\nexperiments to demonstrate the effectiveness of the proposed model,\noutperforming the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sitong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conformer and Blind Noisy Students for Improved Image Quality Assessment. (arXiv:2204.12819v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12819","description":"<p>Generative models for image restoration, enhancement, and generation have\nsignificantly improved the quality of the generated images. Surprisingly, these\nmodels produce more pleasant images to the human eye than other methods, yet,\nthey may get a lower perceptual quality score using traditional perceptual\nquality metrics such as PSNR or SSIM. Therefore, it is necessary to develop a\nquantitative metric to reflect the performance of new algorithms, which should\nbe well-aligned with the person's mean opinion score (MOS). Learning-based\napproaches for perceptual image quality assessment (IQA) usually require both\nthe distorted and reference image for measuring the perceptual quality\naccurately. However, commonly only the distorted or generated image is\navailable. In this work, we explore the performance of transformer-based\nfull-reference IQA models. We also propose a method for IQA based on\nsemi-supervised knowledge distillation from full-reference teacher models into\nblind student models using noisy pseudo-labeled data. Our approaches achieved\ncompetitive results on the NTIRE 2022 Perceptual Image Quality Assessment\nChallenge: our full-reference model was ranked 4th, and our blind noisy student\nwas ranked 3rd among 70 participants, each in their respective track.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burchi_M/0/1/0/all/0/1\">Maxime Burchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Power Bundle Adjustment for Large-Scale 3D Reconstruction. (arXiv:2204.12834v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12834","description":"<p>We present the design and the implementation of a new expansion type\nalgorithm to solve large-scale bundle adjustment problems. Our approach --\ncalled Power Bundle Adjustment -- is based on the power series expansion of the\ninverse Schur complement. This initiates a new family of solvers that we call\ninverse expansion methods. We show with the real-world BAL dataset that the\nproposed solver challenges the traditional direct and iterative methods. The\nsolution of the normal equation is significantly accelerated, even for reaching\na very high accuracy. Last but not least, our solver can also complement a\nrecently presented distributed bundle adjustment framework. We demonstrate that\nemploying the proposed Power Bundle Adjustment as a sub-problem solver greatly\nimproves speed and accuracy of the distributed optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1\">Simon Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demmel_N/0/1/0/all/0/1\">Nikolaus Demmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BBBD: Bounding Box Based Detector for Occlusion Detection and Order Recovery. (arXiv:2204.12841v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12841","description":"<p>Occlusion handling is one of the challenges of object detection and\nsegmentation, and scene understanding. Because objects appear differently when\nthey are occluded in varying degree, angle, and locations. Therefore,\ndetermining the existence of occlusion between objects and their order in a\nscene is a fundamental requirement for semantic understanding. Existing works\nmostly use deep learning based models to retrieve the order of the instances in\nan image or for occlusion detection. This requires labelled occluded data and\nit is time consuming. In this paper, we propose a simpler and faster method\nthat can perform both operations without any training and only requires the\nmodal segmentation masks. For occlusion detection, instead of scanning the two\nobjects entirely, we only focus on the intersected area between their bounding\nboxes. Similarly, we use the segmentation mask inside the same area to recover\nthe depth-ordering. When tested on COCOA dataset, our method achieves +8% and\n+5% more accuracy than the baselines in order recovery and occlusion detection\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saleh_K/0/1/0/all/0/1\">Kaziwa Saleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vamossy_Z/0/1/0/all/0/1\">Zoltan Vamossy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forecasting Urban Development from Satellite Images. (arXiv:2204.12875v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12875","description":"<p>Forecasting where and when new buildings will emerge is a rather unexplored\nniche topic, but relevant in disciplines such as urban planning, agriculture,\nresource management, and even autonomous flight. In this work, we present a\nmethod that accomplishes this task using satellite images and a custom neural\nnetwork training procedure. In stage A, a DeepLapv3+ backbone is pretrained\nthrough a Siamese network architecture aimed at solving a building change\ndetection task. In stage B, we transfer the backbone into a change forecasting\nmodel that relies solely on the initial input image. We also transfer the\nbackbone into a forecasting model predicting the correct time range of the\nfuture change. For our experiments, we use the SpaceNet7 dataset with 960 km2\nspatial extension and 24 monthly frames. We found that our training strategy\nconsistently outperforms the traditional pretraining on the ImageNet dataset.\nEspecially with longer forecasting ranges of 24 months, we observe F1 scores of\n24% instead of 16%. Furthermore, we found that our method performed well in\nforecasting the times of future building constructions. Hereby, the strengths\nof our custom pretraining become especially apparent when we increase the\ndifficulty of the task by predicting finer time windows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Metzger_N/0/1/0/all/0/1\">Nando Metzger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-rank Meets Sparseness: An Integrated Spatial-Spectral Total Variation Approach to Hyperspectral Denoising. (arXiv:2204.12879v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12879","description":"<p>Spatial-Spectral Total Variation (SSTV) can quantify local smoothness of\nimage structures, so it is widely used in hyperspectral image (HSI) processing\ntasks. Essentially, SSTV assumes a sparse structure of gradient maps calculated\nalong the spatial and spectral directions. In fact, these gradient tensors are\nnot only sparse, but also (approximately) low-rank under FFT, which we have\nverified by numerical tests and theoretical analysis. Based on this fact, we\npropose a novel TV regularization to simultaneously characterize the sparsity\nand low-rank priors of the gradient map (LRSTV). The new regularization not\nonly imposes sparsity on the gradient map itself, but also penalize the rank on\nthe gradient map after Fourier transform along the spectral dimension. It\nnaturally encodes the sparsity and lowrank priors of the gradient map, and thus\nis expected to reflect the inherent structure of the original image more\nfaithfully. Further, we use LRSTV to replace conventional SSTV and embed it in\nthe HSI processing model to improve its performance. Experimental results on\nmultiple public data-sets with heavy mixed noise show that the proposed model\ncan get 1.5dB improvement of PSNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Haijin Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoguang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongyong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_H/0/1/0/all/0/1\">Hiep Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philips_W/0/1/0/all/0/1\">Wilfried Philips</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gleo-Det: Deep Convolution Feature-Guided Detector with Local Entropy Optimization for Salient Points. (arXiv:2204.12884v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12884","description":"<p>Feature detection is an important procedure for image matching, where\nunsupervised feature detection methods are the detection approaches that have\nbeen mostly studied recently, including the ones that are based on\nrepeatability requirement to define loss functions, and the ones that attempt\nto use descriptor matching to drive the optimization of the pipelines. For the\nformer type, mean square error (MSE) is usually used which cannot provide\nstrong constraint for training and can make the model easy to be stuck into the\ncollapsed solution. For the later one, due to the down sampling operation and\nthe expansion of receptive fields, the details can be lost for local\ndescriptors can be lost, making the constraint not fine enough. Considering the\nissues above, we propose to combine both ideas, which including three aspects.\n1) We propose to achieve fine constraint based on the requirement of\nrepeatability while coarse constraint with guidance of deep convolution\nfeatures. 2) To address the issue that optimization with MSE is limited,\nentropy-based cost function is utilized, both soft cross-entropy and\nself-information. 3) With the guidance of convolution features, we define the\ncost function from both positive and negative sides. Finally, we study the\neffect of each modification proposed and experiments demonstrate that our\nmethod achieves competitive results over the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yanan You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenli Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global Trajectory Helps Person Retrieval in a Camera Network. (arXiv:2204.12900v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12900","description":"<p>We are concerned about retrieving a query person from the videos taken by a\nnon-overlapping camera network. Existing methods often rely on pure visual\nmatching or consider temporal constraint, but ignore the spatial information of\nthe camera network. To address this problem, we propose a framework of person\nretrieval based on cross-camera trajectory generation which integrates both\ntemporal and spatial information. To obtain the pedestrian trajectories, we\npropose a new cross-camera spatio-temporal model that integrates the walking\nhabits of pedestrians and the path layout between cameras, forming a joint\nprobability distribution. Such a spatio-temporal model among a camera network\ncan be specified using sparsely sampled pedestrian data. Based on the\nspatio-temporal model, the cross-camera trajectories of a specific pedestrian\ncan be extracted by the conditional random field model, and further optimized\nby the restricted nonnegative matrix factorization. Finally, a trajectory\nre-ranking technology is proposed to improve the person retrieval results. To\nverify the effectiveness of our approach, we build the first dataset of\ncross-camera pedestrian trajectories over an actual monitoring scenario, namely\nthe Person Trajectory Dataset. Extensive experiments have verified the\neffectiveness and robustness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohua Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jianhuang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Epicardial Adipose Tissue Segmentation from CT Images with A Semi-3D Neural Network. (arXiv:2204.12904v1 [eess.IV])","link":"http://arxiv.org/abs/2204.12904","description":"<p>Epicardial adipose tissue is a type of adipose tissue located between the\nheart wall and a protective layer around the heart called the pericardium. The\nvolume and thickness of epicardial adipose tissue are linked to various\ncardiovascular diseases. It is shown to be an independent cardiovascular\ndisease risk factor. Fully automatic and reliable measurements of epicardial\nadipose tissue from CT scans could provide better disease risk assessment and\nenable the processing of large CT image data sets for a systemic epicardial\nadipose tissue study. This paper proposes a method for fully automatic semantic\nsegmentation of epicardial adipose tissue from CT images using a deep neural\nnetwork. The proposed network uses a U-Net-based architecture with slice depth\ninformation embedded in the input image to segment a pericardium region of\ninterest, which is used to obtain an epicardial adipose tissue segmentation.\nImage augmentation is used to increase model robustness. Cross-validation of\nthe proposed method yields a Dice score of 0.86 on the CT scans of 20 patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bencevic_M/0/1/0/all/0/1\">Marin Ben&#x10d;evi&#x107;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Habijan_M/0/1/0/all/0/1\">Marija Habijan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Galic_I/0/1/0/all/0/1\">Irena Gali&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Iterative Labeling Method for Annotating Fisheries Imagery. (arXiv:2204.12934v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12934","description":"<p>In this paper, we present a methodology for fisheries-related data that\nallows us to converge on a labeled image dataset by iterating over the dataset\nwith multiple training and production loops that can exploit crowdsourcing\ninterfaces. We present our algorithm and its results on two separate sets of\nimage data collected using the Seabed autonomous underwater vehicle. The first\ndataset comprises of 2,026 completely unlabeled images, while the second\nconsists of 21,968 images that were point annotated by experts. Our results\nindicate that training with a small subset and iterating on that to build a\nlarger set of labeled data allows us to converge to a fully annotated dataset\nwith a small number of iterations. Even in the case of a dataset labeled by\nexperts, a single iteration of the methodology improves the labels by\ndiscovering additional complicated examples of labels associated with fish that\noverlap, are very small, or obscured by the contrast limitations associated\nwith underwater imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaveti_P/0/1/0/all/0/1\">Pushyami Kaveti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Hanumant Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_A/0/1/0/all/0/1\">Abigail Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fruh_E/0/1/0/all/0/1\">Erica Fruh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clarke_M/0/1/0/all/0/1\">M. Elizabeth Clarke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Unbiased Visual Representations. (arXiv:2204.12941v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12941","description":"<p>Deep neural networks are known for their inability to learn robust\nrepresentations when biases exist in the dataset. This results in a poor\ngeneralization to unbiased datasets, as the predictions strongly rely on\nperipheral and confounding factors, which are erroneously learned by the\nnetwork. Many existing works deal with this issue by either employing an\nexplicit supervision on the bias attributes, or assuming prior knowledge about\nthe bias. In this work we study this problem in a more difficult scenario, in\nwhich no explicit annotation about the bias is available, and without any prior\nknowledge about its nature. We propose a fully unsupervised debiasing\nframework, consisting of three steps: first, we exploit the natural preference\nfor learning malignant biases, obtaining a bias-capturing model; then, we\nperform a pseudo-labelling step to obtain bias labels; finally we employ\nstate-of-the-art supervised debiasing techniques to obtain an unbiased model.\nWe also propose a theoretical framework to assess the biasness of a model, and\nprovide a detailed analysis on how biases affect the training of neural\nnetworks. We perform experiments on synthetic and real-world datasets, showing\nthat our method achieves state-of-the-art performance in a variety of settings,\nsometimes even higher than fully supervised debiasing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbano_C/0/1/0/all/0/1\">Carlo Alberto Barbano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1\">Enzo Tartaglione</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangetto_M/0/1/0/all/0/1\">Marco Grangetto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAPLE-Edge: A Runtime Latency Predictor for Edge Devices. (arXiv:2204.12950v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12950","description":"<p>Neural Architecture Search (NAS) has enabled automatic discovery of more\nefficient neural network architectures, especially for mobile and embedded\nvision applications. Although recent research has proposed ways of quickly\nestimating latency on unseen hardware devices with just a few samples, little\nfocus has been given to the challenges of estimating latency on runtimes using\noptimized graphs, such as TensorRT and specifically for edge devices. In this\nwork, we propose MAPLE-Edge, an edge device-oriented extension of MAPLE, the\nstate-of-the-art latency predictor for general purpose hardware, where we train\na regression network on architecture-latency pairs in conjunction with a\nhardware-runtime descriptor to effectively estimate latency on a diverse pool\nof edge devices. Compared to MAPLE, MAPLE-Edge can describe the runtime and\ntarget device platform using a much smaller set of CPU performance counters\nthat are widely available on all Linux kernels, while still achieving up to\n+49.6% accuracy gains against previous state-of-the-art baseline methods on\noptimized edge device runtimes, using just 10 measurements from an unseen\ntarget device. We also demonstrate that unlike MAPLE which performs best when\ntrained on a pool of devices sharing a common runtime, MAPLE-Edge can\neffectively generalize across runtimes by applying a trick of normalizing\nperformance counters by the operator latency, in the measured hardware-runtime\ndescriptor. Lastly, we show that for runtimes exhibiting lower than desired\naccuracy, performance can be boosted by collecting additional samples from the\ntarget device, with an extra 90 samples translating to gains of nearly +40%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Saeejith Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasi_S/0/1/0/all/0/1\">Saad Abbasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Alexander Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1\">Mohammad Javad Shafiee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards assessing agricultural land suitability with causal machine learning. (arXiv:2204.12956v1 [cs.LG])","link":"http://arxiv.org/abs/2204.12956","description":"<p>Understanding the suitability of agricultural land for applying specific\nmanagement practices is of great importance for sustainable and resilient\nagriculture against climate change. Recent developments in the field of causal\nmachine learning enable the estimation of intervention impacts on an outcome of\ninterest, for samples described by a set of observed characteristics. We\nintroduce an extensible data-driven framework that leverages earth observations\nand frames agricultural land suitability as a geospatial impact assessment\nproblem, where the estimated effects of agricultural practices on\nagroecosystems serve as a land suitability score and guide decision making. We\nformulate this as a causal machine learning task and discuss how this approach\ncan be used for agricultural planning in a changing climate. Specifically, we\nextract the agricultural management practices of \"crop rotation\" and \"landscape\ncrop diversity\" from crop type maps, account for climate and land use data, and\nuse double machine learning to estimate their heterogeneous effect on Net\nPrimary Productivity (NPP), within the Flanders region of Belgium from 2010 to\n2020. We find that the effect of crop rotation was insignificant, while\nlandscape crop diversity had a small negative effect on NPP. Finally, we\nobserve considerable effect heterogeneity in space for both practices and\nanalyze it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giannarakis_G/0/1/0/all/0/1\">Georgios Giannarakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitokonstantinou_V/0/1/0/all/0/1\">Vasileios Sitokonstantinou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lorilla_R/0/1/0/all/0/1\">Roxanne Suzette Lorilla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontoes_C/0/1/0/all/0/1\">Charalampos Kontoes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CapOnImage: Context-driven Dense-Captioning on Image. (arXiv:2204.12974v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12974","description":"<p>Existing image captioning systems are dedicated to generating narrative\ncaptions for images, which are spatially detached from the image in\npresentation. However, texts can also be used as decorations on the image to\nhighlight the key points and increase the attractiveness of images. In this\nwork, we introduce a new task called captioning on image (CapOnImage), which\naims to generate dense captions at different locations of the image based on\ncontextual information. To fully exploit the surrounding visual context to\ngenerate the most suitable caption for each location, we propose a multi-modal\npre-training model with multi-level pre-training tasks that progressively learn\nthe correspondence between texts and image locations from easy to difficult.\nSince the model may generate redundant captions for nearby locations, we\nfurther enhance the location embedding with neighbor locations as context. For\nthis new task, we also introduce a large-scale benchmark called CapOnImage2M,\nwhich contains 2.1 million product images, each with an average of 4.8\nspatially localized captions. Compared with other image captioning model\nvariants, our model achieves the best results in both captioning accuracy and\ndiversity aspects. We will make code and datasets public to facilitate future\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yiqi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xinglin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanmeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1\">Tiezheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DearKD: Data-Efficient Early Knowledge Distillation for Vision Transformers. (arXiv:2204.12997v1 [cs.CV])","link":"http://arxiv.org/abs/2204.12997","description":"<p>Transformers are successfully applied to computer vision due to their\npowerful modeling capacity with self-attention. However, the excellent\nperformance of transformers heavily depends on enormous training images. Thus,\na data-efficient transformer solution is urgently needed. In this work, we\npropose an early knowledge distillation framework, which is termed as DearKD,\nto improve the data efficiency required by transformers. Our DearKD is a\ntwo-stage framework that first distills the inductive biases from the early\nintermediate layers of a CNN and then gives the transformer full play by\ntraining without distillation. Further, our DearKD can be readily applied to\nthe extreme data-free case where no real images are available. In this case, we\npropose a boundary-preserving intra-divergence loss based on DeepInversion to\nfurther close the performance gap against the full-data counterpart. Extensive\nexperiments on ImageNet, partial ImageNet, data-free setting and other\ndownstream tasks prove the superiority of DearKD over its baselines and\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xianing_C/0/1/0/all/0/1\">Chen Xianing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiong_C/0/1/0/all/0/1\">Cao Qiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yujie_Z/0/1/0/all/0/1\">Zhong Yujie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhang Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenghua_G/0/1/0/all/0/1\">Gao Shenghua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dacheng_T/0/1/0/all/0/1\">Tao Dacheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relevance-based Margin for Contrastively-trained Video Retrieval Models. (arXiv:2204.13001v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13001","description":"<p>Video retrieval using natural language queries has attracted increasing\ninterest due to its relevance in real-world applications, from intelligent\naccess in private media galleries to web-scale video search. Learning the\ncross-similarity of video and text in a joint embedding space is the dominant\napproach. To do so, a contrastive loss is usually employed because it organizes\nthe embedding space by putting similar items close and dissimilar items far.\nThis framework leads to competitive recall rates, as they solely focus on the\nrank of the groundtruth items. Yet, assessing the quality of the ranking list\nis of utmost importance when considering intelligent retrieval systems, since\nmultiple items may share similar semantics, hence a high relevance. Moreover,\nthe aforementioned framework uses a fixed margin to separate similar and\ndissimilar items, treating all non-groundtruth items as equally irrelevant. In\nthis paper we propose to use a variable margin: we argue that varying the\nmargin used during training based on how much relevant an item is to a given\nquery, i.e. a relevance-based margin, easily improves the quality of the\nranking lists measured through nDCG and mAP. We demonstrate the advantages of\nour technique using different models on EPIC-Kitchens-100 and YouCook2. We show\nthat even if we carefully tuned the fixed margin, our technique (which does not\nhave the margin as a hyper-parameter) would still achieve better performance.\nFinally, extensive ablation studies and qualitative analysis support the\nrobustness of our approach. Code will be released at\n\\url{https://github.com/aranciokov/RelevanceMargin-ICMR22}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Falcon_A/0/1/0/all/0/1\">Alex Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1\">Swathikiran Sudhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_G/0/1/0/all/0/1\">Giuseppe Serra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanz_O/0/1/0/all/0/1\">Oswald Lanz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Defending Against Person Hiding Adversarial Patch Attack with a Universal White Frame. (arXiv:2204.13004v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13004","description":"<p>Object detection has attracted great attention in the computer vision area\nand has emerged as an indispensable component in many vision systems. In the\nera of deep learning, many high-performance object detection networks have been\nproposed. Although these detection networks show high performance, they are\nvulnerable to adversarial patch attacks. Changing the pixels in a restricted\nregion can easily fool the detection network in the physical world. In\nparticular, person-hiding attacks are emerging as a serious problem in many\nsafety-critical applications such as autonomous driving and surveillance\nsystems. Although it is necessary to defend against an adversarial patch\nattack, very few efforts have been dedicated to defending against person-hiding\nattacks. To tackle the problem, in this paper, we propose a novel defense\nstrategy that mitigates a person-hiding attack by optimizing defense patterns,\nwhile previous methods optimize the model. In the proposed method, a\nframe-shaped pattern called a 'universal white frame' (UWF) is optimized and\nplaced on the outside of the image. To defend against adversarial patch\nattacks, UWF should have three properties (i) suppressing the effect of the\nadversarial patch, (ii) maintaining its original prediction, and (iii)\napplicable regardless of images. To satisfy the aforementioned properties, we\npropose a novel pattern optimization algorithm that can defend against the\nadversarial patch. Through comprehensive experiments, we demonstrate that the\nproposed method effectively defends against the adversarial patch attack.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Youngjoon Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hong Joo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hakmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1\">Yong Man Ro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ollivier-Ricci Curvature For Head Pose Estimation From a Single Image. (arXiv:2204.13006v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13006","description":"<p>Head pose estimation is a crucial challenge for many real-world applications,\nsuch as attention and human behavior analysis. This paper aims to estimate head\npose from a single image by applying notions of network curvature. In the real\nworld, many complex networks have groups of nodes that are well connected to\neach other with significant functional roles. Similarly, the interactions of\nfacial landmarks can be represented as complex dynamic systems modeled by\nweighted graphs. The functionalities of such systems are therefore\nintrinsically linked to the topology and geometry of the underlying graph. In\nthis work, using the geometric notion of Ollivier-Ricci curvature (ORC) on\nweighted graphs as input to the XGBoost regression model, we show that the\nintrinsic geometric basis of ORC offers a natural approach to discovering\nunderlying common structure within a pool of poses. Experiments on the BIWI,\nAFLW2000 and Pointing'04 datasets show that the ORC_XGB method performs well\ncompared to state-of-the-art methods, both landmark-based and image-only.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cascone_L/0/1/0/all/0/1\">Lucia Cascone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Distasi_R/0/1/0/all/0/1\">Riccardo Distasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nappi_M/0/1/0/all/0/1\">Michele Nappi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dropout Inference with Non-Uniform Weight Scaling. (arXiv:2204.13047v1 [cs.LG])","link":"http://arxiv.org/abs/2204.13047","description":"<p>Dropout as regularization has been used extensively to prevent overfitting\nfor training neural networks. During training, units and their connections are\nrandomly dropped, which could be considered as sampling many different\nsubmodels from the original model. At test time, weight scaling and Monte Carlo\napproximation are two widely applied approaches to approximate the outputs.\nBoth approaches work well practically when all submodels are low-bias complex\nlearners. However, in this work, we demonstrate scenarios where some submodels\nbehave closer to high-bias models and a non-uniform weight scaling is a better\napproximation for inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhaoyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Arpit Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution. (arXiv:2204.13062v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13062","description":"<p>Estimating the pose and shape of hands and objects under interaction finds\nnumerous applications including augmented and virtual reality. Existing\napproaches for hand and object reconstruction require explicitly defined\nphysical constraints and known objects, which limits its application domains.\nOur algorithm is agnostic to object models, and it learns the physical rules\ngoverning hand-object interaction. This requires automatically inferring the\nshapes and physical interaction of hands and (potentially unknown) objects. We\nseek to approach this challenging problem by proposing a collaborative learning\nstrategy where two-branches of deep networks are learning from each other.\nSpecifically, we transfer hand mesh information to the object branch and vice\nversa for the hand branch. The resulting optimisation (training) problem can be\nunstable, and we address this via two strategies: (i) attention-guided graph\nconvolution which helps identify and focus on mutual occlusion and (ii)\nunsupervised associative loss which facilitates the transfer of information\nbetween the branches. Experiments using four widely-used benchmarks show that\nour framework achieves beyond state-of-the-art accuracy in 3D pose estimation,\nas well as recovers dense 3D hand and object shapes. Each technical component\nabove contributes meaningfully in the ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tse_T/0/1/0/all/0/1\">Tze Ho Elden Tse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwang In Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hyung Jin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Consistency on Visual Corruptions for Single-Source Domain Generalization. (arXiv:2204.13091v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13091","description":"<p>Generalizing visual recognition models trained on a single distribution to\nunseen input distributions (i.e. domains) requires making them robust to\nsuperfluous correlations in the training set. In this work, we achieve this\ngoal by altering the training images to simulate new domains and imposing\nconsistent visual attention across the different views of the same sample. We\ndiscover that the first objective can be simply and effectively met through\nvisual corruptions. Specifically, we alter the content of the training images\nusing the nineteen corruptions of the ImageNet-C benchmark and three additional\ntransformations based on Fourier transform. Since these corruptions preserve\nobject locations, we propose an attention consistency loss to ensure that class\nactivation maps across original and corrupted versions of the same training\nsample are aligned. We name our model Attention Consistency on Visual\nCorruptions (ACVC). We show that ACVC consistently achieves the state of the\nart on three single-source domain generalization benchmarks, PACS, COCO, and\nthe large-scale DomainNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cugu_I/0/1/0/all/0/1\">Ilke Cugu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1\">Massimiliano Mancini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanbei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Magic Mirror: Clothing Reconstruction from a Single Image via a Causal Perspective. (arXiv:2204.13096v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13096","description":"<p>This research aims to study a self-supervised 3D clothing reconstruction\nmethod, which recovers the geometry shape, and texture of human clothing from a\nsingle 2D image. Compared with existing methods, we observe that three primary\nchallenges remain: (1) the conventional template-based methods are limited to\nmodeling non-rigid clothing objects, e.g., handbags and dresses, which are\ncommon in fashion images; (2) 3D ground-truth meshes of clothing are usually\ninaccessible due to annotation difficulties and time costs. (3) It remains\nchallenging to simultaneously optimize four reconstruction factors, i.e.,\ncamera viewpoint, shape, texture, and illumination. The inherent ambiguity\ncompromises the model training, such as the dilemma between a large shape with\na remote camera or a small shape with a close camera.\n</p>\n<p>In an attempt to address the above limitations, we propose a causality-aware\nself-supervised learning method to adaptively reconstruct 3D non-rigid objects\nfrom 2D images without 3D annotations. In particular, to solve the inherent\nambiguity among four implicit variables, i.e., camera position, shape, texture,\nand illumination, we study existing works and introduce an explainable\nstructural causal map (SCM) to build our model. The proposed model structure\nfollows the spirit of the causal map, which explicitly considers the prior\ntemplate in the camera estimation and shape prediction. When optimization, the\ncausality intervention tool, i.e., two expectation-maximization loops, is\ndeeply embedded in our algorithm to (1) disentangle four encoders and (2) help\nthe prior template update. Extensive experiments on two 2D fashion benchmarks,\ne.g., ATR, and Market-HQ, show that the proposed method could yield\nhigh-fidelity 3D reconstruction. Furthermore, we also verify the scalability of\nthe proposed method on a fine-grained bird dataset, i.e., CUB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhedong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiayin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Head Swapping in the Wild. (arXiv:2204.13100v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13100","description":"<p>The head swapping task aims at flawlessly placing a source head onto a target\nbody, which is of great importance to various entertainment scenarios. While\nface swapping has drawn much attention, the task of head swapping has rarely\nbeen explored, particularly under the few-shot setting. It is inherently\nchallenging due to its unique needs in head modeling and background blending.\nIn this paper, we present the Head Swapper (HeSer), which achieves few-shot\nhead swapping in the wild through two delicately designed modules. Firstly, a\nHead2Head Aligner is devised to holistically migrate pose and expression\ninformation from the target to the source head by examining multi-scale\ninformation. Secondly, to tackle the challenges of skin color variations and\nhead-background mismatches in the swapping procedure, a Head2Scene Blender is\nintroduced to simultaneously modify facial skin color and fill mismatched gaps\nin the background around the head. Particularly, seamless blending is achieved\nwith the help of a Semantic-Guided Color Reference Creation procedure and a\nBlending UNet. Extensive experiments demonstrate that the proposed method\nproduces superior head swapping results in a variety of scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1\">Changyong Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hemao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhibin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingtuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning of Object Parts for Semantic Segmentation. (arXiv:2204.13101v1 [cs.CV])","link":"http://arxiv.org/abs/2204.13101","description":"<p>Progress in self-supervised learning has brought strong general image\nrepresentation learning methods. Yet so far, it has mostly focused on\nimage-level learning. In turn, tasks such as unsupervised image segmentation\nhave not benefited from this trend as they require spatially-diverse\nrepresentations. However, learning dense representations is challenging, as in\nthe unsupervised context it is not clear how to guide the model to learn\nrepresentations that correspond to various potential object categories. In this\npaper, we argue that self-supervised learning of object parts is a solution to\nthis issue. Object parts are generalizable: they are a priori independent of an\nobject definition, but can be grouped to form objects a posteriori. To this\nend, we leverage the recently proposed Vision Transformer's capability of\nattending to objects and combine it with a spatially dense clustering task for\nfine-tuning the spatial tokens. Our method surpasses the state-of-the-art on\nthree semantic segmentation benchmarks by 17%-3%, showing that our\nrepresentations are versatile under various object definitions. Finally, we\nextend this to fully unsupervised segmentation - which refrains completely from\nusing label information even at test-time - and demonstrate that a simple\nmethod for automatically merging discovered object parts based on community\ndetection yields substantial gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_A/0/1/0/all/0/1\">Adrian Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Chiral Domain of a Camera Arrangement. (arXiv:2003.09265v4 [math.AG] UPDATED)","link":"http://arxiv.org/abs/2003.09265","description":"<p>We introduce the chiral domain of an arrangement of cameras $\\mathcal{A} =\n\\{A_1,\\dots, A_m\\}$ which is the subset of $\\mathbb{P}^3$ visible in\n$\\mathcal{A}$. It generalizes the classical definition of chirality to include\nall of $\\mathbb{P}^3$ and offers a unifying framework for studying multiview\nchirality. We give an algebraic description of the chiral domain which allows\nus to define and describe a chiral version of Triggs' joint image. We then use\nthe chiral domain to re-derive and extend prior results on chirality due to\nHartley.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Agarwal_S/0/1/0/all/0/1\">Sameer Agarwal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pryhuber_A/0/1/0/all/0/1\">Andrew Pryhuber</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sinn_R/0/1/0/all/0/1\">Rainer Sinn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Thomas_R/0/1/0/all/0/1\">Rekha R. Thomas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A closed-form solution to estimate uncertainty in non-rigid structure from motion. (arXiv:2005.04810v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.04810","description":"<p>Semi-Definite Programming (SDP) with low-rank prior has been widely applied\nin Non-Rigid Structure from Motion (NRSfM). Based on a low-rank constraint, it\navoids the inherent ambiguity of basis number selection in conventional\nbase-shape or base-trajectory methods. Despite the efficiency in deformable\nshape reconstruction, it remains unclear how to assess the uncertainty of the\nrecovered shape from the SDP process. In this paper, we present a statistical\ninference on the element-wise uncertainty quantification of the estimated\ndeforming 3D shape points in the case of the exact low-rank SDP problem. A\nclosed-form uncertainty quantification method is proposed and tested. Moreover,\nwe extend the exact low-rank uncertainty quantification to the approximate\nlow-rank scenario with a numerical optimal rank selection method, which enables\nsolving practical application in SDP based NRSfM scenario. The proposed method\nprovides an independent module to the SDP method and only requires the\nstatistic information of the input 2D tracked points. Extensive experiments\nprove that the output 3D points have identical normal distribution to the 2D\ntrackings, the proposed method and quantify the uncertainty accurately, and\nsupports that it has desirable effects on routinely SDP low-rank based NRSfM\nsolver.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Mitesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jasour_A/0/1/0/all/0/1\">Ashkan Jasour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqDialN: Sequential Visual Dialog Networks in Joint Visual-Linguistic Representation Space. (arXiv:2008.00397v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.00397","description":"<p>In this work, we formulate a visual dialog as an information flow in which\neach piece of information is encoded with the joint visual-linguistic\nrepresentation of a single dialog round. Based on this formulation, we consider\nthe visual dialog task as a sequence problem consisting of ordered\nvisual-linguistic vectors. For featurization, we use a Dense Symmetric\nCo-Attention network as a lightweight vison-language joint representation\ngenerator to fuse multimodal features (i.e., image and text), yielding better\ncomputation and data efficiencies. For inference, we propose two Sequential\nDialog Networks (SeqDialN): the first uses LSTM for information propagation\n(IP) and the second uses a modified Transformer for multi-step reasoning (MR).\nOur architecture separates the complexity of multimodal feature fusion from\nthat of inference, which allows simpler design of the inference engine. IP\nbased SeqDialN is our baseline with a simple 2-layer LSTM design that achieves\ndecent performance. MR based SeqDialN, on the other hand, recurrently refines\nthe semantic question/history representations through the self-attention stack\nof Transformer and produces promising results on the visual dialog task. On\nVisDial v1.0 test-std dataset, our best single generative SeqDialN achieves\n62.54% NDCG and 48.63% MRR; our ensemble generative SeqDialN achieves 63.78%\nNDCG and 49.98% MRR, which set a new state-of-the-art generative visual dialog\nmodel. We fine-tune discriminative SeqDialN with dense annotations and boost\nthe performance up to 72.41% NDCG and 55.11% MRR. In this work, we discuss the\nextensive experiments we have conducted to demonstrate the effectiveness of our\nmodel components. We also provide visualization for the reasoning process from\nthe relevant conversation rounds and discuss our fine-tuning methods. Our code\nis available at https://github.com/xiaoxiaoheimei/SeqDialN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation. (arXiv:2011.14672v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14672","description":"<p>Model-based 3D pose and shape estimation methods reconstruct a full 3D mesh\nfor the human body by estimating several parameters. However, learning the\nabstract parameters is a highly non-linear process and suffers from image-model\nmisalignment, leading to mediocre model performance. In contrast, 3D keypoint\nestimation methods combine deep CNN network with the volumetric representation\nto achieve pixel-level localization accuracy but may predict unrealistic body\nstructure. In this paper, we address the above issues by bridging the gap\nbetween body mesh estimation and 3D keypoint estimation. We propose a novel\nhybrid inverse kinematics solution (HybrIK). HybrIK directly transforms\naccurate 3D joints to relative body-part rotations for 3D body mesh\nreconstruction, via the twist-and-swing decomposition. The swing rotation is\nanalytically solved with 3D joints, and the twist rotation is derived from the\nvisual cues through the neural network. We show that HybrIK preserves both the\naccuracy of 3D pose and the realistic body structure of the parametric human\nmodel, leading to a pixel-aligned 3D body mesh and a more accurate 3D pose than\nthe pure 3D keypoint estimation methods. Without bells and whistles, the\nproposed method surpasses the state-of-the-art methods by a large margin on\nvarious 3D human pose and shape benchmarks. As an illustrative example, HybrIK\noutperforms all the previous methods by 13.2 mm MPJPE and 21.9 mm PVE on 3DPW\ndataset. Our code is available at https://github.com/Jeff-sjtu/HybrIK.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiefeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhicun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lixin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperspectral Image Classification-Traditional to Deep Models: A Survey for Future Prospects. (arXiv:2101.06116v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.06116","description":"<p>Hyperspectral Imaging (HSI) has been extensively utilized in many real-life\napplications because it benefits from the detailed spectral information\ncontained in each pixel. Notably, the complex characteristics i.e., the\nnonlinear relation among the captured spectral information and the\ncorresponding object of HSI data make accurate classification challenging for\ntraditional methods. In the last few years, Deep Learning (DL) has been\nsubstantiated as a powerful feature extractor that effectively addresses the\nnonlinear problems that appeared in a number of computer vision tasks. This\nprompts the deployment of DL for HSI classification (HSIC) which revealed good\nperformance. This survey enlists a systematic overview of DL for HSIC and\ncompared state-of-the-art strategies on the said topic. Primarily, we will\nencapsulate the main challenges of traditional machine learning for HSIC and\nthen we will acquaint the superiority of DL to address these problems. This\nsurvey breakdown the state-of-the-art DL frameworks into spectral features,\nspatial features, and together spatial-spectral features to systematically\nanalyze the achievements (future research directions as well) of these\nframeworks for HSIC. Moreover, we will consider the fact that DL requires a\nlarge number of labeled training examples whereas acquiring such a number for\nHSIC is challenging in terms of time and cost. Therefore, this survey discusses\nsome strategies to improve the generalization performance of DL strategies\nwhich can provide some future guidelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_M/0/1/0/all/0/1\">Muhammad Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shabbir_S/0/1/0/all/0/1\">Sidrah Shabbir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1\">Swalpa Kumar Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_D/0/1/0/all/0/1\">Danfeng Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xin Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1\">Jing Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1\">Adil Mehmood Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazzara_M/0/1/0/all/0/1\">Manuel Mazzara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Distefano_S/0/1/0/all/0/1\">Salvatore Distefano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chanussot_J/0/1/0/all/0/1\">Jocelyn Chanussot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAI Estimation of Cucumber Crop Based on Improved Fully Convolutional Network. (arXiv:2104.07955v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.07955","description":"<p>LAI (Leaf Area Index) is of great importance for crop yield estimation in\nagronomy. It is directly related to plant growth status, net assimilation rate,\nplant photosynthesis, and carbon dioxide in the environment. How to measure LAI\naccurately and efficiently is the key to the crop yield estimation problem.\nManual measurement consumes a lot of human resources and material resources.\nRemote sensing technology is not suitable for near-Earth LAI measurement.\nBesides, methods based on traditional digital image processing are greatly\naffected by environmental noise and image exposure. Nowadays, deep learning is\nwidely used in many fields. The improved FCN (Fully Convolutional Network) is\nproposed in our study for LAI measure task. Eighty-two cucumber images\ncollected from our greenhouse are labeled to fine-tuning the pre-trained model.\nThe result shows that the improved FCN model performs well on our dataset. Our\nmethod's mean IoU can reach 0.908, which is 11% better than conventional\nmethods and 4.7% better than the basic FCN model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_W/0/1/0/all/0/1\">Weiqi Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Ling Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bolong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting Anchors by the Detector Itself. (arXiv:2105.14086v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14086","description":"<p>Usually, it is difficult to determine the scale and aspect ratio of anchors\nfor anchor-based object detection methods. Current state-of-the-art object\ndetectors either determine anchor parameters according to objects' shape and\nscale in a dataset, or avoid this problem by utilizing anchor-free methods,\nhowever, the former scheme is dataset-specific and the latter methods could not\nget better performance than the former ones. In this paper, we propose a novel\nanchor augmentation method named AADI, which means Augmenting Anchors by the\nDetector Itself. AADI is not an anchor-free method, instead, it can convert the\nscale and aspect ratio of anchors from a continuous space to a discrete space,\nwhich greatly alleviates the problem of anchors' designation. Furthermore, AADI\nis a learning-based anchor augmentation method, but it does not add any\nparameters or hyper-parameters, which is beneficial for research and downstream\ntasks. Extensive experiments on COCO dataset demonstrate the effectiveness of\nAADI, specifically, AADI achieves significant performance boosts on many\nstate-of-the-art object detectors (eg. at least +2.4 box AP on Faster R-CNN,\n+2.2 box AP on Mask R-CNN, and +0.9 box AP on Cascade Mask R-CNN). We hope that\nthis simple and cost-efficient method can be widely used in object detection.\nCode and models are available at https://github.com/WanXiaopei/aadi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaopei Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Contrastive Learning for Image Reconstruction: Learning Transferable Representations from Noisy Images. (arXiv:2106.10070v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10070","description":"<p>This paper is concerned with contrastive learning (CL) for low-level image\nrestoration and enhancement tasks. We propose a new label-efficient learning\nparadigm based on residuals, residual contrastive learning (RCL), and derive an\nunsupervised visual representation learning framework, suitable for low-level\nvision tasks with noisy inputs. While supervised image reconstruction aims to\nminimize residual terms directly, RCL alternatively builds a connection between\nresiduals and CL by defining a novel instance discrimination pretext task,\nusing residuals as the discriminative feature. Our formulation mitigates the\nsevere task misalignment between instance discrimination pretext tasks and\ndownstream image reconstruction tasks, present in existing CL frameworks.\nExperimentally, we find that RCL can learn robust and transferable\nrepresentations that improve the performance of various downstream tasks, such\nas denoising and super resolution, in comparison with recent self-supervised\nmethods designed specifically for noisy inputs. Additionally, our unsupervised\npre-training can significantly reduce annotation costs whilst maintaining\nperformance competitive with fully-supervised image reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maggioni_M/0/1/0/all/0/1\">Matteo Maggioni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1\">Eduardo P&#xe9;rez-Pellitero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Aware Sampling Layer for Point-Wise Analysis. (arXiv:2107.04291v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04291","description":"<p>Sampling, grouping, and aggregation are three important components in the\nmulti-scale analysis of point clouds. In this paper, we present a novel\ndata-driven sampler learning strategy for point-wise analysis tasks. Unlike the\nwidely used sampling technique, Farthest Point Sampling (FPS), we propose to\nlearn sampling and downstream applications jointly. Our key insight is that\nuniform sampling methods like FPS are not always optimal for different tasks:\nsampling more points around boundary areas can make the point-wise\nclassification easier for segmentation. Towards this end, we propose a novel\nsampler learning strategy that learns sampling point displacement supervised by\ntask-related ground truth information and can be trained jointly with the\nunderlying tasks. We further demonstrate our methods in various point-wise\nanalysis tasks, including semantic part segmentation, point cloud completion,\nand keypoint detection. Our experiments show that jointly learning of the\nsampler and task brings better performance than using FPS in various\npoint-based networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yiqun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lichang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Learning for Image-Based Camera Localization. (arXiv:2108.09112v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09112","description":"<p>For several emerging technologies such as augmented reality, autonomous\ndriving and robotics, visual localization is a critical component. Directly\nregressing camera pose/3D scene coordinates from the input image using deep\nneural networks has shown great potential. However, such methods assume a\nstationary data distribution with all scenes simultaneously available during\ntraining. In this paper, we approach the problem of visual localization in a\ncontinual learning setup -- whereby the model is trained on scenes in an\nincremental manner. Our results show that similar to the classification domain,\nnon-stationary data induces catastrophic forgetting in deep networks for visual\nlocalization. To address this issue, a strong baseline based on storing and\nreplaying images from a fixed buffer is proposed. Furthermore, we propose a new\nsampling method based on coverage score (Buff-CS) that adapts the existing\nsampling strategies in the buffering process to the problem of visual\nlocalization. Results demonstrate consistent improvements over standard\nbuffering methods on two challenging datasets -- 7Scenes, 12Scenes, and also\n19Scenes by combining the former scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuzhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskar_Z/0/1/0/all/0/1\">Zakaria Laskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melekhov_I/0/1/0/all/0/1\">Iaroslav Melekhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannala_J/0/1/0/all/0/1\">Juho Kannala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bio-Inspired Audio-Visual Cues Integration for Visual Attention Prediction. (arXiv:2109.08371v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08371","description":"<p>Visual Attention Prediction (VAP) methods simulates the human selective\nattention mechanism to perceive the scene, which is significant and imperative\nin many vision tasks. Most existing methods only consider visual cues, while\nneglect the accompanied audio information, which can provide complementary\ninformation for the scene understanding. In fact, there exists a strong\nrelation between auditory and visual cues, and humans generally perceive the\nsurrounding scene by simultaneously sensing these cues. Motivated by this, a\nbio-inspired audio-visual cues integration method is proposed for the VAP task,\nwhich explores the audio modality to better predict the visual attention map by\nassisting vision modality. The proposed method consists of three parts: 1)\naudio-visual encoding, 2) audio-visual location, and 3) multi-cues aggregation\nparts. Firstly, a refined SoundNet architecture is adopted to encode audio\nmodality for obtaining corresponding features, and a modified 3D ResNet-50\narchitecture is employed to learn visual features, containing both spatial\nlocation and temporal motion information. Secondly, an audio-visual location\npart is devised to locate the sound source in the visual scene by learning the\ncorrespondence between audio-visual information. Thirdly, a multi-cues\naggregation part is devised to adaptively aggregate audio-visual information\nand center-bias prior to generate the final visual attention map. Extensive\nexperiments are conducted on six challenging audiovisual eye-tracking datasets,\nincluding DIEM, AVAD, Coutrot1, Coutrot2, SumMe, and ETMD, which shows\nsignificant superiority over state-of-the-art visual attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_H/0/1/0/all/0/1\">Hailong Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bin Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BINAS: Bilinear Interpretable Neural Architecture Search. (arXiv:2110.12399v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.12399","description":"<p>Practical use of neural networks often involves requirements on latency,\nenergy and memory among others. A popular approach to find networks under such\nrequirements is through constrained Neural Architecture Search (NAS). However,\nprevious methods use complicated predictors for the accuracy of the network.\nThose predictors are hard to interpret and sensitive to many hyperparameters to\nbe tuned, hence, the resulting accuracy of the generated models is often\nharmed. In this work we resolve this by introducing Bilinear Interpretable\nNeural Architecture Search (BINAS), that is based on an accurate and simple\nbilinear formulation of both an accuracy estimator and the expected resource\nrequirement, together with a scalable search method with theoretical\nguarantees. The simplicity of our proposed estimator together with the\nintuitive way it is constructed bring interpretability through many insights\nabout the contribution of different design choices. For example, we find that\nin the examined search space, adding depth and width is more effective at\ndeeper stages of the network and at the beginning of each resolution stage. Our\nexperiments show that BINAS generates comparable to or better architectures\nthan other state-of-the-art NAS methods within a reduced marginal search cost,\nwhile strictly satisfying the resource constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayman_N/0/1/0/all/0/1\">Niv Nayman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aflalo_Y/0/1/0/all/0/1\">Yonathan Aflalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Detect Open Carry and Concealed Object with 77GHz Radar. (arXiv:2111.00551v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2111.00551","description":"<p>Detecting harmful carried objects plays a key role in intelligent\nsurveillance systems and has widespread applications, for example, in airport\nsecurity. In this paper, we focus on the relatively unexplored area of using\nlow-cost 77GHz mmWave radar for the carried objects detection problem. The\nproposed system is capable of real-time detecting three classes of objects -\nlaptop, phone, and knife - under open carry and concealed cases where objects\nare hidden with clothes or bags. This capability is achieved by the initial\nsignal processing for localization and generating range-azimuth-elevation image\ncubes, followed by a deep learning-based prediction network and a multi-shot\npost-processing module for detecting objects. Extensive experiments for\nvalidating the system performance on detecting open carry and concealed objects\nhave been presented with a self-built radar-camera testbed and collected\ndataset. Additionally, the influence of different input formats, factors, and\nparameters on system performance is analyzed, providing an intuitive\nunderstanding of the system. This system would be the very first baseline for\nother future works aiming to detect carried objects using 77GHz radar.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xiangyu Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_S/0/1/0/all/0/1\">Sumit Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xing_G/0/1/0/all/0/1\">Guanbin Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alansari_A/0/1/0/all/0/1\">Ali Alansari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1\">Youchen Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-pass Object-adaptive Data Undersampling and Reconstruction for MRI. (arXiv:2111.09212v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.09212","description":"<p>There is much recent interest in techniques to accelerate the data\nacquisition process in MRI by acquiring limited measurements. Often\nsophisticated reconstruction algorithms are deployed to maintain high image\nquality in such settings. In this work, we propose a data-driven sampler using\na convolutional neural network, MNet, to provide object-specific sampling\npatterns adaptive to each scanned object. The network observes very limited\nlow-frequency k-space data for each object and rapidly predicts the desired\nundersampling pattern in one go that achieves high image reconstruction\nquality.\n</p>\n<p>We propose an accompanying alternating-type training framework with a\nmask-backward procedure that efficiently generates training labels for the\nsampler network and jointly trains an image reconstruction network.\nExperimental results on the fastMRI knee dataset demonstrate the ability of the\nproposed learned undersampling network to generate object-specific masks at\nfourfold and eightfold acceleration that achieve superior image reconstruction\nperformance than several existing schemes. The source code for the proposed\njoint sampling and reconstruction learning framework is available at\nhttps://github.com/zhishenhuang/mri.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Zhishen Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ravishankar_S/0/1/0/all/0/1\">Saiprasad Ravishankar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis. (arXiv:2111.15186v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.15186","description":"<p>Obtaining annotations for large training sets is expensive, especially in\nsettings where domain knowledge is required, such as behavior analysis. Weak\nsupervision has been studied to reduce annotation costs by using weak labels\nfrom task-specific labeling functions (LFs) to augment ground truth labels.\nHowever, domain experts still need to hand-craft different LFs for different\ntasks, limiting scalability. To reduce expert effort, we present AutoSWAP: a\nframework for automatically synthesizing data-efficient task-level LFs. The key\nto our approach is to efficiently represent expert knowledge in a reusable\ndomain-specific language and more general domain-level LFs, with which we use\nstate-of-the-art program synthesis techniques and a small labeled dataset to\ngenerate task-level LFs. Additionally, we propose a novel structural diversity\ncost that allows for efficient synthesis of diverse sets of LFs, further\nimproving AutoSWAP's performance. We evaluate AutoSWAP in three behavior\nanalysis domains and demonstrate that AutoSWAP outperforms existing approaches\nusing only a fraction of the data. Our results suggest that AutoSWAP is an\neffective way to automatically generate LFs that can significantly reduce\nexpert effort for behavior analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_A/0/1/0/all/0/1\">Albert Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Keypoint Discovery in Behavioral Videos. (arXiv:2112.05121v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05121","description":"<p>We propose a method for learning the posture and structure of agents from\nunlabelled behavioral videos. Starting from the observation that behaving\nagents are generally the main sources of movement in behavioral videos, our\nmethod, Behavioral Keypoint Discovery (B-KinD), uses an encoder-decoder\narchitecture with a geometric bottleneck to reconstruct the spatiotemporal\ndifference between video frames. By focusing only on regions of movement, our\napproach works directly on input videos without requiring manual annotations.\nExperiments on a variety of agent types (mouse, fly, human, jellyfish, and\ntrees) demonstrate the generality of our approach and reveal that our\ndiscovered keypoints represent semantically meaningful body parts, which\nachieve state-of-the-art performance on keypoint regression among\nself-supervised methods. Additionally, B-KinD achieve comparable performance to\nsupervised keypoints on downstream tasks, such as behavior classification,\nsuggesting that our method can dramatically reduce model training costs\nvis-a-vis supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer J. Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryou_S/0/1/0/all/0/1\">Serim Ryou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldshmid_R/0/1/0/all/0/1\">Roni Goldshmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weissbourd_B/0/1/0/all/0/1\">Brandon Weissbourd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabiri_J/0/1/0/all/0/1\">John Dabiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_D/0/1/0/all/0/1\">David J. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1\">Ann Kennedy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1\">Pietro Perona</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSSID: Online Self-Supervised Instance Detection by (and for) Pose Estimation. (arXiv:2201.07309v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07309","description":"<p>Real-time object pose estimation is necessary for many robot manipulation\nalgorithms. However, state-of-the-art methods for object pose estimation are\ntrained for a specific set of objects; these methods thus need to be retrained\nto estimate the pose of each new object, often requiring tens of GPU-days of\ntraining for optimal performance. In this paper, we propose the OSSID\nframework, leveraging a slow zero-shot pose estimator to self-supervise the\ntraining of a fast detection algorithm. This fast detector can then be used to\nfilter the input to the pose estimator, drastically improving its inference\nspeed. We show that this self-supervised training exceeds the performance of\nexisting zero-shot detection methods on two widely used object pose estimation\nand detection datasets, without requiring any human annotations. Further, we\nshow that the resulting method for pose estimation has a significantly faster\ninference speed, due to the ability to filter out large parts of the image.\nThus, our method for self-supervised online learning of a detector (trained\nusing pseudo-labels from a slow pose estimator) leads to accurate pose\nestimation at real-time speeds, without requiring human annotations.\nSupplementary materials and code can be found at\nhttps://georgegu1997.github.io/OSSID/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1\">Qiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1\">Brian Okorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-by-Novel-View-Synthesis for Full-Face Appearance-Based 3D Gaze Estimation. (arXiv:2201.07927v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07927","description":"<p>Despite recent advances in appearance-based gaze estimation techniques, the\nneed for training data that covers the target head pose and gaze distribution\nremains a crucial challenge for practical deployment. This work examines a\nnovel approach for synthesizing gaze estimation training data based on\nmonocular 3D face reconstruction. Unlike prior works using multi-view\nreconstruction, photo-realistic CG models, or generative neural networks, our\napproach can manipulate and extend the head pose range of existing training\ndata without any additional requirements. We introduce a projective matching\nprocedure to align the reconstructed 3D facial mesh with the camera coordinate\nsystem and synthesize face images with accurate gaze labels. We also propose a\nmask-guided gaze estimation model and data augmentation strategies to further\nimprove the estimation accuracy by taking advantage of synthetic training data.\nExperiments using multiple public datasets show that our approach significantly\nimproves the estimation performance on challenging cross-dataset settings with\nnon-overlapping gaze distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiawei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimoyama_T/0/1/0/all/0/1\">Takuru Shimoyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Real-Time Rendering Method for Light Field Display. (arXiv:2201.08266v4 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2201.08266","description":"<p>A real-time elemental image array (EIA) generation method which does not\nsacrifice accuracy nor rely on high-performance hardware is developed, through\nraytracing and pre-stored voxel-pixel lookup table (LUT). Benefiting from both\noffline and online working flow, experiments will verified the effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Q/0/1/0/all/0/1\">Quanzhen Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On scale-invariant properties in natural images and their simulations. (arXiv:2201.13312v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13312","description":"<p>We study samples of natural images for which a set of statistical\ncharacteristics is computed and scale-invariant properties of samples are\ndemonstrated computationally. Computations of the power spectrum are carried\nout and a power-law decaying power spectrum is observed on samples taken from\nvan Hateren images of natural scenes. We propose a dynamic model to reproduce\nthe observed slope in the power spectrum qualitatively. For two types of\nsources for this model the behaviour of power spectrum is investigated and\nscale-invariance confirmed numerically. We then discuss potential applications\nof scale-invariant properties of natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_M/0/1/0/all/0/1\">Maxim Koroteev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aistov_K/0/1/0/all/0/1\">Kirill Aistov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISNet: Costless and Implicit Image Segmentation for Deep Classifiers, with Application in COVID-19 Detection. (arXiv:2202.00232v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.00232","description":"<p>This work proposes a novel deep neural network (DNN) architecture, Implicit\nSegmentation Neural Network (ISNet), to solve the task of image segmentation\nfollowed by classification. It substitutes the common pipeline of two DNNs with\na single model. We designed the ISNet for high flexibility and performance: it\nallows virtually any classification neural network architecture to analyze a\ncommon image as if it had been previously segmented. Furthermore, in relation\nto the unmodified classifier, the ISNet does not cause any increment in\ncomputational cost at run-time. We test the architecture with two applications:\nCOVID-19 detection in chest X-rays, and facial attribute estimation. We\nimplement an ISNet based on a DenseNet121 classifier, and compare the model to\na U-net (performing lung/face segmentation) followed by a DenseNet121, and to a\nstandalone DenseNet121. The new architecture matched the other DNNs in facial\nattribute estimation. Moreover, it strongly surpassed them in COVID-19\ndetection, according to an external test dataset. The ISNet precisely ignored\nthe image regions outside of the lungs or faces. Therefore, in COVID-19\ndetection it reduced the effects of background bias and shortcut learning, and\nit improved security in facial attribute estimation. ISNet presents an\naccurate, fast, and light methodology. The successful implicit segmentation,\nconsidering two largely diverse fields, highlights the architecture's general\napplicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1\">Pedro R.A.S. Bassi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cavalli_A/0/1/0/all/0/1\">Andrea Cavalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ROMNet: Renovate the Old Memories. (arXiv:2202.02606v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02606","description":"<p>Renovating the memories in old photos is an intriguing research topic in\ncomputer vision fields. These legacy images often suffer from severe and\ncommingled degradations such as cracks, noise, and color-fading, while lack of\nlarge-scale paired old photo datasets makes this restoration task very\nchallenging. In this work, we present a novel reference-based end-to-end\nlearning framework that can jointly repair and colorize the degraded legacy\npictures. Specifically, the proposed framework consists of three modules: a\nrestoration sub-network for degradation restoration, a similarity sub-network\nfor color histogram matching and transfer, and a colorization subnet that\nlearns to predict the chroma elements of the images conditioned on chromatic\nreference signals. The whole system takes advantage of the color histogram\npriors in a given reference image, which vastly reduces the dependency on\nlarge-scale training data. Apart from the proposed method, we also create, to\nour knowledge, the first public and real-world old photo dataset with paired\nground truth for evaluating old photo restoration models, wherein each old\nphoto is paired with a manually restored pristine image by PhotoShop experts.\nOur extensive experiments conducted on both synthetic and real-world datasets\ndemonstrate that our method significantly outperforms state-of-the-arts both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Runsheng Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyu Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinlong Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zibo Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+YU_H/0/1/0/all/0/1\">Hongkai YU</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Perspective Deformation in X-Ray Transmission Imaging. (arXiv:2202.06366v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.06366","description":"<p>In cone-beam X-ray transmission imaging, due to the divergence of X-rays,\nimaged structures with different depths have different magnification factors on\nan X-ray detector, which results in perspective deformation. Perspective\ndeformation causes difficulty in direct, accurate geometric assessments of\nanatomical structures. In this work, to reduce perspective deformation in X-ray\nimages acquired from regular cone-beam computed tomography (CBCT) systems, we\ninvestigate on learning perspective deformation, i.e., converting perspective\nprojections into orthogonal projections. Directly converting a single\nperspective projection image into an orthogonal projection image is extremely\nchallenging due to the lack of depth information. Therefore, we propose to\nutilize one additional perspective projection, a complementary (180-degree) or\northogonal (90-degree) view, to provide a certain degree of depth information.\nFurthermore, learning perspective deformation in different spatial domains is\ninvestigated. Our proposed method is evaluated on numerical spherical bead\nphantoms as well as patients' chest and head X-ray data. The experiments on\nnumerical bead phantom data demonstrate that learning perspective deformation\nin polar coordinates has significant advantages over learning in Cartesian\ncoordinates, as root-mean-square error (RMSE) decreases from 5.31 to 1.40,\nwhile learning in log-polar coordinates has no further considerable improvement\n(RMSE = 1.85). In addition, using a complementary view (RMSE = 1.40) is better\nthan an orthogonal view (RMSE = 3.87). The experiments on patients' chest and\nhead data demonstrate that learning perspective deformation using dual\ncomplementary views is also applicable in anatomical X-ray data, allowing\naccurate cardiothoracic ratio measurements in chest X-ray images and\ncephalometric analysis in synthetic cephalograms from cone-beam X-ray\nprojections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yixing Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fietkau_R/0/1/0/all/0/1\">Rainer Fietkau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bert_C/0/1/0/all/0/1\">Christoph Bert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putz_F/0/1/0/all/0/1\">Florian Putz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wavelet-based Dual-stream Network for Underwater Image Enhancement. (arXiv:2202.08758v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.08758","description":"<p>We present a wavelet-based dual-stream network that addresses color cast and\nblurry details in underwater images. We handle these artifacts separately by\ndecomposing an input image into multiple frequency bands using discrete wavelet\ntransform, which generates the downsampled structure image and detail images.\nThese sub-band images are used as input to our dual-stream network that\nincorporates two sub-networks: the multi-color space fusion network and the\ndetail enhancement network. The multi-color space fusion network takes the\ndecomposed structure image as input and estimates the color corrected output by\nemploying the feature representations from diverse color spaces of the input.\nThe detail enhancement network addresses the blurriness of the original\nunderwater image by improving the image details from high-frequency sub-bands.\nWe validate the proposed method on both real-world and synthetic underwater\ndatasets and show the effectiveness of our model in color correction and blur\nremoval with low computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changjae Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeRF-Supervision: Learning Dense Object Descriptors from Neural Radiance Fields. (arXiv:2203.01913v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.01913","description":"<p>Thin, reflective objects such as forks and whisks are common in our daily\nlives, but they are particularly challenging for robot perception because it is\nhard to reconstruct them using commodity RGB-D cameras or multi-view stereo\ntechniques. While traditional pipelines struggle with objects like these,\nNeural Radiance Fields (NeRFs) have recently been shown to be remarkably\neffective for performing view synthesis on objects with thin structures or\nreflective materials. In this paper we explore the use of NeRF as a new source\nof supervision for robust robot vision systems. In particular, we demonstrate\nthat a NeRF representation of a scene can be used to train dense object\ndescriptors. We use an optimized NeRF to extract dense correspondences between\nmultiple views of an object, and then use these correspondences as training\ndata for learning a view-invariant representation of the object. NeRF's usage\nof a density field allows us to reformulate the correspondence problem with a\nnovel distribution-of-depths formulation, as opposed to the conventional\napproach of using a depth map. Dense correspondence models supervised with our\nmethod significantly outperform off-the-shelf learned descriptors by 106%\n(PCK@3px metric, more than doubling performance) and outperform our baseline\nsupervised with multi-view stereo by 29%. Furthermore, we demonstrate the\nlearned dense descriptors enable robots to perform accurate 6-degree of freedom\n(6-DoF) pick and place of thin and reflective objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1\">Pete Florence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1\">Alberto Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Things not Written in Text: Exploring Spatial Commonsense from Visual Signals. (arXiv:2203.08075v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08075","description":"<p>Spatial commonsense, the knowledge about spatial position and relationship\nbetween objects (like the relative size of a lion and a girl, and the position\nof a boy relative to a bicycle when cycling), is an important part of\ncommonsense knowledge. Although pretrained language models (PLMs) succeed in\nmany NLP tasks, they are shown to be ineffective in spatial commonsense\nreasoning. Starting from the observation that images are more likely to exhibit\nspatial commonsense than texts, we explore whether models with visual signals\nlearn more spatial commonsense than text-based PLMs. We propose a spatial\ncommonsense benchmark that focuses on the relative scales of objects, and the\npositional relationship between people and objects under different actions. We\nprobe PLMs and models with visual signals, including vision-language pretrained\nmodels and image synthesis models, on this benchmark, and find that image\nsynthesis models are more capable of learning accurate and consistent spatial\nknowledge than other models. The spatial knowledge from image synthesis models\nalso helps in natural language understanding tasks that require spatial\ncommonsense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating natural images with direct Patch Distributions Matching. (arXiv:2203.11862v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11862","description":"<p>Many traditional computer vision algorithms generate realistic images by\nrequiring that each patch in the generated image be similar to a patch in a\ntraining image and vice versa. Recently, this classical approach has been\nreplaced by adversarial training with a patch discriminator. The adversarial\napproach avoids the computational burden of finding nearest neighbors of\npatches but often requires very long training times and may fail to match the\ndistribution of patches. In this paper we leverage the recently developed\nSliced Wasserstein Distance and develop an algorithm that explicitly and\nefficiently minimizes the distance between patch distributions in two images.\nOur method is conceptually simple, requires no training and can be implemented\nin a few lines of codes. On a number of image generation tasks we show that our\nresults are often superior to single-image-GANs, require no training, and can\ngenerate high quality images in a few seconds. Our implementation is available\nat https://github.com/ariel415el/GPDM\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elnekave_A/0/1/0/all/0/1\">Ariel Elnekave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_Y/0/1/0/all/0/1\">Yair Weiss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning. (arXiv:2203.14542v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14542","description":"<p>Supervised deep learning methods require a large repository of annotated\ndata; hence, label noise is inevitable. Training with such noisy data\nnegatively impacts the generalization performance of deep neural networks. To\ncombat label noise, recent state-of-the-art methods employ some sort of sample\nselection mechanism to select a possibly clean subset of data. Next, an\noff-the-shelf semi-supervised learning method is used for training where\nrejected samples are treated as unlabeled data. Our comprehensive analysis\nshows that current selection methods disproportionately select samples from\neasy (fast learnable) classes while rejecting those from relatively harder\nones. This creates class imbalance in the selected clean set and in turn,\ndeteriorates performance under high label noise. In this work, we propose\nUNICON, a simple yet effective sample selection method which is robust to high\nlabel noise. To address the disproportionate selection of easy and hard\nsamples, we introduce a Jensen-Shannon divergence based uniform selection\nmechanism which does not require any probabilistic modeling and hyperparameter\ntuning. We complement our selection method with contrastive learning to further\ncombat the memorization of noisy labels. Extensive experimentation on multiple\nbenchmark datasets demonstrates the effectiveness of UNICON; we obtain an 11.4%\nimprovement over the current state-of-the-art on CIFAR100 dataset with a 90%\nnoise rate. Our code is publicly available\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1\">Ajmal Mian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrative Few-Shot Learning for Classification and Segmentation. (arXiv:2203.15712v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15712","description":"<p>We introduce the integrative task of few-shot classification and segmentation\n(FS-CS) that aims to both classify and segment target objects in a query image\nwhen the target classes are given with a few examples. This task combines two\nconventional few-shot learning problems, few-shot classification and\nsegmentation. FS-CS generalizes them to more realistic episodes with arbitrary\nimage pairs, where each target class may or may not be present in the query. To\naddress the task, we propose the integrative few-shot learning (iFSL) framework\nfor FS-CS, which trains a learner to construct class-wise foreground maps for\nmulti-label classification and pixel-wise segmentation. We also develop an\neffective iFSL model, attentive squeeze network (ASNet), that leverages deep\nsemantic correlation and global self-attention to produce reliable foreground\nmaps. In experiments, the proposed method shows promising performance on the\nFS-CS task and also achieves the state of the art on standard few-shot\nsegmentation benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Dahyun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. The image convolution operation\nhelps CNNs to get good performance on image-related tasks. However, the image\nconvolution has high computation complexity and hard to be implemented. This\npaper proposes the CEMNet, which can be trained in the frequency domain. The\nmost important motivation of this research is that we can use the\nstraightforward element-wise multiplication operation to replace the image\nconvolution in the frequency domain based on the Cross-Correlation Theorem,\nwhich obviously reduces the computation complexity. We further introduce a\nWeight Fixation mechanism to alleviate the problem of over-fitting, and analyze\nthe working behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Pretrained Models: A Survey. (arXiv:2204.07356v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07356","description":"<p>Pretrained models have produced great success in both Computer Vision (CV)\nand Natural Language Processing (NLP). This progress leads to learning joint\nrepresentations of vision and language pretraining by feeding visual and\nlinguistic contents into a multi-layer transformer, Visual-Language Pretrained\nModels (VLPMs). In this paper, we present an overview of the major advances\nachieved in VLPMs for producing joint representations of vision and language.\nAs the preliminaries, we briefly describe the general task definition and\ngenetic architecture of VLPMs. We first discuss the language and vision data\nencoding methods and then present the mainstream VLPM structure as the core\ncontent. We further summarise several essential pretraining and fine-tuning\nstrategies. Finally, we highlight three future directions for both CV and NLP\nresearchers to provide insightful guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Siqu Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_F/0/1/0/all/0/1\">Feiqi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Soyeon Caren Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haiqin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEarth 2022 -- Multimodal Learning for Earth and Environment Workshop and Challenge. (arXiv:2204.07649v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.07649","description":"<p>The Multimodal Learning for Earth and Environment Challenge (MultiEarth 2022)\nwill be the first competition aimed at the monitoring and analysis of\ndeforestation in the Amazon rainforest at any time and in any weather\nconditions. The goal of the Challenge is to provide a common benchmark for\nmultimodal information processing and to bring together the earth and\nenvironmental science communities as well as multimodal representation learning\ncommunities to compare the relative merits of the various multimodal learning\nmethods to deforestation estimation under well-defined and strictly comparable\nconditions. MultiEarth 2022 will have three sub-challenges: 1) matrix\ncompletion, 2) deforestation estimation, and 3) image-to-image translation.\nThis paper presents the challenge guidelines, datasets, and evaluation metrics\nfor the three sub-challenges. Our challenge website is available at\nhttps://sites.google.com/view/rainforest-challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cha_M/0/1/0/all/0/1\">Miriam Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Morgan Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelides_G/0/1/0/all/0/1\">Gregory Angelides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamilton_M/0/1/0/all/0/1\">Mark Hamilton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_S/0/1/0/all/0/1\">Sam Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabrera_A/0/1/0/all/0/1\">Armando Cabrera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perron_T/0/1/0/all/0/1\">Taylor Perron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_B/0/1/0/all/0/1\">Bill Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swenson_B/0/1/0/all/0/1\">Brandon Swenson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piou_J/0/1/0/all/0/1\">Jean Piou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning 3D Semantics from Pose-Noisy 2D Images with Hierarchical Full Attention Network. (arXiv:2204.08084v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08084","description":"<p>We propose a novel framework to learn 3D point cloud semantics from 2D\nmulti-view image observations containing pose error. On the one hand, directly\nlearning from the massive, unstructured and unordered 3D point cloud is\ncomputationally and algorithmically more difficult than learning from\ncompactly-organized and context-rich 2D RGB images. On the other hand, both\nLiDAR point cloud and RGB images are captured in standard automated-driving\ndatasets. This motivates us to conduct a \"task transfer\" paradigm so that 3D\nsemantic segmentation benefits from aggregating 2D semantic cues, albeit pose\nnoises are contained in 2D image observations. Among all difficulties, pose\nnoise and erroneous prediction from 2D semantic segmentation approaches are the\nmain challenges for the task transfer. To alleviate the influence of those\nfactor, we perceive each 3D point using multi-view images and for each single\nimage a patch observation is associated. Moreover, the semantic labels of a\nblock of neighboring 3D points are predicted simultaneously, enabling us to\nexploit the point structure prior to further improve the performance. A\nhierarchical full attention network~(HiFANet) is designed to sequentially\naggregates patch, bag-of-frames and inter-point semantic cues, with\nhierarchical attention mechanism tailored for different level of semantic cues.\nAlso, each preceding attention block largely reduces the feature size before\nfeeding to the next attention block, making our framework slim. Experiment\nresults on Semantic-KITTI show that the proposed framework outperforms existing\n3D point cloud based methods significantly, it requires much less training data\nand exhibits tolerance to pose noise. The code is available at\nhttps://github.com/yuhanghe01/HiFANet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Junkun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPGNet: Cascade Point-Grid Fusion Network for Real-Time LiDAR Semantic Segmentation. (arXiv:2204.09914v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.09914","description":"<p>LiDAR semantic segmentation essential for advanced autonomous driving is\nrequired to be accurate, fast, and easy-deployed on mobile platforms. Previous\npoint-based or sparse voxel-based methods are far away from real-time\napplications since time-consuming neighbor searching or sparse 3D convolution\nare employed. Recent 2D projection-based methods, including range view and\nmulti-view fusion, can run in real time, but suffer from lower accuracy due to\ninformation loss during the 2D projection. Besides, to improve the performance,\nprevious methods usually adopt test time augmentation (TTA), which further\nslows down the inference process. To achieve a better speed-accuracy trade-off,\nwe propose Cascade Point-Grid Fusion Network (CPGNet), which ensures both\neffectiveness and efficiency mainly by the following two techniques: 1) the\nnovel Point-Grid (PG) fusion block extracts semantic features mainly on the 2D\nprojected grid for efficiency, while summarizes both 2D and 3D features on 3D\npoint for minimal information loss; 2) the proposed transformation consistency\nloss narrows the gap between the single-time model inference and TTA. The\nexperiments on the SemanticKITTI and nuScenes benchmarks demonstrate that the\nCPGNet without ensemble models or TTA is comparable with the state-of-the-art\nRPVNet, while it runs 4.7 times faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenhua Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Deepfakes to Close the Domain Gap between Real and Synthetic Images in Facial Capture Pipelines. (arXiv:2204.10746v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10746","description":"<p>We propose an end-to-end pipeline for both building and tracking 3D facial\nmodels from personalized in-the-wild (cellphone, webcam, youtube clips, etc.)\nvideo data. First, we present a method for automatic data curation and\nretrieval based on a hierarchical clustering framework typical of collision\ndetection algorithms in traditional computer graphics pipelines. Subsequently,\nwe utilize synthetic turntables and leverage deepfake technology in order to\nbuild a synthetic multi-view stereo pipeline for appearance capture that is\nrobust to imperfect synthetic geometry and image misalignment. The resulting\nmodel is fit with an animation rig, which is then used to track facial\nperformances. Notably, our novel use of deepfake technology enables us to\nperform robust tracking of in-the-wild data using differentiable renderers\ndespite a significant synthetic-to-real domain gap. Finally, we outline how we\ntrain a motion capture regressor, leveraging the aforementioned techniques to\navoid the need for real-world ground truth data and/or a high-end calibrated\ncamera capture setup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Winnie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yilin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Demi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedkiw_R/0/1/0/all/0/1\">Ron Fedkiw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identity Preserving Loss for Learned Image Compression. (arXiv:2204.10869v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.10869","description":"<p>Deep learning model inference on embedded devices is challenging due to the\nlimited availability of computation resources. A popular alternative is to\nperform model inference on the cloud, which requires transmitting images from\nthe embedded device to the cloud. Image compression techniques are commonly\nemployed in such cloud-based architectures to reduce transmission latency over\nlow bandwidth networks. This work proposes an end-to-end image compression\nframework that learns domain-specific features to achieve higher compression\nratios than standard HEVC/JPEG compression techniques while maintaining\naccuracy on downstream tasks (e.g., recognition). Our framework does not\nrequire fine-tuning of the downstream task, which allows us to drop-in any\noff-the-shelf downstream task model without retraining. We choose faces as an\napplication domain due to the ready availability of datasets and off-the-shelf\nrecognition models as representative downstream tasks. We present a novel\nIdentity Preserving Reconstruction (IPR) loss function which achieves\nBits-Per-Pixel (BPP) values that are ~38% and ~42% of CRF-23 HEVC compression\nfor LFW (low-resolution) and CelebA-HQ (high-resolution) datasets,\nrespectively, while maintaining parity in recognition accuracy. The superior\ncompression ratio is achieved as the model learns to retain the domain-specific\nfeatures (e.g., facial features) while sacrificing details in the background.\nFurthermore, images reconstructed by our proposed compression model are robust\nto changes in downstream model architectures. We show at-par recognition\nperformance on the LFW dataset with an unseen recognition model while retaining\na lower BPP value of ~38% of CRF-23 HEVC compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiuhong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_L/0/1/0/all/0/1\">Lavisha Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1\">Prithviraj Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1\">Manoj Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medioni_G/0/1/0/all/0/1\">Gerard Medioni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Audio Strikes Back: Boosting Augmentations Towards An Efficient Audio Classification Network. (arXiv:2204.11479v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.11479","description":"<p>While efficient architectures and a plethora of augmentations for end-to-end\nimage classification tasks have been suggested and heavily investigated,\nstate-of-the-art techniques for audio classifications still rely on numerous\nrepresentations of the audio signal together with large architectures,\nfine-tuned from large datasets. By utilizing the inherited lightweight nature\nof audio and novel audio augmentations, we were able to present an efficient\nend-to-end network with strong generalization ability. Experiments on a variety\nof sound classification sets demonstrate the effectiveness and robustness of\nour approach, by achieving state-of-the-art results in various settings. Public\ncode will be available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gazneli_A/0/1/0/all/0/1\">Avi Gazneli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1\">Gadi Zimerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1\">Tal Ridnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharir_G/0/1/0/all/0/1\">Gilad Sharir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation for Monocular 3D Object Detection via Self-Training. (arXiv:2204.11590v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11590","description":"<p>Monocular 3D object detection (Mono3D) has achieved unprecedented success\nwith the advent of deep learning techniques and emerging large-scale autonomous\ndriving datasets. However, drastic performance degradation remains an\nunwell-studied challenge for practical cross-domain deployment as the lack of\nlabels on the target domain. In this paper, we first comprehensively\ninvestigate the significant underlying factor of the domain gap in Mono3D,\nwhere the critical observation is a depth-shift issue caused by the geometric\nmisalignment of domains. Then, we propose STMono3D, a new self-teaching\nframework for unsupervised domain adaptation on Mono3D. To mitigate the\ndepth-shift, we introduce the geometry-aligned multi-scale training strategy to\ndisentangle the camera parameters and guarantee the geometry consistency of\ndomains. Based on this, we develop a teacher-student paradigm to generate\nadaptive pseudo labels on the target domain. Benefiting from the end-to-end\nframework that provides richer information of the pseudo labels, we propose the\nquality-aware supervision strategy to take instance-level pseudo confidences\ninto account and improve the effectiveness of the target-domain training\nprocess. Moreover, the positive focusing training strategy and dynamic\nthreshold are proposed to handle tremendous FN and FP pseudo samples. STMono3D\nachieves remarkable performance on all evaluated datasets and even surpasses\nfully supervised results on the KITTI 3D object detection dataset. To the best\nof our knowledge, this is the first study to explore effective UDA methods for\nMono3D.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zehui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Liangji Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xianming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Junjun Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performer: A Novel PPG to ECG Reconstruction Transformer For a Digital Biomarker of Cardiovascular Disease Detection. (arXiv:2204.11795v2 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2204.11795","description":"<p>Cardiovascular diseases (CVDs) have become the top one cause of death;\nthree-quarters of these deaths occur in lower-income communities.\nElectrocardiography (ECG), an electrical measurement capturing the cardiac\nactivities, is a gold-standard to diagnose CVDs. However, ECG is infeasible for\ncontinuous cardiac monitoring due to its requirement for user participation.\nMeanwhile, photoplethysmography (PPG) is easy to collect, but the limited\naccuracy constrains its clinical usage. In this research, a novel\nTransformer-based architecture, Performer, is invented to reconstruct ECG from\nPPG and to create a novel digital biomarker, PPG along with its reconstructed\nECG, as multiple modalities for CVD detection. This architecture, for the first\ntime, performs Transformer sequence to sequence translation on biomedical\nwaveforms, while also utilizing the advantages of the easily accessible PPG and\nthe well-studied base of ECG. Shifted Patch-based Attention (SPA) is created to\nmaximize the signal features by fetching the various sequence lengths as\nhierarchical stages into the training while also capturing cross-patch\nconnections through the shifted patch mechanism. This architecture generates a\nstate-of-the-art performance of 0.29 RMSE for reconstructing ECG from PPG,\nachieving an average of 95.9% diagnosis for CVDs on the MIMIC III dataset and\n75.9% for diabetes on the PPG-BP dataset. Performer, along with its novel\ndigital biomarker, offers a low-cost and non-invasive solution for continuous\ncardiac monitoring, only requiring the easily extractable PPG data to\nreconstruct the not-as-accessible ECG data. As a prove of concept, an earring\nwearable, named PEARL (prototype), is designed to scale up the point-of-care\n(POC) healthcare system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lan_E/0/1/0/all/0/1\">Ella Lan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive learning-based computational histopathology predict differential expression of cancer driver genes. (arXiv:2204.11994v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.11994","description":"<p>Digital pathological analysis is run as the main examination used for cancer\ndiagnosis. Recently, deep learning-driven feature extraction from pathology\nimages is able to detect genetic variations and tumor environment, but few\nstudies focus on differential gene expression in tumor cells. In this paper, we\npropose a self-supervised contrastive learning framework, HistCode, to infer\ndifferential gene expressions from whole slide images (WSIs). We leveraged\ncontrastive learning on large-scale unannotated WSIs to derive slide-level\nhistopathological feature in latent space, and then transfer it to tumor\ndiagnosis and prediction of differentially expressed cancer driver genes. Our\nextensive experiments showed that our method outperformed other\nstate-of-the-art models in tumor diagnosis tasks, and also effectively\npredicted differential gene expressions. Interestingly, we found the higher\nfold-changed genes can be more precisely predicted. To intuitively illustrate\nthe ability to extract informative features from pathological images, we\nspatially visualized the WSIs colored by the attentive scores of image tiles.\nWe found that the tumor and necrosis areas were highly consistent with the\nannotations of experienced pathologists. Moreover, the spatial heatmap\ngenerated by lymphocyte-specific gene expression patterns was also consistent\nwith the manually labeled WSI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haojie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gongming Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xuejun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dachuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the ability of generative adversarial networks to learn canonical medical image statistics. (arXiv:2204.12007v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.12007","description":"<p>In recent years, generative adversarial networks (GANs) have gained\ntremendous popularity for potential applications in medical imaging, such as\nmedical image synthesis, restoration, reconstruction, translation, as well as\nobjective image quality assessment. Despite the impressive progress in\ngenerating high-resolution, perceptually realistic images, it is not clear if\nmodern GANs reliably learn the statistics that are meaningful to a downstream\nmedical imaging application. In this work, the ability of a state-of-the-art\nGAN to learn the statistics of canonical stochastic image models (SIMs) that\nare relevant to objective assessment of image quality is investigated. It is\nshown that although the employed GAN successfully learned several basic first-\nand second-order statistics of the specific medical SIMs under consideration\nand generated images with high perceptual quality, it failed to correctly learn\nseveral per-image statistics pertinent to the these SIMs, highlighting the\nurgent need to assess medical image GANs in terms of objective measures of\nimage quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kelkar_V/0/1/0/all/0/1\">Varun A. Kelkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gotsis_D/0/1/0/all/0/1\">Dimitrios S. Gotsis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+KC_P/0/1/0/all/0/1\">Prabhat KC</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Myers_K/0/1/0/all/0/1\">Kyle J. Myers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_R/0/1/0/all/0/1\">Rongping Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Framework for Characterization of Tumor-Immune Spatial Relationships in Tumor Microenvironment. (arXiv:2204.12283v2 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2204.12283","description":"<p>Understanding the impact of tumor biology on the composition of nearby cells\noften requires characterizing the impact of biologically distinct tumor\nregions. Biomarkers have been developed to label biologically distinct tumor\nregions, but challenges arise because of differences in the spatial extent and\ndistribution of differentially labeled regions. In this work, we present a\nframework for systematically investigating the impact of distinct tumor regions\non cells near the tumor borders, accounting their cross spatial distributions.\nWe apply the framework to multiplex immunohistochemistry (mIHC) studies of\npancreatic cancer and show its efficacy in demonstrating how biologically\ndifferent tumor regions impact the immune response in the tumor\nmicroenvironment. Furthermore, we show that the proposed framework can be\nextended to largescale whole slide image analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Hasan_M/0/1/0/all/0/1\">Mahmudul Hasan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kaczmarzyk_J/0/1/0/all/0/1\">Jakub R. Kaczmarzyk</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Paredes_D/0/1/0/all/0/1\">David Paredes</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Oblein_L/0/1/0/all/0/1\">Lyanne Oblein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Oentoro_J/0/1/0/all/0/1\">Jaymie Oentoro</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Abousamra_S/0/1/0/all/0/1\">Shahira Abousamra</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Horowitz_M/0/1/0/all/0/1\">Michael Horowitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kurc_T/0/1/0/all/0/1\">Tahsin Kurc</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shroyer_K/0/1/0/all/0/1\">Kenneth R. Shroyer</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saltz_J/0/1/0/all/0/1\">Joel Saltz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Quality of a Synthesized Motion with the Fr\\'echet Motion Distance. (arXiv:2204.12318v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12318","description":"<p>Evaluating the Quality of a Synthesized Motion with the Fr\\'echet Motion\nDistance\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maiorca_A/0/1/0/all/0/1\">Antoine Maiorca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_Y/0/1/0/all/0/1\">Youngwoo Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutoit_T/0/1/0/all/0/1\">Thierry Dutoit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding The Robustness in Vision Transformers. (arXiv:2204.12451v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12451","description":"<p>Recent studies show that Vision Transformers(ViTs) exhibit strong robustness\nagainst various corruptions. Although this property is partly attributed to the\nself-attention mechanism, there is still a lack of systematic understanding. In\nthis paper, we examine the role of self-attention in learning robust\nrepresentations. Our study is motivated by the intriguing properties of the\nemerging visual grouping in Vision Transformers, which indicates that\nself-attention may promote robustness through improved mid-level\nrepresentations. We further propose a family of fully attentional networks\n(FANs) that strengthen this capability by incorporating an attentional channel\nprocessing design. We validate the design comprehensively on various\nhierarchical backbones. Our model achieves a state of-the-art 87.1% accuracy\nand 35.8% mCE on ImageNet-1k and ImageNet-C with 76.8M parameters. We also\ndemonstrate state-of-the-art accuracy and robustness in two downstream tasks:\nsemantic segmentation and object detection. Code will be available at\nhttps://github.com/NVlabs/FAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Daquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}