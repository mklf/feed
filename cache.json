{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-06-10T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Words are all you need? Capturing human sensory similarity with textual descriptors. (arXiv:2206.04105v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04105","description":"<p>Recent advances in multimodal training use textual descriptions to\nsignificantly enhance machine understanding of images and videos. Yet, it\nremains unclear to what extent language can fully capture sensory experiences\nacross different modalities. A well-established approach for characterizing\nsensory experiences relies on similarity judgments, namely, the degree to which\npeople perceive two distinct stimuli as similar. We explore the relation\nbetween human similarity judgments and language in a series of large-scale\nbehavioral studies ($N=1,823$ participants) across three modalities (images,\naudio, and video) and two types of text descriptors: simple word tags and\nfree-text captions. In doing so, we introduce a novel adaptive pipeline for tag\nmining that is both efficient and domain-general. We show that our prediction\npipeline based on text descriptors exhibits excellent performance, and we\ncompare it against a comprehensive array of 611 baseline models based on\nvision-, audio-, and video-processing architectures. We further show that the\ndegree to which textual descriptors and models predict human similarity varies\nacross and within modalities. Taken together, these studies illustrate the\nvalue of integrating machine learning and cognitive science approaches to\nbetter understand the similarities and differences between human and machine\nrepresentations. We present an interactive visualization at\nhttps://words-are-all-you-need.s3.amazonaws.com/index.html for exploring the\nsimilarity between stimuli as experienced by humans and different methods\nreported in the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marjieh_R/0/1/0/all/0/1\">Raja Marjieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijn_P/0/1/0/all/0/1\">Pol van Rijn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sucholutsky_I/0/1/0/all/0/1\">Ilia Sucholutsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumers_T/0/1/0/all/0/1\">Theodore R. Sumers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Harin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1\">Nori Jacoby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Text Normalization. (arXiv:2206.04137v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04137","description":"<p>Text-based adversarial attacks are becoming more commonplace and accessible\nto general internet users. As these attacks proliferate, the need to address\nthe gap in model robustness becomes imminent. While retraining on adversarial\ndata may increase performance, there remains an additional class of\ncharacter-level attacks on which these models falter. Additionally, the process\nto retrain a model is time and resource intensive, creating a need for a\nlightweight, reusable defense. In this work, we propose the Adversarial Text\nNormalizer, a novel method that restores baseline performance on attacked\ncontent with low computational overhead. We evaluate the efficacy of the\nnormalizer on two problem areas prone to adversarial attacks, i.e. Hate Speech\nand Natural Language Inference. We find that text normalization provides a\ntask-agnostic defense against character-level attacks that can be implemented\nsupplementary to adversarial retraining solutions, which are more suited for\nsemantic alterations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bitton_J/0/1/0/all/0/1\">Joanna Bitton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlova_M/0/1/0/all/0/1\">Maya Pavlova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Key Event Detection from Massive Text Corpora. (arXiv:2206.04153v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04153","description":"<p>Automated event detection from news corpora is a crucial task towards mining\nfast-evolving structured knowledge. As real-world events have different\ngranularities, from the top-level themes to key events and then to event\nmentions corresponding to concrete actions, there are generally two lines of\nresearch: (1) theme detection identifies from a news corpus major themes (e.g.,\n\"2019 Hong Kong Protests\" vs. \"2020 U.S. Presidential Election\") that have very\ndistinct semantics; and (2) action extraction extracts from one document\nmention-level actions (e.g., \"the police hit the left arm of the protester\")\nthat are too fine-grained for comprehending the event. In this paper, we\npropose a new task, key event detection at the intermediate level, aiming to\ndetect from a news corpus key events (e.g., \"HK Airport Protest on Aug.\n12-14\"), each happening at a particular time/location and focusing on the same\ntopic. This task can bridge event understanding and structuring and is\ninherently challenging because of the thematic and temporal closeness of key\nevents and the scarcity of labeled data due to the fast-evolving nature of news\narticles. To address these challenges, we develop an unsupervised key event\ndetection framework, EvMine, that (1) extracts temporally frequent peak phrases\nusing a novel ttf-itf score, (2) merges peak phrases into event-indicative\nfeature sets by detecting communities from our designed peak phrase graph that\ncaptures document co-occurrences, semantic similarities, and temporal closeness\nsignals, and (3) iteratively retrieves documents related to each key event by\ntraining a classifier with automatically generated pseudo labels from the\nevent-indicative feature sets and refining the detected key events using the\nretrieved documents. Extensive experiments and case studies show EvMine\noutperforms all the baseline methods and its ablations on two real-world news\ncorpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1\">Fang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved two-stage hate speech classification for twitter based on Deep Neural Networks. (arXiv:2206.04162v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04162","description":"<p>Hate speech is a form of online harassment that involves the use of abusive\nlanguage, and it is commonly seen in social media posts. This sort of\nharassment mainly focuses on specific group characteristics such as religion,\ngender, ethnicity, etc and it has both societal and economic consequences\nnowadays. The automatic detection of abusive language in text postings has\nalways been a difficult task, but it is lately receiving much interest from the\nscientific community. This paper addresses the important problem of discerning\nhateful content in social media. The model we propose in this work is an\nextension of an existing approach based on LSTM neural network architectures,\nwhich we appropriately enhanced and fine-tuned to detect certain forms of\nhatred language, such as racism or sexism, in a short text. The most\nsignificant enhancement is the conversion to a two-stage scheme consisting of\nRecurrent Neural Network (RNN) classifiers. The output of all One-vs-Rest (OvR)\nclassifiers from the first stage are combined and used to train the second\nstage classifier, which finally determines the type of harassment. Our study\nincludes a performance comparison of several proposed alternative methods for\nthe second stage evaluated on a public corpus of 16k tweets, followed by a\ngeneralization study on another dataset. The reported results show the superior\nclassification quality of the proposed scheme in the task of hate speech\ndetection as compared to the current state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pitsilis_G/0/1/0/all/0/1\">Georgios K. Pitsilis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstraction not Memory: BERT and the English Article System. (arXiv:2206.04184v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04184","description":"<p>Article prediction is a task that has long defied accurate linguistic\ndescription. As such, this task is ideally suited to evaluate models on their\nability to emulate native-speaker intuition. To this end, we compare the\nperformance of native English speakers and pre-trained models on the task of\narticle prediction set up as a three way choice (a/an, the, zero). Our\nexperiments with BERT show that BERT outperforms humans on this task across all\narticles. In particular, BERT is far superior to humans at detecting the zero\narticle, possibly because we insert them using rules that the deep neural model\ncan easily pick up. More interestingly, we find that BERT tends to agree more\nwith annotators than with the corpus when inter-annotator agreement is high but\nswitches to agreeing more with the corpus as inter-annotator agreement drops.\nWe contend that this alignment with annotators, despite being trained on the\ncorpus, suggests that BERT is not memorising article use, but captures a high\nlevel generalisation of article use akin to human intuition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madabushi_H/0/1/0/all/0/1\">Harish Tayyar Madabushi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Divjak_D/0/1/0/all/0/1\">Dagmar Divjak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milin_P/0/1/0/all/0/1\">Petar Milin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Question Generation for Personalized Feedback in Intelligent Tutoring Systems. (arXiv:2206.04187v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04187","description":"<p>Existing work on generating hints in Intelligent Tutoring Systems (ITS)\nfocuses mostly on manual and non-personalized feedback. In this work, we\nexplore automatically generated questions as personalized feedback in an ITS.\nOur personalized feedback can pinpoint correct and incorrect or missing phrases\nin student answers as well as guide them towards correct answer by asking a\nquestion in natural language. Our approach combines cause-effect analysis to\nbreak down student answers using text similarity-based NLP Transformer models\nto identify correct and incorrect or missing parts. We train a few-shot Neural\nQuestion Generation and Question Re-ranking models to show questions addressing\ncomponents missing in the student answers which steers students towards the\ncorrect answer. Our model vastly outperforms both simple and strong baselines\nin terms of student learning gains by 45% and 23% respectively when tested in a\nreal dialogue-based ITS. Finally, we show that our personalized corrective\nfeedback system has the potential to improve Generative Question Answering\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kulshreshtha_D/0/1/0/all/0/1\">Devang Kulshreshtha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shayan_M/0/1/0/all/0/1\">Muhammad Shayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belfer_R/0/1/0/all/0/1\">Robert Belfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serban_I/0/1/0/all/0/1\">Iulian Vlad Serban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kochmar_E/0/1/0/all/0/1\">Ekaterina Kochmar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Folktales of Different Regions Using Topic Modeling and Clustering. (arXiv:2206.04221v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04221","description":"<p>This paper employs two major natural language processing techniques, topic\nmodeling and clustering, to find patterns in folktales and reveal cultural\nrelationships between regions. In particular, we used Latent Dirichlet\nAllocation and BERTopic to extract the recurring elements as well as K-means\nclustering to group folktales. Our paper tries to answer the question what are\nthe similarities and differences between folktales, and what do they say about\nculture. Here we show that the common trends between folktales are family,\nfood, traditional gender roles, mythological figures, and animals. Also,\nfolktales topics differ based on geographical location with folktales found in\ndifferent regions having different animals and environment. We were not\nsurprised to find that religious figures and animals are some of the common\ntopics in all cultures. However, we were surprised that European and Asian\nfolktales were often paired together. Our results demonstrate the prevalence of\ncertain elements in cultures across the world. We anticipate our work to be a\nresource to future research of folktales and an example of using natural\nlanguage processing to analyze documents in specific domains. Furthermore,\nsince we only analyzed the documents based on their topics, more work could be\ndone in analyzing the structure, sentiment, and the characters of these\nfolktales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Werzinsky_J/0/1/0/all/0/1\">Jacob Werzinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhiyan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1\">Xuedan Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Crosslinguistic word order variation reflects evolutionary pressures of dependency and information locality. (arXiv:2206.04239v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04239","description":"<p>Languages vary considerably in syntactic structure. About 40% of the world's\nlanguages have subject-verb-object order, and about 40% have\nsubject-object-verb order. Extensive work has sought to explain this word order\nvariation across languages. However, the existing approaches are not able to\nexplain coherently the frequency distribution and evolution of word order in\nindividual languages. We propose that variation in word order reflects\ndifferent ways of balancing competing pressures of dependency locality and\ninformation locality, whereby languages favor placing elements together when\nthey are syntactically related or contextually informative about each other.\nUsing data from 80 languages in 17 language families and phylogenetic modeling,\nwe demonstrate that languages evolve to balance these pressures, such that word\norder change is accompanied by change in the frequency distribution of the\nsyntactic structures which speakers communicate to maintain overall efficiency.\nVariability in word order thus reflects different ways in which languages\nresolve these evolutionary pressures. We identify relevant characteristics that\nresult from this joint optimization, particularly the frequency with which\nsubjects and objects are expressed together for the same verb. Our findings\nsuggest that syntactic structure and usage across languages co-adapt to support\nefficient communication under limited cognitive resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1\">Michael Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLTS+: A New Chinese Long Text Summarization Dataset with Abstractive Summaries. (arXiv:2206.04253v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04253","description":"<p>The abstractive methods lack of creative ability is particularly a problem in\nautomatic text summarization. The summaries generated by models are mostly\nextracted from the source articles. One of the main causes for this problem is\nthe lack of dataset with abstractiveness, especially for Chinese. In order to\nsolve this problem, we paraphrase the reference summaries in CLTS, the Chinese\nLong Text Summarization dataset, correct errors of factual inconsistencies, and\npropose the first Chinese Long Text Summarization dataset with a high level of\nabstractiveness, CLTS+, which contains more than 180K article-summary pairs and\nis available online. Additionally, we introduce an intrinsic metric based on\nco-occurrence words to evaluate the dataset we constructed. We analyze the\nextraction strategies used in CLTS+ summaries against other datasets to\nquantify the abstractiveness and difficulty of our new data and train several\nbaselines on CLTS+ to verify the utility of it for improving the creative\nability of models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaojun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_S/0/1/0/all/0/1\">Shunan Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yangyang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FOAM: A Follower-aware Speaker Model For Vision-and-Language Navigation. (arXiv:2206.04294v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04294","description":"<p>The speaker-follower models have proven to be effective in\nvision-and-language navigation, where a speaker model is used to synthesize new\ninstructions to augment the training data for a follower navigation model.\nHowever, in many of the previous methods, the generated instructions are not\ndirectly trained to optimize the performance of the follower. In this paper, we\npresent \\textsc{foam}, a \\textsc{Fo}llower-\\textsc{a}ware speaker\n\\textsc{M}odel that is constantly updated given the follower feedback, so that\nthe generated instructions can be more suitable to the current learning state\nof the follower. Specifically, we optimize the speaker using a bi-level\noptimization framework and obtain its training signals by evaluating the\nfollower on labeled data. Experimental results on the Room-to-Room and\nRoom-across-Room datasets demonstrate that our methods can outperform strong\nbaseline models across settings. Analyses also reveal that our generated\ninstructions are of higher quality than the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unveiling Transformers with LEGO: a synthetic reasoning task. (arXiv:2206.04301v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04301","description":"<p>We propose a synthetic task, LEGO (Learning Equality and Group Operations),\nthat encapsulates the problem of following a chain of reasoning, and we study\nhow the transformer architecture learns this task. We pay special attention to\ndata effects such as pretraining (on seemingly unrelated NLP tasks) and dataset\ncomposition (e.g., differing chain length at training and test time), as well\nas architectural variants such as weight-tied layers or adding convolutional\ncomponents. We study how the trained models eventually succeed at the task, and\nin particular, we are able to understand (to some extent) some of the attention\nheads as well as how the information flows in the network. Based on these\nobservations we propose a hypothesis that here pretraining helps merely due to\nbeing a smart initialization rather than some deep knowledge stored in the\nnetwork. We also observe that in some data regime the trained transformer finds\n\"shortcut\" solutions to follow the chain of reasoning, which impedes the\nmodel's ability to generalize to simple variants of the main task, and moreover\nwe find that one can prevent such shortcut with appropriate architecture\nmodification or careful data preparation. Motivated by our findings, we begin\nto explore the task of learning to execute C programs, where a convolutional\nmodification to transformers, namely adding convolutional structures in the\nkey/query/value maps, shows an encouraging edge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Backurs_A/0/1/0/all/0/1\">Arturs Backurs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1\">S&#xe9;bastien Bubeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1\">Ronen Eldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1\">Tal Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context-based out-of-vocabulary word recovery for ASR systems in Indian languages. (arXiv:2206.04305v1 [eess.AS])","link":"http://arxiv.org/abs/2206.04305","description":"<p>Detecting and recovering out-of-vocabulary (OOV) words is always challenging\nfor Automatic Speech Recognition (ASR) systems. Many existing methods focus on\nmodeling OOV words by modifying acoustic and language models and integrating\ncontext words cleverly into models. To train such complex models, we need a\nlarge amount of data with context words, additional training time, and\nincreased model size. However, after getting the ASR transcription to recover\ncontext-based OOV words, the post-processing method has not been explored much.\nIn this work, we propose a post-processing technique to improve the performance\nof context-based OOV recovery. We created an acoustically boosted language\nmodel with a sub-graph made at phone level with an OOV words list. We proposed\ntwo methods to determine a suitable cost function to retrieve the OOV words\nbased on the context. The cost function is defined based on phonetic and\nacoustic knowledge for matching and recovering the correct context words in the\ndecode. The effectiveness of the proposed cost function is evaluated at both\nword-level and sentence-level. The evaluation results show that this approach\ncan recover an average of 50% context-based OOV words across multiple\ncategories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baby_A/0/1/0/all/0/1\">Arun Baby</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vinnaitherthan_S/0/1/0/all/0/1\">Saranya Vinnaitherthan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kerhalkar_A/0/1/0/all/0/1\">Akhil Kerhalkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jawale_P/0/1/0/all/0/1\">Pranav Jawale</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adavanne_S/0/1/0/all/0/1\">Sharath Adavanne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adiga_N/0/1/0/all/0/1\">Nagaraj Adiga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Aware Evaluation and Transformer Methods for Topic-Controllable Summarization. (arXiv:2206.04317v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04317","description":"<p>Topic-controllable summarization is an emerging research area with a wide\nrange of potential applications. However, existing approaches suffer from\nsignificant limitations. First, there is currently no established evaluation\nmetric for this task. Furthermore, existing methods built upon recurrent\narchitectures, which can significantly limit their performance compared to more\nrecent Transformer-based architectures, while they also require modifications\nto the model's architecture for controlling the topic. In this work, we propose\na new topic-oriented evaluation measure to automatically evaluate the generated\nsummaries based on the topic affinity between the generated summary and the\ndesired topic. We also conducted a user study that validates the reliability of\nthis measure. Finally, we propose simple, yet powerful methods for\ntopic-controllable summarization either incorporating topic embeddings into the\nmodel's architecture or employing control tokens to guide the summary\ngeneration. Experimental results show that control tokens can achieve better\nperformance compared to more complicated embedding-based approaches while being\nat the same time significantly faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passali_T/0/1/0/all/0/1\">Tatiana Passali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1\">Grigorios Tsoumakas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Identification for Austronesian Languages. (arXiv:2206.04327v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04327","description":"<p>This paper provides language identification models for low- and\nunder-resourced languages in the Pacific region with a focus on previously\nunavailable Austronesian languages. Accurate language identification is an\nimportant part of developing language resources. The approach taken in this\npaper combines 29 Austronesian languages with 171 non-Austronesian languages to\ncreate an evaluation set drawn from eight data sources. After evaluating six\napproaches to language identification, we find that a classifier based on\nskip-gram embeddings reaches a significantly higher performance than alternate\nmethods. We then systematically increase the number of non-Austronesian\nlanguages in the model up to a total of 800 languages to evaluate whether an\nincreased language inventory leads to less precise predictions for the\nAustronesian languages of interest. This evaluation finds that there is only a\nminimal impact on accuracy caused by increasing the inventory of\nnon-Austronesian languages. Further experiments adapt these language\nidentification models for code-switching detection, achieving high accuracy\nacross all 29 languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jonathan Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nijhof_W/0/1/0/all/0/1\">Wikke Nijhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Embedding Reliability in Low-Resource Settings Using Corpus Similarity Measures. (arXiv:2206.04330v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04330","description":"<p>This paper simulates a low-resource setting across 17 languages in order to\nevaluate embedding similarity, stability, and reliability under different\nconditions. The goal is to use corpus similarity measures before training to\npredict properties of embeddings after training. The main contribution of the\npaper is to show that it is possible to predict downstream embedding similarity\nusing upstream corpus similarity measures. This finding is then applied to\nlow-resource settings by modelling the reliability of embeddings created from\nvery limited training data. Results show that it is possible to estimate the\nreliability of low-resource embeddings using corpus similarity measures that\nremain robust on small amounts of data. These findings have significant\nimplications for the evaluation of truly low-resource languages in which such\nsystematic downstream validation methods are not possible because of data\nlimitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jonathan Dunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastre_D/0/1/0/all/0/1\">Damian Sastre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corpus Similarity Measures Remain Robust Across Diverse Languages. (arXiv:2206.04332v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04332","description":"<p>This paper experiments with frequency-based corpus similarity measures across\n39 languages using a register prediction task. The goal is to quantify (i) the\ndistance between different corpora from the same language and (ii) the\nhomogeneity of individual corpora. Both of these goals are essential for\nmeasuring how well corpus-based linguistic analysis generalizes from one\ndataset to another. The problem is that previous work has focused on\nIndo-European languages, raising the question of whether these measures are\nable to provide robust generalizations across diverse languages. This paper\nuses a register prediction task to evaluate competing measures across 39\nlanguages: how well are they able to distinguish between corpora representing\ndifferent contexts of production? Each experiment compares three corpora from a\nsingle language, with the same three digital registers shared across all\nlanguages: social media, web pages, and Wikipedia. Results show that measures\nof corpus similarity retain their validity across different language families,\nwriting systems, and types of morphology. Further, the measures remain robust\nwhen evaluated on out-of-domain corpora, when applied to low-resource\nlanguages, and when applied to different sets of registers. These findings are\nsignificant given our need to make generalizations across the rapidly\nincreasing number of corpora available for analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1\">Jonathan Dunn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ancestor-to-Creole Transfer is Not a Walk in the Park. (arXiv:2206.04371v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04371","description":"<p>We aim to learn language models for Creole languages for which large volumes\nof data are not readily available, and therefore explore the potential transfer\nfrom ancestor languages (the 'Ancestry Transfer Hypothesis'). We find that\nstandard transfer methods do not facilitate ancestry transfer. Surprisingly,\ndifferent from other non-Creole languages, a very distinct two-phase pattern\nemerges for Creoles: As our training losses plateau, and language models begin\nto overfit on their source languages, perplexity on the Creoles drop. We\nexplore if this compression phase can lead to practically useful language\nmodels (the 'Ancestry Bottleneck Hypothesis'), but also falsify this. Moreover,\nwe show that Creoles even exhibit this two-phase pattern even when training on\nrandom, unrelated languages. Thus Creoles seem to be typological outliers and\nwe speculate whether there is a link between the two observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lent_H/0/1/0/all/0/1\">Heather Lent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dict-NMT: Bilingual Dictionary based NMT for Extremely Low Resource Languages. (arXiv:2206.04439v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04439","description":"<p>Neural Machine Translation (NMT) models have been effective on large\nbilingual datasets. However, the existing methods and techniques show that the\nmodel's performance is highly dependent on the number of examples in training\ndata. For many languages, having such an amount of corpora is a far-fetched\ndream. Taking inspiration from monolingual speakers exploring new languages\nusing bilingual dictionaries, we investigate the applicability of bilingual\ndictionaries for languages with extremely low, or no bilingual corpus. In this\npaper, we explore methods using bilingual dictionaries with an NMT model to\nimprove translations for extremely low resource languages. We extend this work\nto multilingual systems, exhibiting zero-shot properties. We present a detailed\nanalysis of the effects of the quality of dictionaries, training dataset size,\nlanguage family, etc., on the translation quality. Results on multiple\nlow-resource test languages show a clear advantage of our bilingual\ndictionary-based method over the baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1\">Nalin Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Subhankar Mishra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Encoder-Decoder Self-Supervised Pre-training for ASR. (arXiv:2206.04465v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04465","description":"<p>Self-supervised learning (SSL) has shown tremendous success in various\nspeech-related downstream tasks, including Automatic Speech Recognition (ASR).\nThe output embeddings of the SSL model are treated as powerful short-time\nrepresentations of the speech signal. However, in the ASR task, the main\nobjective is to get the correct sequence of acoustic units, characters, or\nbyte-pair encodings (BPEs). Usually, encoder-decoder architecture works\nexceptionally well for a sequence-to-sequence task like ASR. Therefore, in this\npaper, we propose a new paradigm that exploits the power of a decoder during\nself-supervised learning. We use Hidden Unit BERT (HuBERT) SSL framework to\ncompute the conventional masked prediction loss for the encoder. In addition,\nwe have introduced a decoder in the SSL framework and proposed a target\npreparation strategy for the decoder. Finally, we use a multitask SSL setup\nwherein we jointly optimize both the encoder and decoder losses. We hypothesize\nthat the presence of a decoder in the SSL model helps it learn an acoustic\nunit-based language model, which might improve the performance of an ASR\ndownstream task. We compare our proposed SSL model with HuBERT and show up to\n25% relative improvement in performance on ASR by finetuning on various\nLibriSpeech subsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+A_A/0/1/0/all/0/1\">Arunkumar A</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_U/0/1/0/all/0/1\">Umesh S</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SsciBERT: A Pre-trained Language Model for Social Science Texts. (arXiv:2206.04510v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04510","description":"<p>The academic literature of social sciences is the literature that records\nhuman civilization and studies human social problems. With the large-scale\ngrowth of this literature, ways to quickly find existing research on relevant\nissues have become an urgent demand for researchers. Previous studies, such as\nSciBERT, have shown that pre-training using domain-specific texts can improve\nthe performance of natural language processing tasks in those fields. However,\nthere is no pre-trained language model for social sciences, so this paper\nproposes a pre-trained model on many abstracts published in the Social Science\nCitation Index (SSCI) journals. The models, which are available on Github\n(https://github.com/S-T-Full-Text-Knowledge-Mining/SSCI-BERT), show excellent\nperformance on discipline classification and abstract structure-function\nrecognition tasks with the social sciences literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Si Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiangfeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Litao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ying Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dongbo Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos. (arXiv:2206.04523v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04523","description":"<p>In this paper, we propose a neural end-to-end system for voice preserving,\nlip-synchronous translation of videos. The system is designed to combine\nmultiple component models and produces a video of the original speaker speaking\nin the target language that is lip-synchronous with the target speech, yet\nmaintains emphases in speech, voice characteristics, face video of the original\nspeaker. The pipeline starts with automatic speech recognition including\nemphasis detection, followed by a translation model. The translated text is\nthen synthesized by a Text-to-Speech model that recreates the original emphases\nmapped from the original sentence. The resulting synthetic voice is then mapped\nback to the original speakers' voice using a voice conversion model. Finally,\nto synchronize the lips of the speaker with the translated audio, a conditional\ngenerative adversarial network-based model generates frames of adapted lip\nmovements with respect to the input face image as well as the output of the\nvoice conversion model. In the end, the system combines the generated video\nwith the converted audio to produce the final output. The result is a video of\na speaker speaking in another language without actually knowing it. To evaluate\nour design, we present a user study of the complete system as well as separate\nevaluations of the single components. Since there is no available dataset to\nevaluate our whole system, we collect a test set and evaluate our system on\nthis test set. The results indicate that our system is able to generate\nconvincing videos of the original speaker speaking the target language while\npreserving the original speaker's characteristics. The collected dataset will\nbe shared.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behr_M/0/1/0/all/0/1\">Moritz Behr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyiokur_F/0/1/0/all/0/1\">Fevziye Irem Eyiokur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullov_C/0/1/0/all/0/1\">Carlos Mullov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demirtas_M/0/1/0/all/0/1\">Mehmet Arif Demirtas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarc&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantin_S/0/1/0/all/0/1\">Stefan Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting End-to-End Speech-to-Text Translation From Scratch. (arXiv:2206.04571v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04571","description":"<p>End-to-end (E2E) speech-to-text translation (ST) often depends on pretraining\nits encoder and/or decoder using source transcripts via speech recognition or\ntext translation tasks, without which translation performance drops\nsubstantially. However, transcripts are not always available, and how\nsignificant such pretraining is for E2E ST has rarely been studied in the\nliterature. In this paper, we revisit this question and explore the extent to\nwhich the quality of E2E ST trained on speech-translation pairs alone can be\nimproved. We reexamine several techniques proven beneficial to ST previously,\nand offer a set of best practices that biases a Transformer-based E2E ST system\ntoward training from scratch. Besides, we propose parameterized distance\npenalty to facilitate the modeling of locality in the self-attention model for\nspeech. On four benchmarks covering 23 languages, our experiments show that,\nwithout using any transcripts or pretraining, the proposed system reaches and\neven outperforms previous studies adopting pretraining, although the gap\nremains in (extremely) low-resource settings. Finally, we discuss neural\nacoustic feature modeling, where a neural model is designed to extract acoustic\nfeatures from raw speech signals directly, with the goal to simplify inductive\nbiases and add freedom to the model in describing speech. For the first time,\nwe demonstrate its feasibility and show encouraging results on ST tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1\">Barry Haddow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Zero-shot Common Sense from Large Language Models for Robot 3D Scene Understanding. (arXiv:2206.04585v1 [cs.RO])","link":"http://arxiv.org/abs/2206.04585","description":"<p>Semantic 3D scene understanding is a problem of critical importance in\nrobotics. While significant advances have been made in simultaneous\nlocalization and mapping algorithms, robots are still far from having the\ncommon sense knowledge about household objects and their locations of an\naverage human. We introduce a novel method for leveraging common sense embedded\nwithin large language models for labelling rooms given the objects contained\nwithin. This algorithm has the added benefits of (i) requiring no task-specific\npre-training (operating entirely in the zero-shot regime) and (ii) generalizing\nto arbitrary room and object labels, including previously-unseen ones -- both\nof which are highly desirable traits in robotic scene understanding algorithms.\nThe proposed algorithm operates on 3D scene graphs produced by modern spatial\nperception systems, and we hope it will pave the way to more generalizable and\nscalable high-level 3D scene understanding for robotics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">William Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Siyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talak_R/0/1/0/all/0/1\">Rajat Talak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Privacy Leakage in Text Classification: A Data Extraction Approach. (arXiv:2206.04591v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04591","description":"<p>Recent work has demonstrated the successful extraction of training data from\ngenerative language models. However, it is not evident whether such extraction\nis feasible in text classification models since the training objective is to\npredict the class label as opposed to next-word prediction. This poses an\ninteresting challenge and raises an important question regarding the privacy of\ntraining data in text classification settings. Therefore, we study the\npotential privacy leakage in the text classification domain by investigating\nthe problem of unintended memorization of training data that is not pertinent\nto the learning task. We propose an algorithm to extract missing tokens of a\npartial text by exploiting the likelihood of the class label provided by the\nmodel. We test the effectiveness of our algorithm by inserting canaries into\nthe training set and attempting to extract tokens in these canaries\npost-training. In our experiments, we demonstrate that successful extraction is\npossible to some extent. This can also be used as an auditing strategy to\nassess any potential unauthorized use of personal data without consent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Elmahdy_A/0/1/0/all/0/1\">Adel Elmahdy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_H/0/1/0/all/0/1\">Huseyin A. Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_R/0/1/0/all/0/1\">Robert Sim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models. (arXiv:2206.04615v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04615","description":"<p>Language models demonstrate both quantitative improvement and new qualitative\ncapabilities with increasing scale. Despite their potentially transformative\nimpact, these new capabilities are as yet poorly characterized. In order to\ninform future research, prepare for disruptive new model capabilities, and\nameliorate socially harmful effects, it is vital that we understand the present\nand near-future capabilities and limitations of language models. To address\nthis challenge, we introduce the Beyond the Imitation Game benchmark\n(BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442\nauthors across 132 institutions. Task topics are diverse, drawing problems from\nlinguistics, childhood development, math, common-sense reasoning, biology,\nphysics, social bias, software development, and beyond. BIG-bench focuses on\ntasks that are believed to be beyond the capabilities of current language\nmodels. We evaluate the behavior of OpenAI's GPT models, Google-internal dense\ntransformer architectures, and Switch-style sparse transformers on BIG-bench,\nacross model sizes spanning millions to hundreds of billions of parameters. In\naddition, a team of human expert raters performed all tasks in order to provide\na strong baseline. Findings include: model performance and calibration both\nimprove with scale, but are poor in absolute terms (and when compared with\nrater performance); performance is remarkably similar across model classes,\nthough with benefits from sparsity; tasks that improve gradually and\npredictably commonly involve a large knowledge or memorization component,\nwhereas tasks that exhibit \"breakthrough\" behavior at a critical scale often\ninvolve multiple steps or components, or brittle metrics; social bias typically\nincreases with scale in settings with ambiguous context, but this can be\nimproved with prompting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Aarohi Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1\">Abhinav Rastogi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Abhishek Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeb_A/0/1/0/all/0/1\">Abu Awal Md Shoeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abid_A/0/1/0/all/0/1\">Abubakar Abid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1\">Adam Fisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1\">Adam R. Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santoro_A/0/1/0/all/0/1\">Adam Santoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garriga_Alonso_A/0/1/0/all/0/1\">Adri&#xe0; Garriga-Alonso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kluska_A/0/1/0/all/0/1\">Agnieszka Kluska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Akshat Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Power_A/0/1/0/all/0/1\">Alethea Power</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Alex Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1\">Alex Warstadt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocurek_A/0/1/0/all/0/1\">Alexander W. Kocurek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safaya_A/0/1/0/all/0/1\">Ali Safaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1\">Ali Tazarv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_A/0/1/0/all/0/1\">Alice Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parrish_A/0/1/0/all/0/1\">Alicia Parrish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_A/0/1/0/all/0/1\">Allen Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Aman Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1\">Amanda Askell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dsouza_A/0/1/0/all/0/1\">Amanda Dsouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahane_A/0/1/0/all/0/1\">Ameet Rahane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Anantharaman S. Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stuhlmuller_A/0/1/0/all/0/1\">Andreas Stuhlm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+La_A/0/1/0/all/0/1\">Andrew La</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampinen_A/0/1/0/all/0/1\">Andrew Lampinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1\">Angela Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Angelica Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_A/0/1/0/all/0/1\">Anh Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Animesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottardi_A/0/1/0/all/0/1\">Anna Gottardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norelli_A/0/1/0/all/0/1\">Antonio Norelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_A/0/1/0/all/0/1\">Anu Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholamidavoodi_A/0/1/0/all/0/1\">Arash Gholamidavoodi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabassum_A/0/1/0/all/0/1\">Arfa Tabassum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirubarajan_A/0/1/0/all/0/1\">Arun Kirubarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullokandov_A/0/1/0/all/0/1\">Asher Mullokandov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herrick_A/0/1/0/all/0/1\">Austin Herrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1\">Avia Efrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1\">Aykut Erdem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karakas_A/0/1/0/all/0/1\">Ayla Karaka&#x15f;</a>, et al. (392 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factuality Enhanced Language Models for Open-Ended Text Generation. (arXiv:2206.04624v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04624","description":"<p>Pretrained language models (LMs) are susceptible to generate text with\nnonfactual information. In this work, we measure and improve the factual\naccuracy of large-scale LMs for open-ended text generation. We design the\nFactualityPrompts test set and metrics to measure the factuality of LM\ngenerations. Based on that, we study the factual accuracy of LMs with parameter\nsizes ranging from 126M to 530B. Interestingly, we find that larger LMs are\nmore factual than smaller ones, although a previous study suggests that larger\nLMs can be less truthful in terms of misconceptions. In addition, popular\nsampling algorithms (e.g., top-p) in open-ended text generation can harm the\nfactuality due to the \"uniform randomness\" introduced at every sampling step.\nWe propose the factual-nucleus sampling algorithm that dynamically adapts the\nrandomness to improve the factuality of generation while maintaining quality.\nFurthermore, we analyze the inefficiencies of the standard training method in\nlearning correct associations between entities from factual text corpus (e.g.,\nWikipedia). We propose a factuality-enhanced training method that uses\nTopicPrefix for better awareness of facts and sentence completion as the\ntraining objective, which can vastly reduce the factual errors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1\">Nayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BigVGAN: A Universal Neural Vocoder with Large-Scale Training. (arXiv:2206.04658v1 [cs.SD])","link":"http://arxiv.org/abs/2206.04658","description":"<p>Despite recent progress in generative adversarial network(GAN)-based\nvocoders, where the model generates raw waveform conditioned on mel\nspectrogram, it is still challenging to synthesize high-fidelity audio for\nnumerous speakers across varied recording environments. In this work, we\npresent BigVGAN, a universal vocoder that generalizes well under various unseen\nconditions in zero-shot setting. We introduce periodic nonlinearities and\nanti-aliased representation into the generator, which brings the desired\ninductive bias for waveform synthesis and significantly improves audio quality.\nBased on our improved generator and the state-of-the-art discriminators, we\ntrain our GAN vocoder at the largest scale up to 112M parameters, which is\nunprecedented in the literature. In particular, we identify and address the\ntraining instabilities specific to such scale, while maintaining high-fidelity\noutput without over-regularization. Our BigVGAN achieves the state-of-the-art\nzero-shot performance for various out-of-distribution scenarios, including new\nspeakers, novel languages, singing voices, music and instrumental audio in\nunseen (even noisy) recording environments. We will release our code and model\nat: https://github.com/NVIDIA/BigVGAN\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-gil Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1\">Wei Ping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1\">Boris Ginsburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jewelry Shop Conversational Chatbot. (arXiv:2206.04659v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04659","description":"<p>Since the advent of chatbots in the commercial sector, they have been widely\nemployed in the customer service department. Typically, these commercial\nchatbots are retrieval-based, so they are unable to respond to queries absent\nin the provided dataset. On the contrary, generative chatbots try to create the\nmost appropriate response, but are mostly unable to create a smooth flow in the\ncustomer-bot dialog. Since the client has few options left for continuing after\nreceiving a response, the dialog becomes short. Through our work, we try to\nmaximize the intelligence of a simple conversational agent so it can answer\nunseen queries, and generate follow-up questions or remarks. We have built a\nchatbot for a jewelry shop that finds the underlying objective of the\ncustomer's query by finding similarity of the input to patterns in the corpus.\nOur system features an audio input interface for clients, so they may speak to\nit in natural language. After converting the audio to text, we trained the\nmodel to extract the intent of the query, to find an appropriate response and\nto speak to the client in a natural human voice. To gauge the system's\nperformance, we used performance metrics such as Recall, Precision and F1\nscore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaid_S/0/1/0/all/0/1\">Safa Zaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1\">Aswah Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatima_K/0/1/0/all/0/1\">Kisa Fatima</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05233","description":"<p>State-of-the-art Named Entity Recognition(NER) models rely heavily on large\namountsof fully annotated training data. However, ac-cessible data are often\nincompletely annotatedsince the annotators usually lack comprehen-sive\nknowledge in the target domain. Normallythe unannotated tokens are regarded as\nnon-entities by default, while we underline thatthese tokens could either be\nnon-entities orpart of any entity. Here, we study NER mod-eling with incomplete\nannotated data whereonly a fraction of the named entities are la-beled, and the\nunlabeled tokens are equiva-lently multi-labeled by every possible label.Taking\nmulti-labeled tokens into account, thenumerous possible paths can distract the\ntrain-ing model from the gold path (ground truthlabel sequence), and thus\nhinders the learn-ing ability. In this paper, we propose AdaK-NER, named the\nadaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion\nwhere the gold path is more likely to belocated. We demonstrate the superiority\nofour approach through extensive experimentson both English and Chinese\ndatasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on\ntwo Chinese datasetscompared with the prior state-of-the-art works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1\">Hongtao Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liying Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peixian Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-Supervised Automatic Post-Editing Data Generation Tool. (arXiv:2111.12284v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.12284","description":"<p>Data building for automatic post-editing (APE) requires extensive and\nexpert-level human effort, as it contains an elaborate process that involves\nidentifying errors in sentences and providing suitable revisions. Hence, we\ndevelop a self-supervised data generation tool, deployable as a web\napplication, that minimizes human supervision and constructs personalized APE\ndata from a parallel corpus for several language pairs with English as the\ntarget language. Data-centric APE research can be conducted using this tool,\ninvolving many language pairs that have not been studied thus far owing to the\nlack of suitable data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1\">Hyeonseok Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanjun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eo_S/0/1/0/all/0/1\">Sugyeong Eo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1\">Jaehyung Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">SeungJun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Heuiseok Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Attention Network for Stock Movements Prediction. (arXiv:2112.13593v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.13593","description":"<p>Stock prices move as piece-wise trending fluctuation rather than a purely\nrandom walk. Traditionally, the prediction of future stock movements is based\non the historical trading record. Nowadays, with the development of social\nmedia, many active participants in the market choose to publicize their\nstrategies, which provides a window to glimpse over the whole market's attitude\ntowards future movements by extracting the semantics behind social media.\nHowever, social media contains conflicting information and cannot replace\nhistorical records completely. In this work, we propose a multi-modality\nattention network to reduce conflicts and integrate semantic and numeric\nfeatures to predict future stock movements comprehensively. Specifically, we\nfirst extract semantic information from social media and estimate their\ncredibility based on posters' identity and public reputation. Then we\nincorporate the semantic from online posts and numeric features from historical\nrecords to make the trading strategy. Experimental results show that our\napproach outperforms previous methods by a significant margin in both\nprediction accuracy (61.20\\%) and trading profits (9.13\\%). It demonstrates\nthat our method improves the performance of stock movements prediction and\ninforms future research on multi-modality fusion towards stock prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Open Text Release 1: Public Domain News in 44 Languages. (arXiv:2201.05609v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.05609","description":"<p>We present Multilingual Open Text (MOT), a new multilingual corpus containing\ntext in 44 languages, many of which have limited existing text resources for\nnatural language processing. The first release of the corpus contains over 2.8\nmillion news articles and an additional 1 million short snippets (photo\ncaptions, video descriptions, etc.) published between 2001--2022 and collected\nfrom Voice of America's news websites. We describe our process for collecting,\nfiltering, and processing the data. The source material is in the public\ndomain, our collection is licensed using a creative commons license (CC BY\n4.0), and all software used to create the corpus is released under the MIT\nLicense. The corpus will be regularly updated as additional documents are\npublished.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">June Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval. (arXiv:2201.12431v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.12431","description":"<p>Retrieval-based language models (R-LM) model the probability of natural\nlanguage text by combining a standard language model (LM) with examples\nretrieved from an external datastore at test time. While effective, a major\nbottleneck of using these models in practice is the computationally costly\ndatastore search, which can be performed as frequently as every time step. In\nthis paper, we present RetoMaton - retrieval automaton - which approximates the\ndatastore search, based on (1) saving pointers between consecutive datastore\nentries, and (2) clustering of entries into \"states\". This effectively results\nin a weighted finite automaton built on top of the datastore, instead of\nrepresenting the datastore as a flat list. The creation of the automaton is\nunsupervised, and a RetoMaton can be constructed from any text collection:\neither the original training corpus or from another domain. Traversing this\nautomaton at inference time, in parallel to the LM inference, reduces its\nperplexity by up to 1.85, or alternatively saves up to 83% of the nearest\nneighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting\nperplexity. Our code and trained models are available at\nhttps://github.com/neulab/retomaton .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alon_U/0/1/0/all/0/1\">Uri Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Sudipta Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16434","description":"<p>We consider the problem of localizing a spatio-temporal tube in a video\ncorresponding to a given text query. This is a challenging task that requires\nthe joint and efficient modeling of temporal, spatial and multi-modal\ninteractions. To address this task, we propose TubeDETR, a transformer-based\narchitecture inspired by the recent success of such models for text-conditioned\nobject detection. Our model notably includes: (i) an efficient video and text\nencoder that models spatial multi-modal interactions over sparsely sampled\nframes and (ii) a space-time decoder that jointly performs spatio-temporal\nlocalization. We demonstrate the advantage of our proposed components through\nan extensive ablation study. We also evaluate our full approach on the\nspatio-temporal video grounding task and demonstrate improvements over the\nstate of the art on the challenging VidSTG and HC-STVG benchmarks. Code and\ntrained models are publicly available at\nhttps://antoyang.github.io/tubedetr.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CREER: A Large-Scale Corpus for Relation Extraction and Entity Recognition. (arXiv:2204.12710v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.12710","description":"<p>We describe the design and use of the CREER dataset, a large corpus annotated\nwith rich English grammar and semantic attributes. The CREER dataset uses the\nStanford CoreNLP Annotator to capture rich language structures from Wikipedia\nplain text. This dataset follows widely used linguistic and semantic\nannotations so that it can be used for not only most natural language\nprocessing tasks but also scaling the dataset. This large supervised dataset\ncan serve as the basis for improving the performance of NLP tasks in the\nfuture. We publicize the dataset through the link:\nhttps://140.116.82.111/share.cgi?ssid=000dOJ4\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Siou Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chung-Hsien Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Russian Texts Detoxification with Levenshtein Editing. (arXiv:2204.13638v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.13638","description":"<p>Text detoxification is a style transfer task of creating neutral versions of\ntoxic texts. In this paper, we use the concept of text editing to build a\ntwo-step tagging-based detoxification model using a parallel corpus of Russian\ntexts. With this model, we achieved the best style transfer accuracy among all\nmodels in the RUSSE Detox shared task, surpassing larger sequence-to-sequence\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Original or Translated? A Causal Analysis of the Impact of Translationese on Machine Translation Performance. (arXiv:2205.02293v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02293","description":"<p>Human-translated text displays distinct features from naturally written text\nin the same language. This phenomena, known as translationese, has been argued\nto confound the machine translation (MT) evaluation. Yet, we find that existing\nwork on translationese neglects some important factors and the conclusions are\nmostly correlational but not causal. In this work, we collect CausalMT, a\ndataset where the MT training data are also labeled with the human translation\ndirections. We inspect two critical factors, the train-test direction match\n(whether the human translation directions in the training and test sets are\naligned), and data-model direction match (whether the model learns in the same\ndirection as the human translation direction in the dataset). We show that\nthese two factors have a large causal effect on the MT performance, in addition\nto the test-model direction mismatch highlighted by existing work on the impact\nof translationese. In light of our findings, we provide a set of suggestions\nfor MT training and evaluation. Our code and data are at\nhttps://github.com/EdisonNi-hku/CausalMT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhijing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large Language Models are Zero-Shot Reasoners. (arXiv:2205.11916v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.11916","description":"<p>Pretrained large language models (LLMs) are widely used in many sub-fields of\nnatural language processing (NLP) and generally known as excellent few-shot\nlearners with task-specific exemplars. Notably, chain of thought (CoT)\nprompting, a recent technique for eliciting complex multi-step reasoning\nthrough step-by-step answer examples, achieved the state-of-the-art\nperformances in arithmetics and symbolic reasoning, difficult system-2 tasks\nthat do not follow the standard scaling laws for LLMs. While these successes\nare often attributed to LLMs' ability for few-shot learning, we show that LLMs\nare decent zero-shot reasoners by simply adding \"Let's think step by step\"\nbefore each answer. Experimental results demonstrate that our Zero-shot-CoT,\nusing the same single prompt template, significantly outperforms zero-shot LLM\nperformances on diverse benchmark reasoning tasks including arithmetics\n(MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin\nFlip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled\nObjects), without any hand-crafted few-shot examples, e.g. increasing the\naccuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with\n175B parameter InstructGPT model, as well as similar magnitudes of improvements\nwith another off-the-shelf large model, 540B parameter PaLM. The versatility of\nthis single prompt across very diverse reasoning tasks hints at untapped and\nunderstudied fundamental zero-shot capabilities of LLMs, suggesting high-level,\nmulti-task broad cognitive capabilities may be extracted by simple prompting.\nWe hope our work not only serves as the minimal strongest zero-shot baseline\nfor the challenging reasoning benchmarks, but also highlights the importance of\ncarefully exploring and analyzing the enormous zero-shot knowledge hidden\ninside LLMs before crafting finetuning datasets or few-shot exemplars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kojima_T/0/1/0/all/0/1\">Takeshi Kojima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwasawa_Y/0/1/0/all/0/1\">Yusuke Iwasawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Dense Graph Do You Need for Self-Attention?. (arXiv:2205.14014v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.14014","description":"<p>Transformers have made progress in miscellaneous tasks, but suffer from\nquadratic computational and memory complexities. Recent works propose sparse\nTransformers with attention on sparse graphs to reduce complexity and remain\nstrong performance. While effective, the crucial parts of how dense a graph\nneeds to be to perform well are not fully explored. In this paper, we propose\nNormalized Information Payload (NIP), a graph scoring function measuring\ninformation transfer on graph, which provides an analysis tool for trade-offs\nbetween performance and complexity. Guided by this theoretical analysis, we\npresent Hypercube Transformer, a sparse Transformer that models token\ninteractions in a hypercube and shows comparable or even better results with\nvanilla Transformer while yielding $O(N\\log N)$ complexity with sequence length\n$N$. Experiments on tasks requiring various sequence lengths lay validation for\nour graph function well.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chu-Tak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qipeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhangyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunhua Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Duplex Conversation: Towards Human-like Interaction in Spoken Dialogue Systems. (arXiv:2205.15060v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.15060","description":"<p>In this paper, we present Duplex Conversation, a multi-turn, multimodal\nspoken dialogue system that enables telephone-based agents to interact with\ncustomers like a human. We use the concept of full-duplex in telecommunication\nto demonstrate what a human-like interactive experience should be and how to\nachieve smooth turn-taking through three subtasks: user state detection,\nbackchannel selection, and barge-in detection. Besides, we propose\nsemi-supervised learning with multimodal data augmentation to leverage\nunlabeled data to increase model generalization. Experimental results on three\nsub-tasks show that the proposed method achieves consistent improvements\ncompared with baselines. We deploy the Duplex Conversation to Alibaba\nintelligent customer service and share lessons learned in production. Online\nA/B experiments show that the proposed system can significantly reduce response\nlatency by 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-En Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation. (arXiv:2206.03354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.03354","description":"<p>Vision-and-language tasks are gaining popularity in the research community,\nbut the focus is still mainly on English. We propose a pipeline that utilizes\nEnglish-only vision-language models to train a monolingual model for a target\nlanguage. We propose to extend OSCAR+, a model which leverages object tags as\nanchor points for learning image-text alignments, to train on visual question\nanswering datasets in different languages. We propose a novel approach to\nknowledge distillation to train the model in other languages using parallel\nsentences. Compared to other models that use the target language in the\npretraining corpora, we can leverage an existing English model to transfer the\nknowledge to the target language using significantly lesser resources. We also\nrelease a large-scale visual question answering dataset in Japanese and Hindi\nlanguage. Though we restrict our work to visual question answering, our model\ncan be extended to any sequence-level classification task, and it can be\nextended to other languages as well. This paper focuses on two languages for\nthe visual question answering task - Japanese and Hindi. Our pipeline\noutperforms the current state-of-the-art models by a relative increase of 4.4%\nand 13.4% respectively in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Prompting Towards Controllable Response Generation. (arXiv:2206.03931v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.03931","description":"<p>Much literature has shown that prompt-based learning is an efficient method\nto make use of the large pre-trained language model. Recent works also exhibit\nthe possibility of steering a chatbot's output by plugging in an appropriate\nprompt. Gradient-based methods are often used to perturb the prompts. However,\nsome language models are not even available to the public. In this work, we\nfirst explored the combination of prompting and reinforcement learning (RL) to\nsteer models' generation without accessing any of the models' parameters.\nSecond, to reduce the training effort and enhance the generalizability to the\nunseen task, we apply multi-task learning to make the model learn to generalize\nto new tasks better. The experiment results show that our proposed method can\nsuccessfully control several state-of-the-art (SOTA) dialogue models without\naccessing their parameters. Furthermore, the model demonstrates the strong\nability to quickly adapt to an unseen task in fewer steps than the baseline\nmodel.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hsuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_P/0/1/0/all/0/1\">Pohan Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shih-Cheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_C/0/1/0/all/0/1\">Chung Ho Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahay_S/0/1/0/all/0/1\">Saurav Sahay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shang-Tse Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"An Improved Deep Convolutional Neural Network by Using Hybrid Optimization Algorithms to Detect and Classify Brain Tumor Using Augmented MRI Images. (arXiv:2206.04056v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04056","description":"<p>Automated brain tumor detection is becoming a highly considerable medical\ndiagnosis research. In recent medical diagnoses, detection and classification\nare highly considered to employ machine learning and deep learning techniques.\nNevertheless, the accuracy and performance of current models need to be\nimproved for suitable treatments. In this paper, an improvement in deep\nconvolutional learning is ensured by adopting enhanced optimization algorithms,\nThus, Deep Convolutional Neural Network (DCNN) based on improved Harris Hawks\nOptimization (HHO), called G-HHO has been considered. This hybridization\nfeatures Grey Wolf Optimization (GWO) and HHO to give better results, limiting\nthe convergence rate and enhancing performance. Moreover, Otsu thresholding is\nadopted to segment the tumor portion that emphasizes brain tumor detection.\nExperimental studies are conducted to validate the performance of the suggested\nmethod on a total number of 2073 augmented MRI images. The technique's\nperformance was ensured by comparing it with the nine existing algorithms on\nhuge augmented MRI images in terms of accuracy, precision, recall, f-measure,\nexecution time, and memory usage. The performance comparison shows that the\nDCNN-G-HHO is much more successful than existing methods, especially on a\nscoring accuracy of 97%. Additionally, the statistical performance analysis\nindicates that the suggested approach is faster and utilizes less memory at\nidentifying and categorizing brain tumor cancers on the MR images. The\nimplementation of this validation is conducted on the Python platform. The\nrelevant codes for the proposed approach are available at:\nhttps://github.com/bryarahassan/DCNN-G-HHO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qader_S/0/1/0/all/0/1\">Shko M. Qader</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_B/0/1/0/all/0/1\">Bryar A. Hassan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1\">Tarik A. Rashid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRHDR: A Dual branch Residual Network for Multi-Bracket High Dynamic Range Imaging. (arXiv:2206.04124v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04124","description":"<p>We introduce DRHDR, a Dual branch Residual Convolutional Neural Network for\nMulti-Bracket HDR Imaging. To address the challenges of fusing multiple\nbrackets from dynamic scenes, we propose an efficient dual branch network that\noperates on two different resolutions. The full resolution branch uses a\nDeformable Convolutional Block to align features and retain high-frequency\ndetails. A low resolution branch with a Spatial Attention Block aims to attend\nwanted areas from the non-reference brackets, and suppress displaced features\nthat could incur on ghosting artifacts. By using a dual branch approach we are\nable to achieve high quality results while constraining the computational\nresources required to estimate the HDR results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marin_Vega_J/0/1/0/all/0/1\">Juan Mar&#xed;n-Vega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sloth_M/0/1/0/all/0/1\">Michael Sloth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_Kamp_P/0/1/0/all/0/1\">Peter Schneider-Kamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_R/0/1/0/all/0/1\">Richard R&#xf6;ttger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Self-supervised and Weight-preserving Neural Architecture Search. (arXiv:2206.04125v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04125","description":"<p>Neural architecture search (NAS) algorithms save tremendous labor from human\nexperts. Recent advancements further reduce the computational overhead to an\naffordable level. However, it is still cumbersome to deploy the NAS techniques\nin real-world applications due to the fussy procedures and the supervised\nlearning paradigm. In this work, we propose the self-supervised and\nweight-preserving neural architecture search (SSWP-NAS) as an extension of the\ncurrent NAS framework by allowing the self-supervision and retaining the\nconcomitant weights discovered during the search stage. As such, we simplify\nthe workflow of NAS to a one-stage and proxy-free procedure. Experiments show\nthat the architectures searched by the proposed framework achieve\nstate-of-the-art accuracy on CIFAR-10, CIFAR-100, and ImageNet datasets without\nusing manual labels. Moreover, we show that employing the concomitant weights\nas initialization consistently outperforms the random initialization and the\ntwo-stage weight pre-training method by a clear margin under semi-supervised\nlearning scenarios. Codes are publicly available at\nhttps://github.com/LzVv123456/SSWP-NAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuowei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yibo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zhenzhou Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HU_Z/0/1/0/all/0/1\">Zhiqiang HU</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Q/0/1/0/all/0/1\">Qing Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Receding Moving Object Segmentation in 3D LiDAR Data Using Sparse 4D Convolutions. (arXiv:2206.04129v1 [cs.RO])","link":"http://arxiv.org/abs/2206.04129","description":"<p>A key challenge for autonomous vehicles is to navigate in unseen dynamic\nenvironments. Separating moving objects from static ones is essential for\nnavigation, pose estimation, and understanding how other traffic participants\nare likely to move in the near future. In this work, we tackle the problem of\ndistinguishing 3D LiDAR points that belong to currently moving objects, like\nwalking pedestrians or driving cars, from points that are obtained from\nnon-moving objects, like walls but also parked cars. Our approach takes a\nsequence of observed LiDAR scans and turns them into a voxelized sparse 4D\npoint cloud. We apply computationally efficient sparse 4D convolutions to\njointly extract spatial and temporal features and predict moving object\nconfidence scores for all points in the sequence. We develop a receding horizon\nstrategy that allows us to predict moving objects online and to refine\npredictions on the go based on new observations. We use a binary Bayes filter\nto recursively integrate new predictions of a scan resulting in more robust\nestimation. We evaluate our approach on the SemanticKITTI moving object\nsegmentation challenge and show more accurate predictions than existing\nmethods. Since our approach only operates on the geometric information of point\nclouds over time, it generalizes well to new, unseen environments, which we\nevaluate on the Apollo dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mersch_B/0/1/0/all/0/1\">Benedikt Mersch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xieyuanli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vizzo_I/0/1/0/all/0/1\">Ignacio Vizzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nunes_L/0/1/0/all/0/1\">Lucas Nunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behley_J/0/1/0/all/0/1\">Jens Behley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1\">Cyrill Stachniss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Estimation of Speckle Statistics Parametric Images. (arXiv:2206.04145v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04145","description":"<p>Quantitative Ultrasound (QUS) provides important information about the tissue\nproperties. QUS parametric image can be formed by dividing the envelope data\ninto small overlapping patches and computing different speckle statistics such\nas parameters of the Nakagami and Homodyned K-distributions (HK-distribution).\nThe calculated QUS parametric images can be erroneous since only a few\nindependent samples are available inside the patches. Another challenge is that\nthe envelope samples inside the patch are assumed to come from the same\ndistribution, an assumption that is often violated given that the tissue is\nusually not homogenous. In this paper, we propose a method based on\nConvolutional Neural Networks (CNN) to estimate QUS parametric images without\npatching. We construct a large dataset sampled from the HK-distribution, having\nregions with random shapes and QUS parameter values. We then use a well-known\nnetwork to estimate QUS parameters in a multi-task learning fashion. Our\nresults confirm that the proposed method is able to reduce errors and improve\nborder definition in QUS parametric images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tehrani_A/0/1/0/all/0/1\">Ali K. Z. Tehrani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosado_Mendez_I/0/1/0/all/0/1\">Ivan M. Rosado-Mendez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rivaz_H/0/1/0/all/0/1\">Hassan Rivaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensembling Framework for Texture Extraction Techniques for Classification. (arXiv:2206.04158v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04158","description":"<p>In the past few years, texture-based classification problems have proven\ntheir significance in many domains, from industrial inspection to\nhealth-related applications. New techniques and CNN-based architectures have\nbeen developed in recent years to solve texture-based classification problems.\nThe limitation of these approaches is that none of them claims to be the best\nsuited for all types of textures. Each technique has its advantage over a\nspecific texture type. To address this issue, we are proposing a framework that\ncombines existing techniques to extract texture features and displays better\nresults than the present ones. The proposed framework works well on the most of\nthe texture types, and in this framework, new techniques can also be added to\nachieve better results than existing ones. We are also presenting the SOTA\nresults on FMD and KTH datasets by combining three existing techniques, using\nthe proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vijay Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gubba_M/0/1/0/all/0/1\">Mayank Gubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faisal_M/0/1/0/all/0/1\">Mohammed Faisal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalra_T/0/1/0/all/0/1\">Trapti Kalra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CASS: Cross Architectural Self-Supervision for Medical Image Analysis. (arXiv:2206.04170v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04170","description":"<p>Recent advances in Deep Learning and Computer Vision have alleviated many of\nthe bottlenecks, allowing algorithms to be label-free with better performance.\nSpecifically, Transformers provide a global perspective of the image, which\nConvolutional Neural Networks (CNN) lack by design. Here we present\n\\textbf{C}ross \\textbf{A}rchitectural - \\textbf{S}elf \\textbf{S}upervision , a\nnovel self-supervised learning approach which leverages transformers and CNN\nsimultaneously, while also being computationally accessible to general\npractitioners via easily available cloud services. Compared to existing\nstate-of-the-art self-supervised learning approaches, we empirically show CASS\ntrained CNNs, and Transformers gained an average of 8.5\\% with 100\\% labelled\ndata, 7.3\\% with 10\\% labelled data, and 11.5\\% with 1\\% labelled data, across\nthree diverse datasets. Notably, one of the employed datasets included\nhistopathology slides of an autoimmune disease, a topic underrepresented in\nMedical Imaging and has minimal data. In addition, our findings reveal that\nCASS is twice as efficient as other state-of-the-art methods in terms of\ntraining time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pranav Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sizikova_E/0/1/0/all/0/1\">Elena Sizikova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cirrone_J/0/1/0/all/0/1\">Jacopo Cirrone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VN-Transformer: Rotation-Equivariant Attention for Vector Neurons. (arXiv:2206.04176v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04176","description":"<p>Rotation equivariance is a desirable property in many practical applications\nsuch as motion forecasting and 3D perception, where it can offer benefits like\nsample efficiency, better generalization, and robustness to input\nperturbations. Vector Neurons (VN) is a recently developed framework offering a\nsimple yet effective approach for deriving rotation-equivariant analogs of\nstandard machine learning operations by extending one-dimensional scalar\nneurons to three-dimensional \"vector neurons.\" We introduce a novel\n\"VN-Transformer\" architecture to address several shortcomings of the current VN\nmodels. Our contributions are: $(i)$ we derive a rotation-equivariant attention\nmechanism which eliminates the need for the heavy feature preprocessing\nrequired by the original Vector Neurons models; $(ii)$ we extend the VN\nframework to support non-spatial attributes, expanding the applicability of\nthese models to real-world datasets; $(iii)$ we derive a rotation-equivariant\nmechanism for multi-scale reduction of point-cloud resolution, greatly speeding\nup inference and training; $(iv)$ we show that small tradeoffs in equivariance\n($\\epsilon$-approximate equivariance) can be used to obtain large improvements\nin numerical stability and training robustness on accelerated hardware, and we\nbound the propagation of equivariance violations in our models. Finally, we\napply our VN-Transformer to 3D shape classification and motion forecasting with\ncompelling results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Assaad_S/0/1/0/all/0/1\">Serge Assaad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_C/0/1/0/all/0/1\">Carlton Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayakanti_N/0/1/0/all/0/1\">Nigamaa Nayakanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1\">Ben Sapp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCAMPS: Synthetics for Camera Measurement of Physiological Signals. (arXiv:2206.04197v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04197","description":"<p>The use of cameras and computational algorithms for noninvasive, low-cost and\nscalable measurement of physiological (e.g., cardiac and pulmonary) vital signs\nis very attractive. However, diverse data representing a range of environments,\nbody motions, illumination conditions and physiological states is laborious,\ntime consuming and expensive to obtain. Synthetic data have proven a valuable\ntool in several areas of machine learning, yet are not widely available for\ncamera measurement of physiological states. Synthetic data offer \"perfect\"\nlabels (e.g., without noise and with precise synchronization), labels that may\nnot be possible to obtain otherwise (e.g., precise pixel level segmentation\nmaps) and provide a high degree of control over variation and diversity in the\ndataset. We present SCAMPS, a dataset of synthetics containing 2,800 videos\n(1.68M frames) with aligned cardiac and respiratory signals and facial action\nintensities. The RGB frames are provided alongside segmentation maps. We\nprovide precise descriptive statistics about the underlying waveforms,\nincluding inter-beat interval, heart rate variability, and pulse arrival time.\nFinally, we present baseline results training on these synthetic data and\ntesting on real-world datasets to illustrate generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wander_M/0/1/0/all/0/1\">Miah Wander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_B/0/1/0/all/0/1\">Brian L. Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_J/0/1/0/all/0/1\">Javier Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lester_J/0/1/0/all/0/1\">Jonathan Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltrusaitis_T/0/1/0/all/0/1\">Tadas Baltrusaitis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JNMR: Joint Non-linear Motion Regression for Video Frame Interpolation. (arXiv:2206.04231v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04231","description":"<p>Video frame interpolation (VFI) aims to generate predictive frames by warping\nlearnable motions from the bidirectional historical references. Most existing\nworks utilize spatio-temporal semantic information extractor to realize motion\nestimation and interpolation modeling, not enough considering with the real\nmechanistic rationality of generated middle motions. In this paper, we\nreformulate VFI as a multi-variable non-linear (MNL) regression problem, and a\nJoint Non-linear Motion Regression (JNMR) strategy is proposed to model\ncomplicated motions of inter-frame. To establish the MNL regression, ConvLSTM\nis adopted to construct the distribution of complete motions in temporal\ndimension. The motion correlations between the target frame and multiple\nreference frames can be regressed by the modeled distribution. Moreover, the\nfeature learning network is designed to optimize for the MNL regression\nmodeling. A coarse-to-fine synthesis enhancement module is further conducted to\nlearn visual dynamics at different resolutions through repetitive regression\nand interpolation. Highly competitive experimental results on frame\ninterpolation show that the effectiveness and significant improvement compared\nwith state-of-the-art performance, and the robustness of complicated motion\nestimation is improved by the MNL motion regression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Meiqin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1\">Chao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cardiac Adipose Tissue Segmentation via Image-Level Annotations. (arXiv:2206.04238v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04238","description":"<p>Automatically identifying the structural substrates underlying cardiac\nabnormalities can potentially provide real-time guidance for interventional\nprocedures. With the knowledge of cardiac tissue substrates, the treatment of\ncomplex arrhythmias such as atrial fibrillation and ventricular tachycardia can\nbe further optimized by detecting arrhythmia substrates to target for treatment\n(i.e., adipose) and identifying critical structures to avoid. Optical coherence\ntomography (OCT) is a real-time imaging modality that aids in addressing this\nneed. Existing approaches for cardiac image analysis mainly rely on fully\nsupervised learning techniques, which suffer from the drawback of workload on\nlabor-intensive annotation process of pixel-wise labeling. To lessen the need\nfor pixel-wise labeling, we develop a two-stage deep learning framework for\ncardiac adipose tissue segmentation using image-level annotations on OCT images\nof human cardiac substrates. In particular, we integrate class activation\nmapping with superpixel segmentation to solve the sparse tissue seed challenge\nraised in cardiac tissue segmentation. Our study bridges the gap between the\ndemand on automatic tissue analysis and the lack of high-quality pixel-wise\nannotations. To the best of our knowledge, this is the first study that\nattempts to address cardiac tissue segmentation on OCT images via weakly\nsupervised learning techniques. Within an in-vitro human cardiac OCT dataset,\nwe demonstrate that our weakly supervised approach on image-level annotations\nachieves comparable performance as fully supervised methods trained on\npixel-wise annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1\">Ziyi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_Y/0/1/0/all/0/1\">Yu Gan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lye_T/0/1/0/all/0/1\">Theresa Lye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haofeng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Laine_A/0/1/0/all/0/1\">Andrew Laine</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Angelini_E/0/1/0/all/0/1\">Elsa Angelini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hendon_C/0/1/0/all/0/1\">Christine Hendon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OOD Augmentation May Be at Odds with Open-Set Recognition. (arXiv:2206.04242v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04242","description":"<p>Despite advances in image classification methods, detecting the samples not\nbelonging to the training classes is still a challenging problem. There has\nbeen a burst of interest in this subject recently, which is called Open-Set\nRecognition (OSR). In OSR, the goal is to achieve both the classification and\ndetecting out-of-distribution (OOD) samples. Several ideas have been proposed\nto push the empirical result further through complicated techniques. We believe\nthat such complication is indeed not necessary. To this end, we have shown that\nMaximum Softmax Probability (MSP), as the simplest baseline for OSR, applied on\nVision Transformers (ViTs) as the base classifier that is trained with non-OOD\naugmentations can surprisingly outperform many recent methods. Non-OOD\naugmentations are the ones that do not alter the data distribution by much. Our\nresults outperform state-of-the-art in CIFAR-10 datasets, and is also better\nthan most of the current methods in SVHN and MNIST. We show that training\naugmentation has a significant effect on the performance of ViTs in the OSR\ntasks, and while they should produce significant diversity in the augmented\nsamples, the generated sample OOD-ness must remain limited.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Azizmalayeri_M/0/1/0/all/0/1\">Mohammad Azizmalayeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SwinCheX: Multi-label classification on chest X-ray images with transformers. (arXiv:2206.04246v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04246","description":"<p>According to the considerable growth in the avail of chest X-ray images in\ndiagnosing various diseases, as well as gathering extensive datasets, having an\nautomated diagnosis procedure using deep neural networks has occupied the minds\nof experts. Most of the available methods in computer vision use a CNN backbone\nto acquire high accuracy on the classification problems. Nevertheless, recent\nresearches show that transformers, established as the de facto method in NLP,\ncan also outperform many CNN-based models in vision. This paper proposes a\nmulti-label classification deep model based on the Swin Transformer as the\nbackbone to achieve state-of-the-art diagnosis classification. It leverages\nMulti-Layer Perceptron, also known as MLP, for the head architecture. We\nevaluate our model on one of the most widely-used and largest x-ray datasets\ncalled \"Chest X-ray14,\" which comprises more than 100,000 frontal/back-view\nimages from over 30,000 patients with 14 famous chest diseases. Our model has\nbeen tested with several number of MLP layers for the head setting, each\nachieves a competitive AUC score on all classes. Comprehensive experiments on\nChest X-ray14 have shown that a 3-layer head attains state-of-the-art\nperformance with an average AUC score of 0.810, compared to the former SOTA\naverage AUC of 0.799. We propose an experimental setup for the fair\nbenchmarking of existing methods, which could be used as a basis for the future\nstudies. Finally, we followed up our results by confirming that the proposed\nmethod attends to the pathologically relevant areas of the chest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taslimi_S/0/1/0/all/0/1\">Sina Taslimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taslimi_S/0/1/0/all/0/1\">Soroush Taslimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fathi_N/0/1/0/all/0/1\">Nima Fathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepVerge: Classification of Roadside Verge Biodiversity and Conservation Potential. (arXiv:2206.04271v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04271","description":"<p>Open space grassland is being increasingly farmed or built upon, leading to a\nramping up of conservation efforts targeting roadside verges. Approximately\nhalf of all UK grassland species can be found along the country's 500,000 km of\nroads, with some 91 species either threatened or near threatened. Careful\nmanagement of these \"wildlife corridors\" is therefore essential to preventing\nspecies extinction and maintaining biodiversity in grassland habitats. Wildlife\ntrusts have often enlisted the support of volunteers to survey roadside verges\nand identify new \"Local Wildlife Sites\" as areas of high conservation\npotential. Using volunteer survey data from 3,900 km of roadside verges\nalongside publicly available street-view imagery, we present DeepVerge; a deep\nlearning-based method that can automatically survey sections of roadside verges\nby detecting the presence of positive indicator species. Using images and\nground truth survey data from the rural county of Lincolnshire, DeepVerge\nachieved a mean accuracy of 88%. Such a method may be used by local authorities\nto identify new local wildlife sites, and aid management and environmental\nplanning in line with legal and government policy obligations, saving thousands\nof hours of manual labour.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perrett_A/0/1/0/all/0/1\">Andrew Perrett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_C/0/1/0/all/0/1\">Charlie Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schofield_M/0/1/0/all/0/1\">Mark Schofield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qie_L/0/1/0/all/0/1\">Lan Qie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosilj_P/0/1/0/all/0/1\">Petra Bosilj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1\">James M. Brown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STEM image analysis based on deep learning: identification of vacancy defects and polymorphs of ${MoS_2}$. (arXiv:2206.04272v1 [cond-mat.mes-hall])","link":"http://arxiv.org/abs/2206.04272","description":"<p>Scanning transmission electron microscopy (STEM) is an indispensable tool for\natomic-resolution structural analysis for a wide range of materials. The\nconventional analysis of STEM images is an extensive hands-on process, which\nlimits efficient handling of high-throughput data. Here we apply a fully\nconvolutional network (FCN) for identification of important structural features\nof two-dimensional crystals. ResUNet, a type of FCN, is utilized in identifying\nsulfur vacancies and polymorph types of ${MoS_2}$ from atomic resolution STEM\nimages. Efficient models are achieved based on training with simulated images\nin the presence of different levels of noise, aberrations, and carbon\ncontamination. The accuracy of the FCN models toward extensive experimental\nSTEM images is comparable to that of careful hands-on analysis. Our work\nprovides a guideline on best practices to train a deep learning model for STEM\nimage analysis and demonstrates FCN's application for efficient processing of a\nlarge volume of STEM data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Lee_K/0/1/0/all/0/1\">Kihyun Lee</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Park_J/0/1/0/all/0/1\">Jinsub Park</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Choi_S/0/1/0/all/0/1\">Soyeon Choi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lee_Y/0/1/0/all/0/1\">Yangjin Lee</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lee_S/0/1/0/all/0/1\">Sol Lee</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jung_J/0/1/0/all/0/1\">Joowon Jung</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lee_J/0/1/0/all/0/1\">Jong-Young Lee</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ullah_F/0/1/0/all/0/1\">Farman Ullah</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Tahir_Z/0/1/0/all/0/1\">Zeeshan Tahir</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kim_Y/0/1/0/all/0/1\">Yong Soo Kim</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lee_G/0/1/0/all/0/1\">Gwan-Hyoung Lee</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kim_K/0/1/0/all/0/1\">Kwanpyo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis. (arXiv:2206.04281v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04281","description":"<p>Recent self-supervised advances in medical computer vision exploit global and\nlocal anatomical self-similarity for pretraining prior to downstream tasks such\nas segmentation. However, current methods assume i.i.d. image acquisition,\nwhich is invalid in clinical study designs where follow-up longitudinal scans\ntrack subject-specific temporal changes. Further, existing self-supervised\nmethods for medically-relevant image-to-image architectures exploit only\nspatial or temporal self-similarity and only do so via a loss applied at a\nsingle image-scale, with naive multi-scale spatiotemporal extensions collapsing\nto degenerate solutions. To these ends, this paper makes two contributions: (1)\nIt presents a local and multi-scale spatiotemporal representation learning\nmethod for image-to-image architectures trained on longitudinal images. It\nexploits the spatiotemporal self-similarity of learned multi-scale\nintra-subject features for pretraining and develops several feature-wise\nregularizations that avoid collapsed identity representations; (2) During\nfinetuning, it proposes a surprisingly simple self-supervised segmentation\nconsistency regularization to exploit intra-subject correlation. Benchmarked in\nthe one-shot segmentation setting, the proposed framework outperforms both\nwell-tuned randomly-initialized baselines and current self-supervised\ntechniques designed for both i.i.d. and longitudinal datasets. These\nimprovements are demonstrated across both longitudinal neurodegenerative adult\nMRI and developing infant brain MRI and yield both higher performance and\nlongitudinal consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1\">Neel Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Styner_M/0/1/0/all/0/1\">Martin A. Styner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Botteron_K/0/1/0/all/0/1\">Kelly Botteron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1\">Guido Gerig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A No-Reference Deep Learning Quality Assessment Method for Super-resolution Images Based on Frequency Maps. (arXiv:2206.04289v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04289","description":"<p>To support the application scenarios where high-resolution (HR) images are\nurgently needed, various single image super-resolution (SISR) algorithms are\ndeveloped. However, SISR is an ill-posed inverse problem, which may bring\nartifacts like texture shift, blur, etc. to the reconstructed images, thus it\nis necessary to evaluate the quality of super-resolution images (SRIs). Note\nthat most existing image quality assessment (IQA) methods were developed for\nsynthetically distorted images, which may not work for SRIs since their\ndistortions are more diverse and complicated. Therefore, in this paper, we\npropose a no-reference deep-learning image quality assessment method based on\nfrequency maps because the artifacts caused by SISR algorithms are quite\nsensitive to frequency information. Specifically, we first obtain the\nhigh-frequency map (HM) and low-frequency map (LM) of SRI by using Sobel\noperator and piecewise smooth image approximation. Then, a two-stream network\nis employed to extract the quality-aware features of both frequency maps.\nFinally, the features are regressed into a single quality value using fully\nconnected layers. The experimental results show that our method outperforms all\ncompared IQA models on the selected three super-resolution quality assessment\n(SRQA) databases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhan Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstruct Face from Features Using GAN Generator as a Distribution Constraint. (arXiv:2206.04295v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04295","description":"<p>Face recognition based on the deep convolutional neural networks (CNN) shows\nsuperior accuracy performance attributed to the high discriminative features\nextracted. Yet, the security and privacy of the extracted features from deep\nlearning models (deep features) have been often overlooked. This paper proposes\nthe reconstruction of face images from deep features without accessing the CNN\nnetwork configurations as a constrained optimization problem. Such optimization\nminimizes the distance between the features extracted from the original face\nimage and the reconstructed face image. Instead of directly solving the\noptimization problem in the image space, we innovatively reformulate the\nproblem by looking for a latent vector of a GAN generator, then use it to\ngenerate the face image. The GAN generator serves as a dual role in this novel\nframework, i.e., face distribution constraint of the optimization goal and a\nface generator. On top of the novel optimization task, we also propose an\nattack pipeline to impersonate the target user based on the generated face\nimage. Our results show that the generated face images can achieve a\nstate-of-the-art successful attack rate of 98.0\\% on LFW under type-I attack @\nFAR of 0.1\\%. Our work sheds light on the biometric deployment to meet the\nprivacy-preserving and security policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xingbo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhihui Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiajun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhe Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenhua Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teoh_A/0/1/0/all/0/1\">Andrew Beng Jin Teoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GSmooth: Certified Robustness against Semantic Transformations via Generalized Randomized Smoothing. (arXiv:2206.04310v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04310","description":"<p>Certified defenses such as randomized smoothing have shown promise towards\nbuilding reliable machine learning systems against $\\ell_p$-norm bounded\nattacks. However, existing methods are insufficient or unable to provably\ndefend against semantic transformations, especially those without closed-form\nexpressions (such as defocus blur and pixelate), which are more common in\npractice and often unrestricted. To fill up this gap, we propose generalized\nrandomized smoothing (GSmooth), a unified theoretical framework for certifying\nrobustness against general semantic transformations via a novel dimension\naugmentation strategy. Under the GSmooth framework, we present a scalable\nalgorithm that uses a surrogate image-to-image network to approximate the\ncomplex transformation. The surrogate model provides a powerful tool for\nstudying the properties of semantic transformations and certifying robustness.\nExperimental results on several datasets demonstrate the effectiveness of our\napproach for robustness certification against multiple kinds of semantic\ntransformations and corruptions, which is not achievable by the alternative\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1\">Zhongkai Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengyang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jian Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Surveillance Image Quality Assessment via Deep Neural Network Combined with the Visual Saliency. (arXiv:2206.04318v1 [cs.MM])","link":"http://arxiv.org/abs/2206.04318","description":"<p>The intelligent video surveillance system (IVSS) can automatically analyze\nthe content of the surveillance image (SI) and reduce the burden of the manual\nlabour. However, the SIs may suffer quality degradations in the procedure of\nacquisition, compression, and transmission, which makes IVSS hard to understand\nthe content of SIs. In this paper, we first conduct an example experiment (i.e.\nthe face detection task) to demonstrate that the quality of the SIs has a\ncrucial impact on the performance of the IVSS, and then propose a\nsaliency-based deep neural network for the blind quality assessment of the SIs,\nwhich helps IVSS to filter the low-quality SIs and improve the detection and\nrecognition performance. Specifically, we first compute the saliency map of the\nSI to select the most salient local region since the salient regions usually\ncontain rich semantic information for machine vision and thus have a great\nimpact on the overall quality of the SIs. Next, the convolutional neural\nnetwork (CNN) is adopted to extract quality-aware features for the whole image\nand local region, which are then mapped into the global and local quality\nscores through the fully connected (FC) network respectively. Finally, the\noverall quality score is computed as the weighted sum of the global and local\nquality scores. Experimental results on the SI quality database (SIQD) show\nthat the proposed method outperforms all compared state-of-the-art BIQA\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CFA: Coupled-hypersphere-based Feature Adaptation for Target-Oriented Anomaly Localization. (arXiv:2206.04325v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04325","description":"<p>For a long time, anomaly localization has been widely used in industries.\nPrevious studies focused on approximating the distribution of normal features\nwithout adaptation to a target dataset. However, since anomaly localization\nshould precisely discriminate normal and abnormal features, the absence of\nadaptation may make the normality of abnormal features overestimated. Thus, we\npropose Coupled-hypersphere-based Feature Adaptation (CFA) which accomplishes\nsophisticated anomaly localization using features adapted to the target\ndataset. CFA consists of (1) a learnable patch descriptor that learns and\nembeds target-oriented features and (2) scalable memory bank independent of the\nsize of the target dataset. And, CFA adopts transfer learning to increase the\nnormal feature density so that abnormal features can be clearly distinguished\nby applying patch descriptor and memory bank to a pre-trained CNN. The proposed\nmethod outperforms the previous methods quantitatively and qualitatively. For\nexample, it provides an AUROC score of 99.5% in anomaly detection and 98.5% in\nanomaly localization of MVTec AD benchmark. In addition, this paper points out\nthe negative effects of biased features of pre-trained CNNs and emphasizes the\nimportance of the adaptation to the target dataset. The code is publicly\navailable at https://github.com/sungwool/CFA_for_anomaly_localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungwook Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Byung Cheol Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel projection schemes for graph-based Light Field coding. (arXiv:2206.04328v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04328","description":"<p>In Light Field compression, graph-based coding is powerful to exploit signal\nredundancy along irregular shapes and obtains good energy compaction. However,\napart from high time complexity to process high dimensional graphs, their graph\nconstruction method is highly sensitive to the accuracy of disparity\ninformation between viewpoints. In real world Light Field or synthetic Light\nField generated by computer software, the use of disparity information for\nsuper-rays projection might suffer from inaccuracy due to vignetting effect and\nlarge disparity between views in the two types of Light Fields respectively.\nThis paper introduces two novel projection schemes resulting in less error in\ndisparity information, in which one projection scheme can also significantly\nreduce time computation for both encoder and decoder. Experimental results show\nprojection quality of super-pixels across views can be considerably enhanced\nusing the proposals, along with rate-distortion performance when compared\nagainst original projection scheme and HEVC-based or JPEG Pleno-based coding\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_B/0/1/0/all/0/1\">Bach Gia Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_C/0/1/0/all/0/1\">Chanh Minh Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duc_T/0/1/0/all/0/1\">Tho Nguyen Duc</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_T/0/1/0/all/0/1\">Tan Xuan Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eiji_K/0/1/0/all/0/1\">Kamioka Eiji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Modeling of Image and Label Statistics for Enhancing Model Generalizability of Medical Image Segmentation. (arXiv:2206.04336v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04336","description":"<p>Although supervised deep-learning has achieved promising performance in\nmedical image segmentation, many methods cannot generalize well on unseen data,\nlimiting their real-world applicability. To address this problem, we propose a\ndeep learning-based Bayesian framework, which jointly models image and label\nstatistics, utilizing the domain-irrelevant contour of a medical image for\nsegmentation. Specifically, we first decompose an image into components of\ncontour and basis. Then, we model the expected label as a variable only related\nto the contour. Finally, we develop a variational Bayesian framework to infer\nthe posterior distributions of these variables, including the contour, the\nbasis, and the label. The framework is implemented with neural networks, thus\nis referred to as deep Bayesian segmentation. Results on the task of\ncross-sequence cardiac MRI segmentation show that our method set a new state of\nthe art for model generalizability. Particularly, the BayeSeg model trained\nwith LGE MRI generalized well on T2 images and outperformed other models with\ngreat margins, i.e., over 0.47 in terms of average Dice. Our code is available\nat https://zmiclab.github.io/projects.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1\">Shangqi Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hangqi Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1\">Yibo Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_X/0/1/0/all/0/1\">Xiahai Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Asynchronous Events Encode Video. (arXiv:2206.04341v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04341","description":"<p>As event-based sensing gains in popularity, theoretical understanding is\nneeded to harness this technology's potential. Instead of recording video by\ncapturing frames, event-based cameras have sensors that emit events when their\ninputs change, thus encoding information in the timing of events. This creates\nnew challenges in establishing reconstruction guarantees and algorithms, but\nalso provides advantages over frame-based video. We use time encoding machines\nto model event-based sensors: TEMs also encode their inputs by emitting events\ncharacterized by their timing and reconstruction from time encodings is well\nunderstood. We consider the case of time encoding bandlimited video and\ndemonstrate a dependence between spatial sensor density and overall spatial and\ntemporal resolution. Such a dependence does not occur in frame-based video,\nwhere temporal resolution depends solely on the frame rate of the video and\nspatial resolution depends solely on the pixel grid. However, this dependence\narises naturally in event-based video and allows oversampling in space to\nprovide better time resolution. As such, event-based vision encourages using\nmore sensors that emit fewer events over time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Adam_K/0/1/0/all/0/1\">Karen Adam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Scholefield_A/0/1/0/all/0/1\">Adam Scholefield</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vetterli_M/0/1/0/all/0/1\">Martin Vetterli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep radiomic signature with immune cell markers predicts the survival of glioma patients. (arXiv:2206.04349v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04349","description":"<p>Imaging biomarkers offer a non-invasive way to predict the response of\nimmunotherapy prior to treatment. In this work, we propose a novel type of deep\nradiomic features (DRFs) computed from a convolutional neural network (CNN),\nwhich capture tumor characteristics related to immune cell markers and overall\nsurvival. Our study uses four MRI sequences (T1-weighted, T1-weighted\npost-contrast, T2-weighted and FLAIR) with corresponding immune cell markers of\n151 patients with brain tumor. The proposed method extracts a total of 180 DRFs\nby aggregating the activation maps of a pre-trained 3D-CNN within labeled tumor\nregions of MRI scans. These features offer a compact, yet powerful\nrepresentation of regional texture encoding tissue heterogeneity. A\ncomprehensive set of experiments is performed to assess the relationship\nbetween the proposed DRFs and immune cell markers, and measure their\nassociation with overall survival. Results show a high correlation between DRFs\nand various markers, as well as significant differences between patients\ngrouped based on these markers. Moreover, combining DRFs, clinical features and\nimmune cell markers as input to a random forest classifier helps discriminate\nbetween short and long survival outcomes, with AUC of 72\\% and\np=2.36$\\times$10$^{-5}$. These results demonstrate the usefulness of proposed\nDRFs as non-invasive biomarker for predicting treatment response in patients\nwith brain tumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaddad_A/0/1/0/all/0/1\">Ahmad Chaddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Paul Daniel Mingli Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathore_S/0/1/0/all/0/1\">Saima Rathore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sargos_P/0/1/0/all/0/1\">Paul Sargos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desrosiers_C/0/1/0/all/0/1\">Christian Desrosiers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niazi_T/0/1/0/all/0/1\">Tamim Niazi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Network for Blind Visual Quality Assessment of 4K Content. (arXiv:2206.04363v1 [cs.MM])","link":"http://arxiv.org/abs/2206.04363","description":"<p>The 4K content can deliver a more immersive visual experience to consumers\ndue to the huge improvement of spatial resolution. However, existing blind\nimage quality assessment (BIQA) methods are not suitable for the original and\nupscaled 4K contents due to the expanded resolution and specific distortions.\nIn this paper, we propose a deep learning-based BIQA model for 4K content,\nwhich on one hand can recognize true and pseudo 4K content and on the other\nhand can evaluate their perceptual visual quality. Considering the\ncharacteristic that high spatial resolution can represent more abundant\nhigh-frequency information, we first propose a Grey-level Co-occurrence Matrix\n(GLCM) based texture complexity measure to select three representative image\npatches from a 4K image, which can reduce the computational complexity and is\nproven to be very effective for the overall quality prediction through\nexperiments. Then we extract different kinds of visual features from the\nintermediate layers of the convolutional neural network (CNN) and integrate\nthem into the quality-aware feature representation. Finally, two multilayer\nperception (MLP) networks are utilized to map the quality-aware features into\nthe class probability and the quality score for each patch respectively. The\noverall quality index is obtained through the average pooling of patch results.\nThe proposed model is trained through the multi-task learning manner and we\nintroduce an uncertainty principle to balance the losses of the classification\nand regression tasks. The experimental results show that the proposed model\noutperforms all compared BIQA metrics on four 4K content quality assessment\ndatabases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenhan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Quan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zicheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CARLA-GeAR: a Dataset Generator for a Systematic Evaluation of Adversarial Robustness of Vision Models. (arXiv:2206.04365v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04365","description":"<p>Adversarial examples represent a serious threat for deep neural networks in\nseveral application domains and a huge amount of work has been produced to\ninvestigate them and mitigate their effects. Nevertheless, no much work has\nbeen devoted to the generation of datasets specifically designed to evaluate\nthe adversarial robustness of neural models. This paper presents CARLA-GeAR, a\ntool for the automatic generation of photo-realistic synthetic datasets that\ncan be used for a systematic evaluation of the adversarial robustness of neural\nmodels against physical adversarial patches, as well as for comparing the\nperformance of different adversarial defense/detection methods. The tool is\nbuilt on the CARLA simulator, using its Python API, and allows the generation\nof datasets for several vision tasks in the context of autonomous driving. The\nadversarial patches included in the generated datasets are attached to\nbillboards or the back of a truck and are crafted by using state-of-the-art\nwhite-box attack strategies to maximize the prediction error of the model under\ntest. Finally, the paper presents an experimental study to evaluate the\nperformance of some defense methods against such attacks, showing how the\ndatasets generated with CARLA-GeAR might be used in future work as a benchmark\nfor adversarial defense in the real world. All the code and datasets used in\nthis paper are available at <a href=\"http://carlagear.retis.santannapisa.it.\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nesti_F/0/1/0/all/0/1\">Federico Nesti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossolini_G/0/1/0/all/0/1\">Giulio Rossolini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmico_G/0/1/0/all/0/1\">Gianluca D&#x27;Amico</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biondi_A/0/1/0/all/0/1\">Alessandro Biondi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buttazzo_G/0/1/0/all/0/1\">Giorgio Buttazzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering bias in the PlantVillage dataset. (arXiv:2206.04374v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04374","description":"<p>We report our investigation on the use of the popular PlantVillage dataset\nfor training deep learning based plant disease detection models. We trained a\nmachine learning model using only 8 pixels from the PlantVillage image\nbackgrounds. The model achieved 49.0% accuracy on the held-out test set, well\nabove the random guessing accuracy of 2.6%. This result indicates that the\nPlantVillage dataset contains noise correlated with the labels and deep\nlearning models can easily exploit this bias to make predictions. Possible\napproaches to alleviate this problem are discussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noyan_M/0/1/0/all/0/1\">Mehmet Alican Noyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STIP: A SpatioTemporal Information-Preserving and Perception-Augmented Model for High-Resolution Video Prediction. (arXiv:2206.04381v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04381","description":"<p>Although significant achievements have been achieved by recurrent neural\nnetwork (RNN) based video prediction methods, their performance in datasets\nwith high resolutions is still far from satisfactory because of the information\nloss problem and the perception-insensitive mean square error (MSE) based loss\nfunctions. In this paper, we propose a Spatiotemporal Information-Preserving\nand Perception-Augmented Model (STIP) to solve the above two problems. To solve\nthe information loss problem, the proposed model aims to preserve the\nspatiotemporal information for videos during the feature extraction and the\nstate transitions, respectively. Firstly, a Multi-Grained Spatiotemporal\nAuto-Encoder (MGST-AE) is designed based on the X-Net structure. The proposed\nMGST-AE can help the decoders recall multi-grained information from the\nencoders in both the temporal and spatial domains. In this way, more\nspatiotemporal information can be preserved during the feature extraction for\nhigh-resolution videos. Secondly, a Spatiotemporal Gated Recurrent Unit (STGRU)\nis designed based on the standard Gated Recurrent Unit (GRU) structure, which\ncan efficiently preserve spatiotemporal information during the state\ntransitions. The proposed STGRU can achieve more satisfactory performance with\na much lower computation load compared with the popular Long Short-Term (LSTM)\nbased predictive memories. Furthermore, to improve the traditional MSE loss\nfunctions, a Learned Perceptual Loss (LP-loss) is further designed based on the\nGenerative Adversarial Networks (GANs), which can help obtain a satisfactory\ntrade-off between the objective quality and the perceptual quality.\nExperimental results show that the proposed STIP can predict videos with more\nsatisfactory visual quality compared with a variety of state-of-the-art\nmethods. Source code has been available at\n\\url{https://github.com/ZhengChang467/STIPHR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1\">Zheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLIP-Actor: Text-Driven Recommendation and Stylization for Animating Human Meshes. (arXiv:2206.04382v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04382","description":"<p>We propose CLIP-Actor, a text-driven motion recommendation and neural mesh\nstylization system for human mesh animation. CLIP-Actor animates a 3D human\nmesh to conform to a text prompt by recommending a motion sequence and learning\nmesh style attributes. Prior work fails to generate plausible results when the\nartist-designed mesh content does not conform to the text from the beginning.\nInstead, we build a text-driven human motion recommendation system by\nleveraging a large-scale human motion dataset with language labels. Given a\nnatural language prompt, CLIP-Actor first suggests a human motion that conforms\nto the prompt in a coarse-to-fine manner. Then, we propose a\nsynthesize-through-optimization method that detailizes and texturizes a\nrecommended mesh sequence in a disentangled way from the pose of each frame. It\nallows the style attribute to conform to the prompt in a temporally-consistent\nand pose-agnostic manner. The decoupled neural optimization also enables\nspatio-temporal view augmentation from multi-frame human motion. We further\npropose the mask-weighted embedding attention, which stabilizes the\noptimization process by rejecting distracting renders containing scarce\nforeground pixels. We demonstrate that CLIP-Actor produces plausible and\nhuman-recognizable style 3D human mesh in motion with detailed geometry and\ntexture from a natural language prompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Youwang_K/0/1/0/all/0/1\">Kim Youwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Yeon_K/0/1/0/all/0/1\">Kim Ji-Yeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depression Recognition using Remote Photoplethysmography from Facial Videos. (arXiv:2206.04399v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04399","description":"<p>Depression is a mental illness that may be harmful to an individual's health.\nThe detection of mental health disorders in the early stages and a precise\ndiagnosis are critical to avoid social, physiological, or psychological side\neffects. This work analyzes physiological signals to observe if different\ndepressive states have a noticeable impact on the blood volume pulse (BVP) and\nthe heart rate variability (HRV) response. Although typically, HRV features are\ncalculated from biosignals obtained with contact-based sensors such as\nwearables, we propose instead a novel scheme that directly extracts them from\nfacial videos, just based on visual information, removing the need for any\ncontact-based device. Our solution is based on a pipeline that is able to\nextract complete remote photoplethysmography signals (rPPG) in a fully\nunsupervised manner. We use these rPPG signals to calculate over 60\nstatistical, geometrical, and physiological features that are further used to\ntrain several machine learning regressors to recognize different levels of\ndepression. Experiments on two benchmark datasets indicate that this approach\noffers comparable results to other audiovisual modalities based on voice or\nfacial expression, potentially complementing them. In addition, the results\nachieved for the proposed method show promising and solid performance that\noutperforms hand-engineered methods and is comparable to deep learning-based\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casado_C/0/1/0/all/0/1\">Constantino &#xc1;lvarez Casado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canellas_M/0/1/0/all/0/1\">Manuel Lage Ca&#xf1;ellas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_M/0/1/0/all/0/1\">Miguel Bordallo L&#xf3;pez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-modal Local Shortest Path and Global Enhancement for Visible-Thermal Person Re-Identification. (arXiv:2206.04401v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04401","description":"<p>In addition to considering the recognition difficulty caused by human posture\nand occlusion, it is also necessary to solve the modal differences caused by\ndifferent imaging systems in the Visible-Thermal cross-modal person\nre-identification (VT-ReID) task. In this paper,we propose the Cross-modal\nLocal Shortest Path and Global Enhancement (CM-LSP-GE) modules,a two-stream\nnetwork based on joint learning of local and global features. The core idea of\nour paper is to use local feature alignment to solve occlusion problem, and to\nsolve modal difference by strengthening global feature. Firstly,\nAttention-based two-stream ResNet network is designed to extract dual-modality\nfeatures and map to a unified feature space. Then, to solve the cross-modal\nperson pose and occlusion problems, the image are cut horizontally into several\nequal parts to obtain local features and the shortest path in local features\nbetween two graphs is used to achieve the fine-grained local feature alignment.\nThirdly, a batch normalization enhancement module applies global features to\nenhance strategy, resulting in difference enhancement between different\nclasses. The multi granularity loss fusion strategy further improves the\nperformance of the algorithm. Finally, joint learning mechanism of local and\nglobal features is used to improve cross-modal person re-identification\naccuracy. The experimental results on two typical datasets show that our model\nis obviously superior to the most state-of-the-art methods. Especially, on\nSYSU-MM01 datasets, our model can achieve a gain of 2.89%and 7.96% in all\nsearch term of Rank-1 and mAP. The source code will be released soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chaoqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiangcai Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VITA: Video Instance Segmentation via Object Token Association. (arXiv:2206.04403v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04403","description":"<p>We introduce a novel paradigm for offline Video Instance Segmentation (VIS),\nbased on the hypothesis that explicit object-oriented information can be a\nstrong clue for understanding the context of the entire sequence. To this end,\nwe propose VITA, a simple structure built on top of an off-the-shelf\nTransformer-based image instance segmentation model. Specifically, we use an\nimage object detector as a means of distilling object-specific contexts into\nobject tokens. VITA accomplishes video-level understanding by associating\nframe-level object tokens without using spatio-temporal backbone features. By\neffectively building relationships between objects using the condensed\ninformation, VITA achieves the state-of-the-art on VIS benchmarks with a\nResNet-50 backbone: 49.8 AP, 45.7 AP on YouTube-VIS 2019 &amp; 2021 and 19.6 AP on\nOVIS. Moreover, thanks to its object token-based structure that is disjoint\nfrom the backbone features, VITA shows several practical advantages that\nprevious offline VIS methods have not explored - handling long and\nhigh-resolution videos with a common GPU and freezing a frame-level detector\ntrained on image domain. Code will be made available at\nhttps://github.com/sukjunhwang/VITA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_M/0/1/0/all/0/1\">Miran Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sukjun Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seoung Wug Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joon-Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seon Joo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of the Total Variation Flow. (arXiv:2206.04406v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04406","description":"<p>The total variation (TV) flow generates a scale-space representation of an\nimage based on the TV functional. This gradient flow observes desirable\nfeatures for images such as sharp edges and enables spectral, scale, and\ntexture analysis. The standard numerical approach for TV flow requires solving\nmultiple non-smooth optimisation problems. Even with state-of-the-art convex\noptimisation techniques, this is often prohibitively expensive and strongly\nmotivates the use of alternative, faster approaches. Inspired by and extending\nthe framework of physics-informed neural networks (PINNs), we propose the\nTVflowNET, a neural network approach to compute the solution of the TV flow\ngiven an initial image and a time instance. We significantly speed up the\ncomputation time by more than one order of magnitude and show that the\nTVflowNET approximates the TV flow solution with high fidelity. This is a\npreliminary report, more details are to follow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grossmann_T/0/1/0/all/0/1\">Tamara G. Grossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittmer_S/0/1/0/all/0/1\">S&#xf6;ren Dittmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korolev_Y/0/1/0/all/0/1\">Yury Korolev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Instance Learning for Digital Pathology: A Review on the State-of-the-Art, Limitations & Future Potential. (arXiv:2206.04425v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04425","description":"<p>Digital whole slides images contain an enormous amount of information\nproviding a strong motivation for the development of automated image analysis\ntools. Particularly deep neural networks show high potential with respect to\nvarious tasks in the field of digital pathology. However, a limitation is given\nby the fact that typical deep learning algorithms require (manual) annotations\nin addition to the large amounts of image data, to enable effective training.\nMultiple instance learning exhibits a powerful tool for learning deep neural\nnetworks in a scenario without fully annotated data. These methods are\nparticularly effective in this domain, due to the fact that labels for a\ncomplete whole slide image are often captured routinely, whereas labels for\npatches, regions or pixels are not. This potential already resulted in a\nconsiderable number of publications, with the majority published in the last\nthree years. Besides the availability of data and a high motivation from the\nmedical perspective, the availability of powerful graphics processing units\nexhibits an accelerator in this field. In this paper, we provide an overview of\nwidely and effectively used concepts of used deep multiple instance learning\napproaches, recent advances and also critically discuss remaining challenges\nand future potential.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gadermayr_M/0/1/0/all/0/1\">Michael Gadermayr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschuchnig_M/0/1/0/all/0/1\">Maximilian Tschuchnig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-boosting of WNNM Image Denoising method by Directional Wavelet Packets. (arXiv:2206.04431v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04431","description":"<p>The paper presents an image denoising scheme by combining a method that is\nbased on directional quasi-analytic wavelet packets (qWPs) with the\nstate-of-the-art Weighted Nuclear Norm Minimization (WNNM) denoising algorithm.\nThe qWP-based denoising method (qWPdn) consists of multiscale qWP transform of\nthe degraded image, application of adaptive localized soft thresholding to the\ntransform coefficients using the Bivariate Shrinkage methodology, and\nrestoration of the image from the thresholded coefficients from several\ndecomposition levels. The combined method consists of several iterations of\nqWPdn and WNNM algorithms in a way that at each iteration the output from one\nalgorithm boosts the input to the other. The proposed methodology couples the\nqWPdn capabilities to capture edges and fine texture patterns even in the\nseverely corrupted images with utilizing the non-local self-similarity in real\nimages that is inherent in the WNNM algorithm.\n</p>\n<p>Multiple experiments, which compared the proposed methodology with six\nadvanced denoising algorithms, including WNNM, confirmed that the combined\ncross-boosting algorithm outperforms most of them in terms of both quantitative\nmeasure and visual perception quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Averbuch_A/0/1/0/all/0/1\">Amir Averbuch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neittaanmaki_P/0/1/0/all/0/1\">Pekka Neittaanm&#xe4;ki</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheludev_V/0/1/0/all/0/1\">Valery Zheludev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salhov_M/0/1/0/all/0/1\">Moshe Salhov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hauser_J/0/1/0/all/0/1\">Jonathan Hauser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmentation Enhanced Lameness Detection in Dairy Cows from RGB and Depth Video. (arXiv:2206.04449v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04449","description":"<p>Cow lameness is a severe condition that affects the life cycle and life\nquality of dairy cows and results in considerable economic losses. Early\nlameness detection helps farmers address illnesses early and avoid negative\neffects caused by the degeneration of cows' condition. We collected a dataset\nof short clips of cows passing through a hallway exiting a milking station and\nannotated the degree of lameness of the cows. This paper explores the resulting\ndataset and provides a detailed description of the data collection process.\nAdditionally, we proposed a lameness detection method that leverages\npre-trained neural networks to extract discriminative features from videos and\nassign a binary score to each cow indicating its condition: \"healthy\" or\n\"lame.\" We improve this approach by forcing the model to focus on the structure\nof the cow, which we achieve by substituting the RGB videos with binary\nsegmentation masks predicted with a trained segmentation model. This work aims\nto encourage research and provide insights into the applicability of computer\nvision models for cow lameness detection on farms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arazo_E/0/1/0/all/0/1\">Eric Arazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aly_R/0/1/0/all/0/1\">Robin Aly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Draft-and-Revise: Effective Image Generation with Contextual RQ-Transformer. (arXiv:2206.04452v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04452","description":"<p>Although autoregressive models have achieved promising results on image\ngeneration, their unidirectional generation process prevents the resultant\nimages from fully reflecting global contexts. To address the issue, we propose\nan effective image generation framework of Draft-and-Revise with Contextual\nRQ-transformer to consider global contexts during the generation process. As a\ngeneralized VQ-VAE, RQ-VAE first represents a high-resolution image as a\nsequence of discrete code stacks. After code stacks in the sequence are\nrandomly masked, Contextual RQ-Transformer is trained to infill the masked code\nstacks based on the unmasked contexts of the image. Then, Contextual\nRQ-Transformer uses our two-phase decoding, Draft-and-Revise, and generates an\nimage, while exploiting the global contexts of the image during the generation\nprocess. Specifically. in the draft phase, our model first focuses on\ngenerating diverse images despite rather low quality. Then, in the revise\nphase, the model iteratively improves the quality of images, while preserving\nthe global contexts of generated images. In experiments, our method achieves\nstate-of-the-art results on conditional image generation. We also validate that\nthe Draft-and-Revise decoding can achieve high performance by effectively\ncontrolling the quality-diversity trade-off in image generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Doyup Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chiheon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Saehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wook-Shin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Missing Link: Finding label relations across datasets. (arXiv:2206.04453v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04453","description":"<p>Computer Vision is driven by the many datasets which can be used for training\nor evaluating novel methods. However, each dataset has different set of class\nlabels, visual definition of classes, images following a specific distribution,\nannotation protocols, etc. In this paper we explore the automatic discovery of\nvisual-semantic relations between labels across datasets. We want to understand\nhow the instances of a certain class in a dataset relate to the instances of\nanother class in another dataset. Are they in an identity, parent/child,\noverlap relation? Or is there no link between them at all? To find relations\nbetween labels across datasets, we propose methods based on language, on\nvision, and on a combination of both. Our methods can effectively discover\nlabel relations across datasets and the type of the relations. We use these\nresults for a deeper inspection on why instances relate, find missing aspects\nof a class, and use our relations to create finer-grained annotations. We\nconclude that label relations cannot be established by looking at the names of\nclasses alone, as they depend strongly on how each of the datasets was\nconstructed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uijlings_J/0/1/0/all/0/1\">Jasper Uijlings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensink_T/0/1/0/all/0/1\">Thomas Mensink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDQ: Stochastic Differentiable Quantization with Mixed Precision. (arXiv:2206.04459v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04459","description":"<p>In order to deploy deep models in a computationally efficient manner, model\nquantization approaches have been frequently used. In addition, as new hardware\nthat supports mixed bitwidth arithmetic operations, recent research on mixed\nprecision quantization (MPQ) begins to fully leverage the capacity of\nrepresentation by searching optimized bitwidths for different layers and\nmodules in a network. However, previous studies mainly search the MPQ strategy\nin a costly scheme using reinforcement learning, neural architecture search,\netc., or simply utilize partial prior knowledge for bitwidth assignment, which\nmight be biased and sub-optimal. In this work, we present a novel Stochastic\nDifferentiable Quantization (SDQ) method that can automatically learn the MPQ\nstrategy in a more flexible and globally-optimized space with smoother gradient\napproximation. Particularly, Differentiable Bitwidth Parameters (DBPs) are\nemployed as the probability factors in stochastic quantization between adjacent\nbitwidth choices. After the optimal MPQ strategy is acquired, we further train\nour network with entropy-aware bin regularization and knowledge distillation.\nWe extensively evaluate our method for several networks on different hardware\n(GPUs and FPGA) and datasets. SDQ outperforms all state-of-the-art mixed or\nsingle precision quantization with a lower bitwidth and is even better than the\nfull-precision counterparts across various ResNet and MobileNet families,\ndemonstrating the effectiveness and superiority of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shichao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xianghong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wicaksana_J/0/1/0/all/0/1\">Jeffry Wicaksana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BSM loss: A superior way in modeling aleatory uncertainty of fine_grained classification. (arXiv:2206.04479v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04479","description":"<p>Artificial intelligence(AI)-assisted method had received much attention in\nthe risk field such as disease diagnosis. Different from the classification of\ndisease types, it is a fine-grained task to classify the medical images as\nbenign or malignant. However, most research only focuses on improving the\ndiagnostic accuracy and ignores the evaluation of model reliability, which\nlimits its clinical application. For clinical practice, calibration presents\nmajor challenges in the low-data regime extremely for over-parametrized models\nand inherent noises. In particular, we discovered that modeling data-dependent\nuncertainty is more conducive to confidence calibrations. Compared with\ntest-time augmentation(TTA), we proposed a modified Bootstrapping loss(BS loss)\nfunction with Mixup data augmentation strategy that can better calibrate\npredictive uncertainty and capture data distribution transformation without\nadditional inference time. Our experiments indicated that BS loss with\nMixup(BSM) model can halve the Expected Calibration Error(ECE) compared to\nstandard data augmentation, deep ensemble and MC dropout. The correlation\nbetween uncertainty and similarity of in-domain data is up to -0.4428 under the\nBSM model. Additionally, the BSM model is able to perceive the semantic\ndistance of out-of-domain data, demonstrating high potential in real-world\nclinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shuang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1\">Kehong Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Maokun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Desheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qiongyu Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cycle text2face: cycle text-to-face gan via transformers. (arXiv:2206.04503v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04503","description":"<p>Text-to-face is a subset of text-to-image that require more complex\narchitecture due to their more detailed production. In this paper, we present\nan encoder-decoder model called Cycle Text2Face. Cycle Text2Face is a new\ninitiative in the encoder part, it uses a sentence transformer and GAN to\ngenerate the image described by the text. The Cycle is completed by reproducing\nthe text of the face in the decoder part of the model. Evaluating the model\nusing the CelebA dataset, leads to better results than previous GAN-based\nmodels. In measuring the quality of the generate face, in addition to\nsatisfying the human audience, we obtain an FID score of 3.458. This model,\nwith high-speed processing, provides quality face images in the short time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gholamrezaie_F/0/1/0/all/0/1\">Faezeh Gholamrezaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manthouri_M/0/1/0/all/0/1\">Mohammad Manthouri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Human Pose Estimation via 3D Event Point Cloud. (arXiv:2206.04511v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04511","description":"<p>Human Pose Estimation (HPE) based on RGB images has experienced a rapid\ndevelopment benefiting from deep learning. However, event-based HPE has not\nbeen fully studied, which remains great potential for applications in extreme\nscenes and efficiency-critical conditions. In this paper, we are the first to\nestimate 2D human pose directly from 3D event point cloud. We propose a novel\nrepresentation of events, the rasterized event point cloud, aggregating events\non the same position of a small time slice. It maintains the 3D features from\nmultiple statistical cues and significantly reduces memory consumption and\ncomputation complexity, proved to be efficient in our work. We then leverage\nthe rasterized event point cloud as input to three different backbones,\nPointNet, DGCNN, and Point Transformer, with two linear layer decoders to\npredict the location of human keypoints. We find that based on our method,\nPointNet achieves promising results with much faster speed, whereas Point\nTransfomer reaches much higher accuracy, even close to previous\nevent-frame-based methods. A comprehensive set of results demonstrates that our\nproposed method is consistently effective for these 3D backbone models in\nevent-driven human pose estimation. Our method based on PointNet with 2048\npoints input achieves 82.46mm in MPJPE3D on the DHP19 dataset, while only has a\nlatency of 12.29ms on an NVIDIA Jetson Xavier NX edge computing platform, which\nis ideally suitable for real-time detection with event cameras. Code will be\nmade publicly at https://github.com/MasterHow/EventPointPose.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hao Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yaozu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kaiwei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR Despeckling using a Denoising Diffusion Probabilistic Model. (arXiv:2206.04514v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04514","description":"<p>Speckle is a multiplicative noise which affects all coherent imaging\nmodalities including Synthetic Aperture Radar (SAR) images. The presence of\nspeckle degrades the image quality and adversely affects the performance of SAR\nimage understanding applications such as automatic target recognition and\nchange detection. Thus, SAR despeckling is an important problem in remote\nsensing. In this paper, we introduce SAR-DDPM, a denoising diffusion\nprobabilistic model for SAR despeckling. The proposed method comprises of a\nMarkov chain that transforms clean images to white Gaussian noise by repeatedly\nadding random noise. The despeckled image is recovered by a reverse process\nwhich iteratively predicts the added noise using a noise predictor which is\nconditioned on the speckled image. In addition, we propose a new inference\nstrategy based on cycle spinning to improve the despeckling performance. Our\nexperiments on both synthetic and real SAR images demonstrate that the proposed\nmethod achieves significant improvements in both quantitative and qualitative\nresults over the state-of-the-art despeckling methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Perera_M/0/1/0/all/0/1\">Malsha V. Perera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nair_N/0/1/0/all/0/1\">Nithin Gopalakrishnan Nair</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face-Dubbing++: Lip-Synchronous, Voice Preserving Translation of Videos. (arXiv:2206.04523v1 [cs.CL])","link":"http://arxiv.org/abs/2206.04523","description":"<p>In this paper, we propose a neural end-to-end system for voice preserving,\nlip-synchronous translation of videos. The system is designed to combine\nmultiple component models and produces a video of the original speaker speaking\nin the target language that is lip-synchronous with the target speech, yet\nmaintains emphases in speech, voice characteristics, face video of the original\nspeaker. The pipeline starts with automatic speech recognition including\nemphasis detection, followed by a translation model. The translated text is\nthen synthesized by a Text-to-Speech model that recreates the original emphases\nmapped from the original sentence. The resulting synthetic voice is then mapped\nback to the original speakers' voice using a voice conversion model. Finally,\nto synchronize the lips of the speaker with the translated audio, a conditional\ngenerative adversarial network-based model generates frames of adapted lip\nmovements with respect to the input face image as well as the output of the\nvoice conversion model. In the end, the system combines the generated video\nwith the converted audio to produce the final output. The result is a video of\na speaker speaking in another language without actually knowing it. To evaluate\nour design, we present a user study of the complete system as well as separate\nevaluations of the single components. Since there is no available dataset to\nevaluate our whole system, we collect a test set and evaluate our system on\nthis test set. The results indicate that our system is able to generate\nconvincing videos of the original speaker speaking the target language while\npreserving the original speaker's characteristics. The collected dataset will\nbe shared.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behr_M/0/1/0/all/0/1\">Moritz Behr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eyiokur_F/0/1/0/all/0/1\">Fevziye Irem Eyiokur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaman_D/0/1/0/all/0/1\">Dogucan Yaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Nam Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mullov_C/0/1/0/all/0/1\">Carlos Mullov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demirtas_M/0/1/0/all/0/1\">Mehmet Arif Demirtas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantarci_A/0/1/0/all/0/1\">Alperen Kantarc&#x131;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantin_S/0/1/0/all/0/1\">Stefan Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel_H/0/1/0/all/0/1\">Haz&#x131;m Kemal Ekenel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DORA: Exploring outlier representations in Deep Neural Networks. (arXiv:2206.04530v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04530","description":"<p>Deep Neural Networks (DNNs) draw their power from the representations they\nlearn. In recent years, however, researchers have found that DNNs, while being\nincredibly effective in learning complex abstractions, also tend to be infected\nwith artifacts, such as biases, Clever Hanses (CH), or Backdoors, due to\nspurious correlations inherent in the training data. So far, existing methods\nfor uncovering such artifactual and malicious behavior in trained models focus\non finding artifacts in the input data, which requires both availabilities of a\ndata set and human intervention. In this paper, we introduce DORA\n(Data-agnOstic Representation Analysis): the first automatic data-agnostic\nmethod for the detection of potentially infected representations in Deep Neural\nNetworks. We further show that contaminated representations found by DORA can\nbe used to detect infected samples in any given dataset. We qualitatively and\nquantitatively evaluate the performance of our proposed method in both,\ncontrolled toy scenarios, and in real-world settings, where we demonstrate the\nbenefit of DORA in safety-critical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1\">Kirill Bykov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1\">Mayukh Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1\">Dennis Grinwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1\">Klaus-Robert M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1\">Marina M.-C. H&#xf6;hne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04531","description":"<p>Convolutional neural networks are being increasingly used in critical\nsystems, where ensuring their robustness and alignment is crucial. In this\ncontext, the field of explainable artificial intelligence has proposed the\ngeneration of high-level explanations through concept extraction. These methods\ndetect whether a concept is present in an image, but are incapable of locating\nwhere. What is more, a fair comparison of approaches is difficult, as proper\nvalidation procedures are missing. To fill these gaps, we propose a novel\nmethod for automatic concept extraction and localization based on\nrepresentations obtained through the pixel-wise aggregations of activation maps\nof CNNs. Further, we introduce a process for the validation of\nconcept-extraction techniques based on synthetic datasets with pixel-wise\nannotations of their main components, reducing human intervention. Through\nextensive experimentation on both synthetic and real-world datasets, our method\nachieves better performance in comparison to state-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Posada_Moreno_A/0/1/0/all/0/1\">Andres Felipe Posada-Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nikita Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1\">Sebastian Trimpe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of COVID-19 in Chest X-ray Images Using Fusion of Deep Features and LightGBM. (arXiv:2206.04548v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04548","description":"<p>The COVID-19 disease was first discovered in Wuhan, China, and spread quickly\nworldwide. After the COVID-19 pandemic, many researchers have begun to identify\na way to diagnose the COVID-19 using chest X-ray images. The early diagnosis of\nthis disease can significantly impact the treatment process. In this article,\nwe propose a new technique that is faster and more accurate than the other\nmethods reported in the literature. The proposed method uses a combination of\nDenseNet169 and MobileNet Deep Neural Networks to extract the features of the\npatient's X-ray images. Using the univariate feature selection algorithm, we\nrefined the features for the most important ones. Then we applied the selected\nfeatures as input to the LightGBM (Light Gradient Boosting Machine) algorithm\nfor classification. To assess the effectiveness of the proposed method, the\nChestX-ray8 dataset, which includes 1125 X-ray images of the patient's chest,\nwas used. The proposed method achieved 98.54% and 91.11% accuracies in the\ntwo-class (COVID-19, Healthy) and multi-class (COVID-19, Healthy, Pneumonia)\nclassification problems, respectively. It is worth mentioning that we have used\nGradient-weighted Class Activation Mapping (Grad-CAM) for further analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nasiri_H/0/1/0/all/0/1\">Hamid Nasiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kheyroddin_G/0/1/0/all/0/1\">Ghazal Kheyroddin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dorrigiv_M/0/1/0/all/0/1\">Morteza Dorrigiv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Esmaeili_M/0/1/0/all/0/1\">Mona Esmaeili</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nafchi_A/0/1/0/all/0/1\">Amir Raeisi Nafchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghorbani_M/0/1/0/all/0/1\">Mohsen Haji Ghorbani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zarkesh_Ha_P/0/1/0/all/0/1\">Payman Zarkesh-Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SparseFormer: Attention-based Depth Completion Network. (arXiv:2206.04557v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04557","description":"<p>Most pipelines for Augmented and Virtual Reality estimate the ego-motion of\nthe camera by creating a map of sparse 3D landmarks. In this paper, we tackle\nthe problem of depth completion, that is, densifying this sparse 3D map using\nRGB images as guidance. This remains a challenging problem due to the low\ndensity, non-uniform and outlier-prone 3D landmarks produced by SfM and SLAM\npipelines. We introduce a transformer block, SparseFormer, that fuses 3D\nlandmarks with deep visual features to produce dense depth. The SparseFormer\nhas a global receptive field, making the module especially effective for depth\ncompletion with low-density and non-uniform landmarks. To address the issue of\ndepth outliers among the 3D landmarks, we introduce a trainable refinement\nmodule that filters outliers through attention between the sparse landmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Warburg_F/0/1/0/all/0/1\">Frederik Warburg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamonjisoa_M/0/1/0/all/0/1\">Michael Ramamonjisoa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Antequera_M/0/1/0/all/0/1\">Manuel L&#xf3;pez-Antequera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BFS-Net: Weakly Supervised Cell Instance Segmentation from Bright-Field Microscopy Z-Stacks. (arXiv:2206.04558v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04558","description":"<p>Despite its broad availability, volumetric information acquisition from\nBright-Field Microscopy (BFM) is inherently difficult due to the projective\nnature of the acquisition process. We investigate the prediction of 3D cell\ninstances from a set of BFM Z-Stack images. We propose a novel two-stage weakly\nsupervised method for volumetric instance segmentation of cells which only\nrequires approximate cell centroids annotation. Created pseudo-labels are\nthereby refined with a novel refinement loss with Z-stack guidance. The\nevaluations show that our approach can generalize not only to BFM Z-Stack data,\nbut to other 3D cell imaging modalities. A comparison of our pipeline against\nfully supervised methods indicates that the significant gain in reduced data\ncollection and labelling results in minor performance difference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghani_S/0/1/0/all/0/1\">Shervin Dehghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasseri_A/0/1/0/all/0/1\">Ali Nasseri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformer based Urdu Handwritten Text Optical Character Reader. (arXiv:2206.04575v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04575","description":"<p>Extracting Handwritten text is one of the most important components of\ndigitizing information and making it available for large scale setting.\nHandwriting Optical Character Reader (OCR) is a research problem in computer\nvision and natural language processing computing, and a lot of work has been\ndone for English, but unfortunately, very little work has been done for low\nresourced languages such as Urdu. Urdu language script is very difficult\nbecause of its cursive nature and change of shape of characters based on it's\nrelative position, therefore, a need arises to propose a model which can\nunderstand complex features and generalize it for every kind of handwriting\nstyle. In this work, we propose a transformer based Urdu Handwritten text\nextraction model. As transformers have been very successful in Natural Language\nUnderstanding task, we explore them further to understand complex Urdu\nHandwriting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaiq_M/0/1/0/all/0/1\">Mohammad Daniyal Shaiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheema_M/0/1/0/all/0/1\">Musa Dildar Ahmed Cheema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamal_A/0/1/0/all/0/1\">Ali Kamal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient and Robust 2D-to-BEV Representation Learning via Geometry-guided Kernel Transformer. (arXiv:2206.04584v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04584","description":"<p>Learning Bird's Eye View (BEV) representation from surrounding-view cameras\nis of great importance for autonomous driving. In this work, we propose a\nGeometry-guided Kernel Transformer (GKT), a novel 2D-to-BEV representation\nlearning mechanism. GKT leverages the geometric priors to guide the transformer\nto focus on discriminative regions and unfolds kernel features to generate BEV\nrepresentation. For fast inference, we further introduce a look-up table (LUT)\nindexing method to get rid of the camera's calibrated parameters at runtime.\nGKT can run at $72.3$ FPS on 3090 GPU / $45.6$ FPS on 2080ti GPU and is robust\nto the camera deviation and the predefined BEV height. And GKT achieves the\nstate-of-the-art real-time segmentation results, i.e., 38.0 mIoU\n(100m$\\times$100m perception range at a 0.5m resolution) on the nuScenes val\nset. Given the efficiency, effectiveness, and robustness, GKT has great\npractical values in autopilot scenarios, especially for real-time running\nsystems. Code and models will be available at\n\\url{https://github.com/hustvl/GKT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianheng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinggang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1\">Wenming Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wenyu Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GASP: Gated Attention For Saliency Prediction. (arXiv:2206.04590v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04590","description":"<p>Saliency prediction refers to the computational task of modeling overt\nattention. Social cues greatly influence our attention, consequently altering\nour eye movements and behavior. To emphasize the efficacy of such features, we\npresent a neural model for integrating social cues and weighting their\ninfluences. Our model consists of two stages. During the first stage, we detect\ntwo social cues by following gaze, estimating gaze direction, and recognizing\naffect. These features are then transformed into spatiotemporal maps through\nimage processing operations. The transformed representations are propagated to\nthe second stage (GASP) where we explore various techniques of late fusion for\nintegrating social cues and introduce two sub-networks for directing attention\nto relevant stimuli. Our experiments indicate that fusion approaches achieve\nbetter results for static integration methods, whereas non-fusion approaches\nfor which the influence of each modality is unknown, result in better outcomes\nwhen coupled with recurrent models for dynamic saliency prediction. We show\nthat gaze direction and affective representations contribute a prediction to\nground-truth correspondence improvement of at least 5% compared to dynamic\nsaliency models without social cues. Furthermore, affective representations\nimprove GASP, supporting the necessity of considering affect-biased attention\nin predicting saliency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abawi_F/0/1/0/all/0/1\">Fares Abawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_T/0/1/0/all/0/1\">Tom Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AttX: Attentive Cross-Connections for Fusion of Wearable Signals in Emotion Recognition. (arXiv:2206.04625v1 [cs.LG])","link":"http://arxiv.org/abs/2206.04625","description":"<p>We propose cross-modal attentive connections, a new dynamic and effective\ntechnique for multimodal representation learning from wearable data. Our\nsolution can be integrated into any stage of the pipeline, i.e., after any\nconvolutional layer or block, to create intermediate connections between\nindividual streams responsible for processing each modality. Additionally, our\nmethod benefits from two properties. First, it can share information\nuni-directionally (from one modality to the other) or bi-directionally. Second,\nit can be integrated into multiple stages at the same time to further allow\nnetwork gradients to be exchanged in several touch-points. We perform extensive\nexperiments on three public multimodal wearable datasets, WESAD, SWELL-KW, and\nCASE, and demonstrate that our method can effectively regulate and share\ninformation between different modalities to learn better representations. Our\nexperiments further demonstrate that once integrated into simple CNN-based\nmultimodal solutions (2, 3, or 4 modalities), our method can result in superior\nor competitive performance to state-of-the-art and outperform a variety of\nbaseline uni-modal and classical multimodal methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1\">Anubhav Bhatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behinaein_B/0/1/0/all/0/1\">Behnam Behinaein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hungler_P/0/1/0/all/0/1\">Paul Hungler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1\">Ali Etemad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Entropy Regularization for Vision Transformers. (arXiv:2206.04636v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04636","description":"<p>Recent work has shown that the attention maps of Vision Transformers (VTs),\nwhen trained with self-supervision, can contain a semantic segmentation\nstructure which does not spontaneously emerge when training is supervised. In\nthis paper, we explicitly encourage the emergence of this spatial clustering as\na form of training regularization, this way including a self-supervised pretext\ntask into the standard supervised learning. In more detail, we propose a VT\nregularization method based on a spatial formulation of the information\nentropy. By minimizing the proposed spatial entropy, we explicitly ask the VT\nto produce spatially ordered attention maps, this way including an object-based\nprior during training. Using extensive experiments, we show that the proposed\nregularization approach is beneficial with different training scenarios,\ndatasets, downstream tasks and VT architectures. The code will be available\nupon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peruzzo_E/0/1/0/all/0/1\">Elia Peruzzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yahui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadai_M/0/1/0/all/0/1\">Marco De Nadai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_W/0/1/0/all/0/1\">Wei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VideoINR: Learning Video Implicit Neural Representation for Continuous Space-Time Super-Resolution. (arXiv:2206.04647v1 [eess.IV])","link":"http://arxiv.org/abs/2206.04647","description":"<p>Videos typically record the streaming and continuous visual data as discrete\nconsecutive frames. Since the storage cost is expensive for videos of high\nfidelity, most of them are stored in a relatively low resolution and frame\nrate. Recent works of Space-Time Video Super-Resolution (STVSR) are developed\nto incorporate temporal interpolation and spatial super-resolution in a unified\nframework. However, most of them only support a fixed up-sampling scale, which\nlimits their flexibility and applications. In this work, instead of following\nthe discrete representations, we propose Video Implicit Neural Representation\n(VideoINR), and we show its applications for STVSR. The learned implicit neural\nrepresentation can be decoded to videos of arbitrary spatial resolution and\nframe rate. We show that VideoINR achieves competitive performances with\nstate-of-the-art STVSR methods on common up-sampling scales and significantly\noutperforms prior works on continuous and out-of-training-distribution scales.\nOur project page is at <a href=\"http://zeyuan-chen.com/VideoINR/\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yinbo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingwen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xingqian Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goel_V/0/1/0/all/0/1\">Vidit Goel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Layer-wise Image Vectorization. (arXiv:2206.04655v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04655","description":"<p>Image rasterization is a mature technique in computer graphics, while image\nvectorization, the reverse path of rasterization, remains a major challenge.\nRecent advanced deep learning-based models achieve vectorization and semantic\ninterpolation of vector graphs and demonstrate a better topology of generating\nnew figures. However, deep models cannot be easily generalized to out-of-domain\ntesting data. The generated SVGs also contain complex and redundant shapes that\nare not quite convenient for further editing. Specifically, the crucial\nlayer-wise topology and fundamental semantics in images are still not well\nunderstood and thus not fully explored. In this work, we propose Layer-wise\nImage Vectorization, namely LIVE, to convert raster images to SVGs and\nsimultaneously maintain its image topology. LIVE can generate compact SVG forms\nwith layer-wise structures that are semantically consistent with human\nperspective. We progressively add new bezier paths and optimize these paths\nwith the layer-wise framework, newly designed loss functions, and\ncomponent-wise path initialization technique. Our experiments demonstrate that\nLIVE presents more plausible vectorized forms than prior works and can be\ngeneralized to new images. With the help of this newly learned topology, LIVE\ninitiates human editable SVGs for both designers and other downstream\napplications. Codes are made available at\nhttps://github.com/Picsart-AI-Research/LIVE-Layerwise-Image-Vectorization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuqian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xingqian Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filev_V/0/1/0/all/0/1\">Valerii Filev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_N/0/1/0/all/0/1\">Nikita Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Cues Lead to a Strong Multi-Object Tracker. (arXiv:2206.04656v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04656","description":"<p>For a long time, the most common paradigm in Multi-Object Tracking was\ntracking-by-detection (TbD), where objects are first detected and then\nassociated over video frames. For association, most models resource to motion\nand appearance cues. While still relying on these cues, recent approaches based\non, e.g., attention have shown an ever-increasing need for training data and\noverall complex frameworks. We claim that 1) strong cues can be obtained from\nlittle amounts of training data if some key design choices are applied, 2)\ngiven these strong cues, standard Hungarian matching-based association is\nenough to obtain impressive results. Our main insight is to identify key\ncomponents that allow a standard reidentification network to excel at\nappearance-based tracking. We extensively analyze its failure cases and show\nthat a combination of our appearance features with a simple motion model leads\nto strong tracking results. Our model achieves state-of-the-art performance on\nMOT17 and MOT20 datasets outperforming previous state-of-the-art trackers by up\nto 5.4pp in IDF1 and 4.4pp in HOTA. We will release the code and models after\nthe paper's acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seidenschwarz_J/0/1/0/all/0/1\">Jenny Seidenschwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braso_G/0/1/0/all/0/1\">Guillem Braso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiSparse: Disentangled Sparsification for Multitask Model Compression. (arXiv:2206.04662v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04662","description":"<p>Despite the popularity of Model Compression and Multitask Learning, how to\neffectively compress a multitask model has been less thoroughly analyzed due to\nthe challenging entanglement of tasks in the parameter space. In this paper, we\npropose DiSparse, a simple, effective, and first-of-its-kind multitask pruning\nand sparse training scheme. We consider each task independently by\ndisentangling the importance measurement and take the unanimous decisions among\nall tasks when performing parameter pruning and selection. Our experimental\nresults demonstrate superior performance on various configurations and settings\ncompared to popular sparse training and pruning methods. Besides the\neffectiveness in compression, DiSparse also provides a powerful tool to the\nmultitask learning community. Surprisingly, we even observed better performance\nthan some dedicated multitask learning methods in several cases despite the\nhigh model sparsity enforced by DiSparse. We analyzed the pruning masks\ngenerated with DiSparse and observed strikingly similar sparse network\narchitecture identified by each task even before the training starts. We also\nobserve the existence of a \"watershed\" layer where the task relatedness sharply\ndrops, implying no benefits in continued parameters sharing. Our code and\nmodels will be available at:\nhttps://github.com/SHI-Labs/DiSparse-Multitask-Model-Compression.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xinglong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Data Scaling in Masked Image Modeling. (arXiv:2206.04664v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04664","description":"<p>An important goal of self-supervised learning is to enable model pre-training\nto benefit from almost unlimited data. However, one method that has recently\nbecome popular, namely masked image modeling (MIM), is suspected to be unable\nto benefit from larger data. In this work, we break this misconception through\nextensive experiments, with data scales ranging from 10\\% of ImageNet-1K to\nfull ImageNet-22K, model sizes ranging from 49 million to 1 billion, and\ntraining lengths ranging from 125K iterations to 500K iterations. Our study\nreveals that: (i) Masked image modeling is also demanding on larger data. We\nobserved that very large models got over-fitted with relatively small data;\n(ii) The length of training matters. Large models trained with masked image\nmodeling can benefit from more data with longer training; (iii) The validation\nloss in pre-training is a good indicator to measure how well the model performs\nfor fine-tuning on multiple tasks. This observation allows us to pre-evaluate\npre-trained models in advance without having to make costly trial-and-error\nassessments of downstream tasks. We hope that our findings will advance the\nunderstanding of masked image modeling in terms of scaling ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yutong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AGConv: Adaptive Graph Convolution on 3D Point Clouds. (arXiv:2206.04665v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04665","description":"<p>Convolution on 3D point clouds is widely researched yet far from perfect in\ngeometric deep learning. The traditional wisdom of convolution characterises\nfeature correspondences indistinguishably among 3D points, arising an intrinsic\nlimitation of poor distinctive feature learning. In this paper, we propose\nAdaptive Graph Convolution (AGConv) for wide applications of point cloud\nanalysis. AGConv generates adaptive kernels for points according to their\ndynamically learned features. Compared with the solution of using\nfixed/isotropic kernels, AGConv improves the flexibility of point cloud\nconvolutions, effectively and precisely capturing the diverse relations between\npoints from different semantic parts. Unlike the popular attentional weight\nschemes, AGConv implements the adaptiveness inside the convolution operation\ninstead of simply assigning different weights to the neighboring points.\nExtensive evaluations clearly show that our method outperforms\nstate-of-the-arts of point cloud classification and segmentation on various\nbenchmark datasets.Meanwhile, AGConv can flexibly serve more point cloud\nanalysis approaches to boost their performance. To validate its flexibility and\neffectiveness, we explore AGConv-based paradigms of completion, denoising,\nupsampling, registration and circle extraction, which are comparable or even\nsuperior to their competitors. Our code is available at\nhttps://github.com/hrzhou2/AdaptConv-master.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zeyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Haoran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1\">Fei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_H/0/1/0/all/0/1\">Huajian Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhilei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhe Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jingbo Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xuefeng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extreme Masking for Learning Instance and Distributed Visual Representations. (arXiv:2206.04667v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04667","description":"<p>The paper presents a scalable approach for learning distributed\nrepresentations over individual tokens and a holistic instance representation\nsimultaneously. We use self-attention blocks to represent distributed tokens,\nfollowed by cross-attention blocks to aggregate the holistic instance. The core\nof the approach is the use of extremely large token masking (75%-90%) as the\ndata augmentation for supervision. Our model, named ExtreMA, follows the plain\nBYOL approach where the instance representation from the unmasked subset is\ntrained to predict that from the intact input. Learning requires the model to\ncapture informative variations in an instance, instead of encouraging\ninvariances. The paper makes three contributions: 1) Random masking is a strong\nand computationally efficient data augmentation for learning generalizable\nattention representations. 2) With multiple sampling per instance, extreme\nmasking greatly speeds up learning and hungers for more data. 3) Distributed\nrepresentations can be learned from the instance supervision alone, unlike\nper-token supervisions in masked modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zihang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GateHUB: Gated History Unit with Background Suppression for Online Action Detection. (arXiv:2206.04668v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04668","description":"<p>Online action detection is the task of predicting the action as soon as it\nhappens in a streaming video. A major challenge is that the model does not have\naccess to the future and has to solely rely on the history, i.e., the frames\nobserved so far, to make predictions. It is therefore important to accentuate\nparts of the history that are more informative to the prediction of the current\nframe. We present GateHUB, Gated History Unit with Background Suppression, that\ncomprises a novel position-guided gated cross-attention mechanism to enhance or\nsuppress parts of the history as per how informative they are for current frame\nprediction. GateHUB further proposes Future-augmented History (FaH) to make\nhistory features more informative by using subsequently observed frames when\navailable. In a single unified framework, GateHUB integrates the transformer's\nability of long-range temporal modeling and the recurrent model's capacity to\nselectively encode relevant information. GateHUB also introduces a background\nsuppression objective to further mitigate false positive background frames that\nclosely resemble the action frames. Extensive validation on three benchmark\ndatasets, THUMOS, TVSeries, and HDD, demonstrates that GateHUB significantly\noutperforms all existing methods and is also more efficient than the existing\nbest work. Furthermore, a flow-free version of GateHUB is able to achieve\nhigher or close accuracy at 2.8x higher frame rate compared to all existing\nmethods that require both RGB and optical flow information for prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1\">Gaurav Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Ye Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yu Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mei Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond RGB: Scene-Property Synthesis with Neural Radiance Fields. (arXiv:2206.04669v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04669","description":"<p>Comprehensive 3D scene understanding, both geometrically and semantically, is\nimportant for real-world applications such as robot perception. Most of the\nexisting work has focused on developing data-driven discriminative models for\nscene understanding. This paper provides a new approach to scene understanding,\nfrom a synthesis model perspective, by leveraging the recent progress on\nimplicit 3D representation and neural rendering. Building upon the great\nsuccess of Neural Radiance Fields (NeRFs), we introduce Scene-Property\nSynthesis with NeRF (SS-NeRF) that is able to not only render photo-realistic\nRGB images from novel viewpoints, but also render various accurate scene\nproperties (e.g., appearance, geometry, and semantics). By doing so, we\nfacilitate addressing a variety of scene understanding tasks under a unified\nframework, including semantic segmentation, surface normal estimation,\nreshading, keypoint detection, and edge detection. Our SS-NeRF framework can be\na powerful tool for bridging generative learning and discriminative learning,\nand thus be beneficial to the investigation of a wide range of interesting\nproblems, such as studying task relationships within a synthesis paradigm,\ntransferring knowledge to novel tasks, facilitating downstream discriminative\ntasks as ways of data augmentation, and serving as auto-labeller for data\ncreation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingtong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuhong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1\">Zhipeng Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1\">Martial Hebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointNeXt: Revisiting PointNet++ with Improved Training and Scaling Strategies. (arXiv:2206.04670v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04670","description":"<p>PointNet++ is one of the most influential neural architectures for point\ncloud understanding. Although the accuracy of PointNet++ has been largely\nsurpassed by recent networks such as PointMLP and Point Transformer, we find\nthat a large portion of the performance gain is due to improved training\nstrategies, i.e. data augmentation and optimization techniques, and increased\nmodel sizes rather than architectural innovations. Thus, the full potential of\nPointNet++ has yet to be explored. In this work, we revisit the classical\nPointNet++ through a systematic study of model training and scaling strategies,\nand offer two major contributions. First, we propose a set of improved training\nstrategies that significantly improve PointNet++ performance. For example, we\nshow that, without any change in architecture, the overall accuracy (OA) of\nPointNet++ on ScanObjectNN object classification can be raised from 77.9\\% to\n86.1\\%, even outperforming state-of-the-art PointMLP. Second, we introduce an\ninverted residual bottleneck design and separable MLPs into PointNet++ to\nenable efficient and effective model scaling and propose PointNeXt, the next\nversion of PointNets. PointNeXt can be flexibly scaled up and outperforms\nstate-of-the-art methods on both 3D classification and segmentation tasks. For\nclassification, PointNeXt reaches an overall accuracy of $87.7\\%$ on\nScanObjectNN, surpassing PointMLP by $2.3\\%$, while being $10 \\times$ faster in\ninference. For semantic segmentation, PointNeXt establishes a new\nstate-of-the-art performance with $74.9\\%$ mean IoU on S3DIS (6-fold\ncross-validation), being superior to the recent Point Transformer. The code and\nmodels are available at https://github.com/guochengqian/pointnext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1\">Guocheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuchen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mai_J/0/1/0/all/0/1\">Jinjie Mai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Challenges in Deep Stereo: the Booster Dataset. (arXiv:2206.04671v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04671","description":"<p>We present a novel high-resolution and challenging stereo dataset framing\nindoor scenes annotated with dense and accurate ground-truth disparities.\nPeculiar to our dataset is the presence of several specular and transparent\nsurfaces, i.e. the main causes of failures for state-of-the-art stereo\nnetworks. Our acquisition pipeline leverages a novel deep space-time stereo\nframework which allows for easy and accurate labeling with sub-pixel precision.\nWe release a total of 419 samples collected in 64 different scenes and\nannotated with dense ground-truth disparities. Each sample include a\nhigh-resolution pair (12 Mpx) as well as an unbalanced pair (Left: 12 Mpx,\nRight: 1.1 Mpx). Additionally, we provide manually annotated material\nsegmentation masks and 15K unlabeled samples. We evaluate state-of-the-art deep\nnetworks based on our dataset, highlighting their limitations in addressing the\nopen challenges in stereo and drawing hints for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_P/0/1/0/all/0/1\">Pierluigi Zama Ramirez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosi_F/0/1/0/all/0/1\">Fabio Tosi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_M/0/1/0/all/0/1\">Matteo Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salti_S/0/1/0/all/0/1\">Samuele Salti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattoccia_S/0/1/0/all/0/1\">Stefano Mattoccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefano_L/0/1/0/all/0/1\">Luigi Di Stefano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Prompt Search. (arXiv:2206.04673v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04673","description":"<p>The size of vision models has grown exponentially over the last few years,\nespecially after the emergence of Vision Transformer. This has motivated the\ndevelopment of parameter-efficient tuning methods, such as learning adapter\nlayers or visual prompt tokens, which allow a tiny portion of model parameters\nto be trained whereas the vast majority obtained from pre-training are frozen.\nHowever, designing a proper tuning method is non-trivial: one might need to try\nout a lengthy list of design choices, not to mention that each downstream\ndataset often requires custom designs. In this paper, we view the existing\nparameter-efficient tuning methods as \"prompt modules\" and propose Neural\nprOmpt seArcH (NOAH), a novel approach that learns, for large vision models,\nthe optimal design of prompt modules through a neural architecture search\nalgorithm, specifically for each downstream dataset. By conducting extensive\nexperiments on over 20 vision datasets, we demonstrate that NOAH (i) is\nsuperior to individual prompt modules, (ii) has a good few-shot learning\nability, and (iii) is domain-generalizable. The code and models are available\nat https://github.com/Davidzhangyuanhan/NOAH.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs. (arXiv:2206.04674v1 [cs.CV])","link":"http://arxiv.org/abs/2206.04674","description":"<p>To build an artificial neural network like the biological intelligence\nsystem, recent works have unified numerous tasks into a generalist model, which\ncan process various tasks with shared parameters and do not have any\ntask-specific modules. While generalist models achieve promising results on\nvarious benchmarks, they have performance degradation on some tasks compared\nwith task-specialized models. In this work, we find that interference among\ndifferent tasks and modalities is the main factor to this phenomenon. To\nmitigate such interference, we introduce the Conditional Mixture-of-Experts\n(Conditional MoEs) to generalist models. Routing strategies under different\nlevels of conditions are proposed to take both the training/inference cost and\ngeneralization ability into account. By incorporating the proposed Conditional\nMoEs, the recently proposed generalist model Uni-Perceiver can effectively\nmitigate the interference across tasks and modalities, and achieves\nstate-of-the-art results on a series of downstream tasks via prompt tuning on\n1% of downstream data. Moreover, the introduction of Conditional MoEs still\nholds the generalization ability of generalist models to conduct zero-shot\ninference on new tasks, e.g., video-text retrieval and video caption. Code and\npre-trained generalist models shall be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jinguo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indoor Depth Completion with Boundary Consistency and Self-Attention. (arXiv:1908.08344v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1908.08344","description":"<p>Depth estimation features are helpful for 3D recognition. Commodity-grade\ndepth cameras are able to capture depth and color image in real-time. However,\nglossy, transparent or distant surface cannot be scanned properly by the\nsensor. As a result, enhancement and restoration from sensing depth is an\nimportant task. Depth completion aims at filling the holes that sensors fail to\ndetect, which is still a complex task for machine to learn. Traditional\nhand-tuned methods have reached their limits, while neural network based\nmethods tend to copy and interpolate the output from surrounding depth values.\nThis leads to blurred boundaries, and structures of the depth map are lost.\nConsequently, our main work is to design an end-to-end network improving\ncompletion depth maps while maintaining edge clarity. We utilize self-attention\nmechanism, previously used in image inpainting fields, to extract more useful\ninformation in each layer of convolution so that the complete depth map is\nenhanced. In addition, we propose boundary consistency concept to enhance the\ndepth map quality and structure. Experimental results validate the\neffectiveness of our self-attention and boundary consistency schema, which\noutperforms previous state-of-the-art depth completion work on Matterport3D\ndataset. Our code is publicly available at\nhttps://github.com/tsunghan-wu/Depth-Completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blacklight: Scalable Defense for Neural Networks against Query-Based Black-Box Attacks. (arXiv:2006.14042v3 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2006.14042","description":"<p>Deep learning systems are known to be vulnerable to adversarial examples. In\nparticular, query-based black-box attacks do not require knowledge of the deep\nlearning model, but can compute adversarial examples over the network by\nsubmitting queries and inspecting returns. Recent work largely improves the\nefficiency of those attacks, demonstrating their practicality on today's\nML-as-a-service platforms.\n</p>\n<p>We propose Blacklight, a new defense against query-based black-box\nadversarial attacks. The fundamental insight driving our design is that, to\ncompute adversarial examples, these attacks perform iterative optimization over\nthe network, producing image queries highly similar in the input space.\nBlacklight detects query-based black-box attacks by detecting highly similar\nqueries, using an efficient similarity engine operating on probabilistic\ncontent fingerprints. We evaluate Blacklight against eight state-of-the-art\nattacks, across a variety of models and image classification tasks. Blacklight\nidentifies them all, often after only a handful of queries. By rejecting all\ndetected queries, Blacklight prevents any attack to complete, even when\nattackers persist to submit queries after account ban or query rejection.\nBlacklight is also robust against several powerful countermeasures, including\nan optimal black-box attack that approximates white-box attacks in efficiency.\nFinally, we illustrate how Blacklight generalizes to other domains like text\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shawn Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenger_E/0/1/0/all/0/1\">Emily Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haitao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Ben Y. Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Mask Self-Supervised Learning for Physics-Guided Neural Networks in Highly Accelerated MRI. (arXiv:2008.06029v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2008.06029","description":"<p>Self-supervised learning has shown great promise due to its capability to\ntrain deep learning MRI reconstruction methods without fully-sampled data.\nCurrent self-supervised learning methods for physics-guided reconstruction\nnetworks split acquired undersampled data into two disjoint sets, where one is\nused for data consistency (DC) in the unrolled network and the other to define\nthe training loss. In this study, we propose an improved self-supervised\nlearning strategy that more efficiently uses the acquired data to train a\nphysics-guided reconstruction network without a database of fully-sampled data.\nThe proposed multi-mask self-supervised learning via data undersampling (SSDU)\napplies a hold-out masking operation on acquired measurements to split it into\nmultiple pairs of disjoint sets for each training sample, while using one of\nthese pairs for DC units and the other for defining loss, thereby more\nefficiently using the undersampled data. Multi-mask SSDU is applied on\nfully-sampled 3D knee and prospectively undersampled 3D brain MRI datasets, for\nvarious acceleration rates and patterns, and compared to CG-SENSE and\nsingle-mask SSDU DL-MRI, as well as supervised DL-MRI when fully-sampled data\nis available. Results on knee MRI show that the proposed multi-mask SSDU\noutperforms SSDU and performs closely with supervised DL-MRI. A clinical reader\nstudy further ranks the multi-mask SSDU higher than supervised DL-MRI in terms\nof SNR and aliasing artifacts. Results on brain MRI show that multi-mask SSDU\nachieves better reconstruction quality compared to SSDU. Reader study\ndemonstrates that multi-mask SSDU at R=8 significantly improves reconstruction\ncompared to single-mask SSDU at R=8, as well as CG-SENSE at R=2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1\">Burhaneddin Yaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_H/0/1/0/all/0/1\">Hongyi Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_S/0/1/0/all/0/1\">Seyed Amir Hossein Hosseini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demirel_O/0/1/0/all/0/1\">Omer Burak Demirel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moeller_S/0/1/0/all/0/1\">Steen Moeller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ellermann_J/0/1/0/all/0/1\">Jutta Ellermann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ugurbil_K/0/1/0/all/0/1\">K&#xe2;mil U&#x11f;urbil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akcakaya_M/0/1/0/all/0/1\">Mehmet Ak&#xe7;akaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Diffusion Implicit Models. (arXiv:2010.02502v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.02502","description":"<p>Denoising diffusion probabilistic models (DDPMs) have achieved high quality\nimage generation without adversarial training, yet they require simulating a\nMarkov chain for many steps to produce a sample. To accelerate sampling, we\npresent denoising diffusion implicit models (DDIMs), a more efficient class of\niterative implicit probabilistic models with the same training procedure as\nDDPMs. In DDPMs, the generative process is defined as the reverse of a\nMarkovian diffusion process. We construct a class of non-Markovian diffusion\nprocesses that lead to the same training objective, but whose reverse process\ncan be much faster to sample from. We empirically demonstrate that DDIMs can\nproduce high quality samples $10 \\times$ to $50 \\times$ faster in terms of\nwall-clock time compared to DDPMs, allow us to trade off computation for sample\nquality, and can perform semantically meaningful image interpolation directly\nin the latent space.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Clinical Decision Support Systems in Medical Imaging using Cycle-Consistent Activation Maximization. (arXiv:2010.05759v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.05759","description":"<p>Clinical decision support using deep neural networks has become a topic of\nsteadily growing interest. While recent work has repeatedly demonstrated that\ndeep learning offers major advantages for medical image classification over\ntraditional methods, clinicians are often hesitant to adopt the technology\nbecause its underlying decision-making process is considered to be\nintransparent and difficult to comprehend. In recent years, this has been\naddressed by a variety of approaches that have successfully contributed to\nproviding deeper insight. Most notably, additive feature attribution methods\nare able to propagate decisions back into the input space by creating a\nsaliency map which allows the practitioner to \"see what the network sees.\"\nHowever, the quality of the generated maps can become poor and the images noisy\nif only limited data is available - a typical scenario in clinical contexts. We\npropose a novel decision explanation scheme based on CycleGAN activation\nmaximization which generates high-quality visualizations of classifier\ndecisions even in smaller data sets. We conducted a user study in which we\nevaluated our method on the LIDC dataset for lung lesion malignancy\nclassification, the BreastMNIST dataset for ultrasound image breast cancer\ndetection, as well as two subsets of the CIFAR-10 dataset for RBG image object\nrecognition. Within this user study, our method clearly outperformed existing\napproaches on the medical imaging datasets and ranked second in the natural\nimage setting. With our approach we make a significant contribution towards a\nbetter understanding of clinical decision support systems based on deep neural\nnetworks and thus aim to foster overall clinical acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Katzmann_A/0/1/0/all/0/1\">Alexander Katzmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taubmann_O/0/1/0/all/0/1\">Oliver Taubmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahmad_S/0/1/0/all/0/1\">Stephen Ahmad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Muhlberg_A/0/1/0/all/0/1\">Alexander M&#xfc;hlberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suhling_M/0/1/0/all/0/1\">Michael S&#xfc;hling</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gross_H/0/1/0/all/0/1\">Horst-Michael Gro&#xdf;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DUT: Learning Video Stabilization by Simply Watching Unstable Videos. (arXiv:2011.14574v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14574","description":"<p>Previous deep learning-based video stabilizers require a large scale of\npaired unstable and stable videos for training, which are difficult to collect.\nTraditional trajectory-based stabilizers, on the other hand, divide the task\ninto several sub-tasks and tackle them subsequently, which are fragile in\ntextureless and occluded regions regarding the usage of hand-crafted features.\nIn this paper, we attempt to tackle the video stabilization problem in a deep\nunsupervised learning manner, which borrows the divide-and-conquer idea from\ntraditional stabilizers while leveraging the representation power of DNNs to\nhandle the challenges in real-world scenarios. Technically, DUT is composed of\na trajectory estimation stage and a trajectory smoothing stage. In the\ntrajectory estimation stage, we first estimate the motion of keypoints,\ninitialize and refine the motion of grids via a novel multi-homography\nestimation strategy and a motion refinement network, respectively, and get the\ngrid-based trajectories via temporal association. In the trajectory smoothing\nstage, we devise a novel network to predict dynamic smoothing kernels for\ntrajectory smoothing, which can well adapt to trajectories with different\ndynamic patterns. We exploit the spatial and temporal coherence of keypoints\nand grid vertices to formulate the training objectives, resulting in an\nunsupervised training scheme. Experiment results on public benchmarks show that\nDUT outperforms state-of-the-art methods both qualitatively and quantitatively.\nThe source code is available at https://github.com/Annbless/DUTCode.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maybank_S/0/1/0/all/0/1\">Stephen J. Maybank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Unsupervised Meta-Learning via the Characteristics of Few-Shot Tasks. (arXiv:2011.14663v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.14663","description":"<p>Meta-learning has become a practical approach towards few-shot image\nclassification, where \"a strategy to learn a classifier\" is meta-learned on\nlabeled base classes and can be applied to tasks with novel classes. We remove\nthe requirement of base class labels and learn generalizable embeddings via\nUnsupervised Meta-Learning (UML). Specifically, episodes of tasks are\nconstructed with data augmentations from unlabeled base classes during\nmeta-training, and we apply embedding-based classifiers to novel tasks with\nlabeled few-shot examples during meta-test. We observe two elements play\nimportant roles in UML, i.e., the way to sample tasks and measure similarities\nbetween instances. Thus we obtain a strong baseline with two simple\nmodifications -- a sufficient sampling strategy constructing multiple tasks per\nepisode efficiently together with a semi-normalized similarity. We then take\nadvantage of the characteristics of tasks from two directions to get further\nimprovements. First, synthesized confusing instances are incorporated to help\nextract more discriminative embeddings. Second, we utilize an additional\ntask-specific embedding transformation as an auxiliary component during\nmeta-training to promote the generalization ability of the pre-adapted\nembeddings. Experiments on few-shot learning benchmarks verify that our\napproaches outperform previous UML methods and achieve comparable or even\nbetter performance than its supervised variants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification. (arXiv:2103.04053v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.04053","description":"<p>Real-world large-scale medical image analysis (MIA) datasets have three\nchallenges: 1) they contain noisy-labelled samples that affect training\nconvergence and generalisation, 2) they usually have an imbalanced distribution\nof samples per class, and 3) they normally comprise a multi-label problem,\nwhere samples can have multiple diagnoses. Current approaches are commonly\ntrained to solve a subset of those problems, but we are unaware of methods that\naddress the three problems simultaneously. In this paper, we propose a new\ntraining module called Non-Volatile Unbiased Memory (NVUM), which\nnon-volatility stores running average of model logits for a new regularization\nloss on noisy multi-label problem. We further unbias the classification\nprediction in NVUM update for imbalanced learning problem. We run extensive\nexperiments to evaluate NVUM on new benchmarks proposed by this paper, where\ntraining is performed on noisy multi-label imbalanced chest X-ray (CXR)\ntraining sets, formed by Chest-Xray14 and CheXpert, and the testing is\nperformed on the clean multi-label CXR datasets OpenI and PadChest. Our method\noutperforms previous state-of-the-art CXR classifiers and previous methods that\ncan deal with noisy labels on all evaluations. Our code is available at\nhttps://github.com/FBLADL/NVUM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuanhong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Face Obfuscation in ImageNet. (arXiv:2103.06191v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.06191","description":"<p>Face obfuscation (blurring, mosaicing, etc.) has been shown to be effective\nfor privacy protection; nevertheless, object recognition research typically\nassumes access to complete, unobfuscated images. In this paper, we explore the\neffects of face obfuscation on the popular ImageNet challenge visual\nrecognition benchmark. Most categories in the ImageNet challenge are not people\ncategories; however, many incidental people appear in the images, and their\nprivacy is a concern. We first annotate faces in the dataset. Then we\ndemonstrate that face obfuscation has minimal impact on the accuracy of\nrecognition models. Concretely, we benchmark multiple deep neural networks on\nobfuscated images and observe that the overall recognition accuracy drops only\nslightly (&lt;= 1.0%). Further, we experiment with transfer learning to 4\ndownstream tasks (object recognition, scene recognition, face attribute\nclassification, and object detection) and show that features learned on\nobfuscated images are equally transferable. Our work demonstrates the\nfeasibility of privacy-aware visual recognition, improves the highly-used\nImageNet challenge benchmark, and suggests an important path for future visual\ndatasets. Data and code are available at\nhttps://github.com/princetonvisualai/imagenet-face-obfuscation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yau_J/0/1/0/all/0/1\">Jacqueline Yau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russakovsky_O/0/1/0/all/0/1\">Olga Russakovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Hierarchical Games for Image Explanations. (arXiv:2104.06164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06164","description":"<p>As modern complex neural networks keep breaking records and solving harder\nproblems, their predictions also become less and less intelligible. The current\nlack of interpretability often undermines the deployment of accurate machine\nlearning tools in sensitive settings. In this work, we present a model-agnostic\nexplanation method for image classification based on a hierarchical extension\nof Shapley coefficients--Hierarchical Shap (h-Shap)--that resolves some of the\nlimitations of current approaches. Unlike other Shapley-based explanation\nmethods, h-Shap is scalable and can be computed without the need of\napproximation. Under certain distributional assumptions, such as those common\nin multiple instance learning, h-Shap retrieves the exact Shapley coefficients\nwith an exponential improvement in computational complexity. We compare our\nhierarchical approach with popular Shapley-based and non-Shapley-based methods\non a synthetic dataset, a medical imaging scenario, and a general computer\nvision problem, showing that h-Shap outperforms the state of the art in both\naccuracy and runtime. Code and experiments are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teneggi_J/0/1/0/all/0/1\">Jacopo Teneggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luster_A/0/1/0/all/0/1\">Alexandre Luster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1\">Jeremias Sulam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on Self-Supervised Learning. (arXiv:2105.10123v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.10123","description":"<p>Large-scale unlabeled data has spurred recent progress in self-supervised\nlearning methods that learn rich visual representations. State-of-the-art\nself-supervised methods for learning representations from images (e.g., MoCo,\nBYOL, MSF) use an inductive bias that random augmentations (e.g., random crops)\nof an image should produce similar embeddings. We show that such methods are\nvulnerable to backdoor attacks - where an attacker poisons a small part of the\nunlabeled data by adding a trigger (image patch chosen by the attacker) to the\nimages. The model performance is good on clean test images, but the attacker\ncan manipulate the decision of the model by showing the trigger at test time.\nBackdoor attacks have been studied extensively in supervised learning and to\nthe best of our knowledge, we are the first to study them for self-supervised\nlearning. Backdoor attacks are more practical in self-supervised learning,\nsince the use of large unlabeled data makes data inspection to remove poisons\nprohibitive. We show that in our targeted attack, the attacker can produce many\nfalse positives for the target category by using the trigger at test time. We\nalso propose a defense method based on knowledge distillation that succeeds in\nneutralizing the attack. Our code is available here:\nhttps://github.com/UMBCvision/SSL-Backdoor .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1\">Aniruddha Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tejankar_A/0/1/0/all/0/1\">Ajinkya Tejankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koohpayegani_S/0/1/0/all/0/1\">Soroush Abbasi Koohpayegani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Deformation Estimation via Multi-Objective Optimization. (arXiv:2106.04139v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04139","description":"<p>The free-form deformation model can represent a wide range of non-rigid\ndeformations by manipulating a control point lattice over the image. However,\ndue to a large number of parameters, it is challenging to fit the free-form\ndeformation model directly to the deformed image for deformation estimation\nbecause of the complexity of the fitness landscape. In this paper, we cast the\nregistration task as a multi-objective optimization problem (MOP) according to\nthe fact that regions affected by each control point overlap with each other.\nSpecifically, by partitioning the template image into several regions and\nmeasuring the similarity of each region independently, multiple objectives are\nbuilt and deformation estimation can thus be realized by solving the MOP with\noff-the-shelf multi-objective evolutionary algorithms (MOEAs). In addition, a\ncoarse-to-fine strategy is realized by image pyramid combined with control\npoint mesh subdivision. Specifically, the optimized candidate solutions of the\ncurrent image level are inherited by the next level, which increases the\nability to deal with large deformation. Also, a post-processing procedure is\nproposed to generate a single output utilizing the Pareto optimal solutions.\nComparative experiments on both synthetic and real-world images show the\neffectiveness and usefulness of our deformation estimation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakane_T/0/1/0/all/0/1\">Takumi Nakane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Haoran Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Counterfactual Visual Explanations With Overdetermination. (arXiv:2106.14556v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.14556","description":"<p>A novel explainable AI method called CLEAR Image is introduced in this paper.\nCLEAR Image is based on the view that a satisfactory explanation should be\ncontrastive, counterfactual and measurable. CLEAR Image explains an image's\nclassification probability by contrasting the image with a corresponding image\ngenerated automatically via adversarial learning. This enables both salient\nsegmentation and perturbations that faithfully determine each segment's\nimportance. CLEAR Image was successfully applied to a medical imaging case\nstudy where it outperformed methods such as Grad-CAM and LIME by an average of\n27% using a novel pointing game metric. CLEAR Image excels in identifying cases\nof \"causal overdetermination\" where there are multiple patches in an image, any\none of which is sufficient by itself to cause the classification probability to\nbe close to one.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Adam White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngan_K/0/1/0/all/0/1\">Kwun Ho Ngan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phelan_J/0/1/0/all/0/1\">James Phelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afgeh_S/0/1/0/all/0/1\">Saman Sadeghi Afgeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryan_K/0/1/0/all/0/1\">Kevin Ryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reyes_Aldasoro_C/0/1/0/all/0/1\">Constantino Carlos Reyes-Aldasoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1\">Artur d&#x27;Avila Garcez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization and Robustness Implications in Object-Centric Learning. (arXiv:2107.00637v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00637","description":"<p>The idea behind object-centric representation learning is that natural scenes\ncan better be modeled as compositions of objects and their relations as opposed\nto distributed representations. This inductive bias can be injected into neural\nnetworks to potentially improve systematic generalization and performance of\ndownstream tasks in scenes with multiple objects. In this paper, we train\nstate-of-the-art unsupervised models on five common multi-object datasets and\nevaluate segmentation metrics and downstream object property prediction. In\naddition, we study generalization and robustness by investigating the settings\nwhere either a single object is out of distribution -- e.g., having an unseen\ncolor, texture, or shape -- or global properties of the scene are altered --\ne.g., by occlusions, cropping, or increasing the number of objects. From our\nexperimental study, we find object-centric representations to be useful for\ndownstream tasks and generally robust to most distribution shifts affecting\nobjects. However, when the distribution shift affects the input in a less\nstructured manner, robustness in terms of segmentation and downstream task\nperformance may vary significantly across models and distribution shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vita_M/0/1/0/all/0/1\">Michele De Vita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.08543","description":"<p>Domain shift is one of the most salient challenges in medical computer\nvision. Due to immense variability in scanners' parameters and imaging\nprotocols, even images obtained from the same person and the same scanner could\ndiffer significantly. We address variability in computed tomography (CT) images\ncaused by different convolution kernels used in the reconstruction process, the\ncritical domain shift factor in CT. The choice of a convolution kernel affects\npixels' granularity, image smoothness, and noise level. We analyze a dataset of\npaired CT images, where smooth and sharp images were reconstructed from the\nsame sinograms with different kernels, thus providing identical anatomy but\ndifferent style. Though identical predictions are desired, we show that the\nconsistency, measured as the average Dice between predictions on pairs, is just\n0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and\nsurprisingly efficient approach to augment CT images in sinogram space\nemulating reconstruction with different kernels. We apply the proposed method\nin a zero-shot domain adaptation setup and show that the consistency boosts\nfrom 0.54 to 0.92 outperforming other augmentation approaches. Neither specific\npreparation of source domain data nor target domain data is required, so our\npublicly released FBPAug can be used as a plug-and-play module for zero-shot\ndomain adaptation in any CT-based task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1\">Talgat Saparov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1\">Anvar Kurmukov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirokikh_B/0/1/0/all/0/1\">Boris Shirokikh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReDAL: Region-based and Diversity-aware Active Learning for Point Cloud Semantic Segmentation. (arXiv:2107.11769v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.11769","description":"<p>Despite the success of deep learning on supervised point cloud semantic\nsegmentation, obtaining large-scale point-by-point manual annotations is still\na significant challenge. To reduce the huge annotation burden, we propose a\nRegion-based and Diversity-aware Active Learning (ReDAL), a general framework\nfor many deep learning approaches, aiming to automatically select only\ninformative and diverse sub-scene regions for label acquisition. Observing that\nonly a small portion of annotated regions are sufficient for 3D scene\nunderstanding with deep learning, we use softmax entropy, color discontinuity,\nand structural complexity to measure the information of sub-scene regions. A\ndiversity-aware selection algorithm is also developed to avoid redundant\nannotations resulting from selecting informative but similar regions in a\nquerying batch. Extensive experiments show that our method highly outperforms\nprevious active learning strategies, and we achieve the performance of 90%\nfully supervised learning, while less than 15% and 5% annotations are required\non S3DIS and SemanticKITTI datasets, respectively. Our code is publicly\navailable at https://github.com/tsunghan-wu/ReDAL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tsung-Han Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hsin-Ying Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Ping-Chia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Monocular Depth Estimation in Highly Complex Environments. (arXiv:2107.13137v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13137","description":"<p>With the development of computational intelligence algorithms, unsupervised\nmonocular depth and pose estimation framework, which is driven by warped\nphotometric consistency, has shown great performance in the daytime scenario.\nWhile in some challenging environments, like night and rainy night, the\nessential photometric consistency hypothesis is untenable because of the\ncomplex lighting and reflection, so that the above unsupervised framework\ncannot be directly applied to these complex scenarios. In this paper, we\ninvestigate the problem of unsupervised monocular depth estimation in highly\ncomplex scenarios and address this challenging problem by adopting an image\ntransfer-based domain adaptation framework. We adapt the depth model trained on\nday-time scenarios to be applicable to night-time scenarios, and constraints on\nboth feature space and output space promote the framework to learn the key\nfeatures for depth decoding. Meanwhile, we further tackle the effects of\nunstable image transfer quality on domain adaptation, and an image adaptation\napproach is proposed to evaluate the quality of transferred images and\nre-weight the corresponding losses, so as to improve the performance of the\nadapted depth model. Extensive experiments show the effectiveness of the\nproposed unsupervised framework in estimating the dense depth map from highly\ncomplex images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiyu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NPBDREG: Uncertainty Assessment in Diffeomorphic Brain MRI Registration using a Non-parametric Bayesian Deep-Learning Based Approach. (arXiv:2108.06771v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06771","description":"<p>Quantification of uncertainty in deep-neural-networks (DNN) based image\nregistration algorithms plays a critical role in the deployment of image\nregistration algorithms for clinical applications such as surgical planning,\nintraoperative guidance, and longitudinal monitoring of disease progression or\ntreatment efficacy as well as in research-oriented processing pipelines.\nCurrently available approaches for uncertainty estimation in DNN-based image\nregistration algorithms may result in sub-optimal clinical decision making due\nto potentially inaccurate estimation of the uncertainty of the registration\nstems for the assumed parametric distribution of the registration latent space.\nWe introduce NPBDREG, a fully non-parametric Bayesian framework for uncertainty\nestimation in DNN-based deformable image registration by combining an Adam\noptimizer with stochastic gradient Langevin dynamics (SGLD) to characterize the\nunderlying posterior distribution through posterior sampling. Thus, it has the\npotential to provide uncertainty estimates that are highly correlated with the\npresence of out of distribution data. We demonstrated the added-value of\nNPBDREG, compared to the baseline probabilistic VoxelMorph model (PrVXM), on\nbrain MRI image registration using $390$ image pairs from four publicly\navailable databases: MGH10, CMUC12, ISBR18 and LPBA40. The NPBDREG shows a\nbetter correlation of the predicted uncertainty with out-of-distribution data\n($r&gt;0.95$ vs. $r&lt;0.5$) as well as a 7.3%improvement in the registration\naccuracy (Dice score, $0.74$ vs. $0.69$, $p \\ll 0.01$), and 18% improvement in\nregistration smoothness (percentage of folds in the deformation field, 0.014\nvs. 0.017, $p \\ll 0.01$). Finally, NPBDREG demonstrated a better generalization\ncapability for data corrupted by a mixed structure noise (Dice score of $0.73$\nvs. $0.69$, $p \\ll 0.01$) compared to the baseline PrVXM approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khawaled_S/0/1/0/all/0/1\">Samah Khawaled</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freiman_M/0/1/0/all/0/1\">Moti Freiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MEDIC: A Multi-Task Learning Dataset for Disaster Image Classification. (arXiv:2108.12828v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12828","description":"<p>Recent research in disaster informatics demonstrates a practical and\nimportant use case of artificial intelligence to save human lives and suffering\nduring natural disasters based on social media contents (text and images).\nWhile notable progress has been made using texts, research on exploiting the\nimages remains relatively under-explored. To advance image-based approaches, we\npropose MEDIC (Available at: https://crisisnlp.qcri.org/medic/index.html),\nwhich is the largest social media image classification dataset for humanitarian\nresponse consisting of 71,198 images to address four different tasks in a\nmulti-task learning setup. This is the first dataset of its kind: social media\nimages, disaster response, and multi-task learning research. An important\nproperty of this dataset is its high potential to facilitate research on\nmulti-task learning, which recently receives much interest from the machine\nlearning community and has shown remarkable results in terms of memory,\ninference speed, performance, and generalization capability. Therefore, the\nproposed dataset is an important resource for advancing image-based disaster\nmanagement and multi-task machine learning research. We experiment with\ndifferent deep learning architectures and report promising results, which are\nabove the majority baselines for all tasks. Along with the dataset, we also\nrelease all relevant scripts (https://github.com/firojalam/medic).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1\">Tanvirul Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Md. Arid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasnat_A/0/1/0/all/0/1\">Abul Hasnat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_M/0/1/0/all/0/1\">Muhammad Imran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ofli_F/0/1/0/all/0/1\">Ferda Ofli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization for Medical Image Segmentation via Hierarchical Consistency Regularization. (arXiv:2109.05742v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.05742","description":"<p>Modern deep neural networks struggle to transfer knowledge and generalize\nacross diverse domains when deployed to real-world applications. Currently,\ndomain generalization (DG) is introduced to learn a universal representation\nfrom multiple domains to improve the network generalization ability on unseen\ndomains. However, previous DG methods only focus on the data-level consistency\nscheme without considering the synergistic regularization among different\nconsistency schemes. In this paper, we present a novel Hierarchical Consistency\nframework for Domain Generalization (HCDG) by integrating Extrinsic Consistency\nand Intrinsic Consistency synergistically. Particularly, for the Extrinsic\nConsistency, we leverage the knowledge across multiple source domains to\nenforce data-level consistency. To better enhance such consistency, we design a\nnovel Amplitude Gaussian-mixing strategy into Fourier-based data augmentation\ncalled DomainUp. For the Intrinsic Consistency, we perform task-level\nconsistency for the same instance under the dual-task scenario. We evaluate the\nproposed HCDG framework on two medical image segmentation tasks, i.e., optic\ncup/disc segmentation on fundus images and prostate MRI segmentation. Extensive\nexperimental results manifest the effectiveness and versatility of our HCDG\nframework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yijun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shujun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lequan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Fast Adversarial Training with Learnable Adversarial Initialization. (arXiv:2110.05007v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05007","description":"<p>Adversarial training (AT) has been demonstrated to be effective in improving\nmodel robustness by leveraging adversarial examples for training. However, most\nAT methods are in face of expensive time and computational cost for calculating\ngradients at multiple steps in generating adversarial examples. To boost\ntraining efficiency, fast gradient sign method (FGSM) is adopted in fast AT\nmethods by calculating gradient only once. Unfortunately, the robustness is far\nfrom satisfactory. One reason may arise from the initialization fashion.\nExisting fast AT generally uses a random sample-agnostic initialization, which\nfacilitates the efficiency yet hinders a further robustness improvement. Up to\nnow, the initialization in fast AT is still not extensively explored. In this\npaper, we boost fast AT with a sample-dependent adversarial initialization,\ni.e., an output from a generative network conditioned on a benign image and its\ngradient information from the target network. As the generative network and the\ntarget network are optimized jointly in the training phase, the former can\nadaptively generate an effective initialization with respect to the latter,\nwhich motivates gradually improved robustness. Experimental evaluations on four\nbenchmark databases demonstrate the superiority of our proposed method over\nstate-of-the-art fast AT methods, as well as comparable robustness to advanced\nmulti-step AT methods. The code is released at\nhttps://github.com//jiaxiaojunQAQ//FGSM-SDI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaojun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1\">Baoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xiaochun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Break Deep Perceptual Hashing: The Use Case NeuralHash. (arXiv:2111.06628v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.06628","description":"<p>Apple recently revealed its deep perceptual hashing system NeuralHash to\ndetect child sexual abuse material (CSAM) on user devices before files are\nuploaded to its iCloud service. Public criticism quickly arose regarding the\nprotection of user privacy and the system's reliability. In this paper, we\npresent the first comprehensive empirical analysis of deep perceptual hashing\nbased on NeuralHash. Specifically, we show that current deep perceptual hashing\nmay not be robust. An adversary can manipulate the hash values by applying\nslight changes in images, either induced by gradient-based approaches or simply\nby performing standard image transformations, forcing or preventing hash\ncollisions. Such attacks permit malicious actors easily to exploit the\ndetection system: from hiding abusive material to framing innocent users,\neverything is possible. Moreover, using the hash values, inferences can still\nbe made about the data stored on user devices. In our view, based on our\nresults, deep perceptual hashing in its current form is generally not ready for\nrobust client-side scanning and should not be used from a privacy perspective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1\">Daniel Neider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Educated Warm Start For Deep Image Prior-Based Micro CT Reconstruction. (arXiv:2111.11926v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.11926","description":"<p>Deep image prior (DIP) was recently introduced as an effective unsupervised\napproach for image restoration tasks. DIP represents the image to be recovered\nas the output of a deep convolutional neural network, and learns the network's\nparameters such that the output matches the corrupted observation. Despite its\nimpressive reconstructive properties, the approach is slow when compared to\nsupervisedly learned, or traditional reconstruction techniques. To address the\ncomputational challenge, we bestow DIP with a two-stage learning paradigm: (i)\nperform a supervised pretraining of the network on a simulated dataset; (ii)\nfine-tune the network's parameters to adapt to the target reconstruction task.\nWe provide a thorough empirical analysis to shed insights into the impacts of\npretraining in the context of image reconstruction. We showcase that\npretraining considerably speeds up and stabilizes the subsequent reconstruction\ntask from real-measured 2D and 3D micro computed tomography data of biological\nspecimens. The code and additional experimental materials are available at\nhttps://educateddip.github.io/docs.educated_deep_image_prior/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Barbano_R/0/1/0/all/0/1\">Riccardo Barbano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leuschner_J/0/1/0/all/0/1\">Johannes Leuschner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_M/0/1/0/all/0/1\">Maximilian Schmidt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Denker_A/0/1/0/all/0/1\">Alexander Denker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hauptmann_A/0/1/0/all/0/1\">Andreas Hauptmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maass_P/0/1/0/all/0/1\">Peter Maa&#xdf;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_B/0/1/0/all/0/1\">Bangti Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FogAdapt: Self-Supervised Domain Adaptation for Semantic Segmentation of Foggy Images. (arXiv:2201.02588v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02588","description":"<p>This paper presents FogAdapt, a novel approach for domain adaptation of\nsemantic segmentation for dense foggy scenes. Although significant research has\nbeen directed to reduce the domain shift in semantic segmentation, adaptation\nto scenes with adverse weather conditions remains an open question. Large\nvariations in the visibility of the scene due to weather conditions, such as\nfog, smog, and haze, exacerbate the domain shift, thus making unsupervised\nadaptation in such scenarios challenging. We propose a self-entropy and\nmulti-scale information augmented self-supervised domain adaptation method\n(FogAdapt) to minimize the domain shift in foggy scenes segmentation. Supported\nby the empirical evidence that an increase in fog density results in high\nself-entropy for segmentation probabilities, we introduce a self-entropy based\nloss function to guide the adaptation method. Furthermore, inferences obtained\nat different image scales are combined and weighted by the uncertainty to\ngenerate scale-invariant pseudo-labels for the target domain. These\nscale-invariant pseudo-labels are robust to visibility and scale variations. We\nevaluate the proposed model on real clear-weather scenes to real foggy scenes\nadaptation and synthetic non-foggy images to real foggy scenes adaptation\nscenarios. Our experiments demonstrate that FogAdapt significantly outperforms\nthe current state-of-the-art in semantic segmentation of foggy images.\nSpecifically, by considering the standard settings compared to state-of-the-art\n(SOTA) methods, FogAdapt gains 3.8% on Foggy Zurich, 6.0% on Foggy\nDriving-dense, and 3.6% on Foggy Driving in mIoU when adapted from Cityscapes\nto Foggy Zurich.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_J/0/1/0/all/0/1\">Javed Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_R/0/1/0/all/0/1\">Rehan Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Mohsen Ali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The CLEAR Benchmark: Continual LEArning on Real-World Imagery. (arXiv:2201.06289v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06289","description":"<p>Continual learning (CL) is widely regarded as crucial challenge for lifelong\nAI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make\nuse of artificial temporal variation and do not align with or generalize to the\nreal-world. In this paper, we introduce CLEAR, the first continual image\nclassification benchmark dataset with a natural temporal evolution of visual\nconcepts in the real world that spans a decade (2004-2014). We build CLEAR from\nexisting large-scale image collections (YFCC100M) through a novel and scalable\nlow-cost approach to visio-linguistic dataset curation. Our pipeline makes use\nof pretrained vision-language models (e.g. CLIP) to interactively build labeled\ndatasets, which are further validated with crowd-sourcing to remove errors and\neven inappropriate images (hidden in original YFCC100M). The major strength of\nCLEAR over prior CL benchmarks is the smooth temporal evolution of visual\nconcepts with real-world imagery, including both high-quality labeled data\nalong with abundant unlabeled samples per time period for continual\nsemi-supervised learning. We find that a simple unsupervised pre-training step\ncan already boost state-of-the-art CL algorithms that only utilize\nfully-supervised data. Our analysis also reveals that mainstream CL evaluation\nprotocols that train and test on iid data artificially inflate performance of\nCL system. To address this, we propose novel \"streaming\" protocols for CL that\nalways test on the (near) future. Interestingly, streaming protocols (a) can\nsimplify dataset curation since today's testset can be repurposed for\ntomorrow's trainset and (b) can produce more generalizable models with more\naccurate estimates of performance since all labeled data from each time-period\nis used for both training and testing (unlike classic iid train-test splits).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiqiu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jia Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1\">Deepak Pathak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. (arXiv:2201.12179v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12179","description":"<p>Model inversion attacks (MIAs) aim to create synthetic images that reflect\nthe class-wise characteristics from a target classifier's private training data\nby exploiting the model's learned knowledge. Previous research has developed\ngenerative MIAs that use generative adversarial networks (GANs) as image priors\ntailored to a specific target model. This makes the attacks time- and\nresource-consuming, inflexible, and susceptible to distributional shifts\nbetween datasets. To overcome these drawbacks, we present Plug &amp; Play Attacks,\nwhich relax the dependency between the target model and image prior, and enable\nthe use of a single GAN to attack a wide range of targets, requiring only minor\nadjustments to the attack. Moreover, we show that powerful MIAs are possible\neven with publicly available pre-trained GANs and under strong distributional\nshifts, for which previous approaches fail to produce meaningful results. Our\nextensive evaluation confirms the improved robustness and flexibility of Plug &amp;\nPlay Attacks and their ability to create high-quality images revealing\nsensitive class characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Antonio De Almeida Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Antonia Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADG-Pose: Automated Dataset Generation for Real-World Human Pose Estimation. (arXiv:2202.00753v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00753","description":"<p>Recent advancements in computer vision have seen a rise in the prominence of\napplications using neural networks to understand human poses. However, while\naccuracy has been steadily increasing on State-of-the-Art datasets, these\ndatasets often do not address the challenges seen in real-world applications.\nThese challenges are dealing with people distant from the camera, people in\ncrowds, and heavily occluded people. As a result, many real-world applications\nhave trained on data that does not reflect the data present in deployment,\nleading to significant underperformance. This article presents ADG-Pose, a\nmethod for automatically generating datasets for real-world human pose\nestimation. These datasets can be customized to determine person distances,\ncrowdedness, and occlusion distributions. Models trained with our method are\nable to perform in the presence of these challenges where those trained on\nother datasets fail. Using ADG-Pose, end-to-end accuracy for real-world\nskeleton-based action recognition sees a 20% increase on scenes with moderate\ndistance and occlusion levels, and a 4X increase on distant scenes where other\nmodels failed to perform better than random.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noghre_G/0/1/0/all/0/1\">Ghazal Alinezhad Noghre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pazho_A/0/1/0/all/0/1\">Armin Danesh Pazho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_J/0/1/0/all/0/1\">Justin Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hewitt_N/0/1/0/all/0/1\">Nathan Hewitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neff_C/0/1/0/all/0/1\">Christopher Neff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computing Multiple Image Reconstructions with a Single Hypernetwork. (arXiv:2202.11009v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11009","description":"<p>Deep learning based techniques achieve state-of-the-art results in a wide\nrange of image reconstruction tasks like compressed sensing. These methods\nalmost always have hyperparameters, such as the weight coefficients that\nbalance the different terms in the optimized loss function. The typical\napproach is to train the model for a hyperparameter setting determined with\nsome empirical or theoretical justification. Thus, at inference time, the model\ncan only compute reconstructions corresponding to the pre-determined\nhyperparameter values. In this work, we present a hypernetwork-based approach,\ncalled HyperRecon, to train reconstruction models that are agnostic to\nhyperparameter settings. At inference time, HyperRecon can efficiently produce\ndiverse reconstructions, which would each correspond to different\nhyperparameter values. In this framework, the user is empowered to select the\nmost useful output(s) based on their own judgement. We demonstrate our method\nin compressed sensing, super-resolution and denoising tasks, using two\nlarge-scale and publicly-available MRI datasets. Our code is available at\nhttps://github.com/alanqrwang/hyperrecon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Alan Q. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabuncu_M/0/1/0/all/0/1\">Mert R. Sabuncu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors. (arXiv:2203.01825v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.01825","description":"<p>Transfer learning is a standard technique to transfer knowledge from one\ndomain to another. For applications in medical imaging, transfer from ImageNet\nhas become the de-facto approach, despite differences in the tasks and image\ncharacteristics between the domains. However, it is unclear what factors\ndetermine whether - and to what extent - transfer learning to the medical\ndomain is useful. The long-standing assumption that features from the source\ndomain get reused has recently been called into question. Through a series of\nexperiments on several medical image benchmark datasets, we explore the\nrelationship between transfer learning, data size, the capacity and inductive\nbias of the model, as well as the distance between the source and target\ndomain. Our findings suggest that transfer learning is beneficial in most\ncases, and we characterize the important role feature reuse plays in its\nsuccess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1\">Christos Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1\">Johan Fredin Haslum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorkhei_M/0/1/0/all/0/1\">Moein Sorkhei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soderberg_M/0/1/0/all/0/1\">Magnus S&#xf6;derberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kevin Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Promoted Supervision for Few-Shot Transformer. (arXiv:2203.07057v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07057","description":"<p>The few-shot learning ability of vision transformers (ViTs) is rarely\ninvestigated though heavily desired. In this work, we empirically find that\nwith the same few-shot learning frameworks, \\eg~Meta-Baseline, replacing the\nwidely used CNN feature extractor with a ViT model often severely impairs\nfew-shot classification performance. Moreover, our empirical study shows that\nin the absence of inductive bias, ViTs often learn the low-qualified token\ndependencies under few-shot learning regime where only a few labeled training\ndata are available, which largely contributes to the above performance\ndegradation. To alleviate this issue, for the first time, we propose a simple\nyet effective few-shot training framework for ViTs, namely Self-promoted\nsUpervisioN (SUN). Specifically, besides the conventional global supervision\nfor global semantic learning SUN further pretrains the ViT on the few-shot\nlearning dataset and then uses it to generate individual location-specific\nsupervision for guiding each patch token. This location-specific supervision\ntells the ViT which patch tokens are similar or dissimilar and thus accelerates\ntoken dependency learning. Moreover, it models the local semantics in each\npatch token to improve the object grounding and recognition capability which\nhelps learn generalizable patterns. To improve the quality of location-specific\nsupervision, we further propose two techniques:~1) background patch filtration\nto filtrate background patches out and assign them into an extra background\nclass; and 2) spatial-consistent augmentation to introduce sufficient diversity\nfor data augmentation while keeping the accuracy of the generated local\nsupervisions. Experimental results show that SUN using ViTs significantly\nsurpasses other few-shot learning frameworks with ViTs and is the first one\nthat achieves higher performance than those CNN state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bowen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation. (arXiv:2203.15041v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.15041","description":"<p>Social navigation is the capability of an autonomous agent, such as a robot,\nto navigate in a 'socially compliant' manner in the presence of other\nintelligent agents such as humans. With the emergence of autonomously\nnavigating mobile robots in human populated environments (e.g., domestic\nservice robots in homes and restaurants and food delivery robots on public\nsidewalks), incorporating socially compliant navigation behaviors on these\nrobots becomes critical to ensuring safe and comfortable human robot\ncoexistence. To address this challenge, imitation learning is a promising\nframework, since it is easier for humans to demonstrate the task of social\nnavigation rather than to formulate reward functions that accurately capture\nthe complex multi objective setting of social navigation. The use of imitation\nlearning and inverse reinforcement learning to social navigation for mobile\nrobots, however, is currently hindered by a lack of large scale datasets that\ncapture socially compliant robot navigation demonstrations in the wild. To fill\nthis gap, we introduce Socially CompliAnt Navigation Dataset (SCAND) a large\nscale, first person view dataset of socially compliant navigation\ndemonstrations. Our dataset contains 8.7 hours, 138 trajectories, 25 miles of\nsocially compliant, human teleoperated driving demonstrations that comprises\nmulti modal data streams including 3D lidar, joystick commands, odometry,\nvisual and inertial information, collected on two morphologically different\nmobile robots a Boston Dynamics Spot and a Clearpath Jackal by four different\nhuman demonstrators in both indoor and outdoor environments. We additionally\nperform preliminary analysis and validation through real world robot\nexperiments and show that navigation policies learned by imitation learning on\nSCAND generate socially compliant behaviors\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Anirudh Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirk_S/0/1/0/all/0/1\">Soeren Pirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hart_J/0/1/0/all/0/1\">Justin Hart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_J/0/1/0/all/0/1\">Joydeep Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TubeDETR: Spatio-Temporal Video Grounding with Transformers. (arXiv:2203.16434v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.16434","description":"<p>We consider the problem of localizing a spatio-temporal tube in a video\ncorresponding to a given text query. This is a challenging task that requires\nthe joint and efficient modeling of temporal, spatial and multi-modal\ninteractions. To address this task, we propose TubeDETR, a transformer-based\narchitecture inspired by the recent success of such models for text-conditioned\nobject detection. Our model notably includes: (i) an efficient video and text\nencoder that models spatial multi-modal interactions over sparsely sampled\nframes and (ii) a space-time decoder that jointly performs spatio-temporal\nlocalization. We demonstrate the advantage of our proposed components through\nan extensive ablation study. We also evaluate our full approach on the\nspatio-temporal video grounding task and demonstrate improvements over the\nstate of the art on the challenging VidSTG and HC-STVG benchmarks. Code and\ntrained models are publicly available at\nhttps://antoyang.github.io/tubedetr.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">Antoine Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miech_A/0/1/0/all/0/1\">Antoine Miech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">Josef Sivic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-based Entity Prediction for Improved Machine Perception in Autonomous Systems. (arXiv:2203.16616v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2203.16616","description":"<p>Knowledge-based entity prediction (KEP) is a novel task that aims to improve\nmachine perception in autonomous systems. KEP leverages relational knowledge\nfrom heterogeneous sources in predicting potentially unrecognized entities. In\nthis paper, we provide a formal definition of KEP as a knowledge completion\ntask. Three potential solutions are then introduced, which employ several\nmachine learning and data mining techniques. Finally, the applicability of KEP\nis demonstrated on two autonomous systems from different domains; namely,\nautonomous driving and smart manufacturing. We argue that in complex real-world\nsystems, the use of KEP would significantly improve machine perception while\npushing the current technology one step closer to achieving full autonomy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wickramarachchi_R/0/1/0/all/0/1\">Ruwan Wickramarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henson_C/0/1/0/all/0/1\">Cory Henson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SQ-VAE: Variational Bayes on Discrete Representation with Self-annealed Stochastic Quantization. (arXiv:2205.07547v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.07547","description":"<p>One noted issue of vector-quantized variational autoencoder (VQ-VAE) is that\nthe learned discrete representation uses only a fraction of the full capacity\nof the codebook, also known as codebook collapse. We hypothesize that the\ntraining scheme of VQ-VAE, which involves some carefully designed heuristics,\nunderlies this issue. In this paper, we propose a new training scheme that\nextends the standard VAE via novel stochastic dequantization and quantization,\ncalled stochastically quantized variational autoencoder (SQ-VAE). In SQ-VAE, we\nobserve a trend that the quantization is stochastic at the initial stage of the\ntraining but gradually converges toward a deterministic quantization, which we\ncall self-annealing. Our experiments show that SQ-VAE improves codebook\nutilization without using common heuristics. Furthermore, we empirically show\nthat SQ-VAE is superior to VAE and VQ-VAE in vision- and speech-related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takida_Y/0/1/0/all/0/1\">Yuhta Takida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shibuya_T/0/1/0/all/0/1\">Takashi Shibuya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1\">WeiHsiang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chieh-Hsin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohmura_J/0/1/0/all/0/1\">Junki Ohmura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uesaka_T/0/1/0/all/0/1\">Toshimitsu Uesaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_N/0/1/0/all/0/1\">Naoki Murata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_S/0/1/0/all/0/1\">Shusuke Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumakura_T/0/1/0/all/0/1\">Toshiyuki Kumakura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitsufuji_Y/0/1/0/all/0/1\">Yuki Mitsufuji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mip-NeRF RGB-D: Depth Assisted Fast Neural Radiance Fields. (arXiv:2205.09351v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.09351","description":"<p>Neural scene representations, such as Neural Radiance Fields (NeRF), are\nbased on training a multilayer perceptron (MLP) using a set of color images\nwith known poses. An increasing number of devices now produce RGB-D(color +\ndepth) information, which has been shown to be very important for a wide range\nof tasks. Therefore, the aim of this paper is to investigate what improvements\ncan be made to these promising implicit representations by incorporating depth\ninformation with the color images. In particular, the recently proposed\nMip-NeRF approach, which uses conical frustums instead of rays for volume\nrendering, allows one to account for the varying area of a pixel with distance\nfrom the camera center. The proposed method additionally models depth\nuncertainty. This allows to address major limitations of NeRF-based approaches\nincluding improving the accuracy of geometry, reduced artifacts, faster\ntraining time, and shortened prediction time. Experiments are performed on\nwell-known benchmark scenes, and comparisons show improved accuracy in scene\ngeometry and photometric reconstruction, while reducing the training time by 3\n- 5 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arnab Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmine_Y/0/1/0/all/0/1\">Yassine Ahmine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew I. Comport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Task-relevant Representations for Generalization via Characteristic Functions of Reward Sequence Distributions. (arXiv:2205.10218v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2205.10218","description":"<p>Generalization across different environments with the same tasks is critical\nfor successful applications of visual reinforcement learning (RL) in real\nscenarios. However, visual distractions -- which are common in real scenes --\nfrom high-dimensional observations can be hurtful to the learned\nrepresentations in visual RL, thus degrading the performance of generalization.\nTo tackle this problem, we propose a novel approach, namely Characteristic\nReward Sequence Prediction (CRESP), to extract the task-relevant information by\nlearning reward sequence distributions (RSDs), as the reward signals are\ntask-relevant in RL and invariant to visual distractions. Specifically, to\neffectively capture the task-relevant information via RSDs, CRESP introduces an\nauxiliary task -- that is, predicting the characteristic functions of RSDs --\nto learn task-relevant representations, because we can well approximate the\nhigh-dimensional distributions by leveraging the corresponding characteristic\nfunctions. Experiments demonstrate that CRESP significantly improves the\nperformance of generalization on unseen environments, outperforming several\nstate-of-the-arts on DeepMind Control tasks with different visual distractions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zijie Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mingxuan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shuiwang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Feng Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniXAI: A Library for Explainable AI. (arXiv:2206.01612v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.01612","description":"<p>We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python\nlibrary of eXplainable AI (XAI), which offers omni-way explainable AI\ncapabilities and various interpretable machine learning techniques to address\nthe pain points of understanding and interpreting the decisions made by machine\nlearning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library\nthat makes explainable AI easy for data scientists, ML researchers and\npractitioners who need explanation for various types of data, models and\nexplanation methods at different stages of ML process (data exploration,\nfeature engineering, model development, evaluation, and decision-making, etc).\nIn particular, our library includes a rich family of explanation methods\nintegrated in a unified interface, which supports multiple data types (tabular\ndata, images, texts, time-series), multiple types of ML models (traditional ML\nin Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of\ndiverse explanation methods including \"model-specific\" and \"model-agnostic\"\nones (such as feature-attribution explanation, counterfactual explanation,\ngradient-based explanation, etc). For practitioners, the library provides an\neasy-to-use unified interface to generate the explanations for their\napplications by only writing a few lines of codes, and also a GUI dashboard for\nvisualization of different explanations for more insights about decisions. In\nthis technical report, we present OmniXAI's design principles, system\narchitectures, and major functionalities, and also demonstrate several example\nuse cases across different types of data, tasks, and models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenzhuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Visual Generation with Composable Diffusion Models. (arXiv:2206.01714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.01714","description":"<p>Large text-guided diffusion models, such as DALLE-2, are able to generate\nstunning photorealistic images given natural language descriptions. While such\nmodels are highly flexible, they struggle to understand the composition of\ncertain concepts, such as confusing the attributes of different objects or\nrelations between objects. In this paper, we propose an alternative structured\napproach for compositional generation using diffusion models. An image is\ngenerated by composing a set of diffusion models, with each of them modeling a\ncertain component of the image. To do this, we interpret diffusion models as\nenergy-based models in which the data distributions defined by the energy\nfunctions may be explicitly combined. The proposed method can generate scenes\nat test time that are substantially more complex than those seen in training,\ncomposing sentence descriptions, object relations, human facial attributes, and\neven generalizing to new combinations that are rarely seen in the real world.\nWe further illustrate how our approach may be used to compose pre-trained\ntext-guided diffusion models and generate photorealistic images containing all\nthe details described in the input descriptions, including the binding of\ncertain object attributes that have been shown difficult for DALLE-2. These\nresults point to the effectiveness of the proposed method in promoting\nstructured generalization for visual generation. Project page:\nhttps://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Nan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yilun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator. (arXiv:2206.02284v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2206.02284","description":"<p>Understanding the underlying relationship between tongue and oropharyngeal\nmuscle deformation seen in tagged-MRI and intelligible speech plays an\nimportant role in advancing speech motor control theories and treatment of\nspeech related-disorders. Because of their heterogeneous representations,\nhowever, direct mapping between the two modalities -- i.e., two-dimensional\n(mid-sagittal slice) plus time tagged-MRI sequence and its corresponding\none-dimensional waveform -- is not straightforward. Instead, we resort to\ntwo-dimensional spectrograms as an intermediate representation, which contains\nboth pitch and resonance, from which to develop an end-to-end deep learning\nframework to translate from a sequence of tagged-MRI to its corresponding audio\nwaveform with limited dataset size.~Our framework is based on a novel fully\nconvolutional asymmetry translator with guidance of a self residual attention\nstrategy to specifically exploit the moving muscular structures during\nspeech.~In addition, we leverage a pairwise correlation of the samples with the\nsame utterances with a latent space representation disentanglement\nstrategy.~Furthermore, we incorporate an adversarial training approach with\ngenerative adversarial networks to offer improved realism on our generated\nspectrograms.~Our experimental results, carried out with a total of 63\ntagged-MRI sequences alongside speech acoustics, showed that our framework\nenabled the generation of clear audio waveforms from a sequence of tagged-MRI,\nsurpassing competing methods. Thus, our framework provides the great potential\nto help better understand the relationship between the two modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1\">Jerry L. Prince</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1\">Jiachen Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Maureen Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-training. (arXiv:2206.02288v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.02288","description":"<p>Unsupervised domain adaptation (UDA) has been vastly explored to alleviate\ndomain shifts between source and target domains, by applying a well-performed\nmodel in an unlabeled target domain via supervision of a labeled source domain.\nRecent literature, however, has indicated that the performance is still far\nfrom satisfactory in the presence of significant domain shifts. Nonetheless,\ndelineating a few target samples is usually manageable and particularly\nworthwhile, due to the substantial performance gain. Inspired by this, we aim\nto develop semi-supervised domain adaptation (SSDA) for medical image\nsegmentation, which is largely underexplored. We, thus, propose to exploit both\nlabeled source and target domain data, in addition to unlabeled target data in\na unified manner. Specifically, we present a novel asymmetric co-training (ACT)\nframework to integrate these subsets and avoid the domination of the source\ndomain data. Following a divide-and-conquer strategy, we explicitly decouple\nthe label supervisions in SSDA into two asymmetric sub-tasks, including\nsemi-supervised learning (SSL) and UDA, and leverage different knowledge from\ntwo segmentors to take into account the distinction between the source and\ntarget label supervisions. The knowledge learned in the two modules is then\nadaptively integrated with ACT, by iteratively teaching each other, based on\nthe confidence-aware pseudo-label. In addition, pseudo label noise is\nwell-controlled with an exponential MixUp decay scheme for smooth propagation.\nExperiments on cross-modality brain tumor MRI segmentation tasks using the\nBraTS18 database showed, even with limited labeled target samples, ACT yielded\nmarked improvements over UDA and state-of-the-art SSDA methods and approached\nan \"upper bound\" of supervised joint training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1\">Fangxu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shusharina_N/0/1/0/all/0/1\">Nadya Shusharina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_R/0/1/0/all/0/1\">Ruth Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C-C Jay Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1\">Georges El Fakhri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1\">Jonghye Woo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVC-Net: Multi-scale V-Net with Conditional Random Fields for Brain Extraction. (arXiv:2206.02837v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.02837","description":"<p>Brain extraction is one of the first steps of pre-processing 3D brain MRI\ndata. It is a prerequisite for any forthcoming brain imaging analyses. However,\nit is not a simple segmentation problem due to the complex structure of the\nbrain and human head. Although multiple solutions have been proposed in the\nliterature, we are still far from having truly robust methods. While previous\nmethods have used machine learning with structural/geometric priors, with the\ndevelopment of deep learning in computer vision tasks, there has been an\nincrease in proposed convolutional neural network architectures for this\nsemantic segmentation task. Yet, most models focus on improving the training\ndata and loss functions with little change in the architecture. In this paper,\nwe propose a novel architecture we call EVC-Net. EVC-Net adds lower scale\ninputs on each encoder block. This enhances the multi-scale scheme of the V-Net\narchitecture, hence increasing the efficiency of the model. Conditional Random\nFields, a popular approach for image segmentation before the deep learning era,\nare re-introduced here as an additional step for refining the network's output\nto capture fine-grained results in segmentation. We compare our model to\nstate-of-the-art methods such as HD-BET, Synthstrip and brainy. Results show\nthat even with limited training resources, EVC-Net achieves higher Dice\nCoefficient and Jaccard Index along with lower surface distance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1\">Jong Sung Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fadnavis_S/0/1/0/all/0/1\">Shreyas Fadnavis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Garyfallidis_E/0/1/0/all/0/1\">Eleftherios Garyfallidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cViL: Cross-Lingual Training of Vision-Language Models using Knowledge Distillation. (arXiv:2206.03354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.03354","description":"<p>Vision-and-language tasks are gaining popularity in the research community,\nbut the focus is still mainly on English. We propose a pipeline that utilizes\nEnglish-only vision-language models to train a monolingual model for a target\nlanguage. We propose to extend OSCAR+, a model which leverages object tags as\nanchor points for learning image-text alignments, to train on visual question\nanswering datasets in different languages. We propose a novel approach to\nknowledge distillation to train the model in other languages using parallel\nsentences. Compared to other models that use the target language in the\npretraining corpora, we can leverage an existing English model to transfer the\nknowledge to the target language using significantly lesser resources. We also\nrelease a large-scale visual question answering dataset in Japanese and Hindi\nlanguage. Though we restrict our work to visual question answering, our model\ncan be extended to any sequence-level classification task, and it can be\nextended to other languages as well. This paper focuses on two languages for\nthe visual question answering task - Japanese and Hindi. Our pipeline\noutperforms the current state-of-the-art models by a relative increase of 4.4%\nand 13.4% respectively in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Long Videos of Dynamic Scenes. (arXiv:2206.03429v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03429","description":"<p>We present a video generation model that accurately reproduces object motion,\nchanges in camera viewpoint, and new content that arises over time. Existing\nvideo generation methods often fail to produce new content as a function of\ntime while maintaining consistencies expected in real environments, such as\nplausible dynamics and object persistence. A common failure case is for content\nto never change due to over-reliance on inductive biases to provide temporal\nconsistency, such as a single latent code that dictates content for the entire\nvideo. On the other extreme, without long-term consistency, generated videos\nmay morph unrealistically between different scenes. To address these\nlimitations, we prioritize the time axis by redesigning the temporal latent\nrepresentation and learning long-term consistency from data by training on\nlonger videos. To this end, we leverage a two-phase training strategy, where we\nseparately train using longer videos at a low resolution and shorter videos at\na high resolution. To evaluate the capabilities of our model, we introduce two\nnew benchmark datasets with explicit focus on long-term temporal dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brooks_T/0/1/0/all/0/1\">Tim Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellsten_J/0/1/0/all/0/1\">Janne Hellsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aittala_M/0/1/0/all/0/1\">Miika Aittala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting-Chun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aila_T/0/1/0/all/0/1\">Timo Aila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehtinen_J/0/1/0/all/0/1\">Jaakko Lehtinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1\">Alexei A. Efros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karras_T/0/1/0/all/0/1\">Tero Karras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Penny for Your (visual) Thoughts: Self-Supervised Reconstruction of Natural Movies from Brain Activity. (arXiv:2206.03544v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.03544","description":"<p>Reconstructing natural videos from fMRI brain recordings is very challenging,\nfor two main reasons: (i) As fMRI data acquisition is difficult, we only have a\nlimited amount of supervised samples, which is not enough to cover the huge\nspace of natural videos; and (ii) The temporal resolution of fMRI recordings is\nmuch lower than the frame rate of natural videos. In this paper, we propose a\nself-supervised approach for natural-movie reconstruction. By employing\ncycle-consistency over Encoding-Decoding natural videos, we can: (i) exploit\nthe full framerate of the training videos, and not be limited only to clips\nthat correspond to fMRI recordings; (ii) exploit massive amounts of external\nnatural videos which the subjects never saw inside the fMRI machine. These\nenable increasing the applicable training data by several orders of magnitude,\nintroducing natural video priors to the decoding network, as well as temporal\ncoherence. Our approach significantly outperforms competing methods, since\nthose train only on the limited supervised data. We further introduce a new and\nsimple temporal prior of natural videos, which - when folded into our fMRI\ndecoder further - allows us to reconstruct videos at a higher frame-rate (HFR)\nof up to x8 of the original fMRI sample rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kupershmidt_G/0/1/0/all/0/1\">Ganit Kupershmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beliy_R/0/1/0/all/0/1\">Roman Beliy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaziv_G/0/1/0/all/0/1\">Guy Gaziv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irani_M/0/1/0/all/0/1\">Michal Irani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hypernetwork-based Personalized Federated Learning for Multi-Institutional CT Imaging. (arXiv:2206.03709v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.03709","description":"<p>Computed tomography (CT) is of great importance in clinical practice due to\nits powerful ability to provide patients' anatomical information without any\ninvasive inspection, but its potential radiation risk is raising people's\nconcerns. Deep learning-based methods are considered promising in CT\nreconstruction, but these network models are usually trained with the measured\ndata obtained from specific scanning protocol and need to centralizedly collect\nlarge amounts of data, which will lead to serious data domain shift, and\nprivacy concerns. To relieve these problems, in this paper, we propose a\nhypernetwork-based federated learning method for personalized CT imaging,\ndubbed as HyperFed. The basic assumption of HyperFed is that the optimization\nproblem for each institution can be divided into two parts: the local data\nadaption problem and the global CT imaging problem, which are implemented by an\ninstitution-specific hypernetwork and a global-sharing imaging network,\nrespectively. The purpose of global-sharing imaging network is to learn stable\nand effective common features from different institutions. The\ninstitution-specific hypernetwork is carefully designed to obtain\nhyperparameters to condition the global-sharing imaging network for\npersonalized local CT reconstruction. Experiments show that HyperFed achieves\ncompetitive performance in CT reconstruction compared with several other\nstate-of-the-art methods. It is believed as a promising direction to improve CT\nimaging quality and achieve personalized demands of different institutions or\nscanners without privacy data sharing. The codes will be released at\nhttps://github.com/Zi-YuanYang/HyperFed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyuan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1\">Wenjun Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zexin Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yingyu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Understanding Why Mask-Reconstruction Pretraining Helps in Downstream Tasks. (arXiv:2206.03826v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2206.03826","description":"<p>For unsupervised pretraining, mask-reconstruction pretraining (MRP)\napproaches randomly mask input patches and then reconstruct pixels or semantic\nfeatures of these masked patches via an auto-encoder. Then for a downstream\ntask, supervised fine-tuning the pretrained encoder remarkably surpasses the\nconventional supervised learning (SL) trained from scratch. However, it is\nstill unclear 1) how MRP performs semantic learning in the pretraining phase\nand 2) why it helps in downstream tasks. To solve these problems, we\ntheoretically show that on an auto-encoder of a two/one-layered convolution\nencoder/decoder, MRP can capture all discriminative semantics in the\npretraining dataset, and accordingly show its provable improvement over SL on\nthe classification downstream task. Specifically, we assume that pretraining\ndataset contains multi-view samples of ratio $1-\\mu$ and single-view samples of\nratio $\\mu$, where multi/single-view samples has multiple/single discriminative\nsemantics. Then for pretraining, we prove that 1) the convolution kernels of\nthe MRP encoder captures all discriminative semantics in the pretraining data;\nand 2) a convolution kernel captures at most one semantic. Accordingly, in the\ndownstream supervised fine-tuning, most semantics would be captured and\ndifferent semantics would not be fused together. This helps the downstream\nfine-tuned network to easily establish the relation between kernels and\nsemantic class labels. In this way, the fine-tuned encoder in MRP provably\nachieves zero test error with high probability for both multi-view and\nsingle-view test data. In contrast, as proved by~[3], conventional SL can only\nobtain a test accuracy between around $0.5\\mu$ for single-view test data. These\nresults together explain the benefits of MRP in downstream tasks. Experimental\nresults testify to multi-view data assumptions and our theoretical\nimplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jiachun Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays. (arXiv:2206.03935v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2206.03935","description":"<p>Chest X-ray (CXR) is the most typical radiological exam for diagnosis of\nvarious diseases. Due to the expensive and time-consuming annotations,\ndetecting anomalies in CXRs in an unsupervised fashion is very promising.\nHowever, almost all of the existing methods consider anomaly detection as a\nOne-Class Classification (OCC) problem. They model the distribution of only\nknown normal images during training and identify the samples not conforming to\nnormal profile as anomalies in the testing phase. A large number of unlabeled\nimages containing anomalies are thus ignored in the training phase, although\nthey are easy to obtain in clinical practice. In this paper, we propose a novel\nstrategy, Dual-distribution Discrepancy for Anomaly Detection (DDAD), utilizing\nboth known normal images and unlabeled images. The proposed method consists of\ntwo modules, denoted as A and B. During training, module A takes both known\nnormal and unlabeled images as inputs, capturing anomalous features from\nunlabeled images in some way, while module B models the distribution of only\nknown normal images. Subsequently, the inter-discrepancy between modules A and\nB, and intra-discrepancy inside module B are designed as anomaly scores to\nindicate anomalies. Experiments on three CXR datasets demonstrate that the\nproposed DDAD achieves consistent, significant gains and outperforms\nstate-of-the-art methods. Code is available at\nhttps://github.com/caiyu6666/DDAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1\">Yu Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_K/0/1/0/all/0/1\">Kwang-Ting Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Score-based Generative Models for High-Resolution Image Synthesis. (arXiv:2206.04029v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.04029","description":"<p>Score-based generative models (SGMs) have recently emerged as a promising\nclass of generative models. The key idea is to produce high-quality images by\nrecurrently adding Gaussian noises and gradients to a Gaussian sample until\nconverging to the target distribution, a.k.a. the diffusion sampling. To ensure\nstability of convergence in sampling and generation quality, however, this\nsequential sampling process has to take a small step size and many sampling\niterations (e.g., 2000). Several acceleration methods have been proposed with\nfocus on low-resolution generation. In this work, we consider the acceleration\nof high-resolution generation with SGMs, a more challenging yet more important\nproblem. We prove theoretically that this slow convergence drawback is\nprimarily due to the ignorance of the target distribution. Further, we\nintroduce a novel Target Distribution Aware Sampling (TDAS) method by\nleveraging the structural priors in space and frequency domains. Extensive\nexperiments on CIFAR-10, CelebA, LSUN, and FFHQ datasets validate that TDAS can\nconsistently accelerate state-of-the-art SGMs, particularly on more challenging\nhigh resolution (1024x1024) image generation tasks by up to 18.4x, whilst\nlargely maintaining the synthesis quality. With fewer sampling iterations, TDAS\ncan still generate good quality images. In contrast, the existing methods\ndegrade drastically or even fails completely\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hengyuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingfeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianfeng Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Dictionary Learning for Anomaly Detection. (arXiv:2003.00293v2 [cs.LG] CROSS LISTED)","link":"http://arxiv.org/abs/2003.00293","description":"<p>We investigate the possibilities of employing dictionary learning to address\nthe requirements of most anomaly detection applications, such as absence of\nsupervision, online formulations, low false positive rates. We present new\nresults of our recent semi-supervised online algorithm, TODDLeR, on a\nanti-money laundering application. We also introduce a novel unsupervised\nmethod of using the performance of the learning algorithm as indication of the\nnature of the samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irofti_P/0/1/0/all/0/1\">Paul Irofti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baltoiu_A/0/1/0/all/0/1\">Andra B&#x103;ltoiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-06-09T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}