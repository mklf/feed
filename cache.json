{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Active Learning Helps Pretrained Models Learn the Intended Task. (arXiv:2204.08491v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08491","description":"<p>Models can fail in unpredictable ways during deployment due to task\nambiguity, when multiple behaviors are consistent with the provided training\ndata. An example is an object classifier trained on red squares and blue\ncircles: when encountering blue squares, the intended behavior is undefined. We\ninvestigate whether pretrained models are better active learners, capable of\ndisambiguating between the possible tasks a user may be trying to specify.\nIntriguingly, we find that better active learning is an emergent property of\nthe pretraining process: pretrained models require up to 5 times fewer labels\nwhen using uncertainty-based active learning, while non-pretrained models see\nno or even negative benefit. We find these gains come from an ability to select\nexamples with attributes that disambiguate the intended behavior, such as rare\nproduct categories or atypical backgrounds. These attributes are far more\nlinearly separable in pretrained model's representation spaces vs\nnon-pretrained models, suggesting a possible mechanism for this behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_S/0/1/0/all/0/1\">Salil Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imagination-Augmented Natural Language Understanding. (arXiv:2204.08535v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08535","description":"<p>Human brains integrate linguistic and perceptual information simultaneously\nto understand natural language, and hold the critical ability to render\nimaginations. Such abilities enable us to construct new abstract concepts or\nconcrete objects, and are essential in involving practical knowledge to solve\nproblems in low-resource scenarios. However, most existing methods for Natural\nLanguage Understanding (NLU) are mainly focused on textual signals. They do not\nsimulate human visual imagination ability, which hinders models from inferring\nand learning efficiently from limited data samples. Therefore, we introduce an\nImagination-Augmented Cross-modal Encoder (iACE) to solve natural language\nunderstanding tasks from a novel learning perspective -- imagination-augmented\ncross-modal understanding. iACE enables visual imagination with external\nknowledge transferred from the powerful generative and pre-trained\nvision-and-language models. Extensive experiments on GLUE and SWAG show that\niACE achieves consistent improvement over visually-supervised pre-trained\nmodels. More importantly, results in extreme and normal few-shot settings\nvalidate the effectiveness of iACE in low-resource natural language\nunderstanding circumstances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yujie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanrong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_M/0/1/0/all/0/1\">Miguel Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CBR-iKB: A Case-Based Reasoning Approach for Question Answering over Incomplete Knowledge Bases. (arXiv:2204.08554v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08554","description":"<p>Knowledge bases (KBs) are often incomplete and constantly changing in\npractice. Yet, in many question answering applications coupled with knowledge\nbases, the sparse nature of KBs is often overlooked. To this end, we propose a\ncase-based reasoning approach, CBR-iKB, for knowledge base question answering\n(KBQA) with incomplete-KB as our main focus. Our method ensembles decisions\nfrom multiple reasoning chains with a novel nonparametric reasoning algorithm.\nBy design, CBR-iKB can seamlessly adapt to changes in KBs without any\ntask-specific training or fine-tuning. Our method achieves 100% accuracy on\nMetaQA and establishes new state-of-the-art on multiple benchmarks. For\ninstance, CBR-iKB achieves an accuracy of 70% on WebQSP under the incomplete-KB\nsetting, outperforming the existing state-of-the-art method by 22.3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thai_D/0/1/0/all/0/1\">Dung Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1\">Srinivas Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_M/0/1/0/all/0/1\">Mudit Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1\">Tahira Naseem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1\">Rajarshi Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1\">Achille Fokoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MASSIVE: A 1M-Example Multilingual Natural Language Understanding Dataset with 51 Typologically-Diverse Languages. (arXiv:2204.08582v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08582","description":"<p>We present the MASSIVE dataset--Multilingual Amazon Slu resource package\n(SLURP) for Slot-filling, Intent classification, and Virtual assistant\nEvaluation. MASSIVE contains 1M realistic, parallel, labeled virtual assistant\nutterances spanning 51 languages, 18 domains, 60 intents, and 55 slots. MASSIVE\nwas created by tasking professional translators to localize the English-only\nSLURP dataset into 50 typologically diverse languages from 29 genera. We also\npresent modeling results on XLM-R and mT5, including exact match accuracy,\nintent classification accuracy, and slot-filling F1 score. We have released our\ndataset, modeling code, and models publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+FitzGerald_J/0/1/0/all/0/1\">Jack FitzGerald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hench_C/0/1/0/all/0/1\">Christopher Hench</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peris_C/0/1/0/all/0/1\">Charith Peris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mackie_S/0/1/0/all/0/1\">Scott Mackie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_K/0/1/0/all/0/1\">Kay Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_A/0/1/0/all/0/1\">Ana Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_A/0/1/0/all/0/1\">Aaron Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbach_L/0/1/0/all/0/1\">Liam Urbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakarala_V/0/1/0/all/0/1\">Vishesh Kakarala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Richa Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_S/0/1/0/all/0/1\">Swetha Ranganath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crist_L/0/1/0/all/0/1\">Laurie Crist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Britan_M/0/1/0/all/0/1\">Misha Britan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeuwis_W/0/1/0/all/0/1\">Wouter Leeuwis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tur_G/0/1/0/all/0/1\">Gokhan Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Syntax-aware Language Modeling through Dependency Tree Conversion. (arXiv:2204.08644v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08644","description":"<p>Incorporating stronger syntactic biases into neural language models (LMs) is\na long-standing goal, but research in this area often focuses on modeling\nEnglish text, where constituent treebanks are readily available. Extending\nconstituent tree-based LMs to the multilingual setting, where dependency\ntreebanks are more common, is possible via dependency-to-constituency\nconversion methods. However, this raises the question of which tree formats are\nbest for learning the model, and for which languages. We investigate this\nquestion by training recurrent neural network grammars (RNNGs) using various\nconversion methods, and evaluating them empirically in a multilingual setting.\nWe examine the effect on LM performance across nine conversion methods and five\nlanguages through seven types of syntactic tests. On average, the performance\nof our best model represents a 19 \\% increase in accuracy over the worst choice\nacross all languages. Our best model shows the advantage over\nsequential/overparameterized LMs, suggesting the positive effect of syntax\ninjection in a multilingual setting. Our experiments highlight the importance\nof choosing the right tree formalism, and provide insights into making an\ninformed decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kando_S/0/1/0/all/0/1\">Shunsuke Kando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noji_H/0/1/0/all/0/1\">Hiroshi Noji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miyao_Y/0/1/0/all/0/1\">Yusuke Miyao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LitMC-BERT: transformer-based multi-label classification of biomedical literature with an application on COVID-19 literature curation. (arXiv:2204.08649v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08649","description":"<p>The rapid growth of biomedical literature poses a significant challenge for\ncuration and interpretation. This has become more evident during the COVID-19\npandemic. LitCovid, a literature database of COVID-19 related papers in PubMed,\nhas accumulated over 180,000 articles with millions of accesses. Approximately\n10,000 new articles are added to LitCovid every month. A main curation task in\nLitCovid is topic annotation where an article is assigned with up to eight\ntopics, e.g., Treatment and Diagnosis. The annotated topics have been widely\nused both in LitCovid (e.g., accounting for ~18% of total uses) and downstream\nstudies such as network generation. However, it has been a primary curation\nbottleneck due to the nature of the task and the rapid literature growth. This\nstudy proposes LITMC-BERT, a transformer-based multi-label classification\nmethod in biomedical literature. It uses a shared transformer backbone for all\nthe labels while also captures label-specific features and the correlations\nbetween label pairs. We compare LITMC-BERT with three baseline models on two\ndatasets. Its micro-F1 and instance-based F1 are 5% and 4% higher than the\ncurrent best results, respectively, and only requires ~18% of the inference\ntime than the Binary BERT baseline. The related datasets and models are\navailable via https://github.com/ncbi/ml-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allot_A/0/1/0/all/0/1\">Alexis Allot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On The Cross-Modal Transfer from Natural Language to Code through Adapter Modules. (arXiv:2204.08653v1 [cs.SE])","link":"http://arxiv.org/abs/2204.08653","description":"<p>Pre-trained neural Language Models (PTLM), such as CodeBERT, are recently\nused in software engineering as models pre-trained on large source code\ncorpora. Their knowledge is transferred to downstream tasks (e.g. code clone\ndetection) via fine-tuning. In natural language processing (NLP), other\nalternatives for transferring the knowledge of PTLMs are explored through using\nadapters, compact, parameter efficient modules inserted in the layers of the\nPTLM. Although adapters are known to facilitate adapting to many downstream\ntasks compared to fine-tuning the model that require retraining all of the\nmodels' parameters -- which owes to the adapters' plug and play nature and\nbeing parameter efficient -- their usage in software engineering is not\nexplored.\n</p>\n<p>Here, we explore the knowledge transfer using adapters and based on the\nNaturalness Hypothesis proposed by Hindle et. al \\cite{hindle2016naturalness}.\nThus, studying the bimodality of adapters for two tasks of cloze test and code\nclone detection, compared to their benchmarks from the CodeXGLUE platform.\nThese adapters are trained using programming languages and are inserted in a\nPTLM that is pre-trained on English corpora (N-PTLM). Three programming\nlanguages, C/C++, Python, and Java, are studied along with extensive\nexperiments on the best setup used for adapters. Improving the results of the\nN-PTLM confirms the success of the adapters in knowledge transfer to software\nengineering, which sometimes are in par with or exceed the results of a PTLM\ntrained on source code; while being more efficient in terms of the number of\nparameters, memory usage, and inference time. Our results can open new\ndirections to build smaller models for more software engineering tasks. We open\nsource all the scripts and the trained adapters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goel_D/0/1/0/all/0/1\">Divyam Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_R/0/1/0/all/0/1\">Ramansh Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_F/0/1/0/all/0/1\">Fatemeh H. Fard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mono vs Multilingual BERT for Hate Speech Detection and Text Classification: A Case Study in Marathi. (arXiv:2204.08669v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08669","description":"<p>Transformers are the most eminent architectures used for a vast range of\nNatural Language Processing tasks. These models are pre-trained over a large\ntext corpus and are meant to serve state-of-the-art results over tasks like\ntext classification. In this work, we conduct a comparative study between\nmonolingual and multilingual BERT models. We focus on the Marathi language and\nevaluate the models on the datasets for hate speech detection, sentiment\nanalysis and simple text classification in Marathi. We use standard\nmultilingual models such as mBERT, indicBERT and xlm-RoBERTa and compare with\nMahaBERT, MahaALBERT and MahaRoBERTa, the monolingual models for Marathi. We\nfurther show that Marathi monolingual models outperform the multilingual BERT\nvariants on five different downstream fine-tuning experiments. We also evaluate\nsentence embeddings from these models by freezing the BERT encoder layers. We\nshow that monolingual MahaBERT based models provide rich representations as\ncompared to sentence embeddings from multi-lingual counterparts. However, we\nobserve that these embeddings are not generic enough and do not work well on\nout of domain social media datasets. We consider two Marathi hate speech\ndatasets L3Cube-MahaHate, HASOC-2021, a Marathi sentiment classification\ndataset L3Cube-MahaSent, and Marathi Headline, Articles classification\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velankar_A/0/1/0/all/0/1\">Abhishek Velankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Hrushikesh Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DecBERT: Enhancing the Language Understanding of BERT with Causal Attention Masks. (arXiv:2204.08688v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08688","description":"<p>Since 2017, the Transformer-based models play critical roles in various\ndownstream Natural Language Processing tasks. However, a common limitation of\nthe attention mechanism utilized in Transformer Encoder is that it cannot\nautomatically capture the information of word order, so explicit position\nembeddings are generally required to be fed into the target model. In contrast,\nTransformer Decoder with the causal attention masks is naturally sensitive to\nthe word order. In this work, we focus on improving the position encoding\nability of BERT with the causal attention masks. Furthermore, we propose a new\npre-trained language model DecBERT and evaluate it on the GLUE benchmark.\nExperimental results show that (1) the causal attention mask is effective for\nBERT on the language understanding tasks; (2) our DecBERT model without\nposition embeddings achieve comparable performance on the GLUE benchmark; and\n(3) our modification accelerates the pre-training process and DecBERT w/ PE\nachieves better overall performance than the baseline systems when pre-training\nwith the same amount of computational resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xiaoxi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Changjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Authentic Adversarial Examples beyond Meaning-preserving with Doubly Round-trip Translation. (arXiv:2204.08689v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08689","description":"<p>Generating adversarial examples for Neural Machine Translation (NMT) with\nsingle Round-Trip Translation (RTT) has achieved promising results by releasing\nthe meaning-preserving restriction. However, a potential pitfall for this\napproach is that we cannot decide whether the generated examples are\nadversarial to the target NMT model or the auxiliary backward one, as the\nreconstruction error through the RTT can be related to either. To remedy this\nproblem, we propose a new criterion for NMT adversarial examples based on the\nDoubly Round-Trip Translation (DRTT). Specifically, apart from the\nsource-target-source RTT, we also consider the target-source-target one, which\nis utilized to pick out the authentic adversarial examples for the target NMT\nmodel. Additionally, to enhance the robustness of the NMT model, we introduce\nthe masked language models to construct bilingual adversarial pairs based on\nDRTT, which are used to train the NMT model directly. Extensive experiments on\nboth the clean and noisy test sets (including the artificial and natural noise)\nshow that our approach substantially improves the robustness of NMT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Siyu Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table-based Fact Verification with Self-adaptive Mixture of Experts. (arXiv:2204.08753v1 [cs.AI])","link":"http://arxiv.org/abs/2204.08753","description":"<p>The table-based fact verification task has recently gained widespread\nattention and yet remains to be a very challenging problem. It inherently\nrequires informative reasoning over natural language together with different\nnumerical and logical reasoning on tables (e.g., count, superlative,\ncomparative). Considering that, we exploit mixture-of-experts and present in\nthis paper a new method: Self-adaptive Mixture-of-Experts Network (SaMoE).\nSpecifically, we have developed a mixture-of-experts neural network to\nrecognize and execute different types of reasoning -- the network is composed\nof multiple experts, each handling a specific part of the semantics for\nreasoning, whereas a management module is applied to decide the contribution of\neach expert network to the verification result. A self-adaptive method is\ndeveloped to teach the management module combining results of different experts\nmore efficiently without external knowledge. The experimental results\nillustrate that our framework achieves 85.1% accuracy on the benchmark dataset\nTabFact, comparable with the previous state-of-the-art models. We hope our\nframework can serve as a new baseline for table-based verification. Our code is\navailable at https://github.com/THUMLP/SaMoE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xien Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Ji Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndicXNLI: Evaluating Multilingual Inference for Indian Languages. (arXiv:2204.08776v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08776","description":"<p>While Indic NLP has made rapid advances recently in terms of the availability\nof corpora and pre-trained models, benchmark datasets on standard NLU tasks are\nlimited. To this end, we introduce IndicXNLI, an NLI dataset for 11 Indic\nlanguages. It has been created by high-quality machine translation of the\noriginal English XNLI dataset and our analysis attests to the quality of\nIndicXNLI. By finetuning different pre-trained LMs on this IndicXNLI, we\nanalyze various cross-lingual transfer techniques with respect to the impact of\nthe choice of language models, languages, multi-linguality, mix-language input,\netc. These experiments provide us with useful insights into the behaviour of\npre-trained models for a diverse set of languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_D/0/1/0/all/0/1\">Divyanshu Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1\">Vivek Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Where Was COVID-19 First Discovered? Designing a Question-Answering System for Pandemic Situations. (arXiv:2204.08787v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08787","description":"<p>The COVID-19 pandemic is accompanied by a massive \"infodemic\" that makes it\nhard to identify concise and credible information for COVID-19-related\nquestions, like incubation time, infection rates, or the effectiveness of\nvaccines. As a novel solution, our paper is concerned with designing a\nquestion-answering system based on modern technologies from natural language\nprocessing to overcome information overload and misinformation in pandemic\nsituations. To carry out our research, we followed a design science research\napproach and applied Ingwersen's cognitive model of information retrieval\ninteraction to inform our design process from a socio-technical lens. On this\nbasis, we derived prescriptive design knowledge in terms of design requirements\nand design principles, which we translated into the construction of a\nprototypical instantiation. Our implementation is based on the comprehensive\nCORD-19 dataset, and we demonstrate our artifact's usefulness by evaluating its\nanswer quality based on a sample of COVID-19 questions labeled by biomedical\nexperts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graf_J/0/1/0/all/0/1\">Johannes Graf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lancho_G/0/1/0/all/0/1\">Gino Lancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zschech_P/0/1/0/all/0/1\">Patrick Zschech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_K/0/1/0/all/0/1\">Kai Heinrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08790","description":"<p>Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets/tasks. However, it remains a challenge to evaluate the\ntransferablity of these foundation models due to the lack of easy-to-use\ntoolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark to\ncompare and evaluate pre-trained language-augmented visual models. Several\nhighlights include: (i) Datasets. As downstream evaluation suites, it consists\nof 20 image classification datasets and 35 object detection datasets, each of\nwhich is augmented with external knowledge. (ii) Toolkit. An automatic\nhyper-parameter tuning toolkit is developed to ensure the fairness in model\nadaption. To leverage the full power of language-augmented visual models, novel\nlanguage-aware initialization methods are proposed to significantly improve the\nadaption performance. (iii) Metrics. A variety of evaluation metrics are used,\nincluding sample-efficiency (zero-shot and few-shot) and parameter-efficiency\n(linear probing and full model fine-tuning). We will release our toolkit and\nevaluation platforms for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Ping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Toxicity Triggers on Reddit in the Context of Singapore. (arXiv:2204.08806v1 [cs.CY])","link":"http://arxiv.org/abs/2204.08806","description":"<p>While the contagious nature of online toxicity sparked increasing interest in\nits early detection and prevention, most of the literature focuses on the\nWestern world. In this work, we demonstrate that 1) it is possible to detect\ntoxicity triggers in an Asian online community, and 2) toxicity triggers can be\nstrikingly different between Western and Eastern contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1\">Yun Yu Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1\">Haewoon Kwak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SmartSales: Sales Script Extraction and Analysis from Sales Chatlog. (arXiv:2204.08811v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08811","description":"<p>In modern sales applications, automatic script extraction and management\ngreatly decrease the need for human labor to collect the winning sales scripts,\nwhich largely boost the success rate for sales and can be shared across the\nsales teams. In this work, we present the SmartSales system to serve both the\nsales representatives and managers to attain the sales insights from the\nlarge-scale sales chatlog. SmartSales consists of three modules: 1) Customer\nfrequently asked questions (FAQ) extraction aims to enrich the FAQ knowledge\nbase by harvesting high quality customer question-answer pairs from the\nchatlog. 2) Customer objection response assists the salespeople to figure out\nthe typical customer objections and corresponding winning sales scripts, as\nwell as search for proper sales responses for a certain customer objection. 3)\nSales manager dashboard helps sales managers to monitor whether a specific\nsales representative or team follows the sales standard operating procedures\n(SOP). The proposed prototype system is empowered by the state-of-the-art\nconversational intelligence techniques and has been running on the Tencent\nCloud to serve the sales teams from several different areas.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hua Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Mengliang Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing for the Usage of Grammatical Number. (arXiv:2204.08831v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08831","description":"<p>A central quest of probing is to uncover how pre-trained models encode a\nlinguistic property within their representations. An encoding, however, might\nbe spurious-i.e., the model might not rely on it when making predictions. In\nthis paper, we try to find encodings that the model actually uses, introducing\na usage-based probing setup. We first choose a behavioral task which cannot be\nsolved without using the linguistic property. Then, we attempt to remove the\nproperty by intervening on the model's representations. We contend that, if an\nencoding is used by the model, its removal should harm the performance on the\nchosen behavioral task. As a case study, we focus on how BERT encodes\ngrammatical number, and on how it uses this encoding to solve the number\nagreement task. Experimentally, we find that BERT relies on a linear encoding\nof grammatical number to produce the correct behavioral output. We also find\nthat BERT uses a separate encoding of grammatical number for nouns and verbs.\nFinally, we identify in which layers information about grammatical number is\ntransferred from a noun to its head verb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasri_K/0/1/0/all/0/1\">Karim Lasri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poibeau_T/0/1/0/all/0/1\">Thierry Poibeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Impact of Tokenization on Language Models: An Analysis for Turkish. (arXiv:2204.08832v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08832","description":"<p>Tokenization is an important text preprocessing step to prepare input tokens\nfor deep language models. WordPiece and BPE are de facto methods employed by\nimportant models, such as BERT and GPT. However, the impact of tokenization can\nbe different for morphologically rich languages, such as Turkic languages,\nwhere many words can be generated by adding prefixes and suffixes. We compare\nfive tokenizers at different granularity levels, i.e. their outputs vary from\nsmallest pieces of characters to the surface form of words, including a\nMorphological-level tokenizer. We train these tokenizers and pretrain\nmedium-sized language models using RoBERTa pretraining procedure on the Turkish\nsplit of the OSCAR corpus. We then fine-tune our models on six downstream\ntasks. Our experiments, supported by statistical tests, reveal that\nMorphological-level tokenizer has challenging performance with de facto\ntokenizers. Furthermore, we find that increasing the vocabulary size improves\nthe performance of Morphological and Word-level tokenizers more than that of de\nfacto tokenizers. The ratio of the number of vocabulary parameters to the total\nnumber of model parameters can be empirically chosen as 20% for de facto\ntokenizers and 40% for other tokenizers to obtain a reasonable trade-off\nbetween model size and performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Eyup Halit Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahinuc_F/0/1/0/all/0/1\">Furkan &#x15e;ahinu&#xe7;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozcelik_O/0/1/0/all/0/1\">Oguzhan Ozcelik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I still have Time(s): Extending HeidelTime for German Texts. (arXiv:2204.08848v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08848","description":"<p>HeidelTime is one of the most widespread and successful tools for detecting\ntemporal expressions in texts. Since HeidelTime's pattern matching system is\nbased on regular expression, it can be extended in a convenient way. We present\nsuch an extension for the German resources of HeidelTime: HeidelTime-EXT . The\nextension has been brought about by means of observing false negatives within\nreal world texts and various time banks. The gain in coverage is 2.7% or 8.5%,\ndepending on the admitted degree of potential overgeneralization. We describe\nthe development of HeidelTime-EXT, its evaluation on text samples from various\ngenres, and share some linguistic observations. HeidelTime ext can be obtained\nfrom https://github.com/texttechnologylab/heideltime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucking_A/0/1/0/all/0/1\">Andy L&#xfc;cking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoeckel_M/0/1/0/all/0/1\">Manuel Stoeckel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrami_G/0/1/0/all/0/1\">Giuseppe Abrami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehler_A/0/1/0/all/0/1\">Alexander Mehler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATP: AMRize Then Parse! Enhancing AMR Parsing with PseudoAMRs. (arXiv:2204.08875v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08875","description":"<p>As Abstract Meaning Representation (AMR) implicitly involves compound\nsemantic annotations, we hypothesize auxiliary tasks which are semantically or\nformally related can better enhance AMR parsing. We find that 1) Semantic role\nlabeling (SRL) and dependency parsing (DP), would bring more performance gain\nthan other tasks e.g. MT and summarization in the text-to-AMR transition even\nwith much less data. 2) To make a better fit for AMR, data from auxiliary tasks\nshould be properly \"AMRized\" to PseudoAMR before training. Knowledge from\nshallow level parsing tasks can be better transferred to AMR Parsing with\nstructure transform. 3) Intermediate-task learning is a better paradigm to\nintroduce auxiliary tasks to AMR parsing, compared to multitask learning. From\nan empirical perspective, we propose a principled method to involve auxiliary\ntasks to boost AMR parsing. Extensive experiments show that our method achieves\nnew state-of-the-art performance on different benchmarks especially in\ntopology-related scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peiyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Runxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tianyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Baobao Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Phrase Retrieval. (arXiv:2204.08887v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08887","description":"<p>Cross-lingual retrieval aims to retrieve relevant text across languages.\nCurrent methods typically achieve cross-lingual retrieval by learning\nlanguage-agnostic text representations in word or sentence level. However, how\nto learn phrase representations for cross-lingual phrase retrieval is still an\nopen problem. In this paper, we propose XPR, a cross-lingual phrase retriever\nthat extracts phrase representations from unlabeled example sentences.\nMoreover, we create a large-scale cross-lingual phrase retrieval dataset, which\ncontains 65K bilingual phrase pairs and 4.2M example sentences in 8\nEnglish-centric language pairs. Experimental results show that XPR outperforms\nstate-of-the-art baselines which utilize word-level or sentence-level\nrepresentations. XPR also shows impressive zero-shot transferability that\nenables the model to perform retrieval in an unseen language pair during\ntraining. Our dataset, code, and trained models are publicly available at\nwww.github.com/cwszz/XPR/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Heqi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_T/0/1/0/all/0/1\">Tan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1\">Tian Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A survey on improving NLP models with human explanations. (arXiv:2204.08892v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08892","description":"<p>Training a model with access to human explanations can improve data\nefficiency and model performance on in- and out-of-domain data. Adding to these\nempirical findings, similarity with the process of human learning makes\nlearning from explanations a promising way to establish a fruitful\nhuman-machine interaction. Several methods have been proposed for improving\nnatural language processing (NLP) models with human explanations, that rely on\ndifferent explanation types and mechanism for integrating these explanations\ninto the learning process. These methods are rarely compared with each other,\nmaking it hard for practitioners to choose the best combination of explanation\ntype and integration mechanism for a specific use-case. In this paper, we give\nan overview of different methods for learning from human explanations, and\ndiscuss different factors that can inform the decision of which method to\nchoose for a specific use-case.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1\">Mareike Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation. (arXiv:2204.08920v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08920","description":"<p>Although Transformers have gained success in several speech processing tasks\nlike spoken language understanding (SLU) and speech translation (ST), achieving\nonline processing while keeping competitive performance is still essential for\nreal-world interaction. In this paper, we take the first step on streaming SLU\nand simultaneous ST using a blockwise streaming Transformer, which is based on\ncontextual block processing and blockwise synchronous beam search. Furthermore,\nwe design an automatic speech recognition (ASR)-based intermediate loss\nregularization for the streaming SLU task to improve the classification\nperformance further. As for the simultaneous ST task, we propose a\ncross-lingual encoding method, which employs a CTC branch optimized with target\nlanguage translations. In addition, the CTC translation output is also used to\nrefine the search space with CTC prefix score, achieving joint CTC/attention\nsimultaneous translation for the first time. Experiments for SLU are conducted\non FSC and SLURP corpora, while the ST task is evaluated on Fisher-CallHome\nSpanish and MuST-C En-De corpora. Experimental results show that the blockwise\nstreaming Transformer achieves competitive results compared to offline models,\nespecially with our proposed methods that further yield a 2.4% accuracy gain on\nthe SLU task and a 4.3 BLEU gain on the ST task over streaming baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1\">Keqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Structure Distillation for BERT Transferring. (arXiv:2204.08922v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08922","description":"<p>Knowledge distillation is an approach to transfer information on\nrepresentations from a teacher to a student by reducing their difference. A\nchallenge of this approach is to reduce the flexibility of the student's\nrepresentations inducing inaccurate learning of the teacher's knowledge. To\nresolve it in BERT transferring, we investigate distillation of structures of\nrepresentations specified to three types: intra-feature, local inter-feature,\nglobal inter-feature structures. To transfer them, we introduce \\textit{feature\nstructure distillation} methods based on the Centered Kernel Alignment, which\nassigns a consistent value to similar features structures and reveals more\ninformative relations. In particular, a memory-augmented transfer method with\nclustering is implemented for the global structures. In the experiments on the\nnine tasks for language understanding of the GLUE dataset, the proposed methods\neffectively transfer the three types of structures and improve performance\ncompared to state-of-the-art distillation methods. Indeed, the code for the\nmethods is available in https://github.com/maroo-sky/FSD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hee-Jun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Na_S/0/1/0/all/0/1\">Seung-Hoon Na</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangil Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodexDB: Generating Code for Processing SQL Queries using GPT-3 Codex. (arXiv:2204.08941v1 [cs.DB])","link":"http://arxiv.org/abs/2204.08941","description":"<p>CodexDB is an SQL processing engine whose internals can be customized via\nnatural language instructions. CodexDB is based on OpenAI's GPT-3 Codex model\nwhich translates text into code. It is a framework on top of GPT-3 Codex that\ndecomposes complex SQL queries into a series of simple processing steps,\ndescribed in natural language. Processing steps are enriched with user-provided\ninstructions and descriptions of database properties. Codex translates the\nresulting text into query processing code. An early prototype of CodexDB is\nable to generate correct code for a majority of queries of the WikiSQL\nbenchmark and can be customized in various ways.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1\">Immanuel Trummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Enhanced Data Augmentation for Question Answering on Privacy Policies. (arXiv:2204.08952v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08952","description":"<p>Prior studies in privacy policies frame the question answering (QA) tasks as\nidentifying the most relevant text segment or a list of sentences from the\npolicy document for a user query. However, annotating such a dataset is\nchallenging as it requires specific domain expertise (e.g., law academics).\nEven if we manage a small-scale one, a bottleneck that remains is that the\nlabeled data are heavily imbalanced (only a few segments are relevant)\n--limiting the gain in this domain. Therefore, in this paper, we develop a\nnovel data augmentation framework based on ensembling retriever models that\ncaptures the relevant text segments from unlabeled policy documents and expand\nthe positive examples in the training set. In addition, to improve the\ndiversity and quality of the augmented data, we leverage multiple pre-trained\nlanguage models (LMs) and cascaded them with noise reduction oracles. Using our\naugmented data on the PrivacyQA benchmark, we elevate the existing baseline by\na large margin (10\\% F1) and achieve a new state-of-the-art F1 score of 50\\%.\nOur ablation studies provide further insights into the effectiveness of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1\">Jianfeng Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building Odia Shallow Parser. (arXiv:2204.08960v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08960","description":"<p>Shallow parsing is an essential task for many NLP applications like machine\ntranslation, summarization, sentiment analysis, aspect identification and many\nmore. Quality annotated corpora is critical for building accurate shallow\nparsers. Many Indian languages are resource poor with respect to the\navailability of corpora in general. So, this paper is an attempt towards\ncreating quality corpora for shallow parsers. The contribution of this paper is\ntwo folds: creation pos and chunk annotated corpora for Odia and development of\nbaseline systems for pos tagging and chunking in Odia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Pruthwik Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dipti Misra Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Text Formality: A Study of Text Classification Approaches. (arXiv:2204.08975v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08975","description":"<p>Formality is an important characteristic of text documents. The automatic\ndetection of the formality level of a text is potentially beneficial for\nvarious natural language processing tasks, such as retrieval of texts with a\ndesired formality level, integration in language learning and document editing\nplatforms, or evaluating the desired conversation tone by chatbots. Recently\ntwo large-scale datasets were introduced for multiple languages featuring\nformality annotation. However, they were primarily used for the training of\nstyle transfer models. However, detection text formality on its own may also be\na useful application. This work proposes the first systematic study of\nformality detection methods based on current (and more classic) machine\nlearning methods and delivers the best-performing models for public usage. We\nconducted three types of experiments -- monolingual, multilingual, and\ncross-lingual. The study shows the overcome of BiLSTM-based models over\ntransformer-based ones for the formality classification task. We release\nformality detection models for several languages yielding state of the art\nresults and possessing tested cross-lingual capabilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dementieva_D/0/1/0/all/0/1\">Daryna Dementieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trifinov_I/0/1/0/all/0/1\">Ivan Trifinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhachev_A/0/1/0/all/0/1\">Andrey Likhachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Benchmark for Automatic Medical Consultation System: Frameworks, Tasks and Datasets. (arXiv:2204.08997v1 [cs.CL])","link":"http://arxiv.org/abs/2204.08997","description":"<p>In recent years, interest has arisen in using machine learning to improve the\nefficiency of automatic medical consultation and enhance patient experience. In\nthis paper, we propose two frameworks to support automatic medical\nconsultation, namely doctor-patient dialogue understanding and task-oriented\ninteraction. A new large medical dialogue dataset with multi-level fine-grained\nannotations is introduced and five independent tasks are established, including\nnamed entity recognition, dialogue act classification, symptom label inference,\nmedical report generation and diagnosis-oriented dialogue policy. We report a\nset of benchmark results for each task, which shows the usability of the\ndataset and sets a baseline for future studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hongyi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1\">Qianyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1\">Jianye Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">J iajie Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Locality of Attention in Direct Speech Translation. (arXiv:2204.09028v1 [cs.CL])","link":"http://arxiv.org/abs/2204.09028","description":"<p>Transformers have achieved state-of-the-art results across multiple NLP\ntasks. However, the self-attention mechanism complexity scales quadratically\nwith the sequence length, creating an obstacle for tasks involving long\nsequences, like in the speech domain. In this paper, we discuss the usefulness\nof self-attention for Direct Speech Translation. First, we analyze the\nlayer-wise token contributions in the self-attention of the encoder, unveiling\nlocal diagonal patterns. To prove that some attention weights are avoidable, we\npropose to substitute the standard self-attention with a local efficient one,\nsetting the amount of context used based on the results of the analysis. With\nthis approach, our model matches the baseline performance, and improves the\nefficiency by skipping the computation of those weights that standard attention\ndiscards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alastruey_B/0/1/0/all/0/1\">Belen Alastruey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1\">Javier Ferrando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning. (arXiv:1910.09134v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1910.09134","description":"<p>Multiple-choice VQA has drawn increasing attention from researchers and\nend-users recently. As the demand for automatically constructing large-scale\nmultiple-choice VQA data grows, we introduce a novel task called textual\nDistractors Generation for VQA (DG-VQA) focusing on generating challenging yet\nmeaningful distractors given the context image, question, and correct answer.\nThe DG-VQA task aims at generating distractors without ground-truth training\nsamples since such resources are rarely available. To tackle the DG-VQA\nunsupervisedly, we propose Gobbet, a reinforcement learning(RL) based framework\nthat utilizes pre-trained VQA models as an alternative knowledge base to guide\nthe distractor generation process. In Gobbet, a pre-trained VQA model serves as\nthe environment in RL setting to provide feedback for the input multi-modal\nquery, while a neural distractor generator serves as the agent to take actions\naccordingly. We propose to use existing VQA models' performance degradation as\nindicators of the quality of generated distractors. On the other hand, we show\nthe utility of generated distractors through data augmentation experiments,\nsince robustness is more and more important when AI models apply to\nunpredictable open-domain scenarios or security-sensitive applications. We\nfurther conduct a manual case study on the factors why distractors generated by\nGobbet can fool existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is BERT a Cross-Disciplinary Knowledge Learner? A Surprising Finding of Pre-trained Models' Transferability. (arXiv:2103.07162v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07162","description":"<p>This paper investigates whether the power of the models pre-trained on text\ndata, such as BERT, can be transferred to general token sequence classification\napplications. To verify pre-trained models' transferability, we test the\npre-trained models on text classification tasks with meanings of tokens\nmismatches, and real-world non-text token sequence classification data,\nincluding amino acid, DNA, and music. We find that even on non-text data, the\nmodels pre-trained on text converge faster, perform better than the randomly\ninitialized models, and only slightly worse than the models using task-specific\nknowledge. We also find that the representations of the text and non-text\npre-trained models share non-trivial similarities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kao_W/0/1/0/all/0/1\">Wei-Tsung Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLM-E: Cross-lingual Language Model Pre-training via ELECTRA. (arXiv:2106.16138v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.16138","description":"<p>In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdapterHub Playground: Simple and Flexible Few-Shot Learning with Adapters. (arXiv:2108.08103v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08103","description":"<p>The open-access dissemination of pretrained language models through online\nrepositories has led to a democratization of state-of-the-art natural language\nprocessing (NLP) research. This also allows people outside of NLP to use such\nmodels and adapt them to specific use-cases. However, a certain amount of\ntechnical proficiency is still required which is an entry barrier for users who\nwant to apply these models to a certain task but lack the necessary knowledge\nor resources. In this work, we aim to overcome this gap by providing a tool\nwhich allows researchers to leverage pretrained models without writing a single\nline of code. Built upon the parameter-efficient adapter modules for transfer\nlearning, our AdapterHub Playground provides an intuitive interface, allowing\nthe usage of adapters for prediction, training and analysis of textual data for\na variety of NLP tasks. We present the tool's architecture and demonstrate its\nadvantages with prototypical use-cases, where we show that predictive\nperformance can easily be increased in a few-shot learning scenario. Finally,\nwe evaluate its usability in a user study. We provide the code and a live\ninterface at https://adapter-hub.github.io/playground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beck_T/0/1/0/all/0/1\">Tilman Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bohlender_B/0/1/0/all/0/1\">Bela Bohlender</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viehmann_C/0/1/0/all/0/1\">Christina Viehmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hane_V/0/1/0/all/0/1\">Vincent Hane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adamson_Y/0/1/0/all/0/1\">Yanik Adamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khuri_J/0/1/0/all/0/1\">Jaber Khuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brossmann_J/0/1/0/all/0/1\">Jonas Brossmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting and Inferring Personal Attributes from Dialogue. (arXiv:2109.12702v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12702","description":"<p>Personal attributes represent structured information about a person, such as\ntheir hobbies, pets, family, likes and dislikes. We introduce the tasks of\nextracting and inferring personal attributes from human-human dialogue, and\nanalyze the linguistic demands of these tasks. To meet these challenges, we\nintroduce a simple and extensible model that combines an autoregressive\nlanguage model utilizing constrained attribute generation with a discriminative\nreranker. Our model outperforms strong baselines on extracting personal\nattributes as well as inferring personal attributes that are not contained\nverbatim in utterances and instead requires commonsense reasoning and lexical\ninferences, which occur frequently in everyday conversation. Finally, we\ndemonstrate the benefit of incorporating personal attributes in social\nchit-chat and task-oriented dialogue settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuhui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koncel_Kedziorski_R/0/1/0/all/0/1\">Rik Koncel-Kedziorski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_A/0/1/0/all/0/1\">Alex Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P-Adapters: Robustly Extracting Factual Information from Language Models with Diverse Prompts. (arXiv:2110.07280v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07280","description":"<p>Recent work (e.g. LAMA (Petroni et al., 2019)) has found that the quality of\nthe factual information extracted from Large Language Models (LLMs) depends on\nthe prompts used to query them. This inconsistency is problematic because\ndifferent users will query LLMs for the same information using different\nwording, but should receive the same, accurate responses regardless. In this\nwork we aim to address this shortcoming by introducing P-Adapters: lightweight\nmodels that sit between the embedding layer and first attention layer of LLMs.\nThey take LLM embeddings as input and output continuous prompts that are used\nto query the LLM. Additionally, we investigate Mixture of Experts (MoE) models\nthat learn a set of continuous prompts (\"experts\") and select one to query the\nLLM. They require a separate classifier trained on human-annotated data to map\nnatural language prompts to the continuous ones. P-Adapters perform comparably\nto the more complex MoE models in extracting factual information from BERT and\nRoBERTa while eliminating the need for additional annotations. P-Adapters show\nbetween 12-26% absolute improvement in precision and 36-50% absolute\nimprovement in consistency over a baseline of only using natural language\nqueries. Finally, we investigate what makes P-Adapters successful and conclude\nthat a significant factor is access to the LLM's embeddings of the original\nnatural language prompt, particularly the subject of the entity pair being\nqueried.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Newman_B/0/1/0/all/0/1\">Benjamin Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1\">Prafulla Kumar Choubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Rajani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. (arXiv:2111.02735v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02735","description":"<p>Speech self-supervised models such as wav2vec 2.0 and HuBERT are making\nrevolutionary progress in Automatic Speech Recognition (ASR). However, they\nhave not been totally proved to produce better performance on tasks other than\nASR. In this work, we explore partial fine-tuning and entire fine-tuning on\nwav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks :\nSpeech Emotion Recognition, Speaker Verification and Spoken Language\nUnderstanding. With simple proposed down-stream frameworks, the best scores\nreach 79.58% weighted accuracy for Speech Emotion Recognition on IEMOCAP, 2.36%\nequal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for\nIntent Classification and 78.92% F1 for Slot Filling on SLURP, thus setting new\nstate-of-the-art on the three benchmarks, showing the strong power of\nfine-tuned wav2vec 2.0 and HuBERT models on learning prosodic, voice-print and\nsemantic representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yingzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boumadane_A/0/1/0/all/0/1\">Abdelmoumene Boumadane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heba_A/0/1/0/all/0/1\">Abdelwahab Heba</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Step-unrolled Denoising Autoencoders for Text Generation. (arXiv:2112.06749v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06749","description":"<p>In this paper we propose a new generative model of text, Step-unrolled\nDenoising Autoencoder (SUNDAE), that does not rely on autoregressive models.\nSimilarly to denoising diffusion techniques, SUNDAE is repeatedly applied on a\nsequence of tokens, starting from random inputs and improving them each time\nuntil convergence. We present a simple new improvement operator that converges\nin fewer iterations than diffusion methods, while qualitatively producing\nbetter samples on natural language datasets. SUNDAE achieves state-of-the-art\nresults (among non-autoregressive methods) on the WMT'14 English-to-German\ntranslation task and good qualitative results on unconditional language\nmodeling on the Colossal Cleaned Common Crawl dataset and a dataset of Python\ncode from GitHub. The non-autoregressive nature of SUNDAE opens up\npossibilities beyond left-to-right prompted generation, by filling in arbitrary\nblank patterns in a template.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savinov_N/0/1/0/all/0/1\">Nikolay Savinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Junyoung Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binkowski_M/0/1/0/all/0/1\">Mikolaj Binkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1\">Aaron van den Oord</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Key Multimodal Backdoors for Visual Question Answering. (arXiv:2112.07668v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07668","description":"<p>The success of deep learning has enabled advances in multimodal tasks that\nrequire non-trivial fusion of multiple input domains. Although multimodal\nmodels have shown potential in many problems, their increased complexity makes\nthem more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of\nsecurity vulnerability wherein an attacker embeds a malicious secret behavior\ninto a network (e.g. targeted misclassification) that is activated when an\nattacker-specified trigger is added to an input. In this work, we show that\nmultimodal networks are vulnerable to a novel type of attack that we refer to\nas Dual-Key Multimodal Backdoors. This attack exploits the complex fusion\nmechanisms used by state-of-the-art networks to embed backdoors that are both\neffective and stealthy. Instead of using a single trigger, the proposed attack\nembeds a trigger in each of the input modalities and activates the malicious\nbehavior only when both the triggers are present. We present an extensive study\nof multimodal backdoors on the Visual Question Answering (VQA) task with\nmultiple architectures and visual feature backbones. A major challenge in\nembedding backdoors in VQA models is that most models use visual features\nextracted from a fixed pretrained object detector. This is challenging for the\nattacker as the detector can distort or ignore the visual trigger entirely,\nwhich leads to models where backdoors are over-reliant on the language trigger.\nWe tackle this problem by proposing a visual trigger optimization strategy\ndesigned for pretrained object detectors. Through this method, we create\nDual-Key Backdoors with over a 98% attack success rate while only poisoning 1%\nof the training data. Finally, we release TrojVQA, a large collection of clean\nand trojan VQA models to enable research in defending against multimodal\nbackdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walmer_M/0/1/0/all/0/1\">Matthew Walmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1\">Karan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sur_I/0/1/0/all/0/1\">Indranil Sur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Table Pre-training: A Survey on Model Architectures, Pre-training Objectives, and Downstream Tasks. (arXiv:2201.09745v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09745","description":"<p>Since a vast number of tables can be easily collected from web pages,\nspreadsheets, PDFs, and various other document types, a flurry of table\npre-training frameworks have been proposed following the success of text and\nimages, and they have achieved new state-of-the-arts on various tasks such as\ntable question answering, table type recognition, column relation\nclassification, table search, formula prediction, etc. To fully use the\nsupervision signals in unlabeled tables, a variety of pre-training objectives\nhave been designed and evaluated, for example, denoising cell values,\npredicting numerical relationships, and implicitly executing SQLs. And to best\nleverage the characteristics of (semi-)structured tables, various tabular\nlanguage models, particularly with specially-designed attention mechanisms,\nhave been explored. Since tables usually appear and interact with free-form\ntext, table pre-training usually takes the form of table-text joint\npre-training, which attracts significant research interests from multiple\ndomains. This survey aims to provide a comprehensive review of different model\ndesigns, pre-training objectives, and downstream tasks for table pre-training,\nand we further share our thoughts and vision on existing challenges and future\nopportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhoujun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Anda Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Fan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Ao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Semi-supervised Learning Approach with Two Teachers to Improve Breakdown Identification in Dialogues. (arXiv:2202.10948v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.10948","description":"<p>Identifying breakdowns in ongoing dialogues helps to improve communication\neffectiveness. Most prior work on this topic relies on human annotated data and\ndata augmentation to learn a classification model. While quality labeled\ndialogue data requires human annotation and is usually expensive to obtain,\nunlabeled data is easier to collect from various sources. In this paper, we\npropose a novel semi-supervised teacher-student learning framework to tackle\nthis task. We introduce two teachers which are trained on labeled data and\nperturbed labeled data respectively. We leverage unlabeled data to improve\nclassification in student training where we employ two teachers to refine the\nlabeling of unlabeled data through teacher-student learning in a bootstrapping\nmanner. Through our proposed training approach, the student can achieve\nimprovements over single-teacher performance. Experimental results on the\nDialogue Breakdown Detection Challenge dataset DBDC5 and Learning to Identify\nFollow-Up Questions dataset LIF show that our approach outperforms all previous\npublished approaches as well as other supervised and semi-supervised baseline\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14260","description":"<p>Understanding realistic visual scene images together with language\ndescriptions is a fundamental task towards generic visual understanding.\nPrevious works have shown compelling comprehensive results by building\nhierarchical structures for visual scenes (e.g., scene graphs) and natural\nlanguages (e.g., dependency trees), individually. However, how to construct a\njoint vision-language (VL) structure has barely been investigated. More\nchallenging but worthwhile, we introduce a new task that targets on inducing\nsuch a joint VL structure in an unsupervised manner. Our goal is to bridge the\nvisual scene graphs and linguistic dependency trees seamlessly. Due to the lack\nof VL structural data, we start by building a new dataset VLParse. Rather than\nusing labor-intensive labeling from scratch, we propose an automatic alignment\nprocedure to produce coarse structures followed by human refinement to produce\nhigh-quality ones. Moreover, we benchmark our dataset by proposing a\ncontrastive learning (CL)-based framework VLGAE, short for Vision-Language\nGraph Autoencoder. Our model obtains superior performance on two derived tasks,\ni.e., language grammar induction and VL phrase grounding. Ablations show the\neffectiveness of both visual cues and dependency relationships on fine-grained\nVL structure construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_C/0/1/0/all/0/1\">Chao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuhuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Sequence-to-Tree Generation for Hierarchical Text Classification. (arXiv:2204.00811v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00811","description":"<p>Hierarchical Text Classification (HTC) is a challenging task where a document\ncan be assigned to multiple hierarchically structured categories within a\ntaxonomy. The majority of prior studies consider HTC as a flat multi-label\nclassification problem, which inevitably leads to \"label inconsistency\"\nproblem. In this paper, we formulate HTC as a sequence generation task and\nintroduce a sequence-to-tree framework (Seq2Tree) for modeling the hierarchical\nlabel structure. Moreover, we design a constrained decoding strategy with\ndynamic vocabulary to secure the label consistency of the results. Compared\nwith previous works, the proposed approach achieves significant and consistent\nimprovements on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Chao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yue Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1\">Longjun Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient comparison of sentence embeddings. (arXiv:2204.00820v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.00820","description":"<p>The domain of natural language processing (NLP), which has greatly evolved\nover the last years, has highly benefited from the recent developments in word\nand sentence embeddings. Such embeddings enable the transformation of complex\nNLP tasks, like semantic similarity or Question and Answering (Q&amp;A), into much\nsimpler to perform vector comparisons. However, such a problem transformation\nraises new challenges like the efficient comparison of embeddings and their\nmanipulation. In this work, we will discuss about various word and sentence\nembeddings algorithms, we will select a sentence embedding algorithm, BERT, as\nour algorithm of choice and we will evaluate the performance of two vector\ncomparison approaches, FAISS and Elasticsearch, in the specific problem of\nsentence embeddings. According to the results, FAISS outperforms Elasticsearch\nwhen used in a centralized environment with only one node, especially when big\ndatasets are included.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zoupanos_S/0/1/0/all/0/1\">Spyros Zoupanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolovos_S/0/1/0/all/0/1\">Stratis Kolovos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanavos_A/0/1/0/all/0/1\">Athanasios Kanavos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadimitriou_O/0/1/0/all/0/1\">Orestis Papadimitriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragoudakis_M/0/1/0/all/0/1\">Manolis Maragoudakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PaLM: Scaling Language Modeling with Pathways. (arXiv:2204.02311v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02311","description":"<p>Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1\">Aakanksha Chowdhery</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devlin_J/0/1/0/all/0/1\">Jacob Devlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1\">Gaurav Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barham_P/0/1/0/all/0/1\">Paul Barham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1\">Sebastian Gehrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuh_P/0/1/0/all/0/1\">Parker Schuh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Kensen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvyashchenko_S/0/1/0/all/0/1\">Sasha Tsvyashchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1\">Abhishek Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_P/0/1/0/all/0/1\">Parker Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1\">Vinodkumar Prabhakaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1\">Nan Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutchinson_B/0/1/0/all/0/1\">Ben Hutchinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pope_R/0/1/0/all/0/1\">Reiner Pope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_J/0/1/0/all/0/1\">James Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Austin_J/0/1/0/all/0/1\">Jacob Austin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isard_M/0/1/0/all/0/1\">Michael Isard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_Ari_G/0/1/0/all/0/1\">Guy Gur-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duke_T/0/1/0/all/0/1\">Toju Duke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levskaya_A/0/1/0/all/0/1\">Anselm Levskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghemawat_S/0/1/0/all/0/1\">Sanjay Ghemawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_K/0/1/0/all/0/1\">Kevin Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_L/0/1/0/all/0/1\">Liam Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_D/0/1/0/all/0/1\">David Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1\">Hyeontaek Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiridonov_A/0/1/0/all/0/1\">Alexander Spiridonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sepassi_R/0/1/0/all/0/1\">Ryan Sepassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Shivani Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omernick_M/0/1/0/all/0/1\">Mark Omernick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1\">Andrew M. Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pillai_T/0/1/0/all/0/1\">Thanumalayan Sankaranarayana Pillai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellat_M/0/1/0/all/0/1\">Marie Pellat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreira_E/0/1/0/all/0/1\">Erica Moreira</a>, et al. (15 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation. (arXiv:2204.02470v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.02470","description":"<p>Self-Supervised Learning (SSL) models have been successfully applied in\nvarious deep learning-based speech tasks, particularly those with a limited\namount of data. However, the quality of SSL representations depends highly on\nthe relatedness between the SSL training domain(s) and the target data domain.\nOn the contrary, spectral feature (SF) extractors such as log Mel-filterbanks\nare hand-crafted non-learnable components, and could be more robust to domain\nshifts. The present work examines the assumption that combining non-learnable\nSF extractors to SSL models is an effective approach to low resource speech\ntasks. We propose a learnable and interpretable framework to combine SF and SSL\nrepresentations. The proposed framework outperforms significantly both baseline\nand SSL models on Automatic Speech Recognition (ASR) and Speech Translation\n(ST) tasks on three low resource datasets. We additionally design a mixture of\nexperts based combination model. This last model reveals that the relative\ncontribution of SSL models over conventional SF extractors is very small in\ncase of domain mismatch between SSL training set and the target language data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berrebbi_D/0/1/0/all/0/1\">Dan Berrebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_Francisco_O/0/1/0/all/0/1\">Osbel Lopez-Francisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amith_J/0/1/0/all/0/1\">Jonathan D. Amith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Base Index Compression via Dimensionality and Precision Reduction. (arXiv:2204.02906v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2204.02906","description":"<p>Recently neural network based approaches to knowledge-intensive NLP tasks,\nsuch as question answering, started to rely heavily on the combination of\nneural retrievers and readers. Retrieval is typically performed over a large\ntextual knowledge base (KB) which requires significant memory and compute\nresources, especially when scaled up. On HotpotQA we systematically investigate\nreducing the size of the KB index by means of dimensionality (sparse random\nprojections, PCA, autoencoders) and numerical precision reduction.\n</p>\n<p>Our results show that PCA is an easy solution that requires very little data\nand is only slightly worse than autoencoders, which are less stable. All\nmethods are sensitive to pre- and post-processing and data should always be\ncentered and normalized both before and after dimension reduction. Finally, we\nshow that it is possible to combine PCA with using 1bit per dimension. Overall\nwe achieve (1) 100$\\times$ compression with 75%, and (2) 24$\\times$ compression\nwith 92% original retrieval performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosbach_M/0/1/0/all/0/1\">Marius Mosbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miaoran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klakow_D/0/1/0/all/0/1\">Dietrich Klakow</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect Rumors in Microblog Posts for Low-Resource Domains via Adversarial Contrastive Learning. (arXiv:2204.08143v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08143","description":"<p>Massive false rumors emerging along with breaking news or trending topics\nseverely hinder the truth. Existing rumor detection approaches achieve\npromising performance on the yesterday's news, since there is enough corpus\ncollected from the same domain for model training. However, they are poor at\ndetecting rumors about unforeseen events especially those propagated in\ndifferent languages due to the lack of training data and prior knowledge (i.e.,\nlow-resource regimes). In this paper, we propose an adversarial contrastive\nlearning framework to detect rumors by adapting the features learned from\nwell-resourced rumor data to that of the low-resourced. Our model explicitly\novercomes the restriction of domain and/or language usage via language\nalignment and a novel supervised contrastive training paradigm. Moreover, we\ndevelop an adversarial augmentation mechanism to further enhance the robustness\nof low-resource rumor representation. Extensive experiments conducted on two\nlow-resource datasets collected from real-world microblog platforms demonstrate\nthat our framework achieves much better performance than state-of-the-art\nmethods and exhibits a superior capacity for detecting rumors at early stages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhiwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Mingfei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08387","description":"<p>Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\nLayoutLMv3 to pre-train multimodal Transformers for Document AI with unified\ntext and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\nhttps://aka.ms/layoutlmv3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix Data Augmentation. (arXiv:2204.08458v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08458","description":"<p>A novel approach of data augmentation based on irregular superpixel\ndecomposition is proposed. This approach called SuperpixelGridMasks permits to\nextend original image datasets that are required by training stages of machine\nlearning-related analysis architectures towards increasing their performances.\nThree variants named SuperpixelGridCut, SuperpixelGridMean and\nSuperpixelGridMix are presented. These grid-based methods produce a new style\nof image transformations using the dropping and fusing of information.\nExtensive experiments using various image classification models and datasets\nshow that baseline performances can be significantly outperformed using our\nmethods. The comparative study also shows that our methods can overpass the\nperformances of other data augmentations. Experimental results obtained over\nimage recognition datasets of varied natures show the efficiency of these new\nmethods. SuperpixelGridCut, SuperpixelGridMean and SuperpixelGridMix codes are\npublicly available at https://github.com/hammoudiproject/SuperpixelGridMasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammoudi_K/0/1/0/all/0/1\">Karim Hammoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabani_A/0/1/0/all/0/1\">Adnane Cabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slika_B/0/1/0/all/0/1\">Bouthaina Slika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benhabiles_H/0/1/0/all/0/1\">Halim Benhabiles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dornaika_F/0/1/0/all/0/1\">Fadi Dornaika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melkemi_M/0/1/0/all/0/1\">Mahmoud Melkemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Convolutional Networks for Action Recognition: Application to Sport Gesture Recognition. (arXiv:2204.08460v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08460","description":"<p>3D convolutional networks is a good means to perform tasks such as video\nsegmentation into coherent spatio-temporal chunks and classification of them\nwith regard to a target taxonomy. In the chapter we are interested in the\nclassification of continuous video takes with repeatable actions, such as\nstrokes of table tennis. Filmed in a free marker less ecological environment,\nthese videos represent a challenge from both segmentation and classification\npoint of view. The 3D convnets are an efficient tool for solving these problems\nwith window-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1\">Pierre-Etienne Martin</a> (LaBRI, MPI-EVA, UB), <a href=\"http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1\">J Benois-Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peteri_R/0/1/0/all/0/1\">R P&#xe9;teri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zemmari_A/0/1/0/all/0/1\">A Zemmari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morlier_J/0/1/0/all/0/1\">J Morlier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Temporal Convolutional Neural Networks for Satellite Image Time Series Classification. (arXiv:2204.08461v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08461","description":"<p>Satellite Image Time Series (SITS) of the Earth's surface provide detailed\nland cover maps, with their quality in the spatial and temporal dimensions\nconsistently improving. These image time series are integral for developing\nsystems that aim to produce accurate, up-to-date land cover maps of the Earth's\nsurface. Applications are wide-ranging, with notable examples including\necosystem mapping, vegetation process monitoring and anthropogenic land-use\nchange tracking. Recently proposed methods for SITS classification have\ndemonstrated respectable merit, but these methods tend to lack native\nmechanisms that exploit the temporal dimension of the data; commonly resulting\nin extensive data pre-processing prohibitively long training times. To overcome\nthese shortcomings, this paper seeks to study and enhance the newly proposed\nmethod for SITS classification from literature; namely Temporal CNNs.\nComprehensive experiments are carried out on two benchmark SITS datasets with\nthe results demonstrating that Temporal CNNs display a superior or competitive\nperformance to the benchmark algorithms for both datasets. Investigations into\nthe Temporal CNNs architecture also highlighted the non-trivial task of\noptimising the model for a new dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brock_J/0/1/0/all/0/1\">James Brock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_Z/0/1/0/all/0/1\">Zahraa S. Abdallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CapillaryX: A Software Design Pattern for Analyzing Medical Images in Real-time using Deep Learning. (arXiv:2204.08462v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08462","description":"<p>Recent advances in digital imaging, e.g., increased number of pixels\ncaptured, have meant that the volume of data to be processed and analyzed from\nthese images has also increased. Deep learning algorithms are state-of-the-art\nfor analyzing such images, given their high accuracy when trained with a large\ndata volume of data. Nevertheless, such analysis requires considerable\ncomputational power, making such algorithms time- and resource-demanding. Such\nhigh demands can be met by using third-party cloud service providers. However,\nanalyzing medical images using such services raises several legal and privacy\nchallenges and does not necessarily provide real-time results. This paper\nprovides a computing architecture that locally and in parallel can analyze\nmedical images in real-time using deep learning thus avoiding the legal and\nprivacy challenges stemming from uploading data to a third-party cloud\nprovider. To make local image processing efficient on modern multi-core\nprocessors, we utilize parallel execution to offset the resource-intensive\ndemands of deep neural networks. We focus on a specific medical-industrial case\nstudy, namely the quantifying of blood vessels in microcirculation images for\nwhich we have developed a working system. It is currently used in an\nindustrial, clinical research setting as part of an e-health application. Our\nresults show that our system is approximately 78% faster than its serial system\ncounterpart and 12% faster than a master-slave parallel system architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Abdou_M/0/1/0/all/0/1\">Maged Abdalla Helmy Abdou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ferreira_P/0/1/0/all/0/1\">Paulo Ferreira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jul_E/0/1/0/all/0/1\">Eric Jul</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Truong_T/0/1/0/all/0/1\">Tuyen Trung Truong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning-Based Automated Thermal Comfort Prediction: Integration of Low-Cost Thermal and Visual Cameras for Higher Accuracy. (arXiv:2204.08463v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08463","description":"<p>Recent research is trying to leverage occupants' demand in the building's\ncontrol loop to consider individuals' well-being and the buildings' energy\nsavings. To that end, a real-time feedback system is needed to provide data\nabout occupants' comfort conditions that can be used to control the building's\nheating, cooling, and air conditioning (HVAC) system. The emergence of thermal\nimaging techniques provides an excellent opportunity for contactless data\ngathering with no interruption in occupant conditions and activities. There is\nincreasing attention to infrared thermal camera usage in public buildings\nbecause of their non-invasive quality in reading the human skin temperature.\nHowever, the state-of-the-art methods need additional modifications to become\nmore reliable. To capitalize potentials and address some existing limitations,\nnew solutions are required to bring a more holistic view toward non-intrusive\nthermal scanning by leveraging the benefit of machine learning and image\nprocessing. This research implements an automated approach to collect and\nregister simultaneous thermal and visual images and read the facial temperature\nin different regions. This paper also presents two additional investigations.\nFirst, through utilizing IButton wearable thermal sensors on the forehead area,\nwe investigate the reliability of an in-expensive thermal camera (FLIR Lepton)\nin reading the skin temperature. Second, by studying the false-color version of\nthermal images, we look into the possibility of non-radiometric thermal images\nfor predicting personalized thermal comfort. The results shows the strong\nperformance of Random Forest and K-Nearest Neighbor prediction algorithms in\npredicting personalized thermal comfort. In addition, we have found that\nnon-radiometric images can also indicate thermal comfort when the algorithm is\ntrained with larger amounts of data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashrafi_R/0/1/0/all/0/1\">Roshanak Ashrafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azarbayjani_M/0/1/0/all/0/1\">Mona Azarbayjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1\">Hamed Tabkhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust PCA Unrolling Network for Super-resolution Vessel Extraction in X-ray Coronary Angiography. (arXiv:2204.08466v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08466","description":"<p>Although robust PCA has been increasingly adopted to extract vessels from\nX-ray coronary angiography (XCA) images, challenging problems such as\ninefficient vessel-sparsity modelling, noisy and dynamic background artefacts,\nand high computational cost still remain unsolved. Therefore, we propose a\nnovel robust PCA unrolling network with sparse feature selection for\nsuper-resolution XCA vessel imaging. Being embedded within a patch-wise\nspatiotemporal super-resolution framework that is built upon a pooling layer\nand a convolutional long short-term memory network, the proposed network can\nnot only gradually prune complex vessel-like artefacts and noisy backgrounds in\nXCA during network training but also iteratively learn and select the\nhigh-level spatiotemporal semantic information of moving contrast agents\nflowing in the XCA-imaged vessels. The experimental results show that the\nproposed method significantly outperforms state-of-the-art methods, especially\nin the imaging of the vessel network and its distal vessels, by restoring the\nintensity and geometry profiles of heterogeneous vessels against complex and\ndynamic backgrounds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qin_B/0/1/0/all/0/1\">Binjie Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_H/0/1/0/all/0/1\">Haohao Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yiming Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_Y/0/1/0/all/0/1\">Yisong Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yueqi Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Song Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IOP-FL: Inside-Outside Personalization for Federated Medical Image Segmentation. (arXiv:2204.08467v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08467","description":"<p>Federated learning (FL) allows multiple medical institutions to\ncollaboratively learn a global model without centralizing all clients data. It\nis difficult, if possible at all, for such a global model to commonly achieve\noptimal performance for each individual client, due to the heterogeneity of\nmedical data from various scanners and patient demographics. This problem\nbecomes even more significant when deploying the global model to unseen clients\noutside the FL with new distributions not presented during federated training.\nTo optimize the prediction accuracy of each individual client for critical\nmedical tasks, we propose a novel unified framework for both Inside and Outside\nmodel Personalization in FL (IOP-FL). Our inside personalization is achieved by\na lightweight gradient-based approach that exploits the local adapted model for\neach client, by accumulating both the global gradients for common knowledge and\nlocal gradients for client-specific optimization. Moreover, and importantly,\nthe obtained local personalized models and the global model can form a diverse\nand informative routing space to personalize a new model for outside FL\nclients. Hence, we design a new test-time routing scheme inspired by the\nconsistency loss with a shape constraint to dynamically incorporate the models,\ngiven the distribution information conveyed by the test data. Our extensive\nexperimental results on two medical image segmentation tasks present\nsignificant improvements over SOTA methods on both inside and outside\npersonalization, demonstrating the great potential of our IOP-FL scheme for\nclinical practice. Code will be released at https://github.com/med-air/IOP-FL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Meirui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Hongzheng Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Chen Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face recognition with small and large size databases. (arXiv:2204.08468v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08468","description":"<p>This paper presents experimental results using the ORL (40 people) and FERET\n(994 people) databases. The ORL database can be useful for securing\napplications where few users attempting to access are expected. This is the\ncase, for instance, of a PDA or PC where the password is the face of the user.\nOn the other hand, the FERET database is useful for studying those situations\nwhere the number of authorized users is around a thousand people.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+roure_Alcobe_J/0/1/0/all/0/1\">Josep roure-Alcob&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand Geometry Based Recognition with a MLP Classifier. (arXiv:2204.08469v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08469","description":"<p>This paper presents a biometric recognition system based on hand geometry. We\ndescribe a database specially collected for research purposes, which consists\nof 50 people and 10 different acquisitions of the right hand. This database can\nbe freely downloaded. In addition, we describe a feature extraction procedure\nand we obtain experimental results using different classification strategies\nbased on Multi Layer Perceptrons (MLP). We have evaluated identification rates\nand Detection Cost Function (DCF) values for verification applications.\nExperimental results reveal up to 100% identification and 0% DCF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_Ballester_M/0/1/0/all/0/1\">Miguel A. Ferrer-Ballester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Travieso_Gonzalez_C/0/1/0/all/0/1\">Carlos M. Travieso-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Duro_V/0/1/0/all/0/1\">Virginia Espinosa-Duro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"U-Net and its variants for Medical Image Segmentation : A short review. (arXiv:2204.08470v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08470","description":"<p>The paper is a short review of medical image segmentation using U-Net and its\nvariants. As we understand going through a medical images is not an easy job\nfor any clinician either radiologist or pathologist. Analysing medical images\nis the only way to perform non-invasive diagnosis. Segmenting out the regions\nof interest has significant importance in medical images and is key for\ndiagnosis. This paper also gives a bird eye view of how medical image\nsegmentation has evolved. Also discusses challenge's and success of the deep\nneural architectures. Following how different hybrid architectures have built\nupon strong techniques from visual recognition tasks. In the end we will see\ncurrent challenges and future directions for medical image segmentation(MIS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ummadi_V/0/1/0/all/0/1\">Vinay Ummadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simultaneous Multiple-Prompt Guided Generation Using Differentiable Optimal Transport. (arXiv:2204.08472v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08472","description":"<p>Recent advances in deep learning, such as powerful generative models and\njoint text-image embeddings, have provided the computational creativity\ncommunity with new tools, opening new perspectives for artistic pursuits.\nText-to-image synthesis approaches that operate by generating images from text\ncues provide a case in point. These images are generated with a latent vector\nthat is progressively refined to agree with text cues. To do so, patches are\nsampled within the generated image, and compared with the text prompts in the\ncommon text-image embedding space; The latent vector is then updated, using\ngradient descent, to reduce the mean (average) distance between these patches\nand text cues. While this approach provides artists with ample freedom to\ncustomize the overall appearance of images, through their choice in generative\nmodels, the reliance on a simple criterion (mean of distances) often causes\nmode collapse: The entire image is drawn to the average of all text cues,\nthereby losing their diversity. To address this issue, we propose using\nmatching techniques found in the optimal transport (OT) literature, resulting\nin images that are able to reflect faithfully a wide diversity of prompts. We\nprovide numerous illustrations showing that OT avoids some of the pitfalls\narising from estimating vectors with mean distances, and demonstrate the\ncapacity of our proposed method to perform better in experiments, qualitatively\nand quantitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingtao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1\">Marco Cuturi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1\">David Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self Supervised Lesion Recognition For Breast Ultrasound Diagnosis. (arXiv:2204.08477v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08477","description":"<p>Previous deep learning based Computer Aided Diagnosis (CAD) system treats\nmultiple views of the same lesion as independent images. Since an ultrasound\nimage only describes a partial 2D projection of a 3D lesion, such paradigm\nignores the semantic relationship between different views of a lesion, which is\ninconsistent with the traditional diagnosis where sonographers analyze a lesion\nfrom at least two views. In this paper, we propose a multi-task framework that\ncomplements Benign/Malignant classification task with lesion recognition (LR)\nwhich helps leveraging relationship among multiple views of a single lesion to\nlearn a complete representation of the lesion. To be specific, LR task employs\ncontrastive learning to encourage representation that pulls multiple views of\nthe same lesion and repels those of different lesions. The task therefore\nfacilitates a representation that is not only invariant to the view change of\nthe lesion, but also capturing fine-grained features to distinguish between\ndifferent lesions. Experiments show that the proposed multi-task framework\nboosts the performance of Benign/Malignant classification as two sub-tasks\ncomplement each other and enhance the learned representation of ultrasound\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfan Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1\">Canqian Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_T/0/1/0/all/0/1\">Tiancheng Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chunxiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Non-mass Breast Ultrasound Cancer Classification With Knowledge Transfer. (arXiv:2204.08478v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08478","description":"<p>Much progress has been made in the deep neural network (DNN) based diagnosis\nof mass lesions breast ultrasound (BUS) images. However, the non-mass lesion is\nless investigated because of the limited data. Based on the insight that mass\ndata is sufficient and shares the same knowledge structure with non-mass data\nof identifying the malignancy of a lesion based on the ultrasound image, we\npropose a novel transfer learning framework to enhance the generalizability of\nthe DNN model for non-mass BUS with the help of mass BUS. Specifically, we\ntrain a shared DNN with combined non-mass and mass data. With the prior of\ndifferent marginal distributions in input and output space, we employ two\ndomain alignment strategies in the proposed transfer learning framework with\nthe insight of capturing domain-specific distribution to address the issue of\ndomain shift. Moreover, we propose a cross-domain semantic-preserve data\ngeneration module called CrossMix to recover the missing distribution between\nnon-mass and mass data that is not presented in training data. Experimental\nresults on an in-house dataset demonstrate that the DNN model trained with\ncombined data by our framework achieves a 10% improvement in AUC on the\nmalignancy prediction task of non-mass BUS compared to training directly on\nnon-mass data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yangrun Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfan Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_M/0/1/0/all/0/1\">Mingda Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_T/0/1/0/all/0/1\">Tiancheng Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_R/0/1/0/all/0/1\">Rong Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inductive Biases for Object-Centric Representations of Complex Textures. (arXiv:2204.08479v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08479","description":"<p>Understanding which inductive biases could be useful for the unsupervised\nlearning of object-centric representations of natural scenes is challenging.\nHere, we use neural style transfer to generate datasets where objects have\ncomplex textures while still retaining ground-truth annotations. We find that,\nwhen a model effectively balances the importance of shape and appearance in the\ntraining objective, it can achieve better separation of the objects and learn\nmore useful object representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning Helps Pretrained Models Learn the Intended Task. (arXiv:2204.08491v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08491","description":"<p>Models can fail in unpredictable ways during deployment due to task\nambiguity, when multiple behaviors are consistent with the provided training\ndata. An example is an object classifier trained on red squares and blue\ncircles: when encountering blue squares, the intended behavior is undefined. We\ninvestigate whether pretrained models are better active learners, capable of\ndisambiguating between the possible tasks a user may be trying to specify.\nIntriguingly, we find that better active learning is an emergent property of\nthe pretraining process: pretrained models require up to 5 times fewer labels\nwhen using uncertainty-based active learning, while non-pretrained models see\nno or even negative benefit. We find these gains come from an ability to select\nexamples with attributes that disambiguate the intended behavior, such as rare\nproduct categories or atypical backgrounds. These attributes are far more\nlinearly separable in pretrained model's representation spaces vs\nnon-pretrained models, suggesting a possible mechanism for this behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tamkin_A/0/1/0/all/0/1\">Alex Tamkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deshpande_S/0/1/0/all/0/1\">Salil Deshpande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jesse Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1\">Noah Goodman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepCore: A Comprehensive Library for Coreset Selection in Deep Learning. (arXiv:2204.08499v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08499","description":"<p>Coreset selection, which aims to select a subset of the most informative\ntraining samples, is a long-standing learning problem that can benefit many\ndownstream tasks such as data-efficient learning, continual learning, neural\narchitecture search, active learning, etc. However, many existing coreset\nselection methods are not designed for deep learning, which may have high\ncomplexity and poor generalization ability to unseen representations. In\naddition, the recently proposed methods are evaluated on models, datasets, and\nsettings of different complexities. To advance the research of coreset\nselection in deep learning, we contribute a comprehensive code library, namely\nDeepCore, and provide an empirical study on popular coreset selection methods\non CIFAR10 and ImageNet datasets. Extensive experiment results show that,\nalthough some methods perform better in certain experiment settings, random\nselection is still a strong baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chengcheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yanbing Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spot the Difference: A Novel Task for Embodied Agents in Changing Environments. (arXiv:2204.08502v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08502","description":"<p>Embodied AI is a recent research area that aims at creating intelligent\nagents that can move and operate inside an environment. Existing approaches in\nthis field demand the agents to act in completely new and unexplored scenes.\nHowever, this setting is far from realistic use cases that instead require\nexecuting multiple tasks in the same environment. Even if the environment\nchanges over time, the agent could still count on its global knowledge about\nthe scene while trying to adapt its internal representation to the current\nstate of the environment. To make a step towards this setting, we propose Spot\nthe Difference: a novel task for Embodied AI where the agent has access to an\noutdated map of the environment and needs to recover the correct layout in a\nfixed time budget. To this end, we collect a new dataset of occupancy maps\nstarting from existing datasets of 3D spaces and generating a number of\npossible layouts for a single environment. This dataset can be employed in the\npopular Habitat simulator and is fully compliant with existing methods that\nemploy reconstructed occupancy maps during navigation. Furthermore, we propose\nan exploration policy that can take advantage of previous knowledge of the\nenvironment and identify changes in the scene faster and more effectively than\nexisting agents. Experimental results show that the proposed architecture\noutperforms existing state-of-the-art models for exploration on this new\nsetting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigazzi_R/0/1/0/all/0/1\">Roberto Bigazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1\">Silvia Cascianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dress Code: High-Resolution Multi-Category Virtual Try-On. (arXiv:2204.08532v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08532","description":"<p>Image-based virtual try-on strives to transfer the appearance of a clothing\nitem onto the image of a target person. Prior work focuses mainly on upper-body\nclothes (e.g. t-shirts, shirts, and tops) and neglects full-body or lower-body\nitems. This shortcoming arises from a main factor: current publicly available\ndatasets for image-based virtual try-on do not account for this variety, thus\nlimiting progress in the field. To address this deficiency, we introduce Dress\nCode, which contains images of multi-category clothes. Dress Code is more than\n3x larger than publicly available datasets for image-based virtual try-on and\nfeatures high-resolution paired images (1024 x 768) with front-view, full-body\nreference models. To generate HD try-on images with high visual quality and\nrich in details, we propose to learn fine-grained discriminating features.\nSpecifically, we leverage a semantic-aware discriminator that makes predictions\nat pixel-level instead of image- or patch-level. Extensive experimental\nevaluation demonstrates that the proposed approach surpasses the baselines and\nstate-of-the-art competitors in terms of visual quality and quantitative\nresults. The Dress Code dataset is publicly available at\nhttps://github.com/aimagelab/dress-code.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Morelli_D/0/1/0/all/0/1\">Davide Morelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fincato_M/0/1/0/all/0/1\">Matteo Fincato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landi_F/0/1/0/all/0/1\">Federico Landi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesari_F/0/1/0/all/0/1\">Fabio Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Region Duplication Detection Algorithm Based on Hybrid Approach. (arXiv:2204.08545v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08545","description":"<p>The digital images from various sources are ubiquitous due to easy\navailability of high bandwidth Internet. Digital images are easy to tamper with\ngood or bad intentions. Non-availability of pre-embedded information in digital\nimages makes the tampering detection process more difficult in case of digital\nforensics. Thus, passive image tampering is difficult to detect. There are\nvarious algorithms available for detecting image tampering. However, these\nalgorithms have some drawbacks, due to which all types of tampering cannot be\ndetected. In this paper researchers intend to present the types of image\ntampering and its detection techniques with example based approach. This paper\nalso illustrates insights into the various existing algorithms and tries to\nfind out efficient algorithm out of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tatkare_K/0/1/0/all/0/1\">Kshipra Tatkare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Devare_M/0/1/0/all/0/1\">Manoj Devare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cylin-Painting: Seamless 360{\\deg} Panoramic Image Outpainting and Beyond with Cylinder-Style Convolutions. (arXiv:2204.08563v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08563","description":"<p>Image outpainting gains increasing attention since it can generate the\ncomplete scene from a partial view, providing a valuable solution to construct\n360{\\deg} panoramic images. As image outpainting suffers from the intrinsic\nissue of unidirectional completion flow, previous methods convert the original\nproblem into inpainting, which allows a bidirectional flow. However, we find\nthat inpainting has its own limitations and is inferior to outpainting in\ncertain situations. The question of how they may be combined for the best of\nboth has as yet remained under-explored. In this paper, we provide a deep\nanalysis of the differences between inpainting and outpainting, which\nessentially depends on how the source pixels contribute to the unknown regions\nunder different spatial arrangements. Motivated by this analysis, we present a\nCylin-Painting framework that involves meaningful collaborations between\ninpainting and outpainting and efficiently fuses the different arrangements,\nwith a view to leveraging their complementary benefits on a consistent and\nseamless cylinder. Nevertheless, directly applying the cylinder-style\nconvolution often generates visually unpleasing results as it could discard\nimportant positional information. To address this issue, we further present a\nlearnable positional embedding strategy and incorporate the missing component\nof positional encoding into the cylinder convolution, which significantly\nimproves the panoramic results. Note that while developed for image\noutpainting, the proposed solution can be effectively extended to other\npanoramic vision tasks, such as object detection, depth estimation, and image\nsuper resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VQGAN-CLIP: Open Domain Image Generation and Editing with Natural Language Guidance. (arXiv:2204.08583v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08583","description":"<p>Generating and editing images from open domain text prompts is a challenging\ntask that heretofore has required expensive and specially trained models. We\ndemonstrate a novel methodology for both tasks which is capable of producing\nimages of high visual quality from text prompts of significant semantic\ncomplexity without any training by using a multimodal encoder to guide image\ngenerations. We demonstrate on a variety of tasks how using CLIP [37] to guide\nVQGAN [11] produces higher visual quality outputs than prior, less flexible\napproaches like DALL-E [38], GLIDE [33] and Open-Edit [24], despite not being\ntrained for the tasks presented. Our code is available in a public repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Crowson_K/0/1/0/all/0/1\">Katherine Crowson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornis_D/0/1/0/all/0/1\">Daniel Kornis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stander_D/0/1/0/all/0/1\">Dashiell Stander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallahan_E/0/1/0/all/0/1\">Eric Hallahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1\">Louis Castricato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1\">Edward Raff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Region-Based Deep Learning Approach to Automated Retail Checkout. (arXiv:2204.08584v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08584","description":"<p>Automating the product checkout process at conventional retail stores is a\ntask poised to have large impacts on society generally speaking. Towards this\nend, reliable deep learning models that enable automated product counting for\nfast customer checkout can make this goal a reality. In this work, we propose a\nnovel, region-based deep learning approach to automate product counting using a\ncustomized YOLOv5 object detection pipeline and the DeepSORT algorithm. Our\nresults on challenging, real-world test videos demonstrate that our method can\ngeneralize its predictions to a sufficient level of accuracy and with a fast\nenough runtime to warrant deployment to real-world commercial settings. Our\nproposed method won 4th place in the 2022 AI City Challenge, Track 4, with an\nF1 score of 0.4400 on experimental validation data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shoman_M/0/1/0/all/0/1\">Maged Shoman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aboah_A/0/1/0/all/0/1\">Armstrong Aboah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morehead_A/0/1/0/all/0/1\">Alex Morehead</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daud_A/0/1/0/all/0/1\">Abdulateef Daud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adu_Gyamfi_Y/0/1/0/all/0/1\">Yaw Adu-Gyamfi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Tour of Visualization Techniques for Computer Vision Datasets. (arXiv:2204.08601v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08601","description":"<p>We survey a number of data visualization techniques for analyzing Computer\nVision (CV) datasets. These techniques help us understand properties and latent\npatterns in such data, by applying dataset-level analysis. We present various\nexamples of how such analysis helps predict the potential impact of the dataset\nproperties on CV models and informs appropriate mitigation of their\nshortcomings. Finally, we explore avenues for further visualization techniques\nof different modalities of CV datasets as well as ones that are tailored to\nsupport specific CV tasks and analysis needs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alsallakh_B/0/1/0/all/0/1\">Bilal Alsallakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Pamela Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_V/0/1/0/all/0/1\">Vanessa Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kokhlikyan_N/0/1/0/all/0/1\">Narine Kokhlikyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reblitz_Richardson_O/0/1/0/all/0/1\">Orion Reblitz-Richardson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1\">Rahul Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_D/0/1/0/all/0/1\">David Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Data Augmentation for Deep Learning: A Survey. (arXiv:2204.08610v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08610","description":"<p>Deep learning has achieved remarkable results in many computer vision tasks.\nDeep neural networks typically rely on large amounts of training data to avoid\noverfitting. However, labeled data for real-world applications may be limited.\nBy improving the quantity and diversity of training data, data augmentation has\nbecome an inevitable part of deep learning model training with image data.\n</p>\n<p>As an effective way to improve the sufficiency and diversity of training\ndata, data augmentation has become a necessary part of successful application\nof deep learning models on image data. In this paper, we systematically review\ndifferent image data augmentation methods. We propose a taxonomy of reviewed\nmethods and present the strengths and limitations of these methods. We also\nconduct extensive experiments with various data augmentation methods on three\ntypical computer vision tasks, including semantic segmentation, image\nclassification and object detection. Finally, we discuss current challenges\nfaced by data augmentation and future research directions to put forward some\nuseful research guidance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Suorong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Weikang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mengcheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Suhan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors. (arXiv:2204.08612v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08612","description":"<p>Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic\nmedia where the likeness of one person is replaced with another. There are\ngrowing concerns that deepfakes can be maliciously used to create misleading\nand harmful digital contents. As deepfakes become more common, there is a dire\nneed for deepfake detection technology to help spot deepfake media. Present\ndeepfake detection models are able to achieve outstanding accuracy (&gt;90%).\nHowever, most of them are limited to within-dataset scenario, where the same\ndataset is used for training and testing. Most models do not generalise well\nenough in cross-dataset scenario, where models are tested on unseen datasets\nfrom another source. Furthermore, state-of-the-art deepfake detection models\nrely on neural network-based classification models that are known to be\nvulnerable to adversarial attacks. Motivated by the need for a robust deepfake\ndetection model, this study adapts metamorphic testing (MT) principles to help\nidentify potential factors that could influence the robustness of the examined\nmodel, while overcoming the test oracle problem in this domain. Metamorphic\ntesting is specifically chosen as the testing technique as it fits our demand\nto address learning-based system testing with probabilistic outcomes from\nlargely black-box components, based on potentially large input domains. We\nperformed our evaluations on MesoInception-4 and TwoStreamNet models, which are\nthe state-of-the-art deepfake detection models. This study identified makeup\napplication as an adversarial attack that could fool deepfake detectors. Our\nexperimental results demonstrate that both the MesoInception-4 and TwoStreamNet\nmodels degrade in their performance by up to 30\\% when the input data is\nperturbed with makeup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_N/0/1/0/all/0/1\">Nyee Thoang Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuan_M/0/1/0/all/0/1\">Meng Yi Kuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1\">Muxin Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_M/0/1/0/all/0/1\">Mei Kuan Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_C/0/1/0/all/0/1\">Chun Yong Chong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Equivariant Learning for Oriented Keypoint Detection. (arXiv:2204.08613v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08613","description":"<p>Detecting robust keypoints from an image is an integral part of many computer\nvision problems, and the characteristic orientation and scale of keypoints play\nan important role for keypoint description and matching. Existing\nlearning-based methods for keypoint detection rely on standard\ntranslation-equivariant CNNs but often fail to detect reliable keypoints\nagainst geometric variations. To learn to detect robust oriented keypoints, we\nintroduce a self-supervised learning framework using rotation-equivariant CNNs.\nWe propose a dense orientation alignment loss by an image pair generated by\nsynthetic transformations for training a histogram-based orientation map. Our\nmethod outperforms the previous methods on an image matching benchmark and a\ncamera pose estimation benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">Minsu Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CorrGAN: Input Transformation Technique Against Natural Corruptions. (arXiv:2204.08623v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08623","description":"<p>Because of the increasing accuracy of Deep Neural Networks (DNNs) on\ndifferent tasks, a lot of real times systems are utilizing DNNs. These DNNs are\nvulnerable to adversarial perturbations and corruptions. Specifically, natural\ncorruptions like fog, blur, contrast etc can affect the prediction of DNN in an\nautonomous vehicle. In real time, these corruptions are needed to be detected\nand also the corrupted inputs are needed to be de-noised to be predicted\ncorrectly. In this work, we propose CorrGAN approach, which can generate benign\ninput when a corrupted input is provided. In this framework, we train\nGenerative Adversarial Network (GAN) with novel intermediate output-based loss\nfunction. The GAN can denoise the corrupted input and generate benign input.\nThrough experimentation, we show that up to 75.2% of the corrupted\nmisclassified inputs can be classified correctly by DNN using CorrGAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haque_M/0/1/0/all/0/1\">Mirazul Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budnik_C/0/1/0/all/0/1\">Christof J. Budnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology and geometry of data manifold in deep learning. (arXiv:2204.08624v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08624","description":"<p>Despite significant advances in the field of deep learning in applications to\nvarious fields, explaining the inner processes of deep learning models remains\nan important and open question. The purpose of this article is to describe and\nsubstantiate the geometric and topological view of the learning process of\nneural networks. Our attention is focused on the internal representation of\nneural networks and on the dynamics of changes in the topology and geometry of\nthe data manifold on different layers. We also propose a method for assessing\nthe generalizing ability of neural networks based on topological descriptors.\nIn this paper, we use the concepts of topological data analysis and intrinsic\ndimension, and we present a wide range of experiments on different datasets and\ndifferent configurations of convolutional neural network architectures. In\naddition, we consider the issue of the geometry of adversarial attacks in the\nclassification task and spoofing attacks on face recognition systems. Our work\nis a contribution to the development of an important area of explainable and\ninterpretable AI through the example of computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magai_G/0/1/0/all/0/1\">German Magai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayzenberg_A/0/1/0/all/0/1\">Anton Ayzenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quaternion Optimized Model with Sparse Regularization for Color Image Recovery. (arXiv:2204.08629v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08629","description":"<p>This paper addresses the color image completion problem in accordance with\nlow-rank quatenrion matrix optimization that is characterized by sparse\nregularization in a transformed domain. This research was inspired by an\nappreciation of the fact that different signal types, including audio formats\nand images, possess structures that are inherently sparse in respect of their\nrespective bases. Since color images can be processed as a whole in the\nquaternion domain, we depicted the sparsity of the color image in the\nquaternion discrete cosine transform (QDCT) domain. In addition, the\nrepresentation of a low-rank structure that is intrinsic to the color image is\na vital issue in the quaternion matrix completion problem. To achieve a more\nsuperior low-rank approximation, the quatenrion-based truncated nuclear norm\n(QTNN) is employed in the proposed model. Moreover, this model is facilitated\nby a competent alternating direction method of multipliers (ADMM) based on the\nalgorithm. Extensive experimental results demonstrate that the proposed method\ncan yield vastly superior completion performance in comparison with the\nstate-of-the-art low-rank matrix/quaternion matrix approximation methods tested\non color image recovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liqiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_K/0/1/0/all/0/1\">Kit Ian Kou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interaction-Aware Labeled Multi-Bernoulli Filter. (arXiv:2204.08655v1 [eess.SP])","link":"http://arxiv.org/abs/2204.08655","description":"<p>Tracking multiple objects through time is an important part of an intelligent\ntransportation system. Random finite set (RFS)-based filters are one of the\nemerging techniques for tracking multiple objects. In multi-object tracking\n(MOT), a common assumption is that each object is moving independent of its\nsurroundings. But in many real-world applications, target objects interact with\none another and the environment. Such interactions, when considered for\ntracking, are usually modeled by an interactive motion model which is\napplication specific. In this paper, we present a novel approach to incorporate\ntarget interactions within the prediction step of an RFS-based multi-target\nfilter, i.e. labeled multi-Bernoulli (LMB) filter. The method has been\ndeveloped for two practical applications of tracking a coordinated swarm and\nvehicles. The method has been tested for a complex vehicle tracking dataset and\ncompared with the LMB filter through the OSPA and OSPA$^{(2)}$ metrics. The\nresults demonstrate that the proposed interaction-aware method depicts\nconsiderable performance enhancement over the LMB filter in terms of the\nselected metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ishtiaq_N/0/1/0/all/0/1\">Nida Ishtiaq</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gostar_A/0/1/0/all/0/1\">Amirali Khodadadian Gostar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bab_Hadiashar_A/0/1/0/all/0/1\">Alireza Bab-Hadiashar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoseinnezhad_R/0/1/0/all/0/1\">Reza Hoseinnezhad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ActAR: Actor-Driven Pose Embeddings for Video Action Recognition. (arXiv:2204.08671v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08671","description":"<p>Human action recognition (HAR) in videos is one of the core tasks of video\nunderstanding. Based on video sequences, the goal is to recognize actions\nperformed by humans. While HAR has received much attention in the visible\nspectrum, action recognition in infrared videos is little studied. Accurate\nrecognition of human actions in the infrared domain is a highly challenging\ntask because of the redundant and indistinguishable texture features present in\nthe sequence. Furthermore, in some cases, challenges arise from the irrelevant\ninformation induced by the presence of multiple active persons not contributing\nto the actual action of interest. Therefore, most existing methods consider a\nstandard paradigm that does not take into account these challenges, which is in\nsome part due to the ambiguous definition of the recognition task in some\ncases. In this paper, we propose a new method that simultaneously learns to\nrecognize efficiently human actions in the infrared spectrum, while\nautomatically identifying the key-actors performing the action without using\nany prior knowledge or explicit annotations. Our method is composed of three\nstages. In the first stage, optical flow-based key-actor identification is\nperformed. Then for each key-actor, we estimate key-poses that will guide the\nframe selection process. A scale-invariant encoding process along with embedded\npose filtering are performed in order to enhance the quality of action\nrepresentations. Experimental results on InfAR dataset show that our proposed\nmodel achieves promising recognition performance and learns useful action\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamghari_S/0/1/0/all/0/1\">Soufiane Lamghari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilodeau_G/0/1/0/all/0/1\">Guillaume-Alexandre Bilodeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. (arXiv:2204.08680v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08680","description":"<p>Vision transformers have achieved great successes in many computer vision\ntasks. Most methods generate vision tokens by splitting an image into a regular\nand fixed grid and treating each cell as a token. However, not all regions are\nequally important in human-centric vision tasks, e.g., the human body needs a\nfine representation with many tokens, while the image background can be modeled\nby a few tokens. To address this problem, we propose a novel Vision\nTransformer, called Token Clustering Transformer (TCFormer), which merges\ntokens by progressive clustering, where the tokens can be merged from different\nlocations with flexible shapes and sizes. The tokens in TCFormer can not only\nfocus on important areas but also adjust the token shapes to fit the semantic\nconcept and adopt a fine resolution for regions containing critical details,\nwhich is beneficial to capturing detailed information. Extensive experiments\nshow that TCFormer consistently outperforms its counterparts on different\nchallenging human-centric tasks and datasets, including whole-body pose\nestimation on COCO-WholeBody and 3D human mesh reconstruction on 3DPW. Code is\navailable at https://github.com/ zengwang430521/TCFormer.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Sheng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wentao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1\">Chen Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wanli_O/0/1/0/all/0/1\">Ouyang Wanli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Thin Format Vision-Based Tactile Sensor with A Micro Lens Array (MLA). (arXiv:2204.08691v1 [cs.RO])","link":"http://arxiv.org/abs/2204.08691","description":"<p>Vision-based tactile sensors have been widely studied in the robotics field\nfor high spatial resolution and compatibility with machine learning algorithms.\nHowever, the currently employed sensor's imaging system is bulky limiting its\nfurther application. Here we present a micro lens array (MLA) based vison\nsystem to achieve a low thickness format of the sensor package with high\ntactile sensing performance. Multiple micromachined micro lens units cover the\nwhole elastic touching layer and provide a stitched clear tactile image,\nenabling high spatial resolution with a thin thickness of 5 mm. The thermal\nreflow and soft lithography method ensure the uniform spherical profile and\nsmooth surface of micro lens. Both optical and mechanical characterization\ndemonstrated the sensor's stable imaging and excellent tactile sensing,\nenabling precise 3D tactile information, such as displacement mapping and force\ndistribution with an ultra compact-thin structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guanlan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Michael Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongyu Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTCNet: A CNN-Transformer Cooperation Network for Face Image Super-Resolution. (arXiv:2204.08696v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08696","description":"<p>Recently, deep convolution neural networks (CNNs) steered face\nsuper-resolution methods have achieved great progress in restoring degraded\nfacial details by jointly training with facial priors. However, these methods\nhave some obvious limitations. On the one hand, multi-task joint learning\nrequires additional marking on the dataset, and the introduced prior network\nwill significantly increase the computational cost of the model. On the other\nhand, the limited receptive field of CNN will reduce the fidelity and\nnaturalness of the reconstructed facial images, resulting in suboptimal\nreconstructed images. In this work, we propose an efficient CNN-Transformer\nCooperation Network (CTCNet) for face super-resolution tasks, which uses the\nmulti-scale connected encoder-decoder architecture as the backbone.\nSpecifically, we first devise a novel Local-Global Feature Cooperation Module\n(LGCM), which is composed of a Facial Structure Attention Unit (FSAU) and a\nTransformer block, to promote the consistency of local facial detail and global\nfacial structure restoration simultaneously. Then, we design an efficient Local\nFeature Refinement Module (LFRM) to enhance the local facial structure\ninformation. Finally, to further improve the restoration of fine facial\ndetails, we present a Multi-scale Feature Fusion Unit (MFFU) to adaptively fuse\nthe features from different stages in the encoder procedure. Comprehensive\nevaluations on various datasets have assessed that the proposed CTCNet can\noutperform other state-of-the-art methods significantly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zixiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1\">Tieyong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Software Engineering Approaches for TinyML based IoT Embedded Vision: A Systematic Literature Review. (arXiv:2204.08702v1 [cs.SE])","link":"http://arxiv.org/abs/2204.08702","description":"<p>Internet of Things (IoT) has catapulted human ability to control our\nenvironments through ubiquitous sensing, communication, computation, and\nactuation. Over the past few years, IoT has joined forces with Machine Learning\n(ML) to embed deep intelligence at the far edge. TinyML (Tiny Machine Learning)\nhas enabled the deployment of ML models for embedded vision on extremely lean\nedge hardware, bringing the power of IoT and ML together. However, TinyML\npowered embedded vision applications are still in a nascent stage, and they are\njust starting to scale to widespread real-world IoT deployment. To harness the\ntrue potential of IoT and ML, it is necessary to provide product developers\nwith robust, easy-to-use software engineering (SE) frameworks and best\npractices that are customized for the unique challenges faced in TinyML\nengineering. Through this systematic literature review, we aggregated the key\nchallenges reported by TinyML developers and identified state-of-art SE\napproaches in large-scale Computer Vision, Machine Learning, and Embedded\nSystems that can help address key challenges in TinyML based IoT embedded\nvision. In summary, our study draws synergies between SE expertise that\nembedded systems developers and ML developers have independently developed to\nhelp address the unique challenges in the engineering of TinyML based IoT\nembedded vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakshman_S/0/1/0/all/0/1\">Shashank Bangalore Lakshman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisty_N/0/1/0/all/0/1\">Nasir U. Eisty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Contrastive Hashing for Cross-Modal Retrieval in Remote Sensing. (arXiv:2204.08707v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08707","description":"<p>The development of cross-modal retrieval systems that can search and retrieve\nsemantically relevant data across different modalities based on a query in any\nmodality has attracted great attention in remote sensing (RS). In this paper,\nwe focus our attention on cross-modal text-image retrieval, where queries from\none modality (e.g., text) can be matched to archive entries from another (e.g.,\nimage). Most of the existing cross-modal text-image retrieval systems in RS\nrequire a high number of labeled training samples and also do not allow fast\nand memory-efficient retrieval. These issues limit the applicability of the\nexisting cross-modal retrieval systems for large-scale applications in RS. To\naddress this problem, in this paper we introduce a novel unsupervised\ncross-modal contrastive hashing (DUCH) method for text-image retrieval in RS.\nTo this end, the proposed DUCH is made up of two main modules: 1) feature\nextraction module, which extracts deep representations of two modalities; 2)\nhashing module that learns to generate cross-modal binary hash codes from the\nextracted representations. We introduce a novel multi-objective loss function\nincluding: i) contrastive objectives that enable similarity preservation in\nintra- and inter-modal similarities; ii) an adversarial objective that is\nenforced across two modalities for cross-modal representation consistency; and\niii) binarization objectives for generating hash codes. Experimental results\nshow that the proposed DUCH outperforms state-of-the-art methods. Our code is\npublicly available at https://git.tu-berlin.de/rsim/duch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mikriukov_G/0/1/0/all/0/1\">Georgii Mikriukov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravanbakhsh_M/0/1/0/all/0/1\">Mahdyar Ravanbakhsh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1\">Beg&#xfc;m Demir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAFSSR: Stereo Image Super-Resolution Using NAFNet. (arXiv:2204.08714v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08714","description":"<p>Stereo image super-resolution aims at enhancing the quality of\nsuper-resolution results by utilizing the complementary information provided by\nbinocular systems. To obtain reasonable performance, most methods focus on\nfinely designing modules, loss functions, and etc. to exploit information from\nanother viewpoint. This has the side effect of increasing system complexity,\nmaking it difficult for researchers to evaluate new ideas and compare methods.\nThis paper inherits a strong and simple image restoration model, NAFNet, for\nsingle-view feature extraction and extends it by adding cross attention modules\nto fuse features between views to adapt to binocular scenarios. The proposed\nbaseline for stereo image super-resolution is noted as NAFSSR. Furthermore,\ntraining/testing strategies are proposed to fully exploit the performance of\nNAFSSR. Extensive experiments demonstrate the effectiveness of our method. In\nparticular, NAFSSR outperforms the state-of-the-art methods on the KITTI 2012,\nKITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place\nin the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models\nwill be released at https://github.com/megvii-research/NAFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenqing Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape-Aware Monocular 3D Object Detection. (arXiv:2204.08717v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08717","description":"<p>The detection of 3D objects through a single perspective camera is a\nchallenging issue. The anchor-free and keypoint-based models receive increasing\nattention recently due to their effectiveness and simplicity. However, most of\nthese methods are vulnerable to occluded and truncated objects. In this paper,\na single-stage monocular 3D object detection model is proposed. An\ninstance-segmentation head is integrated into the model training, which allows\nthe model to be aware of the visible shape of a target object. The detection\nlargely avoids interference from irrelevant regions surrounding the target\nobjects. In addition, we also reveal that the popular IoU-based evaluation\nmetrics, which were originally designed for evaluating stereo or LiDAR-based\ndetection methods, are insensitive to the improvement of monocular 3D object\ndetection algorithms. A novel evaluation metric, namely average depth\nsimilarity (ADS) is proposed for the monocular 3D object detection models. Our\nmethod outperforms the baseline on both the popular and the proposed evaluation\nmetrics while maintaining real-time efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wan-Lei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Song-Yuan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Token Fusion for Vision Transformers. (arXiv:2204.08721v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08721","description":"<p>Many adaptations of transformers have emerged to address the single-modal\nvision tasks, where self-attention modules are stacked to handle input sources\nlike images. Intuitively, feeding multiple modalities of data to vision\ntransformers could improve the performance, yet the inner-modal attentive\nweights may also be diluted, which could thus undermine the final performance.\nIn this paper, we propose a multimodal token fusion method (TokenFusion),\ntailored for transformer-based vision tasks. To effectively fuse multiple\nmodalities, TokenFusion dynamically detects uninformative tokens and\nsubstitutes these tokens with projected and aggregated inter-modal features.\nResidual positional alignment is also adopted to enable explicit utilization of\nthe inter-modal alignments after fusion. The design of TokenFusion allows the\ntransformer to learn correlations among multimodal features, while the\nsingle-modal transformer architecture remains largely intact. Extensive\nexperiments are conducted on a variety of homogeneous and heterogeneous\nmodalities and demonstrate that TokenFusion surpasses state-of-the-art methods\nin three typical vision tasks: multimodal image-to-image translation, RGB-depth\nsemantic segmentation, and 3D object detection with point cloud and images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yikai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Lele Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks. (arXiv:2204.08726v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08726","description":"<p>Deep neural networks have become an integral part of our software\ninfrastructure and are being deployed in many widely-used and safety-critical\napplications. However, their integration into many systems also brings with it\nthe vulnerability to test time attacks in the form of Universal Adversarial\nPerturbations (UAPs). UAPs are a class of perturbations that when applied to\nany input causes model misclassification. Although there is an ongoing effort\nto defend models against these adversarial attacks, it is often difficult to\nreconcile the trade-offs in model accuracy and robustness to adversarial\nattacks. Jacobian regularization has been shown to improve the robustness of\nmodels against UAPs, whilst model ensembles have been widely adopted to improve\nboth predictive performance and model robustness. In this work, we propose a\nnovel approach, Jacobian Ensembles-a combination of Jacobian regularization and\nmodel ensembles to significantly increase the robustness against UAPs whilst\nmaintaining or improving model accuracy. Our results show that Jacobian\nEnsembles achieves previously unseen levels of accuracy and robustness, greatly\nimproving over previous methods that tend to skew towards only either accuracy\nor robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Co_K/0/1/0/all/0/1\">Kenneth T. Co</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Rego_D/0/1/0/all/0/1\">David Martinez-Rego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hau_Z/0/1/0/all/0/1\">Zhongyuan Hau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lupu_E/0/1/0/all/0/1\">Emil C. Lupu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Proposal-free Lidar Panoptic Segmentation with Pillar-level Affinity. (arXiv:2204.08744v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08744","description":"<p>We propose a simple yet effective proposal-free architecture for lidar\npanoptic segmentation. We jointly optimize both semantic segmentation and\nclass-agnostic instance classification in a single network using a pillar-based\nbird's-eye view representation. The instance classification head learns\npairwise affinity between pillars to determine whether the pillars belong to\nthe same instance or not. We further propose a local clustering algorithm to\npropagate instance ids by merging semantic segmentation and affinity\npredictions. Our experiments on nuScenes dataset show that our approach\noutperforms previous proposal-free methods and is comparable to proposal-based\nmethods which requires extra annotation from object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vora_S/0/1/0/all/0/1\">Sourabh Vora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmentation of Atmospheric Turbulence Effects on Thermal Adapted Object Detection Models. (arXiv:2204.08745v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08745","description":"<p>Atmospheric turbulence has a degrading effect on the image quality of\nlong-range observation systems. As a result of various elements such as\ntemperature, wind velocity, humidity, etc., turbulence is characterized by\nrandom fluctuations in the refractive index of the atmosphere. It is a\nphenomenon that may occur in various imaging spectra such as the visible or the\ninfrared bands. In this paper, we analyze the effects of atmospheric turbulence\non object detection performance in thermal imagery. We use a geometric\nturbulence model to simulate turbulence effects on a medium-scale thermal image\nset, namely \"FLIR ADAS v2\". We apply thermal domain adaptation to\nstate-of-the-art object detectors and propose a data augmentation strategy to\nincrease the performance of object detectors which utilizes turbulent images in\ndifferent severity levels as training data. Our results show that the proposed\ndata augmentation strategy yields an increase in performance for both turbulent\nand non-turbulent thermal test images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uzun_E/0/1/0/all/0/1\">Engin Uzun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dursun_A/0/1/0/all/0/1\">Ahmet Anil Dursun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akagunduz_E/0/1/0/all/0/1\">Erdem Akagunduz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Spatial-Temporal Network for Continuous Sign Language Recognition. (arXiv:2204.08747v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08747","description":"<p>Sign language is a beautiful visual language and is also the primary language\nused by speaking and hearing-impaired people. However, sign language has many\ncomplex expressions, which are difficult for the public to understand and\nmaster. Sign language recognition algorithms will significantly facilitate\ncommunication between hearing-impaired people and normal people. Traditional\ncontinuous sign language recognition often uses a sequence learning method\nbased on Convolutional Neural Network (CNN) and Long Short-Term Memory Network\n(LSTM). These methods can only learn spatial and temporal features separately,\nwhich cannot learn the complex spatial-temporal features of sign language. LSTM\nis also difficult to learn long-term dependencies. To alleviate these problems,\nthis paper proposes a multi-view spatial-temporal continuous sign language\nrecognition network. The network consists of three parts. The first part is a\nMulti-View Spatial-Temporal Feature Extractor Network (MSTN), which can\ndirectly extract the spatial-temporal features of RGB and skeleton data; the\nsecond is a sign language encoder network based on Transformer, which can learn\nlong-term dependencies; the third is a Connectionist Temporal Classification\n(CTC) decoder network, which is used to predict the whole meaning of the\ncontinuous sign language. Our algorithm is tested on two public sign language\ndatasets SLR-100 and PHOENIX-Weather 2014T (RWTH). As a result, our method\nachieves excellent performance on both datasets. The word error rate on the\nSLR-100 dataset is 1.9%, and the word error rate on the RWTHPHOENIX-Weather\ndataset is 22.8%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ronghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Lu Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Point Cloud Denoising via Gradient Fields. (arXiv:2204.08755v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08755","description":"<p>3D dynamic point clouds provide a discrete representation of real-world\nobjects or scenes in motion, which have been widely applied in immersive\ntelepresence, autonomous driving, surveillance, etc. However, point clouds\nacquired from sensors are usually perturbed by noise, which affects downstream\ntasks such as surface reconstruction and analysis. Although many efforts have\nbeen made for static point cloud denoising, dynamic point cloud denoising\nremains under-explored. In this paper, we propose a novel gradient-field-based\ndynamic point cloud denoising method, exploiting the temporal correspondence\nvia the estimation of gradient fields -- a fundamental problem in dynamic point\ncloud processing and analysis. The gradient field is the gradient of the\nlog-probability function of the noisy point cloud, based on which we perform\ngradient ascent so as to converge each point to the underlying clean surface.\nWe estimate the gradient of each surface patch and exploit the temporal\ncorrespondence, where the temporally corresponding patches are searched\nleveraging on rigid motion in classical mechanics. In particular, we treat each\npatch as a rigid object, which moves in the gradient field of an adjacent frame\nvia force until reaching a balanced state, i.e., when the sum of gradients over\nthe patch reaches 0. Since the gradient would be smaller when the point is\ncloser to the underlying surface, the balanced patch would fit the underlying\nsurface well, thus leading to the temporal correspondence. Finally, the\nposition of each point in the patch is updated along the direction of the\ngradient averaged from corresponding patches in adjacent frames. Experimental\nresults demonstrate that the proposed model outperforms state-of-the-art\nmethods under both synthetic noise and simulated real-world noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qianjiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge-enhanced Feature Distillation Network for Efficient Super-Resolution. (arXiv:2204.08759v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08759","description":"<p>With the recently massive development in convolution neural networks,\nnumerous lightweight CNN-based image super-resolution methods have been\nproposed for practical deployments on edge devices. However, most existing\nmethods focus on one specific aspect: network or loss design, which leads to\nthe difficulty of minimizing the model size. To address the issue, we conclude\nblock devising, architecture searching, and loss design to obtain a more\nefficient SR structure. In this paper, we proposed an edge-enhanced feature\ndistillation network, named EFDN, to preserve the high-frequency information\nunder constrained resources. In detail, we build an edge-enhanced convolution\nblock based on the existing reparameterization methods. Meanwhile, we propose\nedge-enhanced gradient loss to calibrate the reparameterized path training.\nExperimental results show that our edge-enhanced strategies preserve the edge\nand significantly improve the final restoration quality. Code is available at\nhttps://github.com/icandle/EFDN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Semi-Supervised and Positive-Unlabeled Learning for Boosting Full Reference Image Quality Assessment. (arXiv:2204.08763v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08763","description":"<p>Full-reference (FR) image quality assessment (IQA) evaluates the visual\nquality of a distorted image by measuring its perceptual difference with\npristine-quality reference, and has been widely used in low-level vision tasks.\nPairwise labeled data with mean opinion score (MOS) are required in training\nFR-IQA model, but is time-consuming and cumbersome to collect. In contrast,\nunlabeled data can be easily collected from an image degradation or restoration\nprocess, making it encouraging to exploit unlabeled training data to boost\nFR-IQA performance. Moreover, due to the distribution inconsistency between\nlabeled and unlabeled data, outliers may occur in unlabeled data, further\nincreasing the training difficulty. In this paper, we suggest to incorporate\nsemi-supervised and positive-unlabeled (PU) learning for exploiting unlabeled\ndata while mitigating the adverse effect of outliers. Particularly, by treating\nall labeled data as positive samples, PU learning is leveraged to identify\nnegative samples (i.e., outliers) from unlabeled data. Semi-supervised learning\n(SSL) is further deployed to exploit positive unlabeled data by dynamically\ngenerating pseudo-MOS. We adopt a dual-branch network including reference and\ndistortion branches. Furthermore, spatial attention is introduced in the\nreference branch to concentrate more on the informative regions, and sliced\nWasserstein distance is used for robust difference map computation to address\nthe misalignment issues caused by images recovered by GAN models. Extensive\nexperiments show that our method performs favorably against state-of-the-arts\non the benchmark datasets PIPAL, KADID-10k, TID2013, LIVE and CSIQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhaolin Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1\">Dongwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zifei Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Missing Annotations for Incremental Learning in Object Detection. (arXiv:2204.08766v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08766","description":"<p>Despite the recent advances in the field of object detection, common\narchitectures are still ill-suited to incrementally detect new categories over\ntime. They are vulnerable to catastrophic forgetting: they forget what has been\nalready learned while updating their parameters in absence of the original\ntraining data. Previous works extended standard classification methods in the\nobject detection task, mainly adopting the knowledge distillation framework.\nHowever, we argue that object detection introduces an additional problem, which\nhas been overlooked. While objects belonging to new classes are learned thanks\nto their annotations, if no supervision is provided for other objects that may\nstill be present in the input, the model learns to associate them to background\nregions. We propose to handle these missing annotations by revisiting the\nstandard knowledge distillation framework. Our approach outperforms current\nstate-of-the-art methods in every setting of the Pascal-VOC dataset. We further\npropose an extension to instance segmentation, outperforming the other\nbaselines. In this work, we propose to handle the missing annotations by\nrevisiting the standard knowledge distillation framework. We show that our\napproach outperforms current state-of-the-art methods in every setting of the\nPascal-VOC 2007 dataset. Moreover, we propose a simple extension to instance\nsegmentation, showing that it outperforms the other baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1\">Fabio Cermelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geraci_A/0/1/0/all/0/1\">Antonino Geraci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1\">Dario Fontanel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Multi Channel Morphological Neural Network. (arXiv:2204.08768v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08768","description":"<p>Neural networks and particularly Deep learning have been comparatively little\nstudied from the theoretical point of view. Conversely, Mathematical Morphology\nis a discipline with solid theoretical foundations. We combine these domains to\npropose a new type of neural architecture that is theoretically more\nexplainable. We introduce a Binary Morphological Neural Network (BiMoNN) built\nupon the convolutional neural network. We design it for learning morphological\nnetworks with binary inputs and outputs. We demonstrate an equivalence between\nBiMoNNs and morphological operators that we can use to binarize entire\nnetworks. These can learn classical morphological operators and show promising\nresults on a medical imaging application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aouad_T/0/1/0/all/0/1\">Theodore Aouad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbot_H/0/1/0/all/0/1\">Hugues Talbot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupNet: Multiscale Hypergraph Neural Networks for Trajectory Prediction with Relational Reasoning. (arXiv:2204.08770v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08770","description":"<p>Demystifying the interactions among multiple agents from their past\ntrajectories is fundamental to precise and interpretable trajectory prediction.\nHowever, previous works only consider pair-wise interactions with limited\nrelational reasoning. To promote more comprehensive interaction modeling for\nrelational reasoning, we propose GroupNet, a multiscale hypergraph neural\nnetwork, which is novel in terms of both interaction capturing and\nrepresentation learning. From the aspect of interaction capturing, we propose a\ntrainable multiscale hypergraph to capture both pair-wise and group-wise\ninteractions at multiple group sizes. From the aspect of interaction\nrepresentation learning, we propose a three-element format that can be learnt\nend-to-end and explicitly reason some relational factors including the\ninteraction strength and category. We apply GroupNet into both CVAE-based\nprediction system and previous state-of-the-art prediction systems for\npredicting socially plausible trajectories with relational reasoning. To\nvalidate the ability of relational reasoning, we experiment with synthetic\nphysics simulations to reflect the ability to capture group behaviors, reason\ninteraction strength and interaction category. To validate the effectiveness of\nprediction, we conduct extensive experiments on three real-world trajectory\nprediction datasets, including NBA, SDD and ETH-UCY; and we show that with\nGroupNet, the CVAE-based prediction system outperforms state-of-the-art\nmethods. We also show that adding GroupNet will further improve the performance\nof previous state-of-the-art prediction systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenxin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Maosen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zhenyang Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sensor Data Fusion in Top-View Grid Maps using Evidential Reasoning with Advanced Conflict Resolution. (arXiv:2204.08780v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08780","description":"<p>We present a new method to combine evidential top-view grid maps estimated\nbased on heterogeneous sensor sources. Dempster's combination rule that is\nusually applied in this context provides undesired results with highly\nconflicting inputs. Therefore, we use more advanced evidential reasoning\ntechniques and improve the conflict resolution by modeling the reliability of\nthe evidence sources. We propose a data-driven reliability estimation to\noptimize the fusion quality using the Kitti-360 dataset. We apply the proposed\nmethod to the fusion of LiDAR and stereo camera data and evaluate the results\nqualitatively and quantitatively. The results demonstrate that our proposed\nmethod robustly combines measurements from heterogeneous sensors and\nsuccessfully resolves sensor conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Richter_S/0/1/0/all/0/1\">Sven Richter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirges_S/0/1/0/all/0/1\">Sascha Wirges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models. (arXiv:2204.08790v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08790","description":"<p>Learning visual representations from natural language supervision has\nrecently shown great promise in a number of pioneering works. In general, these\nlanguage-augmented visual models demonstrate strong transferability to a\nvariety of datasets/tasks. However, it remains a challenge to evaluate the\ntransferablity of these foundation models due to the lack of easy-to-use\ntoolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation\nof Language-augmented Visual Task-level Transfer), the first benchmark to\ncompare and evaluate pre-trained language-augmented visual models. Several\nhighlights include: (i) Datasets. As downstream evaluation suites, it consists\nof 20 image classification datasets and 35 object detection datasets, each of\nwhich is augmented with external knowledge. (ii) Toolkit. An automatic\nhyper-parameter tuning toolkit is developed to ensure the fairness in model\nadaption. To leverage the full power of language-augmented visual models, novel\nlanguage-aware initialization methods are proposed to significantly improve the\nadaption performance. (iii) Metrics. A variety of evaluation metrics are used,\nincluding sample-efficiency (zero-shot and few-shot) and parameter-efficiency\n(linear probing and full model fine-tuning). We will release our toolkit and\nevaluation platforms for the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haotian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aneja_J/0/1/0/all/0/1\">Jyoti Aneja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1\">Ping Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Jae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Houdong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A qualitative investigation of optical flow algorithms for video denoising. (arXiv:2204.08791v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08791","description":"<p>A good optical flow estimation is crucial in many video analysis and\nrestoration algorithms employed in application fields like media industry,\nindustrial inspection and automotive. In this work, we investigate how well\noptical flow algorithms perform qualitatively when integrated into a state of\nthe art video denoising algorithm. Both classic optical flow algorithms (e.g.\nTV-L1) as well as recent deep learning based algorithm (like RAFT or BMBC) will\nbe taken into account. For the qualitative investigation, we will employ\nrealistic content with challenging characteristic (noisy content, large motion\netc.) instead of the standard images used in most publications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fassold_H/0/1/0/all/0/1\">Hannes Fassold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stream Graph Convolutional Network for Intra-oral Scanner Image Segmentation. (arXiv:2204.08797v1 [eess.IV])","link":"http://arxiv.org/abs/2204.08797","description":"<p>Precise segmentation of teeth from intra-oral scanner images is an essential\ntask in computer-aided orthodontic surgical planning. The state-of-the-art deep\nlearning-based methods often simply concatenate the raw geometric attributes\n(i.e., coordinates and normal vectors) of mesh cells to train a single-stream\nnetwork for automatic intra-oral scanner image segmentation. However, since\ndifferent raw attributes reveal completely different geometric information, the\nnaive concatenation of different raw attributes at the (low-level) input stage\nmay bring unnecessary confusion in describing and differentiating between mesh\ncells, thus hampering the learning of high-level geometric representations for\nthe segmentation task. To address this issue, we design a two-stream graph\nconvolutional network (i.e., TSGCN), which can effectively handle inter-view\nconfusion between different raw attributes to more effectively fuse their\ncomplementary information and learn discriminative multi-view geometric\nrepresentations. Specifically, our TSGCN adopts two input-specific\ngraph-learning streams to extract complementary high-level geometric\nrepresentations from coordinates and normal vectors, respectively. Then, these\nsingle-view representations are further fused by a self-attention module to\nadaptively balance the contributions of different views in learning more\ndiscriminative multi-view representations for accurate and fully automatic\ntooth segmentation. We have evaluated our TSGCN on a real-patient dataset of\ndental (mesh) models acquired by 3D intraoral scanners. Experimental results\nshow that our TSGCN significantly outperforms state-of-the-art methods in 3D\ntooth (surface) segmentation. Github:\nhttps://github.com/ZhangLingMing1/TSGCNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lingming Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Z/0/1/0/all/0/1\">Zhiming Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1\">Chenqiang Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Energy-Based Prior for Generative Saliency. (arXiv:2204.08803v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08803","description":"<p>We propose a novel energy-based prior for generative saliency prediction,\nwhere the latent variables follow an informative energy-based prior. Both the\nsaliency generator and the energy-based prior are jointly trained via Markov\nchain Monte Carlo-based maximum likelihood estimation, in which the sampling\nfrom the intractable posterior and prior distributions of the latent variables\nare performed by Langevin dynamics. With the generative saliency model, we can\nobtain a pixel-wise uncertainty map from an image, indicating model confidence\nin the saliency prediction. Different from existing generative models, which\ndefine the prior distribution of the latent variable as a simple isotropic\nGaussian distribution, our model uses an energy-based informative prior which\ncan be more expressive in capturing the latent space of the data. With the\ninformative energy-based prior, we extend the Gaussian distribution assumption\nof generative models to achieve a more representative distribution of the\nlatent space, leading to more reliable uncertainty estimation. We apply the\nproposed frameworks to both RGB and RGB-D salient object detection tasks with\nboth transformer and convolutional neural network backbones. Experimental\nresults show that our generative saliency model with an energy-based prior can\nachieve not only accurate saliency predictions but also reliable uncertainty\nmaps that are consistent with human perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SePiCo: Semantic-Guided Pixel Contrast for Domain Adaptive Semantic Segmentation. (arXiv:2204.08808v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08808","description":"<p>Domain adaptive semantic segmentation attempts to make satisfactory dense\npredictions on an unlabeled target domain by utilizing the model trained on a\nlabeled source domain. One solution is self-training, which retrains models\nwith target pseudo labels. Many methods tend to alleviate noisy pseudo labels,\nhowever, they ignore intrinsic connections among cross-domain pixels with\nsimilar semantic concepts. Thus, they would struggle to deal with the semantic\nvariations across domains, leading to less discrimination and poor\ngeneralization. In this work, we propose Semantic-Guided Pixel Contrast\n(SePiCo), a novel one-stage adaptation framework that highlights the semantic\nconcepts of individual pixel to promote learning of class-discriminative and\nclass-balanced pixel embedding space across domains. Specifically, to explore\nproper semantic concepts, we first investigate a centroid-aware pixel contrast\nthat employs the category centroids of the entire source domain or a single\nsource image to guide the learning of discriminative features. Considering the\npossible lack of category diversity in semantic concepts, we then blaze a trail\nof distributional perspective to involve a sufficient quantity of instances,\nnamely distribution-aware pixel contrast, in which we approximate the true\ndistribution of each semantic category from the statistics of labeled source\ndata. Moreover, such an optimization objective can derive a closed-form upper\nbound by implicitly involving an infinite number of (dis)similar pairs.\nExtensive experiments show that SePiCo not only helps stabilize training but\nalso yields discriminative features, making significant progress in both\ndaytime and nighttime scenarios. Most notably, SePiCo establishes excellent\nresults on tasks of GTAV/SYNTHIA-to-Cityscapes and Cityscapes-to-Dark Zurich,\nimproving by 12.8, 8.8, and 9.2 mIoUs compared to the previous best method,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_B/0/1/0/all/0/1\">Binhui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingjia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Gao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoren Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UID2021: An Underwater Image Dataset for Evaluation of No-reference Quality Assessment Metrics. (arXiv:2204.08813v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08813","description":"<p>Achieving subjective and objective quality assessment of underwater images is\nof high significance in underwater visual perception and image/video\nprocessing. However, the development of underwater image quality assessment\n(UIQA) is limited for the lack of comprehensive human subjective user study\nwith publicly available dataset and reliable objective UIQA metric. To address\nthis issue, we establish a large-scale underwater image dataset, dubbed\nUID2021, for evaluating no-reference UIQA metrics. The constructed dataset\ncontains 60 multiply degraded underwater images collected from various sources,\ncovering six common underwater scenes (i.e. bluish scene, bluish-green scene,\ngreenish scene, hazy scene, low-light scene, and turbid scene), and their\ncorresponding 900 quality improved versions generated by employing fifteen\nstate-of-the-art underwater image enhancement and restoration algorithms. Mean\nopinion scores (MOS) for UID2021 are also obtained by using the pair comparison\nsorting method with 52 observers. Both in-air NR-IQA and underwater-specific\nalgorithms are tested on our constructed dataset to fairly compare the\nperformance and analyze their strengths and weaknesses. Our proposed UID2021\ndataset enables ones to evaluate NR UIQA algorithms comprehensively and paves\nthe way for further research on UIQA. Our UID2021 will be a free download and\nutilized for research purposes at: https://github.com/Hou-Guojia/UID2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_G/0/1/0/all/0/1\">Guojia Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Huan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhenkuan Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Efficient Domain-Incremental Learning Approach to Drive in All Weather Conditions. (arXiv:2204.08817v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08817","description":"<p>Although deep neural networks enable impressive visual perception performance\nfor autonomous driving, their robustness to varying weather conditions still\nrequires attention. When adapting these models for changed environments, such\nas different weather conditions, they are prone to forgetting previously\nlearned information. This catastrophic forgetting is typically addressed via\nincremental learning approaches which usually re-train the model by either\nkeeping a memory bank of training samples or keeping a copy of the entire model\nor model parameters for each scenario. While these approaches show impressive\nresults, they can be prone to scalability issues and their applicability for\nautonomous driving in all weather conditions has not been shown. In this paper\nwe propose DISC -- Domain Incremental through Statistical Correction -- a\nsimple online zero-forgetting approach which can incrementally learn new tasks\n(i.e weather conditions) without requiring re-training or expensive memory\nbanks. The only information we store for each task are the statistical\nparameters as we categorize each domain by the change in first and second order\nstatistics. Thus, as each task arrives, we simply 'plug and play' the\nstatistical vectors for the corresponding task into the model and it\nimmediately starts to perform well on that task. We show the efficacy of our\napproach by testing it for object detection in a challenging domain-incremental\nautonomous driving scenario where we encounter different adverse weather\nconditions, such as heavy rain, fog, and snow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirza_M/0/1/0/all/0/1\">M. Jehanzeb Mirza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masana_M/0/1/0/all/0/1\">Marc Masana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1\">Horst Possegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised 3D shape segmentation with multilevel consistency and part substitution. (arXiv:2204.08824v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08824","description":"<p>The lack of fine-grained 3D shape segmentation data is the main obstacle to\ndeveloping learning-based 3D segmentation techniques. We propose an effective\nsemi-supervised method for learning 3D segmentations from a few labeled 3D\nshapes and a large amount of unlabeled 3D data. For the unlabeled data, we\npresent a novel \\emph{multilevel consistency} loss to enforce consistency of\nnetwork predictions between perturbed copies of a 3D shape at multiple levels:\npoint-level, part-level, and hierarchical level. For the labeled data, we\ndevelop a simple yet effective part substitution scheme to augment the labeled\n3D shapes with more structural variations to enhance training. Our method has\nbeen extensively validated on the task of 3D object semantic segmentation on\nPartNet and ShapeNetPart, and indoor scene semantic segmentation on ScanNet. It\nexhibits superior performance to existing semi-supervised and unsupervised\npre-training 3D approaches. Our code and trained models are publicly available\nat \\url{https://github.com/isunchy/semi_supervised_3d_segmentation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chun-Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu-Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hao-Xiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng-Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1\">Xin Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect-and-describe: Joint learning framework for detection and description of objects. (arXiv:2204.08828v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08828","description":"<p>Traditional object detection answers two questions; \"what\" (what the object\nis?) and \"where\" (where the object is?). \"what\" part of the object detection\ncan be fine-grained further i.e. \"what type\", \"what shape\" and \"what material\"\netc. This results in the shifting of the object detection tasks to the object\ndescription paradigm. Describing an object provides additional detail that\nenables us to understand the characteristics and attributes of the object\n(\"plastic boat\" not just boat, \"glass bottle\" not just bottle). This additional\ninformation can implicitly be used to gain insight into unseen objects (e.g.\nunknown object is \"metallic\", \"has wheels\"), which is not possible in\ntraditional object detection. In this paper, we present a new approach to\nsimultaneously detect objects and infer their attributes, we call it Detect and\nDescribe (DaD) framework. DaD is a deep learning-based approach that extends\nobject detection to object attribute prediction as well. We train our model on\naPascal train set and evaluate our approach on aPascal test set. We achieve\n97.0% in Area Under the Receiver Operating Characteristic Curve (AUC) for\nobject attributes prediction on aPascal test set. We also show qualitative\nresults for object attribute prediction on unseen objects, which demonstrate\nthe effectiveness of our approach for describing unknown objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_A/0/1/0/all/0/1\">Addel Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_U/0/1/0/all/0/1\">Umar Khalid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning of Efficient Geometry-Aware Neural Articulated Representations. (arXiv:2204.08839v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08839","description":"<p>We propose an unsupervised method for 3D geometry-aware representation\nlearning of articulated objects. Though photorealistic images of articulated\nobjects can be rendered with explicit pose control through existing 3D neural\nrepresentations, these methods require ground truth 3D pose and foreground\nmasks for training, which are expensive to obtain. We obviate this need by\nlearning the representations with GAN training. From random poses and latent\nvectors, the generator is trained to produce realistic images of articulated\nobjects by adversarial training. To avoid a large computational cost for GAN\ntraining, we propose an efficient neural representation for articulated objects\nbased on tri-planes and then present a GAN-based framework for its unsupervised\ntraining. Experiments demonstrate the efficiency of our method and show that\nGAN-based training enables learning of controllable 3D representations without\nsupervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noguchi_A/0/1/0/all/0/1\">Atsuhiro Noguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Box Image Recognition and its Improvement with a New Augmentation Technique. (arXiv:2204.08853v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08853","description":"<p>Most methods for automated full-bore rock core image analysis (description,\ncolour, properties distribution, etc.) are based on separate core column\nanalyses. The core is usually imaged in a box because of the significant amount\nof time taken to get an image for each core column. The work presents an\ninnovative method and algorithm for core columns extraction from core boxes.\nThe conditions for core boxes imaging may differ tremendously. Such differences\nare disastrous for machine learning algorithms which need a large dataset\ndescribing all possible data variations. Still, such images have some standard\nfeatures - a box and core. Thus, we can emulate different environments with a\nunique augmentation described in this work. It is called template-like\naugmentation (TLA). The method is described and tested on various environments,\nand results are compared on an algorithm trained on both 'traditional' data and\na mix of traditional and TLA data. The algorithm trained with TLA data provides\nbetter metrics and can detect core on most new images, unlike the algorithm\ntrained on data without TLA. The algorithm for core column extraction\nimplemented in an automated core description system speeds up the core box\nprocessing by a factor of 20.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baraboshkin_E/0/1/0/all/0/1\">E.E. Baraboshkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demidov_A/0/1/0/all/0/1\">A.E. Demidov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1\">D.M. Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1\">D.A. Koroteev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenGlue: Open Source Graph Neural Net Based Pipeline for Image Matching. (arXiv:2204.08870v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08870","description":"<p>We present OpenGlue: a free open-source framework for image matching, that\nuses a Graph Neural Network-based matcher inspired by SuperGlue\n\\cite{sarlin20superglue}. We show that including additional geometrical\ninformation, such as local feature scale, orientation, and affine geometry,\nwhen available (e.g. for SIFT features), significantly improves the performance\nof the OpenGlue matcher. We study the influence of the various attention\nmechanisms on accuracy and speed. We also present a simple architectural\nimprovement by combining local descriptors with context-aware descriptors. The\ncode and pretrained OpenGlue models for the different local features are\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Viniavskyi_O/0/1/0/all/0/1\">Ostap Viniavskyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobko_M/0/1/0/all/0/1\">Mariia Dobko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobosevych_O/0/1/0/all/0/1\">Oles Dobosevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less than Few: Self-Shot Video Instance Segmentation. (arXiv:2204.08874v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08874","description":"<p>The goal of this paper is to bypass the need for labelled examples in\nfew-shot video understanding at run time. While proven effective, in many\npractical video settings even labelling a few examples appears unrealistic.\nThis is especially true as the level of details in spatio-temporal video\nunderstanding and with it, the complexity of annotations continues to increase.\nRather than performing few-shot learning with a human oracle to provide a few\ndensely labelled support videos, we propose to automatically learn to find\nappropriate support videos given a query. We call this self-shot learning and\nwe outline a simple self-supervised learning method to generate an embedding\nspace well-suited for unsupervised retrieval of relevant samples. To showcase\nthis novel setting, we tackle, for the first time, video instance segmentation\nin a self-shot (and few-shot) setting, where the goal is to segment instances\nat the pixel-level across the spatial and temporal domains. We provide strong\nbaseline performances that utilize a novel transformer-based model and show\nthat self-shot learning can even surpass few-shot and can be positively\ncombined for further performance gains. Experiments on new benchmarks show that\nour approach achieves strong performance, is competitive to oracle support in\nsome settings, scales to large unlabelled video collections, and can be\ncombined in a semi-supervised setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Pengwan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invertible Mask Network for Face Privacy-Preserving. (arXiv:2204.08895v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08895","description":"<p>Face privacy-preserving is one of the hotspots that arises dramatic interests\nof research. However, the existing face privacy-preserving methods aim at\ncausing the missing of semantic information of face and cannot preserve the\nreusability of original facial information. To achieve the naturalness of the\nprocessed face and the recoverability of the original protected face, this\npaper proposes face privacy-preserving method based on Invertible \"Mask\"\nNetwork (IMN). In IMN, we introduce a Mask-net to generate \"Mask\" face firstly.\nThen, put the \"Mask\" face onto the protected face and generate the masked face,\nin which the masked face is indistinguishable from \"Mask\" face. Finally, \"Mask\"\nface can be put off from the masked face and obtain the recovered face to the\nauthorized users, in which the recovered face is visually indistinguishable\nfrom the protected face. The experimental results show that the proposed method\ncan not only effectively protect the privacy of the protected face, but also\nalmost perfectly recover the protected face from the masked face.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yiyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1\">Ming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kejiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Efficient Single Image Dehazing and Desnowing. (arXiv:2204.08899v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08899","description":"<p>Removing adverse weather conditions like rain, fog, and snow from images is a\nchallenging problem. Although the current recovery algorithms targeting a\nspecific condition have made impressive progress, it is not flexible enough to\ndeal with various degradation types. We propose an efficient and compact image\nrestoration network named DAN-Net (Degradation-Adaptive Neural Network) to\naddress this problem, which consists of multiple compact expert networks with\none adaptive gated neural. A single expert network efficiently addresses\nspecific degradation in nasty winter scenes relying on the compact architecture\nand three novel components. Based on the Mixture of Experts strategy, DAN-Net\ncaptures degradation information from each input image to adaptively modulate\nthe outputs of task-specific expert networks to remove various adverse winter\nweather conditions. Specifically, it adopts a lightweight Adaptive Gated Neural\nNetwork to estimate gated attention maps of the input image, while different\ntask-specific experts with the same topology are jointly dispatched to process\nthe degraded image. Such novel image restoration pipeline handles different\ntypes of severe weather scenes effectively and efficiently. It also enjoys the\nbenefit of coordinate boosting in which the whole network outperforms each\nexpert trained without coordination.\n</p>\n<p>Extensive experiments demonstrate that the presented manner outperforms the\nstate-of-the-art single-task methods on image quality and has better inference\nefficiency. Furthermore, we have collected the first real-world winter scenes\ndataset to evaluate winter image restoration methods, which contains various\nhazy and snowy images snapped in winter. Both the dataset and source code will\nbe publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Sixiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Erkang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuche Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Photorealistic Monocular 3D Reconstruction of Humans Wearing Clothing. (arXiv:2204.08906v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08906","description":"<p>We present PHORHUM, a novel, end-to-end trainable, deep neural network\nmethodology for photorealistic 3D human reconstruction given just a monocular\nRGB image. Our pixel-aligned method estimates detailed 3D geometry and, for the\nfirst time, the unshaded surface color together with the scene illumination.\nObserving that 3D supervision alone is not sufficient for high fidelity color\nreconstruction, we introduce patch-based rendering losses that enable reliable\ncolor reconstruction on visible parts of the human, and detailed and plausible\ncolor estimation for the non-visible parts. Moreover, our method specifically\naddresses methodological and practical limitations of prior work in terms of\nrepresenting geometry, albedo, and illumination effects, in an end-to-end model\nwhere factors can be effectively disentangled. In extensive experiments, we\ndemonstrate the versatility and robustness of our approach. Our\nstate-of-the-art results validate the method qualitatively and for different\nmetrics, for both geometric and color reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alldieck_T/0/1/0/all/0/1\">Thiemo Alldieck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1\">Mihai Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Calibrated Efficient Transformer for Lightweight Super-Resolution. (arXiv:2204.08913v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08913","description":"<p>Recently, deep learning has been successfully applied to the single-image\nsuper-resolution (SISR) with remarkable performance. However, most existing\nmethods focus on building a more complex network with a large number of layers,\nwhich can entail heavy computational costs and memory storage. To address this\nproblem, we present a lightweight Self-Calibrated Efficient Transformer (SCET)\nnetwork to solve this problem. The architecture of SCET mainly consists of the\nself-calibrated module and efficient transformer block, where the\nself-calibrated module adopts the pixel attention mechanism to extract image\nfeatures effectively. To further exploit the contextual information from\nfeatures, we employ an efficient transformer to help the network obtain similar\nfeatures over long distances and thus recover sufficient texture details. We\nprovide comprehensive results on different settings of the overall network. Our\nproposed method achieves more remarkable performance than baseline methods. The\nsource code and pre-trained models are available at\nhttps://github.com/AlexZou14/SCET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenbin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weixin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunchen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Global-and-Local Collaborative Learning for Co-Salient Object Detection. (arXiv:2204.08917v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08917","description":"<p>The goal of co-salient object detection (CoSOD) is to discover salient\nobjects that commonly appear in a query group containing two or more relevant\nimages. Therefore, how to effectively extract inter-image correspondence is\ncrucial for the CoSOD task. In this paper, we propose a global-and-local\ncollaborative learning architecture, which includes a global correspondence\nmodeling (GCM) and a local correspondence modeling (LCM) to capture\ncomprehensive inter-image corresponding relationship among different images\nfrom the global and local perspectives. Firstly, we treat different images as\ndifferent time slices and use 3D convolution to integrate all intra features\nintuitively, which can more fully extract the global group semantics. Secondly,\nwe design a pairwise correlation transformation (PCT) to explore similarity\ncorrespondence between pairwise images and combine the multiple local pairwise\ncorrespondences to generate the local inter-image relationship. Thirdly, the\ninter-image relationships of the GCM and LCM are integrated through a\nglobal-and-local correspondence aggregation (GLA) module to explore more\ncomprehensive inter-image collaboration cues. Finally, the intra- and\ninter-features are adaptively integrated by an intra-and-inter weighting fusion\n(AEWF) module to learn co-saliency features and predict the co-saliency map.\nThe proposed GLNet is evaluated on three prevailing CoSOD benchmark datasets,\ndemonstrating that our model trained on a small dataset (about 3k images) still\noutperforms eleven state-of-the-art competitors trained on some large datasets\n(about 8k-200k images).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1\">Runmin Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Ning Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Imagine: Diversify Memory for Incremental Learning using Unlabeled Data. (arXiv:2204.08932v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08932","description":"<p>Deep neural network (DNN) suffers from catastrophic forgetting when learning\nincrementally, which greatly limits its applications. Although maintaining a\nhandful of samples (called `exemplars`) of each task could alleviate forgetting\nto some extent, existing methods are still limited by the small number of\nexemplars since these exemplars are too few to carry enough task-specific\nknowledge, and therefore the forgetting remains. To overcome this problem, we\npropose to `imagine` diverse counterparts of given exemplars referring to the\nabundant semantic-irrelevant information from unlabeled data. Specifically, we\ndevelop a learnable feature generator to diversify exemplars by adaptively\ngenerating diverse counterparts of exemplars based on semantic information from\nexemplars and semantically-irrelevant information from unlabeled data. We\nintroduce semantic contrastive learning to enforce the generated samples to be\nsemantic consistent with exemplars and perform semanticdecoupling contrastive\nlearning to encourage diversity of generated samples. The diverse generated\nsamples could effectively prevent DNN from forgetting when learning new tasks.\nOur method does not bring any extra inference cost and outperforms\nstate-of-the-art methods on two benchmarks CIFAR-100 and ImageNet-Subset by a\nclear margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yu-Ming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yi-Xing Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning-based surrogate model for 3-D patient-specific computational fluid dynamics. (arXiv:2204.08939v1 [physics.med-ph])","link":"http://arxiv.org/abs/2204.08939","description":"<p>Optimization and uncertainty quantification have been playing an increasingly\nimportant role in computational hemodynamics. However, existing methods based\non principled modeling and classic numerical techniques have faced significant\nchallenges, particularly when it comes to complex 3D patient-specific shapes in\nthe real world. First, it is notoriously challenging to parameterize the input\nspace of arbitrarily complex 3-D geometries. Second, the process often involves\nmassive forward simulations, which are extremely computationally demanding or\neven infeasible. We propose a novel deep learning surrogate modeling solution\nto address these challenges and enable rapid hemodynamic predictions.\nSpecifically, a statistical generative model for 3-D patient-specific shapes is\ndeveloped based on a small set of baseline patient-specific geometries. An\nunsupervised shape correspondence solution is used to enable geometric morphing\nand scalable shape synthesis statistically. Moreover, a simulation routine is\ndeveloped for automatic data generation by automatic meshing, boundary setting,\nsimulation, and post-processing. An efficient supervised learning solution is\nproposed to map the geometric inputs to the hemodynamics predictions in latent\nspaces. Numerical studies on aortic flows are conducted to demonstrate the\neffectiveness and merit of the proposed techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Du_P/0/1/0/all/0/1\">Pan Du</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaozhi Zhu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_J/0/1/0/all/0/1\">Jian-Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Missingness Bias in Model Debugging. (arXiv:2204.08945v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08945","description":"<p>Missingness, or the absence of features from an input, is a concept\nfundamental to many model debugging tools. However, in computer vision, pixels\ncannot simply be removed from an image. One thus tends to resort to heuristics\nsuch as blacking out pixels, which may in turn introduce bias into the\ndebugging process. We study such biases and, in particular, show how\ntransformer-based architectures can enable a more natural implementation of\nmissingness, which side-steps these issues and improves the reliability of\nmodel debugging in practice. Our code is available at\nhttps://github.com/madrylab/missingness\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salman_H/0/1/0/all/0/1\">Hadi Salman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1\">Eric Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemprala_S/0/1/0/all/0/1\">Sai Vemprala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Vicinal Risk Minimization for Partially Supervised Multi-Label Classification Under Data Scarcity. (arXiv:2204.08954v1 [cs.LG])","link":"http://arxiv.org/abs/2204.08954","description":"<p>Due to the high human cost of annotation, it is non-trivial to curate a\nlarge-scale medical dataset that is fully labeled for all classes of interest.\nInstead, it would be convenient to collect multiple small partially labeled\ndatasets from different matching sources, where the medical images may have\nonly been annotated for a subset of classes of interest. This paper offers an\nempirical understanding of an under-explored problem, namely partially\nsupervised multi-label classification (PSMLC), where a multi-label classifier\nis trained with only partially labeled medical images. In contrast to the fully\nsupervised counterpart, the partial supervision caused by medical data scarcity\nhas non-trivial negative impacts on the model performance. A potential remedy\ncould be augmenting the partial labels. Though vicinal risk minimization (VRM)\nhas been a promising solution to improve the generalization ability of the\nmodel, its application to PSMLC remains an open question. To bridge the\nmethodological gap, we provide the first VRM-based solution to PSMLC. The\nempirical results also provide insights into future research directions on\npartially supervised learning under data scarcity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1\">Nanqing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiayi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voiculescu_I/0/1/0/all/0/1\">Irina Voiculescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MANIQA: Multi-dimension Attention Network for No-Reference Image Quality Assessment. (arXiv:2204.08958v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08958","description":"<p>No-Reference Image Quality Assessment (NR-IQA) aims to assess the perceptual\nquality of images in accordance with human subjective perception.\nUnfortunately, existing NR-IQA methods are far from meeting the needs of\npredicting accurate quality scores on GAN-based distortion images. To this end,\nwe propose Multi-dimension Attention Network for no-reference Image Quality\nAssessment (MANIQA) to improve the performance on GAN-based distortion. We\nfirstly extract features via ViT, then to strengthen global and local\ninteractions, we propose the Transposed Attention Block (TAB) and the Scale\nSwin Transformer Block (SSTB). These two modules apply attention mechanisms\nacross the channel and spatial dimension, respectively. In this\nmulti-dimensional manner, the modules cooperatively increase the interaction\namong different regions of images globally and locally. Finally, a dual branch\nstructure for patch-weighted quality prediction is applied to predict the final\nscore depending on the weight of each patch's score. Experimental results\ndemonstrate that MANIQA outperforms state-of-the-art methods on four standard\ndatasets (LIVE, TID2013, CSIQ, and KADID-10K) by a large margin. Besides, our\nmethod ranked first place in the final testing phase of the NTIRE 2022\nPerceptual Image Quality Assessment Challenge Track 2: No-Reference. Codes and\nmodels are available at https://github.com/IIGROUP/MANIQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sidi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shanshan Lao Yuan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rendering Nighttime Image Via Cascaded Color and Brightness Compensation. (arXiv:2204.08970v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08970","description":"<p>Image signal processing (ISP) is crucial for camera imaging, and neural\nnetworks (NN) solutions are extensively deployed for daytime scenes. The lack\nof sufficient nighttime image dataset and insights on nighttime illumination\ncharacteristics poses a great challenge for high-quality rendering using\nexisting NN ISPs. To tackle it, we first built a high-resolution nighttime\nRAW-RGB (NR2R) dataset with white balance and tone mapping annotated by expert\nprofessionals. Meanwhile, to best capture the characteristics of nighttime\nillumination light sources, we develop the CBUnet, a two-stage NN ISP to\ncascade the compensation of color and brightness attributes. Experiments show\nthat our method has better visual quality compared to traditional ISP pipeline,\nand is ranked at the second place in the NTIRE 2022 Night Photography Rendering\nChallenge for two tracks by respective People's and Professional Photographer's\nchoices. The code and relevant materials are avaiable on our website:\nhttps://njuvision.github.io/CBUnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1\">Si Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shallow camera pipeline for night photography rendering. (arXiv:2204.08972v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08972","description":"<p>We introduce a camera pipeline for rendering visually pleasing photographs in\nlow light conditions, as part of the NTIRE2022 Night Photography Rendering\nchallenge. Given the nature of the task, where the objective is verbally\ndefined by an expert photographer instead of relying on explicit ground truth\nimages, we design an handcrafted solution, characterized by a shallow structure\nand by a low parameter count. Our pipeline exploits a local light enhancer as a\nform of high dynamic range correction, followed by a global adjustment of the\nimage histogram to prevent washed-out results. We proportionally apply image\ndenoising to darker regions, where it is more easily perceived, without losing\ndetails on brighter regions. The solution reached the fifth place in the\ncompetition, with a preference vote count comparable to those of other entries,\nbased on deep convolutional neural networks. Code is available at\nwww.github.com/AvailableAfterAcceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zini_S/0/1/0/all/0/1\">Simone Zini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_C/0/1/0/all/0/1\">Claudio Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buzzelli_M/0/1/0/all/0/1\">Marco Buzzelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bianco_S/0/1/0/all/0/1\">Simone Bianco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schettini_R/0/1/0/all/0/1\">Raimondo Schettini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comparison of different atmospheric turbulence simulation methods for image restoration. (arXiv:2204.08974v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08974","description":"<p>Atmospheric turbulence deteriorates the quality of images captured by\nlong-range imaging systems by introducing blur and geometric distortions to the\ncaptured scene. This leads to a drastic drop in performance when computer\nvision algorithms like object/face recognition and detection are performed on\nthese images. In recent years, various deep learning-based atmospheric\nturbulence mitigation methods have been proposed in the literature. These\nmethods are often trained using synthetically generated images and tested on\nreal-world images. Hence, the performance of these restoration methods depends\non the type of simulation used for training the network. In this paper, we\nsystematically evaluate the effectiveness of various turbulence simulation\nmethods on image restoration. In particular, we evaluate the performance of two\nstate-or-the-art restoration networks using six simulations method on a\nreal-world LRFID dataset consisting of face images degraded by turbulence. This\npaper will provide guidance to the researchers and practitioners working in\nthis field to choose the suitable data generation models for training deep\nmodels for turbulence mitigation. The implementation codes for the simulation\nmethods, source codes for the networks, and the pre-trained models will be\npublicly made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_N/0/1/0/all/0/1\">Nithin Gopalakrishnan Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_K/0/1/0/all/0/1\">Kangfu Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Face Recognition System. (arXiv:2204.08978v1 [cs.CV])","link":"http://arxiv.org/abs/2204.08978","description":"<p>Over the past few decades, interest in algorithms for face recognition has\nbeen growing rapidly and has even surpassed human-level performance. Despite\ntheir accomplishments, their practical integration with a real-time\nperformance-hungry system is not feasible due to high computational costs. So\nin this paper, we explore the recent, fast, and accurate face recognition\nsystem that can be easily integrated with real-time devices, and tested the\nalgorithms on robot hardware platforms to confirm their robustness and speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghimire_A/0/1/0/all/0/1\">Adarsh Ghimire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werghi_N/0/1/0/all/0/1\">Naoufel Werghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1\">Sajid Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dias_J/0/1/0/all/0/1\">Jorge Dias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Deep Learning-based Estimation of the Vital Signs on Smartphones. (arXiv:2204.08989v1 [eess.SP])","link":"http://arxiv.org/abs/2204.08989","description":"<p>Nowadays, due to the widespread use of smartphones in everyday life and the\nimprovement of computational capabilities of these devices, many complex tasks\ncan now be deployed on them. Concerning the need for continuous monitoring of\nvital signs, especially for the elderly or those with certain types of\ndiseases, the development of algorithms that can estimate vital signs using\nsmartphones has attracted researchers worldwide. Such algorithms estimate vital\nsigns (heart rate and oxygen saturation level) by processing an input PPG\nsignal. These methods often apply multiple pre-processing steps to the input\nsignal before the prediction step. This can increase the computational\ncomplexity of these methods, meaning only a limited number of mobile devices\ncan run them. Furthermore, multiple pre-processing steps also require the\ndesign of a couple of hand-crafted stages to obtain an optimal result. This\nresearch proposes a novel end-to-end solution to mobile-based vital sign\nestimation by deep learning. The proposed method does not require any\npre-processing. Due to the use of fully convolutional architecture, the\nparameter count of our proposed model is, on average, a quarter of the ordinary\narchitectures that use fully-connected layers as the prediction heads. As a\nresult, the proposed model has less over-fitting chance and computational\ncomplexity. A public dataset for vital sign estimation, including 62 videos\ncollected from 35 men and 27 women, is also provided. The experimental results\ndemonstrate state-of-the-art estimation accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Samavati_T/0/1/0/all/0/1\">Taha Samavati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farvardin_M/0/1/0/all/0/1\">Mahdi Farvardin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Domain Image Synthesis using Segmentation-Guided GAN. (arXiv:2204.09015v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09015","description":"<p>We introduce a segmentation-guided approach to synthesise images that\nintegrate features from two distinct domains. Images synthesised by our\ndual-domain model belong to one domain within the semantic mask, and to another\nin the rest of the image - smoothly integrated. We build on the successes of\nfew-shot StyleGAN and single-shot semantic segmentation to minimise the amount\nof training required in utilising two domains. The method combines a few-shot\ncross-domain StyleGAN with a latent optimiser to achieve images containing\nfeatures of two distinct domains. We use a segmentation-guided perceptual loss,\nwhich compares both pixel-level and activations between domain-specific and\ndual-domain synthetic images. Results demonstrate qualitatively and\nquantitatively that our model is capable of synthesising dual-domain images on\na variety of objects (faces, horses, cats, cars), domains (natural, caricature,\nsketches) and part-based masks (eyes, nose, mouth, hair, car bonnet). The code\nis publicly available at:\nhttps://github.com/denabazazian/Dual-Domain-Synthesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bazazian_D/0/1/0/all/0/1\">Dena Bazazian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1\">Andrew Calway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damen_D/0/1/0/all/0/1\">Dima Damen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised detection of ash dieback disease (Hymenoscyphus fraxineus) using diffusion-based hyperspectral image clustering. (arXiv:2204.09041v1 [cs.CV])","link":"http://arxiv.org/abs/2204.09041","description":"<p>Ash dieback (Hymenoscyphus fraxineus) is an introduced fungal disease that is\ncausing the widespread death of ash trees across Europe. Remote sensing\nhyperspectral images encode rich structure that has been exploited for the\ndetection of dieback disease in ash trees using supervised machine learning\ntechniques. However, to understand the state of forest health at\nlandscape-scale, accurate unsupervised approaches are needed. This article\ninvestigates the use of the unsupervised Diffusion and VCA-Assisted Image\nSegmentation (D-VIS) clustering algorithm for the detection of ash dieback\ndisease in a forest site near Cambridge, United Kingdom. The unsupervised\nclustering presented in this work has high overlap with the supervised\nclassification of previous work on this scene (overall accuracy = 71%). Thus,\nunsupervised learning may be used for the remote detection of ash dieback\ndisease without the need for expert labeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1\">Sam L. Polk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1\">Aland H. Y. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kangning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1\">Robert J. Plemmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coomes_D/0/1/0/all/0/1\">David A. Coomes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1\">James M. Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good, Better, Best: Textual Distractors Generation for Multiple-Choice Visual Question Answering via Reinforcement Learning. (arXiv:1910.09134v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1910.09134","description":"<p>Multiple-choice VQA has drawn increasing attention from researchers and\nend-users recently. As the demand for automatically constructing large-scale\nmultiple-choice VQA data grows, we introduce a novel task called textual\nDistractors Generation for VQA (DG-VQA) focusing on generating challenging yet\nmeaningful distractors given the context image, question, and correct answer.\nThe DG-VQA task aims at generating distractors without ground-truth training\nsamples since such resources are rarely available. To tackle the DG-VQA\nunsupervisedly, we propose Gobbet, a reinforcement learning(RL) based framework\nthat utilizes pre-trained VQA models as an alternative knowledge base to guide\nthe distractor generation process. In Gobbet, a pre-trained VQA model serves as\nthe environment in RL setting to provide feedback for the input multi-modal\nquery, while a neural distractor generator serves as the agent to take actions\naccordingly. We propose to use existing VQA models' performance degradation as\nindicators of the quality of generated distractors. On the other hand, we show\nthe utility of generated distractors through data augmentation experiments,\nsince robustness is more and more important when AI models apply to\nunpredictable open-domain scenarios or security-sensitive applications. We\nfurther conduct a manual case study on the factors why distractors generated by\nGobbet can fool existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiaying Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yezhou Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Hashing Methods. (arXiv:2003.03369v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.03369","description":"<p>Nearest neighbor search is to find the data points in the database such that\nthe distances from them to the query are the smallest, which is a fundamental\nproblem in various domains, such as computer vision, recommendation systems and\nmachine learning. Hashing is one of the most widely used methods for its\ncomputational and storage efficiency. With the development of deep learning,\ndeep hashing methods show more advantages than traditional methods. In this\npaper, we present a comprehensive survey of the deep hashing algorithms\nincluding deep supervised hashing and deep unsupervised hashing. Specifically,\nwe categorize deep supervised hashing methods into pairwise methods,\nranking-based methods, pointwise methods as well as quantization according to\nhow measuring the similarities of the learned hash codes. Moreover, deep\nunsupervised hashing is categorized into similarity reconstruction-based\nmethods, pseudo-label-based methods and prediction-free self-supervised\nlearning-based methods based on their semantic learning manners. We also\nintroduce three related important topics including semi-supervised deep\nhashing, domain adaption deep hashing and multi-modal deep hashing. Meanwhile,\nwe present some commonly used public datasets and the scheme to measure the\nperformance of deep hashing algorithms. Finally, we discuss some potential\nresearch directions in conclusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haixin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Daqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Minghua Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptron Synthesis Network: Rethinking the Action Scale Variances in Videos. (arXiv:2007.11460v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.11460","description":"<p>Video action recognition has been partially addressed by the CNNs stacking of\nfixed-size 3D kernels. However, these methods may under-perform for only\ncapturing rigid spatial-temporal patterns in single-scale spaces, while\nneglecting the scale variances across different action primitives. To overcome\nthis limitation, we propose to learn the optimal-scale kernels from the data.\nMore specifically, an \\textit{action perceptron synthesizer} is proposed to\ngenerate the kernels from a bag of fixed-size kernels that are interacted by\ndense routing paths. To guarantee the interaction richness and the information\ncapacity of the paths, we design the novel \\textit{optimized feature fusion\nlayer}. This layer establishes a principled universal paradigm that suffices to\ncover most of the current feature fusion techniques (e.g., channel shuffling,\nand channel dropout) for the first time. By inserting the \\textit{synthesizer},\nour method can easily adapt the traditional 2D CNNs to the video understanding\ntasks such as action recognition with marginal additional computation cost. The\nproposed method is thoroughly evaluated over several challenging datasets\n(i.e., Somehting-to-Somthing, Kinetics and Diving48) that highly require\ntemporal reasoning or appearance discriminating, achieving new state-of-the-art\nresults. Particularly, our low-resolution model outperforms the recent strong\nbaseline methods, i.e., TSM and GST, with less than 30\\% of their computation\ncost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuan Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhiyong Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuralAnnot: Neural Annotator for 3D Human Mesh Training Sets. (arXiv:2011.11232v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11232","description":"<p>Most 3D human mesh regressors are fully supervised with 3D pseudo-GT human\nmodel parameters and weakly supervised with GT 2D/3D joint coordinates as the\n3D pseudo-GTs bring great performance gain. The 3D pseudo-GTs are obtained by\nannotators, systems that iteratively fit 3D human model parameters to GT 2D/3D\njoint coordinates of training sets in the pre-processing stage of the\nregressors. The fitted 3D parameters at the last fitting iteration become the\n3D pseudo-GTs, used to fully supervise the regressors. Optimization-based\nannotators, such as SMPLify-X, have been widely used to obtain the 3D\npseudo-GTs. However, they often produce wrong 3D pseudo-GTs as they fit the 3D\nparameters to GT of each sample independently. To overcome the limitation, we\npresent NeuralAnnot, a neural network-based annotator. The main idea of\nNeuralAnnot is to employ a neural network-based regressor and dedicate it for\nthe annotation. Assuming no 3D pseudo-GTs are available, NeuralAnnot is weakly\nsupervised with GT 2D/3D joint coordinates of training sets. The testing\nresults on the same training sets become 3D pseudo-GTs, used to fully supervise\nthe regressors. We show that 3D pseudo-GTs of NeuralAnnot are highly beneficial\nto train the regressors. We made our 3D pseudo-GTs publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_G/0/1/0/all/0/1\">Gyeongsik Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hongsuk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate 3D Hand Pose Estimation for Whole-Body 3D Human Mesh Estimation. (arXiv:2011.11534v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11534","description":"<p>Whole-body 3D human mesh estimation aims to reconstruct the 3D human body,\nhands, and face simultaneously. Although several methods have been proposed,\naccurate prediction of 3D hands, which consist of 3D wrist and fingers, still\nremains challenging due to two reasons. First, the human kinematic chain has\nnot been carefully considered when predicting the 3D wrists. Second, previous\nworks utilize body features for the 3D fingers, where the body feature barely\ncontains finger information. To resolve the limitations, we present Hand4Whole,\nwhich has two strong points over previous works. First, we design Pose2Pose, a\nmodule that utilizes joint features for 3D joint rotations. Using Pose2Pose,\nHand4Whole utilizes hand MCP joint features to predict 3D wrists as MCP joints\nlargely contribute to 3D wrist rotations in the human kinematic chain. Second,\nHand4Whole discards the body feature when predicting 3D finger rotations. Our\nHand4Whole is trained in an end-to-end manner and produces much better 3D hand\nresults than previous whole-body 3D human mesh estimation methods. The codes\nare available here at https://github.com/mks0601/Hand4Whole_RELEASE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_G/0/1/0/all/0/1\">Gyeongsik Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hongsuk Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Density Ratio-Guided Subsampling of Conditional GANs, With Conditioning on a Class or a Continuous Variable. (arXiv:2103.11166v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11166","description":"<p>Recently, subsampling or refining images generated from unconditional GANs\nhas been actively studied to improve the overall image quality. Unfortunately,\nthese methods are often observed less effective or inefficient in handling\nconditional GANs (cGANs) -- conditioning on a class (aka class-conditional\nGANs) or a continuous variable (aka continuous cGANs or CcGANs). In this work,\nwe introduce an effective and efficient subsampling scheme, named conditional\ndensity ratio-guided rejection sampling (cDR-RS), to sample high-quality images\nfrom cGANs. Specifically, we first develop a novel conditional density ratio\nestimation method, termed cDRE-F-cSP, by proposing the conditional Softplus\n(cSP) loss and an improved feature extraction mechanism. We then derive the\nerror bound of a density ratio model trained with the cSP loss. Finally, we\naccept or reject a fake image in terms of its estimated conditional density\nratio. A filtering scheme is also developed to increase fake images' label\nconsistency without losing diversity when sampling from CcGANs. We extensively\ntest the effectiveness and efficiency of cDR-RS in sampling from both\nclass-conditional GANs and CcGANs on five benchmark datasets. When sampling\nfrom class-conditional GANs, cDR-RS outperforms modern state-of-the-art methods\nby a large margin (except DRE-F-SP+RS) in terms of effectiveness. Although the\neffectiveness of cDR-RS is often comparable to that of DRE-F-SP+RS, cDR-RS is\nsubstantially more efficient. When sampling from CcGANs, the superiority of\ncDR-RS is even more noticeable in terms of both effectiveness and efficiency.\nNotably, with the consumption of reasonable computational resources, cDR-RS can\nsubstantially reduce Label Score without decreasing the diversity of\nCcGAN-generated images, while other methods often need to trade much diversity\nfor slightly improved Label Score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_W/0/1/0/all/0/1\">William J. Welch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine learning method for light field refocusing. (arXiv:2103.16020v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.16020","description":"<p>Light field imaging introduced the capability to refocus an image after\ncapturing. Currently there are two popular methods for refocusing,\nshift-and-sum and Fourier slice methods. Neither of these two methods can\nrefocus the light field in real-time without any pre-processing. In this paper\nwe introduce a machine learning based refocusing technique that is capable of\nextracting 16 refocused images with refocusing parameters of\n\\alpha=0.125,0.250,0.375,...,2.0 in real-time. We have trained our network,\nwhich is called RefNet, in two experiments. Once using the Fourier slice method\nas the training -- i.e., \"ground truth\" -- data and another using the\nshift-and-sum method as the training data. We showed that in both cases, not\nonly is the RefNet method at least 134x faster than previous approaches, but\nalso the color prediction of RefNet is superior to both Fourier slice and\nshift-and-sum methods while having similar depth of field and focus distance\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hedayati_E/0/1/0/all/0/1\">Eisa Hedayati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Havens_T/0/1/0/all/0/1\">Timothy C. Havens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bos_J/0/1/0/all/0/1\">Jeremy P. Bos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fourier Image Transformer. (arXiv:2104.02555v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.02555","description":"<p>Transformer architectures show spectacular performance on NLP tasks and have\nrecently also been used for tasks such as image completion or image\nclassification. Here we propose to use a sequential image representation, where\neach prefix of the complete sequence describes the whole image at reduced\nresolution. Using such Fourier Domain Encodings (FDEs), an auto-regressive\nimage completion task is equivalent to predicting a higher resolution output\ngiven a low-resolution input. Additionally, we show that an encoder-decoder\nsetup can be used to query arbitrary Fourier coefficients given a set of\nFourier domain observations. We demonstrate the practicality of this approach\nin the context of computed tomography (CT) image reconstruction. In summary, we\nshow that Fourier Image Transformer (FIT) can be used to solve relevant image\nanalysis tasks in Fourier space, a domain inherently inaccessible to\nconvolutional architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Buchholz_T/0/1/0/all/0/1\">Tim-Oliver Buchholz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jug_F/0/1/0/all/0/1\">Florian Jug</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling and Transferring Knowledge via cGAN-generated Samples for Image Classification and Regression. (arXiv:2104.03164v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03164","description":"<p>Knowledge distillation (KD) has been actively studied for image\nclassification tasks in deep learning, aiming to improve the performance of a\nstudent based on the knowledge from a teacher. However, applying KD in image\nregression with a scalar response variable has been rarely studied, and there\nexists no KD method applicable to both classification and regression tasks yet.\nMoreover, existing KD methods often require a practitioner to carefully select\nor adjust the teacher and student architectures, making these methods less\nflexible in practice. To address the above problems in a unified way, we\npropose a comprehensive KD framework based on cGANs, termed cGAN-KD.\nFundamentally different from existing KD methods, cGAN-KD distills and\ntransfers knowledge from a teacher model to a student model via cGAN-generated\nsamples. This novel mechanism makes cGAN-KD suitable for both classification\nand regression tasks, compatible with other KD methods, and insensitive to the\nteacher and student architectures. An error bound for a student model trained\nin the cGAN-KD framework is derived in this work, providing a theory for why\ncGAN-KD is effective as well as guiding the practical implementation of\ncGAN-KD. Extensive experiments on CIFAR-100 and ImageNet-100 show that we can\ncombine state of the art KD methods with the cGAN-KD framework to yield a new\nstate of the art. Moreover, experiments on Steering Angle and UTKFace\ndemonstrate the effectiveness of cGAN-KD in image regression tasks, where\nexisting KD methods are inapplicable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zuheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Z. Jane Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_W/0/1/0/all/0/1\">William J. Welch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection. (arXiv:2104.09770v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.09770","description":"<p>The widespread dissemination of Deepfakes demands effective approaches that\ncan detect perceptually convincing forged images. In this paper, we aim to\ncapture the subtle manipulation artifacts at different scales using transformer\nmodels. In particular, we introduce a Multi-modal Multi-scale TRansformer\n(M2TR), which operates on patches of different sizes to detect local\ninconsistencies in images at different spatial levels. M2TR further learns to\ndetect forgery artifacts in the frequency domain to complement RGB information\nthrough a carefully designed cross modality fusion block. In addition, to\nstimulate Deepfake detection research, we introduce a high-quality Deepfake\ndataset, SR-DF, which consists of 4,000 DeepFake videos generated by\nstate-of-the-art face swapping and facial reenactment methods. We conduct\nextensive experiments to verify the effectiveness of the proposed method, which\noutperforms state-of-the-art Deepfake detection methods by clear margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junke Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1\">Wenhao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xintong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keypoint Transformer: Solving Joint Identification in Challenging Hands and Object Interactions for Accurate 3D Pose Estimation. (arXiv:2104.14639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14639","description":"<p>We propose a robust and accurate method for estimating the 3D poses of two\nhands in close interaction from a single color image. This is a very\nchallenging problem, as large occlusions and many confusions between the joints\nmay happen. State-of-the-art methods solve this problem by regressing a heatmap\nfor each joint, which requires solving two problems simultaneously: localizing\nthe joints and recognizing them. In this work, we propose to separate these\ntasks by relying on a CNN to first localize joints as 2D keypoints, and on\nself-attention between the CNN features at these keypoints to associate them\nwith the corresponding hand joint. The resulting architecture, which we call\n\"Keypoint Transformer\", is highly efficient as it achieves state-of-the-art\nperformance with roughly half the number of model parameters on the\nInterHand2.6M dataset. We also show it can be easily extended to estimate the\n3D pose of an object manipulated by one or two hands with high performance.\nMoreover, we created a new dataset of more than 75,000 images of two hands\nmanipulating an object fully annotated in 3D and will make it publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hampali_S/0/1/0/all/0/1\">Shreyas Hampali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sayan Deb Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rad_M/0/1/0/all/0/1\">Mahdi Rad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSRNet: Cascaded Selective Resolution Network for Real-time Semantic Segmentation. (arXiv:2106.04400v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04400","description":"<p>Real-time semantic segmentation has received considerable attention due to\ngrowing demands in many practical applications, such as autonomous vehicles,\nrobotics, etc. Existing real-time segmentation approaches often utilize feature\nfusion to improve segmentation accuracy. However, they fail to fully consider\nthe feature information at different resolutions and the receptive fields of\nthe networks are relatively limited, thereby compromising the performance. To\ntackle this problem, we propose a light Cascaded Selective Resolution Network\n(CSRNet) to improve the performance of real-time segmentation through multiple\ncontext information embedding and enhanced feature aggregation. The proposed\nnetwork builds a three-stage segmentation system, which integrates feature\ninformation from low resolution to high resolution and achieves feature\nrefinement progressively. CSRNet contains two critical modules: the Shorted\nPyramid Fusion Module (SPFM) and the Selective Resolution Module (SRM). The\nSPFM is a computationally efficient module to incorporate the global context\ninformation and significantly enlarge the receptive field at each stage. The\nSRM is designed to fuse multi-resolution feature maps with various receptive\nfields, which assigns soft channel attentions across the feature maps and helps\nto remedy the problem caused by multi-scale objects. Comprehensive experiments\non two well-known datasets demonstrate that the proposed CSRNet effectively\nimproves the performance for real-time segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jingjing Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_L/0/1/0/all/0/1\">Lai-Man Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wing-Yin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_P/0/1/0/all/0/1\">Pengfei Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_W/0/1/0/all/0/1\">Weifeng Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2106.14256","description":"<p>Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Bojing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yinxi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1\">Philippe Weitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1\">Johan Lindberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartman_J/0/1/0/all/0/1\">Johan Hartman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1\">Lars Egevad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1\">Henrik Gr&#xf6;nberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1\">Martin Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1\">Mattias Rantalainen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An overview of mixing augmentation methods and augmentation strategies. (arXiv:2107.09887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.09887","description":"<p>Deep Convolutional Neural Networks have made an incredible progress in many\nComputer Vision tasks. This progress, however, often relies on the availability\nof large amounts of the training data, required to prevent over-fitting, which\nin many domains entails significant cost of manual data labeling. An\nalternative approach is application of data augmentation (DA) techniques that\naim at model regularization by creating additional observations from the\navailable ones. This survey focuses on two DA research streams: image mixing\nand automated selection of augmentation strategies. First, the presented\nmethods are briefly described, and then qualitatively compared with respect to\ntheir key characteristics. Various quantitative comparisons are also included\nbased on the results reported in recent DA literature. This review mainly\ncovers the methods published in the materials of top-tier conferences and in\nleading journals in the years 2017-2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lewy_D/0/1/0/all/0/1\">Dominik Lewy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandziuk_J/0/1/0/all/0/1\">Jacek Ma&#x144;dziuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning-Based Unified Framework for Red Lesions Detection on Retinal Fundus Images. (arXiv:2109.05021v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05021","description":"<p>Red-lesions, microaneurysms (MAs) and hemorrhages (HMs), are the early signs\nof diabetic retinopathy (DR). The automatic detection of MAs and HMs on retinal\nfundus images is a challenging task. Most of the existing methods detect either\nonly MAs or only HMs because of the difference in their texture, sizes, and\nmorphology. Though some methods detect both MAs and HMs, they suffer from the\ncurse of dimensionality of shape and colors features and fail to detect all\nshape variations of HMs such as flame-shaped. Leveraging the progress in deep\nlearning, we proposed a two-stream red lesions detection system dealing\nsimultaneously with small and large red lesions. For this system, we introduced\na new ROIs candidates generation method for large red lesions on fundus images;\nit is based on blood vessel segmentation and morphological operations, and\nreduces the computational complexity, and enhances the detection accuracy by\ngenerating a small number of potential candidates. For detection, we proposed a\nframework with two streams. We used pretrained VGGNet as a backbone model and\ncarried out several extensive experiments to tune it for vessels segmentation\nand candidates generation, and finally learning the appropriate mapping, which\nyields better detection of the red lesions comparing with the state-of-the-art\nmethods. The experimental results validated the effectiveness of the system in\nthe detection of both MAs and HMs; it yields higher performance for per lesion\ndetection; its sensitivity equals 0.8589 and good FROC score under 8 FPIs on\nDiaretDB1-MA reports FROC=0.7518, and with SN=0.7552 and good FROC score under\n2,4and 8 FPIs on DiaretDB1-HM, and SN=0.8157 on e-ophtha with overall\nFROC=0.4537 and on ROCh dataset with FROC=0.3461 which is higher than the\nstate-of-the art methods. For DR screening, the system performs well with good\nAUC on DiaretDB1-MA, DiaretDB1-HM, and e-ophtha datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Asiri_N/0/1/0/all/0/1\">Norah Asiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_M/0/1/0/all/0/1\">Muhammad Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adel_F/0/1/0/all/0/1\">Fadwa Al Adel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aboalsamh_H/0/1/0/all/0/1\">Hatim Aboalsamh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HarrisZ$^+$: Harris Corner Selection for Next-Gen Image Matching Pipelines. (arXiv:2109.12925v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.12925","description":"<p>Due to its role in many computer vision tasks, image matching has been\nsubjected to an active investigation by researchers, which has lead to better\nand more discriminant feature descriptors and to more robust matching\nstrategies, also thanks to the advent of the deep learning and the increased\ncomputational power of the modern hardware. Despite of these achievements, the\nkeypoint extraction process at the base of the image matching pipeline has not\nseen equivalent progresses. This paper presents HarrisZ$^+$, an upgrade to the\nHarrisZ corner detector, optimized to synergically take advance of the recent\nimprovements of the other steps of the image matching pipeline. HarrisZ$^+$\ndoes not only consists of a tuning of the setup parameters, but introduces\nfurther refinements to the selection criteria delineated by HarrisZ, so\nproviding more, yet discriminative, keypoints, which are better distributed on\nthe image and with higher localization accuracy. The image matching pipeline\nincluding HarrisZ$^+$, together with the other modern components, obtained in\ndifferent recent matching benchmarks state-of-the-art results among the classic\nimage matching pipelines. These results are quite close to those obtained by\nthe more recent fully deep end-to-end trainable approaches and show that there\nis still a proper margin of improvement that can be granted by the research in\nclassic image matching methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1\">Fabio Bellavia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishkin_D/0/1/0/all/0/1\">Dmytro Mishkin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-to-Fine Reasoning for Visual Question Answering. (arXiv:2110.02526v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.02526","description":"<p>Bridging the semantic gap between image and question is an important step to\nimprove the accuracy of the Visual Question Answering (VQA) task. However, most\nof the existing VQA methods focus on attention mechanisms or visual relations\nfor reasoning the answer, while the features at different semantic levels are\nnot fully utilized. In this paper, we present a new reasoning framework to fill\nthe gap between visual features and semantic clues in the VQA task. Our method\nfirst extracts the features and predicates from the image and question. We then\npropose a new reasoning framework to effectively jointly learn these features\nand predicates in a coarse-to-fine manner. The intensively experimental results\non three large-scale VQA datasets show that our proposed approach achieves\nsuperior accuracy comparing with other state-of-the-art methods. Furthermore,\nour reasoning framework also provides an explainable way to understand the\ndecision of the deep neural network when predicting the answer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh X. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Huy Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1\">Erman Tjiputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPSN: Motion-aware Pseudo Siamese Network for Indoor Video Head Detection in Buildings. (arXiv:2110.03302v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03302","description":"<p>Head detection in the indoor video is an essential component of building\noccupancy detection. While deep models have achieved remarkable progress in\ngeneral object detection, they are not satisfying enough in complex indoor\nscenes. The indoor surveillance video often includes cluttered background\nobjects, among which heads have small scales and diverse poses. In this paper,\nwe propose Motion-aware Pseudo Siamese Network (MPSN), an end-to-end approach\nthat leverages head motion information to guide the deep model to extract\neffective head features in indoor scenarios. By taking the pixel-wise\ndifference of adjacent frames as the auxiliary input, MPSN effectively enhances\nhuman head motion information and removes the irrelevant objects in the\nbackground. Compared with prior methods, it achieves superior performance on\nthe two indoor video datasets. Our experiments show that MPSN successfully\nsuppresses static background objects and highlights the moving instances,\nespecially human heads in indoor videos. We also compare different methods to\ncapture head motion, which demonstrates the simplicity and flexibility of MPSN.\nTo validate the robustness of MPSN, we conduct adversarial experiments with a\nmathematical solution of small perturbations for robust model selection.\nFinally, for confirming its potential in building control systems, we apply\nMPSN to occupancy counting. Code is available at\nhttps://github.com/pl-share/MPSN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kailai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaoteng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qianchuan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastDOG: Fast Discrete Optimization on GPU. (arXiv:2111.10270v3 [math.OC] UPDATED)","link":"http://arxiv.org/abs/2111.10270","description":"<p>We present a massively parallel Lagrange decomposition method for solving\n0--1 integer linear programs occurring in structured prediction. We propose a\nnew iterative update scheme for solving the Lagrangean dual and a perturbation\ntechnique for decoding primal solutions. For representing subproblems we follow\nLange et al. (2021) and use binary decision diagrams (BDDs). Our primal and\ndual algorithms require little synchronization between subproblems and\noptimization over BDDs needs only elementary operations without complicated\ncontrol flow. This allows us to exploit the parallelism offered by GPUs for all\ncomponents of our method. We present experimental results on combinatorial\nproblems from MAP inference for Markov Random Fields, quadratic assignment and\ncell tracking for developmental biology. Our highly parallel GPU implementation\nimproves upon the running times of the algorithms from Lange et al. (2021) by\nup to an order of magnitude. In particular, we come close to or outperform some\nstate-of-the-art specialized heuristics while being problem agnostic. Our\nimplementation is available at https://github.com/LPMP/BDD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/math/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling. (arXiv:2111.12698v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12698","description":"<p>Open-vocabulary instance segmentation aims at segmenting novel classes\nwithout mask annotations. It is an important step toward reducing laborious\nhuman supervision. Most existing works first pretrain a model on captioned\nimages covering many novel classes and then finetune it on limited base classes\nwith mask annotations. However, the high-level textual information learned from\ncaption pretraining alone cannot effectively encode the details required for\npixel-wise segmentation. To address this, we propose a cross-modal\npseudo-labeling framework, which generates training pseudo masks by aligning\nword semantics in captions with visual features of object masks in images.\nThus, our framework is capable of labeling novel classes in captions via their\nword semantics to self-train a student model. To account for noises in pseudo\nmasks, we design a robust student model that selectively distills mask\nknowledge by estimating the mask noise levels, hence mitigating the adverse\nimpact of noisy pseudo masks. By extensive experiments, we show the\neffectiveness of our framework, where we significantly improve mAP score by\n4.5% on MS-COCO and 5.1% on the large-scale Open Images &amp; Conceptual Captions\ndatasets compared to the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1\">Dat Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jiuxiang Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhamifar_E/0/1/0/all/0/1\">Ehsan Elhamifar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Space Smoothing for Individually Fair Representations. (arXiv:2111.13650v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13650","description":"<p>Fair representation learning transforms user data into a representation that\nensures fairness and utility regardless of the downstream application. However,\nlearning individually fair representations, i.e., guaranteeing that similar\nindividuals are treated similarly, remains challenging in high-dimensional\nsettings such as computer vision. In this work, we introduce LASSI, the first\nrepresentation learning method for certifying individual fairness of\nhigh-dimensional data. Our key insight is to leverage recent advances in\ngenerative modeling to capture the set of similar individuals in the generative\nlatent space. This enables us to learn individually fair representations that\nmap similar individuals close together by using adversarial training to\nminimize the distance between their representations. Finally, we employ\nrandomized smoothing to provably map similar individuals close together, in\nturn ensuring that local robustness verification of the downstream application\nresults in end-to-end fairness certification. Our experimental evaluation on\nchallenging real-world image data demonstrates that our method increases\ncertified individual fairness by up to 90% without significantly affecting task\nutility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peychev_M/0/1/0/all/0/1\">Momchil Peychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruoss_A/0/1/0/all/0/1\">Anian Ruoss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balunovic_M/0/1/0/all/0/1\">Mislav Balunovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1\">Maximilian Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Symmetry Detection and Shape Matching for Non-Rigid Point Cloud. (arXiv:2112.02713v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02713","description":"<p>Despite the success of deep functional maps in non-rigid 3D shape matching,\nthere exists no learning framework that models both self-symmetry and shape\nmatching simultaneously. This is despite the fact that errors due to symmetry\nmismatch are a major challenge in non-rigid shape matching. In this paper, we\npropose a novel framework that simultaneously learns both self symmetry as well\nas a pairwise map between a pair of shapes. Our key idea is to couple a self\nsymmetry map and a pairwise map through a regularization term that provides a\njoint constraint on both of them, thereby, leading to more accurate maps. We\nvalidate our method on several benchmarks where it outperforms many competitive\nbaselines on both tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abhishek Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1\">Maks Ovsjanikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"I M Avatar: Implicit Morphable Head Avatars from Videos. (arXiv:2112.07471v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07471","description":"<p>Traditional 3D morphable face models (3DMMs) provide fine-grained control\nover expression but cannot easily capture geometric and appearance details.\nNeural volumetric representations approach photorealism but are hard to animate\nand do not generalize well to unseen expressions. To tackle this problem, we\npropose IMavatar (Implicit Morphable avatar), a novel method for learning\nimplicit head avatars from monocular videos. Inspired by the fine-grained\ncontrol mechanisms afforded by conventional 3DMMs, we represent the expression-\nand pose- related deformations via learned blendshapes and skinning fields.\nThese attributes are pose-independent and can be used to morph the canonical\ngeometry and texture fields given novel expression and pose parameters. We\nemploy ray marching and iterative root-finding to locate the canonical surface\nintersection for each pixel. A key contribution is our novel analytical\ngradient formulation that enables end-to-end training of IMavatars from videos.\nWe show quantitatively and qualitatively that our method improves geometry and\ncovers a more complete expression space compared to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yufeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1\">Victoria Fern&#xe1;ndez Abrevaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_M/0/1/0/all/0/1\">Marcel C. B&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-Key Multimodal Backdoors for Visual Question Answering. (arXiv:2112.07668v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07668","description":"<p>The success of deep learning has enabled advances in multimodal tasks that\nrequire non-trivial fusion of multiple input domains. Although multimodal\nmodels have shown potential in many problems, their increased complexity makes\nthem more vulnerable to attacks. A Backdoor (or Trojan) attack is a class of\nsecurity vulnerability wherein an attacker embeds a malicious secret behavior\ninto a network (e.g. targeted misclassification) that is activated when an\nattacker-specified trigger is added to an input. In this work, we show that\nmultimodal networks are vulnerable to a novel type of attack that we refer to\nas Dual-Key Multimodal Backdoors. This attack exploits the complex fusion\nmechanisms used by state-of-the-art networks to embed backdoors that are both\neffective and stealthy. Instead of using a single trigger, the proposed attack\nembeds a trigger in each of the input modalities and activates the malicious\nbehavior only when both the triggers are present. We present an extensive study\nof multimodal backdoors on the Visual Question Answering (VQA) task with\nmultiple architectures and visual feature backbones. A major challenge in\nembedding backdoors in VQA models is that most models use visual features\nextracted from a fixed pretrained object detector. This is challenging for the\nattacker as the detector can distort or ignore the visual trigger entirely,\nwhich leads to models where backdoors are over-reliant on the language trigger.\nWe tackle this problem by proposing a visual trigger optimization strategy\ndesigned for pretrained object detectors. Through this method, we create\nDual-Key Backdoors with over a 98% attack success rate while only poisoning 1%\nof the training data. Finally, we release TrojVQA, a large collection of clean\nand trojan VQA models to enable research in defending against multimodal\nbackdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Walmer_M/0/1/0/all/0/1\">Matthew Walmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sikka_K/0/1/0/all/0/1\">Karan Sikka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sur_I/0/1/0/all/0/1\">Indranil Sur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Susmit Jha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPTS: Single-Point Text Spotting. (arXiv:2112.07917v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07917","description":"<p>Existing scene text spotting (i.e., end-to-end text detection and\nrecognition) methods rely on costly bounding box annotations (e.g., text-line,\nword-level, or character-level bounding boxes). For the first time, we\ndemonstrate that training scene text spotting models can be achieved with an\nextremely low-cost annotation of a single-point for each instance. We propose\nan end-to-end scene text spotting method that tackles scene text spotting as a\nsequence prediction task. Given an image as input, we formulate the desired\ndetection and recognition results as a sequence of discrete tokens and use an\nauto-regressive Transformer to predict the sequence. The proposed method is\nsimple yet effective, which can achieve state-of-the-art results on widely used\nbenchmarks. Most significantly, we show that the performance is not very\nsensitive to the positions of the point annotation, meaning that it can be much\neasier to be annotated or even be automatically generated than the bounding box\nthat requires precise positions. We believe that such a pioneer attempt\nindicates a significant opportunity for scene text spotting applications of a\nmuch larger scale than previously possible. The code will be publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1\">Dezhi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Mingxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Songxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lianwen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Wanderings of Odysseus in 3D Scenes. (arXiv:2112.09251v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09251","description":"<p>Our goal is to populate digital environments, in which digital humans have\ndiverse body shapes, move perpetually, and have plausible body-scene contact.\nThe core challenge is to generate realistic, controllable, and infinitely long\nmotions for diverse 3D bodies. To this end, we propose generative motion\nprimitives via body surface markers, or GAMMA in short. In our solution, we\ndecompose the long-term motion into a time sequence of motion primitives. We\nexploit body surface markers and conditional variational autoencoder to model\neach motion primitive, and generate long-term motion by implementing the\ngenerative model recursively. To control the motion to reach a goal, we apply a\npolicy network to explore the generative model's latent space and use a\ntree-based search to preserve the motion quality during testing. Experiments\nshow that our method can produce more realistic and controllable motion than\nstate-of-the-art data-driven methods. With conventional path-finding\nalgorithms, the generated human bodies can realistically move long distances\nfor a long period of time in the scene. Code is released for research purposes\nat: \\url{https://yz-cnsdqz.github.io/eigenmotion/GAMMA/}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Queries for Efficient Local Attention. (arXiv:2112.11435v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11435","description":"<p>Vision Transformers (ViT) serve as powerful vision models. Unlike\nconvolutional neural networks, which dominated vision research in previous\nyears, vision transformers enjoy the ability to capture long-range dependencies\nin the data. Nonetheless, an integral part of any transformer architecture, the\nself-attention mechanism, suffers from high latency and inefficient memory\nutilization, making it less suitable for high-resolution input images. To\nalleviate these shortcomings, hierarchical vision models locally employ\nself-attention on non-interleaving windows. This relaxation reduces the\ncomplexity to be linear in the input size; however, it limits the cross-window\ninteraction, hurting the model performance. In this paper, we propose a new\nshift-invariant local attention layer, called query and attend (QnA), that\naggregates the input locally in an overlapping manner, much like convolutions.\nThe key idea behind QnA is to introduce learned queries, which allow fast and\nefficient implementation. We verify the effectiveness of our layer by\nincorporating it into a hierarchical vision transformer model. We show\nimprovements in speed and memory complexity while achieving comparable accuracy\nwith state-of-the-art models. Finally, our layer scales especially well with\nwindow size, requiring up-to x10 less memory while being up-to x5 faster than\nexisting methods. The code is publicly available at\n\\url{https://github.com/moabarar/qna}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1\">Moab Arar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Modal Retrieval with Querybank Normalisation. (arXiv:2112.12777v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12777","description":"<p>Profiting from large-scale training datasets, advances in neural architecture\ndesign and efficient inference, joint embeddings have become the dominant\napproach for tackling cross-modal retrieval. In this work we first show that,\ndespite their effectiveness, state-of-the-art joint embeddings suffer\nsignificantly from the longstanding \"hubness problem\" in which a small number\nof gallery embeddings form the nearest neighbours of many queries. Drawing\ninspiration from the NLP literature, we formulate a simple but effective\nframework called Querybank Normalisation (QB-Norm) that re-normalises query\nsimilarities to account for hubs in the embedding space. QB-Norm improves\nretrieval performance without requiring retraining. Differently from prior\nwork, we show that QB-Norm works effectively without concurrent access to any\ntest set queries. Within the QB-Norm framework, we also propose a novel\nsimilarity normalisation method, the Dynamic Inverted Softmax, that is\nsignificantly more robust than existing approaches. We showcase QB-Norm across\na range of cross modal retrieval models and benchmarks where it consistently\nenhances strong baselines beyond the state of the art. Code is available at\nhttps://vladbogo.github.io/QB-Norm/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogolin_S/0/1/0/all/0/1\">Simion-Vlad Bogolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_I/0/1/0/all/0/1\">Ioana Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch2PQ: Freeform Planar Quadrilateral Mesh Design via a Single Sketch. (arXiv:2201.09367v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2201.09367","description":"<p>The freeform architectural modeling process often involves two important\nstages: concept design and digital modeling. In the first stage, architects\nusually sketch the overall 3D shape and the panel layout on a physical or\ndigital paper briefly. In the second stage, a digital 3D model is created using\nthe sketch as a reference. The digital model needs to incorporate geometric\nrequirements for its components, such as the planarity of panels due to\nconsideration of construction costs, which can make the modeling process more\nchallenging. In this work, we present a novel sketch-based system to bridge the\nconcept design and digital modeling of freeform roof-like shapes represented as\nplanar quadrilateral (PQ) meshes. Our system allows the user to sketch the\nsurface boundary and contour lines under axonometric projection and supports\nthe sketching of occluded regions. In addition, the user can sketch feature\nlines to provide directional guidance to the PQ mesh layout. Given the 2D\nsketch input, we propose a deep neural network to infer in real-time the\nunderlying surface shape along with a dense conjugate direction field, both of\nwhich are used to extract the final PQ mesh. To train and validate our network,\nwe generate a large synthetic dataset that mimics architect sketching of\nfreeform quadrilateral patches. The effectiveness and usability of our system\nare demonstrated with quantitative and qualitative evaluation as well as user\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jabi_W/0/1/0/all/0/1\">Wassim Jabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Juyong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bailin Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Multispectral Satellite Imagery of South American Wildfires Using Deep Learning. (arXiv:2201.09671v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.09671","description":"<p>Since frequent severe droughts are lengthening the dry season in the Amazon\nRainforest, it is important to detect wildfires promptly and forecast possible\nspread for effective suppression response. Current wildfire detection models\nare not versatile enough for the low-technology conditions of South American\nhot spots. This deep learning study first trains a Fully Convolutional Neural\nNetwork on Landsat 8 images of Ecuador and the Galapagos, using Green and\nShort-wave Infrared bands to predict pixel-level binary fire masks. This model\nachieves a 0.962 validation F2 score and a 0.932 F2 score on test data from\nGuyana and Suriname. Afterward, image segmentation is conducted on the Cirrus\nband using K-Means Clustering to simplify continuous pixel values into three\ndiscrete classes representing differing degrees of cirrus cloud contamination.\nThree additional Convolutional Neural Networks are trained to conduct a\nsensitivity analysis measuring the effect of simplified features on model\naccuracy and train time. The Experimental model trained on the segmented cirrus\nimages provides a statistically significant decrease in train time compared to\nthe Control model trained on raw cirrus images, without compromising binary\naccuracy. This proof of concept reveals that feature engineering can improve\nthe performance of wildfire detection models by lowering computational expense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Christopher Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Smart Glasses Dream of Sentimental Visions? Deep Emotionship Analysis for Eyewear Devices. (arXiv:2201.09933v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.09933","description":"<p>Emotion recognition in smart eyewear devices is highly valuable but\nchallenging. One key limitation of previous works is that the\nexpression-related information like facial or eye images is considered as the\nonly emotional evidence. However, emotional status is not isolated; it is\ntightly associated with people's visual perceptions, especially those\nsentimental ones. However, little work has examined such associations to better\nillustrate the cause of different emotions. In this paper, we study the\nemotionship analysis problem in eyewear systems, an ambitious task that\nrequires not only classifying the user's emotions but also semantically\nunderstanding the potential cause of such emotions. To this end, we devise\nEMOShip, a deep-learning-based eyewear system that can automatically detect the\nwearer's emotional status and simultaneously analyze its associations with\nsemantic-level visual perceptions. Experimental studies with 20 participants\ndemonstrate that, thanks to the emotionship awareness, EMOShip not only\nachieves superior emotion recognition accuracy over existing methods (80.2% vs.\n69.4%), but also provides a valuable understanding of the cause of emotions.\nPilot studies with 20 participants further motivate the potential use of\nEMOShip to empower emotion-aware applications, such as emotionship\nself-reflection and emotionship life-logging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yuhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Ning Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Li Shang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bootstrapped Representation Learning for Skeleton-Based Action Recognition. (arXiv:2202.02232v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02232","description":"<p>In this work, we study self-supervised representation learning for 3D\nskeleton-based action recognition. We extend Bootstrap Your Own Latent (BYOL)\nfor representation learning on skeleton sequence data and propose a new data\naugmentation strategy including two asymmetric transformation pipelines. We\nalso introduce a multi-viewpoint sampling method that leverages multiple\nviewing angles of the same action captured by different cameras. In the\nsemi-supervised setting, we show that the performance can be further improved\nby knowledge distillation from wider networks, leveraging once more the\nunlabeled samples. We conduct extensive experiments on the NTU-60 and NTU-120\ndatasets to demonstrate the performance of our proposed method. Our method\nconsistently outperforms the current state of the art on both linear evaluation\nand semi-supervised benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moliner_O/0/1/0/all/0/1\">Olivier Moliner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sangxia Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+%7B%5CAA%7Dstrom_K/0/1/0/all/0/1\">Kalle &#xc5;str&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A comprehensive benchmark analysis for sand dust image reconstruction. (arXiv:2202.03031v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.03031","description":"<p>Numerous sand dust image enhancement algorithms have been proposed in recent\nyears. To our best acknowledge, however, most methods evaluated their\nperformance with no-reference way using few selected real-world images from\ninternet. It is unclear how to quantitatively analysis the performance of the\nalgorithms in a supervised way and how we could gauge the progress in the\nfield. Moreover, due to the absence of large-scale benchmark datasets, there\nare no well-known reports of data-driven based method for sand dust image\nenhancement up till now. To advance the development of deep learning-based\nalgorithms for sand dust image reconstruction, while enabling supervised\nobjective evaluation of algorithm performance. In this paper, we presented a\ncomprehensive perceptual study and analysis of real-world sand dust images,\nthen constructed a Sand-dust Image Reconstruction Benchmark (SIRB) for training\nConvolutional Neural Networks (CNNs) and evaluating algorithms performance. In\naddition, we adopted the existing image transformation neural network trained\non SIRB as baseline to illustrate the generalization of SIRB for training CNNs.\nFinally, we conducted the qualitative and quantitative evaluation to\ndemonstrate the performance and limitations of the state-of-the-arts (SOTA),\nwhich shed light on future research in sand dust image reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Si_Y/0/1/0/all/0/1\">Yazhong Si</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1\">Ya Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yipu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-Level Region Contrast for Object Detection Pre-Training. (arXiv:2202.04639v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04639","description":"<p>In this work we present point-level region contrast, a self-supervised\npre-training approach for the task of object detection. This approach is\nmotivated by the two key factors in detection: localization and recognition.\nWhile accurate localization favors models that operate at the pixel- or\npoint-level, correct recognition typically relies on a more holistic,\nregion-level view of objects. Incorporating this perspective in pre-training,\nour approach performs contrastive learning by directly sampling individual\npoint pairs from different regions. Compared to an aggregated representation\nper region, our approach is more robust to the change in input region quality,\nand further enables us to implicitly improve initial region assignments via\nonline knowledge distillation during training. Both advantages are important\nwhen dealing with imperfect regions encountered in the unsupervised setting.\nExperiments show point-level region contrast improves on state-of-the-art\npre-training methods for object detection and segmentation across multiple\ntasks and datasets, and we provide extensive ablation studies and\nvisualizations to aid understanding. Code will be made available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yutong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinlei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_A/0/1/0/all/0/1\">Alexander C. Berg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FILM: Frame Interpolation for Large Motion. (arXiv:2202.04901v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04901","description":"<p>We present a frame interpolation algorithm that synthesizes multiple\nintermediate frames from two input images with large in-between motion. Recent\nmethods use multiple networks to estimate optical flow or depth and a separate\nnetwork dedicated to frame synthesis. This is often complex and requires scarce\noptical flow or depth ground-truth. In this work, we present a single unified\nnetwork, distinguished by a multi-scale feature extractor that shares weights\nat all scales, and is trainable from frames alone. To synthesize crisp and\npleasing frames, we propose to optimize our network with the Gram matrix loss\nthat measures the correlation difference between feature maps. Our approach\noutperforms state-of-the-art methods on the Xiph large motion benchmark. We\nalso achieve higher scores on Vimeo-90K, Middlebury and UCF101, when comparing\nto methods that use perceptual losses. We study the effect of weight sharing\nand of training with datasets of increasing motion range. Finally, we\ndemonstrate our model's effectiveness in synthesizing high quality and\ntemporally coherent videos on a challenging near-duplicate photos dataset.\nCodes and pre-trained models are available at\nhttps://github.com/google-research/frame-interpolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reda_F/0/1/0/all/0/1\">Fitsum Reda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kontkanen_J/0/1/0/all/0/1\">Janne Kontkanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tabellion_E/0/1/0/all/0/1\">Eric Tabellion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1\">Deqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pantofaru_C/0/1/0/all/0/1\">Caroline Pantofaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Curless_B/0/1/0/all/0/1\">Brian Curless</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAMMA Challenge:Glaucoma grAding from Multi-Modality imAges. (arXiv:2202.06511v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.06511","description":"<p>Color fundus photography and Optical Coherence Tomography (OCT) are the two\nmost cost-effective tools for glaucoma screening. Both two modalities of images\nhave prominent biomarkers to indicate glaucoma suspected. Clinically, it is\noften recommended to take both of the screenings for a more accurate and\nreliable diagnosis. However, although numerous algorithms are proposed based on\nfundus images or OCT volumes in computer-aided diagnosis, there are still few\nmethods leveraging both of the modalities for the glaucoma assessment. Inspired\nby the success of Retinal Fundus Glaucoma Challenge (REFUGE) we held\npreviously, we set up the Glaucoma grAding from Multi-Modality imAges (GAMMA)\nChallenge to encourage the development of fundus \\&amp; OCT-based glaucoma grading.\nThe primary task of the challenge is to grade glaucoma from both the 2D fundus\nimages and 3D OCT scanning volumes. As part of GAMMA, we have publicly released\na glaucoma annotated dataset with both 2D fundus color photography and 3D OCT\nvolumes, which is the first multi-modality dataset for glaucoma grading. In\naddition, an evaluation framework is also established to evaluate the\nperformance of the submitted methods. During the challenge, 1272 results were\nsubmitted, and finally, top-10 teams were selected to the final stage. We\nanalysis their results and summarize their methods in the paper. Since all\nthese teams submitted their source code in the challenge, a detailed ablation\nstudy is also conducted to verify the effectiveness of the particular modules\nproposed. We find many of the proposed techniques are practical for the\nclinical diagnosis of glaucoma. As the first in-depth study of fundus \\&amp; OCT\nmulti-modality glaucoma grading, we believe the GAMMA Challenge will be an\nessential starting point for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Junde Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Huihui Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fengbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiongcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lexing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1\">Qinji Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Sifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xinxing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wensai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lingxiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shihua Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhichao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_C/0/1/0/all/0/1\">Chubin Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xifei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bingyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobbi_R/0/1/0/all/0/1\">Riadh Kobbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiaoying Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlando_J/0/1/0/all/0/1\">Jos&#xe9; Ignacio Orlando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiulan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"H4D: Human 4D Modeling by Learning Neural Compositional Representation. (arXiv:2203.01247v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01247","description":"<p>Despite the impressive results achieved by deep learning based 3D\nreconstruction, the techniques of directly learning to model 4D human captures\nwith detailed geometry have been less studied. This work presents a novel\nframework that can effectively learn a compact and compositional representation\nfor dynamic human by exploiting the human body prior from the widely used SMPL\nparametric model. Particularly, our representation, named H4D, represents a\ndynamic 3D human over a temporal span with the SMPL parameters of shape and\ninitial pose, and latent codes encoding motion and auxiliary information. A\nsimple yet effective linear motion model is proposed to provide a rough and\nregularized motion estimation, followed by per-frame compensation for pose and\ngeometry details with the residual encoded in the auxiliary code. Technically,\nwe introduce novel GRU-based architectures to facilitate learning and improve\nthe representation capability. Extensive experiments demonstrate our method is\nnot only efficacy in recovering dynamic human with accurate motion and detailed\ngeometry, but also amenable to various 4D human related tasks, including motion\nretargeting, motion completion and future prediction. Please check out the\nproject page for video and code: https://boyanjiang.github.io/H4D/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Boyan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingkui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic segmentation with highly imbalanced semantic labels. (arXiv:2203.11692v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.11692","description":"<p>We describe here the panoptic segmentation method we devised for our\nparticipation in the CoNIC: Colon Nuclei Identification and Counting Challenge\nat ISBI 2022. Key features of our method are a weighted loss specifically\nengineered for semantic segmentation of highly imbalanced cell types, and a\nstate-of-the art nuclei instance segmentation model, which we combine in a\nHovernet-like architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rumberger_J/0/1/0/all/0/1\">Josef Lorenz Rumberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baumann_E/0/1/0/all/0/1\">Elias Baumann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hirsch_P/0/1/0/all/0/1\">Peter Hirsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Janowczyk_A/0/1/0/all/0/1\">Andrew Janowczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zlobec_I/0/1/0/all/0/1\">Inti Zlobec</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainmueller_D/0/1/0/all/0/1\">Dagmar Kainmueller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R3M: A Universal Visual Representation for Robot Manipulation. (arXiv:2203.12601v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2203.12601","description":"<p>We study how visual representations pre-trained on diverse human video data\ncan enable data-efficient learning of downstream robotic manipulation tasks.\nConcretely, we pre-train a visual representation using the Ego4D human video\ndataset using a combination of time-contrastive learning, video-language\nalignment, and an L1 penalty to encourage sparse and compact representations.\nThe resulting representation, R3M, can be used as a frozen perception module\nfor downstream policy learning. Across a suite of 12 simulated robot\nmanipulation tasks, we find that R3M improves task success by over 20% compared\nto training from scratch and by over 10% compared to state-of-the-art visual\nrepresentations like CLIP and MoCo. Furthermore, R3M enables a Franka Emika\nPanda arm to learn a range of manipulation tasks in a real, cluttered apartment\ngiven just 20 demonstrations. Code and pre-trained models are available at\nhttps://tinyurl.com/robotr3m.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1\">Suraj Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1\">Aravind Rajeswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vikash Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Vision-Language Parsing: Seamlessly Bridging Visual Scene Graphs with Language Structures via Dependency Relationships. (arXiv:2203.14260v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14260","description":"<p>Understanding realistic visual scene images together with language\ndescriptions is a fundamental task towards generic visual understanding.\nPrevious works have shown compelling comprehensive results by building\nhierarchical structures for visual scenes (e.g., scene graphs) and natural\nlanguages (e.g., dependency trees), individually. However, how to construct a\njoint vision-language (VL) structure has barely been investigated. More\nchallenging but worthwhile, we introduce a new task that targets on inducing\nsuch a joint VL structure in an unsupervised manner. Our goal is to bridge the\nvisual scene graphs and linguistic dependency trees seamlessly. Due to the lack\nof VL structural data, we start by building a new dataset VLParse. Rather than\nusing labor-intensive labeling from scratch, we propose an automatic alignment\nprocedure to produce coarse structures followed by human refinement to produce\nhigh-quality ones. Moreover, we benchmark our dataset by proposing a\ncontrastive learning (CL)-based framework VLGAE, short for Vision-Language\nGraph Autoencoder. Our model obtains superior performance on two derived tasks,\ni.e., language grammar induction and VL phrase grounding. Ablations show the\neffectiveness of both visual cues and dependency relationships on fine-grained\nVL structure construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_C/0/1/0/all/0/1\">Chao Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wenjuan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuhuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zilong Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15359","description":"<p>The networks trained on the long-tailed dataset vary remarkably, despite the\nsame training settings, which shows the great uncertainty in long-tailed\nlearning. To alleviate the uncertainty, we propose a Nested Collaborative\nLearning (NCL), which tackles the problem by collaboratively learning multiple\nexperts together. NCL consists of two core components, namely Nested Individual\nLearning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on\nthe individual supervised learning for each single expert and the knowledge\ntransferring among multiple experts, respectively. To learn representations\nmore thoroughly, both NIL and NBOD are formulated in a nested way, in which the\nlearning is conducted on not just all categories from a full perspective but\nsome hard categories from a partial perspective. Regarding the learning in the\npartial perspective, we specifically select the negative categories with high\npredicted scores as the hard categories by using a proposed Hard Category\nMining (HCM). In the NCL, the learning from two perspectives is nested, highly\nrelated and complementary, and helps the network to capture not only global and\nrobust features but also meticulous distinguishing ability. Moreover,\nself-supervision is further utilized for feature enhancement. Extensive\nexperiments manifest the superiority of our method with outperforming the\nstate-of-the-art whether by using a single model or an ensemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ball 3D Localization From A Single Calibrated Image. (arXiv:2204.00003v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00003","description":"<p>Ball 3D localization in team sports has various applications including\nautomatic offside detection in soccer, or shot release localization in\nbasketball. Today, this task is either resolved by using expensive multi-views\nsetups, or by restricting the analysis to ballistic trajectories. In this work,\nwe propose to address the task on a single image from a calibrated monocular\ncamera by estimating ball diameter in pixels and use the knowledge of real ball\ndiameter in meters. This approach is suitable for any game situation where the\nball is (even partly) visible. To achieve this, we use a small neural network\ntrained on image patches around candidates generated by a conventional ball\ndetector. Besides predicting ball diameter, our network outputs the confidence\nof having a ball in the image patch. Validations on 3 basketball datasets\nreveals that our model gives remarkable predictions on ball 3D localization. In\naddition, through its confidence output, our model improves the detection rate\nby filtering the candidates produced by the detector. The contributions of this\nwork are (i) the first model to address 3D ball localization on a single image,\n(ii) an effective method for ball 3D annotation from single calibrated images,\n(iii) a high quality 3D ball evaluation dataset annotated from a single\nviewpoint. In addition, the code to reproduce this research is be made freely\navailable at https://github.com/gabriel-vanzandycke/deepsport.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zandycke_G/0/1/0/all/0/1\">Gabriel Van Zandycke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1\">Christophe De Vleeschouwer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Learning of Feature Extraction and Cost Aggregation for Semantic Correspondence. (arXiv:2204.02164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02164","description":"<p>Establishing dense correspondences across semantically similar images is one\nof the challenging tasks due to the significant intra-class variations and\nbackground clutters. To solve these problems, numerous methods have been\nproposed, focused on learning feature extractor or cost aggregation\nindependently, which yields sub-optimal performance. In this paper, we propose\na novel framework for jointly learning feature extraction and cost aggregation\nfor semantic correspondence. By exploiting the pseudo labels from each module,\nthe networks consisting of feature extraction and cost aggregation modules are\nsimultaneously learned in a boosting fashion. Moreover, to ignore unreliable\npseudo labels, we present a confidence-aware contrastive loss function for\nlearning the networks in a weakly-supervised manner. We demonstrate our\ncompetitive results on standard benchmarks for semantic correspondence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1\">Youngjo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Mira Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M$^2$BEV: Multi-Camera Joint 3D Detection and Segmentation with Unified Birds-Eye View Representation. (arXiv:2204.05088v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.05088","description":"<p>In this paper, we propose M$^2$BEV, a unified framework that jointly performs\n3D object detection and map segmentation in the Birds Eye View~(BEV) space with\nmulti-camera image inputs. Unlike the majority of previous works which\nseparately process detection and segmentation, M$^2$BEV infers both tasks with\na unified model and improves efficiency. M$^2$BEV efficiently transforms\nmulti-view 2D image features into the 3D BEV feature in ego-car coordinates.\nSuch BEV representation is important as it enables different tasks to share a\nsingle encoder. Our framework further contains four important designs that\nbenefit both accuracy and efficiency: (1) An efficient BEV encoder design that\nreduces the spatial dimension of a voxel feature map. (2) A dynamic box\nassignment strategy that uses learning-to-match to assign ground-truth 3D boxes\nwith anchors. (3) A BEV centerness re-weighting that reinforces with larger\nweights for more distant predictions, and (4) Large-scale 2D detection\npre-training and auxiliary supervision. We show that these designs\nsignificantly benefit the ill-posed camera-based 3D perception tasks where\ndepth information is missing. M$^2$BEV is memory efficient, allowing\nsignificantly higher resolution images as input, with faster inference speed.\nExperiments on nuScenes show that M$^2$BEV achieves state-of-the-art results in\nboth 3D object detection and BEV segmentation, with the best single model\nachieving 42.5 mAP and 57.0 mIoU in these two tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Daquan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Philion_J/0/1/0/all/0/1\">Jonah Philion</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transparent Shape from Single Polarization Images. (arXiv:2204.06331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06331","description":"<p>This paper presents a data-driven approach for transparent shape from\npolarization. Due to the inherent high transmittance, the previous shape from\npolarization(SfP) methods based on specular reflection model have difficulty in\nestimating transparent shape, and the lack of datasets for transparent SfP also\nlimits the application of the data-driven approach. Hence, we construct the\ntransparent SfP dataset which consists of both synthetic and real-world\ndatasets. To determine the reliability of the physics-based reflection model,\nwe define the physics-based prior confidence by exploiting the inherent fault\nof polarization information, then we propose a multi-branch fusion network to\nembed the confidence. Experimental results show that our approach outperforms\nother SfP methods. Compared with the previous method, the mean and median\nangular error of our approach are reduced from $19.00^\\circ$ and $14.91^\\circ$\nto $16.72^\\circ$ and $13.36^\\circ$, and the accuracy $11.25^\\circ, 22.5^\\circ,\n30^\\circ$ are improved from $38.36\\%, 77.36\\%, 87.48\\%$ to $45.51\\%, 78.86\\%,\n89.98\\%$, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1\">Mingqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Chongkun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhendong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junnan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xueqian Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in the Frequency Domain. (arXiv:2204.06718v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) has achieved impressive success in\ncomputer vision during the past few decades. As the core of CNNs, the image\nconvolution operation helps CNNs to get good performance on image-related\ntasks. However, the image convolution is hard to be implemented and\nparallelized. This paper proposes a novel neural network model, namely CEMNet,\nwhich can be trained in the frequency domain. The most important motivation of\nthis research is that we can use the straightforward element-wise\nmultiplication operation to replace the image convolution in the frequency\ndomain based on the Cross-Correlation Theorem. We further introduce a Weight\nFixation mechanism to alleviate the problem of over-fitting, and analyze the\nworking behavior of Batch Normalization, Leaky ReLU, and Dropout in the\nfrequency domain to design their counterparts for CEMNet. Also, to deal with\ncomplex inputs brought by Discrete Fourier Transform, we design a two-branches\nnetwork structure for CEMNet. Experimental results imply that CEMNet achieves\ngood performance on MNIST and CIFAR-10 databases. To the best of our knowledge,\nCEMNet is the first model trained in Fourier Domain that achieves more than\n70\\% validation accuracy on CIFAR-10 database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yixin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_X/0/1/0/all/0/1\">Xin Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.06929","description":"<p>Ultrasound (US) imaging is widely used for anatomical structure inspection in\nclinical diagnosis. The training of new sonographers and deep learning based\nalgorithms for US image analysis usually requires a large amount of data.\nHowever, obtaining and labeling large-scale US imaging data are not easy tasks,\nespecially for diseases with low incidence. Realistic US image synthesis can\nalleviate this problem to a great extent. In this paper, we propose a\ngenerative adversarial network (GAN) based image synthesis framework. Our main\ncontributions include: 1) we present the first work that can synthesize\nrealistic B-mode US images with high-resolution and customized texture editing\nfeatures; 2) to enhance structural details of generated images, we propose to\nintroduce auxiliary sketch guidance into a conditional GAN. We superpose the\nedge sketch onto the object mask and use the composite mask as the network\ninput; 3) to generate high-resolution US images, we adopt a progressive\ntraining strategy to gradually generate high-resolution images from\nlow-resolution images. In addition, a feature loss is proposed to minimize the\ndifference of high-level features between the generated and real images, which\nfurther improves the quality of generated images; 4) the proposed US image\nsynthesis method is quite universal and can also be generalized to the US\nimages of other anatomical structures besides the three ones tested in our\nstudy (lung, hip joint, and ovary); 5) extensive experiments on three large US\nimage datasets are conducted to validate our method. Ablation studies,\ncustomized texture editing, user studies, and segmentation tests demonstrate\npromising results of our method in synthesizing realistic US images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jiamin Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_S/0/1/0/all/0/1\">Shuangchi He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zejian Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1\">Wufeng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention Mechanism based Cognition-level Scene Understanding. (arXiv:2204.08027v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08027","description":"<p>Given a question-image input, the Visual Commonsense Reasoning (VCR) model\ncan predict an answer with the corresponding rationale, which requires\ninference ability from the real world. The VCR task, which calls for exploiting\nthe multi-source information as well as learning different levels of\nunderstanding and extensive commonsense knowledge, is a cognition-level scene\nunderstanding task. The VCR task has aroused researchers' interest due to its\nwide range of applications, including visual question answering, automated\nvehicle systems, and clinical decision support. Previous approaches to solving\nthe VCR task generally rely on pre-training or exploiting memory with long\ndependency relationship encoded models. However, these approaches suffer from a\nlack of generalizability and losing information in long sequences. In this\npaper, we propose a parallel attention-based cognitive VCR network PAVCR, which\nfuses visual-textual information efficiently and encodes semantic information\nin parallel to enable the model to capture rich information for cognition-level\ninference. Extensive experiments show that the proposed model yields\nsignificant improvements over existing methods on the benchmark VCR dataset.\nMoreover, the proposed model provides intuitive interpretation into visual\ncommonsense reasoning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xuejiao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quy_T/0/1/0/all/0/1\">Tai Le Quy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1\">Kea Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palade_V/0/1/0/all/0/1\">Vasile Palade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haque_I/0/1/0/all/0/1\">Israat Haque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1\">Chris Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenbin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Super-Resolution. (arXiv:2204.08192v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.08192","description":"<p>Super-Resolution is the technique to improve the quality of a low-resolution\nphoto by boosting its plausible resolution. The computer vision community has\nextensively explored the area of Super-Resolution. However, previous\nSuper-Resolution methods require vast amounts of data for training which\nbecomes problematic in domains where very few low-resolution, high-resolution\npairs might be available. One such area is statistical downscaling, where\nsuper-resolution is increasingly being used to obtain high-resolution climate\ninformation from low-resolution data. Acquiring high-resolution climate data is\nextremely expensive and challenging. To reduce the cost of generating\nhigh-resolution climate information, Super-Resolution algorithms should be able\nto train with a limited number of low-resolution, high-resolution pairs. This\npaper tries to solve the aforementioned problem by introducing a\nsemi-supervised way to perform super-resolution that can generate sharp,\nhigh-resolution images with as few as 500 paired examples. The proposed\nsemi-supervised technique can be used as a plug-and-play module with any\nsupervised GAN-based Super-Resolution method to enhance its performance. We\nquantitatively and qualitatively analyze the performance of the proposed model\nand compare it with completely supervised methods as well as other unsupervised\ntechniques. Comprehensive evaluations show the superiority of our method over\nother methods on different metrics. We also offer the applicability of our\napproach in statistical downscaling to obtain high-resolution climate images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1\">Ankur Singh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rai_P/0/1/0/all/0/1\">Piyush Rai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MHSCNet: A Multimodal Hierarchical Shot-aware Convolutional Network for Video Summarization. (arXiv:2204.08352v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08352","description":"<p>Video summarization intends to produce a concise video summary by effectively\ncapturing and combining the most informative parts of the whole content.\nExisting approaches for video summarization regard the task as a frame-wise\nkeyframe selection problem and generally construct the frame-wise\nrepresentation by combining the long-range temporal dependency with the\nunimodal or bimodal information. However, the optimal video summaries need to\nreflect the most valuable keyframe with its own information, and one with\nsemantic power of the whole content. Thus, it is critical to construct a more\npowerful and robust frame-wise representation and predict the frame-level\nimportance score in a fair and comprehensive manner. To tackle the above\nissues, we propose a multimodal hierarchical shot-aware convolutional network,\ndenoted as MHSCNet, to enhance the frame-wise representation via combining the\ncomprehensive available multimodal information. Specifically, we design a\nhierarchical ShotConv network to incorporate the adaptive shot-aware\nframe-level representation by considering the short-range and long-range\ntemporal dependency. Based on the learned shot-aware representations, MHSCNet\ncan predict the frame-level importance score in the local and global view of\nthe video. Extensive experiments on two standard video summarization datasets\ndemonstrate that our proposed method consistently outperforms state-of-the-art\nbaselines. Source code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wujiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaoshuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qiongxu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yunan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaobo Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yifei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking. (arXiv:2204.08387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.08387","description":"<p>Self-supervised pre-training techniques have achieved remarkable progress in\nDocument AI. Most multimodal pre-trained models use a masked language modeling\nobjective to learn bidirectional representations on the text modality, but they\ndiffer in pre-training objectives for the image modality. This discrepancy adds\ndifficulty to multimodal representation learning. In this paper, we propose\nLayoutLMv3 to pre-train multimodal Transformers for Document AI with unified\ntext and image masking. Additionally, LayoutLMv3 is pre-trained with a\nword-patch alignment objective to learn cross-modal alignment by predicting\nwhether the corresponding image patch of a text word is masked. The simple\nunified architecture and training objectives make LayoutLMv3 a general-purpose\npre-trained model for both text-centric and image-centric Document AI tasks.\nExperimental results show that LayoutLMv3 achieves state-of-the-art performance\nnot only in text-centric tasks, including form understanding, receipt\nunderstanding, and document visual question answering, but also in\nimage-centric tasks such as document image classification and document layout\nanalysis. The code and models are publicly available at\nhttps://aka.ms/layoutlmv3.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yupan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Consistency Regularization for Semi-supervised Change Detection in Remote Sensing Images. (arXiv:2204.08454v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.08454","description":"<p>Remote-sensing (RS) Change Detection (CD) aims to detect \"changes of\ninterest\" from co-registered bi-temporal images. The performance of existing\ndeep supervised CD methods is attributed to the large amounts of annotated data\nused to train the networks. However, annotating large amounts of remote sensing\nimages is labor-intensive and expensive, particularly with bi-temporal images,\nas it requires pixel-wise comparisons by a human expert. On the other hand, we\noften have access to unlimited unlabeled multi-temporal RS imagery thanks to\never-increasing earth observation programs. In this paper, we propose a simple\nyet effective way to leverage the information from unlabeled bi-temporal images\nto improve the performance of CD approaches. More specifically, we propose a\nsemi-supervised CD model in which we formulate an unsupervised CD loss in\naddition to the supervised Cross-Entropy (CE) loss by constraining the output\nchange probability map of a given unlabeled bi-temporal image pair to be\nconsistent under the small random perturbations applied on the deep feature\ndifference map that is obtained by subtracting their latent feature\nrepresentations. Experiments conducted on two publicly available CD datasets\nshow that the proposed semi-supervised CD method can reach closer to the\nperformance of supervised CD even with access to as little as 10% of the\nannotated training data. Code available at https://github.com/wgcban/SemiCD\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bandara_W/0/1/0/all/0/1\">Wele Gedara Chaminda Bandara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Land Cover Classification from Remote Sensing Images Based on Multi-Scale Fully Convolutional Network. (arXiv:2008.00168v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2008.00168","description":"<p>In this paper, a Multi-Scale Fully Convolutional Network (MSFCN) with\nmulti-scale convolutional kernel is proposed to exploit discriminative\nrepresentations from two-dimensional (2D) satellite images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1\">Chenxi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Binary Segmentation of Seismic Facies Using Encoder-Decoder Neural Networks. (arXiv:2012.03675v1 [eess.IV] CROSS LISTED)","link":"http://arxiv.org/abs/2012.03675","description":"<p>The interpretation of seismic data is vital for characterizing sediments'\nshape in areas of geological study. In seismic interpretation, deep learning\nbecomes useful for reducing the dependence on handcrafted facies segmentation\ngeometry and the time required to study geological areas. This work presents a\nDeep Neural Network for Facies Segmentation (DNFS) to obtain state-of-the-art\nresults for seismic facies segmentation. DNFS is trained using a combination of\ncross-entropy and Jaccard loss functions. Our results show that DNFS obtains\nhighly detailed predictions for seismic facies segmentation using fewer\nparameters than StNet and U-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lima_G/0/1/0/all/0/1\">Gefersom Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ramos_G/0/1/0/all/0/1\">Gabriel Ramos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rigo_S/0/1/0/all/0/1\">Sandro Rigo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeiser_F/0/1/0/all/0/1\">Felipe Zeiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silveira_A/0/1/0/all/0/1\">Ariane da Silveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}