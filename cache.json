{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-06T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ZeroBERTo -- Leveraging Zero-Shot Text Classification by Topic Modeling. (arXiv:2201.01337v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01337","description":"<p>Traditional text classification approaches often require a good amount of\nlabeled data, which is difficult to obtain, especially in restricted domains or\nless widespread languages. This lack of labeled data has led to the rise of\nlow-resource methods, that assume low data availability in natural language\nprocessing. Among them, zero-shot learning stands out, which consists of\nlearning a classifier without any previously labeled data. The best results\nreported with this approach use language models such as Transformers, but fall\ninto two problems: high execution time and inability to handle long texts as\ninput. This paper proposes a new model, ZeroBERTo, which leverages an\nunsupervised clustering step to obtain a compressed data representation before\nthe classification task. We show that ZeroBERTo has better performance for long\ninputs and shorter execution time, outperforming XLM-R by about 12% in the F1\nscore in the FolhaUOL dataset. Keywords: Low-Resource NLP, Unlabeled data,\nZero-Shot Learning, Topic Modeling, Transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alcoforado_A/0/1/0/all/0/1\">Alexandre Alcoforado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraz_T/0/1/0/all/0/1\">Thomas Palmeira Ferraz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerber_R/0/1/0/all/0/1\">Rodrigo Gerber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustos_E/0/1/0/all/0/1\">Enzo Bustos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Andr&#xe9; Seidel Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veloso_B/0/1/0/all/0/1\">Bruno Miguel Veloso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siqueira_F/0/1/0/all/0/1\">Fabio Levy Siqueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1\">Anna Helena Reali Costa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Hierarchical Model for Spoken Language Recognition. (arXiv:2201.01364v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01364","description":"<p>Spoken language recognition (SLR) refers to the automatic process used to\ndetermine the language present in a speech sample. SLR is an important task in\nits own right, for example, as a tool to analyze or categorize large amounts of\nmulti-lingual data. Further, it is also an essential tool for selecting\ndownstream applications in a work flow, for example, to chose appropriate\nspeech recognition or machine translation models. SLR systems are usually\ncomposed of two stages, one where an embedding representing the audio sample is\nextracted and a second one which computes the final scores for each language.\nIn this work, we approach the SLR task as a detection problem and implement the\nsecond stage as a probabilistic linear discriminant analysis (PLDA) model. We\nshow that discriminative training of the PLDA parameters gives large gains with\nrespect to the usual generative training. Further, we propose a novel\nhierarchical approach were two PLDA models are trained, one to generate scores\nfor clusters of highly related languages and a second one to generate scores\nconditional to each cluster. The final language detection scores are computed\nas a combination of these two sets of scores. The complete model is trained\ndiscriminatively to optimize a cross-entropy objective. We show that this\nhierarchical approach consistently outperforms the non-hierarchical one for\ndetection of highly related languages, in many cases by large margins. We train\nour systems on a collection of datasets including 100 languages and test them\nboth on matched and mismatched conditions, showing that the gains are robust to\ncondition mismatch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castan_D/0/1/0/all/0/1\">Diego Castan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLaren_M/0/1/0/all/0/1\">Mitchell McLaren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawson_A/0/1/0/all/0/1\">Aaron Lawson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining Adverse Drug Reactions from Unstructured Mediums at Scale. (arXiv:2201.01405v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01405","description":"<p>Adverse drug reactions / events (ADR/ADE) have a major impact on patient\nhealth and health care costs. Detecting ADR's as early as possible and sharing\nthem with regulators, pharma companies, and healthcare providers can prevent\nmorbidity and save many lives. While most ADR's are not reported via formal\nchannels, they are often documented in a variety of unstructured conversations\nsuch as social media posts by patients, customer support call transcripts, or\nCRM notes of meetings between healthcare providers and pharma sales reps. In\nthis paper, we propose a natural language processing (NLP) solution that\ndetects ADR's in such unstructured free-text conversations, which improves on\nprevious work in three ways. First, a new Named Entity Recognition (NER) model\nobtains new state-of-the-art accuracy for ADR and Drug entity extraction on the\nADE, CADEC, and SMM4H benchmark datasets (91.75%, 78.76%, and 83.41% F1 scores\nrespectively). Second, two new Relation Extraction (RE) models are introduced -\none based on BioBERT while the other utilizing crafted features over a Fully\nConnected Neural Network (FCNN) - are shown to perform on par with existing\nstate-of-the-art models, and outperform them when trained with a supplementary\nclinician-annotated RE dataset. Third, a new text classification model, for\ndeciding if a conversation includes an ADR, obtains new state-of-the-art\naccuracy on the CADEC dataset (86.69% F1 score). The complete solution is\nimplemented as a unified NLP pipeline in a production-grade library built on\ntop of Apache Spark, making it natively scalable and able to process millions\nof batch or streaming records on commodity clusters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Talby_H/0/1/0/all/0/1\">Hasham Ul Haq Veysel Kocaman David Talby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hyperparameter-free Continuous Learning for Domain Classification in Natural Language Understanding. (arXiv:2201.01420v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01420","description":"<p>Domain classification is the fundamental task in natural language\nunderstanding (NLU), which often requires fast accommodation to new emerging\ndomains. This constraint makes it impossible to retrain all previous domains,\neven if they are accessible to the new model. Most existing continual learning\napproaches suffer from low accuracy and performance fluctuation, especially\nwhen the distributions of old and new data are significantly different. In\nfact, the key real-world problem is not the absence of old data, but the\ninefficiency to retrain the model with the whole old dataset. Is it potential\nto utilize some old data to yield high accuracy and maintain stable\nperformance, while at the same time, without introducing extra hyperparameters?\nIn this paper, we proposed a hyperparameter-free continual learning model for\ntext data that can stably produce high performance under various environments.\nSpecifically, we utilize Fisher information to select exemplars that can\n\"record\" key information of the original model. Also, a novel scheme called\ndynamical weight consolidation is proposed to enable hyperparameter-free\nlearning during the retrain process. Extensive experiments demonstrate that\nbaselines suffer from fluctuated performance and therefore useless in practice.\nOn the contrary, our proposed model CCFI significantly and consistently\noutperforms the best state-of-the-art method by up to 20% in average accuracy,\nand each component of CCFI contributes effectively to overall performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Ting Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Changsheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Learning from Naturally Imbalanced Pseudo-Labels for Zero-Shot and Semi-Supervised Learning. (arXiv:2201.01490v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01490","description":"<p>This work studies the bias issue of pseudo-labeling, a natural phenomenon\nthat widely occurs but often overlooked by prior research. Pseudo-labels are\ngenerated when a classifier trained on source data is transferred to unlabeled\ntarget data. We observe heavy long-tailed pseudo-labels when a semi-supervised\nlearning model FixMatch predicts labels on the unlabeled set even though the\nunlabeled data is curated to be balanced. Without intervention, the training\nmodel inherits the bias from the pseudo-labels and end up being sub-optimal. To\neliminate the model bias, we propose a simple yet effective method DebiasMatch,\ncomprising of an adaptive debiasing module and an adaptive marginal loss. The\nstrength of debiasing and the size of margins can be automatically adjusted by\nmaking use of an online updated queue. Benchmarked on ImageNet-1K, DebiasMatch\nsignificantly outperforms previous state-of-the-arts by more than 26% and 8.7%\non semi-supervised learning (0.2% annotated data) and zero-shot learning tasks\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monitoring Energy Trends through Automatic Information Extraction. (arXiv:2201.01559v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01559","description":"<p>Energy research is of crucial public importance but the use of computer\nscience technologies like automatic text processing and data management for the\nenergy domain is still rare. Employing these technologies in the energy domain\nwill be a significant contribution to the interdisciplinary topic of ``energy\ninformatics\", just like the related progress within the interdisciplinary area\nof ``bioinformatics\". In this paper, we present the architecture of a Web-based\nsemantic system called EneMonIE (Energy Monitoring through Information\nExtraction) for monitoring up-to-date energy trends through the use of\nautomatic, continuous, and guided information extraction from diverse types of\nmedia available on the Web. The types of media handled by the system will\ninclude online news articles, social media texts, online news videos, and\nopen-access scholarly papers and technical reports as well as various numeric\nenergy data made publicly available by energy organizations. The system will\nutilize and contribute to the energy-related ontologies and its ultimate form\nwill comprise components for (i) text categorization, (ii) named entity\nrecognition, (iii) temporal expression extraction, (iv) event extraction, (v)\nsocial network construction, (vi) sentiment analysis, (vii) information fusion\nand summarization, (viii) media interlinking, and (ix) Web-based information\nretrieval and visualization. Wits its diverse data sources, automatic text\nprocessing capabilities, and presentation facilities open for public use;\nEneMonIE will be an important source of distilled and concise information for\ndecision-makers including energy generation, transmission, and distribution\nsystem operators, energy research centres, related investors and entrepreneurs\nas well as for academicians, students, other individuals interested in the pace\nof energy events and technologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kucuk_D/0/1/0/all/0/1\">Dilek K&#xfc;&#xe7;&#xfc;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need In Sign Language Production. (arXiv:2201.01609v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01609","description":"<p>Sign Language is the dominant form of communication language used in the deaf\nand hearing-impaired community. To make an easy and mutual communication\nbetween the hearing-impaired and the hearing communities, building a robust\nsystem capable of translating the spoken language into sign language and vice\nversa is fundamental. To this end, sign language recognition and production are\ntwo necessary parts for making such a two-way system. Sign language recognition\nand production need to cope with some critical challenges. In this survey, we\nreview recent advances in Sign Language Production (SLP) and related areas\nusing deep learning. To have more realistic perspectives to sign language, we\npresent an introduction to the Deaf culture, Deaf centers, psychological\nperspective of sign language, the main differences between spoken language and\nsign language. Furthermore, we present the fundamental components of a\nbi-directional sign language translation system, discussing the main challenges\nin this area. Also, the backbone architectures and methods in SLP are briefly\nintroduced and the proposed taxonomy on SLP is presented. Finally, a general\nframework for SLP and performance evaluation, and also a discussion on the\nrecent developments, advantages, and limitations in SLP, commenting on possible\nlines for future research are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SMDT: Selective Memory-Augmented Neural Document Translation. (arXiv:2201.01631v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01631","description":"<p>Existing document-level neural machine translation (NMT) models have\nsufficiently explored different context settings to provide guidance for target\ngeneration. However, little attention is paid to inaugurate more diverse\ncontext for abundant context information. In this paper, we propose a Selective\nMemory-augmented Neural Document Translation model to deal with documents\ncontaining large hypothesis space of the context. Specifically, we retrieve\nsimilar bilingual sentence pairs from the training corpus to augment global\ncontext and then extend the two-stream attention model with selective mechanism\nto capture local context and diverse global contexts. This unified approach\nallows our model to be trained elegantly on three publicly document-level\nmachine translation datasets and significantly outperforms previous\ndocument-level NMT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haoyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongdong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relationship extraction for knowledge graph creation from biomedical literature. (arXiv:2201.01647v1 [cs.AI])","link":"http://arxiv.org/abs/2201.01647","description":"<p>Biomedical research is growing in such an exponential pace that scientists,\nresearchers and practitioners are no more able to cope with the amount of\npublished literature in the domain. The knowledge presented in the literature\nneeds to be systematized in such a ways that claims and hypothesis can be\neasily found, accessed and validated. Knowledge graphs can provide such\nframework for semantic knowledge representation from literature. However, in\norder to build knowledge graph, it is necessary to extract knowledge in form of\nrelationships between biomedical entities and normalize both entities and\nrelationship types. In this paper, we present and compare few rule-based and\nmachine learning-based (Naive Bayes, Random Forests as examples of traditional\nmachine learning methods and T5-based model as an example of modern deep\nlearning) methods for scalable relationship extraction from biomedical\nliterature for the integration into the knowledge graphs. We examine how\nresilient are these various methods to unbalanced and fairly small datasets,\nshowing that T5 model handles well both small datasets, due to its pre-training\non large C4 dataset as well as unbalanced data. The best performing model was\nT5 model fine-tuned on balanced data, with reported F1-score of 0.88.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Milosevic_N/0/1/0/all/0/1\">Nikola Milosevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thielemann_W/0/1/0/all/0/1\">Wolfgang Thielemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strategies of Effective Digitization of Commentaries and Sub-commentaries: Towards the Construction of Textual History. (arXiv:2201.01693v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01693","description":"<p>This paper describes additional aspects of a digital tool called the 'Textual\nHistory Tool'. We describe its various salient features with special reference\nto those of its features that may help the philologist digitize commentaries\nand sub-commentaries on a text. This tool captures the historical evolution of\na text through various temporal stages, and interrelated data culled from\nvarious types of related texts. We use the text of the K\\=a\\'sik\\=avrtti (KV)\nas a sample text, and with the help of philologists, we digitize the\ncommentaries available to us. We digitize the Ny\\=asa (Ny), the Padama\\~njar\\=i\n(Pm) and sub commentaries on the KV text known as the Tantraprad\\=ipa (Tp), and\nthe Makaranda (Mk). We divide each commentary and sub-commentary into\nfunctional units and describe the methodology and motivation behind the\nfunctional unit division. Our functional unit division helps generate more\naccurate phylogenetic trees for the text, based on distance methods using the\ndata entered in the tool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_M/0/1/0/all/0/1\">Malhar Kulkarni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodekar_S/0/1/0/all/0/1\">Sayali Ghodekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahrs_E/0/1/0/all/0/1\">Eivind Kahrs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Some Strategies to Capture Karaka-Yogyata with Special Reference to apadana. (arXiv:2201.01700v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01700","description":"<p>In today's digital world language technology has gained importance. Several\nsoftwares, have been developed and are available in the field of computational\nlinguistics. Such tools play a crucial role in making classical language texts\neasily accessible. Some Indian philosophical schools have contributed towards\nvarious techniques of verbal cognition to analyze sentences correctly. These\ntheories can be used to build computational tools for word sense disambiguation\n(WSD). In the absence of WSD, one cannot have proper verbal cognition. These\ntheories considered the concept of 'Yogyat\\=a' (congruity or compatibility) as\nthe indispensable cause of verbal cognition. In this work, we come up with some\ninsights on the basis of these theories to create a tool that will capture\nYogyat\\=a of words. We describe the problem of ambiguity in a text and present\na method to resolve it computationally with the help of Yogyat\\=a. Here, only\ntwo major schools i.e. Ny\\=aya and Vy\\=akarana are considered. Our paper\nattempts to show the implication of the creation of our tool in this area.\nAlso, our tool involves the creation of an 'ontological tag-set' as well as\nstrategies to mark up the lexicon. The introductory description of ablation is\nalso covered in this paper. Such strategies and some case studies shall form\nthe core of our paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salaskar_S/0/1/0/all/0/1\">Swaraja Salaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulkarni_M/0/1/0/all/0/1\">Malhar Kulkarni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi Document Reading Comprehension. (arXiv:2201.01706v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01706","description":"<p>Reading Comprehension (RC) is a task of answering a question from a given\npassage or a set of passages. In the case of multiple passages, the task is to\nfind the best possible answer to the question. Recent trials and experiments in\nthe field of Natural Language Processing (NLP) have proved that machines can be\nprovided with the ability to not only process the text in the passage and\nunderstand its meaning to answer the question from the passage, but also can\nsurpass the Human Performance on many datasets such as Standford's Question\nAnswering Dataset (SQuAD). This paper presents a study on Reading Comprehension\nand its evolution in Natural Language Processing over the past few decades. We\nshall also study how the task of Single Document Reading Comprehension acts as\na building block for our Multi-Document Reading Comprehension System. In the\nlatter half of the paper, we'll be studying about a recently proposed model for\nMulti-Document Reading Comprehension - RE3QA that is comprised of a Reader,\nRetriever, and a Re-ranker based network to fetch the best possible answer from\na given set of passages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chawla_A/0/1/0/all/0/1\">Avi Chawla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atomized Search Length: Beyond User Models. (arXiv:2201.01745v1 [cs.IR])","link":"http://arxiv.org/abs/2201.01745","description":"<p>We argue that current IR metrics, modeled on optimizing user experience,\nmeasure too narrow a portion of the IR space. If IR systems are weak, these\nmetrics undersample or completely filter out the deeper documents that need\nimprovement. If IR systems are relatively strong, these metrics undersample\ndeeper relevant documents that could underpin even stronger IR systems, ones\nthat could present content from tens or hundreds of relevant documents in a\nuser-digestible hierarchy or text summary. We reanalyze over 70 TREC tracks\nfrom the past 28 years, showing that roughly half undersample top ranked\ndocuments and nearly all undersample tail documents. We show that in the 2020\nDeep Learning tracks, neural systems were actually near-optimal at top-ranked\ndocuments, compared to only modest gains over BM25 on tail documents. Our\nanalysis is based on a simple new systems-oriented metric, 'atomized search\nlength', which is capable of accurately and evenly measuring all relevant\ndocuments at any depth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alex_J/0/1/0/all/0/1\">John Alex</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1\">Keith Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-automatic WordNet Linking using Word Embeddings. (arXiv:2201.01747v1 [cs.CL])","link":"http://arxiv.org/abs/2201.01747","description":"<p>Wordnets are rich lexico-semantic resources. Linked wordnets are extensions\nof wordnets, which link similar concepts in wordnets of different languages.\nSuch resources are extremely useful in many Natural Language Processing (NLP)\napplications, primarily those based on knowledge-based approaches. In such\napproaches, these resources are considered as gold standard/oracle. Thus, it is\ncrucial that these resources hold correct information. Thereby, they are\ncreated by human experts. However, manual maintenance of such resources is a\ntedious and costly affair. Thus techniques that can aid the experts are\ndesirable. In this paper, we propose an approach to link wordnets. Given a\nsynset of the source language, the approach returns a ranked list of potential\ncandidate synsets in the target language from which the human expert can choose\nthe correct one(s). Our technique is able to retrieve a winner synset in the\ntop 10 ranked list for 60% of all synsets and 70% of noun synsets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1\">Kevin Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanojia_D/0/1/0/all/0/1\">Diptesh Kanojia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1\">Pushpak Bhattacharyya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Multi-modal, Multi-task Learning based Pre-training Framework for Document Representation Learning. (arXiv:2009.14457v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.14457","description":"<p>Recent approaches in literature have exploited the multi-modal information in\ndocuments (text, layout, image) to serve specific downstream document tasks.\nHowever, they are limited by their - (i) inability to learn cross-modal\nrepresentations across text, layout and image dimensions for documents and (ii)\ninability to process multi-page documents. Pre-training techniques have been\nshown in Natural Language Processing (NLP) domain to learn generic textual\nrepresentations from large unlabelled datasets, applicable to various\ndownstream NLP tasks. In this paper, we propose a multi-task learning-based\nframework that utilizes a combination of self-supervised and supervised\npre-training tasks to learn a generic document representation applicable to\nvarious downstream document tasks. Specifically, we introduce Document Topic\nModelling and Document Shuffle Prediction as novel pre-training tasks to learn\nrich image representations along with the text and layout representations for\ndocuments. We utilize the Longformer network architecture as the backbone to\nencode the multi-modal information from multi-page documents in an end-to-end\nfashion. We showcase the applicability of our pre-training framework on a\nvariety of different real-world document tasks such as document classification,\ndocument information extraction, and document retrieval. We evaluate our\nframework on different standard document datasets and conduct exhaustive\nexperiments to compare performance against various ablations of our framework\nand state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Subhojeet Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mujumdar_S/0/1/0/all/0/1\">Shashank Mujumdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1\">Hima Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting of a Patient's Condition From Clinical Narratives Using Natural Language Representation. (arXiv:2104.03969v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03969","description":"<p>The rapid progress in clinical data management systems and artificial\nintelligence approaches enable the era of personalized medicine. Intensive care\nunits (ICUs) are the ideal clinical research environment for such development\nbecause they collect many clinical data and are highly computerized\nenvironments. We designed a retrospective clinical study on a prospective ICU\ndatabase using clinical natural language to help in the early diagnosis of\nheart failure in critically ill children. The methodology consisted of\nempirical experiments of a learning algorithm to learn the hidden\ninterpretation and presentation of the French clinical note data. This study\nincluded 1386 patients' clinical notes with 5444 single lines of notes. There\nwere 1941 positive cases (36 % of total) and 3503 negative cases classified by\ntwo independent physicians using a standardized approach. The multilayer\nperceptron neural network outperforms other discriminative and generative\nclassifiers. Consequently, the proposed framework yields an overall\nclassification performance with 89 % accuracy, 88 % recall, and 89 % precision.\nThis study successfully applied learning representation and machine learning\nalgorithms to detect heart failure from clinical natural language in a single\nFrench institution. Further work is needed to use the same methodology in other\ninstitutions and other languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1\">Jerome Rambaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1\">Guillaume Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Interpretations from Entity-Linked Segmentations. (arXiv:2105.08581v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2105.08581","description":"<p>Web search queries can be ambiguous: is \"source of the nile\" meant to find\ninformation on the actual river or on a board game of that name? We tackle this\nproblem by deriving entity-based query interpretations: given some query, the\ntask is to derive all reasonable ways of linking suitable parts of the query to\nsemantically compatible entities in a background knowledge base. Our suggested\napproach focuses on effectiveness but also on efficiency since web search\nresponse times should not exceed some hundreds of milliseconds. In our\napproach, we use query segmentation as a pre-processing step that finds\npromising segment-based \"interpretation skeletons\". The individual segments\nfrom these skeletons are then linked to entities from a knowledge base and the\nreasonable combinations are ranked in a final step. An experimental comparison\non a combined corpus of all existing query entity linking datasets shows our\napproach to have a better interpretation accuracy at a better run time than the\npreviously most effective methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kasturia_V/0/1/0/all/0/1\">Vaibhav Kasturia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gohsen_M/0/1/0/all/0/1\">Marcel Gohsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagen_M/0/1/0/all/0/1\">Matthias Hagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups. (arXiv:2106.00676v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00676","description":"<p>Accurately extracting structured content from PDFs is a critical first step\nfor NLP over scientific papers. Recent work has improved extraction accuracy by\nincorporating elementary layout information, e.g., each token's 2D position on\nthe page, into language model pretraining. We introduce new methods that\nexplicitly model VIsual LAyout (VILA) groups, i.e., text lines or text blocks,\nto further improve performance. In our I-VILA approach, we show that simply\ninserting special tokens denoting layout group boundaries into model inputs can\nlead to a 1.9% Macro F1 improvement in token classification. In the H-VILA\napproach, we show that hierarchical encoding of layout-groups can result in\nup-to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike\nprior layout-aware approaches, our methods do not require expensive additional\npretraining, only fine-tuning, which we show can reduce training cost by up to\n95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE,\nthat unifies existing automatically-labeled datasets and includes a new dataset\nof manual annotations covering diverse papers from 19 scientific disciplines.\nPre-trained weights, benchmark datasets, and source code are available at\nhttps://github.com/allenai/VILA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Knowledge Graph Embedding Extrapolate to Unseen Data: a Semantic Evidence View. (arXiv:2109.11800v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11800","description":"<p>Knowledge Graph Embedding (KGE) aims to learn representations for entities\nand relations. Most KGE models have gained great success, especially on\nextrapolation scenarios. Specifically, given an unseen triple (h, r, t), a\ntrained model can still correctly predict t from (h, r, ?), or h from (?, r,\nt), such extrapolation ability is impressive. However, most existing KGE works\nfocus on the design of delicate triple modeling function, which mainly tells us\nhow to measure the plausibility of observed triples, but offers limited\nexplanation of why the methods can extrapolate to unseen data, and what are the\nimportant factors to help KGE extrapolate. Therefore in this work, we attempt\nto study the KGE extrapolation of two problems: 1. How does KGE extrapolate to\nunseen data? 2. How to design the KGE model with better extrapolation ability?\nFor the problem 1, we first discuss the impact factors for extrapolation and\nfrom relation, entity and triple level respectively, propose three Semantic\nEvidences (SEs), which can be observed from train set and provide important\nsemantic information for extrapolation. Then we verify the effectiveness of SEs\nthrough extensive experiments on several typical KGE methods. For the problem\n2, to make better use of the three levels of SE, we propose a novel GNN-based\nKGE model, called Semantic Evidence aware Graph Neural Network (SE-GNN). In\nSE-GNN, each level of SE is modeled explicitly by the corresponding neighbor\npattern, and merged sufficiently by the multi-layer aggregation, which\ncontributes to obtaining more extrapolative knowledge representation. Finally,\nthrough extensive experiments on FB15k-237 and WN18RR datasets, we show that\nSE-GNN achieves state-of-the-art performance on Knowledge Graph Completion task\nand performs a better extrapolation ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yanan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qiannan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_G/0/1/0/all/0/1\">Guanqun Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1\">Fang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple but Effective Bidirectional Framework for Relational Triple Extraction. (arXiv:2112.04940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04940","description":"<p>Tagging based relational triple extraction methods are attracting growing\nresearch attention recently. However, most of these methods take a\nunidirectional extraction framework that first extracts all subjects and then\nextracts objects and relations simultaneously based on the subjects extracted.\nThis framework has an obvious deficiency that it is too sensitive to the\nextraction results of subjects. To overcome this deficiency, we propose a\nbidirectional extraction framework based method that extracts triples based on\nthe entity pairs extracted from two complementary directions. Concretely, we\nfirst extract all possible subject-object pairs from two paralleled directions.\nThese two extraction directions are connected by a shared encoder component,\nthus the extraction features from one direction can flow to another direction\nand vice versa. By this way, the extractions of two directions can boost and\ncomplement each other. Next, we assign all possible relations for each entity\npair by a biaffine model. During training, we observe that the share structure\nwill lead to a convergence rate inconsistency issue which is harmful to\nperformance. So we propose a share-aware learning mechanism to address it. We\nevaluate the proposed model on multiple benchmark datasets. Extensive\nexperimental results show that the proposed model is very effective and it\nachieves state-of-the-art results on all of these datasets. Moreover,\nexperiments show that both the proposed bidirectional extraction framework and\nthe share-aware learning mechanism have good adaptability and can be used to\nimprove the performance of other tagging based methods. The source code of our\nwork is available at: https://github.com/neukg/BiRTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shujuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Stance Detection of Tweets Via Distant Network Supervision. (arXiv:2201.00614v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2201.00614","description":"<p>Detecting and labeling stance in social media text is strongly motivated by\nhate speech detection, poll prediction, engagement forecasting, and concerted\npropaganda detection. Today's best neural stance detectors need large volumes\nof training data, which is difficult to curate given the fast-changing\nlandscape of social media text and issues on which users opine. Homophily\nproperties over the social network provide strong signal of coarse-grained\nuser-level stance. But semi-supervised approaches for tweet-level stance\ndetection fail to properly leverage homophily. In light of this, We present\nSANDS, a new semi-supervised stance detector. SANDS starts from very few\nlabeled tweets. It builds multiple deep feature views of tweets. It also uses a\ndistant supervision signal from the social network to provide a surrogate loss\nsignal to the component learners. We prepare two new tweet datasets comprising\nover 236,000 politically tinted tweets from two demographics (US and India)\nposted by over 87,000 users, their follower-followee graph, and over 8,000\ntweets annotated by linguists. SANDS achieves a macro-F1 score of 0.55 (0.49)\non US (India)-based datasets, outperforming 17 baselines (including variants of\nSANDS) substantially, particularly for minority stance labels and noisy text.\nNumerous ablation experiments on SANDS disentangle the dynamics of textual and\nnetwork-propagated stance signals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1\">Subhabrata Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caur_S/0/1/0/all/0/1\">Samiya Caur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1\">Soumen Chakrabarti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Linear Variational State Space Filtering. (arXiv:2201.01353v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01353","description":"<p>We introduce Variational State-Space Filters (VSSF), a new method for\nunsupervised learning, identification, and filtering of latent Markov state\nspace models from raw pixels. We present a theoretically sound framework for\nlatent state space inference under heterogeneous sensor configurations. The\nresulting model can integrate an arbitrary subset of the sensor measurements\nused during training, enabling the learning of semi-supervised state\nrepresentations, thus enforcing that certain components of the learned latent\nstate space to agree with interpretable measurements. From this framework we\nderive L-VSSF, an explicit instantiation of this model with linear latent\ndynamics and Gaussian distribution parameterizations. We experimentally\ndemonstrate L-VSSF's ability to filter in latent space beyond the sequence\nlength of the training dataset across several different test environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfrommer_D/0/1/0/all/0/1\">Daniel Pfrommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1\">Nikolai Matni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseTact: Optical Tactile Sensor for Dense Shape Reconstruction. (arXiv:2201.01367v1 [cs.RO])","link":"http://arxiv.org/abs/2201.01367","description":"<p>Increasing the performance of tactile sensing in robots enables versatile,\nin-hand manipulation. Vision-based tactile sensors have been widely used as\nrich tactile feedback has been shown to be correlated with increased\nperformance in manipulation tasks. Existing tactile sensor solutions with high\nresolution have limitations that include low accuracy, expensive components, or\nlack of scalability. In this paper, an inexpensive, scalable, and compact\ntactile sensor with high-resolution surface deformation modeling for surface\nreconstruction of the 3D sensor surface is proposed. By measuring the image\nfrom the fisheye camera, it is shown that the sensor can successfully estimate\nthe surface deformation in real-time (1.8ms) by using deep convolutional neural\nnetworks. This sensor in its design and sensing abilities represents a\nsignificant step toward better object in-hand localization, classification, and\nsurface estimation all enabled by high-resolution shape reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Do_W/0/1/0/all/0/1\">Won Kyung Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kennedy_M/0/1/0/all/0/1\">Monroe Kennedy III</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Processing Methods for Coronal Hole Segmentation, Matching, and Map Classification. (arXiv:2201.01380v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01380","description":"<p>The paper presents the results from a multi-year effort to develop and\nvalidate image processing methods for selecting the best physical models based\non solar image observations. The approach consists of selecting the physical\nmodels based on their agreement with coronal holes extracted from the images.\nUltimately, the goal is to use physical models to predict geomagnetic storms.\nWe decompose the problem into three subproblems: (i) coronal hole segmentation\nbased on physical constraints, (ii) matching clusters of coronal holes between\ndifferent maps, and (iii) physical map classification. For segmenting coronal\nholes, we develop a multi-modal method that uses segmentation maps from three\ndifferent methods to initialize a level-set method that evolves the initial\ncoronal hole segmentation to the magnetic boundary. Then, we introduce a new\nmethod based on Linear Programming for matching clusters of coronal holes. The\nfinal matching is then performed using Random Forests. The methods were\ncarefully validated using consensus maps derived from multiple readers, manual\nclustering, manual map classification, and method validation for 50 maps. The\nproposed multi-modal segmentation method significantly outperformed SegNet,\nU-net, Henney-Harvey, and FCN by providing accurate boundary detection.\nOverall, the method gave a 95.5% map classification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jatla_V/0/1/0/all/0/1\">V. Jatla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pattichis_M/0/1/0/all/0/1\">M.S. Pattichis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arge_C/0/1/0/all/0/1\">C.N. Arge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Approach to Addressing Zero-Shot Learning Problem. (arXiv:2201.01391v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01391","description":"<p>In recent years, self-supervised learning has had significant success in\napplications involving computer vision and natural language processing. The\ntype of pretext task is important to this boost in performance. One common\npretext task is the measure of similarity and dissimilarity between pairs of\nimages. In this scenario, the two images that make up the negative pair are\nvisibly different to humans. However, in entomology, species are nearly\nindistinguishable and thus hard to differentiate. In this study, we explored\nthe performance of a Siamese neural network using contrastive loss by learning\nto push apart embeddings of bumblebee species pair that are dissimilar, and\npull together similar embeddings. Our experimental results show a 61% F1-score\non zero-shot instances, a performance showing 11% improvement on samples of\nclasses that share intersections with the training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okerinde_A/0/1/0/all/0/1\">Ademola Okerinde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoggatt_S/0/1/0/all/0/1\">Sam Hoggatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkireddy_D/0/1/0/all/0/1\">Divya Vani Lakkireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_N/0/1/0/all/0/1\">Nolan Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">William Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_L/0/1/0/all/0/1\">Lior Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiseman_B/0/1/0/all/0/1\">Brian Spiseman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corrupting Data to Remove Deceptive Perturbation: Using Preprocessing Method to Improve System Robustness. (arXiv:2201.01399v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01399","description":"<p>Although deep neural networks have achieved great performance on\nclassification tasks, recent studies showed that well trained networks can be\nfooled by adding subtle noises. This paper introduces a new approach to improve\nneural network robustness by applying the recovery process on top of the\nnaturally trained classifier. In this approach, images will be intentionally\ncorrupted by some significant operator and then be recovered before passing\nthrough the classifiers. SARGAN -- an extension on Generative Adversarial\nNetworks (GAN) is capable of denoising radar signals. This paper will show that\nSARGAN can also recover corrupted images by removing the adversarial effects.\nOur results show that this approach does improve the performance of naturally\ntrained networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Hieu Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_H/0/1/0/all/0/1\">Hans Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1\">Dung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1\">Peter Chin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing Convolutional Neural Network and Geometric Constraint for Image-based Indoor Localization. (arXiv:2201.01408v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01408","description":"<p>This paper proposes a new image-based localization framework that explicitly\nlocalizes the camera/robot by fusing Convolutional Neural Network (CNN) and\nsequential images' geometric constraints. The camera is localized using a\nsingle or few observed images and training images with 6-degree-of-freedom pose\nlabels. A Siamese network structure is adopted to train an image descriptor\nnetwork, and the visually similar candidate image in the training set is\nretrieved to localize the testing image geometrically. Meanwhile, a\nprobabilistic motion model predicts the pose based on a constant velocity\nassumption. The two estimated poses are finally fused using their uncertainties\nto yield an accurate pose prediction. This method leverages the geometric\nuncertainty and is applicable in indoor scenarios predominated by diffuse\nillumination. Experiments on simulation and real data sets demonstrate the\nefficiency of our proposed method. The results further show that combining the\nCNN-based framework with geometric constraint achieves better accuracy when\ncompared with CNN-only methods, especially when the training data size is\nsmall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingwei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_M/0/1/0/all/0/1\">Mitesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1\">Maani Ghaffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthesizing Tensor Transformations for Visual Self-attention. (arXiv:2201.01410v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01410","description":"<p>Self-attention shows outstanding competence in capturing long-range\nrelationships while enhancing performance on vision tasks, such as image\nclassification and image captioning. However, the self-attention module highly\nrelies on the dot product multiplication and dimension alignment among\nquery-key-value features, which cause two problems: (1) The dot product\nmultiplication results in exhaustive and redundant computation. (2) Due to the\nvisual feature map often appearing as a multi-dimensional tensor, reshaping the\nscale of the tensor feature to adapt to the dimension alignment might destroy\nthe internal structure of the tensor feature map. To address these problems,\nthis paper proposes a self-attention plug-in module with its variants, namely,\nSynthesizing Tensor Transformations (STT), for directly processing image tensor\nfeatures. Without computing the dot-product multiplication among\nquery-key-value, the basic STT is composed of the tensor transformation to\nlearn the synthetic attention weight from visual information. The effectiveness\nof STT series is validated on the image classification and image caption.\nExperiments show that the proposed STT achieves competitive performance while\nkeeping robustness compared to self-attention based above vision tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xihao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Hai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1\">JiaMing Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Problem-dependent attention and effort in neural networks with an application to image resolution. (arXiv:2201.01415v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01415","description":"<p>This paper introduces a new neural network-based estimation approach that is\ninspired by the biological phenomenon whereby humans and animals vary the\nlevels of attention and effort that they dedicate to a problem depending upon\nits difficulty. The proposed approach leverages alternate models' internal\nlevels of confidence in their own projections. If the least costly model is\nconfident in its classification, then that is the classification used; if not,\nthe model with the next lowest cost of implementation is run, and so on. This\nuse of successively more complex models -- together with the models' internal\npropensity scores to evaluate their likelihood of being correct -- makes it\npossible to substantially reduce resource use while maintaining high standards\nfor classification accuracy. The approach is applied to the digit recognition\nproblem from Google's Street View House Numbers dataset, using Multilayer\nPerceptron (MLP) neural networks trained on high- and low-resolution versions\nof the digit images. The algorithm examines the low-resolution images first,\nonly moving to higher resolution images if the classification from the initial\nlow-resolution pass does not have a high degree of confidence. For the MLPs\nconsidered here, this sequential approach enables a reduction in resource usage\nof more than 50\\% without any sacrifice in classification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohlfs_C/0/1/0/all/0/1\">Chris Rohlfs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Vector Expansion using Autoencoder for Anomaly Detection. (arXiv:2201.01416v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01416","description":"<p>Deep learning methods can classify various unstructured data such as images,\nlanguage, and voice as input data. As the task of classifying anomalies becomes\nmore important in the real world, various methods exist for classifying using\ndeep learning with data collected in the real world. As the task of classifying\nanomalies becomes more important in the real world, there are various methods\nfor classifying using deep learning with data collected in the real world.\nAmong the various methods, the representative approach is a method of\nextracting and learning the main features based on a transition model from\npre-trained models, and a method of learning an autoencoderbased structure only\nwith normal data and classifying it as abnormal through a threshold value.\nHowever, if the dataset is imbalanced, even the state-of-the-arts models do not\nachieve good performance. This can be addressed by augmenting normal and\nabnormal features in imbalanced data as features with strong distinction. We\nuse the features of the autoencoder to train latent vectors from low to high\ndimensionality. We train normal and abnormal data as a feature that has a\nstrong distinction among the features of imbalanced data. We propose a latent\nvector expansion autoencoder model that improves classification performance at\nimbalanced data. The proposed method shows performance improvement compared to\nthe basic autoencoder using imbalanced anomaly dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gim_U/0/1/0/all/0/1\">UJu Gim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">YeongHyeon Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Advancing 3D Medical Image Analysis with Variable Dimension Transform based Supervised 3D Pre-training. (arXiv:2201.01426v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01426","description":"<p>The difficulties in both data acquisition and annotation substantially\nrestrict the sample sizes of training datasets for 3D medical imaging\napplications. As a result, constructing high-performance 3D convolutional\nneural networks from scratch remains a difficult task in the absence of a\nsufficient pre-training parameter. Previous efforts on 3D pre-training have\nfrequently relied on self-supervised approaches, which use either predictive or\ncontrastive learning on unlabeled data to build invariant 3D representations.\nHowever, because of the unavailability of large-scale supervision information,\nobtaining semantically invariant and discriminative representations from these\nlearning frameworks remains problematic. In this paper, we revisit an\ninnovative yet simple fully-supervised 3D network pre-training framework to\ntake advantage of semantic supervisions from large-scale 2D natural image\ndatasets. With a redesigned 3D network architecture, reformulated natural\nimages are used to address the problem of data scarcity and develop powerful 3D\nrepresentations. Comprehensive experiments on four benchmark datasets\ndemonstrate that the proposed pre-trained models can effectively accelerate\nconvergence while also improving accuracy for a variety of 3D medical imaging\ntasks such as classification, segmentation and detection. In addition, as\ncompared to training from scratch, it can save up to 60% of annotation efforts.\nOn the NIH DeepLesion dataset, it likewise achieves state-of-the-art detection\nperformance, outperforming earlier self-supervised and fully-supervised\npre-training approaches, as well as methods that do training from scratch. To\nfacilitate further development of 3D medical models, our code and pre-trained\nmodel weights are publicly available at https://github.com/urmagicsmine/CSPR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Shu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zihao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hong-Yu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1\">Jiechao Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Dual Supervised Decoder for RGBD Semantic Segmentation. (arXiv:2201.01427v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01427","description":"<p>Encoder-decoder models have been widely used in RGBD semantic segmentation,\nand most of them are designed via a two-stream network. In general, jointly\nreasoning the color and geometric information from RGBD is beneficial for\nsemantic segmentation. However, most existing approaches fail to\ncomprehensively utilize multimodal information in both the encoder and decoder.\nIn this paper, we propose a novel attention-based dual supervised decoder for\nRGBD semantic segmentation. In the encoder, we design a simple yet effective\nattention-based multimodal fusion module to extract and fuse deeply multi-level\npaired complementary information. To learn more robust deep representations and\nrich multi-modal information, we introduce a dual-branch decoder to effectively\nleverage the correlations and complementary cues of different tasks. Extensive\nexperiments on NYUDv2 and SUN-RGBD datasets demonstrate that our method\nachieves superior performance against the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guodong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural KEM: A Kernel Method with Deep Coefficient Prior for PET Image Reconstruction. (arXiv:2201.01443v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01443","description":"<p>Image reconstruction of low-count positron emission tomography (PET) data is\nchallenging. Kernel methods address the challenge by incorporating image prior\ninformation in the forward model of iterative PET image reconstruction. The\nkernelized expectation-maximization (KEM) algorithm has been developed and\ndemonstrated to be effective and easy to implement. A common approach for a\nfurther improvement of the kernel method would be adding an explicit\nregularization, which however leads to a complex optimization problem. In this\npaper, we propose an implicit regularization for the kernel method by using a\ndeep coefficient prior, which represents the kernel coefficient image in the\nPET forward model using a convolutional neural-network. To solve the\nmaximum-likelihood neural network-based reconstruction problem, we apply the\nprinciple of optimization transfer to derive a neural KEM algorithm. Each\niteration of the algorithm consists of two separate steps: a KEM step for image\nupdate from the projection data and a deep-learning step in the image domain\nfor updating the kernel coefficient image using the neural network. This\noptimization algorithm is guaranteed to monotonically increase the data\nlikelihood. The results from computer simulations and real patient data have\ndemonstrated that the neural KEM can outperform existing KEM and deep image\nprior methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1\">Siqi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gong_K/0/1/0/all/0/1\">Kuang Gong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Badawi_R/0/1/0/all/0/1\">Ramsey D. Badawi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_E/0/1/0/all/0/1\">Edward J. Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qi_J/0/1/0/all/0/1\">Jinyi Qi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guobao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning-Based Sparse Whole-Slide Image Analysis for the Diagnosis of Gastric Intestinal Metaplasia. (arXiv:2201.01449v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01449","description":"<p>In recent years, deep learning has successfully been applied to automate a\nwide variety of tasks in diagnostic histopathology. However, fast and reliable\nlocalization of small-scale regions-of-interest (ROI) has remained a key\nchallenge, as discriminative morphologic features often occupy only a small\nfraction of a gigapixel-scale whole-slide image (WSI). In this paper, we\npropose a sparse WSI analysis method for the rapid identification of high-power\nROI for WSI-level classification. We develop an evaluation framework inspired\nby the early classification literature, in order to quantify the tradeoff\nbetween diagnostic performance and inference time for sparse analytic\napproaches. We test our method on a common but time-consuming task in pathology\n- that of diagnosing gastric intestinal metaplasia (GIM) on hematoxylin and\neosin (H&amp;E)-stained slides from endoscopic biopsy specimens. GIM is a\nwell-known precursor lesion along the pathway to development of gastric cancer.\nWe performed a thorough evaluation of the performance and inference time of our\napproach on a test set of GIM-positive and GIM-negative WSI, finding that our\nmethod successfully detects GIM in all positive WSI, with a WSI-level\nclassification area under the receiver operating characteristic curve (AUC) of\n0.98 and an average precision (AP) of 0.95. Furthermore, we show that our\nmethod can attain these metrics in under one minute on a standard CPU. Our\nresults are applicable toward the goal of developing neural networks that can\neasily be deployed in clinical settings to support pathologists in quickly\nlocalizing and diagnosing small-scale morphologic features in WSI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braatz_J/0/1/0/all/0/1\">Jon Braatz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Stephanie Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1\">Jeanne Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust photon-efficient imaging using a pixel-wise residual shrinkage network. (arXiv:2201.01453v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01453","description":"<p>Single-photon light detection and ranging (LiDAR) has been widely applied to\n3D imaging in challenging scenarios. However, limited signal photon counts and\nhigh noises in the collected data have posed great challenges for predicting\nthe depth image precisely. In this paper, we propose a pixel-wise residual\nshrinkage network for photon-efficient imaging from high-noise data, which\nadaptively generates the optimal thresholds for each pixel and denoises the\nintermediate features by soft thresholding. Besides, redefining the\noptimization target as pixel-wise classification provides a sharp advantage in\nproducing confident and accurate depth estimation when compared with existing\nresearch. Comprehensive experiments conducted on both simulated and real-world\ndatasets demonstrate that the proposed model outperforms the state-of-the-arts\nand maintains robust imaging performance under different signal-to-noise ratios\nincluding the extreme case of 1:100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yao_G/0/1/0/all/0/1\">Gongxin Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yiwei Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xiaomin Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-SRN: Structure-Preserving Super-Resolution Network with Cross Convolution. (arXiv:2201.01458v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01458","description":"<p>It is challenging to restore low-resolution (LR) images to super-resolution\n(SR) images with correct and clear details. Existing deep learning works almost\nneglect the inherent structural information of images, which acts as an\nimportant role for visual perception of SR results. In this paper, we design a\nhierarchical feature exploitation network to probe and preserve structural\ninformation in a multi-scale feature fusion manner. First, we propose a cross\nconvolution upon traditional edge detectors to localize and represent edge\nfeatures. Then, cross convolution blocks (CCBs) are designed with feature\nnormalization and channel attention to consider the inherent correlations of\nfeatures. Finally, we leverage multi-scale feature fusion group (MFFG) to embed\nthe cross convolution blocks and develop the relations of structural features\nin different scales hierarchically, invoking a lightweight structure-preserving\nnetwork named as Cross-SRN. Experimental results demonstrate the Cross-SRN\nachieves competitive or superior restoration performances against the\nstate-of-the-art methods with accurate and clear structural details. Moreover,\nwe set a criterion to select images with rich structural textures. The proposed\nCross-SRN outperforms the state-of-the-art methods on the selected benchmark,\nwhich demonstrates that our network has a significant advantage in preserving\nedges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_Q/0/1/0/all/0/1\">Qi Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1\">Xin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges of Artificial Intelligence -- From Machine Learning and Computer Vision to Emotional Intelligence. (arXiv:2201.01466v1 [cs.AI])","link":"http://arxiv.org/abs/2201.01466","description":"<p>Artificial intelligence (AI) has become a part of everyday conversation and\nour lives. It is considered as the new electricity that is revolutionizing the\nworld. AI is heavily invested in both industry and academy. However, there is\nalso a lot of hype in the current AI debate. AI based on so-called deep\nlearning has achieved impressive results in many problems, but its limits are\nalready visible. AI has been under research since the 1940s, and the industry\nhas seen many ups and downs due to over-expectations and related\ndisappointments that have followed.\n</p>\n<p>The purpose of this book is to give a realistic picture of AI, its history,\nits potential and limitations. We believe that AI is a helper, not a ruler of\nhumans. We begin by describing what AI is and how it has evolved over the\ndecades. After fundamentals, we explain the importance of massive data for the\ncurrent mainstream of artificial intelligence. The most common representations\nfor AI, methods, and machine learning are covered. In addition, the main\napplication areas are introduced. Computer vision has been central to the\ndevelopment of AI. The book provides a general introduction to computer vision,\nand includes an exposure to the results and applications of our own research.\nEmotions are central to human intelligence, but little use has been made in AI.\nWe present the basics of emotional intelligence and our own research on the\ntopic. We discuss super-intelligence that transcends human understanding,\nexplaining why such achievement seems impossible on the basis of present\nknowledge,and how AI could be improved. Finally, a summary is made of the\ncurrent state of AI and what to do in the future. In the appendix, we look at\nthe development of AI education, especially from the perspective of contents at\nour own university.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pietikainen_M/0/1/0/all/0/1\">Matti Pietik&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silven_O/0/1/0/all/0/1\">Olli Silven</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign Language Recognition System using TensorFlow Object Detection API. (arXiv:2201.01486v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01486","description":"<p>Communication is defined as the act of sharing or exchanging information,\nideas or feelings. To establish communication between two people, both of them\nare required to have knowledge and understanding of a common language. But in\nthe case of deaf and dumb people, the means of communication are different.\nDeaf is the inability to hear and dumb is the inability to speak. They\ncommunicate using sign language among themselves and with normal people but\nnormal people do not take seriously the importance of sign language. Not\neveryone possesses the knowledge and understanding of sign language which makes\ncommunication difficult between a normal person and a deaf and dumb person. To\novercome this barrier, one can build a model based on machine learning. A model\ncan be trained to recognize different gestures of sign language and translate\nthem into English. This will help a lot of people in communicating and\nconversing with deaf and dumb people. The existing Indian Sing Language\nRecognition systems are designed using machine learning algorithms with single\nand double-handed gestures but they are not real-time. In this paper, we\npropose a method to create an Indian Sign Language dataset using a webcam and\nthen using transfer learning, train a TensorFlow model to create a real-time\nSign Language Recognition system. The system achieves a good level of accuracy\neven with a limited size dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Sharvani Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gangwar_A/0/1/0/all/0/1\">Amisha Gangwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_R/0/1/0/all/0/1\">Richa Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sudhakar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exemplar-free Class Incremental Learning via Discriminative and Comparable One-class Classifiers. (arXiv:2201.01488v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01488","description":"<p>The exemplar-free class incremental learning requires classification models\nto learn new class knowledge incrementally without retaining any old samples.\nRecently, the framework based on parallel one-class classifiers (POC), which\ntrains a one-class classifier (OCC) independently for each category, has\nattracted extensive attention, since it can naturally avoid catastrophic\nforgetting. POC, however, suffers from weak discriminability and comparability\ndue to its independent training strategy for different OOCs. To meet this\nchallenge, we propose a new framework, named Discriminative and Comparable\nOne-class classifiers for Incremental Learning (DisCOIL). DisCOIL follows the\nbasic principle of POC, but it adopts variational auto-encoders (VAE) instead\nof other well-established one-class classifiers (e.g. deep SVDD), because a\ntrained VAE can not only identify the probability of an input sample belonging\nto a class but also generate pseudo samples of the class to assist in learning\nnew tasks. With this advantage, DisCOIL trains a new-class VAE in contrast with\nthe old-class VAEs, which forces the new-class VAE to reconstruct better for\nnew-class samples but worse for the old-class pseudo samples, thus enhancing\nthe comparability. Furthermore, DisCOIL introduces a hinge reconstruction loss\nto ensure the discriminability. We evaluate our method extensively on MNIST,\nCIFAR10, and Tiny-ImageNet. The experimental results show that DisCOIL achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenju Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1\">Yangli-ao Geng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased Learning from Naturally Imbalanced Pseudo-Labels for Zero-Shot and Semi-Supervised Learning. (arXiv:2201.01490v1 [cs.LG])","link":"http://arxiv.org/abs/2201.01490","description":"<p>This work studies the bias issue of pseudo-labeling, a natural phenomenon\nthat widely occurs but often overlooked by prior research. Pseudo-labels are\ngenerated when a classifier trained on source data is transferred to unlabeled\ntarget data. We observe heavy long-tailed pseudo-labels when a semi-supervised\nlearning model FixMatch predicts labels on the unlabeled set even though the\nunlabeled data is curated to be balanced. Without intervention, the training\nmodel inherits the bias from the pseudo-labels and end up being sub-optimal. To\neliminate the model bias, we propose a simple yet effective method DebiasMatch,\ncomprising of an adaptive debiasing module and an adaptive marginal loss. The\nstrength of debiasing and the size of margins can be automatically adjusted by\nmaking use of an online updated queue. Benchmarked on ImageNet-1K, DebiasMatch\nsignificantly outperforms previous state-of-the-arts by more than 26% and 8.7%\non semi-supervised learning (0.2% annotated data) and zero-shot learning tasks\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAVER: Blind Quality Prediction of Variable Frame Rate Videos. (arXiv:2201.01492v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01492","description":"<p>Video quality assessment (VQA) remains an important and challenging problem\nthat affects many applications at the widest scales. Recent advances in mobile\ndevices and cloud computing techniques have made it possible to capture,\nprocess, and share high resolution, high frame rate (HFR) videos across the\nInternet nearly instantaneously. Being able to monitor and control the quality\nof these streamed videos can enable the delivery of more enjoyable content and\nperceptually optimized rate control. Accordingly, there is a pressing need to\ndevelop VQA models that can be deployed at enormous scales. While some recent\neffects have been applied to full-reference (FR) analysis of variable frame\nrate and HFR video quality, the development of no-reference (NR) VQA algorithms\ntargeting frame rate variations has been little studied. Here, we propose a\nfirst-of-a-kind blind VQA model for evaluating HFR videos, which we dub the\nFramerate-Aware Video Evaluator w/o Reference (FAVER). FAVER uses extended\nmodels of spatial natural scene statistics that encompass space-time\nwavelet-decomposed video signals, to conduct efficient frame rate sensitive\nquality prediction. Our extensive experiments on several HFR video quality\ndatasets show that FAVER outperforms other blind VQA algorithms at a reasonable\ncomputational cost. To facilitate reproducible research and public evaluation,\nan implementation of FAVER is being made freely available online:\n\\url{https://github.com/uniqzheng/HFR-BVQA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Madhusudana_P/0/1/0/all/0/1\">Pavan C. Madhusudana</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1\">Xiaoyang Zeng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yibo Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Object Detection, Multi-object Tracking, and Re-Identification for Disaster Response Drones. (arXiv:2201.01494v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01494","description":"<p>We aim to detect and identify multiple objects using multiple cameras and\ncomputer vision for disaster response drones. The major challenges are taming\ndetection errors, resolving ID switching and fragmentation, adapting to\nmulti-scale features and multiple views with global camera motion. Two simple\napproaches are proposed to solve these issues. One is a fast multi-camera\nsystem that added a tracklet association, and the other is incorporating a\nhigh-performance detector and tracker to resolve restrictions. (...) The\naccuracy of our first approach (85.71%) is slightly improved compared to our\nbaseline, FairMOT (85.44%) in the validation dataset. In the final results\ncalculated based on L2-norm error, the baseline was 48.1, while the proposed\nmodel combination was 34.9, which is a great reduction of error by a margin of\n27.4%. In the second approach, although DeepSORT only processes a quarter of\nall frames due to hardware and time limitations, our model with DeepSORT\n(42.9%) outperforms FairMOT (71.4%) in terms of recall. Both of our models\nranked second and third place in the `AI Grand Challenge' organized by the\nKorean Ministry of Science and ICT in 2020 and 2021, respectively. The source\ncodes are publicly available at these URLs\n(github.com/mlvlab/drone_ai_challenge, github.com/mlvlab/Drone_Task1,\ngithub.com/mlvlab/Rony2_task3, github.com/mlvlab/Drone_task4).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paik_C/0/1/0/all/0/1\">Chongkeun Paik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunwoo J. Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation and Focal Loss. (arXiv:2201.01501v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01501","description":"<p>Depth estimation is solved as a regression or classification problem in\nexisting learning-based multi-view stereo methods. Although these two\nrepresentations have recently demonstrated their excellent performance, they\nstill have apparent shortcomings, e.g., regression methods tend to overfit due\nto the indirect learning cost volume, and classification methods cannot\ndirectly infer the exact depth due to its discrete prediction. In this paper,\nwe propose a novel representation, termed Unification, to unify the advantages\nof regression and classification. It can directly constrain the cost volume\nlike classification methods, but also realize the sub-pixel depth prediction\nlike regression methods. To excavate the potential of unification, we design a\nnew loss function named Unified Focal Loss, which is more uniform and\nreasonable to combat the challenge of sample imbalance. Combining these two\nunburdened modules, we present a coarse-to-fine framework, that we call\nUniMVSNet. The results of ranking first on both DTU and Tanks and Temples\nbenchmarks verify that our model not only performs the best but also has the\nbest generalization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1\">Rui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rongjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhenyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yawen Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ronggang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Uniform Point Distribution in Feature-preserving Point Cloud Filtering. (arXiv:2201.01503v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01503","description":"<p>As a popular representation of 3D data, point cloud may contain noise and\nneed to be filtered before use. Existing point cloud filtering methods either\ncannot preserve sharp features or result in uneven point distribution in the\nfiltered output. To address this problem, this paper introduces a point cloud\nfiltering method that considers both point distribution and feature\npreservation during filtering. The key idea is to incorporate a repulsion term\nwith a data term in energy minimization. The repulsion term is responsible for\nthe point distribution, while the data term is to approximate the noisy\nsurfaces while preserving the geometric features. This method is capable of\nhandling models with fine-scale features and sharp features. Extensive\nexperiments show that our method yields better results with a more uniform\npoint distribution ($5.8\\times10^{-5}$ Chamfer Distance on average) in seconds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuaijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinxi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meili Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xuequan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Culture-to-Culture Image Translation with Generative Adversarial Networks. (arXiv:2201.01565v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01565","description":"<p>This article introduces the concept of image \"culturization\", i.e., defined\nas the process of altering the \"brushstroke of cultural features\" that make\nobjects perceived as belonging to a given culture while preserving their\nfunctionalities. First, we propose a pipeline for translating objects' images\nfrom a source to a target cultural domain based on Generative Adversarial\nNetworks (GAN). Then, we gather data through an online questionnaire to test\nfour hypotheses concerning the preferences of Italian participants towards\nobjects and environments belonging to different cultures. As expected, results\ndepend on individual tastes and preference: however, they are in line with our\nconjecture that some people, during the interaction with a robot or another\nintelligent system, might prefer to be shown images whose cultural domain has\nbeen modified to match their cultural background.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaino_G/0/1/0/all/0/1\">Giulia Zaino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recchiuto_C/0/1/0/all/0/1\">Carmine Tommaso Recchiuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sgorbissa_A/0/1/0/all/0/1\">Antonio Sgorbissa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning True Rate-Distortion-Optimization for End-To-End Image Compression. (arXiv:2201.01586v1 [eess.IV])","link":"http://arxiv.org/abs/2201.01586","description":"<p>Even though rate-distortion optimization is a crucial part of traditional\nimage and video compression, not many approaches exist which transfer this\nconcept to end-to-end-trained image compression. Most frameworks contain static\ncompression and decompression models which are fixed after training, so\nefficient rate-distortion optimization is not possible. In a previous work, we\nproposed RDONet, which enables an RDO approach comparable to adaptive block\npartitioning in HEVC. In this paper, we enhance the training by introducing\nlow-complexity estimations of the RDO result into the training. Additionally,\nwe propose fast and very fast RDO inference modes. With our novel training\nmethod, we achieve average rate savings of 19.6% in MS-SSIM over the previous\nRDONet model, which equals rate savings of 27.3% over a comparable conventional\ndeep image coder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Brand_F/0/1/0/all/0/1\">Fabian Brand</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischer_K/0/1/0/all/0/1\">Kristian Fischer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kopte_A/0/1/0/all/0/1\">Alexander Kopte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaup_A/0/1/0/all/0/1\">Andr&#xe9; Kaup</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biphasic Face Photo-Sketch Synthesis via Semantic-Driven Generative Adversarial Network with Graph Representation Learning. (arXiv:2201.01592v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01592","description":"<p>In recent years, significant progress has been achieved in biphasic face\nphoto-sketch synthesis with the development of Generative Adversarial Network\n(GAN). Biphasic face photo-sketch synthesis could be applied in wide-ranging\nfields such as digital entertainment and law enforcement. However, generating\nrealistic photos and distinct sketches suffers from great challenges due to the\nlow quality of sketches and complex photo variations in the real scenes. To\nthis end, we propose a novel Semantic-Driven Generative Adversarial Network to\naddress the above issues, cooperating with the Graph Representation Learning.\nSpecifically, we inject class-wise semantic layouts into the generator to\nprovide style-based spatial supervision for synthesized face photos and\nsketches. In addition, to improve the fidelity of the generated results, we\nleverage the semantic layouts to construct two types of Representational Graphs\nwhich indicate the intra-class semantic features and inter-class structural\nfeatures of the synthesized images. Furthermore, we design two types of\nconstraints based on the proposed Representational Graphs which facilitate the\npreservation of the details in generated face photos and sketches. Moreover, to\nfurther enhance the perceptual quality of synthesized images, we propose a\nnovel biphasic training strategy which is dedicated to refine the generated\nresults through Iterative Cycle Training. Extensive experiments are conducted\non CUFS and CUFSF datasets to demonstrate the prominent ability of our proposed\nmethod which achieves the state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Probabilistic Graph Matching. (arXiv:2201.01603v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01603","description":"<p>Most previous learning-based graph matching algorithms solve the\n\\textit{quadratic assignment problem} (QAP) by dropping one or more of the\nmatching constraints and adopting a relaxed assignment solver to obtain\nsub-optimal correspondences. Such relaxation may actually weaken the original\ngraph matching problem, and in turn hurt the matching performance. In this\npaper we propose a deep learning-based graph matching framework that works for\nthe original QAP without compromising on the matching constraints. In\nparticular, we design an affinity-assignment prediction network to jointly\nlearn the pairwise affinity and estimate the node assignments, and we then\ndevelop a differentiable solver inspired by the probabilistic perspective of\nthe pairwise affinities. Aiming to obtain better matching results, the\nprobabilistic solver refines the estimated assignments in an iterative manner\nto impose both discrete and one-to-one matching constraints. The proposed\nmethod is evaluated on three popularly tested benchmarks (Pascal VOC, Willow\nObject and SPair-71k), and it outperforms all previous state-of-the-arts on all\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">He Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Congyan Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Songhe Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need In Sign Language Production. (arXiv:2201.01609v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01609","description":"<p>Sign Language is the dominant form of communication language used in the deaf\nand hearing-impaired community. To make an easy and mutual communication\nbetween the hearing-impaired and the hearing communities, building a robust\nsystem capable of translating the spoken language into sign language and vice\nversa is fundamental. To this end, sign language recognition and production are\ntwo necessary parts for making such a two-way system. Sign language recognition\nand production need to cope with some critical challenges. In this survey, we\nreview recent advances in Sign Language Production (SLP) and related areas\nusing deep learning. To have more realistic perspectives to sign language, we\npresent an introduction to the Deaf culture, Deaf centers, psychological\nperspective of sign language, the main differences between spoken language and\nsign language. Furthermore, we present the fundamental components of a\nbi-directional sign language translation system, discussing the main challenges\nin this area. Also, the backbone architectures and methods in SLP are briefly\nintroduced and the proposed taxonomy on SLP is presented. Finally, a general\nframework for SLP and performance evaluation, and also a discussion on the\nrecent developments, advantages, and limitations in SLP, commenting on possible\nlines for future research are presented.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athitsos_V/0/1/0/all/0/1\">Vassilis Athitsos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention. (arXiv:2201.01615v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01615","description":"<p>Multi-scale representations are crucial for semantic segmentation. The\ncommunity has witnessed the flourish of semantic segmentation convolutional\nneural networks (CNN) exploiting multi-scale contextual information. Motivated\nby that the vision transformer (ViT) is powerful in image classification, some\nsemantic segmentation ViTs are recently proposed, most of them attaining\nimpressive results but at a cost of computational economy. In this paper, we\nsucceed in introducing multi-scale representations into semantic segmentation\nViT via window attention mechanism and further improves the performance and\nefficiency. To this end, we introduce large window attention which allows the\nlocal window to query a larger area of context window at only a little\ncomputation overhead. By regulating the ratio of the context area to the query\narea, we enable the large window attention to capture the contextual\ninformation at multiple scales. Moreover, the framework of spatial pyramid\npooling is adopted to collaborate with the large window attention, which\npresents a novel decoder named large window attention spatial pyramid pooling\n(LawinASPP) for semantic segmentation ViT. Our resulting ViT, Lawin\nTransformer, is composed of an efficient hierachical vision transformer (HVT)\nas encoder and a LawinASPP as decoder. The empirical results demonstrate that\nLawin Transformer offers an improved efficiency compared to the existing\nmethod. Lawin Transformer further sets new state-of-the-art performance on\nCityscapes (84.4\\% mIoU), ADE20K (56.2\\% mIoU) and COCO-Stuff datasets. The\ncode will be released at https://github.com/yan-hao-tian/lawin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Haotian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Ming Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tackling the Class Imbalance Problem of Deep Learning Based Head and Neck Organ Segmentation. (arXiv:2201.01636v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01636","description":"<p>The segmentation of organs at risk (OAR) is a required precondition for the\ncancer treatment with image guided radiation therapy. The automation of the\nsegmentation task is therefore of high clinical relevance. Deep Learning (DL)\nbased medical image segmentation is currently the most successful approach, but\nsuffers from the over-presence of the background class and the anatomically\ngiven organ size difference, which is most severe in the head and neck (HAN)\narea. To tackle the HAN area specific class imbalance problem we first optimize\nthe patch-size of the currently best performing general purpose segmentation\nframework, the nnU-Net, based on the introduced class imbalance measurement,\nand second, introduce the class adaptive Dice loss to further compensate for\nthe highly imbalanced setting. Both the patch-size and the loss function are\nparameters with direct influence on the class imbalance and their optimization\nleads to a 3\\% increase of the Dice score and 22% reduction of the 95%\nHausdorff distance compared to the baseline, finally reaching $0.8\\pm0.15$ and\n$3.17\\pm1.7$ mm for the segmentation of seven HAN organs using a single and\nsimple neural network. The patch-size optimization and the class adaptive Dice\nloss are both simply integrable in current DL based segmentation approaches and\nallow to increase the performance for class imbalanced segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tappeiner_E/0/1/0/all/0/1\">Elias Tappeiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welk_M/0/1/0/all/0/1\">Martin Welk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_R/0/1/0/all/0/1\">Rainer Schubert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TableParser: Automatic Table Parsing with Weak Supervision from Spreadsheets. (arXiv:2201.01654v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01654","description":"<p>Tables have been an ever-existing structure to store data. There exist now\ndifferent approaches to store tabular data physically. PDFs, images,\nspreadsheets, and CSVs are leading examples. Being able to parse table\nstructures and extract content bounded by these structures is of high\nimportance in many applications. In this paper, we devise TableParser, a system\ncapable of parsing tables in both native PDFs and scanned images with high\nprecision. We have conducted extensive experiments to show the efficacy of\ndomain adaptation in developing such a tool. Moreover, we create TableAnnotator\nand ExcelAnnotator, which constitute a spreadsheet-based weak supervision\nmechanism and a pipeline to enable table parsing. We share these resources with\nthe research community to facilitate further research in this interesting\ndirection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1\">Susie Xi Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rausch_J/0/1/0/all/0/1\">Johannes Rausch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_P/0/1/0/all/0/1\">Peter Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluation of Thermal Imaging on Embedded GPU Platforms for Application in Vehicular Assistance Systems. (arXiv:2201.01661v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01661","description":"<p>This study is focused on evaluating the real-time performance of thermal\nobject detection for smart and safe vehicular systems by deploying the trained\nnetworks on GPU &amp; single-board EDGE-GPU computing platforms for onboard\nautomotive sensor suite testing. A novel large-scale thermal dataset comprising\nof &gt; 35,000 distinct frames is acquired, processed, and open-sourced in\nchallenging weather and environmental scenarios. The dataset is a recorded from\nlost-cost yet effective uncooled LWIR thermal camera, mounted stand-alone and\non an electric vehicle to minimize mechanical vibrations. State-of-the-art\nYOLO-V5 networks variants are trained using four different public datasets as\nwell newly acquired local dataset for optimal generalization of DNN by\nemploying SGD optimizer. The effectiveness of trained networks is validated on\nextensive test data using various quantitative metrics which include precision,\nrecall curve, mean average precision, and frames per second. The smaller\nnetwork variant of YOLO is further optimized using TensorRT inference\naccelerator to explicitly boost the frames per second rate. Optimized network\nengine increases the frames per second rate by 3.5 times when testing on low\npower edge devices thus achieving 11 fps on Nvidia Jetson Nano and 60 fps on\nNvidia Xavier NX development boards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1\">Muhammad Ali Farooq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shariff_W/0/1/0/all/0/1\">Waseem Shariff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1\">Peter Corcoran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface-Aligned Neural Radiance Fields for Controllable 3D Human Synthesis. (arXiv:2201.01683v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01683","description":"<p>We propose a new method for reconstructing controllable implicit 3D human\nmodels from sparse multi-view RGB videos. Our method defines the neural scene\nrepresentation on the mesh surface points and signed distances from the surface\nof a human body mesh. We identify an indistinguishability issue that arises\nwhen a point in 3D space is mapped to its nearest surface point on a mesh for\nlearning surface-aligned neural scene representation. To address this issue, we\npropose projecting a point onto a mesh surface using a barycentric\ninterpolation with modified vertex normals. Experiments with the ZJU-MoCap and\nHuman3.6M datasets show that our approach achieves a higher quality in a\nnovel-view and novel-pose synthesis than existing methods. We also demonstrate\nthat our method easily supports the control of body shape and clothes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianhan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_Y/0/1/0/all/0/1\">Yasuhiro Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsumoto_E/0/1/0/all/0/1\">Eiichi Matsumoto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation Of Ben-ford's Law Divergence And Machine Learning Techniques For Separability Of Fingerprint Images. (arXiv:2201.01699v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01699","description":"<p>Protecting a fingerprint database against attackers is very vital in order to\nprotect against false acceptance rate or false rejection rate. A key property\nin distinguishing fingerprint images is by exploiting the characteristics of\nthese different types of fingerprint images. The aim of this paper is to\nperform the classification of fingerprint images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iorliam_A/0/1/0/all/0/1\">Aamo Iorliam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emmanuel_O/0/1/0/all/0/1\">Orgem Emmanuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shehu_Y/0/1/0/all/0/1\">Yahaya I. Shehu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing TryOnGAN. (arXiv:2201.01703v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01703","description":"<p>TryOnGAN is a recent virtual try-on approach, which generates highly\nrealistic images and outperforms most previous approaches. In this article, we\nreproduce the TryOnGAN implementation and probe it along diverse angles: impact\nof transfer learning, variants of conditioning image generation with poses and\nproperties of latent space interpolation. Some of these facets have never been\nexplored in literature earlier. We find that transfer helps training initially\nbut gains are lost as models train longer and pose conditioning via\nconcatenation performs better. The latent space self-disentangles the pose and\nthe style features and enables style transfer across poses. Our code and models\nare available in open source.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Saurabh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1\">Nishant Sinha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Model Compression on Fairness in Facial Expression Recognition. (arXiv:2201.01709v1 [cs.CV])","link":"http://arxiv.org/abs/2201.01709","description":"<p>Deep neural networks have proved hugely successful, achieving human-like\nperformance on a variety of tasks. However, they are also computationally\nexpensive, which has motivated the development of model compression techniques\nwhich reduce the resource consumption associated with deep learning models.\nNevertheless, recent studies have suggested that model compression can have an\nadverse effect on algorithmic fairness, amplifying existing biases in machine\nlearning models. With this project we aim to extend those studies to the\ncontext of facial expression recognition. To do that, we set up a neural\nnetwork classifier to perform facial expression recognition and implement\nseveral model compression techniques on top of it. We then run experiments on\ntwo facial expression datasets, namely the Extended Cohn-Kanade Dataset (CK+DB)\nand the Real-World Affective Faces Database (RAF-DB), to examine the individual\nand combined effect that compression techniques have on the model size,\naccuracy and fairness. Our experimental results show that: (i) Compression and\nquantisation achieve significant reduction in model size with minimal impact on\noverall accuracy for both CK+DB and RAF-DB; (ii) in terms of model accuracy,\nthe classifier trained and tested on RAF-DB seems more robust to compression\ncompared to the CK+ DB; (iii) for RAF-DB, the different compression strategies\ndo not seem to increase the gap in predictive performance across the sensitive\nattributes of gender, race and age which is in contrast with the results on the\nCK+DB, where compression seems to amplify existing biases for gender. We\nanalyse the results and discuss the potential reasons for our findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stoychev_S/0/1/0/all/0/1\">Samuil Stoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunes_H/0/1/0/all/0/1\">Hatice Gunes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Robot Collaborative Perception with Graph Neural Networks. (arXiv:2201.01760v1 [cs.RO])","link":"http://arxiv.org/abs/2201.01760","description":"<p>Multi-robot systems such as swarms of aerial robots are naturally suited to\noffer additional flexibility, resilience, and robustness in several tasks\ncompared to a single robot by enabling cooperation among the agents. To enhance\nthe autonomous robot decision-making process and situational awareness,\nmulti-robot systems have to coordinate their perception capabilities to\ncollect, share, and fuse environment information among the agents in an\nefficient and meaningful way such to accurately obtain context-appropriate\ninformation or gain resilience to sensor noise or failures. In this paper, we\npropose a general-purpose Graph Neural Network (GNN) with the main goal to\nincrease, in multi-robot perception tasks, single robots' inference perception\naccuracy as well as resilience to sensor failures and disturbances. We show\nthat the proposed framework can address multi-view visual perception problems\nsuch as monocular depth estimation and semantic segmentation. Several\nexperiments both using photo-realistic and real data gathered from multiple\naerial robots' viewpoints show the effectiveness of the proposed approach in\nchallenging inference conditions including images corrupted by heavy noise and\ncamera occlusions or failures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jiuhong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loianno_G/0/1/0/all/0/1\">Giuseppe Loianno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Self-Supervised Audio-Visual Speech Recognition. (arXiv:2201.01763v1 [cs.SD])","link":"http://arxiv.org/abs/2201.01763","description":"<p>Audio-based automatic speech recognition (ASR) degrades significantly in\nnoisy environments and is particularly vulnerable to interfering speech, as the\nmodel cannot determine which speaker to transcribe. Audio-visual speech\nrecognition (AVSR) systems improve robustness by complementing the audio stream\nwith the visual information that is invariant to noise and helps the model\nfocus on the desired speaker. However, previous AVSR work focused solely on the\nsupervised learning setup; hence the progress was hindered by the amount of\nlabeled data available. In this work, we present a self-supervised AVSR\nframework built upon Audio-Visual HuBERT (AV-HuBERT), a state-of-the-art\naudio-visual speech representation learning model. On the largest available\nAVSR benchmark dataset LRS3, our approach outperforms prior state-of-the-art by\n~50% (28.0% vs. 14.1%) using less than 10% of labeled data (433hr vs. 30hr) in\nthe presence of babble noise, while reducing the WER of an audio-based model by\nover 75% (25.8% vs. 5.8%) on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Bowen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Semantic Multimodal Hashing Network for Scalable Image-Text and Video-Text Retrievals. (arXiv:1901.02662v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1901.02662","description":"<p>Hashing has been widely applied to multimodal retrieval on large-scale\nmultimedia data due to its efficiency in computation and storage. In this\narticle, we propose a novel deep semantic multimodal hashing network (DSMHN)\nfor scalable image-text and video-text retrieval. The proposed deep hashing\nframework leverages 2-D convolutional neural networks (CNN) as the backbone\nnetwork to capture the spatial information for image-text retrieval, while the\n3-D CNN as the backbone network to capture the spatial and temporal information\nfor video-text retrieval. In the DSMHN, two sets of modality-specific hash\nfunctions are jointly learned by explicitly preserving both intermodality\nsimilarities and intramodality semantic labels. Specifically, with the\nassumption that the learned hash codes should be optimal for the classification\ntask, two stream networks are jointly trained to learn the hash functions by\nembedding the semantic labels on the resultant hash codes. Moreover, a unified\ndeep multimodal hashing framework is proposed to learn compact and high-quality\nhash codes by exploiting the feature representation learning, intermodality\nsimilarity-preserving learning, semantic label-preserving learning, and hash\nfunction learning with different types of loss functions simultaneously. The\nproposed DSMHN method is a generic and scalable deep hashing framework for both\nimage-text and video-text retrievals, which can be flexibly integrated with\ndifferent types of loss functions. We conduct extensive experiments for both\nsingle modal- and cross-modal-retrieval tasks on four widely used\nmultimodal-retrieval data sets. Experimental results on both image-text- and\nvideo-text-retrieval tasks demonstrate that the DSMHN significantly outperforms\nthe state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zechao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jinhui Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Network for Image Super-Resolution. (arXiv:2005.09964v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2005.09964","description":"<p>Single image super-resolution (SISR), as a traditional ill-conditioned\ninverse problem, has been greatly revitalized by the recent development of\nconvolutional neural networks (CNN). These CNN-based methods generally map a\nlow-resolution image to its corresponding high-resolution version with\nsophisticated network structures and loss functions, showing impressive\nperformances. This paper provides a new insight on conventional SISR algorithm,\nand proposes a substantially different approach relying on the iterative\noptimization. A novel iterative super-resolution network (ISRN) is proposed on\ntop of the iterative optimization. We first analyze the observation model of\nimage SR problem, inspiring a feasible solution by mimicking and fusing each\niteration in a more general and efficient manner. Considering the drawbacks of\nbatch normalization, we propose a feature normalization (F-Norm, FN) method to\nregulate the features in network. Furthermore, a novel block with FN is\ndeveloped to improve the network representation, termed as FNB.\nResidual-in-residual structure is proposed to form a very deep network, which\ngroups FNBs with a long skip connection for better information delivery and\nstabling the training phase. Extensive experimental results on testing\nbenchmarks with bicubic (BI) degradation show our ISRN can not only recover\nmore structural information, but also achieve competitive or better PSNR/SSIM\nresults with much fewer parameters compared to other works. Besides BI, we\nsimulate the real-world degradation with blur-downscale (BD) and\ndownscale-noise (DN). ISRN and its extension ISRN+ both achieve better\nperformance than others with BD and DN degradation models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yuqing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshe Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Accurate Entropy Model with Global Reference for Image Compression. (arXiv:2010.08321v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2010.08321","description":"<p>In recent deep image compression neural networks, the entropy model plays a\ncritical role in estimating the prior distribution of deep image encodings.\nExisting methods combine hyperprior with local context in the entropy\nestimation function. This greatly limits their performance due to the absence\nof a global vision. In this work, we propose a novel Global Reference Model for\nimage compression to effectively leverage both the local and the global context\ninformation, leading to an enhanced compression rate. The proposed method scans\ndecoded latents and then finds the most relevant latent to assist the\ndistribution estimating of the current latent. A by-product of this work is the\ninnovation of a mean-shifting GDN module that further improves the performance.\nExperimental results demonstrate that the proposed model outperforms the\nrate-distortion performance of most of the state-of-the-art methods in the\nindustry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1\">Yichen Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tan_Z/0/1/0/all/0/1\">Zhiyu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1\">Dongyang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenhong Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Channel Sparsity Search via Weight Sharing within Filters. (arXiv:2010.14714v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.14714","description":"<p>In this paper, we propose the differentiable channel sparsity search (DCSS)\nfor convolutional neural networks. Unlike traditional channel pruning\nalgorithms which require users to manually set prune ratios for each\nconvolutional layer, DCSS automatically searches the optimal combination of\nsparsities. Inspired by the differentiable architecture search (DARTS), we draw\nlessons from the continuous relaxation and leverage the gradient information to\nbalance the computational cost and metrics. Since directly applying the scheme\nof DARTS causes shape mismatching and excessive memory consumption, we\nintroduce a novel technique called weight sharing within filters. This\ntechnique elegantly eliminates the problem of shape mismatching with negligible\nadditional resources. We conduct comprehensive experiments on not only image\nclassification but also find-grained tasks including semantic segmentation and\nimage super resolution to verify the effectiveness of DCSS. Compared with\nprevious network pruning approaches, DCSS achieves state-of-the-art results for\nimage classification. Experimental results of semantic segmentation and image\nsuper resolution indicate that task-specific search achieves better performance\nthan transferring slim models, demonstrating the wide applicability and high\nefficiency of DCSS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Kuei Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Suppression of Correlated Noise with Similarity-based Unsupervised Deep Learning. (arXiv:2011.03384v6 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.03384","description":"<p>Image denoising is a prerequisite for downstream tasks in many fields.\nLow-dose and photon-counting computed tomography (CT) denoising can optimize\ndiagnostic performance at minimized radiation dose. Supervised deep denoising\nmethods are popular but require paired clean or noisy samples that are often\nunavailable in practice. Limited by the independent noise assumption, current\nunsupervised denoising methods cannot process correlated noises as in CT\nimages. Here we propose the first-of-its-kind similarity-based unsupervised\ndeep denoising approach, referred to as Noise2Sim, that works in a nonlocal and\nnonlinear fashion to suppress not only independent but also correlated noises.\nTheoretically, Noise2Sim is asymptotically equivalent to supervised learning\nmethods under mild conditions. Experimentally, Nosie2Sim recovers intrinsic\nfeatures from noisy low-dose CT and photon-counting CT images as effectively as\nor even better than supervised learning methods on practical datasets visually,\nquantitatively and statistically. Noise2Sim is a general unsupervised denoising\napproach and has great potential in diverse applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengzhou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1\">Fenglei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weiwen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaodong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMLight: Lighting Estimation via Geometric Distribution Approximation. (arXiv:2102.10244v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10244","description":"<p>Inferring the scene illumination from a single image is an essential yet\nchallenging task in computer vision and computer graphics. Existing works\nestimate lighting by regressing representative illumination parameters or\ngenerating illumination maps directly. However, these methods often suffer from\npoor accuracy and generalization. This paper presents Geometric Mover's Light\n(GMLight), a lighting estimation framework that employs a regression network\nand a generative projector for effective illumination estimation. We\nparameterize illumination scenes in terms of the geometric light distribution,\nlight intensity, ambient term, and auxiliary depth, which can be estimated by a\nregression network. Inspired by the earth mover's distance, we design a novel\ngeometric mover's loss to guide the accurate regression of light distribution\nparameters. With the estimated light parameters, the generative projector\nsynthesizes panoramic illumination maps with realistic appearance and\nhigh-frequency details. Extensive experiments show that GMLight achieves\naccurate illumination estimation and superior fidelity in relighting for 3D\nobject insertion. The codes are available at\n\\href{https://github.com/fnzhan/Illumination-Estimation}{https://github.com/fnzhan/Illumination-Estimation}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changgong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wenbo Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Feiying Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monocular 3D Vehicle Detection Using Uncalibrated Traffic Cameras through Homography. (arXiv:2103.15293v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15293","description":"<p>This paper proposes a method to extract the position and pose of vehicles in\nthe 3D world from a single traffic camera. Most previous monocular 3D vehicle\ndetection algorithms focused on cameras on vehicles from the perspective of a\ndriver, and assumed known intrinsic and extrinsic calibration. On the contrary,\nthis paper focuses on the same task using uncalibrated monocular traffic\ncameras. We observe that the homography between the road plane and the image\nplane is essential to 3D vehicle detection and the data synthesis for this\ntask, and the homography can be estimated without the camera intrinsics and\nextrinsics. We conduct 3D vehicle detection by estimating the rotated bounding\nboxes (r-boxes) in the bird's eye view (BEV) images generated from inverse\nperspective mapping. We propose a new regression target called tailed r-box and\na dual-view network architecture which boosts the detection accuracy on warped\nBEV images. Experiments show that the proposed method can generalize to new\ncamera and environment setups despite not seeing imaged from them during\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Minghan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yuanxin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pingping Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Huei Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenneman_J/0/1/0/all/0/1\">John Lenneman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two stages for visual object tracking. (arXiv:2104.13648v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13648","description":"<p>Siamese-based trackers have achived promising performance on visual object\ntracking tasks. Most existing Siamese-based trackers contain two separate\nbranches for tracking, including classification branch and bounding box\nregression branch. In addition, image segmentation provides an alternative way\nto obetain the more accurate target region. In this paper, we propose a novel\ntracker with two-stages: detection and segmentation. The detection stage is\ncapable of locating the target by Siamese networks. Then more accurate tracking\nresults are obtained by segmentation module given the coarse state estimation\nin the first stage. We conduct experiments on four benchmarks. Our approach\nachieves state-of-the-art results, with the EAO of 52.6$\\%$ on VOT2016,\n51.3$\\%$ on VOT2018, and 39.0$\\%$ on VOT2019 datasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Fei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaodong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Real-Time Monocular SLAM Using Semantic Segmentation on Selective Frames. (arXiv:2105.00114v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.00114","description":"<p>Monocular simultaneous localization and mapping (SLAM) is emerging in\nadvanced driver assistance systems and autonomous driving, because a single\ncamera is cheap and easy to install. Conventional monocular SLAM has two major\nchallenges leading inaccurate localization and mapping. First, it is\nchallenging to estimate scales in localization and mapping. Second,\nconventional monocular SLAM uses inappropriate mapping factors such as dynamic\nobjects and low-parallax areas in mapping. This paper proposes an improved\nreal-time monocular SLAM that resolves the aforementioned challenges by\nefficiently using deep learning-based semantic segmentation. To achieve the\nreal-time execution of the proposed method, we apply semantic segmentation only\nto downsampled keyframes in parallel with mapping processes. In addition, the\nproposed method corrects scales of camera poses and three-dimensional (3D)\npoints, using estimated ground plane from road-labeled 3D points and the real\ncamera height. The proposed method also removes inappropriate corner features\nlabeled as moving objects and low parallax areas. Experiments with eight video\nsequences demonstrate that the proposed monocular SLAM system achieves\nsignificantly improved and comparable trajectory tracking accuracy, compared to\nexisting state-of-the-art monocular and stereo SLAM systems, respectively. The\nproposed system can achieve real-time tracking on a standard CPU potentially\nwith a standard GPU support, whereas existing segmentation-aided monocular SLAM\ndoes not.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_M/0/1/0/all/0/1\">Muhyun Back</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Soo Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun_I/0/1/0/all/0/1\">Il Yong Chun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups. (arXiv:2106.00676v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00676","description":"<p>Accurately extracting structured content from PDFs is a critical first step\nfor NLP over scientific papers. Recent work has improved extraction accuracy by\nincorporating elementary layout information, e.g., each token's 2D position on\nthe page, into language model pretraining. We introduce new methods that\nexplicitly model VIsual LAyout (VILA) groups, i.e., text lines or text blocks,\nto further improve performance. In our I-VILA approach, we show that simply\ninserting special tokens denoting layout group boundaries into model inputs can\nlead to a 1.9% Macro F1 improvement in token classification. In the H-VILA\napproach, we show that hierarchical encoding of layout-groups can result in\nup-to 47% inference time reduction with less than 0.8% Macro F1 loss. Unlike\nprior layout-aware approaches, our methods do not require expensive additional\npretraining, only fine-tuning, which we show can reduce training cost by up to\n95%. Experiments are conducted on a newly curated evaluation suite, S2-VLUE,\nthat unifies existing automatically-labeled datasets and includes a new dataset\nof manual annotations covering diverse papers from 19 scientific disciplines.\nPre-trained weights, benchmark datasets, and source code are available at\nhttps://github.com/allenai/VILA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zejiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lucy Lu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Convolutional Neural Networks for Onychomycosis Detection. (arXiv:2106.16139v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.16139","description":"<p>The diagnosis of superficial fungal infections in dermatology is still mostly\nbased on manual direct microscopic examination with Potassium Hydroxide (KOH)\nsolution. However, this method can be time consuming and its diagnostic\naccuracy rates vary widely depending on the clinician's experience. With the\nincrease of neural network applications in the field of clinical microscopy, it\nis now possible to automate such manual processes increasing both efficiency\nand accuracy. This study presents a deep neural network structure that enables\nthe rapid solutions for these problems and can perform automatic fungi\ndetection in grayscale images without dyes. 160 microscopic field photographs\ncontaining the fungal element, obtained from patients with onychomycosis, and\n297 microscopic field photographs containing dissolved keratin obtained from\nnormal nails were collected. Smaller patches containing 4234 fungi and 4981\nkeratin were extracted from these images. In order to detect fungus and\nkeratin, VGG16 and InceptionV3 models were developed. The VGG16 model had\n95.98% accuracy, and the area under the curve (AUC) value of 0.9930, while the\nInceptionV3 model had 95.90% accuracy and the AUC value of 0.9917. However,\naverage accuracy and AUC value of clinicians is 72.8% and 0.87, respectively.\nThis deep learning model allows the development of an automated system that can\ndetect fungi within microscopic images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Abdurrahim Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1\">Fatih Goktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1\">Rahmetullah Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1\">Gulsum Gencoglan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1\">Huseyin Uvet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rectifying the Shortcut Learning of Background for Few-Shot Learning. (arXiv:2107.07746v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.07746","description":"<p>The category gap between training and evaluation has been characterised as\none of the main obstacles to the success of Few-Shot Learning (FSL). In this\npaper, we for the first time empirically identify image background, common in\nrealistic images, as a shortcut knowledge helpful for in-class classification\nbut ungeneralizable beyond training categories in FSL. A novel framework,\nCOSOC, is designed to tackle this problem by extracting foreground objects in\nimages at both training and evaluation without any extra supervision. Extensive\nexperiments carried on inductive FSL tasks demonstrate the effectiveness of our\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xu Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Longhui Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Liangjian Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lingxi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations. (arXiv:2108.01073v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.01073","description":"<p>Guided image synthesis enables everyday users to create and edit\nphoto-realistic images with minimum effort. The key challenge is balancing\nfaithfulness to the user input (e.g., hand-drawn colored strokes) and realism\nof the synthesized image. Existing GAN-based methods attempt to achieve such\nbalance using either conditional GANs or GAN inversions, which are challenging\nand often require additional training data or loss functions for individual\napplications. To address these issues, we introduce a new image synthesis and\nediting method, Stochastic Differential Editing (SDEdit), based on a diffusion\nmodel generative prior, which synthesizes realistic images by iteratively\ndenoising through a stochastic differential equation (SDE). Given an input\nimage with user guide of any type, SDEdit first adds noise to the input, then\nsubsequently denoises the resulting image through the SDE prior to increase its\nrealism. SDEdit does not require task-specific training or inversions and can\nnaturally achieve the balance between realism and faithfulness. SDEdit\nsignificantly outperforms state-of-the-art GAN-based methods by up to 98.09% on\nrealism and 91.72% on overall satisfaction scores, according to a human\nperception study, on multiple tasks, including stroke-based image synthesis and\nediting as well as image compositing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chenlin Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yutong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun-Yan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bi-Temporal Semantic Reasoning for the Semantic Change Detection in HR Remote Sensing Images. (arXiv:2108.06103v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06103","description":"<p>Semantic change detection (SCD) extends the multi-class change detection\n(MCD) task to provide not only the change locations but also the detailed\nland-cover/land-use (LCLU) categories before and after the observation\nintervals. This fine-grained semantic change information is very useful in many\napplications. Recent studies indicate that the SCD can be modeled through a\ntriple-branch Convolutional Neural Network (CNN), which contains two temporal\nbranches and a change branch. However, in this architecture, the communications\nbetween the temporal branches and the change branch are insufficient. To\novercome the limitations in existing methods, we propose a novel CNN\narchitecture for the SCD, where the semantic temporal features are merged in a\ndeep CD unit. Furthermore, we elaborate on this architecture to reason the\nbi-temporal semantic correlations. The resulting Bi-temporal Semantic Reasoning\nNetwork (Bi-SRNet) contains two types of semantic reasoning blocks to reason\nboth single-temporal and cross-temporal semantic correlations, as well as a\nnovel loss function to improve the semantic consistency of change detection\nresults. Experimental results on a benchmark dataset show that the proposed\narchitecture obtains significant accuracy improvements over the existing\napproaches, while the added designs in the Bi-SRNet further improves the\nsegmentation of both semantic categories and the changed areas. The codes in\nthis paper are accessible at: github.com/ggsDing/Bi-SRNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Haitao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sicong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1\">Lichao Mou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"No-Reference Image Quality Assessment via Transformers, Relative Ranking, and Self-Consistency. (arXiv:2108.06858v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.06858","description":"<p>The goal of No-Reference Image Quality Assessment (NR-IQA) is to estimate the\nperceptual image quality in accordance with subjective evaluations, it is a\ncomplex and unsolved problem due to the absence of the pristine reference\nimage. In this paper, we propose a novel model to address the NR-IQA task by\nleveraging a hybrid approach that benefits from Convolutional Neural Networks\n(CNNs) and self-attention mechanism in Transformers to extract both local and\nnon-local features from the input image. We capture local structure information\nof the image via CNNs, then to circumvent the locality bias among the extracted\nCNNs features and obtain a non-local representation of the image, we utilize\nTransformers on the extracted features where we model them as a sequential\ninput to the Transformer model. Furthermore, to improve the monotonicity\ncorrelation between the subjective and objective scores, we utilize the\nrelative distance information among the images within each batch and enforce\nthe relative ranking among them. Last but not least, we observe that the\nperformance of NR-IQA models degrades when we apply equivariant transformations\n(e.g. horizontal flipping) to the inputs. Therefore, we propose a method that\nleverages self-consistency as a source of self-supervision to improve the\nrobustness of NRIQA models. Specifically, we enforce self-consistency between\nthe outputs of our quality assessment model for each image and its\ntransformation (horizontally flipped) to utilize the rich self-supervisory\ninformation and reduce the uncertainty of the model. To demonstrate the\neffectiveness of our work, we evaluate it on seven standard IQA datasets (both\nsynthetic and authentic) and show that our model achieves state-of-the-art\nresults on various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Golestaneh_S/0/1/0/all/0/1\">S. Alireza Golestaneh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dadsetan_S/0/1/0/all/0/1\">Saba Dadsetan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementary Feature Enhanced Network with Vision Transformer for Image Dehazing. (arXiv:2109.07100v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07100","description":"<p>Conventional CNNs-based dehazing models suffer from two essential issues: the\ndehazing framework (limited in interpretability) and the convolution layers\n(content-independent and ineffective to learn long-range dependency\ninformation). In this paper, firstly, we propose a new complementary feature\nenhanced framework, in which the complementary features are learned by several\ncomplementary subtasks and then together serve to boost the performance of the\nprimary task. One of the prominent advantages of the new framework is that the\npurposively chosen complementary tasks can focus on learning weakly dependent\ncomplementary features, avoiding repetitive and ineffective learning of the\nnetworks. We design a new dehazing network based on such a framework.\nSpecifically, we select the intrinsic image decomposition as the complementary\ntasks, where the reflectance and shading prediction subtasks are used to\nextract the color-wise and texture-wise complementary features. To effectively\naggregate these complementary features, we propose a complementary features\nselection module (CFSM) to select the more useful features for image dehazing.\nFurthermore, we introduce a new version of vision transformer block, named\nHybrid Local-Global Vision Transformer (HyLoG-ViT), and incorporate it within\nour dehazing networks. The HyLoG-ViT block consists of the local and the global\nvision transformer paths used to capture local and global dependencies. As a\nresult, the HyLoG-ViT introduces locality in the networks and captures the\nglobal and long-range dependencies. Extensive experiments on homogeneous,\nnon-homogeneous, and nighttime dehazing tasks reveal that the proposed dehazing\nnetwork can achieve comparable or even better performance than CNNs-based\ndehazing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Long Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Gradient Non-sign Methods. (arXiv:2110.12734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12734","description":"<p>Adversarial attacks make their success in \"fooling\" DNNs and among them,\ngradient-based algorithms become one of the mainstreams. Based on the linearity\nhypothesis [12], under $\\ell_\\infty$ constraint, $sign$ operation applied to\nthe gradients is a good choice for generating perturbations. However, the\nside-effect from such operation exists since it leads to the bias of direction\nbetween the real gradients and the perturbations. In other words, current\nmethods contain a gap between real gradients and actual noises, which leads to\nbiased and inefficient attacks. Therefore in this paper, based on the Taylor\nexpansion, the bias is analyzed theoretically and the correction of $\\sign$,\ni.e., Fast Gradient Non-sign Method (FGNM), is further proposed. Notably, FGNM\nis a general routine, which can seamlessly replace the conventional $sign$\noperation in gradient-based attacks with negligible extra computational cost.\nExtensive experiments demonstrate the effectiveness of our methods.\nSpecifically, ours outperform them by \\textbf{27.5\\%} at most and\n\\textbf{9.5\\%} on average. Our anonymous code is publicly available:\n\\url{https://git.io/mm-fgnm}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yaya Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaosu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qilong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lianli Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jingkuan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Versatile Learned Video Compression. (arXiv:2111.03386v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.03386","description":"<p>Learned video compression methods have demonstrated great promise in catching\nup with traditional video codecs in their rate-distortion (R-D) performance.\nHowever, existing learned video compression schemes are limited by the binding\nof the prediction mode and the fixed network framework. They are unable to\nsupport various inter prediction modes and thus inapplicable for various\nscenarios. In this paper, to break this limitation, we propose a versatile\nlearned video compression (VLVC) framework that uses one model to support all\npossible prediction modes. Specifically, to realize versatile compression, we\nfirst build a motion compensation module that applies multiple 3D motion vector\nfields (i.e., voxel flows) for weighted trilinear warping in spatial-temporal\nspace. The voxel flows convey the information of temporal reference position\nthat helps to decouple inter prediction modes away from framework designing.\nSecondly, in case of multiple-reference-frame prediction, we apply a flow\nprediction module to predict accurate motion trajectories with unified\npolynomial functions. We show that the flow prediction module can largely\nreduce the transmission cost of voxel flows. Experimental results demonstrate\nthat our proposed VLVC not only supports versatile compression in various\nsettings, but also is the first end-to-end learned video compression method\nthat outperforms the latest VVC/H.266 standard reference software in terms of\nMS-SSIM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1\">Zongyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Guided 3D Image Recognition for Carbonate Pore and Mineral Volumes Determination. (arXiv:2111.04612v2 [physics.geo-ph] UPDATED)","link":"http://arxiv.org/abs/2111.04612","description":"<p>Automated image processing algorithms can improve the quality, efficiency,\nand consistency of classifying the morphology of heterogeneous carbonate rock\nand can deal with a massive amount of data and images seamlessly. Geoscientists\nface difficulties in setting the direction of the optimum method for\ndetermining petrophysical properties from rock images, Micro-Computed\nTomography (uCT), or Magnetic Resonance Imaging (MRI). Most of the successful\nwork is from the homogeneous rocks focusing on 2D images with less focus on 3D\nand requiring numerical simulation. Currently, image analysis methods converge\nto three approaches: image processing, artificial intelligence, and combined\nimage processing with artificial intelligence. In this work, we propose two\nmethods to determine the porosity from 3D uCT and MRI images: an image\nprocessing method with Image Resolution Optimized Gaussian Algorithm (IROGA);\nadvanced image recognition method enabled by Machine Learning Difference of\nGaussian Random Forest (MLDGRF). We have built reference 3D micro models and\ncollected images for calibration of IROGA and MLDGRF methods. To evaluate the\npredictive capability of these calibrated approaches, we ran them on 3D uCT and\nMRI images of natural heterogeneous carbonate rock. We measured the porosity\nand lithology of the carbonate rock using three and two industry-standard ways,\nrespectively, as reference values. Notably, IROGA and MLDGRF have produced\nporosity results with an accuracy of 96.2% and 97.1% on the training set and\n91.7% and 94.4% on blind test validation, respectively, in comparison with the\nthree experimental measurements. We measured limestone and pyrite reference\nvalues using two methods, X-ray powder diffraction, and grain density\nmeasurements. MLDGRF has produced lithology (limestone and Pyrite) volumes with\n97.7% accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Raza_A/0/1/0/all/0/1\">Aikifa Raza</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1\">Hongtao Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozzane_D/0/1/0/all/0/1\">Djamel Ozzane</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_T/0/1/0/all/0/1\">Tiejun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Rule-induction Visual Reasoning on RPMs with an Application to Video Prediction. (arXiv:2111.12301v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12301","description":"<p>Raven's Progressive Matrices (RPMs) are frequently used in evaluating human's\nvisual reasoning ability. Researchers have made considerable efforts in\ndeveloping systems to automatically solve the RPM problem, often through a\nblack-box end-to-end convolutional neural network for both visual recognition\nand logical reasoning tasks. Based on the two intrinsic natures of RPM problem,\nvisual recognition and logical reasoning, we propose a Two-stage Rule-Induction\nVisual Reasoner (TRIVR), which consists of a perception module and a reasoning\nmodule, to tackle the challenges of real-world visual recognition and\nsubsequent logical reasoning tasks, respectively. For the reasoning module, we\nfurther propose a \"2+1\" formulation that models human's thinking in solving\nRPMs and significantly reduces the model complexity. It derives a reasoning\nrule from each RPM sample, which is not feasible for existing methods. As a\nresult, the proposed reasoning module is capable of yielding a set of reasoning\nrules modeling human in solving the RPM problems. To validate the proposed\nmethod on real-world applications, an RPM-like Video Prediction (RVP) dataset\nis constructed, where visual reasoning is conducted on RPMs constructed using\nreal-world video frames. Experimental results on various RPM-like datasets\ndemonstrate that the proposed TRIVR achieves a significant and consistent\nperformance gain compared with the state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wentao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jianfeng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1\">Ruibin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xudong Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphology Decoder: A Machine Learning Guided 3D Vision Quantifying Heterogenous Rock Permeability for Planetary Surveillance and Robotic Functions. (arXiv:2111.13460v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13460","description":"<p>Permeability has a dominant influence on the flow properties of a natural\nfluid. Lattice Boltzmann simulator determines permeability from the nano and\nmicropore network. The simulator holds millions of flow dynamics calculations\nwith its accumulated errors and high consumption of computing power. To\nefficiently and consistently predict permeability, we propose a morphology\ndecoder, a parallel and serial flow reconstruction of machine learning\nsegmented heterogeneous Cretaceous texture from 3D micro computerized\ntomography and nuclear magnetic resonance images. For 3D vision, we introduce\ncontrollable-measurable-volume as new supervised segmentation, in which a\nunique set of voxel intensity corresponds to grain and pore throat sizes. The\nmorphology decoder demarks and aggregates the morphologies boundaries in a\nnovel way to produce permeability. Morphology decoder method consists of five\nnovel processes, which describes in this paper, these novel processes are: (1)\nGeometrical 3D Permeability, (2) Machine Learning guided 3D Properties\nRecognition of Rock Morphology, (3) 3D Image Properties Integration Model for\nPermeability, (4) MRI Permeability Imager, and (5) Morphology Decoder (the\nprocess that integrates the other four novel processes).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raza_A/0/1/0/all/0/1\">Aikifa Raza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouzzane_D/0/1/0/all/0/1\">Djamel Ouzzane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongxia Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiejun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TCGL: Temporal Contrastive Graph for Self-supervised Video Representation Learning. (arXiv:2112.03587v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03587","description":"<p>Video self-supervised learning is a challenging task, which requires\nsignificant expressive power from the model to leverage rich spatial-temporal\nknowledge and generate effective supervisory signals from large amounts of\nunlabeled videos. However, existing methods fail to increase the temporal\ndiversity of unlabeled videos and ignore elaborately modeling multi-scale\ntemporal dependencies in an explicit way. To overcome these limitations, we\ntake advantage of the multi-scale temporal dependencies within videos and\nproposes a novel video self-supervised learning framework named Temporal\nContrastive Graph Learning (TCGL), which jointly models the inter-snippet and\nintra-snippet temporal dependencies for temporal representation learning with a\nhybrid graph contrastive learning strategy. Specifically, a Spatial-Temporal\nKnowledge Discovering (STKD) module is first introduced to extract\nmotion-enhanced spatial-temporal representations from videos based on the\nfrequency domain analysis of discrete cosine transform. To explicitly model\nmulti-scale temporal dependencies of unlabeled videos, our TCGL integrates the\nprior knowledge about the frame and snippet orders into graph structures, i.e.,\nthe intra-/inter- snippet Temporal Contrastive Graphs (TCG). Then, specific\ncontrastive learning modules are designed to maximize the agreement between\nnodes in different graph views. To generate supervisory signals for unlabeled\nvideos, we introduce an Adaptive Snippet Order Prediction (ASOP) module which\nleverages the relational knowledge among video snippets to learn the global\ncontext representation and recalibrate the channel-wise features adaptively.\nExperimental results demonstrate the superiority of our TCGL over the\nstate-of-the-art methods on large-scale action recognition and video retrieval\nbenchmarks.The code is publicly available at\nhttps://github.com/YangLiu9208/TCGL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1\">Haoyuan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital Rock Typing DRT Algorithm Formulation with Optimal Supervised Semantic Segmentation. (arXiv:2112.15068v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.15068","description":"<p>Each grid block in a 3D geological model requires a rock type that represents\nall physical and chemical properties of that block. The properties that\nclassify rock types are lithology, permeability, and capillary pressure.\nScientists and engineers determined these properties using conventional\nlaboratory measurements, which embedded destructive methods to the sample or\naltered some of its properties (i.e., wettability, permeability, and porosity)\nbecause the measurements process includes sample crushing, fluid flow, or fluid\nsaturation. Lately, Digital Rock Physics (DRT) has emerged to quantify these\nproperties from micro-Computerized Tomography (uCT) and Magnetic Resonance\nImaging (MRI) images. However, the literature did not attempt rock typing in a\nwholly digital context. We propose performing Digital Rock Typing (DRT) by: (1)\nintegrating the latest DRP advances in a novel process that honors digital rock\nproperties determination, while; (2) digitalizing the latest rock typing\napproaches in carbonate, and (3) introducing a novel carbonate rock typing\nprocess that utilizes computer vision capabilities to provide more insight\nabout the heterogeneous carbonate rock texture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfarisi_O/0/1/0/all/0/1\">Omar Alfarisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouzzane_D/0/1/0/all/0/1\">Djamel Ouzzane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sassi_M/0/1/0/all/0/1\">Mohamed Sassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tiejun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"P2P-Loc: Point to Point Tiny Person Localization. (arXiv:2112.15344v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.15344","description":"<p>Bounding-box annotation form has been the most frequently used method for\nvisual object localization tasks. However, bounding-box annotation relies on a\nlarge amount of precisely annotating bounding boxes, and it is expensive and\nlaborious. It is impossible to be employed in practical scenarios and even\nredundant for some applications (such as tiny person localization) that the\nsize would not matter. Therefore, we propose a novel point-based framework for\nthe person localization task by annotating each person as a coarse point\n(CoarsePoint) instead of an accurate bounding box that can be any point within\nthe object extent. Then, the network predicts the person's location as a 2D\ncoordinate in the image. Although this greatly simplifies the data annotation\npipeline, the CoarsePoint annotation inevitably decreases label reliability\n(label uncertainty) and causes network confusion during training. As a result,\nwe propose a point self-refinement approach that iteratively updates point\nannotations in a self-paced way. The proposed refinement system alleviates the\nlabel uncertainty and progressively improves localization performance.\nExperimental results show that our approach has achieved comparable object\nlocalization performance while saving up to 80$\\%$ of annotation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xuehui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qixiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jianbin Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhenjun Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCoPnet: Exploring Local Motion and Contrast Priors for Infrared Small Target Super-Resolution. (arXiv:2201.01014v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.01014","description":"<p>Infrared small target super-resolution (SR) aims to recover reliable and\ndetailed high-resolution image with highcontrast targets from its\nlow-resolution counterparts. Since the infrared small target lacks color and\nfine structure information, it is significant to exploit the supplementary\ninformation among sequence images to enhance the target. In this paper, we\npropose the first infrared small target SR method named local motion and\ncontrast prior driven deep network (MoCoPnet) to integrate the domain knowledge\nof infrared small target into deep network, which can mitigate the intrinsic\nfeature scarcity of infrared small targets. Specifically, motivated by the\nlocal motion prior in the spatio-temporal dimension, we propose a local\nspatiotemporal attention module to perform implicit frame alignment and\nincorporate the local spatio-temporal information to enhance the local features\n(especially for small targets). Motivated by the local contrast prior in the\nspatial dimension, we propose a central difference residual group to\nincorporate the central difference convolution into the feature extraction\nbackbone, which can achieve center-oriented gradient-aware feature extraction\nto further improve the target contrast. Extensive experiments have demonstrated\nthat our method can recover accurate spatial dependency and improve the target\ncontrast. Comparative results show that MoCoPnet can outperform the\nstate-of-the-art video SR and single image SR methods in terms of both SR\nperformance and target enhancement. Based on the SR results, we further\ninvestigate the influence of SR on infrared small target detection and the\nexperimental results demonstrate that MoCoPnet promotes the detection\nperformance. The code is available at https://github.com/XinyiYing/MoCoPnet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ying_X/0/1/0/all/0/1\">Xinyi Ying</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Longguang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheng_W/0/1/0/all/0/1\">Weidong Sheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zaipin Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1\">Shilin Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-05T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}