{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment. (arXiv:2203.14987v1 [cs.AI])","link":"http://arxiv.org/abs/2203.14987","description":"<p>Predicting missing facts in a knowledge graph (KG) is crucial as modern KGs\nare far from complete. Due to labor-intensive human labeling, this phenomenon\ndeteriorates when handling knowledge represented in various languages. In this\npaper, we explore multilingual KG completion, which leverages limited seed\nalignment as a bridge, to embrace the collective knowledge from multiple\nlanguages. However, language alignment used in prior works is still not fully\nexploited: (1) alignment pairs are treated equally to maximally push parallel\nentities to be close, which ignores KG capacity inconsistency; (2) seed\nalignment is scarce and new alignment identification is usually in a noisily\nunsupervised manner. To tackle these issues, we propose a novel self-supervised\nadaptive graph alignment (SS-AGA) method. Specifically, SS-AGA fuses all KGs as\na whole graph by regarding alignment as a new edge type. As such, information\npropagation and noise influence across KGs can be adaptively controlled via\nrelation-aware attention weights. Meanwhile, SS-AGA features a new pair\ngenerator that dynamically captures potential alignment pairs in a\nself-supervised paradigm. Extensive experiments on both the public multilingual\nDBPedia KG and newly-created industrial multilingual E-commerce KG empirically\ndemonstrate the effectiveness of SS-AG\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zijie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haoming Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Tianyu Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1\">Bing Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subbian_K/0/1/0/all/0/1\">Karthik Subbian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing in context: Improving cosine similarity measures with a metric tensor. (arXiv:2203.14996v1 [cs.CL])","link":"http://arxiv.org/abs/2203.14996","description":"<p>Cosine similarity is a widely used measure of the relatedness of pre-trained\nword embeddings, trained on a language modeling goal. Datasets such as\nWordSim-353 and SimLex-999 rate how similar words are according to human\nannotators, and as such are often used to evaluate the performance of language\nmodels. Thus, any improvement on the word similarity task requires an improved\nword representation. In this paper, we propose instead the use of an extended\ncosine similarity measure to improve performance on that task, with gains in\ninterpretability. We explore the hypothesis that this approach is particularly\nuseful if the word-similarity pairs share the same context, for which distinct\ncontextualized similarity measures can be learned. We first use the dataset of\nRichie et al. (2020) to learn contextualized metrics and compare the results\nwith the baseline values obtained using the standard cosine similarity measure,\nwhich consistently shows improvement. We also train a contextualized similarity\nmeasure for both SimLex-999 and WordSim-353, comparing the results with the\ncorresponding baselines, and using these datasets as independent test sets for\nthe all-context similarity measure learned on the contextualized dataset,\nobtaining positive results for a number of tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vos_I/0/1/0/all/0/1\">Isa M. Apallius de Vos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boogerd_G/0/1/0/all/0/1\">Ghislaine L. van den Boogerd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fennema_M/0/1/0/all/0/1\">Mara D. Fennema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Adriana D. Correia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v1 [eess.AS])","link":"http://arxiv.org/abs/2203.15081","description":"<p>We present a method for visually-grounded spoken term discovery. After\ntraining either a HuBERT or wav2vec2.0 model to associate spoken captions with\nnatural images, we show that powerful word segmentation and clustering\ncapability emerges within the model's self-attention heads. Our experiments\nreveal that this ability is not present to nearly the same extent in the base\nHuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a\ncrucial component of the word discovery capability we observe. We also evaluate\nour method on the Buckeye word segmentation and ZeroSpeech spoken term\ndiscovery tasks, where we outperform all currently published methods on several\nmetrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated Named Entity Recognition. (arXiv:2203.15101v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15101","description":"<p>We present an analysis of the performance of Federated Learning in a\nparadigmatic natural-language processing task: Named-Entity Recognition (NER).\nFor our evaluation, we use the language-independent CoNLL-2003 dataset as our\nbenchmark dataset and a Bi-LSTM-CRF model as our benchmark NER model. We show\nthat federated training reaches almost the same performance as the centralized\nmodel, though with some performance degradation as the learning environments\nbecome more heterogeneous. We also show the convergence rate of federated\nmodels for NER. Finally, we discuss existing challenges of Federated Learning\nfor NLP applications that can foster future research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1\">Joel Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stripelis_D/0/1/0/all/0/1\">Dimitris Stripelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambite_J/0/1/0/all/0/1\">Jos&#xe9; Luis Ambite</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Well-Composed Text is Half Done! Composition Sampling for Diverse Conditional Generation. (arXiv:2203.15108v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15108","description":"<p>We propose Composition Sampling, a simple but effective method to generate\ndiverse outputs for conditional generation of higher quality compared to\nprevious stochastic decoding strategies. It builds on recently proposed\nplan-based neural generation models (Narayan et al, 2021) that are trained to\nfirst create a composition of the output and then generate by conditioning on\nit and the input. Our approach avoids text degeneration by first sampling a\ncomposition in the form of an entity chain and then using beam search to\ngenerate the best possible text grounded to this entity chain. Experiments on\nsummarization (CNN/DailyMail and XSum) and question generation (SQuAD), using\nexisting and newly proposed automatic metrics together with human-based\nevaluation, demonstrate that Composition Sampling is currently the best\navailable decoding strategy for generating diverse meaningful outputs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simoes_G/0/1/0/all/0/1\">Gon&#xe7;alo Sim&#xf5;es</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15125","description":"<p>Natural language-based communication with mobile devices and home appliances\nis becoming increasingly popular and has the potential to become natural for\ncommunicating with mobile robots in the future. Towards this goal, we\ninvestigate cross-modal text-to-point-cloud localization that will allow us to\nspecify, for example, a vehicle pick-up or goods delivery location. In\nparticular, we propose Text2Pos, a cross-modal localization module that learns\nto align textual descriptions with localization cues in a coarse- to-fine\nmanner. Given a point cloud of the environment, Text2Pos locates a position\nthat is specified via a natural language-based description of the immediate\nsurroundings. To train Text2Pos and study its performance, we construct\nKITTI360Pose, the first dataset for this task based on the recently introduced\nKITTI360 dataset. Our experiments show that we can localize 65% of textual\nqueries within 15m distance to query locations for top-10 retrieved locations.\nThis is a starting point that we hope will spark future developments towards\nlanguage-based navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolmet_M/0/1/0/all/0/1\">Manuel Kolmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qunjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljosa Osep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filler Word Detection and Classification: A Dataset and Benchmark. (arXiv:2203.15135v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15135","description":"<p>Filler words such as `uh' or `um' are sounds or words people use to signal\nthey are pausing to think. Finding and removing filler words from recordings is\na common and tedious task in media editing. Automatically detecting and\nclassifying filler words could greatly aid in this task, but few studies have\nbeen published on this problem. A key reason is the absence of a dataset with\nannotated filler words for training and evaluation. In this work, we present a\nnovel speech dataset, PodcastFillers, with 35K annotated filler words and 50K\nannotations of other sounds that commonly occur in podcasts such as breaths,\nlaughter, and word repetitions. We propose a pipeline that leverages VAD and\nASR to detect filler candidates and a classifier to distinguish between filler\nword types. We evaluate our proposed pipeline on PodcastFillers, compare to\nseveral baselines, and present a detailed ablation study. In particular, we\nevaluate the importance of using ASR and how it compares to a\ntranscription-free approach resembling keyword spotting. We show that our\npipeline obtains state-of-the-art results, and that leveraging ASR strongly\noutperforms a keyword spotting approach. We make PodcastFillers publicly\navailable, and hope our work serves as a benchmark for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Ge Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caceres_J/0/1/0/all/0/1\">Juan-Pablo Caceres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salamon_J/0/1/0/all/0/1\">Justin Salamon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-AI Collaboration Enables More Empathic Conversations in Text-based Peer-to-Peer Mental Health Support. (arXiv:2203.15144v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15144","description":"<p>Advances in artificial intelligence (AI) are enabling systems that augment\nand collaborate with humans to perform simple, mechanistic tasks like\nscheduling meetings and grammar-checking text. However, such Human-AI\ncollaboration poses challenges for more complex, creative tasks, such as\ncarrying out empathic conversations, due to difficulties of AI systems in\nunderstanding complex human emotions and the open-ended nature of these tasks.\nHere, we focus on peer-to-peer mental health support, a setting in which\nempathy is critical for success, and examine how AI can collaborate with humans\nto facilitate peer empathy during textual, online supportive conversations. We\ndevelop Hailey, an AI-in-the-loop agent that provides just-in-time feedback to\nhelp participants who provide support (peer supporters) respond more\nempathically to those seeking help (support seekers). We evaluate Hailey in a\nnon-clinical randomized controlled trial with real-world peer supporters on\nTalkLife (N=300), a large online peer-to-peer support platform. We show that\nour Human-AI collaboration approach leads to a 19.60% increase in\nconversational empathy between peers overall. Furthermore, we find a larger\n38.88% increase in empathy within the subsample of peer supporters who\nself-identify as experiencing difficulty providing support. We systematically\nanalyze the Human-AI collaboration patterns and find that peer supporters are\nable to use the AI feedback both directly and indirectly without becoming\noverly reliant on AI while reporting improved self-efficacy post-feedback. Our\nfindings demonstrate the potential of feedback-driven, AI-in-the-loop writing\nsystems to empower humans in open-ended, social, creative tasks such as\nempathic conversations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Ashish Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_I/0/1/0/all/0/1\">Inna W. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miner_A/0/1/0/all/0/1\">Adam S. Miner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atkins_D/0/1/0/all/0/1\">David C. Atkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_T/0/1/0/all/0/1\">Tim Althoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Separate What You Describe: Language-Queried Audio Source Separation. (arXiv:2203.15147v1 [eess.AS])","link":"http://arxiv.org/abs/2203.15147","description":"<p>In this paper, we introduce the task of language-queried audio source\nseparation (LASS), which aims to separate a target source from an audio mixture\nbased on a natural language query of the target source (e.g., \"a man tells a\njoke followed by people laughing\"). A unique challenge in LASS is associated\nwith the complexity of natural language description and its relation with the\naudio sources. To address this issue, we proposed LASS-Net, an end-to-end\nneural network that is learned to jointly process acoustic and linguistic\ninformation, and separate the target source that is consistent with the\nlanguage query from an audio mixture. We evaluate the performance of our\nproposed system with a dataset created from the AudioCaps dataset. Experimental\nresults show that LASS-Net achieves considerable improvements over baseline\nmethods. Furthermore, we observe that LASS-Net achieves promising\ngeneralization results when using diverse human-annotated descriptions as\nqueries, indicating its potential use in real-world scenarios. The separated\naudio samples and source code are available at\nhttps://liuxubo717.github.io/LASS-demopage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xubo Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1\">Haohe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kong_Q/0/1/0/all/0/1\">Qiuqiang Kong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_X/0/1/0/all/0/1\">Xinhao Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_J/0/1/0/all/0/1\">Jinzheng Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Q/0/1/0/all/0/1\">Qiushi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Plumbley_M/0/1/0/all/0/1\">Mark D. Plumbley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1\">Wenwu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation Dataset for Legal Word Embedding: A Case Study On Chinese Codex. (arXiv:2203.15173v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15173","description":"<p>Word embedding is a modern distributed word representations approach widely\nused in many natural language processing tasks. Converting the vocabulary in a\nlegal document into a word embedding model facilitates subjecting legal\ndocuments to machine learning, deep learning, and other algorithms and\nsubsequently performing the downstream tasks of natural language processing\nvis-\\`a-vis, for instance, document classification, contract review, and\nmachine translation. The most common and practical approach of accuracy\nevaluation with the word embedding model uses a benchmark set with linguistic\nrules or the relationship between words to perform analogy reasoning via\nalgebraic calculation. This paper proposes establishing a 1,134 Legal\nAnalogical Reasoning Questions Set (LARQS) from the 2,388 Chinese Codex corpus\nusing five kinds of legal relations, which are then used to evaluate the\naccuracy of the Chinese word embedding model. Moreover, we discovered that\nlegal relations might be ubiquitous in the word embedding model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chun-Hsien Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1\">Pu-Jen Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Generalization of Deep Neural Network Acoustic Models with Length Perturbation and N-best Based Label Smoothing. (arXiv:2203.15176v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15176","description":"<p>We introduce two techniques, length perturbation and n-best based label\nsmoothing, to improve generalization of deep neural network (DNN) acoustic\nmodels for automatic speech recognition (ASR). Length perturbation is a data\naugmentation algorithm that randomly drops and inserts frames of an utterance\nto alter the length of the speech feature sequence. N-best based label\nsmoothing randomly injects noise to ground truth labels during training in\norder to avoid overfitting, where the noisy labels are generated from n-best\nhypotheses. We evaluate these two techniques extensively on the 300-hour\nSwitchboard (SWB300) dataset and an in-house 500-hour Japanese (JPN500) dataset\nusing recurrent neural network transducer (RNNT) acoustic models for ASR. We\nshow that both techniques improve the generalization of RNNT models\nindividually and they can also be complementary. In particular, they yield good\nimprovements over a strong SWB300 baseline and give state-of-art performance on\nSWB300 using RNNT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaodong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagano_T/0/1/0/all/0/1\">Tohru Nagano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_M/0/1/0/all/0/1\">Masayuki Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuda_T/0/1/0/all/0/1\">Takashi Fukuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurata_G/0/1/0/all/0/1\">Gakuto Kurata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visualizations of Complex Sequences of Family-Infant Vocalizations Using Bag-of-Audio-Words Approach Based on Wav2vec 2.0 Features. (arXiv:2203.15183v1 [eess.AS])","link":"http://arxiv.org/abs/2203.15183","description":"<p>In the U.S., approximately 15-17% of children 2-8 years of age are estimated\nto have at least one diagnosed mental, behavioral or developmental disorder.\nHowever, such disorders often go undiagnosed, and the ability to evaluate and\ntreat disorders in the first years of life is limited. To analyze infant\ndevelopmental changes, previous studies have shown advanced ML models excel at\nclassifying infant and/or parent vocalizations collected using cell phone,\nvideo, or audio-only recording device like LENA. In this study, we pilot test\nthe audio component of a new infant wearable multi-modal device that we have\ndeveloped called LittleBeats (LB). LB audio pipeline is advanced in that it\nprovides reliable labels for both speaker diarization and vocalization\nclassification tasks, compared with other platforms that only record audio\nand/or provide speaker diarization labels. We leverage wav2vec 2.0 to obtain\nsuperior and more nuanced results with the LB family audio stream. We use a\nbag-of-audio-words method with wav2vec 2.0 features to create high-level\nvisualizations to understand family-infant vocalization interactions. We\ndemonstrate that our high-quality visualizations capture major types of family\nvocalization interactions, in categories indicative of mental, behavioral, and\ndevelopmental health, for both labeled and unlabeled LB audio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jialu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McElwain_N/0/1/0/all/0/1\">Nancy L. McElwain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifted Chunk Encoder for Transformer Based Streaming End-to-End ASR. (arXiv:2203.15206v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15206","description":"<p>Currently, there are mainly three Transformer encoder based streaming End to\nEnd (E2E) Automatic Speech Recognition (ASR) approaches, namely time-restricted\nmethods, chunk-wise methods, and memory based methods. However, all of them\nhave some limitations in aspects of global context modeling, linear\ncomputational complexity, and model parallelism. In this work, we aim to build\na single model to achieve the benefits of all the three aspects for streaming\nE2E ASR. Particularly, we propose to use a shifted chunk mechanism instead of\nthe conventional chunk mechanism for streaming Transformer and Conformer. This\nshifted chunk mechanism can significantly enhance modeling power through\nallowing chunk self-attention to capture global context across local chunks,\nwhile keeping linear computational complexity and parallel trainable. We name\nthe Shifted Chunk Transformer and Conformer as SChunk-Transofromer and\nSChunk-Conformer, respectively. And we verify their performance on the widely\nused AISHELL-1 benckmark. Experiments show that the SChunk-Transformer and\nSChunk-Conformer achieve CER 6.43% and 5.77%, respectively. That surpasses the\nexisting chunk-wise and memory based methods by a large margin, and is\ncompetitive even compared with the state-of-the-art time-restricted methods\nwhich have quadratic computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fangyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bo Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of EEG frequency bands for Envisioned Speech Recognition. (arXiv:2203.15250v1 [eess.SP])","link":"http://arxiv.org/abs/2203.15250","description":"<p>The use of Automatic speech recognition (ASR) interfaces have become\nincreasingly popular in daily life for use in interaction and control of\nelectronic devices. The interfaces currently being used are not feasible for a\nvariety of users such as those suffering from a speech disorder, locked-in\nsyndrome, paralysis or people with utmost privacy requirements. In such cases,\nan interface that can identify envisioned speech using electroencephalogram\n(EEG) signals can be of great benefit. Various works targeting this problem\nhave been done in the past. However, there has been limited work in identifying\nthe frequency bands ($\\delta, \\theta, \\alpha, \\beta, \\gamma$) of the EEG signal\nthat contribute towards envisioned speech recognition. Therefore, in this work,\nwe aim to analyze the significance of different EEG frequency bands and signals\nobtained from different lobes of the brain and their contribution towards\nrecognizing envisioned speech. Signals obtained from different lobes and\nbandpass filtered for different frequency bands are fed to a spatio-temporal\ndeep learning architecture with Convolutional Neural Network (CNN) and Long\nShort-Term Memory (LSTM). The performance is evaluated on a publicly available\ndataset comprising of three classification tasks - digit, character and images.\nWe obtain a classification accuracy of $85.93\\%$, $87.27\\%$ and $87.51\\%$ for\nthe three tasks respectively. The code for the implementation has been made\navailable at https://github.com/ayushayt/ImaginedSpeechRecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tripathi_A/0/1/0/all/0/1\">Ayush Tripathi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Applying Syntax$\\unicode{x2013}$Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis. (arXiv:2203.15276v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15276","description":"<p>End-to-end text-to-speech synthesis (TTS), which generates speech sounds\ndirectly from strings of texts or phonemes, has improved the quality of speech\nsynthesis over the conventional TTS. However, most previous studies have been\nevaluated based on subjective naturalness and have not objectively examined\nwhether they can reproduce pitch patterns of phonological phenomena such as\ndownstep, rhythmic boost, and initial lowering that reflect syntactic\nstructures in Japanese. These phenomena can be linguistically explained by\nphonological constraints and the syntax$\\unicode{x2013}$prosody mapping\nhypothesis (SPMH), which assumes projections from syntactic structures to\nphonological hierarchy. Although some experiments in psycholinguistics have\nverified the validity of the SPMH, it is crucial to investigate whether it can\nbe implemented in TTS. To synthesize linguistic phenomena involving syntactic\nor phonological constraints, we propose a model using phonological symbols\nbased on the SPMH and prosodic well-formedness constraints. Experimental\nresults showed that the proposed method synthesized similar pitch patterns to\nthose reported in linguistics experiments for the phenomena of initial lowering\nand rhythmic boost. The proposed model efficiently synthesizes phonological\nphenomena in the test data that were not explicitly included in the training\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Furukawa_K/0/1/0/all/0/1\">Kei Furukawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kishiyama_T/0/1/0/all/0/1\">Takeshi Kishiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can NMT Understand Me? Towards Perturbation-based Evaluation of NMT Models for Code Generation. (arXiv:2203.15319v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15319","description":"<p>Neural Machine Translation (NMT) has reached a level of maturity to be\nrecognized as the premier method for the translation between different\nlanguages and aroused interest in different research areas, including software\nengineering. A key step to validate the robustness of the NMT models consists\nin evaluating the performance of the models on adversarial inputs, i.e., inputs\nobtained from the original ones by adding small amounts of perturbation.\nHowever, when dealing with the specific task of the code generation (i.e., the\ngeneration of code starting from a description in natural language), it has not\nyet been defined an approach to validate the robustness of the NMT models. In\nthis work, we address the problem by identifying a set of perturbations and\nmetrics tailored for the robustness assessment of such models. We present a\npreliminary experimental evaluation, showing what type of perturbations affect\nthe model the most and deriving useful insights for future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1\">Pietro Liguori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Improta_C/0/1/0/all/0/1\">Cristina Improta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivo_S/0/1/0/all/0/1\">Simona De Vivo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1\">Roberto Natella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cukic_B/0/1/0/all/0/1\">Bojan Cukic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1\">Domenico Cotroneo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Noise-robust Speech Recognition with 10 Minutes Unparalleled In-domain Data. (arXiv:2203.15321v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15321","description":"<p>Noise-robust speech recognition systems require large amounts of training\ndata including noisy speech data and corresponding transcripts to achieve\nstate-of-the-art performances in face of various practical environments.\nHowever, such plenty of in-domain data is not always available in the real-life\nworld. In this paper, we propose a generative adversarial network to simulate\nnoisy spectrum from the clean spectrum (Simu-GAN), where only 10 minutes of\nunparalleled in-domain noisy speech data is required as labels. Furthermore, we\nalso propose a dual-path speech recognition system to improve the robustness of\nthe system under noisy conditions. Experimental results show that the proposed\nspeech recognition system achieves 7.3% absolute improvement with simulated\nnoisy data by Simu-GAN over the best baseline in terms of word error rate\n(WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shirol_S/0/1/0/all/0/1\">Shashank Shirol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Persian Relation Extraction Models by Data Augmentation. (arXiv:2203.15323v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15323","description":"<p>Relation extraction that is the task of predicting semantic relation type\nbetween entities in a sentence or document is an important task in natural\nlanguage processing. Although there are many researches and datasets for\nEnglish, Persian suffers from sufficient researches and comprehensive datasets.\nThe only available Persian dataset for this task is PERLEX, which is a Persian\nexpert-translated version of the SemEval-2010-Task-8 dataset. In this paper, we\npresent our augmented dataset and the results and findings of our system,\nparticipated in the Persian relation Extraction shared task of NSURL 2021\nworkshop. We use PERLEX as the base dataset and enhance it by applying some\ntext preprocessing steps and by increasing its size via data augmentation\ntechniques to improve the generalization and robustness of applied models. We\nthen employ two different models including ParsBERT and multilingual BERT for\nrelation extraction on the augmented PERLEX dataset. Our best model obtained\n64.67% of Macro-F1 on the test phase of the contest and it achieved 83.68% of\nMacro-F1 on the test set of PERLEX.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sartakhti_M/0/1/0/all/0/1\">Moein Salimi Sartakhti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etezadi_R/0/1/0/all/0/1\">Romina Etezadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LDKP: A Dataset for Identifying Keyphrases from Long Scientific Documents. (arXiv:2203.15349v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15349","description":"<p>Identifying keyphrases (KPs) from text documents is a fundamental task in\nnatural language processing and information retrieval. Vast majority of the\nbenchmark datasets for this task are from the scientific domain containing only\nthe document title and abstract information. This limits keyphrase extraction\n(KPE) and keyphrase generation (KPG) algorithms to identify keyphrases from\nhuman-written summaries that are often very short (approx 8 sentences). This\npresents three challenges for real-world applications: human-written summaries\nare unavailable for most documents, the documents are almost always long, and a\nhigh percentage of KPs are directly found beyond the limited context of title\nand abstract. Therefore, we release two extensive corpora mapping KPs of ~1.3M\nand ~100K scientific articles with their fully extracted text and additional\nmetadata including publication venue, year, author, field of study, and\ncitations for facilitating research on this real-world problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahata_D/0/1/0/all/0/1\">Debanjan Mahata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1\">Naveen Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Dibya Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Amardeep Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1\">Anish Acharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Inefficiency of Language Models in Scholarly Retrieval: An Experimental Walk-through. (arXiv:2203.15364v1 [cs.IR])","link":"http://arxiv.org/abs/2203.15364","description":"<p>Language models are increasingly becoming popular in AI-powered scientific IR\nsystems. This paper evaluates popular scientific language models in handling\n(i) short-query texts and (ii) textual neighbors. Our experiments showcase the\ninability to retrieve relevant documents for a short-query text even under the\nmost relaxed conditions. Additionally, we leverage textual neighbors, generated\nby small perturbations to the original text, to demonstrate that not all\nperturbations lead to close neighbors in the embedding space. Further, an\nexhaustive categorization yields several classes of orthographically and\nsemantically related, partially related, and completely unrelated neighbors.\nRetrieval performance turns out to be more influenced by the surface form\nrather than the semantics of the text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Shruti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mayank Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Short-Term Word-Learning in a Dynamically Changing Environment. (arXiv:2203.15404v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15404","description":"<p>Neural sequence-to-sequence automatic speech recognition (ASR) systems are in\nprinciple open vocabulary systems, when using appropriate modeling units. In\npractice, however, they often fail to recognize words not seen during training,\ne.g., named entities, numbers or technical terms. To alleviate this problem,\nHuber et al. proposed to supplement an end-to-end ASR system with a word/phrase\nmemory and a mechanism to access this memory to recognize the words and phrases\ncorrectly. In this paper we study, a) methods to acquire important words for\nthis memory dynamically and, b) the trade-off between improvement in\nrecognition accuracy of new words and the potential danger of false alarms for\nthose added words. We demonstrate significant improvements in the detection\nrate of new words with only a minor increase in false alarms (F1 score 0.30\n$\\rightarrow$ 0.80), when using an appropriate number of new words. In\naddition, we show that important keywords can be extracted from supporting\ndocuments and used effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huber_C/0/1/0/all/0/1\">Christian Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rishu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Waibel_A/0/1/0/all/0/1\">Alexander Waibel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality Assurance of Generative Dialog Models in an Evolving Conversational Agent Used for Swedish Language Practice. (arXiv:2203.15414v1 [cs.SE])","link":"http://arxiv.org/abs/2203.15414","description":"<p>Due to the migration megatrend, efficient and effective second-language\nacquisition is vital. One proposed solution involves AI-enabled conversational\nagents for person-centered interactive language practice. We present results\nfrom ongoing action research targeting quality assurance of proprietary\ngenerative dialog models trained for virtual job interviews. The action team\nelicited a set of 38 requirements for which we designed corresponding automated\ntest cases for 15 of particular interest to the evolving solution. Our results\nshow that six of the test case designs can detect meaningful differences\nbetween candidate models. While quality assurance of natural language\nprocessing applications is complex, we provide initial steps toward an\nautomated framework for machine learning model selection in the context of an\nevolving conversational agent. Future work will focus on model selection in an\nMLOps setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Borg_M/0/1/0/all/0/1\">Markus Borg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengtsson_J/0/1/0/all/0/1\">Johan Bengtsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osterling_H/0/1/0/all/0/1\">Harald &#xd6;sterling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagelborn_A/0/1/0/all/0/1\">Alexander Hagelborn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gagner_I/0/1/0/all/0/1\">Isabella Gagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomaszewski_P/0/1/0/all/0/1\">Piotr Tomaszewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic properties of English nominal pluralization: Insights from word embeddings. (arXiv:2203.15424v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15424","description":"<p>Semantic differentiation of nominal pluralization is grammaticalized in many\nlanguages. For example, plural markers may only be relevant for human nouns.\nEnglish does not appear to make such distinctions. Using distributional\nsemantics, we show that English nominal pluralization exhibits semantic\nclusters. For instance, pluralization of fruit words is more similar to one\nanother and less similar to pluralization of other semantic classes. Therefore,\nreduction of the meaning shift in plural formation to the addition of an\nabstract plural meaning is too simplistic. A semantically informed method,\ncalled CosClassAvg, is introduced that outperforms pluralization methods in\ndistributional semantics which assume plural formation amounts to the addition\nof a fixed plural vector. In comparison with our approach, a method from\ncompositional distributional semantics, called FRACSS, predicted plural vectors\nthat were more similar to the corpus-extracted plural vectors in terms of\ndirection but not vector length. A modeling study reveals that the observed\ndifference between the two predicted semantic spaces by CosClassAvg and FRACSS\ncarries over to how well a computational model of the listener can understand\npreviously unencountered plural forms. Mappings from word forms, represented\nwith triphone vectors, to predicted semantic vectors are more productive when\nCosClassAvg-generated semantic vectors are employed as gold standard vectors\ninstead of FRACSS-generated vectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shafaei_Bajestan_E/0/1/0/all/0/1\">Elnaz Shafaei-Bajestan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradipour_Tari_M/0/1/0/all/0/1\">Masoumeh Moradipour-Tari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhrig_P/0/1/0/all/0/1\">Peter Uhrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeNet 2.0: More Productive End-to-End Speech Recognition Toolkit. (arXiv:2203.15455v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15455","description":"<p>Recently, we made available WeNet, a production-oriented end-to-end speech\nrecognition toolkit, which introduces a unified two-pass (U2) framework and a\nbuilt-in runtime to address the streaming and non-streaming decoding modes in a\nsingle model. To further improve ASR performance and facilitate various\nproduction requirements, in this paper, we present WeNet 2.0 with four\nimportant updates. (1) We propose U2++, a unified two-pass framework with\nbidirectional attention decoders, which includes the future contextual\ninformation by a right-to-left attention decoder to improve the representative\nability of the shared encoder and the performance during the rescoring stage.\n(2) We introduce an n-gram based language model and a WFST-based decoder into\nWeNet 2.0, promoting the use of rich text data in production scenarios. (3) We\ndesign a unified contextual biasing framework, which leverages user-specific\ncontext (e.g., contact lists) to provide rapid adaptation ability for\nproduction and improves ASR accuracy in both with-LM and without-LM scenarios.\n(4) We design a unified IO to support large-scale data for effective model\ntraining. In summary, the brand-new WeNet 2.0 achieves up to 10\\% relative\nrecognition performance improvement over the original WeNet on various corpora\nand makes available several important production-oriented features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xingchen Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1\">Hang Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1\">Fuping Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1\">Jianwei Niu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speech Segmentation Optimization using Segmented Bilingual Speech Corpus for End-to-end Speech Translation. (arXiv:2203.15479v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15479","description":"<p>Speech segmentation, which splits long speech into short segments, is\nessential for speech translation (ST). Popular VAD tools like WebRTC VAD have\ngenerally relied on pause-based segmentation. Unfortunately, pauses in speech\ndo not necessarily match sentence boundaries, and sentences can be connected by\na very short pause that is difficult to detect by VAD. In this study, we\npropose a speech segmentation method using a binary classification model\ntrained using a segmented bilingual speech corpus. We also propose a hybrid\nmethod that combines VAD and the above speech segmentation method. Experimental\nresults revealed that the proposed method is more suitable for cascade and\nend-to-end ST systems than conventional segmentation methods. The hybrid\napproach further improved the translation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fukuda_R/0/1/0/all/0/1\">Ryo Fukuda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sudoh_K/0/1/0/all/0/1\">Katsuhito Sudoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Representing `how you say' with `what you say': English corpus of focused speech and text reflecting corresponding implications. (arXiv:2203.15483v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15483","description":"<p>In speech communication, how something is said (paralinguistic information)\nis as crucial as what is said (linguistic information). As a type of\nparalinguistic information, English speech uses sentence stress, the heaviest\nprominence within a sentence, to convey emphasis. While different placements of\nsentence stress communicate different emphatic implications, current speech\ntranslation systems return the same translations if the utterances are\nlinguistically identical, losing paralinguistic information. Concentrating on\nfocus, a type of emphasis, we propose mapping paralinguistic information into\nthe linguistic domain within the source language using lexical and grammatical\ndevices. This method enables us to translate the paraphrased text\nrepresentations instead of the transcription of the original speech and obtain\ntranslations that preserve paralinguistic information. As a first step, we\npresent the collection of an English corpus containing speech that differed in\nthe placement of focus along with the corresponding text, which was designed to\nreflect the implied meaning of the speech. Also, analyses of our corpus\ndemonstrated that mapping of focus from the paralinguistic domain into the\nlinguistic domain involved various lexical and grammatical methods. The data\nand insights from our analysis will further advance research into\nparalinguistic translation. The corpus will be published via LDC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_N/0/1/0/all/0/1\">Naoaki Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_S/0/1/0/all/0/1\">Satoshi Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Audio-text Representation for Automated Audio Captioning with Contrastive Learning. (arXiv:2203.15526v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15526","description":"<p>Automated Audio captioning (AAC) is a cross-modal task that generates natural\nlanguage to describe the content of input audio. Most prior works usually\nextract single-modality acoustic features and are therefore sub-optimal for the\ncross-modal decoding task. In this work, we propose a novel AAC system called\nCLIP-AAC to learn interactive cross-modality representation with both acoustic\nand textual information. Specifically, the proposed CLIP-AAC introduces an\naudio-head and a text-head in the pre-trained encoder to extract audio-text\ninformation. Furthermore, we also apply contrastive learning to narrow the\ndomain difference by learning the correspondence between the audio signal and\nits paired captions. Experimental results show that the proposed CLIP-AAC\napproach surpasses the best baseline by a significant margin on the Clotho\ndataset in terms of NLP evaluation metrics. The ablation study indicates that\nboth the pre-trained model and contrastive learning contribute to the\nperformance gain of the AAC model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_N/0/1/0/all/0/1\">Nana Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Heqing Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xiaofeng Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1\">Eng Siong Chng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Compute-Optimal Large Language Models. (arXiv:2203.15556v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15556","description":"<p>We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over \\nummodels language models ranging from 70\nmillion to over 16 billion parameters on 5 to 500 billion tokens, we find that\nfor compute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, \\chinchilla, that uses the same compute budget\nas \\gopher but with 70B parameters and 4$\\times$ more more data. \\chinchilla\nuniformly and significantly outperforms \\Gopher (280B), GPT-3 (175B),\nJurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of\ndownstream evaluation tasks. This also means that \\chinchilla uses\nsubstantially less compute for fine-tuning and inference, greatly facilitating\ndownstream usage. As a highlight, \\chinchilla reaches a state-of-the-art\naverage accuracy of 67.5\\% on the MMLU benchmark, greater than a 7\\%\nimprovement over \\gopher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchatskaya_E/0/1/0/all/0/1\">Elena Buchatskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casas_D/0/1/0/all/0/1\">Diego de Las Casas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_A/0/1/0/all/0/1\">Aidan Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noland_E/0/1/0/all/0/1\">Eric Noland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damoc_B/0/1/0/all/0/1\">Bogdan Damoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_A/0/1/0/all/0/1\">Aurelia Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1\">Simon Osindero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack W. Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Speech Emotion Recognition in Greek Theatrical Plays. (arXiv:2203.15568v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15568","description":"<p>Machine learning methodologies can be adopted in cultural applications and\npropose new ways to distribute or even present the cultural content to the\npublic. For instance, speech analytics can be adopted to automatically generate\nsubtitles in theatrical plays, in order to (among other purposes) help people\nwith hearing loss. Apart from a typical speech-to-text transcription with\nAutomatic Speech Recognition (ASR), Speech Emotion Recognition (SER) can be\nused to automatically predict the underlying emotional content of speech\ndialogues in theatrical plays, and thus to provide a deeper understanding how\nthe actors utter their lines. However, real-world datasets from theatrical\nplays are not available in the literature. In this work we present GreThE, the\nGreek Theatrical Emotion dataset, a new publicly available data collection for\nspeech emotion recognition in Greek theatrical plays. The dataset contains\nutterances from various actors and plays, along with respective valence and\narousal annotations. Towards this end, multiple annotators have been asked to\nprovide their input for each speech recording and inter-annotator agreement is\ntaken into account in the final ground truth generation. In addition, we\ndiscuss the results of some indicative experiments that have been conducted\nwith machine and deep learning frameworks, using the dataset, along with some\nwidely used databases in the field of speech emotion recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moutti_M/0/1/0/all/0/1\">Maria Moutti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eleftheriou_S/0/1/0/all/0/1\">Sofia Eleftheriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koromilas_P/0/1/0/all/0/1\">Panagiotis Koromilas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giannakopoulos_T/0/1/0/all/0/1\">Theodoros Giannakopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subspace-based Representation and Learning for Phonotactic Spoken Language Recognition. (arXiv:2203.15576v1 [cs.SD])","link":"http://arxiv.org/abs/2203.15576","description":"<p>Phonotactic constraints can be employed to distinguish languages by\nrepresenting a speech utterance as a multinomial distribution or phone events.\nIn the present study, we propose a new learning mechanism based on\nsubspace-based representation, which can extract concealed phonotactic\nstructures from utterances, for language verification and dialect/accent\nidentification. The framework mainly involves two successive parts. The first\npart involves subspace construction. Specifically, it decodes each utterance\ninto a sequence of vectors filled with phone-posteriors and transforms the\nvector sequence into a linear orthogonal subspace based on low-rank matrix\nfactorization or dynamic linear modeling. The second part involves subspace\nlearning based on kernel machines, such as support vector machines and the\nnewly developed subspace-based neural networks (SNNs). The input layer of SNNs\nis specifically designed for the sample represented by subspaces. The topology\nensures that the same output can be derived from identical subspaces by\nmodifying the conventional feed-forward pass to fit the mathematical definition\nof subspace similarity. Evaluated on the \"General LR\" test of NIST LRE 2007,\nthe proposed method achieved up to 52%, 46%, 56%, and 27% relative reductions\nin equal error rates over the sequence-based PPR-LM, PPR-VSM, and PPR-IVEC\nmethods and the lattice-based PPR-LM method, respectively. Furthermore, on the\ndialect/accent identification task of NIST LRE 2009, the SNN-based system\nperformed better than the aforementioned four baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeng_S/0/1/0/all/0/1\">Shyh-Kang Jeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heuristic-based Inter-training to Improve Few-shot Multi-perspective Dialog Summarization. (arXiv:2203.15590v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15590","description":"<p>Many organizations require their customer-care agents to manually summarize\ntheir conversations with customers. These summaries are vital for decision\nmaking purposes of the organizations. The perspective of the summary that is\nrequired to be created depends on the application of the summaries. With this\nwork, we study the multi-perspective summarization of customer-care\nconversations between support agents and customers. We observe that there are\ndifferent heuristics that are associated with summaries of different\nperspectives, and explore these heuristics to create weak-labeled data for\nintermediate training of the models before fine-tuning with scarce human\nannotated summaries. Most importantly, we show that our approach supports\nmodels to generate multi-perspective summaries with a very small amount of\nannotated data. For example, our approach achieves 94\\% of the performance\n(Rouge-2) of a model trained with the original data, by training only with 7\\%\nof the original data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sznajder_B/0/1/0/all/0/1\">Benjamin Sznajder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekara_C/0/1/0/all/0/1\">Chulaka Gunasekara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lev_G/0/1/0/all/0/1\">Guy Lev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1\">Sachin Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shnarch_E/0/1/0/all/0/1\">Eyal Shnarch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Earnings-22: A Practical Benchmark for Accents in the Wild. (arXiv:2203.15591v1 [cs.CL])","link":"http://arxiv.org/abs/2203.15591","description":"<p>Modern automatic speech recognition (ASR) systems have achieved superhuman\nWord Error Rate (WER) on many common corpora despite lacking adequate\nperformance on speech in the wild. Beyond that, there is a lack of real-world,\naccented corpora to properly benchmark academic and commercial models. To\nensure this type of speech is represented in ASR benchmarking, we present\nEarnings-22, a 125 file, 119 hour corpus of English-language earnings calls\ngathered from global companies. We run a comparison across 4 commercial models\nshowing the variation in performance when taking country of origin into\nconsideration. Looking at hypothesis transcriptions, we explore errors common\nto all ASR systems tested. By examining Individual Word Error Rate (IWER), we\nfind that key speech features impact model performance more for certain accents\nthan others. Earnings-22 provides a free-to-use benchmark of real-world,\naccented audio to bridge academic and industrial research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rio_M/0/1/0/all/0/1\">Miguel Del Rio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_P/0/1/0/all/0/1\">Peter Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McNamara_Q/0/1/0/all/0/1\">Quinten McNamara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_C/0/1/0/all/0/1\">Corey Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1\">Shipra Chandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving Selective Prediction Ability of NLP Systems. (arXiv:2008.09371v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.09371","description":"<p>It's better to say \"I can't answer\" than to answer incorrectly. This\nselective prediction ability is crucial for NLP systems to be reliably deployed\nin real-world applications. Prior work has shown that existing selective\nprediction techniques fail to perform well, especially in the out-of-domain\nsetting. In this work, we propose a method that improves probability estimates\nof models by calibrating them using prediction confidence and difficulty score\nof instances. Using these two signals, we first annotate held-out instances and\nthen train a calibrator to predict the likelihood of correctness of the model's\nprediction. We instantiate our method with Natural Language Inference (NLI) and\nDuplicate Detection (DD) tasks and evaluate it in both In-Domain (IID) and\nOut-of-Domain (OOD) settings. In (IID, OOD) settings, we show that the\nrepresentations learned by our calibrator result in an improvement of (15.81%,\n5.64%) and (6.19%, 13.9%) over MaxProb -- a selective prediction baseline -- on\nNLI and DD tasks respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10407","description":"<p>The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Transformers. (arXiv:2103.01209v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01209","description":"<p>We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Coreference Resolution Models through Active Learning. (arXiv:2104.07611v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07611","description":"<p>Neural coreference resolution models trained on one dataset may not transfer\nto new, low-resource domains. Active learning mitigates this problem by\nsampling a small subset of data for annotators to label. While active learning\nis well-defined for classification tasks, its application to coreference\nresolution is neither well-defined nor fully understood. This paper explores\nhow to actively label coreference, examining sources of model uncertainty and\ndocument reading costs. We compare uncertainty sampling strategies and their\nadvantages through thorough error analysis. In both synthetic and human\nexperiments, labeling spans within the same document is more effective than\nannotating spans across documents. The findings contribute to a more realistic\ndevelopment of coreference resolution models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1\">Michelle Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_P/0/1/0/all/0/1\">Patrick Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_C/0/1/0/all/0/1\">Chandler May</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Greedy-layer Pruning: Speeding up Transformer Models for Natural Language Processing. (arXiv:2105.14839v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.14839","description":"<p>Fine-tuning transformer models after unsupervised pre-training reaches a very\nhigh performance on many different natural language processing tasks.\nUnfortunately, transformers suffer from long inference times which greatly\nincreases costs in production. One possible solution is to use knowledge\ndistillation, which solves this problem by transferring information from large\nteacher models to smaller student models. Knowledge distillation maintains high\nperformance and reaches high compression rates, nevertheless, the size of the\nstudent model is fixed after pre-training and can not be changed individually\nfor a given downstream task and use-case to reach a desired performance/speedup\nratio. Another solution to reduce the size of models in a much more\nfine-grained and computationally cheaper fashion is to prune layers after the\npre-training. The price to pay is that the performance of layer-wise pruning\nalgorithms is not on par with state-of-the-art knowledge distillation methods.\nIn this paper, Greedy-layer pruning is introduced to (1) outperform current\nstate-of-the-art for layer-wise pruning, (2) close the performance gap when\ncompared to knowledge distillation, while (3) providing a method to adapt the\nmodel size dynamically to reach a desired performance/speedup tradeoff without\nthe need of additional pre-training phases. Our source code is available on\nhttps://github.com/deepopinion/greedy-layer-pruning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peer_D/0/1/0/all/0/1\">David Peer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stabinger_S/0/1/0/all/0/1\">Sebastian Stabinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engl_S/0/1/0/all/0/1\">Stefan Engl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Sanchez_A/0/1/0/all/0/1\">Antonio Rodriguez-Sanchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Emphasis control for parallel neural TTS. (arXiv:2110.03012v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.03012","description":"<p>Recent parallel neural text-to-speech (TTS) synthesis methods are able to\ngenerate speech with high fidelity while maintaining high performance. However,\nthese systems often lack control over the output prosody, thus restricting the\nsemantic information conveyable for a given text. This paper proposes a\nhierarchical parallel neural TTS system for prosodic emphasis control by\nlearning a latent space that directly corresponds to a change in emphasis.\nThree candidate features for the latent space are compared: 1) Variance of\npitch and duration within words in a sentence, 2) Wavelet-based feature\ncomputed from pitch, energy, and duration, and 3) Learned combination of the\ntwo aforementioned approaches. At inference time, word-level prosodic emphasis\nis achieved by increasing the feature values of the latent space for the given\nwords. Experiments show that all the proposed methods are able to achieve the\nperception of increased emphasis with little loss in overall quality. Moreover,\nemphasized utterances were preferred in a pairwise comparison test over the\nnon-emphasized utterances, indicating promise for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Seshadri_S/0/1/0/all/0/1\">Shreyas Seshadri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raitio_T/0/1/0/all/0/1\">Tuomo Raitio</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Castellani_D/0/1/0/all/0/1\">Dan Castellani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangchuan Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangled dimensionality reduction for noise-robust speaker diarisation. (arXiv:2110.03380v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.03380","description":"<p>The objective of this work is to train noise-robust speaker embeddings\nadapted for speaker diarisation. Speaker embeddings play a crucial role in the\nperformance of diarisation systems, but they often capture spurious information\nsuch as noise and reverberation, adversely affecting performance. Our previous\nwork has proposed an auto-encoder-based dimensionality reduction module to help\nremove the redundant information. However, they do not explicitly separate such\ninformation and have also been found to be sensitive to hyper-parameter values.\nTo this end, we propose two contributions to overcome these issues: (i) a novel\ndimensionality reduction framework that can disentangle spurious information\nfrom the speaker embeddings; (ii) the use of a speech/non-speech indicator to\nprevent the speaker code from representing the background noise. Through a\nrange of experiments conducted on four different datasets, our approach\nconsistently demonstrates the state-of-the-art performance among models without\nsystem fusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">You Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_H/0/1/0/all/0/1\">Hee-Soo Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jee-weon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Youngki Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bong-Jin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Joon Son Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDERank: A Masked Document Embedding Rank Approach for Unsupervised Keyphrase Extraction. (arXiv:2110.06651v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06651","description":"<p>Keyphrase extraction (KPE) automatically extracts phrases in a document that\nprovide a concise summary of the core content, which benefits downstream\ninformation retrieval and NLP tasks. Previous state-of-the-art (SOTA) methods\nselect candidate keyphrases based on the similarity between learned\nrepresentations of the candidates and the document. They suffer performance\ndegradation on long documents due to discrepancy between sequence lengths which\ncauses mismatch between representations of keyphrase candidates and the\ndocument. In this work, we propose a novel unsupervised embedding-based KPE\napproach, Masked Document Embedding Rank (MDERank), to address this problem by\nleveraging a mask strategy and ranking candidates by the similarity between\nembeddings of the source document and the masked document. We further develop a\nKPE-oriented BERT (KPEBERT) model by proposing a novel self-supervised\ncontrastive learning method, which is more compatible to MDERank than vanilla\nBERT. Comprehensive evaluations on six KPE benchmarks demonstrate that the\nproposed MDERank outperforms state-of-the-art unsupervised KPE approach by\naverage 1.80 $F1@15$ improvement. MDERank further benefits from KPEBERT and\noverall achieves average 3.53 $F1@15$ improvement over the SOTA SIFRank. Our\ncode is available at \\url{https://github.com/LinhanZ/mderank}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Linhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xin Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Start to Finish: Latency Reduction Strategies for Incremental Speech Synthesis in Simultaneous Speech-to-Speech Translation. (arXiv:2110.08214v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08214","description":"<p>Speech-to-speech translation (S2ST) converts input speech to speech in\nanother language. A challenge of delivering S2ST in real time is the\naccumulated delay between the translation and speech synthesis modules. While\nrecently incremental text-to-speech (iTTS) models have shown large quality\nimprovements, they typically require additional future text inputs to reach\noptimal performance. In this work, we minimize the initial waiting time of iTTS\nby adapting the upstream speech translator to generate high-quality pseudo\nlookahead for the speech synthesizer. After mitigating the initial delay, we\ndemonstrate that the duration of synthesized speech also plays a crucial role\non latency. We formalize this as a latency metric and then present a simple yet\neffective duration-scaling approach for latency reduction. Our approaches\nconsistently reduce latency by 0.2-0.5 second without sacrificing speech\ntranslation quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1\">Hongyu Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xutai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1\">Juan Pino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Case-based Reasoning for Better Generalization in Textual Reinforcement Learning. (arXiv:2110.08470v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08470","description":"<p>Text-based games (TBG) have emerged as promising environments for driving\nresearch in grounded language understanding and studying problems like\ngeneralization and sample efficiency. Several deep reinforcement learning (RL)\nmethods with varying architectures and learning schemes have been proposed for\nTBGs. However, these methods fail to generalize efficiently, especially under\ndistributional shifts. In a departure from deep RL approaches, in this paper,\nwe propose a general method inspired by case-based reasoning to train agents\nand generalize out of the training distribution. The case-based reasoner\ncollects instances of positive experiences from the agent's interaction with\nthe world in the past and later reuses the collected experiences to act\nefficiently. The method can be applied in conjunction with any existing\non-policy neural agent in the literature for TBGs. Our experiments show that\nthe proposed approach consistently improves existing methods, obtains good\nout-of-distribution generalization, and achieves new state-of-the-art results\non widely used environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Atzeni_M/0/1/0/all/0/1\">Mattia Atzeni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1\">Shehzaad Dhuliawala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murugesan_K/0/1/0/all/0/1\">Keerthiram Murugesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dialogue Response Generation. (arXiv:2110.08515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08515","description":"<p>Responsing with image has been recognized as an important capability for an\nintelligent conversational agent. Yet existing works only focus on exploring\nthe multimodal dialogue models which depend on retrieval-based methods, but\nneglecting generation methods. To fill in the gaps, we first present a\nmultimodal dialogue generation model, which takes the dialogue history as\ninput, then generates a textual sequence or an image as response. Learning such\na model often requires multimodal dialogues containing both texts and images\nwhich are difficult to obtain. Motivated by the challenge in practice, we\nconsider multimodal dialogue generation under a natural assumption that only\nlimited training examples are available. In such a low-resource setting, we\ndevise a novel conversational agent, Divter, in order to isolate parameters\nthat depend on multimodal dialogues from the entire generation model. By this\nmeans, the major part of the model can be learned from a large number of\ntext-only dialogues and text-image pairs respectively, then the whole\nparameters can be well fitted using the limited training examples. Extensive\nexperiments demonstrate our method achieves state-of-the-art results in both\nautomatic and human evaluation, and can generate informative text and\nhigh-resolution image responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jessica Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Private Language Model Adaptation for Speech Recognition. (arXiv:2110.10026v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.10026","description":"<p>Speech model adaptation is crucial to handle the discrepancy between\nserver-side proxy training data and actual data received on local devices of\nusers. With the use of federated learning (FL), we introduce an efficient\napproach on continuously adapting neural network language models (NNLMs) on\nprivate devices with applications on automatic speech recognition (ASR). To\naddress the potential speech transcription errors in the on-device training\ncorpus, we perform empirical studies on comparing various strategies of\nleveraging token-level confidence scores to improve the NNLM quality in the FL\nsettings. Experiments show that compared with no model adaptation, the proposed\nmethod achieves relative 2.6% and 10.8% word error rate (WER) reductions on two\nspeech evaluation datasets, respectively. We also provide analysis in\nevaluating privacy guarantees of our presented procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakshi_S/0/1/0/all/0/1\">Shreyan Bakshi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_F/0/1/0/all/0/1\">Fuchun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Captioner: Inducing Content-Style Separation in Vision-and-Language Model Training. (arXiv:2111.12727v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12727","description":"<p>While captioning models have obtained compelling results in describing\nnatural images, there is a growing effort to increase their capability of\ndealing with real-world concepts. In this paper, we address the task of\ngenerating fluent descriptions by training on a non-uniform combination of data\nsources, containing both human- and automatically-collected captions. To this\nend, we propose a model which induces a separation between content and\ndescriptive style through the incorporation of stylistic parameters and\nkeywords extracted from large-scale multi-modal models as pivotal data. In\nterms of visual features, our model avoids the need of object detectors and\nemploys grid-like features together with a single objective of prompt language\nmodeling. Experimentally, we consistently outperform existing methods in terms\nof caption quality and capability of describing out-of-domain concepts.\nFinally, our model obtains a new state of the art on both COCO and nocaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection. (arXiv:2111.14592v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.14592","description":"<p>Pre-trained models have proved to be powerful in enhancing task-oriented\ndialog systems. However, current pre-training methods mainly focus on enhancing\ndialog understanding and generation tasks while neglecting the exploitation of\ndialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog\nmodel that explicitly learns dialog policy from limited labeled dialogs and\nlarge-scale unlabeled dialog corpora via semi-supervised learning.\nSpecifically, we introduce a dialog act prediction task for policy optimization\nduring pre-training and employ a consistency regularization term to refine the\nlearned representation with the help of unlabeled dialogs. We also implement a\ngating mechanism to weigh suitable unlabeled dialog samples. Empirical results\nshow that GALAXY substantially improves the performance of task-oriented dialog\nsystems, and achieves new state-of-the-art results on benchmark datasets:\nIn-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores\nby 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a\nstronger few-shot ability than existing models under various low-resource\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dermot Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03109","description":"<p>How to learn a universal facial representation that boosts all face analysis\ntasks? This paper takes one step toward this goal. In this paper, we study the\ntransfer performance of pre-trained models on face analysis tasks and introduce\na framework, called FaRL, for general Facial Representation Learning in a\nvisual-linguistic manner. On one hand, the framework involves a contrastive\nloss to learn high-level semantic meaning from image-text pairs. On the other\nhand, we propose exploring low-level information simultaneously to further\nenhance the face representation, by adding a masked image modeling. We perform\npre-training on LAION-FACE, a dataset containing large amount of face\nimage-text pairs, and evaluate the representation capability on multiple\ndownstream tasks. We show that FaRL achieves better transfer performance\ncompared with previous pre-trained models. We also verify its superiority in\nthe low-data regime. More importantly, our model surpasses the state-of-the-art\nmethods on face analysis tasks including face parsing and face alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PM-MMUT: Boosted Phone-mask Data Augmentation using Multi-Modeling Unit Training for Phonetic-Reduction-Robust E2E Speech Recognition. (arXiv:2112.06721v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.06721","description":"<p>Consonant and vowel reduction are often encountered in speech, which might\ncause performance degradation in automatic speech recognition (ASR). Our\nrecently proposed learning strategy based on masking, Phone Masking Training\n(PMT), alleviates the impact of such phenomenon in Uyghur ASR. Although PMT\nachieves remarkably improvements, there still exists room for further gains due\nto the granularity mismatch between the masking unit of PMT (phoneme) and the\nmodeling unit (word-piece). To boost the performance of PMT, we propose\nmulti-modeling unit training (MMUT) architecture fusion with PMT (PM-MMUT). The\nidea of MMUT framework is to split the Encoder into two parts including\nacoustic feature sequences to phoneme-level representation (AF-to-PLR) and\nphoneme-level representation to word-piece-level representation (PLR-to-WPLR).\nIt allows AF-to-PLR to be optimized by an intermediate phoneme-based CTC loss\nto learn the rich phoneme-level context information brought by PMT.\nExperimental results on Uyghur ASR show that the proposed approaches outperform\nobviously the pure PMT. We also conduct experiments on the 960-hour Librispeech\nbenchmark using ESPnet1, which achieves about 10% relative WER reduction on all\nthe test set without LM fusion comparing with the latest official ESPnet1\npre-trained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1\">Guodong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Pengfei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yolwas_N/0/1/0/all/0/1\">Nurmemet Yolwas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Prompts: Towards memory and compute efficient domain adaptation of ASR systems. (arXiv:2112.08718v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08718","description":"<p>Automatic Speech Recognition (ASR) systems have found their use in numerous\nindustrial applications in very diverse domains creating a need to adapt to new\ndomains with small memory and deployment overhead. In this work, we introduce\ndomain prompts, a methodology that involves training a small number of domain\nembedding parameters to prime a Transformer-based Language Model (LM) to a\nparticular domain. Using this domain-adapted LM for rescoring ASR hypotheses\ncan achieve 7-13% WER reduction for a new domain with just 1000 unlabeled\ntextual domain-specific sentences and a handful of additional parameters. Our\nmethod can match or even beat the performance of models fully fine-tuned\ntowards a particular domain with only 0.02% of the parameters. Given the\nparameter efficiency and negligible deployment overhead, our experiments\nshowcase that such a method is an ideal choice for on-the-fly adaptation of LMs\nused in ASR systems to progressively scale it to new domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. (arXiv:2202.01279v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2202.01279","description":"<p>PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bach_S/0/1/0/all/0/1\">Stephen H. Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zheng-Xin Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1\">Albert Webson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nayak_N/0/1/0/all/0/1\">Nihal V. Nayak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_M/0/1/0/all/0/1\">M Saiful Bari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alyafeai_Z/0/1/0/all/0/1\">Zaid Alyafeai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dey_M/0/1/0/all/0/1\">Manan Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santilli_A/0/1/0/all/0/1\">Andrea Santilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhiqing Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1\">Srulik Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fries_J/0/1/0/all/0/1\">Jason Alan Fries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_shaibani_M/0/1/0/all/0/1\">Maged S. Al-shaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shanya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Almubarak_K/0/1/0/all/0/1\">Khalid Almubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mike Tian-Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Discrimination to Generation: Knowledge Graph Completion with Generative Transformer. (arXiv:2202.02113v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02113","description":"<p>Knowledge graph completion aims to address the problem of extending a KG with\nmissing triples. In this paper, we provide an approach GenKGC, which converts\nknowledge graph completion to sequence-to-sequence generation task with the\npre-trained language model. We further introduce relation-guided demonstration\nand entity-aware hierarchical decoding for better representation learning and\nfast inference. Experimental results on three datasets show that our approach\ncan obtain better or comparable performance than baselines and achieve faster\ninference speed compared with previous methods with pre-trained language\nmodels. We also release a new large-scale Chinese knowledge graph dataset\nAliopenKG500 for research purpose. Code and datasets are available in\nhttps://github.com/zjunlp/PromptKG/tree/main/GenKGC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhoubo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1\">Feiyu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mosha Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. (arXiv:2202.04774v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.04774","description":"<p>Speech translation models are unable to directly process long audios, like\nTED talks, which have to be split into shorter segments. Speech translation\ndatasets provide manual segmentations of the audios, which are not available in\nreal-world scenarios, and existing segmentation methods usually significantly\nreduce translation quality at inference time. To bridge the gap between the\nmanual segmentation of training and the automatic one at inference, we propose\nSupervised Hybrid Audio Segmentation (SHAS), a method that can effectively\nlearn the optimal segmentation from any manually segmented speech corpus.\nFirst, we train a classifier to identify the included frames in a segmentation,\nusing speech representations from a pre-trained wav2vec 2.0. The optimal\nsplitting points are then found by a probabilistic Divide-and-Conquer algorithm\nthat progressively splits at the frame of lowest probability until all segments\nare below a pre-specified length. Experiments on MuST-C and mTEDx show that the\ntranslation of the segments produced by our method approaches the quality of\nthe manual segmentation on 5 languages pairs. Namely, SHAS retains 95-98% of\nthe manual segmentation's BLEU score, compared to the 87-93% of the best\nexisting methods. Our method is additionally generalizable to different domains\nand achieves high zero-shot performance in unseen languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderived from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps of the semantic features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor perform poorly in the TAGV task. To bridge these gaps, we\npropose a visual-prompt text span localizing (VPTSL) method, which introduces\nthe timestamped subtitles as a passage to perform the text span localization\nfor the input text question, and prompts the visual highlight features into the\npre-trained language model (PLM) for enhancing the joint semantic\nrepresentations. Specifically, the context query attention is utilized to\nperform cross-modal interaction between the extracted textual and visual\nfeatures. Then, the highlight features are obtained through the video-text\nhighlighting for the visual prompt. To alleviate semantic differences between\ntextual and visual features, we design the text span predictor by encoding the\nquestion, the subtitles, and the prompted visual highlight features with the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms the\nstate-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,\nwhich demonstrates the effectiveness of the proposed visual prompt and the text\nspan predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Feasibility Study of Answer-Agnostic Question Generation for Education. (arXiv:2203.08685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08685","description":"<p>We conduct a feasibility study into the applicability of answer-agnostic\nquestion generation models to textbook passages. We show that a significant\nportion of errors in such systems arise from asking irrelevant or\nuninterpretable questions and that such errors can be ameliorated by providing\nsummarized input. We find that giving these models human-written summaries\ninstead of the original text results in a significant increase in acceptability\nof generated questions (33% $\\rightarrow$ 83%) as determined by expert\nannotators. We also find that, in the absence of human-written summaries,\nautomatic summarization can serve as a good middle ground.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dugan_L/0/1/0/all/0/1\">Liam Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miltsakaki_E/0/1/0/all/0/1\">Eleni Miltsakaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Shriyash Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginsberg_E/0/1/0/all/0/1\">Etan Ginsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_H/0/1/0/all/0/1\">Hannah Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1\">Dayheon Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chuning Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Factually Grounded Content Transfer with Factual Ablation. (arXiv:2203.10133v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10133","description":"<p>Despite recent success, large neural models often generate factually\nincorrect text. Compounding this is the lack of a standard automatic evaluation\nfor factuality--it cannot be meaningfully improved if it cannot be measured.\nGrounded generation promises a path to solving both of these problems: models\ndraw on a reliable external document (grounding) for factual information,\nsimplifying the challenge of factuality. Measuring factuality is also\nsimplified--to factual consistency, testing whether the generation agrees with\nthe grounding, rather than all facts. Yet, without a standard automatic metric\nfor factual consistency, factually grounded generation remains an open problem.\n</p>\n<p>We study this problem for content transfer, in which generations extend a\nprompt, using information from factual grounding. Particularly, this domain\nallows us to introduce the notion of factual ablation for automatically\nmeasuring factual consistency: this captures the intuition that the model\nshould be less likely to produce an output given a less relevant grounding\ndocument. In practice, we measure this by presenting a model with two grounding\ndocuments, and the model should prefer to use the more factually relevant one.\nWe contribute two evaluation sets to measure this. Applying our new evaluation,\nwe propose multiple novel methods improving over strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quirk_C/0/1/0/all/0/1\">Chris Quirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1\">Michel Galley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enriching Unsupervised User Embedding via Medical Concepts. (arXiv:2203.10627v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.10627","description":"<p>Clinical notes in Electronic Health Records (EHR) present rich documented\ninformation of patients to inference phenotype for disease diagnosis and study\npatient characteristics for cohort selection. Unsupervised user embedding aims\nto encode patients into fixed-length vectors without human supervisions.\nMedical concepts extracted from the clinical notes contain rich connections\nbetween patients and their clinical categories. However, existing unsupervised\napproaches of user embeddings from clinical notes do not explicitly incorporate\nmedical concepts. In this study, we propose a concept-aware unsupervised user\nembedding that jointly leverages text documents and medical concepts from two\nclinical corpora, MIMIC-III and Diabetes. We evaluate user embeddings on both\nextrinsic and intrinsic tasks, including phenotype classification, in-hospital\nmortality prediction, patient retrieval, and patient relatedness. Experiments\non the two clinical corpora show our approach exceeds unsupervised baselines,\nand incorporating medical concepts can significantly improve the baseline\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12667","description":"<p>A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefani_E/0/1/0/all/0/1\">Eliana Stefani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Label Is Better Than Human Label. (arXiv:2203.12668v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.12668","description":"<p>State-of-the-art automatic speech recognition (ASR) systems are trained with\ntens of thousands of hours of labeled speech data. Human transcription is\nexpensive and time consuming. Factors such as the quality and consistency of\nthe transcription can greatly affect the performance of the ASR models trained\nwith these data. In this paper, we show that we can train a strong teacher\nmodel to produce high quality pseudo labels by utilizing recent self-supervised\nand semi-supervised learning techniques. Specifically, we use JUST (Joint\nUnsupervised/Supervised Training) and iterative noisy student teacher training\nto train a 600 million parameter bi-directional teacher model. This model\nachieved 4.0% word error rate (WER) on a voice search task, 11.1% relatively\nbetter than a baseline. We further show that by using this strong teacher model\nto generate high-quality pseudo labels for training, we can achieve 13.6%\nrelative WER reduction (5.9% to 5.1%) for a streaming model compared to using\nhuman labels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1\">Dongseong Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1\">Khe Chai Sim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Z/0/1/0/all/0/1\">Zhouyuan Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1\">Trevor Strohman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-text Retrieval in Context. (arXiv:2203.13645v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13645","description":"<p>Audio-text retrieval based on natural language descriptions is a challenging\ntask. It involves learning cross-modality alignments between long sequences\nunder inadequate data conditions. In this work, we investigate several audio\nfeatures as well as sequence aggregation methods for better audio-text\nalignment. Moreover, through a qualitative analysis we observe that semantic\nmapping is more important than temporal relations in contextual retrieval.\nUsing pre-trained audio features and a descriptor-based aggregation method, we\nbuild our contextual audio-text retrieval system. Specifically, we utilize\nPANNs features pre-trained on a large sound event dataset and NetRVLAD pooling,\nwhich directly works with averaged descriptors. Experiments are conducted on\nthe AudioCaps and CLOTHO datasets, and results are compared with the previous\nstate-of-the-art system. With our proposed system, a significant improvement\nhas been achieved on bidirectional audio-text retrieval, on all metrics\nincluding recall, median and mean rank.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_S/0/1/0/all/0/1\">Siyu Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuenan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Mengyue Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1\">Kai Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MQDD: Pre-training of Multimodal Question Duplicity Detection for Software Engineering Domain. (arXiv:2203.14093v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14093","description":"<p>This work proposes a new pipeline for leveraging data collected on the Stack\nOverflow website for pre-training a multimodal model for searching duplicates\non question answering websites. Our multimodal model is trained on question\ndescriptions and source codes in multiple programming languages. We design two\nnew learning objectives to improve duplicate detection capabilities. The result\nof this work is a mature, fine-tuned Multimodal Question Duplicity Detection\n(MQDD) model, ready to be integrated into a Stack Overflow search system, where\nit can help users find answers for already answered questions. Alongside the\nMQDD model, we release two datasets related to the software engineering domain.\nThe first Stack Overflow Dataset (SOD) represents a massive corpus of paired\nquestions and answers. The second Stack Overflow Duplicity Dataset (SODD)\ncontains data for training duplicate detection models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pasek_J/0/1/0/all/0/1\">Jan Pa&#x161;ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sido_J/0/1/0/all/0/1\">Jakub Sido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konopik_M/0/1/0/all/0/1\">Miloslav Konop&#xed;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prazak_O/0/1/0/all/0/1\">Ond&#x159;ej Pra&#x17e;&#xe1;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Simultaneous Speech Translation. (arXiv:2203.14835v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.14835","description":"<p>Applications designed for simultaneous speech translation during events such\nas conferences or meetings need to balance quality and lag while displaying\ntranslated text to deliver a good user experience. One common approach to\nbuilding online spoken language translation systems is by leveraging models\nbuilt for offline speech translation. Based on a technique to adapt end-to-end\nmonolingual models, we investigate multilingual models and different\narchitectures (end-to-end and cascade) on the ability to perform online speech\ntranslation. On the multilingual TEDx corpus, we show that the approach\ngeneralizes to different architectures. We see similar gains in latency\nreduction (40% relative) across languages and architectures. However, the\nend-to-end architecture leads to smaller translation quality losses after\nadapting to the online model. Furthermore, the approach even scales to\nzero-shot directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Subramanya_S/0/1/0/all/0/1\">Shashank Subramanya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning to segment fetal brain tissue from noisy annotations. (arXiv:2203.14962v1 [eess.IV])","link":"http://arxiv.org/abs/2203.14962","description":"<p>Automatic fetal brain tissue segmentation can enhance the quantitative\nassessment of brain development at this critical stage. Deep learning methods\nrepresent the state of the art in medical image segmentation and have also\nachieved impressive results in brain segmentation. However, effective training\nof a deep learning model to perform this task requires a large number of\ntraining images to represent the rapid development of the transient fetal brain\nstructures. On the other hand, manual multi-label segmentation of a large\nnumber of 3D images is prohibitive. To address this challenge, we segmented 272\ntraining images, covering 19-39 gestational weeks, using an automatic\nmulti-atlas segmentation strategy based on deformable registration and\nprobabilistic atlas fusion, and manually corrected large errors in those\nsegmentations. Since this process generated a large training dataset with noisy\nsegmentations, we developed a novel label smoothing procedure and a loss\nfunction to train a deep learning model with smoothed noisy segmentations. Our\nproposed methods properly account for the uncertainty in tissue boundaries. We\nevaluated our method on 23 manually-segmented test images of a separate set of\nfetuses. Results show that our method achieves an average Dice similarity\ncoefficient of 0.893 and 0.916 for the transient structures of younger and\nolder fetuses, respectively. Our method generated results that were\nsignificantly more accurate than several state-of-the-art methods including\nnnU-Net that achieved the closest results to our method. Our trained model can\nserve as a valuable tool to enhance the accuracy and reproducibility of fetal\nbrain analysis in MRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Karimi_D/0/1/0/all/0/1\">Davood Karimi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rollins_C/0/1/0/all/0/1\">Caitlin K. Rollins</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Velasco_Annis_C/0/1/0/all/0/1\">Clemente Velasco-Annis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ouaalam_A/0/1/0/all/0/1\">Abdelhakim Ouaalam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gholipour_A/0/1/0/all/0/1\">Ali Gholipour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TL-GAN: Improving Traffic Light Recognition via Data Synthesis for Autonomous Driving. (arXiv:2203.15006v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15006","description":"<p>Traffic light recognition, as a critical component of the perception module\nof self-driving vehicles, plays a vital role in the intelligent transportation\nsystems. The prevalent deep learning based traffic light recognition methods\nheavily hinge on the large quantity and rich diversity of training data.\nHowever, it is quite challenging to collect data in various rare scenarios such\nas flashing, blackout or extreme weather, thus resulting in the imbalanced\ndistribution of training data and consequently the degraded performance in\nrecognizing rare classes. In this paper, we seek to improve traffic light\nrecognition by leveraging data synthesis. Inspired by the generative\nadversarial networks (GANs), we propose a novel traffic light generation\napproach TL-GAN to synthesize the data of rare classes to improve traffic light\nrecognition for autonomous driving. TL-GAN disentangles traffic light sequence\ngeneration into image synthesis and sequence assembling. In the image synthesis\nstage, our approach enables conditional generation to allow full control of the\ncolor of the generated traffic light images. In the sequence assembling stage,\nwe design the style mixing and adaptive template to synthesize realistic and\ndiverse traffic light sequences. Extensive experiments show that the proposed\nTL-GAN renders remarkable improvement over the baseline without using the\ngenerated data, leading to the state-of-the-art performance in comparison with\nthe competing algorithms that are used for general image synthesis and data\nimbalance tackling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Danfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaodong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Registering Explicit to Implicit: Towards High-Fidelity Garment mesh Reconstruction from Single Images. (arXiv:2203.15007v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15007","description":"<p>Fueled by the power of deep learning techniques and implicit shape learning,\nrecent advances in single-image human digitalization have reached unprecedented\naccuracy and could recover fine-grained surface details such as garment\nwrinkles. However, a common problem for the implicit-based methods is that they\ncannot produce separated and topology-consistent mesh for each garment piece,\nwhich is crucial for the current 3D content creation pipeline. To address this\nissue, we proposed a novel geometry inference framework ReEF that reconstructs\ntopology-consistent layered garment mesh by registering the explicit garment\ntemplate to the whole-body implicit fields predicted from single images.\nExperiments demonstrate that our method notably outperforms its counterparts on\nsingle-image layered garment reconstruction and could bring high-quality\ndigital assets for further content creation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Heming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1\">Lingteng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yuda Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaoguang Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Interactive Learning-based ovarian cancer segmentation of H&E-stained whole slide images to study morphological patterns of BRCA mutation. (arXiv:2203.15015v1 [eess.IV])","link":"http://arxiv.org/abs/2203.15015","description":"<p>Deep learning has been widely used to analyze digitized hematoxylin and eosin\n(H&amp;E)-stained histopathology whole slide images. Automated cancer segmentation\nusing deep learning can be used to diagnose malignancy and to find novel\nmorphological patterns to predict molecular subtypes. To train pixel-wise\ncancer segmentation models, manual annotation from pathologists is generally a\nbottleneck due to its time-consuming nature. In this paper, we propose Deep\nInteractive Learning with a pretrained segmentation model from a different\ncancer type to reduce manual annotation time. Instead of annotating all pixels\nfrom cancer and non-cancer regions on giga-pixel whole slide images, an\niterative process of annotating mislabeled regions from a segmentation model\nand training/finetuning the model with the additional annotation can reduce the\ntime. Especially, employing a pretrained segmentation model can further reduce\nthe time than starting annotation from scratch. We trained an accurate ovarian\ncancer segmentation model with a pretrained breast segmentation model by 3.5\nhours of manual annotation which achieved intersection-over-union of 0.74,\nrecall of 0.86, and precision of 0.84. With automatically extracted high-grade\nserous ovarian cancer patches, we attempted to train another deep learning\nmodel to predict BRCA mutation. The segmentation model and code have been\nreleased at https://github.com/MSKCC-Computational-Pathology/DMMN-ovary.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ho_D/0/1/0/all/0/1\">David Joon Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chui_M/0/1/0/all/0/1\">M. Herman Chui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vanderbilt_C/0/1/0/all/0/1\">Chad M. Vanderbilt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_J/0/1/0/all/0/1\">Jiwon Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robson_M/0/1/0/all/0/1\">Mark E. Robson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Park_C/0/1/0/all/0/1\">Chan-Sik Park</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roh_J/0/1/0/all/0/1\">Jin Roh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fuchs_T/0/1/0/all/0/1\">Thomas J. Fuchs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Object Detection with Fully Cross-Transformer. (arXiv:2203.15021v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15021","description":"<p>Few-shot object detection (FSOD), with the aim to detect novel objects using\nvery few training examples, has recently attracted great research interest in\nthe community. Metric-learning based methods have been demonstrated to be\neffective for this task using a two-branch based siamese network, and calculate\nthe similarity between image regions and few-shot examples for detection.\nHowever, in previous works, the interaction between the two branches is only\nrestricted in the detection head, while leaving the remaining hundreds of\nlayers for separate feature extraction. Inspired by the recent work on vision\ntransformers and vision-language transformers, we propose a novel Fully\nCross-Transformer based model (FCT) for FSOD by incorporating cross-transformer\ninto both the feature backbone and detection head. The asymmetric-batched\ncross-attention is proposed to aggregate the key information from the two\nbranches with different batch sizes. Our model can improve the few-shot\nsimilarity learning between the two branches by introducing the multi-level\ninteractions. Comprehensive experiments on both PASCAL VOC and MSCOCO FSOD\nbenchmarks demonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A systematic review and meta-analysis of Digital Elevation Model (DEM) fusion: pre-processing, methods and applications. (arXiv:2203.15026v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15026","description":"<p>The remote sensing community has identified data fusion as one of the key\nchallenging topics of the 21st century. The subject of image fusion in\ntwo-dimensional (2D) space has been covered in several published reviews.\nHowever, the special case of 2.5D/3D Digital Elevation Model (DEM) fusion has\nnot been addressed till date. DEM fusion is a key application of data fusion in\nremote sensing. It takes advantage of the complementary characteristics of\nmulti-source DEMs to deliver a more complete, accurate and reliable elevation\ndataset. Although several methods for fusing DEMs have been developed, the\nabsence of a well-rounded review has limited their proliferation among\nresearchers and end-users. It is often required to combine knowledge from\nmultiple studies to inform a holistic perspective and guide further research.\nIn response, this paper provides a systematic review of DEM fusion: the\npre-processing workflow, methods and applications, enhanced with a\nmeta-analysis. Through the discussion and comparative analysis, unresolved\nchallenges and open issues were identified, and future directions for research\nwere proposed. This review is a timely solution and an invaluable source of\ninformation for researchers within the fields of remote sensing and spatial\ninformation science, and the data fusion community at large.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Okolie_C/0/1/0/all/0/1\">Chukwuma Okolie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smit_J/0/1/0/all/0/1\">Julian Smit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Socially Compliant Navigation Dataset (SCAND): A Large-Scale Dataset of Demonstrations for Social Navigation. (arXiv:2203.15041v1 [cs.RO])","link":"http://arxiv.org/abs/2203.15041","description":"<p>Social navigation is the capability of an autonomous agent, such as a robot,\nto navigate in a 'socially compliant' manner in the presence of other\nintelligent agents such as humans. With the emergence of autonomously\nnavigating mobile robots in human populated environments (e.g., domestic\nservice robots in homes and restaurants and food delivery robots on public\nsidewalks), incorporating socially compliant navigation behaviors on these\nrobots becomes critical to ensuring safe and comfortable human robot\ncoexistence. To address this challenge, imitation learning is a promising\nframework, since it is easier for humans to demonstrate the task of social\nnavigation rather than to formulate reward functions that accurately capture\nthe complex multi objective setting of social navigation. The use of imitation\nlearning and inverse reinforcement learning to social navigation for mobile\nrobots, however, is currently hindered by a lack of large scale datasets that\ncapture socially compliant robot navigation demonstrations in the wild. To fill\nthis gap, we introduce Socially CompliAnt Navigation Dataset (SCAND) a large\nscale, first person view dataset of socially compliant navigation\ndemonstrations. Our dataset contains 8.7 hours, 138 trajectories, 25 miles of\nsocially compliant, human teleoperated driving demonstrations that comprises\nmulti modal data streams including 3D lidar, joystick commands, odometry,\nvisual and inertial information, collected on two morphologically different\nmobile robots a Boston Dynamics Spot and a Clearpath Jackal by four different\nhuman demonstrators in both indoor and outdoor environments. We additionally\nperform preliminary analysis and validation through real world robot\nexperiments and show that navigation policies learned by imitation learning on\nSCAND generate socially compliant behaviors\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1\">Anirudh Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuesu Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirk_S/0/1/0/all/0/1\">Soeren Pirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toshev_A/0/1/0/all/0/1\">Alexander Toshev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hart_J/0/1/0/all/0/1\">Justin Hart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biswas_J/0/1/0/all/0/1\">Joydeep Biswas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A distribution-dependent Mumford-Shah model for unsupervised hyperspectral image segmentation. (arXiv:2203.15058v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15058","description":"<p>Hyperspectral images provide a rich representation of the underlying spectrum\nfor each pixel, allowing for a pixel-wise classification/segmentation into\ndifferent classes. As the acquisition of labeled training data is very\ntime-consuming, unsupervised methods become crucial in hyperspectral image\nanalysis. The spectral variability and noise in hyperspectral data make this\ntask very challenging and define special requirements for such methods.\n</p>\n<p>Here, we present a novel unsupervised hyperspectral segmentation framework.\nIt starts with a denoising and dimensionality reduction step by the\nwell-established Minimum Noise Fraction (MNF) transform. Then, the Mumford-Shah\n(MS) segmentation functional is applied to segment the data. We equipped the MS\nfunctional with a novel robust distribution-dependent indicator function\ndesigned to handle the characteristic challenges of hyperspectral data. To\noptimize our objective function with respect to the parameters for which no\nclosed form solution is available, we propose an efficient fixed point\niteration scheme. Numerical experiments on four public benchmark datasets show\nthat our method produces competitive results, which outperform two\nstate-of-the-art methods substantially on three of these datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cohrs_J/0/1/0/all/0/1\">Jan-Christopher Cohrs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berkels_B/0/1/0/all/0/1\">Benjamin Berkels</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Technique using a Sequence of Follow Up X-Rays for Disease classification. (arXiv:2203.15060v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15060","description":"<p>The ability to predict lung and heart based diseases using deep learning\ntechniques is central to many researchers, particularly in the medical field\naround the world. In this paper, we present a unique outlook of a very familiar\nproblem of disease classification using X-rays. We present a hypothesis that\nX-rays of patients included with the follow up history of their most recent\nthree chest X-ray images would perform better in disease classification in\ncomparison to one chest X-ray image input using an internal CNN to perform\nfeature extraction. We have discovered that our generic deep learning\narchitecture which we propose for solving this problem performs well with 3\ninput X ray images provided per sample for each patient. In this paper, we have\nalso established that without additional layers before the output\nclassification, the CNN models will improve the performance of predicting the\ndisease labels for each patient. We have provided our results in ROC curves and\nAUROC scores. We define a fresh approach of collecting three X-ray images for\ntraining deep learning models, which we have concluded has clearly improved the\nperformance of the models. We have shown that ResNet, in general, has a better\nresult than any other CNN model used in the feature extraction phase. With our\noriginal approach to data pre-processing, image training, and pre-trained\nmodels, we believe that the current research will assist many medical\ninstitutions around the world, and this will improve the prediction of\npatients' symptoms and diagnose them with more accurate cure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_S/0/1/0/all/0/1\">Sairamvinay Vijayaraghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddad_D/0/1/0/all/0/1\">David Haddad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shikun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1\">Seongwoo Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cycle-Consistent Counterfactuals by Latent Transformations. (arXiv:2203.15064v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15064","description":"<p>CounterFactual (CF) visual explanations try to find images similar to the\nquery image that change the decision of a vision system to a specified outcome.\nExisting methods either require inference-time optimization or joint training\nwith a generative adversarial model which makes them time-consuming and\ndifficult to use in practice. We propose a novel approach, Cycle-Consistent\nCounterfactuals by Latent Transformations (C3LT), which learns a latent\ntransformation that automatically generates visual CFs by steering in the\nlatent space of generative models. Our method uses cycle consistency between\nthe query and CF latent representations which helps our training to find better\nsolutions. C3LT can be easily plugged into any state-of-the-art pretrained\ngenerative network. This enables our method to generate high-quality and\ninterpretable CF images at high resolution such as those in ImageNet. In\naddition to several established metrics for evaluating CF explanations, we\nintroduce a novel metric tailored to assess the quality of the generated CF\nexamples and validate the effectiveness of our method on an extensive set of\nexperiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khorram_S/0/1/0/all/0/1\">Saeed Khorram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuxin_L/0/1/0/all/0/1\">Li Fuxin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepShadow: Neural Shape from Shadow. (arXiv:2203.15065v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15065","description":"<p>This paper presents DeepShadow, a one-shot method for recovering the depth\nmap and surface normals from photometric stereo shadow maps. Previous works\nthat try to recover the surface normals from photometric stereo images treat\ncast shadows as a disturbance. We show that the self and cast shadows not only\ndo not disturb 3D reconstruction, but can be used alone, as a strong learning\nsignal, to recover the depth map and surface normals. We demonstrate that 3D\nreconstruction from shadows can even outperform shape-from-shading in certain\ncases. To the best of our knowledge, our method is the first to reconstruct 3D\nshape-from-shadows using neural networks. The method does not require any\npre-training or expensive labeled data, and is optimized during inference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnieli_A/0/1/0/all/0/1\">Asaf Karnieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1\">Ohad Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hel_Or_Y/0/1/0/all/0/1\">Yacov Hel-Or</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Verification Bypass. (arXiv:2203.15068v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15068","description":"<p>Face verification systems aim to validate the claimed identity using feature\nvectors and distance metrics. However, no attempt has been made to bypass such\na system using generated images that are constrained by the same feature\nvectors. In this work, we train StarGAN v2 to generate diverse images based on\na human user, that have similar feature vectors yet qualitatively look\ndifferent. We then demonstrate a proof of concept on a custom face verification\nsystem and verify our claims by demonstrating the same proof of concept in a\nblack box setting on dating applications that utilize similar face verification\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarda_S/0/1/0/all/0/1\">Sanjana Sarda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Motion Correction Via Iterative Nonlinear Optimization and Animation. (arXiv:2203.15072v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15072","description":"<p>Here, we present an end-to-end method to create 2D animation for a goalkeeper\nattempting to block a penalty kick, and then correct that motion using an\niterative nonlinear optimization scheme. The input is a raw video that is fed\ninto pose and object detection networks to find the skeleton of the goalkeeper\nand the ball. The output is a set of key frames of the skeleton associated with\nthe corrected motion so that if the goalkeeper missed the ball, the animation\nwill show then successfully deflecting it. Our method is robust enough correct\ndifferent kinds of mistakes the goalkeeper can make, such as not lunging far\nenough or jumping to the incorrect side. Our method is also meant to be\nsemantically similar to the goalkeeper's original motion, which helps keep our\nanimation grounded with respect to actual human behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vijayaraghavan_S/0/1/0/all/0/1\">Sairamvinay Vijayaraghavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jinxiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wan-Jhen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livanos_M/0/1/0/all/0/1\">Michael J Livanos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neurosymbolic hybrid approach to driver collision warning. (arXiv:2203.15076v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15076","description":"<p>There are two main algorithmic approaches to autonomous driving systems: (1)\nAn end-to-end system in which a single deep neural network learns to map\nsensory input directly into appropriate warning and driving responses. (2) A\nmediated hybrid recognition system in which a system is created by combining\nindependent modules that detect each semantic feature. While some researchers\nbelieve that deep learning can solve any problem, others believe that a more\nengineered and symbolic approach is needed to cope with complex environments\nwith less data. Deep learning alone has achieved state-of-the-art results in\nmany areas, from complex gameplay to predicting protein structures. In\nparticular, in image classification and recognition, deep learning models have\nachieved accuracies as high as humans. But sometimes it can be very difficult\nto debug if the deep learning model doesn't work. Deep learning models can be\nvulnerable and are very sensitive to changes in data distribution.\nGeneralization can be problematic. It's usually hard to prove why it works or\ndoesn't. Deep learning models can also be vulnerable to adversarial attacks.\nHere, we combine deep learning-based object recognition and tracking with an\nadaptive neurosymbolic network agent, called the Non-Axiomatic Reasoning System\n(NARS), that can adapt to its environment by building concepts based on\nperceptual sequences. We achieved an improved intersection-over-union (IOU)\nobject recognition performance of 0.65 in the adaptive retraining model\ncompared to IOU 0.31 in the COCO data pre-trained model. We improved the object\ndetection limits using RADAR sensors in a simulated environment, and\ndemonstrated the weaving car detection capability by combining deep\nlearning-based object detection and tracking with a neurosymbolic model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kyongsik Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Thomas Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huyen_A/0/1/0/all/0/1\">Alexander Huyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_P/0/1/0/all/0/1\">Patrick Hammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CD-Net: Histopathology Representation Learning using Pyramidal Context-Detail Network. (arXiv:2203.15078v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15078","description":"<p>Extracting rich phenotype information, such as cell density and arrangement,\nfrom whole slide histology images (WSIs), requires analysis of large field of\nview, i.e more contexual information. This can be achieved through analyzing\nthe digital slides at lower resolution. A potential drawback is missing out on\ndetails present at a higher resolution. To jointly leverage complementary\ninformation from multiple resolutions, we present a novel transformer based\nPyramidal Context-Detail Network (CD-Net). CD-Net exploits the WSI pyramidal\nstructure through co-training of proposed Context and Detail Modules, which\noperate on inputs from multiple resolutions. The residual connections between\nthe modules enable the joint training paradigm while learning self-supervised\nrepresentation for WSIs. The efficacy of CD-Net is demonstrated in classifying\nLung Adenocarcinoma from Squamous cell carcinoma.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapse_S/0/1/0/all/0/1\">Saarthak Kapse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative, Deep Synthetic Aperture Sonar Image Segmentation. (arXiv:2203.15082v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15082","description":"<p>Synthetic aperture sonar (SAS) systems produce high-resolution images of the\nseabed environment. Moreover, deep learning has demonstrated superior ability\nin finding robust features for automating imagery analysis. However, the\nsuccess of deep learning is conditioned on having lots of labeled training\ndata, but obtaining generous pixel-level annotations of SAS imagery is often\npractically infeasible. This challenge has thus far limited the adoption of\ndeep learning methods for SAS segmentation. Algorithms exist to segment SAS\nimagery in an unsupervised manner, but they lack the benefit of\nstate-of-the-art learning methods and the results present significant room for\nimprovement. In view of the above, we propose a new iterative algorithm for\nunsupervised SAS image segmentation combining superpixel formation, deep\nlearning, and traditional clustering methods. We call our method Iterative Deep\nUnsupervised Segmentation (IDUS). IDUS is an unsupervised learning framework\nthat can be divided into four main steps: 1) A deep network estimates class\nassignments. 2) Low-level image features from the deep network are clustered\ninto superpixels. 3) Superpixels are clustered into class assignments (which we\ncall pseudo-labels) using $k$-means. 4) Resulting pseudo-labels are used for\nloss backpropagation of the deep network prediction. These four steps are\nperformed iteratively until convergence. A comparison of IDUS to current\nstate-of-the-art methods on a realistic benchmark dataset for SAS image\nsegmentation demonstrates the benefits of our proposal even as the IDUS incurs\na much lower computational burden during inference (actual labeling of a test\nimage). Finally, we also develop a semi-supervised (SS) extension of IDUS\ncalled IDSS and demonstrate experimentally that it can further enhance\nperformance while outperforming supervised alternatives that exploit the same\nlabeled training imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yung-Chen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerg_I/0/1/0/all/0/1\">Isaac D. Gerg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval. (arXiv:2203.15086v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15086","description":"<p>In text-video retrieval, the objective is to learn a cross-modal similarity\nfunction between a text and a video that ranks relevant text-video pairs higher\nthan irrelevant pairs. However, videos inherently express a much wider gamut of\ninformation than texts. Instead, texts often capture sub-regions of entire\nvideos and are most semantically similar to certain frames within videos.\nTherefore, for a given text, a retrieval model should focus on the text's most\nsemantically similar video sub-regions to make a more relevant comparison. Yet,\nmost existing works aggregate entire videos without directly considering text.\nCommon text-agnostic aggregations schemes include mean-pooling or\nself-attention over the frames, but these are likely to encode misleading\nvisual information not described in the given text. To address this, we propose\na cross-modal attention model called X-Pool that reasons between a text and the\nframes of a video. Our core mechanism is a scaled dot product attention for a\ntext to attend to its most semantically similar frames. We then generate an\naggregated video representation conditioned on the text's attention weights\nover the frames. We evaluate our method on three benchmark datasets of MSR-VTT,\nMSVD and LSMDC, achieving new state-of-the-art results by up to 12% in relative\nimprovement in Recall@1. Our findings thereby highlight the importance of joint\ntext-video reasoning to extract important visual cues according to text. Full\ncode and demo can be found at: https://layer6ai-labs.github.io/xpool/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gorti_S/0/1/0/all/0/1\">Satya Krishna Gorti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vouitsis_N/0/1/0/all/0/1\">Noel Vouitsis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Junwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golestan_K/0/1/0/all/0/1\">Keyvan Golestan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1\">Maksims Volkovs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Animesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1\">Guangwei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Optical Flow, Depth, and Scene Flow without Real-World Labels. (arXiv:2203.15089v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15089","description":"<p>Self-supervised monocular depth estimation enables robots to learn 3D\nperception from raw video streams. This scalable approach leverages projective\ngeometry and ego-motion to learn via view synthesis, assuming the world is\nmostly static. Dynamic scenes, which are common in autonomous driving and\nhuman-robot interaction, violate this assumption. Therefore, they require\nmodeling dynamic objects explicitly, for instance via estimating pixel-wise 3D\nmotion, i.e. scene flow. However, the simultaneous self-supervised learning of\ndepth and scene flow is ill-posed, as there are infinitely many combinations\nthat result in the same 3D point. In this paper we propose DRAFT, a new method\ncapable of jointly learning depth, optical flow, and scene flow by combining\nsynthetic data with geometric self-supervision. Building upon the RAFT\narchitecture, we learn optical flow as an intermediate task to bootstrap depth\nand scene flow learning via triangulation. Our algorithm also leverages\ntemporal and geometric consistency losses across tasks to improve multi-task\nlearning. Our DRAFT architecture simultaneously establishes a new state of the\nart in all three tasks in the self-supervised monocular setting on the standard\nKITTI benchmark. Project page: https://sites.google.com/tri.global/draft.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guizilini_V/0/1/0/all/0/1\">Vitor Guizilini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kuan-Hui Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1\">Rares Ambrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1\">Adrien Gaidon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New pyramidal hybrid textural and deep features based automatic skin cancer classification model: Ensemble DarkNet and textural feature extractor. (arXiv:2203.15090v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15090","description":"<p>Background: Skin cancer is one of the widely seen cancer worldwide and\nautomatic classification of skin cancer can be benefited dermatology clinics\nfor an accurate diagnosis. Hence, a machine learning-based automatic skin\ncancer detection model must be developed. Material and Method: This research\ninterests to overcome automatic skin cancer detection problem. A colored skin\ncancer image dataset is used. This dataset contains 3297 images with two\nclasses. An automatic multilevel textural and deep features-based model is\npresented. Multilevel fuse feature generation using discrete wavelet transform\n(DWT), local phase quantization (LPQ), local binary pattern (LBP), pre-trained\nDarkNet19, and DarkNet53 are utilized to generate features of the skin cancer\nimages, top 1000 features are selected threshold value-based neighborhood\ncomponent analysis (NCA). The chosen top 1000 features are classified using the\n10-fold cross-validation technique. Results: To obtain results, ten-fold\ncross-validation is used and 91.54% classification accuracy results are\nobtained by using the recommended pyramidal hybrid feature generator and NCA\nselector-based model. Further, various training and testing separation ratios\n(90:10, 80:20, 70:30, 60:40, 50:50) are used and the maximum classification\nrate is calculated as 95.74% using the 90:10 separation ratio. Conclusions: The\nfindings and accuracies calculated are denoted that this model can be used in\ndermatology and pathology clinics to simplify the skin cancer detection process\nand help physicians.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baygin_M/0/1/0/all/0/1\">Mehmet Baygin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuncer_T/0/1/0/all/0/1\">Turker Tuncer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogan_S/0/1/0/all/0/1\">Sengul Dogan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding out-of-distribution accuracies through quantifying difficulty of test samples. (arXiv:2203.15100v1 [cs.LG])","link":"http://arxiv.org/abs/2203.15100","description":"<p>Existing works show that although modern neural networks achieve remarkable\ngeneralization performance on the in-distribution (ID) dataset, the accuracy\ndrops significantly on the out-of-distribution (OOD) datasets\n\\cite{recht2018cifar, recht2019imagenet}. To understand why a variety of models\nconsistently make more mistakes in the OOD datasets, we propose a new metric to\nquantify the difficulty of the test images (either ID or OOD) that depends on\nthe interaction of the training dataset and the model. In particular, we\nintroduce \\textit{confusion score} as a label-free measure of image difficulty\nwhich quantifies the amount of disagreement on a given test image based on the\nclass conditional probabilities estimated by an ensemble of trained models.\nUsing the confusion score, we investigate CIFAR-10 and its OOD derivatives.\nNext, by partitioning test and OOD datasets via their confusion scores, we\npredict the relationship between ID and OOD accuracies for various\narchitectures. This allows us to obtain an estimator of the OOD accuracy of a\ngiven model only using ID test labels. Our observations indicate that the\nbiggest contribution to the accuracy drop comes from images with high confusion\nscores. Upon further inspection, we report on the nature of the misclassified\nimages grouped by their confusion scores: \\textit{(i)} images with high\nconfusion scores contain \\textit{weak spurious correlations} that appear in\nmultiple classes in the training data and lack clear \\textit{class-specific\nfeatures}, and \\textit{(ii)} images with low confusion scores exhibit spurious\ncorrelations that belong to another class, namely \\textit{class-specific\nspurious correlations}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simsek_B/0/1/0/all/0/1\">Berfin Simsek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_M/0/1/0/all/0/1\">Melissa Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1\">Levent Sagun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Semantic Segmentation: A Prototype View. (arXiv:2203.15102v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15102","description":"<p>Prevalent semantic segmentation solutions, despite their different network\ndesigns (FCN based or attention based) and mask decoding strategies (parametric\nsoftmax based or pixel-query based), can be placed in one category, by\nconsidering the softmax weights or query vectors as learnable class prototypes.\nIn light of this prototype view, this study uncovers several limitations of\nsuch parametric segmentation regime, and proposes a nonparametric alternative\nbased on non-learnable prototypes. Instead of prior methods learning a single\nweight/query vector for each class in a fully parametric manner, our model\nrepresents each class as a set of non-learnable prototypes, relying solely on\nthe mean features of several training pixels within that class. The dense\nprediction is thus achieved by nonparametric nearest prototype retrieving. This\nallows our model to directly shape the pixel embedding space, by optimizing the\narrangement between embedded pixels and anchored prototypes. It is able to\nhandle arbitrary number of classes with a constant amount of learnable\nparameters. We empirically show that, with FCN based and attention based\nsegmentation models (i.e., HRNet, Swin, SegFormer) and backbones (i.e., ResNet,\nHRNet, Swin, MiT), our nonparametric framework yields compelling results over\nseveral datasets (i.e., ADE20K, Cityscapes, COCO-Stuff), and performs well in\nthe large-vocabulary situation. We expect this work will provoke a rethink of\nthe current de facto semantic segmentation model design.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1\">Ender Konukoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LiDAR Snowfall Simulation for Robust 3D Object Detection. (arXiv:2203.15118v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15118","description":"<p>3D object detection is a central task for applications such as autonomous\ndriving, in which the system needs to localize and classify surrounding traffic\nagents, even in the presence of adverse weather. In this paper, we address the\nproblem of LiDAR-based 3D object detection under snowfall. Due to the\ndifficulty of collecting and annotating training data in this setting, we\npropose a physically based method to simulate the effect of snowfall on real\nclear-weather LiDAR point clouds. Our method samples snow particles in 2D space\nfor each LiDAR line and uses the induced geometry to modify the measurement for\neach LiDAR beam accordingly. Moreover, as snowfall often causes wetness on the\nground, we also simulate ground wetness on LiDAR point clouds. We use our\nsimulation to generate partially synthetic snowy LiDAR data and leverage these\ndata for training 3D object detection models that are robust to snowfall. We\nconduct an extensive evaluation using several state-of-the-art 3D object\ndetection methods and show that our simulation consistently yields significant\nperformance gains on the real snowy STF dataset compared to clear-weather\nbaselines and competing simulation approaches, while not sacrificing\nperformance in clear weather. Our code is available at\nwww.github.com/SysCV/LiDAR_snow_sim.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hahner_M/0/1/0/all/0/1\">Martin Hahner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakaridis_C/0/1/0/all/0/1\">Christos Sakaridis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bijelic_M/0/1/0/all/0/1\">Mario Bijelic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Odometry for RGB-D Cameras. (arXiv:2203.15119v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15119","description":"<p>Visual odometry is the process of estimating the position and orientation of\na camera by analyzing the images associated to it. This paper develops a quick\nand accurate approach to visual odometry of a moving RGB-D camera navigating on\na static environment. The proposed algorithm uses SURF (Speeded Up Robust\nFeatures) as feature extractor, RANSAC (Random Sample Consensus) to filter the\nresults and Minimum Mean Square to estimate the rigid transformation of six\nparameters between successive video frames. Data from a Kinect camera were used\nin the tests. The results show that this approach is feasible and promising,\nsurpassing in performance the algorithms ICP (Interactive Closest Point) and\nSfM (Structure from Motion) in tests using a publicly available dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fontes_A/0/1/0/all/0/1\">Afonso Fontes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maia_J/0/1/0/all/0/1\">Jose Everardo Bessa Maia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text2Pos: Text-to-Point-Cloud Cross-Modal Localization. (arXiv:2203.15125v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15125","description":"<p>Natural language-based communication with mobile devices and home appliances\nis becoming increasingly popular and has the potential to become natural for\ncommunicating with mobile robots in the future. Towards this goal, we\ninvestigate cross-modal text-to-point-cloud localization that will allow us to\nspecify, for example, a vehicle pick-up or goods delivery location. In\nparticular, we propose Text2Pos, a cross-modal localization module that learns\nto align textual descriptions with localization cues in a coarse- to-fine\nmanner. Given a point cloud of the environment, Text2Pos locates a position\nthat is specified via a natural language-based description of the immediate\nsurroundings. To train Text2Pos and study its performance, we construct\nKITTI360Pose, the first dataset for this task based on the recently introduced\nKITTI360 dataset. Our experiments show that we can localize 65% of textual\nqueries within 15m distance to query locations for top-10 retrieved locations.\nThis is a starting point that we hope will spark future developments towards\nlanguage-based navigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolmet_M/0/1/0/all/0/1\">Manuel Kolmet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qunjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljosa Osep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taixe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LocalBins: Improving Depth Estimation by Learning Local Distributions. (arXiv:2203.15132v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15132","description":"<p>We propose a novel architecture for depth estimation from a single image. The\narchitecture itself is based on the popular encoder-decoder architecture that\nis frequently used as a starting point for all dense regression tasks. We build\non AdaBins which estimates a global distribution of depth values for the input\nimage and evolve the architecture in two ways. First, instead of predicting\nglobal depth distributions, we predict depth distributions of local\nneighborhoods at every pixel. Second, instead of predicting depth distributions\nonly towards the end of the decoder, we involve all layers of the decoder. We\ncall this new architecture LocalBins. Our results demonstrate a clear\nimprovement over the state-of-the-art in all metrics on the NYU-Depth V2\ndataset. Code and pretrained models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Shariq Farooq Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhashim_I/0/1/0/all/0/1\">Ibraheem Alhashim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1\">Peter Wonka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards End-to-End Unified Scene Text Detection and Layout Analysis. (arXiv:2203.15143v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15143","description":"<p>Scene text detection and document layout analysis have long been treated as\ntwo separate tasks in different image domains. In this paper, we bring them\ntogether and introduce the task of unified scene text detection and layout\nanalysis. The first hierarchical scene text dataset is introduced to enable\nthis novel research task. We also propose a novel method that is able to\nsimultaneously detect scene text and form text clusters in a unified way.\nComprehensive experiments show that our unified model achieves better\nperformance than multiple well-designed baseline methods. Additionally, this\nmodel achieves state-of-the-art results on multiple scene text detection\ndatasets without the need of complex post-processing. Dataset and code:\nhttps://github.com/google-research-datasets/hiertext.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Long_S/0/1/0/all/0/1\">Shangbang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_S/0/1/0/all/0/1\">Siyang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panteleev_D/0/1/0/all/0/1\">Dmitry Panteleev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissacco_A/0/1/0/all/0/1\">Alessandro Bissacco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_M/0/1/0/all/0/1\">Michalis Raptis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Synthesize Volumetric Meshes from Vision-based Tactile Imprints. (arXiv:2203.15155v1 [cs.RO])","link":"http://arxiv.org/abs/2203.15155","description":"<p>Vision-based tactile sensors typically utilize a deformable elastomer and a\ncamera mounted above to provide high-resolution image observations of contacts.\nObtaining accurate volumetric meshes for the deformed elastomer can provide\ndirect contact information and benefit robotic grasping and manipulation. This\npaper focuses on learning to synthesize the volumetric mesh of the elastomer\nbased on the image imprints acquired from vision-based tactile sensors.\nSynthetic image-mesh pairs and real-world images are gathered from 3D finite\nelement methods (FEM) and physical sensors, respectively. A graph neural\nnetwork (GNN) is introduced to learn the image-to-mesh mappings with supervised\nlearning. A self-supervised adaptation method and image augmentation techniques\nare proposed to transfer networks from simulation to reality, from primitive\ncontacts to unseen contacts, and from one sensor to another. Using these\nlearned and adapted networks, our proposed method can accurately reconstruct\nthe deformation of the real-world tactile sensor elastomer in various domains,\nas indicated by the quantitative and qualitative results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinghao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddarth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1\">Masayoshi Tomizuka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1\">Jeroen van Baar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Aspects of Zero-Shot Learning. (arXiv:2203.15158v1 [cs.LG])","link":"http://arxiv.org/abs/2203.15158","description":"<p>One of important areas of machine learning research is zero-shot learning. It\nis applied when properly labeled training data set is not available. A number\nof zero-shot algorithms have been proposed and experimented with. However, none\nof them seems to be the \"overall winner\". In situations like this, it may be\npossible to develop a meta-classifier that would combine \"best aspects\" of\nindividual classifiers and outperform all of them. In this context, the goal of\nthis contribution is twofold. First, multiple state-of-the-art zero-shot\nlearning methods are compared for standard benchmark datasets. Second, multiple\nmeta-classifiers are suggested and experimentally compared (for the same\ndatasets).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saad_E/0/1/0/all/0/1\">Elie Saad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paprzycki_M/0/1/0/all/0/1\">Marcin Paprzycki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganzha_M/0/1/0/all/0/1\">Maria Ganzha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAT-Net: A Cross-Slice Attention Transformer Model for Prostate Zonal Segmentation in MRI. (arXiv:2203.15163v1 [eess.IV])","link":"http://arxiv.org/abs/2203.15163","description":"<p>Prostate cancer is the second leading cause of cancer death among men in the\nUnited States. The diagnosis of prostate MRI often relies on the accurate\nprostate zonal segmentation. However, state-of-the-art automatic segmentation\nmethods often fail to produce well-contained volumetric segmentation of the\nprostate zones since certain slices of prostate MRI, such as base and apex\nslices, are harder to segment than other slices. This difficulty can be\novercome by accounting for the cross-slice relationship of adjacent slices, but\ncurrent methods do not fully learn and exploit such relationships. In this\npaper, we propose a novel cross-slice attention mechanism, which we use in a\nTransformer module to systematically learn the cross-slice relationship at\ndifferent scales. The module can be utilized in any existing learning-based\nsegmentation framework with skip connections. Experiments show that our\ncross-slice attention is able to capture the cross-slice information in\nprostate zonal segmentation and improve the performance of current\nstate-of-the-art methods. Our method significantly improves segmentation\naccuracy in the peripheral zone, such that the segmentation results are\nconsistent across all the prostate slices (apex, mid-gland, and base).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1\">Alex Ling Yu Hung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Haoxin Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miao_Q/0/1/0/all/0/1\">Qi Miao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raman_S/0/1/0/all/0/1\">Steven S. Raman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Terzopoulos_D/0/1/0/all/0/1\">Demetri Terzopoulos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sung_K/0/1/0/all/0/1\">Kyunghyun Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Light Field Depth Estimation Using Epipolar Plane Images. (arXiv:2203.15171v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15171","description":"<p>Exploiting light field data makes it possible to obtain dense and accurate\ndepth map. However, synthetic scenes with limited disparity range cannot\ncontain the diversity of real scenes. By training in synthetic data, current\nlearning-based methods do not perform well in real scenes. In this paper, we\npropose a self-supervised learning framework for light field depth estimation.\nDifferent from the existing end-to-end training methods using disparity label\nper pixel, our approach implements network training by estimating EPI disparity\nshift after refocusing, which extends the disparity range of epipolar lines. To\nreduce the sensitivity of EPI to noise, we propose a new input mode called\nEPI-Stack, which stacks EPIs in the view dimension. This method is less\nsensitive to noise scenes than traditional input mode and improves the\nefficiency of estimation. Compared with other state-of-the-art methods, the\nproposed method can also obtain higher quality results in real-world scenarios,\nespecially in the complex occlusion and depth discontinuity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_M/0/1/0/all/0/1\">Meibin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Object Motion and Occlusion for Unsupervised Multi-frame Monocular Depth. (arXiv:2203.15174v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15174","description":"<p>Conventional self-supervised monocular depth prediction methods are based on\na static environment assumption, which leads to accuracy degradation in dynamic\nscenes due to the mismatch and occlusion problems introduced by object motions.\nExisting dynamic-object-focused methods only partially solved the mismatch\nproblem at the training loss level. In this paper, we accordingly propose a\nnovel multi-frame monocular depth prediction method to solve these problems at\nboth the prediction and supervision loss levels. Our method, called\nDynamicDepth, is a new framework trained via a self-supervised cycle consistent\nlearning scheme. A Dynamic Object Motion Disentanglement (DOMD) module is\nproposed to disentangle object motions to solve the mismatch problem. Moreover,\nnovel occlusion-aware Cost Volume and Re-projection Loss are designed to\nalleviate the occlusion effects of object motions. Extensive analyses and\nexperiments on the Cityscapes and KITTI datasets show that our method\nsignificantly outperforms the state-of-the-art monocular depth prediction\nmethods, especially in the areas of dynamic objects. Our code will be made\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">YingLi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Transformer Tracker for Object Tracking. (arXiv:2203.15175v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15175","description":"<p>As an important area in computer vision, object tracking has formed two\nseparate communities that respectively study Single Object Tracking (SOT) and\nMultiple Object Tracking (MOT). However, current methods in one tracking\nscenario are not easily adapted to the other due to the divergent training\ndatasets and tracking objects of both tasks. Although UniTrack\n\\cite{wang2021different} demonstrates that a shared appearance model with\nmultiple heads can be used to tackle individual tracking tasks, it fails to\nexploit the large-scale tracking datasets for training and performs poorly on\nsingle object tracking. In this work, we present the Unified Transformer\nTracker (UTT) to address tracking problems in different scenarios with one\nparadigm. A track transformer is developed in our UTT to track the target in\nboth SOT and MOT. The correlation between the target and tracking frame\nfeatures is exploited to localize the target. We demonstrate that both SOT and\nMOT tasks can be solved within this framework. The model can be simultaneously\nend-to-end trained by alternatively optimizing the SOT and MOT objectives on\nthe datasets of individual tasks. Extensive experiments are conducted on\nseveral benchmarks with a unified model trained on SOT and MOT datasets. Code\nwill be available at https://github.com/Flowerfan/Trackron.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Linchao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haoqi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yilei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhicheng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Min-Max Similarity: A Contrastive Learning Based Semi-Supervised Learning Network for Surgical Tools Segmentation. (arXiv:2203.15177v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15177","description":"<p>Segmentation of images is a popular topic in medical AI. This is mainly due\nto the difficulty to obtain a significant number of pixel-level annotated data\nto train a neural network. To address this issue, we proposed a semi-supervised\nsegmentation network based on contrastive learning. In contrast to the previous\nstate-of-the-art, we introduce a contrastive learning form of dual-view\ntraining by employing classifiers and projectors to build all-negative, and\npositive and negative feature pairs respectively to formulate the learning\nproblem as solving min-max similarity problem. The all-negative pairs are used\nto supervise the networks learning from different views and make sure to\ncapture general features, and the consistency of unlabeled predictions is\nmeasured by pixel-wise contrastive loss between positive and negative pairs. To\nquantitative and qualitative evaluate our proposed method, we test it on two\npublic endoscopy surgical tool segmentation datasets and one cochlear implant\nsurgery dataset which we manually annotate the cochlear implant in surgical\nvideos. The segmentation performance (dice coefficients) indicates that our\nproposed method outperforms state-of-the-art semi-supervised and fully\nsupervised segmentation algorithms consistently. The code is publicly available\nat: https://github.com/AngeLouCN/Min_Max_Similarity\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1\">Ange Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xing Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziteng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_J/0/1/0/all/0/1\">Jack Noble</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-term Visual Map Sparsification with Heterogeneous GNN. (arXiv:2203.15182v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15182","description":"<p>We address the problem of map sparsification for long-term visual\nlocalization. For map sparsification, a commonly employed assumption is that\nthe pre-build map and the later captured localization query are consistent.\nHowever, this assumption can be easily violated in the dynamic world.\nAdditionally, the map size grows as new data accumulate through time, causing\nlarge data overhead in the long term. In this paper, we aim to overcome the\nenvironmental changes and reduce the map size at the same time by selecting\npoints that are valuable to future localization. Inspired by the recent\nprogress in Graph Neural Network(GNN), we propose the first work that models\nSfM maps as heterogeneous graphs and predicts 3D point importance scores with a\nGNN, which enables us to directly exploit the rich information in the SfM map\ngraph. Two novel supervisions are proposed: 1) a data-fitting term for\nselecting valuable points to future localization based on training queries; 2)\na K-Cover term for selecting sparse points with full map coverage. The\nexperiments show that our method selected map points on stable and widely\nvisible structures and outperformed baselines in localization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Fang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yipu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajvi Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engel_J/0/1/0/all/0/1\">Jakob J. Engel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaess_M/0/1/0/all/0/1\">Michael Kaess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucey_S/0/1/0/all/0/1\">Simon Lucey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASM-Loc: Action-aware Segment Modeling for Weakly-Supervised Temporal Action Localization. (arXiv:2203.15187v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15187","description":"<p>Weakly-supervised temporal action localization aims to recognize and localize\naction segments in untrimmed videos given only video-level action labels for\ntraining. Without the boundary information of action segments, existing methods\nmostly rely on multiple instance learning (MIL), where the predictions of\nunlabeled instances (i.e., video snippets) are supervised by classifying\nlabeled bags (i.e., untrimmed videos). However, this formulation typically\ntreats snippets in a video as independent instances, ignoring the underlying\ntemporal structures within and across action segments. To address this problem,\nwe propose \\system, a novel WTAL framework that enables explicit, action-aware\nsegment modeling beyond standard MIL-based methods. Our framework entails three\nsegment-centric components: (i) dynamic segment sampling for compensating the\ncontribution of short actions; (ii) intra- and inter-segment attention for\nmodeling action dynamics and capturing temporal dependencies; (iii) pseudo\ninstance-level supervision for improving action boundary prediction.\nFurthermore, a multi-step refinement strategy is proposed to progressively\nimprove action proposals along the model training process. Extensive\nexperiments on THUMOS-14 and ActivityNet-v1.3 demonstrate the effectiveness of\nour approach, establishing new state of the art on both datasets. The code and\nmodels are publicly available at~\\url{https://github.com/boheumd/ASM-Loc}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xitong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse to Fine: Image Restoration Boosted by Multi-Scale Low-Rank Tensor Completion. (arXiv:2203.15189v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15189","description":"<p>Existing low-rank tensor completion (LRTC) approaches aim at restoring a\npartially observed tensor by imposing a global low-rank constraint on the\nunderlying completed tensor. However, such a global rank assumption suffers the\ntrade-off between restoring the originally details-lacking parts and neglecting\nthe potentially complex objects, making the completion performance\nunsatisfactory on both sides. To address this problem, we propose a novel and\npractical strategy for image restoration that restores the partially observed\ntensor in a coarse-to-fine (C2F) manner, which gets rid of such trade-off by\nsearching proper local ranks for both low- and high-rank parts. Extensive\nexperiments are conducted to demonstrate the superiority of the proposed C2F\nscheme. The codes are available at: https://github.com/RuiLin0212/C2FLRTC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1\">Rui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_N/0/1/0/all/0/1\">Ngai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow. (arXiv:2203.15190v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15190","description":"<p>Reconstructing 3D shape from a single 2D image is a challenging task, which\nneeds to estimate the detailed 3D structures based on the semantic attributes\nfrom 2D image. So far, most of the previous methods still struggle to extract\nsemantic attributes for 3D reconstruction task. Since the semantic attributes\nof a single image are usually implicit and entangled with each other, it is\nstill challenging to reconstruct 3D shape with detailed semantic structures\nrepresented by the input image. To address this problem, we propose 3DAttriFlow\nto disentangle and extract semantic attributes through different semantic\nlevels in the input images. These disentangled semantic attributes will be\nintegrated into the 3D shape reconstruction process, which can provide definite\nguidance to the reconstruction of specific attribute on 3D shape. As a result,\nthe 3D decoder can explicitly capture high-level semantic features at the\nbottom of the network, and utilize low-level features at the top of the\nnetwork, which allows to reconstruct more accurate 3D shapes. Note that the\nexplicit disentangling is learned without extra labels, where the only\nsupervision used in our training is the input image and its corresponding 3D\nshape. Our comprehensive experiments on ShapeNet dataset demonstrate that\n3DAttriFlow outperforms the state-of-the-art shape reconstruction methods, and\nwe also validate its generalization ability on shape completion task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1\">Xin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Junsheng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu-Shen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhizhong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnoDFDNet: A Deep Feature Difference Network for Anomaly Detection. (arXiv:2203.15195v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15195","description":"<p>This paper proposed a novel anomaly detection (AD) approach of High-speed\nTrain images based on convolutional neural networks and the Vision Transformer.\nDifferent from previous AD works, in which anomalies are identified with a\nsingle image using classification, segmentation, or object detection methods,\nthe proposed method detects abnormal difference between two images taken at\ndifferent times of the same region. In other words, we cast anomaly detection\nproblem with a single image into a difference detection problem with two\nimages. The core idea of the proposed method is that the 'anomaly' usually\nrepresents an abnormal state instead of a specific object, and this state\nshould be identified by a pair of images. In addition, we introduced a deep\nfeature difference AD network (AnoDFDNet) which sufficiently explored the\npotential of the Vision Transformer and convolutional neural networks. To\nverify the effectiveness of the proposed AnoDFDNet, we collected three\ndatasets, a difference dataset (Diff Dataset), a foreign body dataset (FB\nDataset), and an oil leakage dataset (OL Dataset). Experimental results on\nabove datasets demonstrate the superiority of proposed method. Source code are\navailable at https://github.com/wangle53/AnoDFDNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhixue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1\">Lin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field Depth Estimation Based on Stitched-EPI. (arXiv:2203.15201v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15201","description":"<p>Depth estimation is one of the most essential problems for light field\napplications. In EPI-based methods, the slope computation usually suffers low\naccuracy due to the discretization error and low angular resolution. In\naddition, recent methods work well in most regions but often struggle with\nblurry edges over occluded regions and ambiguity over texture-less regions. To\naddress these challenging issues, we first propose the stitched-EPI and\nhalf-stitched-EPI algorithms for non-occluded and occluded regions,\nrespectively. The algorithms improve slope computation by shifting and\nconcatenating lines in different EPIs but related to the same point in 3D\nscene, while the half-stitched-EPI only uses non-occluded part of lines.\nCombined with the joint photo-consistency cost proposed by us, the more\naccurate and robust depth map can be obtained in both occluded and non-occluded\nregions. Furthermore, to improve the depth estimation in texture-less regions,\nwe propose a depth propagation strategy that determines their depth from the\nedge to interior, from accurate regions to coarse regions. Experimental and\nablation results demonstrate that the proposed method achieves accurate and\nrobust depth maps in all regions effectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Ping Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimT: Handling Open-set Noise for Domain Adaptive Semantic Segmentation. (arXiv:2203.15202v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15202","description":"<p>This paper studies a practical domain adaptive (DA) semantic segmentation\nproblem where only pseudo-labeled target data is accessible through a black-box\nmodel. Due to the domain gap and label shift between two domains,\npseudo-labeled target data contains mixed closed-set and open-set label noises.\nIn this paper, we propose a simplex noise transition matrix (SimT) to model the\nmixed noise distributions in DA semantic segmentation and formulate the problem\nas estimation of SimT. By exploiting computational geometry analysis and\nproperties of segmentation, we design three complementary regularizers, i.e.\nvolume regularization, anchor guidance, convex guarantee, to approximate the\ntrue SimT. Specifically, volume regularization minimizes the volume of simplex\nformed by rows of the non-square SimT, which ensures outputs of segmentation\nmodel to fit into the ground truth label distribution. To compensate for the\nlack of open-set knowledge, anchor guidance and convex guarantee are devised to\nfacilitate the modeling of open-set noise distribution and enhance the\ndiscriminative feature learning among closed-set and open-set classes. The\nestimated SimT is further utilized to correct noise issues in pseudo labels and\npromote the generalization ability of segmentation model on target domain data.\nExtensive experimental results demonstrate that the proposed SimT can be\nflexibly plugged into existing DA methods to boost the performance. The source\ncode is available at \\url{https://github.com/CityU-AIM-Group/SimT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaoqing Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yiyuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Periocular Biometrics and its Relevance to Partially Masked Faces: A Survey. (arXiv:2203.15203v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15203","description":"<p>The performance of face recognition systems can be negatively impacted in the\npresence of masks and other types of facial coverings that have become\nprevalent due to the COVID-19 pandemic. In such cases, the periocular region of\nthe human face becomes an important biometric cue. In this article, we present\na detailed review of periocular biometrics. We first examine the various face\nand periocular techniques specially designed to recognize humans wearing a face\nmask. Then, we review different aspects of periocular biometrics: (a) the\nanatomical cues present in the periocular region useful for recognition, (b)\nthe various feature extraction and matching techniques developed, (c)\nrecognition across different spectra, (d) fusion with other biometric\nmodalities (face or iris), (e) recognition on mobile devices, (f) its\nusefulness in other applications, (g) periocular datasets, and (h) competitions\norganized for evaluating the efficacy of this biometric modality. Finally, we\ndiscuss various challenges and future directions in the field of periocular\nbiometrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Renu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Arun Ross</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPAct: Self-supervised Privacy Preservation for Action Recognition. (arXiv:2203.15205v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15205","description":"<p>Visual private information leakage is an emerging key issue for the fast\ngrowing applications of video understanding like activity recognition. Existing\napproaches for mitigating privacy leakage in action recognition require privacy\nlabels along with the action labels from the video dataset. However, annotating\nframes of video dataset for privacy labels is not feasible. Recent developments\nof self-supervised learning (SSL) have unleashed the untapped potential of the\nunlabeled data. For the first time, we present a novel training framework which\nremoves privacy information from input video in a self-supervised manner\nwithout requiring privacy labels. Our training framework consists of three main\ncomponents: anonymization function, self-supervised privacy removal branch, and\naction recognition branch. We train our framework using a minimax optimization\nstrategy to minimize the action recognition cost function and maximize the\nprivacy cost function through a contrastive self-supervised loss. Employing\nexisting protocols of known-action and privacy attributes, our framework\nachieves a competitive action-privacy trade-off to the existing\nstate-of-the-art supervised methods. In addition, we introduce a new protocol\nto evaluate the generalization of learned the anonymization function to\nnovel-action and privacy attributes and show that our self-supervised framework\noutperforms existing supervised methods. Code available at:\nhttps://github.com/DAVEISHAN/SPAct\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dave_I/0/1/0/all/0/1\">Ishan Rajendrakumar Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Few-Shot NAS with Gradient Matching. (arXiv:2203.15207v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15207","description":"<p>Efficient performance estimation of architectures drawn from large search\nspaces is essential to Neural Architecture Search. One-Shot methods tackle this\nchallenge by training one supernet to approximate the performance of every\narchitecture in the search space via weight-sharing, thereby drastically\nreducing the search cost. However, due to coupled optimization between child\narchitectures caused by weight-sharing, One-Shot supernet's performance\nestimation could be inaccurate, leading to degraded search outcomes. To address\nthis issue, Few-Shot NAS reduces the level of weight-sharing by splitting the\nOne-Shot supernet into multiple separated sub-supernets via edge-wise\n(layer-wise) exhaustive partitioning. Since each partition of the supernet is\nnot equally important, it necessitates the design of a more effective splitting\ncriterion. In this work, we propose a gradient matching score (GM) that\nleverages gradient information at the shared weight for making informed\nsplitting decisions. Intuitively, gradients from different child models can be\nused to identify whether they agree on how to update the shared modules, and\nsubsequently to decide if they should share the same weight. Compared with\nexhaustive partitioning, the proposed criterion significantly reduces the\nbranching factor per edge. This allows us to split more edges (layers) for a\ngiven budget, resulting in substantially improved performance as NAS search\nspaces usually include dozens of edges (layers). Extensive empirical\nevaluations of the proposed method on a wide range of search spaces\n(NASBench-201, DARTS, MobileNet Space), datasets (cifar10, cifar100, ImageNet)\nand search algorithms (DARTS, SNAS, RSPS, ProxylessNAS, OFA) demonstrate that\nit significantly outperforms its Few-Shot counterparts while surpassing\nprevious comparable methods in terms of the accuracy of derived architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shoukang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruochen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1\">Lanqing Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Cho-Jui Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Camera-Conditioned Stable Feature Generation for Isolated Camera Supervised Person Re-IDentification. (arXiv:2203.15210v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15210","description":"<p>To learn camera-view invariant features for person Re-IDentification (Re-ID),\nthe cross-camera image pairs of each person play an important role. However,\nsuch cross-view training samples could be unavailable under the ISolated Camera\nSupervised (ISCS) setting, e.g., a surveillance system deployed across distant\nscenes.To handle this challenging problem, a new pipeline is introduced by\nsynthesizing the cross-camera samples in the feature space for model training.\nSpecifically, the feature encoder and generator are end-to-end optimized under\na novel method, Camera-Conditioned Stable Feature Generation (CCSFG). Its joint\nlearning procedure raises concern on the stability of generative model\ntraining. Therefore, a new feature generator, $\\sigma$-Regularized Conditional\nVariational Autoencoder ($\\sigma$-Reg.~CVAE), is proposed with theoretical and\nexperimental analysis on its robustness. Extensive experiments on two ISCS\nperson Re-ID datasets demonstrate the superiority of our CCSFG to the\ncompetitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wenhang Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ancong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaobin Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affine Medical Image Registration with Coarse-to-Fine Vision Transformer. (arXiv:2203.15216v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15216","description":"<p>Affine registration is indispensable in a comprehensive medical image\nregistration pipeline. However, only a few studies focus on fast and robust\naffine registration algorithms. Most of these studies utilize convolutional\nneural networks (CNNs) to learn joint affine and non-parametric registration,\nwhile the standalone performance of the affine subnetwork is less explored.\nMoreover, existing CNN-based affine registration approaches focus either on the\nlocal misalignment or the global orientation and position of the input to\npredict the affine transformation matrix, which are sensitive to spatial\ninitialization and exhibit limited generalizability apart from the training\ndataset. In this paper, we present a fast and robust learning-based algorithm,\nCoarse-to-Fine Vision Transformer (C2FViT), for 3D affine medical image\nregistration. Our method naturally leverages the global connectivity and\nlocality of the convolutional vision transformer and the multi-resolution\nstrategy to learn the global affine registration. We evaluate our method on 3D\nbrain atlas registration and template-matching normalization. Comprehensive\nresults demonstrate that our method is superior to the existing CNNs-based\naffine registration methods in terms of registration accuracy, robustness and\ngeneralizability while preserving the runtime advantage of the learning-based\nmethods. The source code is available at https://github.com/cwmok/C2FViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Could Be Better Than All: Feature Sampling and Grouping for Scene Text Detection. (arXiv:2203.15221v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15221","description":"<p>Recently, transformer-based methods have achieved promising progresses in\nobject detection, as they can eliminate the post-processes like NMS and enrich\nthe deep representations. However, these methods cannot well cope with scene\ntext due to its extreme variance of scales and aspect ratios. In this paper, we\npresent a simple yet effective transformer-based architecture for scene text\ndetection. Different from previous approaches that learn robust deep\nrepresentations of scene text in a holistic manner, our method performs scene\ntext detection based on a few representative features, which avoids the\ndisturbance by background and reduces the computational cost. Specifically, we\nfirst select a few representative features at all scales that are highly\nrelevant to foreground text. Then, we adopt a transformer for modeling the\nrelationship of the sampled features, which effectively divides them into\nreasonable groups. As each feature group corresponds to a text instance, its\nbounding box can be easily obtained without any post-processing operation.\nUsing the basic feature pyramid network for feature extraction, our method\nconsistently achieves state-of-the-art results on several popular datasets for\nscene text detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jingqun Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongye Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">MingKun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1\">Guanglong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic NeRF: 3D-to-2D Label Transfer for Panoptic Urban Scene Segmentation. (arXiv:2203.15224v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15224","description":"<p>Large-scale training data with high-quality annotations is critical for\ntraining semantic and instance segmentation models. Unfortunately, pixel-wise\nannotation is labor-intensive and costly, raising the demand for more efficient\nlabeling strategies. In this work, we present a novel 3D-to-2D label transfer\nmethod, Panoptic NeRF, which aims for obtaining per-pixel 2D semantic and\ninstance labels from easy-to-obtain coarse 3D bounding primitives. Our method\nutilizes NeRF as a differentiable tool to unify coarse 3D annotations and 2D\nsemantic cues transferred from existing datasets. We demonstrate that this\ncombination allows for improved geometry guided by semantic information,\nenabling rendering of accurate semantic maps across multiple views.\nFurthermore, this fusion process resolves label ambiguity of the coarse 3D\nannotations and filters noise in the 2D predictions. By inferring in 3D space\nand rendering to 2D labels, our 2D semantic and instance labels are multi-view\nconsistent by design. Experimental results show that Panoptic NeRF outperforms\nexisting semantic and instance label transfer methods in terms of accuracy and\nmulti-view consistency on challenging urban scenes of the KITTI-360 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shangzhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianrun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yichong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lanyun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yiyi Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation. (arXiv:2203.15227v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15227","description":"<p>Multi-frame human pose estimation has long been a compelling and fundamental\nproblem in computer vision. This task is challenging due to fast motion and\npose occlusion that frequently occur in videos. State-of-the-art methods strive\nto incorporate additional visual evidences from neighboring frames (supporting\nframes) to facilitate the pose estimation of the current frame (key frame). One\naspect that has been obviated so far, is the fact that current methods directly\naggregate unaligned contexts across frames. The spatial-misalignment between\npose features of the current frame and neighboring frames might lead to\nunsatisfactory results. More importantly, existing approaches build upon the\nstraightforward pose estimation loss, which unfortunately cannot constrain the\nnetwork to fully leverage useful information from neighboring frames. To tackle\nthese problems, we present a novel hierarchical alignment framework, which\nleverages coarse-to-fine deformations to progressively update a neighboring\nframe to align with the current frame at the feature level. We further propose\nto explicitly supervise the knowledge extraction from neighboring frames,\nguaranteeing that useful complementary cues are extracted. To achieve this\ngoal, we theoretically analyzed the mutual information between the frames and\narrived at a loss that maximizes the task-relevant mutual information. These\nallow us to rank No.1 in the Multi-frame Person Pose Estimation Challenge on\nbenchmark dataset PoseTrack2017, and obtain state-of-the-art performance on\nbenchmarks Sub-JHMDB and Pose-Track2018. Our code is released at\nhttps://github. com/Pose-Group/FAMI-Pose, hoping that it will be useful to the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Runyang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yixing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHOP: A Deep Learning Based Pipeline for near Real-Time Detection of Small Handheld Objects Present in Blurry Video. (arXiv:2203.15228v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15228","description":"<p>While prior works have investigated and developed computational models\ncapable of object detection, models still struggle to reliably interpret images\nwith motion blur and small objects. Moreover, none of these models are\nspecifically designed for handheld object detection. In this work, we present\nSHOP (Small Handheld Object Pipeline), a pipeline that reliably and efficiently\ninterprets blurry images containing handheld objects. The specific models used\nin each stage of the pipeline are flexible and can be changed based on\nperformance requirements. First, images are deblurred and then run through a\npose detection system where areas-of-interest are proposed around the hands of\nany people present. Next, object detection is performed on the images by a\nsingle-stage object detector. Finally, the proposed areas-of-interest are used\nto filter out low confidence detections. Testing on a handheld subset of\nMicrosoft Common Objects in Context (MS COCO) demonstrates that this 3 stage\nprocess results in a 70 percent decrease in false positives while only reducing\ntrue positives by 17 percent in its strongest configuration. We also present a\nsubset of MS COCO consisting solely of handheld objects that can be used to\ncontinue the development of handheld object detection methods.\nhttps://github.com/spider-sense/SHOP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1\">Abhinav Ganguly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_A/0/1/0/all/0/1\">Amar C Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_S/0/1/0/all/0/1\">Sylvia E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jeffrey D Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudson_I/0/1/0/all/0/1\">Ian M Hudson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Edge Detection and Deep Learning Based SETI Signal Classification Method. (arXiv:2203.15229v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15229","description":"<p>Scientists at the Berkeley SETI Research Center are Searching for\nExtraterrestrial Intelligence (SETI) by a new signal detection method that\nconverts radio signals into spectrograms through Fourier transforms and\nclassifies signals represented by two-dimensional time-frequency spectrums,\nwhich successfully converts a signal classification problem into an image\nclassification task. In view of the negative impact of background noises on the\naccuracy of spectrograms classification, a new method is introduced in this\npaper. After Gaussian convolution smoothing the signals, edge detection\nfunctions are applied to detect the edge of the signals and enhance the outline\nof the signals, then the processed spectrograms are used to train the deep\nneural network to compare the classification accuracy of various image\nclassification networks. The results show that the proposed method can\neffectively improve the classification accuracy of SETI spectrums.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhewei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haider_S/0/1/0/all/0/1\">Sami Ahmed Haider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Query Transfer Attacks on Context-Aware Object Detectors. (arXiv:2203.15230v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15230","description":"<p>Adversarial attacks perturb images such that a deep neural network produces\nincorrect classification results. A promising approach to defend against\nadversarial attacks on natural multi-object scenes is to impose a\ncontext-consistency check, wherein, if the detected objects are not consistent\nwith an appropriately defined context, then an attack is suspected. Stronger\nattacks are needed to fool such context-aware detectors. We present the first\napproach for generating context-consistent adversarial attacks that can evade\nthe context-consistency check of black-box object detectors operating on\ncomplex, natural scenes. Unlike many black-box attacks that perform repeated\nattempts and open themselves to detection, we assume a \"zero-query\" setting,\nwhere the attacker has no knowledge of the classification decisions of the\nvictim system. First, we derive multiple attack plans that assign incorrect\nlabels to victim objects in a context-consistent manner. Then we design and use\na novel data structure that we call the perturbation success probability\nmatrix, which enables us to filter the attack plans and choose the one most\nlikely to succeed. This final attack plan is implemented using a\nperturbation-bounded adversarial attack algorithm. We compare our zero-query\nattack against a few-query scheme that repeatedly checks if the victim system\nis fooled. We also compare against state-of-the-art context-agnostic attacks.\nAgainst a context-aware defense, the fooling rate of our zero-query approach is\nsignificantly higher than context-agnostic approaches and higher than that\nachievable with up to three rounds of the few-query scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zikui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rane_S/0/1/0/all/0/1\">Shantanu Rane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brito_A/0/1/0/all/0/1\">Alejandro E. Brito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth V. Krishnamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoPoly: Predicting a Polygonal Mesh Construction Sequence from a Silhouette Image. (arXiv:2203.15233v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15233","description":"<p>Polygonal modeling is a core task of content creation in Computer Graphics.\nThe complexity of modeling, in terms of the number and the order of operations\nand time required to execute them makes it challenging to learn and execute.\nOur goal is to automatically derive a polygonal modeling sequence for a given\ntarget. Then, one can learn polygonal modeling by observing the resulting\nsequence and also expedite the modeling process by starting from the\nauto-generated result. As a starting point for building a system for 3D\nmodeling in the future, we tackle the 2D shape modeling problem and present\nAutoPoly, a hybrid method that generates a polygonal mesh construction sequence\nfrom a silhouette image. The key idea of our method is the use of the Monte\nCarlo tree search (MCTS) algorithm and differentiable rendering to separately\npredict sequential topological actions and geometric actions. Our hybrid method\ncan alter topology, whereas the recently proposed inverse shape estimation\nmethods using differentiable rendering can only handle a fixed topology. Our\nnovel reward function encourages MCTS to select topological actions that lead\nto a simpler shape without self-intersection. We further designed two deep\nlearning-based methods to improve the expansion and simulation steps in the\nMCTS search process: an $n$-step \"future action prediction\" network (nFAP-Net)\nto generate candidates for potential topological actions, and a shape warping\nnetwork (WarpNet) to predict polygonal shapes given the predicted rendered\nimages and topological actions. We demonstrate the efficiency of our method on\n2D polygonal shapes of multiple man-made object categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_I/0/1/0/all/0/1\">I-Chao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu Ju Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaick_O/0/1/0/all/0/1\">Oliver van Kaick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_T/0/1/0/all/0/1\">Takeo Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Equivariance Allows Handling Multiple Nuisance Variables When Analyzing Pooled Neuroimaging Datasets. (arXiv:2203.15234v1 [cs.LG])","link":"http://arxiv.org/abs/2203.15234","description":"<p>Pooling multiple neuroimaging datasets across institutions often enables\nimprovements in statistical power when evaluating associations (e.g., between\nrisk factors and disease outcomes) that may otherwise be too weak to detect.\nWhen there is only a {\\em single} source of variability (e.g., different\nscanners), domain adaptation and matching the distributions of representations\nmay suffice in many scenarios. But in the presence of {\\em more than one}\nnuisance variable which concurrently influence the measurements, pooling\ndatasets poses unique challenges, e.g., variations in the data can come from\nboth the acquisition method as well as the demographics of participants\n(gender, age). Invariant representation learning, by itself, is ill-suited to\nfully model the data generation process. In this paper, we show how bringing\nrecent results on equivariant representation learning (for studying symmetries\nin neural networks) instantiated on structured spaces together with simple use\nof classical results on causal inference provides an effective practical\nsolution. In particular, we demonstrate how our model allows dealing with more\nthan one nuisance variable under some assumptions and can enable analysis of\npooled scientific datasets in scenarios that would otherwise entail removing a\nlarge portion of the samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lokhande_V/0/1/0/all/0/1\">Vishnu Suresh Lokhande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Rudrasis Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1\">Sathya N. Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vikas Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pop-Out Motion: 3D-Aware Image Deformation via Learning the Shape Laplacian. (arXiv:2203.15235v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15235","description":"<p>We propose a framework that can deform an object in a 2D image as it exists\nin 3D space. Most existing methods for 3D-aware image manipulation are limited\nto (1) only changing the global scene information or depth, or (2) manipulating\nan object of specific categories. In this paper, we present a 3D-aware image\ndeformation method with minimal restrictions on shape category and deformation\ntype. While our framework leverages 2D-to-3D reconstruction, we argue that\nreconstruction is not sufficient for realistic deformations due to the\nvulnerability to topological errors. Thus, we propose to take a supervised\nlearning-based approach to predict the shape Laplacian of the underlying volume\nof a 3D reconstruction represented as a point cloud. Given the deformation\nenergy calculated using the predicted shape Laplacian and user-defined\ndeformation handles (e.g., keypoints), we obtain bounded biharmonic weights to\nmodel plausible handle-based image deformation. In the experiments, we present\nour results of deforming 2D character and clothed human images. We also\nquantitatively show that our approach can produce more accurate deformation\nweights compared to alternative methods (i.e., mesh reconstruction and point\ncloud Laplacian methods).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jihyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1\">Minhyuk Sung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyunjin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Kyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Image-to-Image Translation using Latent Space Mapping. (arXiv:2203.15241v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15241","description":"<p>Recent image-to-image translation works have been transferred from supervised\nto unsupervised settings due to the expensive cost of capturing or labeling\nlarge amounts of paired data. However, current unsupervised methods using the\ncycle-consistency constraint may not find the desired mapping, especially for\ndifficult translation tasks. On the other hand, a small number of paired data\nare usually accessible. We therefore introduce a general framework for\nsemi-supervised image translation. Unlike previous works, our main idea is to\nlearn the translation over the latent feature space instead of the image space.\nThanks to the low dimensional feature space, it is easier to find the desired\nmapping function, resulting in improved quality of translation results as well\nas the stability of the translation model. Empirically we show that using\nfeature translation generates better results, even using a few bits of paired\ndata. Experimental comparisons with state-of-the-art approaches demonstrate the\neffectiveness of the proposed framework on a variety of challenging\nimage-to-image translation tasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Image Transformers using Learnable Memory. (arXiv:2203.15243v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15243","description":"<p>In this paper we propose augmenting Vision Transformer models with learnable\nmemory tokens. Our approach allows the model to adapt to new tasks, using few\nparameters, while optionally preserving its capabilities on previously learned\ntasks. At each layer we introduce a set of learnable embedding vectors that\nprovide contextual information useful for specific datasets. We call these\n\"memory tokens\". We show that augmenting a model with just a handful of such\ntokens per layer significantly improves accuracy when compared to conventional\nhead-only fine-tuning, and performs only slightly below the significantly more\nexpensive full fine-tuning. We then propose an attention-masking approach that\nenables extension to new downstream tasks, with a computation reuse. In this\nsetup in addition to being parameters efficient, models can execute both old\nand new tasks as a part of single inference at a small incremental cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sandler_M/0/1/0/all/0/1\">Mark Sandler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhmoginov_A/0/1/0/all/0/1\">Andrey Zhmoginov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vladymyrov_M/0/1/0/all/0/1\">Max Vladymyrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jackson_A/0/1/0/all/0/1\">Andrew Jackson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Structured Declarative Classifiers for 3D Point Clouds: Defending Adversarial Attacks with Implicit Gradients. (arXiv:2203.15245v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15245","description":"<p>Deep neural networks for 3D point cloud classification, such as PointNet,\nhave been demonstrated to be vulnerable to adversarial attacks. Current\nadversarial defenders often learn to denoise the (attacked) point clouds by\nreconstruction, and then feed them to the classifiers as input. In contrast to\nthe literature, we propose a family of robust structured declarative\nclassifiers for point cloud classification, where the internal constrained\noptimization mechanism can effectively defend adversarial attacks through\nimplicit gradients. Such classifiers can be formulated using a bilevel\noptimization framework. We further propose an effective and efficient\ninstantiation of our approach, namely, Lattice Point Classifier (LPC), based on\nstructured sparse coding in the permutohedral lattice and 2D convolutional\nneural networks (CNNs) that is end-to-end trainable. We demonstrate\nstate-of-the-art robust point cloud classification performance on ModelNet40\nand ScanNet under seven different attackers. For instance, we achieve 89.51%\nand 83.16% test accuracy on each dataset under the recent JGBA attacker that\noutperforms DUP-Net and IF-Defense with PointNet by ~70%. Demo code is\navailable at https://zhang-vislab.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cuncong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Intra- and Inter-Video Relation for Surgical Semantic Scene Segmentation. (arXiv:2203.15251v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15251","description":"<p>Automatic surgical scene segmentation is fundamental for facilitating\ncognitive intelligence in the modern operating theatre. Previous works rely on\nconventional aggregation modules (e.g., dilated convolution, convolutional\nLSTM), which only make use of the local context. In this paper, we propose a\nnovel framework STswinCL that explores the complementary intra- and inter-video\nrelations to boost segmentation performance, by progressively capturing the\nglobal context. We firstly develop a hierarchy Transformer to capture\nintra-video relation that includes richer spatial and temporal cues from\nneighbor pixels and previous frames. A joint space-time window shift scheme is\nproposed to efficiently aggregate these two cues into each pixel embedding.\nThen, we explore inter-video relation via pixel-to-pixel contrastive learning,\nwhich well structures the global embedding space. A multi-source contrast\ntraining objective is developed to group the pixel embeddings across videos\nwith the ground-truth guidance, which is crucial for learning the global\nproperty of the whole data. We extensively validate our approach on two public\nsurgical video benchmarks, including EndoVis18 Challenge and CaDIS dataset.\nExperimental results demonstrate the promising performance of our method, which\nconsistently exceeds previous state-of-the-art approaches. Code will be\navailable at https://github.com/YuemingJin/STswinCL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1\">Danail Stoyanov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identification and classification of exfoliated graphene flakes from microscopy images using a hierarchical deep convolutional neural network. (arXiv:2203.15252v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15252","description":"<p>Identification of the mechanically exfoliated graphene flakes and\nclassification of the thickness is important in the nanomanufacturing of\nnext-generation materials and devices that overcome the bottleneck of Moore's\nLaw. Currently, identification and classification of exfoliated graphene flakes\nare conducted by human via inspecting the optical microscope images. The\nexisting state-of-the-art automatic identification by machine learning is not\nable to accommodate images with different backgrounds while different\nbackgrounds are unavoidable in experiments. This paper presents a deep learning\nmethod to automatically identify and classify the thickness of exfoliated\ngraphene flakes on Si/SiO2 substrates from optical microscope images with\nvarious settings and background colors. The presented method uses a\nhierarchical deep convolutional neural network that is capable of learning new\nimages while preserving the knowledge from previous images. The deep learning\nmodel was trained and used to classify exfoliated graphene flakes into\nmonolayer (1L), bi-layer (2L), tri-layer (3L), four-to-six-layer (4-6L),\nseven-to-ten-layer (7-10L), and bulk categories. Compared with existing machine\nlearning methods, the presented method possesses high accuracy and efficiency\nas well as robustness to the backgrounds and resolutions of images. The results\nindicated that our deep learning model has accuracy as high as 99% in\nidentifying and classifying exfoliated graphene flakes. This research will shed\nlight on scaled-up manufacturing and characterization of graphene for advanced\nmaterials and devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahjoubi_S/0/1/0/all/0/1\">Soroush Mahjoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1\">Weina Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Reflectance Capture with a Deep Gated Mixture-of-Experts. (arXiv:2203.15258v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15258","description":"<p>We present a novel framework to efficiently acquire near-planar anisotropic\nreflectance in a pixel-independent fashion, using a deep gated\nmixtureof-experts. While existing work employs a unified network to handle all\npossible input, our network automatically learns to condition on the input for\nenhanced reconstruction. We train a gating module to select one out of a number\nof specialized decoders for reflectance reconstruction, based on photometric\nmeasurements, essentially trading generality for quality. A common, pre-trained\nlatent transform module is also appended to each decoder, to offset the burden\nof the increased number of decoders. In addition, the illumination conditions\nduring acquisition can be jointly optimized. The effectiveness of our framework\nis validated on a wide variety of challenging samples using a near-field\nlightstage. Compared with the state-of-the-art technique, our results are\nimproved at the same input bandwidth, and our bandwidth can be reduced to about\n1/3 for equal-quality results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaohe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yaxin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eigencontours: Novel Contour Descriptors Based on Low-Rank Approximation. (arXiv:2203.15259v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15259","description":"<p>Novel contour descriptors, called eigencontours, based on low-rank\napproximation are proposed in this paper. First, we construct a contour matrix\ncontaining all object boundaries in a training set. Second, we decompose the\ncontour matrix into eigencontours via the best rank-M approximation. Third, we\nrepresent an object boundary by a linear combination of the M eigencontours. We\nalso incorporate the eigencontours into an instance segmentation framework.\nExperimental results demonstrate that the proposed eigencontours can represent\nobject boundaries more effectively and more efficiently than existing\ndescriptors in a low-dimensional space. Furthermore, the proposed algorithm\nyields meaningful performances on instance segmentation datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Wonhui Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dongkwon Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Multi-Class Tiny-Object Detection. (arXiv:2203.15266v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15266","description":"<p>Annotating tens or hundreds of tiny objects in a given image is laborious yet\ncrucial for a multitude of Computer Vision tasks. Such imagery typically\ncontains objects from various categories, yet the multi-class interactive\nannotation setting for the detection task has thus far been unexplored. To\naddress these needs, we propose a novel interactive annotation method for\nmultiple instances of tiny objects from multiple classes, based on a few\npoint-based user inputs. Our approach, C3Det, relates the full image context\nwith annotator inputs in a local and global manner via late-fusion and\nfeature-correlation, respectively. We perform experiments on the Tiny-DOTA and\nLCell datasets using both two-stage and one-stage object detection\narchitectures to verify the efficacy of our approach. Our approach outperforms\nexisting approaches in interactive annotation, achieving higher mAP with fewer\nclicks. Furthermore, we validate the annotation efficiency of our approach in a\nuser study where it is shown to be 2.85x faster and yield only 0.36x task load\n(NASA-TLX, lower is better) compared to manual annotation. The code is\navailable at\nhttps://github.com/ChungYi347/Interactive-Multi-Class-Tiny-Object-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chunggi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seonwook Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Heon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1\">Jeongun Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sanghoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Haejoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_S/0/1/0/all/0/1\">S&#xe9;rgio Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_D/0/1/0/all/0/1\">Donggeun Yoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformers in Medical Computer Vision -- A Contemplative Retrospection. (arXiv:2203.15269v1 [eess.IV])","link":"http://arxiv.org/abs/2203.15269","description":"<p>Recent escalation in the field of computer vision underpins a huddle of\nalgorithms with the magnificent potential to unravel the information contained\nwithin images. These computer vision algorithms are being practised in medical\nimage analysis and are transfiguring the perception and interpretation of\nImaging data. Among these algorithms, Vision Transformers are evolved as one of\nthe most contemporary and dominant architectures that are being used in the\nfield of computer vision. These are immensely utilized by a plenty of\nresearchers to perform new as well as former experiments. Here, in this article\nwe investigate the intersection of Vision Transformers and Medical images and\nproffered an overview of various ViTs based frameworks that are being used by\ndifferent researchers in order to decipher the obstacles in Medical Computer\nVision. We surveyed the application of Vision transformers in different areas\nof medical computer vision such as image-based disease classification,\nanatomical structure segmentation, registration, region-based lesion Detection,\ncaptioning, report generation, reconstruction using multiple medical imaging\nmodalities that greatly assist in medical diagnosis and hence treatment\nprocess. Along with this, we also demystify several imaging modalities used in\nMedical Computer Vision. Moreover, to get more insight and deeper\nunderstanding, self-attention mechanism of transformers is also explained\nbriefly. Conclusively, we also put some light on available data sets, adopted\nmethodology, their performance measures, challenges and their solutions in form\nof discussion. We hope that this review article will open future directions for\nresearchers in medical computer vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Parvaiz_A/0/1/0/all/0/1\">Arshi Parvaiz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalid_M/0/1/0/all/0/1\">Muhammad Anwaar Khalid</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zafar_R/0/1/0/all/0/1\">Rukhsana Zafar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ameer_H/0/1/0/all/0/1\">Huma Ameer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1\">Muhammad Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fraz_M/0/1/0/all/0/1\">Muhammad Moazam Fraz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MAT: Mask-Aware Transformer for Large Hole Image Inpainting. (arXiv:2203.15270v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15270","description":"<p>Recent studies have shown the importance of modeling long-range interactions\nin the inpainting problem. To achieve this goal, existing approaches exploit\neither standalone attention techniques or transformers, but usually under a low\nresolution in consideration of computational cost. In this paper, we present a\nnovel transformer-based model for large hole inpainting, which unifies the\nmerits of transformers and convolutions to efficiently process high-resolution\nimages. We carefully design each component of our framework to guarantee the\nhigh fidelity and diversity of recovered images. Specifically, we customize an\ninpainting-oriented transformer block, where the attention module aggregates\nnon-local information only from partial valid tokens, indicated by a dynamic\nmask. Extensive experiments demonstrate the state-of-the-art performance of the\nnew model on multiple benchmark datasets. Code is released at\nhttps://github.com/fenglinglwb/MAT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenbo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Image based Navigation Architecture to Mitigate the need of precise Localization in Mobile Robots. (arXiv:2203.15272v1 [cs.RO])","link":"http://arxiv.org/abs/2203.15272","description":"<p>Traditional simultaneous localization and mapping (SLAM) methods focus on\nimprovement in the robot's localization under environment and sensor\nuncertainty. This paper, however, focuses on mitigating the need for exact\nlocalization of a mobile robot to pursue autonomous navigation using a sparse\nset of images. The proposed method consists of a model architecture - RoomNet,\nfor unsupervised learning resulting in a coarse identification of the\nenvironment and a separate local navigation policy for local identification and\nnavigation. The former learns and predicts the scene based on the short term\nimage sequences seen by the robot along with the transition image scenarios\nusing long term image sequences. The latter uses sparse image matching to\ncharacterise the similarity of frames achieved vis-a-vis the frames viewed by\nthe robot during the mapping and training stage. A sparse graph of the image\nsequence is created which is then used to carry out robust navigation purely on\nthe basis of visual goals. The proposed approach is evaluated on two robots in\na test environment and demonstrates the ability to navigate in dynamic\nenvironments where landmarks are obscured and classical localization methods\nfail.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Pranay Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rajesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_S/0/1/0/all/0/1\">Sarthak Upadhyay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Line Detection Using Mirror Attention and Comparative Ranking and Matching. (arXiv:2203.15285v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15285","description":"<p>A novel algorithm to detect semantic lines is proposed in this paper. We\ndevelop three networks: detection network with mirror attention (D-Net) and\ncomparative ranking and matching networks (R-Net and M-Net). D-Net extracts\nsemantic lines by exploiting rich contextual information. To this end, we\ndesign the mirror attention module. Then, through pairwise comparisons of\nextracted semantic lines, we iteratively select the most semantic line and\nremove redundant ones overlapping with the selected one. For the pairwise\ncomparisons, we develop R-Net and M-Net in the Siamese architecture.\nExperiments demonstrate that the proposed algorithm outperforms the\nconventional semantic line detector significantly. Moreover, we apply the\nproposed algorithm to detect two important kinds of semantic lines\nsuccessfully: dominant parallel lines and reflection symmetry axes. Our codes\nare available at https://github.com/dongkwonjin/Semantic-Line-DRM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dongkwon Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jun-Tae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Adaptation for Self-Supervised 3D Human Pose Estimation. (arXiv:2203.15293v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15293","description":"<p>The advances in monocular 3D human pose estimation are dominated by\nsupervised techniques that require large-scale 2D/3D pose annotations. Such\nmethods often behave erratically in the absence of any provision to discard\nunfamiliar out-of-distribution data. To this end, we cast the 3D human pose\nlearning as an unsupervised domain adaptation problem. We introduce MRP-Net\nthat constitutes a common deep network backbone with two output heads\nsubscribing to two diverse configurations; a) model-free joint localization and\nb) model-based parametric regression. Such a design allows us to derive\nsuitable measures to quantify prediction uncertainty at both pose and joint\nlevel granularity. While supervising only on labeled synthetic samples, the\nadaptation process aims to minimize the uncertainty for the unlabeled target\nimages while maximizing the same for an extreme out-of-distribution dataset\n(backgrounds). Alongside synthetic-to-real 3D pose adaptation, the\njoint-uncertainties allow expanding the adaptation to work on in-the-wild\nimages even in the presence of occlusion and truncation scenarios. We present a\ncomprehensive evaluation of the proposed approach and demonstrate\nstate-of-the-art performance on benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kundu_J/0/1/0/all/0/1\">Jogendra Nath Kundu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_S/0/1/0/all/0/1\">Siddharth Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+YM_P/0/1/0/all/0/1\">Pradyumna YM</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1\">Varun Jampani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_A/0/1/0/all/0/1\">Anirban Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernel Modulation: A Parameter-Efficient Method for Training Convolutional Neural Networks. (arXiv:2203.15297v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15297","description":"<p>Deep Neural Networks, particularly Convolutional Neural Networks (ConvNets),\nhave achieved incredible success in many vision tasks, but they usually require\nmillions of parameters for good accuracy performance. With increasing\napplications that use ConvNets, updating hundreds of networks for multiple\ntasks on an embedded device can be costly in terms of memory, bandwidth, and\nenergy. Approaches to reduce this cost include model compression and\nparameter-efficient models that adapt a subset of network layers for each new\ntask. This work proposes a novel parameter-efficient kernel modulation (KM)\nmethod that adapts all parameters of a base network instead of a subset of\nlayers. KM uses lightweight task-specialized kernel modulators that require\nonly an additional 1.4% of the base network parameters. With multiple tasks,\nonly the task-specialized KM weights are communicated and stored on the\nend-user device. We applied this method in training ConvNets for Transfer\nLearning and Meta-Learning scenarios. Our results show that KM delivers up to\n9% higher accuracy than other parameter-efficient methods on the Transfer\nLearning benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuhuang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eigenlanes: Data-Driven Lane Descriptors for Structurally Diverse Lanes. (arXiv:2203.15302v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15302","description":"<p>A novel algorithm to detect road lanes in the eigenlane space is proposed in\nthis paper. First, we introduce the notion of eigenlanes, which are data-driven\ndescriptors for structurally diverse lanes, including curved, as well as\nstraight, lanes. To obtain eigenlanes, we perform the best rank-M approximation\nof a lane matrix containing all lanes in a training set. Second, we generate a\nset of lane candidates by clustering the training lanes in the eigenlane space.\nThird, using the lane candidates, we determine an optimal set of lanes by\ndeveloping an anchor-based detection network, called SIIC-Net. Experimental\nresults demonstrate that the proposed algorithm provides excellent detection\nperformance for structurally diverse lanes. Our codes are available at\nhttps://github.com/dongkwonjin/Eigenlanes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Dongkwon Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Wonhui Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Seong-Gyun Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1\">Heeyeon Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-based Point Cloud Registration for 6D Object Pose Estimation in the Real World. (arXiv:2203.15309v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15309","description":"<p>In this work, we tackle the task of estimating the 6D pose of an object from\npoint cloud data. While recent learning-based approaches to addressing this\ntask have shown great success on synthetic datasets, we have observed them to\nfail in the presence of real-world data. We thus analyze the causes of these\nfailures, which we trace back to the difference between the feature\ndistributions of the source and target point clouds, and the sensitivity of the\nwidely-used SVD-based loss function to the range of rotation between the two\npoint clouds. We address the first challenge by introducing a new normalization\nstrategy, Match Normalization, and the second via the use of a loss function\nbased on the negative log likelihood of point correspondences. Our two\ncontributions are general and can be applied to many existing learning-based 3D\nobject registration frameworks, which we illustrate by implementing them in two\nof them, DCP and IDAM. Our experiments on the real-scene TUD-L, LINEMOD and\nOccluded-LINEMOD datasets evidence the benefits of our strategies. They allow\nfor the first time learning-based 3D object registration methods to achieve\nmeaningful results on real-world data. We therefore expect them to be key to\nthe future development of point cloud registration methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dang_Z/0/1/0/all/0/1\">Zheng Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Routing Transformer for Zero-Shot Learning. (arXiv:2203.15310v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15310","description":"<p>Zero-shot learning (ZSL) aims to learn models that can recognize unseen image\nsemantics based on the training of data with seen semantics. Recent studies\neither leverage the global image features or mine discriminative local patch\nfeatures to associate the extracted visual features to the semantic attributes.\nHowever, due to the lack of the necessary top-down guidance and semantic\nalignment for ensuring the model attending to the real attribute-correlation\nregions, these methods still encounter a significant semantic gap between the\nvisual modality and the attribute modality, which makes their prediction on\nunseen semantics unreliable. To solve this problem, this paper establishes a\nnovel transformer encoder-decoder model, called hybrid routing transformer\n(HRT). In HRT encoder, we embed an active attention, which is constructed by\nboth the bottom-up and the top-down dynamic routing pathways to generate the\nattribute-aligned visual feature. While in HRT decoder, we use static routing\nto calculate the correlation among the attribute-aligned visual features, the\ncorresponding attribute semantics, and the class attribute vectors to generate\nthe final class label predictions. This design makes the presented transformer\nmodel a hybrid of 1) top-down and bottom-up attention pathways and 2) dynamic\nand static routing pathways. Comprehensive experiments on three widely-used\nbenchmark datasets, namely CUB, SUN, and AWA2, are conducted. The obtained\nexperimental results demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">De Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gerong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"In-N-Out Generative Learning for Dense Unsupervised Video Segmentation. (arXiv:2203.15312v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15312","description":"<p>In this paper, we focus on the unsupervised Video Object Segmentation (VOS)\ntask which learns visual correspondence from unlabeled videos. Previous methods\nare mainly based on the contrastive learning paradigm, which optimize either in\npixel level or image level and show unsatisfactory scalability. Image-level\noptimization learns pixel-wise information implicitly therefore is sub-optimal\nfor such dense prediction task, while pixel-level optimization ignores the\nhigh-level semantic scope for capturing object deformation. To complementarily\nlearn these two levels of information in an unified framework, we propose the\nIn-aNd-Out (INO) generative learning from a purely generative perspective,\nwhich captures both high-level and fine-grained semantics by leveraging the\nstructural superiority of Vision Transformer (ViT) and achieves better\nscalability. Specifically, the in-generative learning recovers the corrupted\nparts of an image via inferring its fine-grained semantic structure, while the\nout-generative learning captures high-level semantics by imagining the global\ninformation of an image given only random fragments. To better discover the\ntemporal information, we additionally force the inter-frame consistency from\nboth feature level and affinity matrix level. Extensive experiments on\nDAVIS-2017 val and YouTube-VOS 2018 val show that our INO outperforms previous\nstate-of-the-art methods by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xiao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peike Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiling Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modality High-Frequency Transformer for MR Image Super-Resolution. (arXiv:2203.15314v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15314","description":"<p>Improving the resolution of magnetic resonance (MR) image data is critical to\ncomputer-aided diagnosis and brain function analysis. Higher resolution helps\nto capture more detailed content, but typically induces to lower\nsignal-to-noise ratio and longer scanning time. To this end, MR image\nsuper-resolution has become a widely-interested topic in recent times. Existing\nworks establish extensive deep models with the conventional architectures based\non convolutional neural networks (CNN). In this work, to further advance this\nresearch field, we make an early effort to build a Transformer-based MR image\nsuper-resolution framework, with careful designs on exploring valuable domain\nprior knowledge. Specifically, we consider two-fold domain priors including the\nhigh-frequency structure prior and the inter-modality context prior, and\nestablish a novel Transformer architecture, called Cross-modality\nhigh-frequency Transformer (Cohf-T), to introduce such priors into\nsuper-resolving the low-resolution (LR) MR images. Comprehensive experiments on\ntwo datasets indicate that Cohf-T achieves new state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lechao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agreement or Disagreement in Noise-tolerant Mutual Learning?. (arXiv:2203.15317v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15317","description":"<p>Deep learning has made many remarkable achievements in many fields but\nsuffers from noisy labels in datasets. The state-of-the-art learning with noisy\nlabel method Co-teaching and Co-teaching+ confronts the noisy label by\nmutual-information between dual-network. However, the dual network always tends\nto convergent which would weaken the dual-network mechanism to resist the noisy\nlabels. In this paper, we proposed a noise-tolerant framework named MLC in an\nend-to-end manner. It adjusts the dual-network with divergent regularization to\nensure the effectiveness of the mechanism. In addition, we correct the label\ndistribution according to the agreement between dual-networks. The proposed\nmethod can utilize the noisy data to improve the accuracy, generalization, and\nrobustness of the network. We test the proposed method on the simulate noisy\ndataset MNIST, CIFAR-10, and the real-world noisy dataset Clothing1M. The\nexperimental result shows that our method outperforms the previous\nstate-of-the-art method. Besides, our method is network-free thus it is\napplicable to many tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiarun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daguang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yukun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruirui Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dressing in the Wild by Watching Dance Videos. (arXiv:2203.15320v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15320","description":"<p>While significant progress has been made in garment transfer, one of the most\napplicable directions of human-centric image generation, existing works\noverlook the in-the-wild imagery, presenting severe garment-person misalignment\nas well as noticeable degradation in fine texture details. This paper,\ntherefore, attends to virtual try-on in real-world scenes and brings essential\nimprovements in authenticity and naturalness especially for loose garment\n(e.g., skirts, formal dresses), challenging poses (e.g., cross arms, bent\nlegs), and cluttered backgrounds. Specifically, we find that the pixel flow\nexcels at handling loose garments whereas the vertex flow is preferred for hard\nposes, and by combining their advantages we propose a novel generative network\ncalled wFlow that can effectively push up garment transfer to in-the-wild\ncontext. Moreover, former approaches require paired images for training.\nInstead, we cut down the laboriousness by working on a newly constructed\nlarge-scale video dataset named Dance50k with self-supervised cross-frame\ntraining and an online cycle optimization. The proposed Dance50k can boost\nreal-world virtual dressing by covering a wide variety of garments under\ndancing poses. Extensive experiments demonstrate the superiority of our wFlow\nin generating realistic garment transfer results for in-the-wild images without\nresorting to expensive paired datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xijin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Daniel K. Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1\">Xiang Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianchao Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Single Image Dehazing Based on Consistent and Contrast-Assisted Reconstruction. (arXiv:2203.15325v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15325","description":"<p>Single image dehazing as a fundamental low-level vision task, is essential\nfor the development of robust intelligent surveillance system. In this paper,\nwe make an early effort to consider dehazing robustness under variational haze\ndensity, which is a realistic while under-studied problem in the research filed\nof singe image dehazing. To properly address this problem, we propose a novel\ndensity-variational learning framework to improve the robustness of the image\ndehzing model assisted by a variety of negative hazy images, to better deal\nwith various complex hazy scenarios. Specifically, the dehazing network is\noptimized under the consistency-regularized framework with the proposed\nContrast-Assisted Reconstruction Loss (CARL). The CARL can fully exploit the\nnegative information to facilitate the traditional positive-orient dehazing\nobjective function, by squeezing the dehazed image to its clean target from\ndifferent directions. Meanwhile, the consistency regularization keeps\nconsistent outputs given multi-level hazy images, thus improving the model\nrobustness. Extensive experimental results on two synthetic and three\nreal-world datasets demonstrate that our method significantly surpasses the\nstate-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1\">De Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiande Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN Filter DB: An Empirical Investigation of Trained Convolutional Filters. (arXiv:2203.15331v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15331","description":"<p>Currently, many theoretical as well as practically relevant questions towards\nthe transferability and robustness of Convolutional Neural Networks (CNNs)\nremain unsolved. While ongoing research efforts are engaging these problems\nfrom various angles, in most computer vision related cases these approaches can\nbe generalized to investigations of the effects of distribution shifts in image\ndata. In this context, we propose to study the shifts in the learned weights of\ntrained CNN models. Here we focus on the properties of the distributions of\ndominantly used 3x3 convolution filter kernels. We collected and publicly\nprovide a dataset with over 1.4 billion filters from hundreds of trained CNNs,\nusing a wide range of datasets, architectures, and vision tasks. In a first use\ncase of the proposed dataset, we can show highly relevant properties of many\npublicly available pre-trained models for practical applications: I) We analyze\ndistribution shifts (or the lack thereof) between trained filters along\ndifferent axes of meta-parameters, like visual category of the dataset, task,\narchitecture, or layer depth. Based on these results, we conclude that model\npre-training can succeed on arbitrary datasets if they meet size and variance\nconditions. II) We show that many pre-trained models contain degenerated\nfilters which make them less robust and less suitable for fine-tuning on target\napplications.\n</p>\n<p>Data &amp; Project website: https://github.com/paulgavrikov/cnn-filter-db\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gavrikov_P/0/1/0/all/0/1\">Paul Gavrikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1\">Janis Keuper</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balanced Multimodal Learning via On-the-fly Gradient Modulation. (arXiv:2203.15332v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15332","description":"<p>Multimodal learning helps to comprehensively understand the world, by\nintegrating different senses. Accordingly, multiple input modalities are\nexpected to boost model performance, but we actually find that they are not\nfully exploited even when the multimodal model outperforms its uni-modal\ncounterpart. Specifically, in this paper we point out that existing multimodal\ndiscriminative models, in which uniform objective is designed for all\nmodalities, could remain under-optimized uni-modal representations, caused by\nanother dominated modality in some scenarios, e.g., sound in blowing wind\nevent, vision in drawing picture event, etc. To alleviate this optimization\nimbalance, we propose on-the-fly gradient modulation to adaptively control the\noptimization of each modality, via monitoring the discrepancy of their\ncontribution towards the learning objective. Further, an extra Gaussian noise\nthat changes dynamically is introduced to avoid possible generalization drop\ncaused by gradient modulation. As a result, we achieve considerable improvement\nover common fusion methods on different multimodal tasks, and this simple\nstrategy can also boost existing multimodal methods, which illustrates its\nefficacy and versatility. The source code is available at\n\\url{https://github.com/GeWu-Lab/OGM-GE_CVPR2022}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaokang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yake Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_A/0/1/0/all/0/1\">Andong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Di Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AnyFace: Free-style Text-to-Face Synthesis and Manipulation. (arXiv:2203.15334v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15334","description":"<p>Existing text-to-image synthesis methods generally are only applicable to\nwords in the training dataset. However, human faces are so variable to be\ndescribed with limited words. So this paper proposes the first free-style\ntext-to-face method namely AnyFace enabling much wider open world applications\nsuch as metaverse, social media, cosmetics, forensics, etc. AnyFace has a novel\ntwo-stream framework for face image synthesis and manipulation given arbitrary\ndescriptions of the human face. Specifically, one stream performs text-to-face\ngeneration and the other conducts face image reconstruction. Facial text and\nimage features are extracted using the CLIP (Contrastive Language-Image\nPre-training) encoders. And a collaborative Cross Modal Distillation (CMD)\nmodule is designed to align the linguistic and visual features across these two\nstreams. Furthermore, a Diverse Triplet Loss (DT loss) is developed to model\nfine-grained features and improve facial diversity. Extensive experiments on\nMulti-modal CelebA-HQ and CelebAText-HQ demonstrate significant advantages of\nAnyFace over state-of-the-art methods. AnyFace can achieve high-quality,\nhigh-resolution, and high-diversity face synthesis and manipulation results\nwithout any constraints on the number and content of input captions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jianxin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Q/0/1/0/all/0/1\">Qiyao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Min Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Compressed Video Representation Learning for Generic Event Boundary Detection. (arXiv:2203.15336v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15336","description":"<p>Generic event boundary detection aims to localize the generic, taxonomy-free\nevent boundaries that segment videos into chunks. Existing methods typically\nrequire video frames to be decoded before feeding into the network, which\ndemands considerable computational power and storage space. To that end, we\npropose a new end-to-end compressed video representation learning for event\nboundary detection that leverages the rich information in the compressed\ndomain, i.e., RGB, motion vectors, residuals, and the internal group of\npictures (GOP) structure, without fully decoding the video. Specifically, we\nfirst use the ConvNets to extract features of the I-frames in the GOPs. After\nthat, a light-weight spatial-channel compressed encoder is designed to compute\nthe feature representations of the P-frames based on the motion vectors,\nresiduals and representations of their dependent I-frames. A temporal\ncontrastive module is proposed to determine the event boundaries of video\nsequences. To remedy the ambiguities of annotations and speed up the training\nprocess, we use the Gaussian kernel to preprocess the ground-truth event\nboundaries. Extensive experiments conducted on the Kinetics-GEBD dataset\ndemonstrate that the proposed method achieves comparable results to the\nstate-of-the-art methods with $4.5\\times$ faster running speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dexiang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tiejian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrared and Visible Image Fusion via Interactive Compensatory Attention Adversarial Learning. (arXiv:2203.15337v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15337","description":"<p>The existing generative adversarial fusion methods generally concatenate\nsource images and extract local features through convolution operation, without\nconsidering their global characteristics, which tends to produce an unbalanced\nresult and is biased towards the infrared image or visible image. Toward this\nend, we propose a novel end-to-end mode based on generative adversarial\ntraining to achieve better fusion balance, termed as \\textit{interactive\ncompensatory attention fusion network} (ICAFusion). In particular, in the\ngenerator, we construct a multi-level encoder-decoder network with a triple\npath, and adopt infrared and visible paths to provide additional intensity and\ngradient information. Moreover, we develop interactive and compensatory\nattention modules to communicate their pathwise information, and model their\nlong-range dependencies to generate attention maps, which can more focus on\ninfrared target perception and visible detail characterization, and further\nincrease the representation power for feature extraction and feature\nreconstruction. In addition, dual discriminators are designed to identify the\nsimilar distribution between fused result and source images, and the generator\nis optimized to produce a more balanced result. Extensive experiments\nillustrate that our ICAFusion obtains superior fusion performance and better\ngeneralization ability, which precedes other advanced methods in the subjective\nvisual description and objective metric evaluation. Our codes will be public at\n\\url{https://github.com/Zhishe-Wang/ICAFusion}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhishe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenyu Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanlin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiawei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-specific Inconsistency Alignment for Domain Adaptive Object Detection. (arXiv:2203.15345v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15345","description":"<p>Detectors trained with massive labeled data often exhibit dramatic\nperformance degradation in some particular scenarios with data distribution\ngap. To alleviate this problem of domain shift, conventional wisdom typically\nconcentrates solely on reducing the discrepancy between the source and target\ndomains via attached domain classifiers, yet ignoring the difficulty of such\ntransferable features in coping with both classification and localization\nsubtasks in object detection. To address this issue, in this paper, we propose\nTask-specific Inconsistency Alignment (TIA), by developing a new alignment\nmechanism in separate task spaces, improving the performance of the detector on\nboth subtasks. Specifically, we add a set of auxiliary predictors for both\nclassification and localization branches, and exploit their behavioral\ninconsistencies as finer-grained domain-specific measures. Then, we devise\ntask-specific losses to align such cross-domain disagreement of both subtasks.\nBy optimizing them individually, we are able to well approximate the category-\nand boundary-wise discrepancies in each task space, and therefore narrow them\nin a decoupled manner. TIA demonstrates superior results on various scenarios\nto the previous state-of-the-art methods. It is also observed that both the\nclassification and localization capabilities of the detector are sufficiently\nstrengthened, further demonstrating the effectiveness of our TIA method. Code\nand trained models are publicly available at https://github.com/MCG-NJU/TIA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harmonizing Pathological and Normal Pixels for Pseudo-healthy Synthesis. (arXiv:2203.15347v1 [eess.IV])","link":"http://arxiv.org/abs/2203.15347","description":"<p>Synthesizing a subject-specific pathology-free image from a pathological\nimage is valuable for algorithm development and clinical practice. In recent\nyears, several approaches based on the Generative Adversarial Network (GAN)\nhave achieved promising results in pseudo-healthy synthesis. However, the\ndiscriminator (i.e., a classifier) in the GAN cannot accurately identify\nlesions and further hampers from generating admirable pseudo-healthy images. To\naddress this problem, we present a new type of discriminator, the segmentor, to\naccurately locate the lesions and improve the visual quality of pseudo-healthy\nimages. Then, we apply the generated images into medical image enhancement and\nutilize the enhanced results to cope with the low contrast problem existing in\nmedical image segmentation. Furthermore, a reliable metric is proposed by\nutilizing two attributes of label noise to measure the health of synthetic\nimages. Comprehensive experiments on the T2 modality of BraTS demonstrate that\nthe proposed method substantially outperforms the state-of-the-art methods. The\nmethod achieves better performance than the existing methods with only 30\\% of\nthe training data. The effectiveness of the proposed method is also\ndemonstrated on the LiTS and the T1 modality of BraTS. The code and the\npre-trained model of this study are publicly available at\nhttps://github.com/Au3C2/Generator-Versus-Segmentor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunlong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1\">Xin Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yihong Zhuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+LiyanSun/0/1/0/all/0/1\">LiyanSun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yue Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xinghao Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Guisheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Transformer Based Model for Image Captioning. (arXiv:2203.15350v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15350","description":"<p>CNN-LSTM based architectures have played an important role in image\ncaptioning, but limited by the training efficiency and expression ability,\nresearchers began to explore the CNN-Transformer based models and achieved\ngreat success. Meanwhile, almost all recent works adopt Faster R-CNN as the\nbackbone encoder to extract region-level features from given images. However,\nFaster R-CNN needs a pre-training on an additional dataset, which divides the\nimage captioning task into two stages and limits its potential applications. In\nthis paper, we build a pure Transformer-based model, which integrates image\ncaptioning into one stage and realizes end-to-end training. Firstly, we adopt\nSwinTransformer to replace Faster R-CNN as the backbone encoder to extract\ngrid-level features from given images; Then, referring to Transformer, we build\na refining encoder and a decoder. The refining encoder refines the grid\nfeatures by capturing the intra-relationship between them, and the decoder\ndecodes the refined features into captions word by word. Furthermore, in order\nto increase the interaction between multi-modal (vision and language) features\nto enhance the modeling capability, we calculate the mean pooling of grid\nfeatures as the global feature, then introduce it into refining encoder to\nrefine with grid features together, and add a pre-fusion process of refined\nglobal feature and generated words in decoder. To validate the effectiveness of\nour proposed model, we conduct experiments on MSCOCO dataset. The experimental\nresults compared to existing published works demonstrate that our model\nachieves new state-of-the-art performances of 138.2% (single model) and 141.0%\n(ensemble of 4 models) CIDEr scores on `Karpathy' offline test split and 136.0%\n(c5) and 138.3% (c40) CIDEr scores on the official online test server. Trained\nmodels and source code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jungang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yingfei Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SIOD: Single Instance Annotated Per Category Per Image for Object Detection. (arXiv:2203.15353v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15353","description":"<p>Object detection under imperfect data receives great attention recently.\nWeakly supervised object detection (WSOD) suffers from severe localization\nissues due to the lack of instance-level annotation, while semi-supervised\nobject detection (SSOD) remains challenging led by the inter-image discrepancy\nbetween labeled and unlabeled data. In this study, we propose the Single\nInstance annotated Object Detection (SIOD), requiring only one instance\nannotation for each existing category in an image. Degraded from inter-task\n(WSOD) or inter-image (SSOD) discrepancies to the intra-image discrepancy, SIOD\nprovides more reliable and rich prior knowledge for mining the rest of\nunlabeled instances and trades off the annotation cost and performance. Under\nthe SIOD setting, we propose a simple yet effective framework, termed\nDual-Mining (DMiner), which consists of a Similarity-based Pseudo Label\nGenerating module (SPLG) and a Pixel-level Group Contrastive Learning module\n(PGCL). SPLG firstly mines latent instances from feature representation space\nto alleviate the annotation missing problem. To avoid being misled by\ninaccurate pseudo labels, we propose PGCL to boost the tolerance to false\npseudo labels. Extensive experiments on MS COCO verify the feasibility of the\nSIOD setting and the superiority of the proposed method, which obtains\nconsistent and significant improvements compared to baseline methods and\nachieves comparable results with fully supervised object detection (FSOD)\nmethods with only 40% instances annotated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hanjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1\">Xingjia Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1\">Ke Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1\">Fan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signing at Scale: Learning to Co-Articulate Signs for Large-Scale Photo-Realistic Sign Language Production. (arXiv:2203.15354v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15354","description":"<p>Sign languages are visual languages, with vocabularies as rich as their\nspoken language counterparts. However, current deep-learning based Sign\nLanguage Production (SLP) models produce under-articulated skeleton pose\nsequences from constrained vocabularies and this limits applicability. To be\nunderstandable and accepted by the deaf, an automatic SLP system must be able\nto generate co-articulated photo-realistic signing sequences for large domains\nof discourse.\n</p>\n<p>In this work, we tackle large-scale SLP by learning to co-articulate between\ndictionary signs, a method capable of producing smooth signing while scaling to\nunconstrained domains of discourse. To learn sign co-articulation, we propose a\nnovel Frame Selection Network (FS-Net) that improves the temporal alignment of\ninterpolated dictionary signs to continuous signing sequences. Additionally, we\npropose SignGAN, a pose-conditioned human synthesis model that produces\nphoto-realistic sign language videos direct from skeleton pose. We propose a\nnovel keypoint-based loss function which improves the quality of synthesized\nhand images.\n</p>\n<p>We evaluate our SLP model on the large-scale meineDGS (mDGS) corpus,\nconducting extensive user evaluation showing our FS-Net approach improves\nco-articulation of interpolated dictionary signs. Additionally, we show that\nSignGAN significantly outperforms all baseline methods for quantitative\nmetrics, human perceptual studies and native deaf signer comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saunders_B/0/1/0/all/0/1\">Ben Saunders</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camgoz_N/0/1/0/all/0/1\">Necati Cihan Camgoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Continual Learning on a Contaminated Data Stream with Blurry Task Boundaries. (arXiv:2203.15355v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15355","description":"<p>Learning under a continuously changing data distribution with incorrect\nlabels is a desirable real-world problem yet challenging. A large body of\ncontinual learning (CL) methods, however, assumes data streams with clean\nlabels, and online learning scenarios under noisy data streams are yet\nunderexplored. We consider a more practical CL task setup of an online learning\nfrom blurry data stream with corrupted labels, where existing CL methods\nstruggle. To address the task, we first argue the importance of both diversity\nand purity of examples in the episodic memory of continual learning models. To\nbalance diversity and purity in the episodic memory, we propose a novel\nstrategy to manage and use the memory by a unified approach of label noise\naware diverse sampling and robust learning with semi-supervised learning. Our\nempirical validations on four real-world or synthetic noise datasets (CIFAR10\nand 100, mini-WebVision, and Food-101N) exhibit that our method significantly\noutperforms prior arts in this realistic and challenging continual learning\nscenario. Code and data splits are available in\nhttps://github.com/clovaai/puridiver.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1\">Jihwan Bang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1\">Hyunseo Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seulki Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Collaborative Learning for Long-Tailed Visual Recognition. (arXiv:2203.15359v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15359","description":"<p>The networks trained on the long-tailed dataset vary remarkably, despite the\nsame training settings, which shows the great uncertainty in long-tailed\nlearning. To alleviate the uncertainty, we propose a Nested Collaborative\nLearning (NCL), which tackles the problem by collaboratively learning multiple\nexperts together. NCL consists of two core components, namely Nested Individual\nLearning (NIL) and Nested Balanced Online Distillation (NBOD), which focus on\nthe individual supervised learning for each single expert and the knowledge\ntransferring among multiple experts, respectively. To learn representations\nmore thoroughly, both NIL and NBOD are formulated in a nested way, in which the\nlearning is conducted on not just all categories from a full perspective but\nsome hard categories from a partial perspective. Regarding the learning in the\npartial perspective, we specifically select the negative categories with high\npredicted scores as the hard categories by using a proposed Hard Category\nMining (HCM). In the NCL, the learning from two perspectives is nested, highly\nrelated and complementary, and helps the network to capture not only global and\nrobust features but also meticulous distinguishing ability. Moreover,\nself-supervision is further utilized for feature enhancement. Extensive\nexperiments manifest the superiority of our method with outperforming the\nstate-of-the-art whether by using a single model or an ensemble.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zichang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Image Representation Learning with Geometric Set Consistency. (arXiv:2203.15361v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15361","description":"<p>We propose a method for self-supervised image representation learning under\nthe guidance of 3D geometric consistency. Our intuition is that 3D geometric\nconsistency priors such as smooth regions and surface discontinuities may imply\nconsistent semantics or object boundaries, and can act as strong cues to guide\nthe learning of 2D image representations without semantic labels. Specifically,\nwe introduce 3D geometric consistency into a contrastive learning framework to\nenforce the feature consistency within image views. We propose to use geometric\nconsistency sets as constraints and adapt the InfoNCE loss accordingly. We show\nthat our learned image representations are general. By fine-tuning our\npre-trained representations for various 2D image-based downstream tasks,\nincluding semantic segmentation, object detection, and instance segmentation on\nreal-world indoor scene datasets, we achieve superior performance compared with\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nenglun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1\">Lei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenping Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Invariant Siamese Attention Mask for Small Object Change Detection via Everyday Indoor Robot Navigation. (arXiv:2203.15362v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15362","description":"<p>The problem of image change detection via everyday indoor robot navigation is\nexplored from a novel perspective of the self-attention technique. Detecting\nsemantically non-distinctive and visually small changes remains a key challenge\nin the robotics community. Intuitively, these small non-distinctive changes may\nbe better handled by the recent paradigm of the attention mechanism, which is\nthe basic idea of this work. However, existing self-attention models require\nsignificant retraining cost per domain, so it is not directly applicable to\nrobotics applications. We propose a new self-attention technique with an\nability of unsupervised on-the-fly domain adaptation, which introduces an\nattention mask into the intermediate layer of an image change detection model,\nwithout modifying the input and output layers of the model. Experiments, in\nwhich an indoor robot aims to detect visually small changes in everyday\nnavigation, demonstrate that our attention technique significantly boosts the\nstate-of-the-art image change detection model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koji Takeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1\">Kanji Tanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_Y/0/1/0/all/0/1\">Yoshimasa Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face segmentation: A comparison between visible and thermal images. (arXiv:2203.15366v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15366","description":"<p>Face segmentation is a first step for face biometric systems. In this paper\nwe present a face segmentation algorithm for thermographic images. This\nalgorithm is compared with the classic Viola and Jones algorithm used for\nvisible images. Experimental results reveal that, when segmenting a\nmultispectral (visible and thermal) face database, the proposed algorithm is\nmore than 10 times faster, while the accuracy of face segmentation in thermal\nimages is higher than in case of Viola-Jones\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espinosa_Duro_V/0/1/0/all/0/1\">Virginia Espinosa-Dur&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mc-BEiT: Multi-choice Discretization for Image BERT Pre-training. (arXiv:2203.15371v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15371","description":"<p>Image BERT pre-training with masked image modeling (MIM) becomes a popular\npractice to cope with self-supervised representation learning. A seminal work,\nBEiT, casts MIM as a classification task with a visual vocabulary, tokenizing\nthe continuous visual signals into discrete vision tokens using a pre-learned\ndVAE. Despite a feasible solution, the improper discretization hinders further\nimprovements of image pre-training. Since image discretization has no\nground-truth answers, we believe that the masked patch should not be assigned\nwith a unique token id even if a better tokenizer can be obtained. In this\nwork, we introduce an improved BERT-style image pre-training method, namely\nmc-BEiT, which performs MIM proxy tasks towards eased and refined multi-choice\ntraining objectives. Specifically, the multi-choice supervision for the masked\nimage patches is formed by the soft probability vectors of the discrete token\nids, which are predicted by the off-the-shelf image tokenizer and further\nrefined by high-level inter-patch perceptions resorting to the observation that\nsimilar patches should share their choices. Extensive experiments on\nclassification, segmentation, and detection tasks demonstrate the superiority\nof our method, e.g., the pre-trained ViT-B achieves 84.1% top-1 fine-tuning\naccuracy on ImageNet-1K classification, 51.2% mIOU on ADE20K semantic\nsegmentation, 51.2% AP^b and 44.3% AP^m of object detection and instance\nsegmentation on COCO, outperforming the competitive counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaotong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zixuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1\">Ling-Yu Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Style-aware Discriminator for Controllable Image Translation. (arXiv:2203.15375v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15375","description":"<p>Current image-to-image translations do not control the output domain beyond\nthe classes used during training, nor do they interpolate between different\ndomains well, leading to implausible results. This limitation largely arises\nbecause labels do not consider the semantic distance. To mitigate such\nproblems, we propose a style-aware discriminator that acts as a critic as well\nas a style encoder to provide conditions. The style-aware discriminator learns\na controllable style space using prototype-based self-supervised learning and\nsimultaneously guides the generator. Experiments on multiple datasets verify\nthat the proposed model outperforms current state-of-the-art image-to-image\ntranslation methods. In contrast with current methods, the proposed approach\nsupports various applications, including style interpolation, content\ntransplantation, and local image translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunhee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sanghun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_E/0/1/0/all/0/1\">Eunyeong Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taehun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daijin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SepViT: Separable Vision Transformer. (arXiv:2203.15380v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15380","description":"<p>Vision Transformers have witnessed prevailing success in a series of vision\ntasks. However, they often require enormous amount of computations to achieve\nhigh performance, which is burdensome to deploy on resource-constrained\ndevices. To address these issues, we draw lessons from depthwise separable\nconvolution and imitate its ideology to design the Separable Vision\nTransformer, abbreviated as SepViT. SepViT helps to carry out the information\ninteraction within and among the windows via a depthwise separable\nself-attention. The novel window token embedding and grouped self-attention are\nemployed to model the attention relationship among windows with negligible\ncomputational cost and capture a long-range visual dependencies of multiple\nwindows, respectively. Extensive experiments on various benchmark tasks\ndemonstrate SepViT can achieve state-of-the-art results in terms of trade-off\nbetween accuracy and latency. Among them, SepViT achieves 84.0% top-1 accuracy\non ImageNet-1K classification while decreasing the latency by 40%, compared to\nthe ones with similar accuracy (e.g., CSWin, PVTV2). As for the downstream\nvision tasks, SepViT with fewer FLOPs can achieve 50.4% mIoU on ADE20K semantic\nsegmentation task, 47.5 AP on the RetinaNet-based COCO detection task, 48.7 box\nAP and 43.9 mask AP on Mask R-CNN-based COCO detection and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xuefeng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Min Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1\">Shiping Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alignment-Uniformity aware Representation Learning for Zero-shot Video Classification. (arXiv:2203.15381v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15381","description":"<p>Most methods tackle zero-shot video classification by aligning\nvisual-semantic representations within seen classes, which limits\ngeneralization to unseen classes. To enhance model generalizability, this paper\npresents an end-to-end framework that preserves alignment and uniformity\nproperties for representations on both seen and unseen classes. Specifically,\nwe formulate a supervised contrastive loss to simultaneously align\nvisual-semantic features (i.e., alignment) and encourage the learned features\nto distribute uniformly (i.e., uniformity). Unlike existing methods that only\nconsider the alignment, we propose uniformity to preserve maximal-info of\nexisting features, which improves the probability that unobserved features fall\naround observed data. Further, we synthesize features of unseen classes by\nproposing a class generator that interpolates and extrapolates the features of\nseen classes. Besides, we introduce two metrics, closeness and dispersion, to\nquantify the two properties and serve as new measurements of model\ngeneralizability. Experiments show that our method significantly outperforms\nSoTA by relative improvements of 28.1% on UCF101 and 27.0% on HMDB51. Code is\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shi Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kaili Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Mao Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Category Guided Attention Network for Brain Tumor Segmentation in MRI. (arXiv:2203.15383v1 [eess.IV])","link":"http://arxiv.org/abs/2203.15383","description":"<p>Objective: Magnetic resonance imaging (MRI) has been widely used for the\nanalysis and diagnosis of brain diseases. Accurate and automatic brain tumor\nsegmentation is of paramount importance for radiation treatment. However, low\ntissue contrast in tumor regions makes it a challenging task.Approach: We\npropose a novel segmentation network named Category Guided Attention U-Net (CGA\nU-Net). In this model, we design a Supervised Attention Module (SAM) based on\nthe attention mechanism, which can capture more accurate and stable long-range\ndependency in feature maps without introducing much computational cost.\nMoreover, we propose an intra-class update approach to reconstruct feature maps\nby aggregating pixels of the same category. Main results: Experimental results\non the BraTS 2019 datasets show that the proposed method outperformers the\nstate-of-the-art algorithms in both segmentation performance and computational\ncomplexity. Significance: The CGA U-Net can effectively capture the global\nsemantic information in the MRI image by using the SAM module, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/delugewalker/CGA-U-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_M/0/1/0/all/0/1\">Meng Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Hybrid Network: Inducting Scattering Features. (arXiv:2203.15392v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15392","description":"<p>Recent work showed that hybrid networks, which combine predefined and learnt\nfilters within a single architecture, are more amenable to theoretical analysis\nand less prone to overfitting in data-limited scenarios. However, their\nperformance has yet to prove competitive against the conventional counterparts\nwhen sufficient amounts of training data are available. In an attempt to\naddress this core limitation of current hybrid networks, we introduce an\nEfficient Hybrid Network (E-HybridNet). We show that it is the first scattering\nbased approach that consistently outperforms its conventional counterparts on a\ndiverse range of datasets. It is achieved with a novel inductive architecture\nthat embeds scattering features into the network flow using Hybrid Fusion\nBlocks. We also demonstrate that the proposed design inherits the key property\nof prior hybrid networks -- an effective generalisation in data-limited\nscenarios. Our approach successfully combines the best of the two worlds:\nflexibility and power of learnt features and stability and predictability of\nscattering representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Minskiy_D/0/1/0/all/0/1\">Dmitry Minskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bober_M/0/1/0/all/0/1\">Miroslaw Bober</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantifying Societal Bias Amplification in Image Captioning. (arXiv:2203.15395v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15395","description":"<p>We study societal bias amplification in image captioning. Image captioning\nmodels have been shown to perpetuate gender and racial biases, however, metrics\nto measure, quantify, and evaluate the societal bias in captions are not yet\nstandardized. We provide a comprehensive study on the strengths and limitations\nof each metric, and propose LIC, a metric to study captioning bias\namplification. We argue that, for image captioning, it is not enough to focus\non the correct prediction of the protected attribute, and the whole context\nshould be taken into account. We conduct extensive evaluation on traditional\nand state-of-the-art image captioning models, and surprisingly find that, by\nonly focusing on the protected attribute prediction, bias mitigation models are\nunexpectedly amplifying bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hirota_Y/0/1/0/all/0/1\">Yusuke Hirota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakashima_Y/0/1/0/all/0/1\">Yuta Nakashima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1\">Noa Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Face Video Compression using Multiple Views. (arXiv:2203.15401v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15401","description":"<p>Recent advances in deep generative models led to the development of neural\nface video compression codecs that use an order of magnitude less bandwidth\nthan engineered codecs. These neural codecs reconstruct the current frame by\nwarping a source frame and using a generative model to compensate for\nimperfections in the warped source frame. Thereby, the warp is encoded and\ntransmitted using a small number of keypoints rather than a dense flow field,\nwhich leads to massive savings compared to traditional codecs. However, by\nrelying on a single source frame only, these methods lead to inaccurate\nreconstructions (e.g. one side of the head becomes unoccluded when turning the\nhead and has to be synthesized). Here, we aim to tackle this issue by relying\non multiple source frames (views of the face) and present encouraging results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Volokitin_A/0/1/0/all/0/1\">Anna Volokitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brugger_S/0/1/0/all/0/1\">Stefan Brugger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benlalah_A/0/1/0/all/0/1\">Ali Benlalah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martin_S/0/1/0/all/0/1\">Sebastian Martin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amberg_B/0/1/0/all/0/1\">Brian Amberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1\">Michael Tschannen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransGAN: a Transductive Adversarial Model for Novelty Detection. (arXiv:2203.15406v1 [cs.LG])","link":"http://arxiv.org/abs/2203.15406","description":"<p>Novelty detection, a widely studied problem in machine learning, is the\nproblem of detecting a novel class of data that has not been previously\nobserved. A common setting for novelty detection is inductive whereby only\nexamples of the negative class are available during training time. Transductive\nnovelty detection on the other hand has only witnessed a recent surge in\ninterest, it not only makes use of the negative class during training but also\nincorporates the (unlabeled) test set to detect novel examples. Several studies\nhave emerged under the transductive setting umbrella that have demonstrated its\nadvantage over its inductive counterpart. Depending on the assumptions about\nthe data, these methods go by different names (e.g. transductive novelty\ndetection, semi-supervised novelty detection, positive-unlabeled learning,\nout-of-distribution detection). With the use of generative adversarial networks\n(GAN), a segment of those studies have adopted a transductive setup in order to\nlearn how to generate examples of the novel class. In this study, we propose\nTransGAN, a transductive generative adversarial network that attempts to learn\nhow to generate image examples from both the novel and negative classes by\nusing a mixture of two Gaussians in the latent space. It achieves that by\nincorporating an adversarial autoencoder with a GAN network, the ability to\ngenerate examples of novel data points offers not only a visual representation\nof novelties, but also overcomes the hurdle faced by many inductive methods of\nhow to tune the model hyperparameters at the decision rule level. Our model has\nshown superior performance over state-of-the-art inductive and transductive\nmethods. Our study is fully reproducible with the code available publicly.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toron_N/0/1/0/all/0/1\">Najiba Toron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mourao_Miranda_J/0/1/0/all/0/1\">Janaina Mourao-Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shawe_Taylor_J/0/1/0/all/0/1\">John Shawe-Taylor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoCoMet: Smart Neural Architecture Search via Co-Regulated Shaping Reinforcement. (arXiv:2203.15408v1 [cs.LG])","link":"http://arxiv.org/abs/2203.15408","description":"<p>Designing suitable deep model architectures, for AI-driven on-device apps and\nfeatures, at par with rapidly evolving mobile hardware and increasingly complex\ntarget scenarios is a difficult task. Though Neural Architecture Search\n(NAS/AutoML) has made this easier by shifting paradigm from extensive manual\neffort to automated architecture learning from data, yet it has major\nlimitations, leading to critical bottlenecks in the context of mobile devices,\nincluding model-hardware fidelity, prohibitive search times and deviation from\nprimary target objective(s). Thus, we propose AutoCoMet that can learn the most\nsuitable DNN architecture optimized for varied types of device hardware and\ntask contexts, ~ 3x faster. Our novel co-regulated shaping reinforcement\ncontroller together with the high fidelity hardware meta-behavior predictor\nproduces a smart, fast NAS framework that adapts to context via a generalized\nformalism for any kind of multi-criteria optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1\">Mayukh Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Brijraj Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chheda_H/0/1/0/all/0/1\">Harsh Kanti Chheda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Pawan Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+NS_P/0/1/0/all/0/1\">Pradeep NS</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Learning for Data-Driven Adaptive Scanning in Ptychography. (arXiv:2203.15413v1 [physics.comp-ph])","link":"http://arxiv.org/abs/2203.15413","description":"<p>We present a method that lowers the dose required for a ptychographic\nreconstruction by adaptively scanning the specimen, thereby providing the\nrequired spatial information redundancy in the regions of highest importance.\nThe proposed method is built upon a deep learning model that is trained by\nreinforcement learning (RL), using prior knowledge of the specimen structure\nfrom training data sets. We show that equivalent low-dose experiments using\nadaptive scanning outperform conventional ptychography experiments in terms of\nreconstruction resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Schloz_M/0/1/0/all/0/1\">Marcel Schloz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Muller_J/0/1/0/all/0/1\">Johannes M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pekin_T/0/1/0/all/0/1\">Thomas C. Pekin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Broek_W/0/1/0/all/0/1\">Wouter Van den Broek</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Koch_C/0/1/0/all/0/1\">Christoph T. Koch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Long-term Video Frame Interpolation via Feature Propagation. (arXiv:2203.15427v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15427","description":"<p>Video frame interpolation (VFI) works generally predict intermediate frame(s)\nby first estimating the motion between inputs and then warping the inputs to\nthe target time with the estimated motion. This approach, however, is not\noptimal when the temporal distance between the input sequence increases as\nexisting motion estimation modules cannot effectively handle large motions.\nHence, VFI works perform well for small frame gaps and perform poorly as the\nframe gap increases. In this work, we propose a novel framework to address this\nproblem. We argue that when there is a large gap between inputs, instead of\nestimating imprecise motion that will eventually lead to inaccurate\ninterpolation, we can safely propagate from one side of the input up to a\nreliable time frame using the other input as a reference. Then, the rest of the\nintermediate frames can be interpolated using standard approaches as the\ntemporal gap is now narrowed. To this end, we propose a propagation network\n(PNet) by extending the classic feature-level forecasting with a novel\nmotion-to-feature approach. To be thorough, we adopt a simple interpolation\nmodel along with PNet as our full model and design a simple procedure to train\nthe full model in an end-to-end manner. Experimental results on several\nbenchmark datasets confirm the effectiveness of our method for long-term VFI\ncompared to state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Argaw_D/0/1/0/all/0/1\">Dawit Mureja Argaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1\">In So Kweon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clean Implicit 3D Structure from Noisy 2D STEM Images. (arXiv:2203.15434v1 [eess.IV])","link":"http://arxiv.org/abs/2203.15434","description":"<p>Scanning Transmission Electron Microscopes (STEMs) acquire 2D images of a 3D\nsample on the scale of individual cell components. Unfortunately, these 2D\nimages can be too noisy to be fused into a useful 3D structure and facilitating\ngood denoisers is challenging due to the lack of clean-noisy pairs.\nAdditionally, representing a detailed 3D structure can be difficult even for\nclean data when using regular 3D grids. Addressing these two limitations, we\nsuggest a differentiable image formation model for STEM, allowing to learn a\njoint model of 2D sensor noise in STEM together with an implicit 3D model. We\nshow, that the combination of these models are able to successfully disentangle\n3D signal and noise without supervision and outperform at the same time several\nbaselines on synthetic and real data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kniesel_H/0/1/0/all/0/1\">Hannah Kniesel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ropinski_T/0/1/0/all/0/1\">Timo Ropinski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bergner_T/0/1/0/all/0/1\">Tim Bergner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Devan_K/0/1/0/all/0/1\">Kavitha Shaga Devan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Read_C/0/1/0/all/0/1\">Clarissa Read</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Walther_P/0/1/0/all/0/1\">Paul Walther</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ritschel_T/0/1/0/all/0/1\">Tobias Ritschel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hermosilla_P/0/1/0/all/0/1\">Pedro Hermosilla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Information Based Anomaly Detection for a Multi-Scene UAV Aerial Videos. (arXiv:2203.15437v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15437","description":"<p>UAV based surveillance is gaining much interest worldwide due to its\nextensive applications in monitoring wildlife, urban planning, disaster\nmanagement, campus security, etc. These videos are analyzed for\nstrange/odd/anomalous patterns which are essential aspects of surveillance. But\nmanual analysis of these videos is tedious and laborious. Hence, the\ndevelopment of computer-aided systems for the analysis of UAV based\nsurveillance videos is crucial. Despite this interest, in literature, several\ncomputer aided systems are developed focusing only on CCTV based surveillance\nvideos. These methods are designed for single scene scenarios and lack\ncontextual knowledge which is required for multi-scene scenarios. Furthermore,\nthe lack of standard UAV based anomaly detection datasets limits the\ndevelopment of these systems. In this regard, the present work aims at the\ndevelopment of a Computer Aided Decision support system to analyse UAV based\nsurveillance videos. A new UAV based multi-scene anomaly detection dataset is\ndeveloped with frame-level annotations for the development of computer aided\nsystems. It holistically uses contextual, temporal and appearance features for\naccurate detection of anomalies. Furthermore, a new inference strategy is\nproposed that utilizes few anomalous samples along with normal samples to\nidentify better decision boundaries. The proposed method is extensively\nevaluated on the UAV based anomaly detection dataset and performed\ncompetitively with respect to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_G/0/1/0/all/0/1\">Girisha S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verma_U/0/1/0/all/0/1\">Ujjwal Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M_M/0/1/0/all/0/1\">Manohara Pai M M</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pai_R/0/1/0/all/0/1\">Radhika M Pai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eventor: An Efficient Event-Based Monocular Multi-View Stereo Accelerator on FPGA Platform. (arXiv:2203.15439v1 [cs.AR])","link":"http://arxiv.org/abs/2203.15439","description":"<p>Event cameras are bio-inspired vision sensors that asynchronously represent\npixel-level brightness changes as event streams. Event-based monocular\nmulti-view stereo (EMVS) is a technique that exploits the event streams to\nestimate semi-dense 3D structure with known trajectory. It is a critical task\nfor event-based monocular SLAM. However, the required intensive computation\nworkloads make it challenging for real-time deployment on embedded platforms.\nIn this paper, Eventor is proposed as a fast and efficient EMVS accelerator by\nrealizing the most critical and time-consuming stages including event\nback-projection and volumetric ray-counting on FPGA. Highly paralleled and\nfully pipelined processing elements are specially designed via FPGA and\nintegrated with the embedded ARM as a heterogeneous system to improve the\nthroughput and reduce the memory footprint. Meanwhile, the EMVS algorithm is\nreformulated to a more hardware-friendly manner by rescheduling, approximate\ncomputing and hybrid data quantization. Evaluation results on DAVIS dataset\nshow that Eventor achieves up to $24\\times$ improvement in energy efficiency\ncompared with Intel i5 CPU platform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingjun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianlei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yingjie Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Meng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yuhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Runze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Weitao Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Bei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weisheng Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UnShadowNet: Illumination Critic Guided Contrastive Learning For Shadow Removal. (arXiv:2203.15441v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15441","description":"<p>Shadows are frequently encountered natural phenomena that significantly\nhinder the performance of computer vision perception systems in practical\nsettings, e.g., autonomous driving. A solution to this would be to eliminate\nshadow regions from the images before the processing of the perception system.\nYet, training such a solution requires pairs of aligned shadowed and\nnon-shadowed images which are difficult to obtain. We introduce a novel weakly\nsupervised shadow removal framework UnShadowNet trained using contrastive\nlearning. It comprises of a DeShadower network responsible for removal of the\nextracted shadow under the guidance of an Illumination network which is trained\nadversarially by the illumination critic and a Refinement network to further\nremove artifacts. We show that UnShadowNet can also be easily extended to a\nfully-supervised setup to exploit the ground-truth when available. UnShadowNet\noutperforms existing state-of-the-art approaches on three publicly available\nshadow datasets (ISTD, adjusted ISTD, SRD) in both the weakly and fully\nsupervised setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Subhrajyoti Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1\">Arindam Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sudip Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1\">Andrei Bursuc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1\">Ujjwal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shifting More Attention to Visual Backbone: Query-modulated Refinement Networks for End-to-End Visual Grounding. (arXiv:2203.15442v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15442","description":"<p>Visual grounding focuses on establishing fine-grained alignment between\nvision and natural language, which has essential applications in multimodal\nreasoning systems. Existing methods use pre-trained query-agnostic visual\nbackbones to extract visual feature maps independently without considering the\nquery information. We argue that the visual features extracted from the visual\nbackbones and the features really needed for multimodal reasoning are\ninconsistent. One reason is that there are differences between pre-training\ntasks and visual grounding. Moreover, since the backbones are query-agnostic,\nit is difficult to completely avoid the inconsistency issue by training the\nvisual backbone end-to-end in the visual grounding framework. In this paper, we\npropose a Query-modulated Refinement Network (QRNet) to address the\ninconsistent issue by adjusting intermediate features in the visual backbone\nwith a novel Query-aware Dynamic Attention (QD-ATT) mechanism and query-aware\nmultiscale fusion. The QD-ATT can dynamically compute query-dependent visual\nattention at the spatial and channel levels of the feature maps produced by the\nvisual backbone. We apply the QRNet to an end-to-end visual grounding\nframework. Extensive experiments show that the proposed method outperforms\nstate-of-the-art methods on five widely used datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiabo Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junfeng Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Ming Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuwu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Ji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Liang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xin Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Naturalistic Database of Thermal Emotional Facial Expressions and Effects of Induced Emotions on Memory. (arXiv:2203.15443v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15443","description":"<p>This work defines a procedure for collecting naturally induced emotional\nfacial expressions through the vision of movie excerpts with high emotional\ncontents and reports experimental data ascertaining the effects of emotions on\nmemory word recognition tasks. The induced emotional states include the four\nbasic emotions of sadness, disgust, happiness, and surprise, as well as the\nneutral emotional state. The resulting database contains both thermal and\nvisible emotional facial expressions, portrayed by forty Italian subjects and\nsimultaneously acquired by appropriately synchronizing a thermal and a standard\nvisible camera. Each subject's recording session lasted 45 minutes, allowing\nfor each mode (thermal or visible) to collect a minimum of 2000 facial\nexpressions from which a minimum of 400 were selected as highly expressive of\neach emotion category. The database is available to the scientific community\nand can be obtained contacting one of the authors. For this pilot study, it was\nfound that emotions and/or emotion categories do not affect individual\nperformance on memory word recognition tasks and temperature changes in the\nface or in some regions of it do not discriminate among emotional states.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esposito_A/0/1/0/all/0/1\">Anna Esposito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Capuano_V/0/1/0/all/0/1\">Vincenzo Capuano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mekyska_J/0/1/0/all/0/1\">Jiri Mekyska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faundez_Zanuy_M/0/1/0/all/0/1\">Marcos Faundez-Zanuy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Virtual View Selection for 3D Hand Pose Estimation. (arXiv:2203.15458v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15458","description":"<p>3D hand pose estimation from single depth is a fundamental problem in\ncomputer vision, and has wide applications.However, the existing methods still\ncan not achieve satisfactory hand pose estimation results due to view variation\nand occlusion of human hand. In this paper, we propose a new virtual view\nselection and fusion module for 3D hand pose estimation from single depth.We\npropose to automatically select multiple virtual viewpoints for pose estimation\nand fuse the results of all and find this empirically delivers accurate and\nrobust pose estimation. In order to select most effective virtual views for\npose fusion, we evaluate the virtual views based on the confidence of virtual\nviews using a light-weight network via network distillation. Experiments on\nthree main benchmark datasets including NYU, ICVL and Hands2019 demonstrate\nthat our method outperforms the state-of-the-arts on NYU and ICVL, and achieves\nvery competitive performance on Hands2019-Task1, and our proposed virtual view\nselection and fusion module is both effective for 3D hand pose estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jian Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yanguang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_D/0/1/0/all/0/1\">Dexin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cuixia Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_P/0/1/0/all/0/1\">Ping Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiaoming Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Abstract Flow for Temporal Semantic Segmentation on the Permutohedral Lattice. (arXiv:2203.15469v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15469","description":"<p>Semantic segmentation is a core ability required by autonomous agents, as\nbeing able to distinguish which parts of the scene belong to which object class\nis crucial for navigation and interaction with the environment. Approaches\nwhich use only one time-step of data cannot distinguish between moving objects\nnor can they benefit from temporal integration. In this work, we extend a\nbackbone LatticeNet to process temporal point cloud data. Additionally, we take\ninspiration from optical flow methods and propose a new module called Abstract\nFlow which allows the network to match parts of the scene with similar abstract\nfeatures and gather the information temporally. We obtain state-of-the-art\nresults on the SemanticKITTI dataset that contains LiDAR scans from real urban\nenvironments. We share the PyTorch implementation of TemporalLatticeNet at\nhttps://github.com/AIS-Bonn/temporal_latticenet .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schutt_P/0/1/0/all/0/1\">Peer Sch&#xfc;tt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosu_R/0/1/0/all/0/1\">Radu Alexandru Rosu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behnke_S/0/1/0/all/0/1\">Sven Behnke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAR-ShipNet: SAR-Ship Detection Neural Network via Bidirectional Coordinate Attention and Multi-resolution Feature Fusion. (arXiv:2203.15480v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15480","description":"<p>This paper studies a practically meaningful ship detection problem from\nsynthetic aperture radar (SAR) images by the neural network. We broadly extract\ndifferent types of SAR image features and raise the intriguing question that\nwhether these extracted features are beneficial to (1) suppress data variations\n(e.g., complex land-sea backgrounds, scattered noise) of real-world SAR images,\nand (2) enhance the features of ships that are small objects and have different\naspect (length-width) ratios, therefore resulting in the improvement of ship\ndetection. To answer this question, we propose a SAR-ship detection neural\nnetwork (call SAR-ShipNet for short), by newly developing Bidirectional\nCoordinate Attention (BCA) and Multi-resolution Feature Fusion (MRF) based on\nCenterNet. Moreover, considering the varying length-width ratio of arbitrary\nships, we adopt elliptical Gaussian probability distribution in CenterNet to\nimprove the performance of base detector models. Experimental results on the\npublic SAR-Ship dataset show that our SAR-ShipNet achieves competitive\nadvantages in both speed and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuwen Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Donghai Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weiwei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jiemin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1\">Mingqiang Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Structured Gaussians to Approximate Deep Ensembles. (arXiv:2203.15485v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15485","description":"<p>This paper proposes using a sparse-structured multivariate Gaussian to\nprovide a closed-form approximator for the output of probabilistic ensemble\nmodels used for dense image prediction tasks. This is achieved through a\nconvolutional neural network that predicts the mean and covariance of the\ndistribution, where the inverse covariance is parameterised by a sparsely\nstructured Cholesky matrix. Similarly to distillation approaches, our single\nnetwork is trained to maximise the probability of samples from pre-trained\nprobabilistic models, in this work we use a fixed ensemble of networks. Once\ntrained, our compact representation can be used to efficiently draw spatially\ncorrelated samples from the approximated output distribution. Importantly, this\napproach captures the uncertainty and structured correlations in the\npredictions explicitly in a formal distribution, rather than implicitly through\nsampling alone. This allows direct introspection of the model, enabling\nvisualisation of the learned structure. Moreover, this formulation provides two\nfurther benefits: estimation of a sample probability, and the introduction of\narbitrary spatial conditioning at test time. We demonstrate the merits of our\napproach on monocular depth estimation and show that the advantages of our\napproach are obtained with comparable quantitative performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1\">Ivor J.A. Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vicente_S/0/1/0/all/0/1\">Sara Vicente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_N/0/1/0/all/0/1\">Neill D.F. Campbell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Powerful Physical Adversarial Examples Against Practical Face Recognition Systems. (arXiv:2203.15498v1 [cs.CR])","link":"http://arxiv.org/abs/2203.15498","description":"<p>It is well-known that the most existing machine learning (ML)-based\nsafety-critical applications are vulnerable to carefully crafted input\ninstances called adversarial examples (AXs). An adversary can conveniently\nattack these target systems from digital as well as physical worlds. This paper\naims to the generation of robust physical AXs against face recognition systems.\nWe present a novel smoothness loss function and a patch-noise combo attack for\nrealizing powerful physical AXs. The smoothness loss interjects the concept of\ndelayed constraints during the attack generation process, thereby causing\nbetter handling of optimization complexity and smoother AXs for the physical\ndomain. The patch-noise combo attack combines patch noise and imperceptibly\nsmall noises from different distributions to generate powerful\nregistration-based physical AXs. An extensive experimental analysis found that\nour smoothness loss results in robust and more transferable digital and\nphysical AXs than the conventional techniques. Notably, our smoothness loss\nresults in a 1.17 and 1.97 times better mean attack success rate (ASR) in\nphysical white-box and black-box attacks, respectively. Our patch-noise combo\nattack furthers the performance gains and results in 2.39 and 4.74 times higher\nmean ASR than conventional technique in physical world white-box and black-box\nattacks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Inderjeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Araki_T/0/1/0/all/0/1\">Toshinori Araki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakizaki_K/0/1/0/all/0/1\">Kazuya Kakizaki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analysis of OODA Loop based on Adversarial for Complex Game Environments. (arXiv:2203.15502v1 [cs.AI])","link":"http://arxiv.org/abs/2203.15502","description":"<p>To address the problem of imperfect confrontation strategy caused by the lack\nof information of game environment in the simulation of non-complete\ninformation dynamic countermeasure modeling for intelligent game, the\nhierarchical analysis game strategy of confrontation model based on OODA ring\n(Observation, Orientation, Decision, Action) theory is proposed. At the same\ntime, taking into account the trend of unmanned future warfare, NetLogo\nsoftware simulation is used to construct a dynamic derivation of the\nconfrontation between two tanks. In the validation process, the OODA loop\ntheory is used to describe the operation process of the complex system between\nred and blue sides, and the four-step cycle of observation, judgment, decision\nand execution is carried out according to the number of armor of both sides,\nand then the OODA loop system adjusts the judgment and decision time\ncoefficients for the next confrontation cycle according to the results of the\nfirst cycle. Compared with traditional simulation methods that consider\nobjective factors such as loss rate and support rate, the OODA-loop-based\nhierarchical game analysis can analyze the confrontation situation more\ncomprehensively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiangri Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Hongbin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhanqing Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Treatment Learning Transformer for Noisy Image Classification. (arXiv:2203.15529v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15529","description":"<p>Current top-notch deep learning (DL) based vision models are primarily based\non exploring and exploiting the inherent correlations between training data\nsamples and their associated labels. However, a known practical challenge is\ntheir degraded performance against \"noisy\" data, induced by different\ncircumstances such as spurious correlations, irrelevant contexts, domain shift,\nand adversarial attacks. In this work, we incorporate this binary information\nof \"existence of noise\" as treatment into image classification tasks to improve\nprediction accuracy by jointly estimating their treatment effects. Motivated\nfrom causal variational inference, we propose a transformer-based architecture,\nTreatment Learning Transformer (TLT), that uses a latent generative model to\nestimate robust feature representations from current observational input for\nnoise image classification. Depending on the estimated noise level (modeled as\na binary treatment factor), TLT assigns the corresponding inference network\ntrained by the designed causal loss for prediction. We also create new noisy\nimage datasets incorporating a wide range of noise factors (e.g., object\nmasking, style transfer, and adversarial perturbation) for performance\nbenchmarking. The superior performance of TLT in noisy image classification is\nfurther validated by several refutation evaluation metrics. As a by-product,\nTLT also improves visual salience methods for perceiving noisy images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_I/0/1/0/all/0/1\">I-Te Danny Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi-Chieh Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSOP: A Multi-Stage One Shot Object Pose Estimation Framework. (arXiv:2203.15533v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15533","description":"<p>We present a novel one-shot method for object detection and 6 DoF pose\nestimation, that does not require training on target objects. At test time, it\ntakes as input a target image and a textured 3D query model. The core idea is\nto represent a 3D model with a number of 2D templates rendered from different\nviewpoints. This enables CNN-based direct dense feature extraction and\nmatching. The object is first localized in 2D, then its approximate viewpoint\nis estimated, followed by dense 2D-3D correspondence prediction. The final pose\nis computed with PnP. We evaluate the method on LineMOD, Occlusion, Homebrewed,\nYCB-V and TLESS datasets and report very competitive performance in comparison\nto the state-of-the-art methods trained on synthetic data, even though our\nmethod is not trained on the object models used for testing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shugurov_I/0/1/0/all/0/1\">Ivan Shugurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information. (arXiv:2203.15536v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15536","description":"<p>Our goal is to recover the 3D shape and pose of dogs from a single image.\nThis is a challenging task because dogs exhibit a wide range of shapes and\nappearances, and are highly articulated. Recent work has proposed to directly\nregress the SMAL animal model, with additional limb scale parameters, from\nimages. Our method, called BARC (Breed-Augmented Regression using\nClassification), goes beyond prior work in several important ways. First, we\nmodify the SMAL shape space to be more appropriate for representing dog shape.\nBut, even with a better shape model, the problem of regressing dog shape from\nan image is still challenging because we lack paired images with 3D ground\ntruth. To compensate for the lack of paired data, we formulate novel losses\nthat exploit information about dog breeds. In particular, we exploit the fact\nthat dogs of the same breed have similar body shapes. We formulate a novel\nbreed similarity loss consisting of two parts: One term encourages the shape of\ndogs from the same breed to be more similar than dogs of different breeds. The\nsecond one, a breed classification loss, helps to produce recognizable\nbreed-specific shapes. Through ablation studies, we find that our breed losses\nsignificantly improve shape accuracy over a baseline without them. We also\ncompare BARC qualitatively to WLDO with a perceptual study and find that our\napproach produces dogs that are significantly more realistic. This work shows\nthat a-priori information about genetic similarity can help to compensate for\nthe lack of 3D training data. This concept may be applicable to other animal\nspecies or groups of species. Our code is publicly available for research\npurposes at https://barc.is.tue.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rueegg_N/0/1/0/all/0/1\">Nadine Rueegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuffi_S/0/1/0/all/0/1\">Silvia Zuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1\">Konrad Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ME-CapsNet: A Multi-Enhanced Capsule Networks with Routing Mechanism. (arXiv:2203.15547v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15547","description":"<p>Convolutional Neural Networks need the construction of informative features,\nwhich are determined by channel-wise and spatial-wise information at the\nnetwork's layers. In this research, we focus on bringing in a novel solution\nthat uses sophisticated optimization for enhancing both the spatial and channel\ncomponents inside each layer's receptive field. Capsule Networks were used to\nunderstand the spatial association between features in the feature map.\nStandalone capsule networks have shown good results on comparatively simple\ndatasets than on complex datasets as a result of the inordinate amount of\nfeature information. Thus, to tackle this issue, we have proposed ME-CapsNet by\nintroducing deeper convolutional layers to extract important features before\npassing through modules of capsule layers strategically to improve the\nperformance of the network significantly. The deeper convolutional layer\nincludes blocks of Squeeze-Excitation networks which uses a soft-pooling\napproach for progressively reducing the spatial size thereby dynamically\nrecalibrating the channels by reconstructing their interdependencies without\nmuch loss of important feature information. Extensive experimentation was done\nusing commonly used datasets demonstrating the efficiency of the proposed\nME-CapsNet, which clearly outperforms various research works by achieving\nhigher accuracy with minimal model complexity in complex datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bright_J/0/1/0/all/0/1\">Jerrin Bright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+R_S/0/1/0/all/0/1\">Suryaprakash R</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_A/0/1/0/all/0/1\">Arockia Selvakumar Arockia Doss</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Segmentation with Adaptive Spatial Priors from Joint Registration. (arXiv:2203.15548v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15548","description":"<p>Image segmentation is a crucial but challenging task that has many\napplications. In medical imaging for instance, intensity inhomogeneity and\nnoise are common. In thigh muscle images, different muscles are closed packed\ntogether and there are often no clear boundaries between them. Intensity based\nsegmentation models cannot separate one muscle from another. To solve such\nproblems, in this work we present a segmentation model with adaptive spatial\npriors from joint registration. This model combines segmentation and\nregistration in a unified framework to leverage their positive mutual\ninfluence. The segmentation is based on a modified Gaussian mixture model\n(GMM), which integrates intensity inhomogeneity and spacial smoothness. The\nregistration plays the role of providing a shape prior. We adopt a modified sum\nof squared difference (SSD) fidelity term and Tikhonov regularity term for\nregistration, and also utilize Gaussian pyramid and parametric method for\nrobustness. The connection between segmentation and registration is guaranteed\nby the cross entropy metric that aims to make the segmentation map (from\nsegmentation) and deformed atlas (from registration) as similar as possible.\nThis joint framework is implemented within a constraint optimization framework,\nwhich leads to an efficient algorithm. We evaluate our proposed model on\nsynthetic and thigh muscle MR images. Numerical results show the improvement as\ncompared to segmentation and registration performed separately and other joint\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weihong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Li Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Dongxing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC. (arXiv:2203.15565v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15565","description":"<p>Learning discriminative deep feature embeddings by using million-scale\nin-the-wild datasets and margin-based softmax loss is the current\nstate-of-the-art approach for face recognition. However, the memory and\ncomputing cost of the Fully Connected (FC) layer linearly scales up to the\nnumber of identities in the training set. Besides, the large-scale training\ndata inevitably suffers from inter-class conflict and long-tailed distribution.\nIn this paper, we propose a sparsely updating variant of the FC layer, named\nPartial FC (PFC). In each iteration, positive class centers and a random subset\nof negative class centers are selected to compute the margin-based softmax\nloss. All class centers are still maintained throughout the whole training\nprocess, but only a subset is selected and updated in each iteration.\nTherefore, the computing requirement, the probability of inter-class conflict,\nand the frequency of passive update on tail class centers, are dramatically\nreduced. Extensive experiments across different training data and backbones\n(e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the\nproposed PFC. The source code is available at\n\\https://github.com/deepinsight/insightface/tree/master/recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+An_X/0/1/0/all/0/1\">Xiang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jia Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Ziyong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuhan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Core Risk Minimization using Salient ImageNet. (arXiv:2203.15566v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15566","description":"<p>Deep neural networks can be unreliable in the real world especially when they\nheavily use spurious features for their predictions. Recently, Singla &amp; Feizi\n(2022) introduced the Salient Imagenet dataset by annotating and localizing\ncore and spurious features of ~52k samples from 232 classes of Imagenet. While\nthis dataset is useful for evaluating the reliance of pretrained models on\nspurious features, its small size limits its usefulness for training models. In\nthis work, we first introduce the Salient Imagenet-1M dataset with more than 1\nmillion soft masks localizing core and spurious features for all 1000 Imagenet\nclasses. Using this dataset, we first evaluate the reliance of several Imagenet\npretrained models (42 total) on spurious features and observe that: (i)\ntransformers are more sensitive to spurious features compared to Convnets, (ii)\nzero-shot CLIP transformers are highly susceptible to spurious features. Next,\nwe introduce a new learning paradigm called Core Risk Minimization (CoRM) whose\nobjective ensures that the model predicts a class using its core features. We\nevaluate different computational approaches for solving CoRM and achieve\nsignificantly higher (+12%) core accuracy (accuracy when non-core regions\ncorrupted using noise) with no drop in clean accuracy compared to models\ntrained via Empirical Risk Minimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singla_S/0/1/0/all/0/1\">Sahil Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moayeri_M/0/1/0/all/0/1\">Mazda Moayeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1\">Soheil Feizi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning a Structured Latent Space for Unsupervised Point Cloud Completion. (arXiv:2203.15580v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15580","description":"<p>Unsupervised point cloud completion aims at estimating the corresponding\ncomplete point cloud of a partial point cloud in an unpaired manner. It is a\ncrucial but challenging problem since there is no paired partial-complete\nsupervision that can be exploited directly. In this work, we propose a novel\nframework, which learns a unified and structured latent space that encoding\nboth partial and complete point clouds. Specifically, we map a series of\nrelated partial point clouds into multiple complete shape and occlusion code\npairs and fuse the codes to obtain their representations in the unified latent\nspace. To enforce the learning of such a structured latent space, the proposed\nmethod adopts a series of constraints including structured ranking\nregularization, latent code swapping constraint, and distribution supervision\non the related partial point clouds. By establishing such a unified and\nstructured latent space, better partial-complete geometry consistency and shape\ncompletion accuracy can be achieved. Extensive experiments show that our\nproposed method consistently outperforms state-of-the-art unsupervised methods\non both synthetic ShapeNet and real-world KITTI, ScanNet, and Matterport3D\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yingjie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kwan-Yee Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaogang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-D Neural Radiance Fields: Local Sampling for Faster Training. (arXiv:2203.15587v1 [cs.CV])","link":"http://arxiv.org/abs/2203.15587","description":"<p>Learning a 3D representation of a scene has been a challenging problem for\ndecades in computer vision. Recent advances in implicit neural representation\nfrom images using neural radiance fields(NeRF) have shown promising results.\nSome of the limitations of previous NeRF based methods include longer training\ntime, and inaccurate underlying geometry. The proposed method takes advantage\nof RGB-D data to reduce training time by leveraging depth sensing to improve\nlocal sampling. This paper proposes a depth-guided local sampling strategy and\na smaller neural network architecture to achieve faster training time without\ncompromising quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arnab Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew I. Comport</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Multi-modal Fusion of Image and Non-image Data in Disease Diagnosis and Prognosis: A Review. (arXiv:2203.15588v1 [cs.LG])","link":"http://arxiv.org/abs/2203.15588","description":"<p>The rapid development of diagnostic technologies in healthcare is leading to\nhigher requirements for physicians to handle and integrate the heterogeneous,\nyet complementary data that are produced during routine practice. For instance,\nthe personalized diagnosis and treatment planning for a single cancer patient\nrelies on the various images (e.g., radiological, pathological, and camera\nimages) and non-image data (e.g., clinical data and genomic data). However,\nsuch decision-making procedures can be subjective, qualitative, and have large\ninter-subject variabilities. With the recent advances in multi-modal deep\nlearning technologies, an increasingly large number of efforts have been\ndevoted to a key question: how do we extract and aggregate multi-modal\ninformation to ultimately provide more objective, quantitative computer-aided\nclinical decision making? This paper reviews the recent studies on dealing with\nsuch a question. Briefly, this review will include the (1) overview of current\nmulti-modal learning workflows, (2) summarization of multi-modal fusion\nmethods, (3) discussion of the performance, (4) applications in disease\ndiagnosis and prognosis, and (5) challenges and future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coburn_L/0/1/0/all/0/1\">Lori A. Coburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_K/0/1/0/all/0/1\">Keith T. Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis. (arXiv:2008.05865v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2008.05865","description":"<p>Synthesizing high-quality realistic images from text descriptions is a\nchallenging task. Existing text-to-image Generative Adversarial Networks\ngenerally employ a stacked architecture as the backbone yet still remain three\nflaws. First, the stacked architecture introduces the entanglements between\ngenerators of different image scales. Second, existing studies prefer to apply\nand fix extra networks in adversarial learning for text-image semantic\nconsistency, which limits the supervision capability of these networks. Third,\nthe cross-modal attention-based text-image fusion that widely adopted by\nprevious works is limited on several special image scales because of the\ncomputational cost. To these ends, we propose a simpler but more effective Deep\nFusion Generative Adversarial Networks (DF-GAN). To be specific, we propose:\n(i) a novel one-stage text-to-image backbone that directly synthesizes\nhigh-resolution images without entanglements between different generators, (ii)\na novel Target-Aware Discriminator composed of Matching-Aware Gradient Penalty\nand One-Way Output, which enhances the text-image semantic consistency without\nintroducing extra networks, (iii) a novel deep text-image fusion block, which\ndeepens the fusion process to make a full fusion between text and visual\nfeatures. Compared with current state-of-the-art methods, our proposed DF-GAN\nis simpler but more efficient to synthesize realistic and text-matching images\nand achieves better performance on widely used datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Ming Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_X/0/1/0/all/0/1\">Xiao-Yuan Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_B/0/1/0/all/0/1\">Bing-Kun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changsheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Animation with Perturbed Masks. (arXiv:2011.06922v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.06922","description":"<p>We present a novel approach for image-animation of a source image by a\ndriving video, both depicting the same type of object. We do not assume the\nexistence of pose models and our method is able to animate arbitrary objects\nwithout the knowledge of the object's structure. Furthermore, both, the driving\nvideo and the source image are only seen during test-time. Our method is based\non a shared mask generator, which separates the foreground object from its\nbackground, and captures the object's general pose and shape. To control the\nsource of the identity of the output frame, we employ perturbations to\ninterrupt the unwanted identity information on the driver's mask. A\nmask-refinement module then replaces the identity of the driver with the\nidentity of the source. Conditioned on the source image, the transformed mask\nis then decoded by a multi-scale generator that renders a realistic image, in\nwhich the content of the source frame is animated by the pose in the driving\nvideo. Due to the lack of fully supervised data, we train on the task of\nreconstructing frames from the same video the source image is taken from. Our\nmethod is shown to greatly outperform the state-of-the-art methods on multiple\nbenchmarks. Our code and samples are available at\nhttps://github.com/itsyoavshalev/Image-Animation-with-Perturbed-Masks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shalev_Y/0/1/0/all/0/1\">Yoav Shalev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Magnification-Flexible Upsampling over 3D Point Clouds. (arXiv:2011.12745v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.12745","description":"<p>This paper addresses the problem of generating dense point clouds from given\nsparse point clouds to model the underlying geometric structures of\nobjects/scenes. To tackle this challenging issue, we propose a novel end-to-end\nlearning-based framework. Specifically, by taking advantage of the linear\napproximation theorem, we first formulate the problem explicitly, which boils\ndown to determining the interpolation weights and high-order approximation\nerrors. Then, we design a lightweight neural network to adaptively learn\nunified and sorted interpolation weights as well as the high-order refinements,\nby analyzing the local geometry of the input point cloud. The proposed method\ncan be interpreted by the explicit formulation, and thus is more\nmemory-efficient than existing ones. In sharp contrast to the existing methods\nthat work only for a pre-defined and fixed upsampling factor, the proposed\nframework only requires a single neural network with one-time training to\nhandle various upsampling factors within a typical range, which is highly\ndesired in real-world applications. In addition, we propose a simple yet\neffective training strategy to drive such a flexible ability. In addition, our\nmethod can handle non-uniformly distributed and noisy data well. Extensive\nexperiments on both synthetic and real-world data demonstrate the superiority\nof the proposed method over state-of-the-art methods both quantitatively and\nqualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yue Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Ying He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Accurate Active Camera Localization. (arXiv:2012.04263v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04263","description":"<p>In this work, we solve the problem of active camera localization, which\ncontrols the camera movements actively to achieve an accurate camera pose. The\npast solutions are mostly based on Markov Localization, which reduces the\nposition-wise camera uncertainty for localization. These approaches localize\nthe camera in the discrete pose space and are agnostic to the\nlocalization-driven scene property, which restrict the camera pose accuracy in\nthe coarse scale. We propose to overcome these limitations via a novel active\ncamera localization algorithm, composed of a passive and an active localization\nmodule. The former one optimizes the camera pose in the continuous pose space\nby establishing the point-wise camera-world correspondences. The latter one\nexplicitly models the scene and camera uncertainty components to plan the right\npath for accurate camera pose estimation. We validate our algorithm on the\nchallenging localization scenarios from both synthetic and scanned real-world\nindoor scenes. Experimental results demonstrate that our algorithm outperforms\nboth the state-of-the-art Markov Localization based approach and other compared\napproaches on the fine-scale camera pose accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qihang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yingda Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1\">Fei Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1\">Siyan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoquan Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning. (arXiv:2012.05567v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.05567","description":"<p>Model explanations such as saliency maps can improve user trust in AI by\nhighlighting important features for a prediction. However, these become\ndistorted and misleading when explaining predictions of images that are subject\nto systematic error (bias) by perturbations and corruptions. Furthermore, the\ndistortions persist despite model fine-tuning on images biased by different\nfactors (blur, color temperature, day/night). We present Debiased-CAM to\nrecover explanation faithfulness across various bias types and levels by\ntraining a multi-input, multi-task model with auxiliary tasks for explanation\nand bias level predictions. In simulation studies, the approach not only\nenhanced prediction accuracy, but also generated highly faithful explanations\nabout these predictions as if the images were unbiased. In user studies,\ndebiased explanations improved user task performance, perceived truthfulness\nand perceived helpfulness. Debiased training can provide a versatile platform\nfor robust performance and explanation faithfulness for a wide range of\napplications with data biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wencan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1\">Mariella Dimiccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_B/0/1/0/all/0/1\">Brian Y. Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Adaptive Negative Class Envision for Few-Shot Open-Set Recognition. (arXiv:2012.13073v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.13073","description":"<p>We study the problem of few-shot open-set recognition (FSOR), which learns a\nrecognition system capable of both fast adaptation to new classes with limited\nlabeled examples and rejection of unknown negative samples. Traditional\nlarge-scale open-set methods have been shown ineffective for FSOR problem due\nto data limitation. Current FSOR methods typically calibrate few-shot\nclosed-set classifiers to be sensitive to negative samples so that they can be\nrejected via thresholding. However, threshold tuning is a challenging process\nas different FSOR tasks may require different rejection powers. In this paper,\nwe instead propose task-adaptive negative class envision for FSOR to integrate\nthreshold tuning into the learning process. Specifically, we augment the\nfew-shot closed-set classifier with additional negative prototypes generated\nfrom few-shot examples. By incorporating few-shot class correlations in the\nnegative generation process, we are able to learn dynamic rejection boundaries\nfor FSOR tasks. Besides, we extend our method to generalized few-shot open-set\nrecognition (GFSOR), which requires classification on both many-shot and\nfew-shot classes as well as rejection of negative samples. Extensive\nexperiments on public benchmarks validate our methods on both problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shiyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiawei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1\">Guangxing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shih-Fu Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning. (arXiv:2102.10407v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.10407","description":"<p>The ability to quickly learn from a small quantity oftraining data widens the\nrange of machine learning applications. In this paper, we propose a\ndata-efficient image captioning model, VisualGPT, which leverages the\nlinguistic knowledge from a large pretrained language model(LM). A crucial\nchallenge is to balance between the use of visual information in the image and\nprior linguistic knowledge acquired from pretraining. We designed a novel\nself-resurrecting encoder-decoder attention mechanism to quickly adapt the\npretrained LM as the language decoder ona small amount of in-domain training\ndata. The proposed self-resurrecting activation unit produces sparse\nactivations but has reduced susceptibility to zero gradients. We train the\nproposed model, VisualGPT, on 0.1%, 0.5% and 1% of MSCOCO and Conceptual\nCaptions training data. Under these conditions, we outperform the best baseline\nmodel by up to 10.8% CIDEr on MS COCO and upto 5.4% CIDEr on Conceptual\nCaptions. Further, Visual-GPT achieves the state-of-the-art result on IU X-ray,\na medical report generation dataset. To the best of our knowledge, this is the\nfirst work that improves data efficiency of image captioning by utilizing LM\npretrained on unimodal data. Our code is available at:\nhttps://github.com/Vision-CAIR/VisualGPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Transformers. (arXiv:2103.01209v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.01209","description":"<p>We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPED: Quantifying Point Cloud Distortion based on Multiscale Potential Energy Discrepancy. (arXiv:2103.02850v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.02850","description":"<p>In this paper, we propose a new distortion quantification method for point\nclouds, the multiscale potential energy discrepancy (MPED). Currently, there is\na lack of effective distortion quantification for a variety of point cloud\nperception tasks. Specifically, for dense point clouds, a distortion\nquantification method is used to predict human subjective scores and optimize\nthe selection of human perception tasks parameters, such as compression and\nenhancement. For sparse point clouds, a distortion quantification methods is\nwork as a loss function to guide the training of deep neural networks for\nunsupervised learning tasks (e.g., point cloud reconstruction, completion and\nupsampling). Therefore, an effective distortion quantification should be\ndifferentiable, distortion discriminable and have a low computational\ncomplexity. However, current distortion quantification cannot satisfy all three\nconditions. To fill this gap, we propose a new point cloud feature description\nmethod, the point potential energy (PPE), inspired by the classical physics. We\nregard the point clouds are systems that have potential energy and the\ndistortion can change the total potential energy. By evaluating at various\nneighborhood sizes, the proposed MPED achieves global-local tradeoffs,\ncapturing distortion in a multiscale fashion. We further theoretically show\nthat classical Chamfer distance is a special case of our MPED. Extensive\nexperiments show the proposed MPED superior to current methods on both human\nand machine perception tasks. Our code is avaliable at\nhttps://github.com/Qi-Yangsjtu/MPED.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yujie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Whitney extension problem for near isometries and beyond. (arXiv:2103.09748v5 [math.CA] UPDATED)","link":"http://arxiv.org/abs/2103.09748","description":"<p>In this memoir, we develop a general framework which allows for a\nsimultaneous study of labeled and unlabeled near alignment data problems in\n$\\mathbb R^D$ and the Whitney near isometry extension problem for discrete and\nnon-discrete subsets of $\\mathbb R^D$ with certain geometries. Connections of\nthis work to clustering, dimension reduction, manifold learning, vision as well\nas minimal energy partitions, discrepancy and min-max optimization are\ndiscussed. Numerous open problems in harmonic analysis, computer vision,\nmanifold learning and signal processing connected to our work are given. A\nsignificant portion of the work in this memoir is based on joint research with\nCharles Fefferman in the papers [48], [49], [50], [51].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Damelin_S/0/1/0/all/0/1\">Steven B. Damelin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MogFace: Towards a Deeper Appreciation on Face Detection. (arXiv:2103.11139v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.11139","description":"<p>Benefiting from the pioneering design of generic object detectors,\nsignificant achievements have been made in the field of face detection.\nTypically, the architectures of the backbone, feature pyramid layer, and\ndetection head module within the face detector all assimilate the excellent\nexperience from general object detectors. However, several effective methods,\nincluding label assignment and scale-level data augmentation strategy, fail to\nmaintain consistent superiority when applying on the face detector directly.\nConcretely, the former strategy involves a vast body of hyper-parameters and\nthe latter one suffers from the challenge of scale distribution bias between\ndifferent detection tasks, which both limit their generalization abilities.\nFurthermore, in order to provide accurate face bounding boxes for facial\ndown-stream tasks, the face detector imperatively requires the elimination of\nfalse alarms. As a result, practical solutions on label assignment, scale-level\ndata augmentation, and reducing false alarms are necessary for advancing face\ndetectors. In this paper, we focus on resolving three aforementioned challenges\nthat exiting methods are difficult to finish off and present a novel face\ndetector, termed MogFace. In our Mogface, three key components, Adaptive Online\nIncremental Anchor Mining Strategy, Selective Scale Enhancement Strategy and\nHierarchical Context-Aware Module, are separately proposed to boost the\nperformance of face detectors. Finally, to the best of our knowledge, our\nMogFace is the best face detector on the Wider Face leader-board, achieving all\nchampions across different testing scenarios. The code is available at\n\\url{https://github.com/damo-cv/MogFace}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhipeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selective Output Smoothing Regularization: Regularize Neural Networks by Softening Output Distributions. (arXiv:2103.15383v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.15383","description":"<p>In this paper, we propose Selective Output Smoothing Regularization, a novel\nregularization method for training the Convolutional Neural Networks (CNNs).\nInspired by the diverse effects on training from different samples, Selective\nOutput Smoothing Regularization improves the performance by encouraging the\nmodel to produce equal logits on incorrect classes when dealing with samples\nthat the model classifies correctly and over-confidently. This plug-and-play\nregularization method can be conveniently incorporated into almost any\nCNN-based project without extra hassle. Extensive experiments have shown that\nSelective Output Smoothing Regularization consistently achieves significant\nimprovement in image classification benchmarks, such as CIFAR-100, Tiny\nImageNet, ImageNet, and CUB-200-2011. Particularly, our method obtains 77.30%\naccuracy on ImageNet with ResNet-50, which gains 1.1% than baseline (76.2%). We\nalso empirically demonstrate the ability of our method to make further\nimprovements when combining with other widely used regularization techniques.\nOn Pascal detection, using the SOSR-trained ImageNet classifier as the\npretrained model leads to better detection performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tianshu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaomin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Q/0/1/0/all/0/1\">Qifeng Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiali Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opening up Open-World Tracking. (arXiv:2104.11221v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11221","description":"<p>Tracking and detecting any object, including ones never-seen-before during\nmodel training, is a crucial but elusive capability of autonomous systems. An\nautonomous agent that is blind to never-seen-before objects poses a safety\nhazard when operating in the real world - and yet this is how almost all\ncurrent systems work. One of the main obstacles towards advancing tracking any\nobject is that this task is notoriously difficult to evaluate. A benchmark that\nwould allow us to perform an apples-to-apples comparison of existing efforts is\na crucial first step towards advancing this important research field. This\npaper addresses this evaluation deficit and lays out the landscape and\nevaluation methodology for detecting and tracking both known and unknown\nobjects in the open-world setting. We propose a new benchmark, TAO-OW: Tracking\nAny Object in an Open World, analyze existing efforts in multi-object tracking,\nand construct a baseline for this task while highlighting future challenges. We\nhope to open a new front in multi-object tracking research that will hopefully\nbring us a step closer to intelligent systems that can operate safely in the\nreal world. https://openworldtracking.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zulfikar_I/0/1/0/all/0/1\">Idil Esen Zulfikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luiten_J/0/1/0/all/0/1\">Jonathon Luiten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1\">Achal Dave</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1\">Bastian Leibe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osep_A/0/1/0/all/0/1\">Aljo&#x161;a O&#x161;ep</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RelTransformer: A Transformer-Based Long-Tail Visual Relationship Recognition. (arXiv:2104.11934v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11934","description":"<p>The visual relationship recognition (VRR) task aims at understanding the\npairwise visual relationships between interacting objects in an image. These\nrelationships typically have a long-tail distribution due to their\ncompositional nature. This problem gets more severe when the vocabulary becomes\nlarge, rendering this task very challenging. This paper shows that modeling an\neffective message-passing flow through an attention mechanism can be critical\nto tackling the compositionality and long-tail challenges in VRR. The method,\ncalled RelTransformer, represents each image as a fully-connected scene graph\nand restructures the whole scene into the relation-triplet and global-scene\ncontexts. It directly passes the message from each element in the\nrelation-triplet and global-scene contexts to the target relation via\nself-attention. We also design a learnable memory to augment the long-tail\nrelation representation learning. Through extensive experiments, we find that\nour model generalizes well on many VRR benchmarks. Our model outperforms the\nbest-performing models on two large-scale long-tail VRR benchmarks, VG8K-LT\n(+2.0% overall acc) and GQA-LT (+26.0% overall acc), both having a highly\nskewed distribution towards the tail. It also achieves strong results on the\nVG200 relation detection task. Our code is available at\nhttps://github.com/Vision-CAIR/RelTransformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Aniket Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelkarim_S/0/1/0/all/0/1\">Sherif Abdelkarim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1\">Deyao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep 3D-to-2D Watermarking: Embedding Messages in 3D Meshes and Extracting Them from 2D Renderings. (arXiv:2104.13450v8 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13450","description":"<p>Digital watermarking is widely used for copyright protection. Traditional 3D\nwatermarking approaches or commercial software are typically designed to embed\nmessages into 3D meshes, and later retrieve the messages directly from\ndistorted/undistorted watermarked 3D meshes. However, in many cases, users only\nhave access to rendered 2D images instead of 3D meshes. Unfortunately,\nretrieving messages from 2D renderings of 3D meshes is still challenging and\nunderexplored. We introduce a novel end-to-end learning framework to solve this\nproblem through: 1) an encoder to covertly embed messages in both mesh geometry\nand textures; 2) a differentiable renderer to render watermarked 3D objects\nfrom different camera angles and under varied lighting conditions; 3) a decoder\nto recover the messages from 2D rendered images. From our experiments, we show\nthat our model can learn to embed information visually imperceptible to humans,\nand to retrieve the embedded information from 2D renderings that undergo 3D\ndistortions. In addition, we demonstrate that our method can also work with\nother renderers, such as ray tracers and real-time renderers with and without\nfine-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_I/0/1/0/all/0/1\">Innfarn Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Huiwen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stava_O/0/1/0/all/0/1\">Ondrej Stava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Ce Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanfar_P/0/1/0/all/0/1\">Peyman Milanfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Feng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Visual Representation Learning by Online Constrained K-Means. (arXiv:2105.11527v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11527","description":"<p>Cluster discrimination is an effective pretext task for unsupervised\nrepresentation learning, which often consists of two phases: clustering and\ndiscrimination. Clustering is to assign each instance a pseudo label that will\nbe used to learn representations in discrimination. The main challenge resides\nin clustering since prevalent clustering methods (e.g., k-means) have to run in\na batch mode. Besides, there can be a trivial solution consisting of a\ndominating cluster. To address these challenges, we first investigate the\nobjective of clustering-based representation learning. Based on this, we\npropose a novel clustering-based pretext task with online \\textbf{Co}nstrained\n\\textbf{K}-m\\textbf{e}ans (\\textbf{CoKe}). Compared with the balanced\nclustering that each cluster has exactly the same size, we only constrain the\nminimal size of each cluster to flexibly capture the inherent data structure.\nMore importantly, our online assignment method has a theoretical guarantee to\napproach the global optimum. By decoupling clustering and discrimination, CoKe\ncan achieve competitive performance when optimizing with only a single view\nfrom each instance. Extensive experiments on ImageNet and other benchmark data\nsets verify both the efficacy and efficiency of our proposal. Code is available\nat \\url{https://github.com/idstcv/CoKe}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Juhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Action Segmentation by Joint Representation Learning and Online Clustering. (arXiv:2105.13353v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.13353","description":"<p>We present a novel approach for unsupervised activity segmentation which uses\nvideo frame clustering as a pretext task and simultaneously performs\nrepresentation learning and online clustering. This is in contrast with prior\nworks where representation learning and clustering are often performed\nsequentially. We leverage temporal information in videos by employing temporal\noptimal transport. In particular, we incorporate a temporal regularization term\nwhich preserves the temporal order of the activity into the standard optimal\ntransport module for computing pseudo-label cluster assignments. The temporal\noptimal transport module enables our approach to learn effective\nrepresentations for unsupervised activity segmentation. Furthermore, previous\nmethods require storing learned features for the entire dataset before\nclustering them in an offline manner, whereas our approach processes one\nmini-batch at a time in an online manner. Extensive evaluations on three public\ndatasets, i.e. 50-Salads, YouTube Instructions, and Breakfast, and our dataset,\ni.e., Desktop Assembly, show that our approach performs on par with or better\nthan previous methods, despite having significantly less memory constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Sateesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1\">Sanjay Haresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Awais Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konin_A/0/1/0/all/0/1\">Andrey Konin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zia_M/0/1/0/all/0/1\">M. Zeeshan Zia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quoc-Huy Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DETReg: Unsupervised Pretraining with Region Priors for Object Detection. (arXiv:2106.04550v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04550","description":"<p>Recent self-supervised pretraining methods for object detection largely focus\non pretraining the backbone of the object detector, neglecting key parts of\ndetection architecture. Instead, we introduce DETReg, a new self-supervised\nmethod that pretrains the entire object detection network, including the object\nlocalization and embedding components. During pretraining, DETReg predicts\nobject localizations to match the localizations from an unsupervised region\nproposal generator and simultaneously aligns the corresponding feature\nembeddings with embeddings from a self-supervised image encoder. We implement\nDETReg using the DETR family of detectors and show that it improves over\ncompetitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship\nbenchmarks. In low-data regimes, including semi-supervised and few-shot\nlearning settings, DETReg establishes many state-of-the-art results, e.g., on\nCOCO we see a +6.0 AP improvement for 10-shot detection and +3.5 AP improvement\nwhen training with only 1\\% of the labels. For code and pretrained models,\nvisit the project page at https://amirbar.net/detreg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bar_A/0/1/0/all/0/1\">Amir Bar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kantorov_V/0/1/0/all/0/1\">Vadim Kantorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reed_C/0/1/0/all/0/1\">Colorado J Reed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1\">Roei Herzig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1\">Amir Globerson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01189","description":"<p>In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-wise Loss for Biomedical Image Segmentation. (arXiv:2108.01405v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.01405","description":"<p>We propose Region-wise (RW) loss for biomedical image segmentation.\nRegion-wise loss is versatile, can simultaneously account for class imbalance\nand pixel importance, and it can be easily implemented as the pixel-wise\nmultiplication between the softmax output and a RW map. We show that, under the\nproposed RW loss framework, certain loss functions, such as Active Contour and\nBoundary loss, can be reformulated similarly with appropriate RW maps, thus\nrevealing their underlying similarities and a new perspective to understand\nthese loss functions. We investigate the observed optimization instability\ncaused by certain RW maps, such as Boundary loss distance maps, and we\nintroduce a mathematically-grounded principle to avoid such instability. This\nprinciple provides excellent adaptability to any dataset and practically\nensures convergence without extra regularization terms or optimization tricks.\nFollowing this principle, we propose a simple version of boundary distance maps\ncalled rectified Region-wise (RRW) maps that, as we demonstrate in our\nexperiments, achieve state-of-the-art performance with similar or better Dice\ncoefficients and Hausdorff distances than Dice, Focal, weighted Cross entropy,\nand Boundary losses in three distinct segmentation tasks. We quantify the\noptimization instability provided by Boundary loss distance maps, and we\nempirically show that our RRW maps are stable to optimize. The code to run all\nour experiments is publicly available at:\nhttps://github.com/jmlipman/RegionWiseLoss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valverde_J/0/1/0/all/0/1\">Juan Miguel Valverde</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1\">Jussi Tohka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-Temporal Sparsity. (arXiv:2108.02297v4 [cs.AR] UPDATED)","link":"http://arxiv.org/abs/2108.02297","description":"<p>Long Short-Term Memory (LSTM) recurrent networks are frequently used for\ntasks involving time-sequential data such as speech recognition. Unlike\nprevious LSTM accelerators that either exploit spatial weight sparsity or\ntemporal activation sparsity, this paper proposes a new accelerator called\n\"Spartus\" that exploits spatio-temporal sparsity to achieve ultralow latency\ninference. Spatial sparsity is induced using a new Column-Balanced Targeted\nDropout (CBTD) structured pruning method, which produces structured sparse\nweight matrices for balanced workloads. The pruned networks running on Spartus\nhardware achieve weight sparsity of up to 96% and 94% with negligible accuracy\nloss on the TIMIT and the Librispeech datasets. To induce temporal sparsity in\nLSTM, we extend the previous DeltaGRU method to the DeltaLSTM method. Combining\nspatio-temporal sparsity with CBTD and DeltaLSTM saves on weight memory access\nand associated arithmetic operations. The Spartus architecture is scalable and\nsupports real-time online speech recognition when implemented on small and\nlarge FPGAs. Spartus per-sample latency for a single DeltaLSTM layer of 1024\nneurons averages 1 us. Exploiting spatio-temporal sparsity leads to 46X speedup\nof Spartus over its theoretical hardware performance to achieve 9.4 TOp/s\neffective batch-1 throughput and 1.1 TOp/s/W power efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1\">Tobi Delbruck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shih-Chii Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Face Inference Models using Hierarchical Network Dissection. (arXiv:2108.10360v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.10360","description":"<p>This paper presents Hierarchical Network Dissection, a general pipeline to\ninterpret the internal representation of face-centric inference models. Using a\nprobabilistic formulation, our pipeline pairs units of the model with concepts\nin our \"Face Dictionary\", a collection of facial concepts with corresponding\nsample images. Our pipeline is inspired by Network Dissection, a popular\ninterpretability model for object-centric and scene-centric models. However,\nour formulation allows to deal with two important challenges of face-centric\nmodels that Network Dissection cannot address: (1) spacial overlap of concepts:\nthere are different facial concepts that simultaneously occur in the same\nregion of the image, like \"nose\" (facial part) and \"pointy nose\" (facial\nattribute); and (2) global concepts: there are units with affinity to concepts\nthat do not refer to specific locations of the face (e.g. apparent age). We use\nHierarchical Network Dissection to dissect different face-centric inference\nmodels trained on widely-used facial datasets. The results show models trained\nfor different tasks learned different internal representations. Furthermore,\nthe interpretability results can reveal some biases in the training data and\nsome interesting characteristics of the face-centric inference tasks. Finally,\nwe conduct controlled experiments on biased data to showcase the potential of\nHierarchical Network Dissection for bias discovery. The results illustrate how\nHierarchical Network Dissection can be used to discover and quantify bias in\nthe training data that is also encoded in the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teotia_D/0/1/0/all/0/1\">Divyang Teotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Semantic to Instance Segmentation: Weakly-Supervised Instance Segmentation via Semantic Knowledge Transfer and Self-Refinement. (arXiv:2109.09477v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09477","description":"<p>Weakly-supervised instance segmentation (WSIS) has been considered as a more\nchallenging task than weakly-supervised semantic segmentation (WSSS). Compared\nto WSSS, WSIS requires instance-wise localization, which is difficult to\nextract from image-level labels. To tackle the problem, most WSIS approaches\nuse off-the-shelf proposal techniques that require pre-training with instance\nor object level labels, deviating the fundamental definition of the\nfully-image-level supervised setting. In this paper, we propose a novel\napproach including two innovative components. First, we propose a semantic\nknowledge transfer to obtain pseudo instance labels by transferring the\nknowledge of WSSS to WSIS while eliminating the need for the off-the-shelf\nproposals. Second, we propose a self-refinement method to refine the pseudo\ninstance labels in a self-supervised scheme and to use the refined labels for\ntraining in an online manner. Here, we discover an erroneous phenomenon,\nsemantic drift, that occurred by the missing instances in pseudo instance\nlabels categorized as background class. This semantic drift occurs confusion\nbetween background and instance in training and consequently degrades the\nsegmentation performance. We term this problem as semantic drift problem and\nshow that our proposed self-refinement method eliminates the semantic drift\nproblem. The extensive experiments on PASCAL VOC 2012 and MS COCO demonstrate\nthe effectiveness of our approach, and we achieve a considerable performance\nwithout off-the-shelf proposal techniques. The code is available at\nhttps://github.com/clovaai/BESTIE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhee_C/0/1/0/all/0/1\">Chaeeun Rhee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Early Lane Change Prediction for Automated Driving Systems Using Multi-Task Attention-based Convolutional Neural Networks. (arXiv:2109.10742v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.10742","description":"<p>Lane change (LC) is one of the safety-critical manoeuvres in highway driving\naccording to various road accident records. Thus, reliably predicting such\nmanoeuvre in advance is critical for the safe and comfortable operation of\nautomated driving systems. The majority of previous studies rely on detecting a\nmanoeuvre that has been already started, rather than predicting the manoeuvre\nin advance. Furthermore, most of the previous works do not estimate the key\ntimings of the manoeuvre (e.g., crossing time), which can actually yield more\nuseful information for the decision making in the ego vehicle. To address these\nshortcomings, this paper proposes a novel multi-task model to simultaneously\nestimate the likelihood of LC manoeuvres and the time-to-lane-change (TTLC). In\nboth tasks, an attention-based convolutional neural network (CNN) is used as a\nshared feature extractor from a bird's eye view representation of the driving\nenvironment. The spatial attention used in the CNN model improves the feature\nextraction process by focusing on the most relevant areas of the surrounding\nenvironment. In addition, two novel curriculum learning schemes are employed to\ntrain the proposed approach. The extensive evaluation and comparative analysis\nof the proposed method in existing benchmark datasets show that the proposed\nmethod outperforms state-of-the-art LC prediction models, particularly\nconsidering long-term prediction performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mozaffari_S/0/1/0/all/0/1\">Sajjad Mozaffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_E/0/1/0/all/0/1\">Eduardo Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dianati_M/0/1/0/all/0/1\">Mehrdad Dianati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1\">Saber Fallah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IntentVizor: Towards Generic Query Guided Interactive Video Summarization. (arXiv:2109.14834v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.14834","description":"<p>The target of automatic video summarization is to create a short skim of the\noriginal long video while preserving the major content/events. There is a\ngrowing interest in the integration of user queries into video summarization or\nquery-driven video summarization. This video summarization method predicts a\nconcise synopsis of the original video based on the user query, which is\ncommonly represented by the input text. However, two inherent problems exist in\nthis query-driven way. First, the text query might not be enough to describe\nthe exact and diverse needs of the user. Second, the user cannot edit once the\nsummaries are produced, while we assume the needs of the user should be subtle\nand need to be adjusted interactively. To solve these two problems, we propose\nIntentVizor, an interactive video summarization framework guided by generic\nmulti-modality queries. The input query that describes the user's needs are not\nlimited to text but also the video snippets. We further represent these\nmulti-modality finer-grained queries as user `intent', which is interpretable,\ninteractable, editable, and can better quantify the user's needs. In this\npaper, we use a set of the proposed intents to represent the user query and\ndesign a new interactive visual analytic interface. Users can interactively\ncontrol and adjust these mixed-initiative intents to obtain a more satisfying\nsummary through the interface. Also, to improve the summarization quality via\nvideo understanding, a novel Granularity-Scalable Ego-Graph Convolutional\nNetworks (GSE-GCN) is proposed. We conduct our experiments on two benchmark\ndatasets. Comparisons with the state-of-the-art methods verify the\neffectiveness of the proposed framework. Code and dataset are available at\nhttps://github.com/jnzs1836/intent-vizor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guande Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jianzhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1\">Claudio T. Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rescoring Sequence-to-Sequence Models for Text Line Recognition with CTC-Prefixes. (arXiv:2110.05909v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05909","description":"<p>In contrast to Connectionist Temporal Classification (CTC) approaches,\nSequence-To-Sequence (S2S) models for Handwritten Text Recognition (HTR) suffer\nfrom errors such as skipped or repeated words which often occur at the end of a\nsequence. In this paper, to combine the best of both approaches, we propose to\nuse the CTC-Prefix-Score during S2S decoding. Hereby, during beam search, paths\nthat are invalid according to the CTC confidence matrix are penalised. Our\nnetwork architecture is composed of a Convolutional Neural Network (CNN) as\nvisual backbone, bidirectional Long-Short-Term-Memory-Cells (LSTMs) as encoder,\nand a decoder which is a Transformer with inserted mutual attention layers. The\nCTC confidences are computed on the encoder while the Transformer is only used\nfor character-wise S2S decoding. We evaluate this setup on three HTR data sets:\nIAM, Rimes, and StAZH. On IAM, we achieve a competitive Character Error Rate\n(CER) of 2.95% when pretraining our model on synthetic data and including a\ncharacter-based language model for contemporary English. Compared to other\nstate-of-the-art approaches, our model requires about 10-20 times less\nparameters. Access our shared implementations via this link to GitHub:\nhttps://github.com/Planet-AI-GmbH/tfaip-hybrid-ctc-s2s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wick_C/0/1/0/all/0/1\">Christoph Wick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">Jochen Z&#xf6;llner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruning_T/0/1/0/all/0/1\">Tobias Gr&#xfc;ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Dialogue Response Generation. (arXiv:2110.08515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08515","description":"<p>Responsing with image has been recognized as an important capability for an\nintelligent conversational agent. Yet existing works only focus on exploring\nthe multimodal dialogue models which depend on retrieval-based methods, but\nneglecting generation methods. To fill in the gaps, we first present a\nmultimodal dialogue generation model, which takes the dialogue history as\ninput, then generates a textual sequence or an image as response. Learning such\na model often requires multimodal dialogues containing both texts and images\nwhich are difficult to obtain. Motivated by the challenge in practice, we\nconsider multimodal dialogue generation under a natural assumption that only\nlimited training examples are available. In such a low-resource setting, we\ndevise a novel conversational agent, Divter, in order to isolate parameters\nthat depend on multimodal dialogues from the entire generation model. By this\nmeans, the major part of the model can be learned from a large number of\ntext-only dialogues and text-image pairs respectively, then the whole\nparameters can be well fitted using the limited training examples. Extensive\nexperiments demonstrate our method achieves state-of-the-art results in both\nautomatic and human evaluation, and can generate informative text and\nhigh-resolution image responses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qingfeng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1\">Kai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yaming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jessica Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Representation Learning for Binary Networks by Joint Classifier Learning. (arXiv:2110.08851v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.08851","description":"<p>Self-supervised learning is a promising unsupervised learning framework that\nhas achieved success with large floating point networks. But such networks are\nnot readily deployable to edge devices. To accelerate deployment of models with\nthe benefit of unsupervised representation learning to such resource limited\ndevices for various downstream tasks, we propose a self-supervised learning\nmethod for binary networks that uses a moving target network. In particular, we\npropose to jointly train a randomly initialized classifier, attached to a\npretrained floating point feature extractor, with a binary network.\nAdditionally, we propose a feature similarity loss, a dynamic loss balancing\nand modified multi-stage training to further improve the accuracy, and call our\nmethod BURN. Our empirical validations over five downstream tasks using seven\ndatasets show that BURN outperforms self-supervised baselines for binary\nnetworks and sometimes outperforms supervised pretraining. Code is availabe at\nhttps://github.com/naver-ai/burn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dahyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jonghyun Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Per-Pixel Lung Thickness and Lung Capacity Estimation on Chest X-Rays using Convolutional Neural Networks. (arXiv:2110.12509v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12509","description":"<p>Estimating the lung depth on x-ray images could provide both an accurate\nopportunistic lung volume estimation during clinical routine and improve image\ncontrast in modern structural chest imaging techniques like x-ray dark-field\nimaging. We present a method based on a convolutional neural network that\nallows a per-pixel lung thickness estimation and subsequent total lung capacity\nestimation. The network was trained and validated using 5250 simulated\nradiographs generated from 525 real CT scans. The network was evaluated on a\ntest set of 131 synthetic radiographs and a retrospective evaluation was\nperformed on another test set of 45 standard clinical radiographs. The standard\nclinical radiographs were obtained from 45 patients, who got a CT examination\nbetween July 1, 2021 and September 1, 2021 and a chest x-ray 6 month before or\nafter the CT. For 45 standard clinical radiographs, the mean-absolute error\nbetween the estimated lung volume and groundtruth volume was 0.75 liter with a\npositive correlation (r = 0.78). When accounting for the patient diameter, the\nerror decreases to 0.69 liter with a positive correlation (r = 0.83).\nAdditionally, we predicted the lung thicknesses on the synthetic test set,\nwhere the mean-absolute error between the total volumes was 0.19 liter with a\npositive correlation (r = 0.99). The results show, that creation of lung\nthickness maps and estimation of approximate total lung volume is possible from\nstandard clinical radiographs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schultheiss_M/0/1/0/all/0/1\">Manuel Schultheiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmette_P/0/1/0/all/0/1\">Philipp Schmette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sellerer_T/0/1/0/all/0/1\">Thorsten Sellerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schick_R/0/1/0/all/0/1\">Rafael Schick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taphorn_K/0/1/0/all/0/1\">Kirsten Taphorn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mechlem_K/0/1/0/all/0/1\">Korbinian Mechlem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birnbacher_L/0/1/0/all/0/1\">Lorenz Birnbacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renger_B/0/1/0/all/0/1\">Bernhard Renger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1\">Marcus R. Makowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_F/0/1/0/all/0/1\">Franz Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_D/0/1/0/all/0/1\">Daniela Pfeiffer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Industrial Scene Text Detection with Refined Feature-attentive Network. (arXiv:2110.12663v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12663","description":"<p>Detecting the marking characters of industrial metal parts remains\nchallenging due to low visual contrast, uneven illumination, corroded character\nstructures, and cluttered background of metal part images. Affected by these\nfactors, bounding boxes generated by most existing methods locate low-contrast\ntext areas inaccurately. In this paper, we propose a refined feature-attentive\nnetwork (RFN) to solve the inaccurate localization problem. Specifically, we\ndesign a parallel feature integration mechanism to construct an adaptive\nfeature representation from multi-resolution features, which enhances the\nperception of multi-scale texts at each scale-specific level to generate a\nhigh-quality attention map. Then, an attentive refinement network is developed\nby the attention map to rectify the location deviation of candidate boxes. In\naddition, a re-scoring mechanism is designed to select text boxes with the best\nrectified location. Moreover, we construct two industrial scene text datasets,\nincluding a total of 102156 images and 1948809 text instances with various\ncharacter structures and metal parts. Extensive experiments on our dataset and\nfour public datasets demonstrate that our proposed method achieves the\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tongkun Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_C/0/1/0/all/0/1\">Chaochen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Changsheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1\">Jingzheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1\">Qi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kaijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_X/0/1/0/all/0/1\">Xinping Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Texture Estimator for Implicit Representation Function. (arXiv:2111.08918v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08918","description":"<p>Recent works with an implicit neural function shed light on representing\nimages in arbitrary resolution. However, a standalone multi-layer perceptron\nshows limited performance in learning high-frequency components. In this paper,\nwe propose a Local Texture Estimator (LTE), a dominant-frequency estimator for\nnatural images, enabling an implicit function to capture fine details while\nreconstructing images in a continuous manner. When jointly trained with a deep\nsuper-resolution (SR) architecture, LTE is capable of characterizing image\ntextures in 2D Fourier space. We show that an LTE-based neural function\nachieves favorable performance against existing deep SR methods within an\narbitrary-scale factor. Furthermore, we demonstrate that our implementation\ntakes the shortest running time compared to previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaewon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kyong Hwan Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent Variational Network: A Deep Learning Inverse Problem Solver applied to the task of Accelerated MRI Reconstruction. (arXiv:2111.09639v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.09639","description":"<p>Magnetic Resonance Imaging can produce detailed images of the anatomy and\nphysiology of the human body that can assist doctors in diagnosing and treating\npathologies such as tumours. However, MRI suffers from very long acquisition\ntimes that make it susceptible to patient motion artifacts and limit its\npotential to deliver dynamic treatments. Conventional approaches such as\nParallel Imaging and Compressed Sensing allow for an increase in MRI\nacquisition speed by reconstructing MR images from sub-sampled MRI data\nacquired using multiple receiver coils. Recent advancements in Deep Learning\ncombined with Parallel Imaging and Compressed Sensing techniques have the\npotential to produce high-fidelity reconstructions from highly accelerated MRI\ndata. In this work we present a novel Deep Learning-based Inverse Problem\nsolver applied to the task of Accelerated MRI Reconstruction, called the\nRecurrent Variational Network (RecurrentVarNet), by exploiting the properties\nof Convolutional Recurrent Neural Networks and unrolled algorithms for solving\nInverse Problems. The RecurrentVarNet consists of multiple recurrent blocks,\neach responsible for one iteration of the unrolled variational optimization\nscheme for solving the inverse problem of multi-coil Accelerated MRI\nReconstruction. Contrary to traditional approaches, the optimization steps are\nperformed in the observation domain ($k$-space) instead of the image domain.\nEach block of the RecurrentVarNet refines the observed $k$-space and comprises\na data consistency term and a recurrent unit which takes as input a learned\nhidden state and the prediction of the previous block. Our proposed method\nachieves new state of the art qualitative and quantitative reconstruction\nresults on 5-fold and 10-fold accelerated data from a public multi-coil brain\ndataset, outperforming previous conventional and deep learning-based\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yiasemis_G/0/1/0/all/0/1\">George Yiasemis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonke_J/0/1/0/all/0/1\">Jan-Jakob Sonke</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sanchez_C/0/1/0/all/0/1\">Clarisa S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Teuwen_J/0/1/0/all/0/1\">Jonas Teuwen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Captioner: Inducing Content-Style Separation in Vision-and-Language Model Training. (arXiv:2111.12727v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12727","description":"<p>While captioning models have obtained compelling results in describing\nnatural images, there is a growing effort to increase their capability of\ndealing with real-world concepts. In this paper, we address the task of\ngenerating fluent descriptions by training on a non-uniform combination of data\nsources, containing both human- and automatically-collected captions. To this\nend, we propose a model which induces a separation between content and\ndescriptive style through the incorporation of stylistic parameters and\nkeywords extracted from large-scale multi-modal models as pivotal data. In\nterms of visual features, our model avoids the need of object detectors and\nemploys grid-like features together with a single objective of prompt language\nmodeling. Experimentally, we consistently outperform existing methods in terms\nof caption quality and capability of describing out-of-domain concepts.\nFinally, our model obtains a new state of the art on both COCO and nocaps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1\">Marcella Cornia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1\">Lorenzo Baraldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiameni_G/0/1/0/all/0/1\">Giuseppe Fiameni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1\">Rita Cucchiara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Representation Transformer: Geometry-Free Novel View Synthesis Through Set-Latent Scene Representations. (arXiv:2111.13152v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13152","description":"<p>A classical problem in computer vision is to infer a 3D scene representation\nfrom few images that can be used to render novel views at interactive rates.\nPrevious work focuses on reconstructing pre-defined 3D representations, e.g.\ntextured meshes, or implicit representations, e.g. radiance fields, and often\nrequires input images with precise camera poses and long processing times for\neach novel scene.\n</p>\n<p>In this work, we propose the Scene Representation Transformer (SRT), a method\nwhich processes posed or unposed RGB images of a new area, infers a \"set-latent\nscene representation\", and synthesises novel views, all in a single\nfeed-forward pass. To calculate the scene representation, we propose a\ngeneralization of the Vision Transformer to sets of images, enabling global\ninformation integration, and hence 3D reasoning. An efficient decoder\ntransformer parameterizes the light field by attending into the scene\nrepresentation to render novel views. Learning is supervised end-to-end by\nminimizing a novel-view reconstruction error.\n</p>\n<p>We show that this method outperforms recent baselines in terms of PSNR and\nspeed on synthetic datasets, including a new dataset created for the paper.\nFurther, we demonstrate that SRT scales to support interactive visualization\nand semantic segmentation of real-world outdoor environments using Street View\nimagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajjadi_M/0/1/0/all/0/1\">Mehdi S. M. Sajjadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_H/0/1/0/all/0/1\">Henning Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pot_E/0/1/0/all/0/1\">Etienne Pot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bergmann_U/0/1/0/all/0/1\">Urs Bergmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greff_K/0/1/0/all/0/1\">Klaus Greff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radwan_N/0/1/0/all/0/1\">Noha Radwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vora_S/0/1/0/all/0/1\">Suhani Vora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucic_M/0/1/0/all/0/1\">Mario Lucic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duckworth_D/0/1/0/all/0/1\">Daniel Duckworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1\">Alexey Dosovitskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1\">Jakob Uszkoreit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1\">Thomas Funkhouser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Principled Disentanglement for Domain Generalization. (arXiv:2111.13839v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.13839","description":"<p>A fundamental challenge for machine learning models is generalizing to\nout-of-distribution (OOD) data, in part due to spurious correlations. To tackle\nthis challenge, we first formalize the OOD generalization problem as\nconstrained optimization, called Disentanglement-constrained Domain\nGeneralization (DDG). We relax this non-trivial constrained optimization\nproblem to a tractable form with finite-dimensional parameterization and\nempirical approximation. Then a theoretical analysis of the extent to which the\nabove transformations deviates from the original problem is provided. Based on\nthe transformation, we propose a primal-dual algorithm for joint representation\ndisentanglement and domain generalization. In contrast to traditional\napproaches based on domain adversarial training and domain labels, DDG jointly\nlearns semantic and variation encoders for disentanglement, enabling flexible\nmanipulation and augmentation on training data. DDG aims to learn intrinsic\nrepresentations of semantic concepts that are invariant to nuisance factors and\ngeneralizable across domains. Comprehensive experiments on popular benchmarks\nshow that DDG can achieve competitive OOD performance and uncover interpretable\nsalient structures within data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi-Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1\">Adrian Weller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Image Transformations for Transfer-based Adversarial Attack. (arXiv:2111.13844v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13844","description":"<p>Adversarial attacks provide a good way to study the robustness of deep\nlearning models. One category of methods in transfer-based black-box attack\nutilizes several image transformation operations to improve the transferability\nof adversarial examples, which is effective, but fails to take the specific\ncharacteristic of the input image into consideration. In this work, we propose\na novel architecture, called Adaptive Image Transformation Learner (AITL),\nwhich incorporates different image transformation operations into a unified\nframework to further improve the transferability of adversarial examples.\nUnlike the fixed combinational transformations used in existing works, our\nelaborately designed transformation learner adaptively selects the most\neffective combination of image transformations specific to the input image.\nExtensive experiments on ImageNet demonstrate that our method significantly\nimproves the attack success rates on both normally trained models and defense\nmodels under various settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-wise Occlusion and Depth Orders in Natural Scenes. (arXiv:2111.14562v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14562","description":"<p>In this paper, we introduce a new dataset, named InstaOrder, that can be used\nto understand the geometrical relationships of instances in an image. The\ndataset consists of 2.9M annotations of geometric orderings for class-labeled\ninstances in 101K natural scenes. The scenes were annotated by 3,659\ncrowd-workers regarding (1) occlusion order that identifies occluder/occludee\nand (2) depth order that describes ordinal relations that consider relative\ndistance from the camera. The dataset provides joint annotation of two kinds of\norderings for the same instances, and we discover that the occlusion order and\ndepth order are complementary. We also introduce a geometric order prediction\nnetwork called InstaOrderNet, which is superior to state-of-the-art approaches.\nMoreover, we propose a dense depth prediction network called InstaDepthNet that\nuses auxiliary geometric order loss to boost the accuracy of the\nstate-of-the-art depth prediction approach, MiDaS [56].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyunmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceAtlasAR: Atlas of Facial Acupuncture Points in Augmented Reality. (arXiv:2111.14755v2 [cs.GR] UPDATED)","link":"http://arxiv.org/abs/2111.14755","description":"<p>Acupuncture is a technique in which practitioners stimulate specific points\non the body. Those points, called acupuncture points (or acupoints),\nanatomically define areas on the skin relative to specific landmarks on the\nbody. However, mapping the acupoints to individuals could be challenging for\ninexperienced acupuncturists. In this project, we proposed a system to localize\nand visualize facial acupoints for individuals in an augmented reality (AR)\ncontext. This system combines a face alignment model and a hair segmentation\nmodel to provide dense reference points for acupoints localization in real-time\n(60FPS). The localization process takes the proportional bone (B-cun or\nskeletal) measurement method, which is commonly operated by specialists;\nhowever, in the real practice, operators sometimes find it inaccurate due to\nskill-related error. With this system, users, even without any skills, can\nlocate the facial acupoints as a part of the self-training or self-treatment\nprocess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Menghe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulze_J/0/1/0/all/0/1\">Jurgen Schulze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Decomposition for Stochastic Normal-Abnormal Transport. (arXiv:2111.14777v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14777","description":"<p>Advection-diffusion equations describe a large family of natural transport\nprocesses, e.g., fluid flow, heat transfer, and wind transport. They are also\nused for optical flow and perfusion imaging computations. We develop a machine\nlearning model, D^2-SONATA, built upon a stochastic advection-diffusion\nequation, which predicts the velocity and diffusion fields that drive 2D/3D\nimage time-series of transport. In particular, our proposed model incorporates\na model of transport atypicality, which isolates abnormal differences between\nexpected normal transport behavior and the observed transport. In a medical\ncontext such a normal-abnormal decomposition can be used, for example, to\nquantify pathologies. Specifically, our model identifies the advection and\ndiffusion contributions from the transport time-series and simultaneously\npredicts an anomaly value field to provide a decomposition into normal and\nabnormal advection and diffusion behavior. To achieve improved estimation\nperformance for the velocity and diffusion-tensor fields underlying the\nadvection-diffusion process and for the estimation of the anomaly fields, we\ncreate a 2D/3D anomaly-encoded advection-diffusion simulator, which allows for\nsupervised learning. We further apply our model on a brain perfusion dataset\nfrom ischemic stroke patients via transfer learning. Extensive comparisons\ndemonstrate that our model successfully distinguishes stroke lesions (abnormal)\nfrom normal brain regions, while reconstructing the underlying velocity and\ndiffusion tensor fields.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peirong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yueh Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aylward_S/0/1/0/all/0/1\">Stephen Aylward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective. (arXiv:2111.14820v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.14820","description":"<p>Learning behavioral patterns from observational data has been a de-facto\napproach to motion forecasting. Yet, the current paradigm suffers from two\nshortcomings: brittle under distribution shifts and inefficient for knowledge\ntransfer. In this work, we propose to address these challenges from a causal\nrepresentation perspective. We first introduce a causal formalism of motion\nforecasting, which casts the problem as a dynamic process with three groups of\nlatent variables, namely invariant variables, style confounders, and spurious\nfeatures. We then introduce a learning framework that treats each group\nseparately: (i) unlike the common practice \\revision{mixing} datasets collected\nfrom different locations, we exploit their subtle distinctions by means of an\ninvariance loss encouraging the model to suppress spurious correlations; (ii)\nwe devise a modular architecture that factorizes the representations of\ninvariant mechanisms and style confounders to approximate a sparse causal\ngraph; (iii) we introduce a style contrastive loss that not only enforces the\nstructure of style representations but also serves as a self-supervisory signal\nfor test-time refinement on the fly. Experiments on synthetic and real datasets\nshow that our proposed method improves the robustness and reusability of\nlearned motion representations, significantly outperforming prior\nstate-of-the-art motion forecasting models for out-of-distribution\ngeneralization and low-shot transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuejiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadei_R/0/1/0/all/0/1\">Riccardo Cadei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweizer_J/0/1/0/all/0/1\">Jonas Schweizer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1\">Sherwin Bahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAFormer: Improving Network Architectures and Training Strategies for Domain-Adaptive Semantic Segmentation. (arXiv:2111.14887v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14887","description":"<p>As acquiring pixel-wise annotations of real-world images for semantic\nsegmentation is a costly process, a model can instead be trained with more\naccessible synthetic data and adapted to real images without requiring their\nannotations. This process is studied in unsupervised domain adaptation (UDA).\nEven though a large number of methods propose new adaptation strategies, they\nare mostly based on outdated network architectures. As the influence of recent\nnetwork architectures has not been systematically studied, we first benchmark\ndifferent network architectures for UDA and newly reveal the potential of\nTransformers for UDA semantic segmentation. Based on the findings, we propose a\nnovel UDA method, DAFormer. The network architecture of DAFormer consists of a\nTransformer encoder and a multi-level context-aware feature fusion decoder. It\nis enabled by three simple but crucial training strategies to stabilize the\ntraining and to avoid overfitting to the source domain: While (1) Rare Class\nSampling on the source domain improves the quality of the pseudo-labels by\nmitigating the confirmation bias of self-training toward common classes, (2) a\nThing-Class ImageNet Feature Distance and (3) a learning rate warmup promote\nfeature transfer from ImageNet pretraining. DAFormer represents a major advance\nin UDA. It improves the state of the art by 10.8 mIoU for GTA-to-Cityscapes and\n5.4 mIoU for Synthia-to-Cityscapes and enables learning even difficult classes\nsuch as train, bus, and truck well. The implementation is available at\nhttps://github.com/lhoyer/DAFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoyer_L/0/1/0/all/0/1\">Lukas Hoyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyperStyle: StyleGAN Inversion with HyperNetworks for Real Image Editing. (arXiv:2111.15666v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.15666","description":"<p>The inversion of real images into StyleGAN's latent space is a well-studied\nproblem. Nevertheless, applying existing approaches to real-world scenarios\nremains an open challenge, due to an inherent trade-off between reconstruction\nand editability: latent space regions which can accurately represent real\nimages typically suffer from degraded semantic control. Recent work proposes to\nmitigate this trade-off by fine-tuning the generator to add the target image to\nwell-behaved, editable regions of the latent space. While promising, this\nfine-tuning scheme is impractical for prevalent use as it requires a lengthy\ntraining phase for each new image. In this work, we introduce this approach\ninto the realm of encoder-based inversion. We propose HyperStyle, a\nhypernetwork that learns to modulate StyleGAN's weights to faithfully express a\ngiven image in editable regions of the latent space. A naive modulation\napproach would require training a hypernetwork with over three billion\nparameters. Through careful network design, we reduce this to be in line with\nexisting encoders. HyperStyle yields reconstructions comparable to those of\noptimization techniques with the near real-time inference capabilities of\nencoders. Lastly, we demonstrate HyperStyle's effectiveness on several\napplications beyond the inversion task, including the editing of out-of-domain\nimages which were never seen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alaluf_Y/0/1/0/all/0/1\">Yuval Alaluf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tov_O/0/1/0/all/0/1\">Omer Tov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokady_R/0/1/0/all/0/1\">Ron Mokady</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1\">Rinon Gal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task2Sim : Towards Effective Pre-training and Transfer from Synthetic Data. (arXiv:2112.00054v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00054","description":"<p>Pre-training models on Imagenet or other massive datasets of real images has\nled to major advances in computer vision, albeit accompanied with shortcomings\nrelated to curation cost, privacy, usage rights, and ethical issues. In this\npaper, for the first time, we study the transferability of pre-trained models\nbased on synthetic data generated by graphics simulators to downstream tasks\nfrom very different domains. In using such synthetic data for pre-training, we\nfind that downstream performance on different tasks are favored by different\nconfigurations of simulation parameters (e.g. lighting, object pose,\nbackgrounds, etc.), and that there is no one-size-fits-all solution. It is thus\nbetter to tailor synthetic pre-training data to a specific downstream task, for\nbest performance. We introduce Task2Sim, a unified model mapping downstream\ntask representations to optimal simulation parameters to generate synthetic\npre-training data for them. Task2Sim learns this mapping by training to find\nthe set of best parameters on a set of \"seen\" tasks. Once trained, it can then\nbe used to predict best simulation parameters for novel \"unseen\" tasks in one\nshot, without requiring additional training. Given a budget in number of images\nper class, our extensive experiments with 20 diverse downstream tasks show\nTask2Sim's task-adaptive pre-training data results in significantly better\ndownstream performance than non-adaptively choosing simulation parameters on\nboth seen and unseen tasks. It is even competitive with pre-training on real\nimages from Imagenet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phoo_C/0/1/0/all/0/1\">Cheng Perng Phoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chun-Fu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1\">Leonid Karlinsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio S. Feris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MonoScene: Monocular 3D Semantic Scene Completion. (arXiv:2112.00726v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00726","description":"<p>MonoScene proposes a 3D Semantic Scene Completion (SSC) framework, where the\ndense geometry and semantics of a scene are inferred from a single monocular\nRGB image. Different from the SSC literature, relying on 2.5 or 3D input, we\nsolve the complex problem of 2D to 3D scene reconstruction while jointly\ninferring its semantics. Our framework relies on successive 2D and 3D UNets\nbridged by a novel 2D-3D features projection inspiring from optics and\nintroduces a 3D context relation prior to enforce spatio-semantic consistency.\nAlong with architectural contributions, we introduce novel global scene and\nlocal frustums losses. Experiments show we outperform the literature on all\nmetrics and datasets while hallucinating plausible scenery even beyond the\ncamera field of view. Our code and trained models are available at\nhttps://github.com/cv-rits/MonoScene.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1\">Anh-Quan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1\">Raoul de Charette</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incremental Learning in Semantic Segmentation from Image Labels. (arXiv:2112.01882v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01882","description":"<p>Although existing semantic segmentation approaches achieve impressive\nresults, they still struggle to update their models incrementally as new\ncategories are uncovered. Furthermore, pixel-by-pixel annotations are expensive\nand time-consuming. This paper proposes a novel framework for Weakly\nIncremental Learning for Semantic Segmentation, that aims at learning to\nsegment new classes from cheap and largely available image-level labels. As\nopposed to existing approaches, that need to generate pseudo-labels offline, we\nuse an auxiliary classifier, trained with image-level labels and regularized by\nthe segmentation model, to obtain pseudo-supervision online and update the\nmodel incrementally. We cope with the inherent noise in the process by using\nsoft-labels generated by the auxiliary classifier. We demonstrate the\neffectiveness of our approach on the Pascal VOC and COCO datasets,\noutperforming offline weakly-supervised methods and obtaining results\ncomparable with incremental learning methods with full supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1\">Fabio Cermelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1\">Dario Fontanel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavera_A/0/1/0/all/0/1\">Antonio Tavera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1\">Marco Ciccone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel Class Discovery in Semantic Segmentation. (arXiv:2112.01900v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01900","description":"<p>We introduce a new setting of Novel Class Discovery in Semantic Segmentation\n(NCDSS), which aims at segmenting unlabeled images containing new classes given\nprior knowledge from a labeled set of disjoint classes. In contrast to existing\napproaches that look at novel class discovery in image classification, we focus\non the more challenging semantic segmentation. In NCDSS, we need to distinguish\nthe objects and background, and to handle the existence of multiple classes\nwithin an image, which increases the difficulty in using the unlabeled data. To\ntackle this new setting, we leverage the labeled base data and a saliency model\nto coarsely cluster novel classes for model training in our basic framework.\nAdditionally, we propose the Entropy-based Uncertainty Modeling and\nSelf-training (EUMS) framework to overcome noisy pseudo-labels, further\nimproving the model performance on the novel classes. Our EUMS utilizes an\nentropy ranking technique and a dynamic reassignment to distill clean labels,\nthereby making full use of the noisy data via self-supervised learning. We\nbuild the NCDSS benchmark on the PASCAL-5$^i$ dataset and COCO-20$^i$ dataset.\nExtensive experiments demonstrate the feasibility of the basic framework\n(achieving an average mIoU of 49.81% on PASCAL-5$^i$) and the effectiveness of\nEUMS framework (outperforming the basic framework by 9.28% mIoU on\nPASCAL-5$^i$).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gim Hee Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemanticStyleGAN: Learning Compositional Generative Priors for Controllable Image Synthesis and Editing. (arXiv:2112.02236v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02236","description":"<p>Recent studies have shown that StyleGANs provide promising prior models for\ndownstream tasks on image synthesis and editing. However, since the latent\ncodes of StyleGANs are designed to control global styles, it is hard to achieve\na fine-grained control over synthesized images. We present SemanticStyleGAN,\nwhere a generator is trained to model local semantic parts separately and\nsynthesizes images in a compositional way. The structure and texture of\ndifferent local parts are controlled by corresponding latent codes.\nExperimental results demonstrate that our model provides a strong\ndisentanglement between different spatial areas. When combined with editing\nmethods designed for StyleGANs, it can achieve a more fine-grained control to\nedit synthesized or real images. The model can also be extended to other\ndomains via transfer learning. Thus, as a generic prior model with built-in\ndisentanglement, it could facilitate the development of GAN-based applications\nand enable more potential downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yichun Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yangyue Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaohui Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactive Disentanglement: Learning Concepts by Interacting with their Prototype Representations. (arXiv:2112.02290v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02290","description":"<p>Learning visual concepts from raw images without strong supervision is a\nchallenging task. In this work, we show the advantages of prototype\nrepresentations for understanding and revising the latent space of neural\nconcept learners. For this purpose, we introduce interactive Concept Swapping\nNetworks (iCSNs), a novel framework for learning concept-grounded\nrepresentations via weak supervision and implicit prototype representations.\niCSNs learn to bind conceptual information to specific prototype slots by\nswapping the latent representations of paired images. This semantically\ngrounded and discrete latent space facilitates human understanding and\nhuman-machine interaction. We support this claim by conducting experiments on\nour novel data set \"Elementary Concept Reasoning\" (ECR), focusing on visual\nconcepts shared by geometric objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1\">Wolfgang Stammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memmel_M/0/1/0/all/0/1\">Marius Memmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Practical Monocular Indoor Depth Estimation. (arXiv:2112.02306v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02306","description":"<p>The majority of prior monocular depth estimation methods without groundtruth\ndepth guidance focus on driving scenarios. We show that such methods generalize\npoorly to unseen complex indoor scenes, where objects are cluttered and\narbitrarily arranged in the near field. To obtain more robustness, we propose a\nstructure distillation approach to learn knacks from an off-the-shelf relative\ndepth estimator that produces structured but metric-agnostic depth. By\ncombining structure distillation with a branch that learns metrics from\nleft-right consistency, we attain structured and metric depth for generic\nindoor scenes and make inferences in real-time. To facilitate learning and\nevaluation, we collect SimSIN, a dataset from simulation with thousands of\nenvironments, and UniSIN, a dataset that contains about 500 real scan sequences\nof generic indoor environments. We experiment in both sim-to-real and\nreal-to-real settings, and show improvements, as well as in downstream\napplications using our depth maps. This work provides a full study, covering\nmethods, data, and applications aspects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cho-Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_M/0/1/0/all/0/1\">Michael Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shuochen Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting LiDAR Registration and Reconstruction: A Range Image Perspective. (arXiv:2112.02779v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.02779","description":"<p>Spinning LiDAR data are prevalent for 3D vision tasks. Since LiDAR data is\npresented in the form of point clouds, expensive 3D operations are usually\nrequired. This paper revisits spinning LiDAR scan formation and presents a\ncylindrical range image representation with a ray-wise projection/unprojection\nmodel. It is built upon raw scans and supports lossless conversion from 2D to\n3D, allowing fast 2D operations, including 2D index-based neighbor search and\ndownsampling. We then propose, to the best of our knowledge, the first\nmulti-scale registration and dense signed distance function (SDF)\nreconstruction system for LiDAR range images. We further collect a dataset of\nindoor and outdoor LiDAR scenes in the posed range image format. A\ncomprehensive evaluation of registration and reconstruction is conducted on the\nproposed dataset and the KITTI dataset. Experiments demonstrate that our\napproach outperforms surface reconstruction baselines and achieves similar\nperformance to state-of-the-art LiDAR registration methods, including a modern\nlearning-based registration approach. Thanks to the simplicity, our\nregistration runs at 100Hz and SDF reconstruction in real time. The dataset and\na modularized C++/Python toolbox will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1\">Wei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_K/0/1/0/all/0/1\">Kwonyoung Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaess_M/0/1/0/all/0/1\">Michael Kaess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Facial Representation Learning in a Visual-Linguistic Manner. (arXiv:2112.03109v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03109","description":"<p>How to learn a universal facial representation that boosts all face analysis\ntasks? This paper takes one step toward this goal. In this paper, we study the\ntransfer performance of pre-trained models on face analysis tasks and introduce\na framework, called FaRL, for general Facial Representation Learning in a\nvisual-linguistic manner. On one hand, the framework involves a contrastive\nloss to learn high-level semantic meaning from image-text pairs. On the other\nhand, we propose exploring low-level information simultaneously to further\nenhance the face representation, by adding a masked image modeling. We perform\npre-training on LAION-FACE, a dataset containing large amount of face\nimage-text pairs, and evaluate the representation capability on multiple\ndownstream tasks. We show that FaRL achieves better transfer performance\ncompared with previous pre-trained models. We also verify its superiority in\nthe low-data regime. More importantly, our model surpasses the state-of-the-art\nmethods on face analysis tasks including face parsing and face alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinglin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yangyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Ming Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MS-TCT: Multi-Scale Temporal ConvTransformer for Action Detection. (arXiv:2112.03902v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03902","description":"<p>Action detection is an essential and challenging task, especially for densely\nlabelled datasets of untrimmed videos. The temporal relation is complex in\nthose datasets, including challenges like composite action, and co-occurring\naction. For detecting actions in those complex videos, efficiently capturing\nboth short-term and long-term temporal information in the video is critical. To\nthis end, we propose a novel ConvTransformer network for action detection. This\nnetwork comprises three main components: (1) Temporal Encoder module\nextensively explores global and local temporal relations at multiple temporal\nresolutions. (2) Temporal Scale Mixer module effectively fuses the multi-scale\nfeatures to have a unified feature representation. (3) Classification module is\nused to learn the instance center-relative position and predict the frame-level\nclassification scores. The extensive experiments on multiple datasets,\nincluding Charades, TSU and MultiTHUMOS, confirm the effectiveness of our\nproposed method. Our network outperforms the state-of-the-art methods on all\nthree datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_R/0/1/0/all/0/1\">Rui Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahatapitiya_K/0/1/0/all/0/1\">Kumara Kahatapitiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vehicle trajectory prediction works, but not everywhere. (arXiv:2112.03909v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03909","description":"<p>Vehicle trajectory prediction is nowadays a fundamental pillar of\nself-driving cars. Both the industry and research communities have acknowledged\nthe need for such a pillar by providing public benchmarks. While\nstate-of-the-art methods are impressive, i.e., they have no off-road\nprediction, their generalization to cities outside of the benchmark remains\nunexplored. In this work, we show that those methods do not generalize to new\nscenes. We present a method that automatically generates realistic scenes\ncausing state-of-the-art models to go off-road. We frame the problem through\nthe lens of adversarial scene generation. The method is a simple yet effective\ngenerative model based on atomic scene generation functions along with physical\nconstraints. Our experiments show that more than 60% of existing scenes from\nthe current benchmarks can be modified in a way to make prediction methods fail\n(i.e., predicting off-road). We further show that the generated scenes (i) are\nrealistic since they do exist in the real world, and (ii) can be used to make\nexisting models more robust, yielding 30-40 reductions in the off-road rate.\nThe code is available online: https://s-attack.github.io/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bahari_M/0/1/0/all/0/1\">Mohammadhossein Bahari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saadatnejad_S/0/1/0/all/0/1\">Saeed Saadatnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_A/0/1/0/all/0/1\">Ahmad Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaverdikondori_M/0/1/0/all/0/1\">Mohammad Shaverdikondori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahidzadeh_A/0/1/0/all/0/1\">Amir-Hossein Shahidzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alahi_A/0/1/0/all/0/1\">Alexandre Alahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures. (arXiv:2112.05135v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.05135","description":"<p>In real-world applications of machine learning, reliable and safe systems\nmust consider measures of performance beyond standard test set accuracy. These\nother goals include out-of-distribution (OOD) robustness, prediction\nconsistency, resilience to adversaries, calibrated uncertainty estimates, and\nthe ability to detect anomalous inputs. However, improving performance towards\nthese goals is often a balancing act that today's methods cannot achieve\nwithout sacrificing performance on other safety axes. For instance, adversarial\ntraining improves adversarial robustness but sharply degrades other classifier\nperformance metrics. Similarly, strong data augmentation and regularization\ntechniques often improve OOD robustness but harm anomaly detection, raising the\nquestion of whether a Pareto improvement on all existing safety measures is\npossible. To meet this challenge, we design a new data augmentation strategy\nutilizing the natural structural complexity of pictures such as fractals, which\noutperforms numerous baselines, is near Pareto-optimal, and roundly improves\nsafety measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1\">Andy Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1\">Mantas Mazeika</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Leonard Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label, Verify, Correct: A Simple Few Shot Object Detection Method. (arXiv:2112.05749v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.05749","description":"<p>The objective of this paper is few-shot object detection (FSOD) -- the task\nof expanding an object detector for a new category given only a few instances\nfor training. We introduce a simple pseudo-labelling method to source\nhigh-quality pseudo-annotations from the training set, for each new category,\nvastly increasing the number of training instances and reducing class\nimbalance; our method finds previously unlabelled instances. Na\\\"ively training\nwith model predictions yields sub-optimal performance; we present two novel\nmethods to improve the precision of the pseudo-labelling process: first, we\nintroduce a verification technique to remove candidate detections with\nincorrect class labels; second, we train a specialised model to correct poor\nquality bounding boxes. After these two novel steps, we obtain a large set of\nhigh-quality pseudo-annotations that allow our final detector to be trained\nend-to-end. Additionally, we demonstrate our method maintains base class\nperformance, and the utility of simple augmentations in FSOD. While\nbenchmarking on PASCAL VOC and MS-COCO, our method achieves state-of-the-art or\nsecond-best performance compared to existing approaches across all number of\nshots.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaul_P/0/1/0/all/0/1\">Prannay Kaul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-shot Keypoint Detection with Uncertainty Learning for Unseen Species. (arXiv:2112.06183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06183","description":"<p>Current non-rigid object keypoint detectors perform well on a chosen kind of\nspecies and body parts, and require a large amount of labelled keypoints for\ntraining. Moreover, their heatmaps, tailored to specific body parts, cannot\nrecognize novel keypoints (keypoints not labelled for training) on unseen\nspecies. We raise an interesting yet challenging question: how to detect both\nbase (annotated for training) and novel keypoints for unseen species given a\nfew annotated samples? Thus, we propose a versatile Few-shot Keypoint Detection\n(FSKD) pipeline, which can detect a varying number of keypoints of different\nkinds. Our FSKD provides the uncertainty estimation of predicted keypoints.\nSpecifically, FSKD involves main and auxiliary keypoint representation\nlearning, similarity learning, and keypoint localization with uncertainty\nmodeling to tackle the localization noise. Moreover, we model the uncertainty\nacross groups of keypoints by multivariate Gaussian distribution to exploit\nimplicit correlations between neighboring keypoints. We show the effectiveness\nof our FSKD on (i) novel keypoint detection for unseen species, and (ii)\nfew-shot Fine-Grained Visual Recognition (FGVR) and (iii) Semantic Alignment\n(SA) downstream tasks. For FGVR, detected keypoints improve the classification\naccuracy. For SA, we showcase a novel thin-plate-spline warping that uses\nestimated keypoint uncertainty under imperfect keypoint corespondences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Changsheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lifelong Unsupervised Domain Adaptive Person Re-identification with Coordinated Anti-forgetting and Adaptation. (arXiv:2112.06632v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06632","description":"<p>Unsupervised domain adaptive person re-identification (ReID) has been\nextensively investigated to mitigate the adverse effects of domain gaps. Those\nworks assume the target domain data can be accessible all at once. However, for\nthe real-world streaming data, this hinders the timely adaptation to changing\ndata statistics and sufficient exploitation of increasing samples. In this\npaper, to address more practical scenarios, we propose a new task, Lifelong\nUnsupervised Domain Adaptive (LUDA) person ReID. This is challenging because it\nrequires the model to continuously adapt to unlabeled data in the target\nenvironments while alleviating catastrophic forgetting for such a fine-grained\nperson retrieval task. We design an effective scheme for this task, dubbed\nCLUDA-ReID, where the anti-forgetting is harmoniously coordinated with the\nadaptation. Specifically, a meta-based Coordinated Data Replay strategy is\nproposed to replay old data and update the network with a coordinated\noptimization direction for both adaptation and memorization. Moreover, we\npropose Relational Consistency Learning for old knowledge\ndistillation/inheritance in line with the objective of retrieval-based tasks.\nWe set up two evaluation settings to simulate the practical application\nscenarios. Extensive experiments demonstrate the effectiveness of our\nCLUDA-ReID for both scenarios with stationary target streams and scenarios with\ndynamic target streams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhipeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1\">Cuiling Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_P/0/1/0/all/0/1\">Peng Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1\">Quanzeng You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Depth Estimation by Fusing Single-View Depth Probability with Multi-View Geometry. (arXiv:2112.08177v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08177","description":"<p>Multi-view depth estimation methods typically require the computation of a\nmulti-view cost-volume, which leads to huge memory consumption and slow\ninference. Furthermore, multi-view matching can fail for texture-less surfaces,\nreflective surfaces and moving objects. For such failure modes, single-view\ndepth estimation methods are often more reliable. To this end, we propose\nMaGNet, a novel framework for fusing single-view depth probability with\nmulti-view geometry, to improve the accuracy, robustness and efficiency of\nmulti-view depth estimation. For each frame, MaGNet estimates a single-view\ndepth probability distribution, parameterized as a pixel-wise Gaussian. The\ndistribution estimated for the reference frame is then used to sample per-pixel\ndepth candidates. Such probabilistic sampling enables the network to achieve\nhigher accuracy while evaluating fewer depth candidates. We also propose depth\nconsistency weighting for the multi-view matching score, to ensure that the\nmulti-view depth is consistent with the single-view predictions. The proposed\nmethod achieves state-of-the-art performance on ScanNet, 7-Scenes and KITTI.\nQualitative evaluation demonstrates that our method is more robust against\nchallenging artifacts such as texture-less/reflective surfaces and moving\nobjects. Our code and model weights are available at\nhttps://github.com/baegwangbin/MaGNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_G/0/1/0/all/0/1\">Gwangbin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budvytis_I/0/1/0/all/0/1\">Ignas Budvytis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1\">Roberto Cipolla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Putting People in their Place: Monocular Regression of 3D People in Depth. (arXiv:2112.08274v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08274","description":"<p>Given an image with multiple people, our goal is to directly regress the pose\nand shape of all the people as well as their relative depth. Inferring the\ndepth of a person in an image, however, is fundamentally ambiguous without\nknowing their height. This is particularly problematic when the scene contains\npeople of very different sizes, e.g. from infants to adults. To solve this, we\nneed several things. First, we develop a novel method to infer the poses and\ndepth of multiple people in a single image. While previous work that estimates\nmultiple people does so by reasoning in the image plane, our method, called\nBEV, adds an additional imaginary Bird's-Eye-View representation to explicitly\nreason about depth. BEV reasons simultaneously about body centers in the image\nand in depth and, by combing these, estimates 3D body position. Unlike prior\nwork, BEV is a single-shot method that is end-to-end differentiable. Second,\nheight varies with age, making it impossible to resolve depth without also\nestimating the age of people in the image. To do so, we exploit a 3D body model\nspace that lets BEV infer shapes from infants to adults. Third, to train BEV,\nwe need a new dataset. Specifically, we create a \"Relative Human\" (RH) dataset\nthat includes age labels and relative depth relationships between the people in\nthe images. Extensive experiments on RH and AGORA demonstrate the effectiveness\nof the model and training scheme. BEV outperforms existing methods on depth\nreasoning, child shape estimation, and robustness to occlusion. The code and\ndataset are released for research purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Q/0/1/0/all/0/1\">Qian Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yili Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ICON: Implicit Clothed humans Obtained from Normals. (arXiv:2112.09127v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09127","description":"<p>Current methods for learning realistic and animatable 3D clothed avatars need\neither posed 3D scans or 2D images with carefully controlled user poses. In\ncontrast, our goal is to learn an avatar from only 2D images of people in\nunconstrained poses. Given a set of images, our method estimates a detailed 3D\nsurface from each image and then combines these into an animatable avatar.\nImplicit functions are well suited to the first task, as they can capture\ndetails like hair and clothes. Current methods, however, are not robust to\nvaried human poses and often produce 3D surfaces with broken or disembodied\nlimbs, missing details, or non-human shapes. The problem is that these methods\nuse global feature encoders that are sensitive to global pose. To address this,\nwe propose ICON (\"Implicit Clothed humans Obtained from Normals\"), which,\ninstead, uses local features. ICON has two main modules, both of which exploit\nthe SMPL(-X) body model. First, ICON infers detailed clothed-human normals\n(front/back) conditioned on the SMPL(-X) normals. Second, a visibility-aware\nimplicit surface regressor produces an iso-surface of a human occupancy field.\nImportantly, at inference time, a feedback loop alternates between refining the\nSMPL(-X) mesh using the inferred clothed normals and then refining the normals.\nGiven multiple reconstructed frames of a subject in varied poses, we use\nSCANimate to produce an animatable avatar from them. Evaluation on the AGORA\nand CAPE datasets shows that ICON outperforms the state of the art in\nreconstruction, even with heavily limited training data. Additionally, it is\nmuch more robust to out-of-distribution samples, e.g., in-the-wild poses/images\nand out-of-frame cropping. ICON takes a step towards robust 3D clothed human\nreconstruction from in-the-wild images. This enables creating avatars directly\nfrom video with personalized and natural pose-dependent cloth deformation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiu_Y/0/1/0/all/0/1\">Yuliang Xiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinlong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1\">Dimitrios Tzionas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light Field Neural Rendering. (arXiv:2112.09687v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09687","description":"<p>Classical light field rendering for novel view synthesis can accurately\nreproduce view-dependent effects such as reflection, refraction, and\ntranslucency, but requires a dense view sampling of the scene. Methods based on\ngeometric reconstruction need only sparse views, but cannot accurately model\nnon-Lambertian effects. We introduce a model that combines the strengths and\nmitigates the limitations of these two directions. By operating on a\nfour-dimensional representation of the light field, our model learns to\nrepresent view-dependent effects accurately. By enforcing geometric constraints\nduring training and inference, the scene geometry is implicitly learned from a\nsparse set of views. Concretely, we introduce a two-stage transformer-based\nmodel that first aggregates features along epipolar lines, then aggregates\nfeatures along reference views to produce the color of a target ray. Our model\noutperforms the state-of-the-art on multiple forward-facing and 360{\\deg}\ndatasets, with larger margins on scenes with severe view-dependent variations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suhail_M/0/1/0/all/0/1\">Mohammed Suhail</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteves_C/0/1/0/all/0/1\">Carlos Esteves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1\">Leonid Sigal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makadia_A/0/1/0/all/0/1\">Ameesh Makadia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mega-NeRF: Scalable Construction of Large-Scale NeRFs for Virtual Fly-Throughs. (arXiv:2112.10703v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10703","description":"<p>We use neural radiance fields (NeRFs) to build interactive 3D environments\nfrom large-scale visual captures spanning buildings or even multiple city\nblocks collected primarily from drones. In contrast to single object scenes (on\nwhich NeRFs are traditionally evaluated), our scale poses multiple challenges\nincluding (1) the need to model thousands of images with varying lighting\nconditions, each of which capture only a small subset of the scene, (2)\nprohibitively large model capacities that make it infeasible to train on a\nsingle GPU, and (3) significant challenges for fast rendering that would enable\ninteractive fly-throughs.\n</p>\n<p>To address these challenges, we begin by analyzing visibility statistics for\nlarge-scale scenes, motivating a sparse network structure where parameters are\nspecialized to different regions of the scene. We introduce a simple geometric\nclustering algorithm for data parallelism that partitions training images (or\nrather pixels) into different NeRF submodules that can be trained in parallel.\n</p>\n<p>We evaluate our approach on existing datasets (Quad 6k and UrbanScene3D) as\nwell as against our own drone footage, improving training speed by 3x and PSNR\nby 12%. We also evaluate recent NeRF fast renderers on top of Mega-NeRF and\nintroduce a novel method that exploits temporal coherence. Our technique\nachieves a 40x speedup over conventional NeRF rendering while remaining within\n0.8 db in PSNR quality, exceeding the fidelity of existing fast renderers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Turki_H/0/1/0/all/0/1\">Haithem Turki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satyanarayanan_M/0/1/0/all/0/1\">Mahadev Satyanarayanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NinjaDesc: Content-Concealing Visual Descriptors via Adversarial Learning. (arXiv:2112.12785v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12785","description":"<p>In the light of recent analyses on privacy-concerning scene revelation from\nvisual descriptors, we develop descriptors that conceal the input image\ncontent. In particular, we propose an adversarial learning framework for\ntraining visual descriptors that prevent image reconstruction, while\nmaintaining the matching accuracy. We let a feature encoding network and image\nreconstruction network compete with each other, such that the feature encoder\ntries to impede the image reconstruction with its generated descriptors, while\nthe reconstructor tries to recover the input image from the descriptors. The\nexperimental results demonstrate that the visual descriptors obtained with our\nmethod significantly deteriorate the image reconstruction quality with minimal\nimpact on correspondence matching and camera localization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1\">Tony Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyo Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1\">Vincent Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeTone_D/0/1/0/all/0/1\">Daniel DeTone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsun-Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianwei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilg_E/0/1/0/all/0/1\">Eddy Ilg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balntas_V/0/1/0/all/0/1\">Vassileios Balntas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Chris Sweeney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EM-driven unsupervised learning for efficient motion segmentation. (arXiv:2201.02074v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02074","description":"<p>In this paper, we present a CNN-based fully unsupervised method for motion\nsegmentation from optical flow. We assume that the input optical flow can be\nrepresented as a piecewise set of parametric motion models, typically, affine\nor quadratic motion models. The core idea of our work is to leverage the\nExpectation-Maximization (EM) framework in order to design in a well-founded\nmanner a loss function and a training procedure of our motion segmentation\nneural network that does not require either ground-truth or manual annotation.\nHowever, in contrast to the classical iterative EM, once the network is\ntrained, we can provide a segmentation for any unseen optical flow field in a\nsingle inference step and without estimating any motion models. Different loss\nfunctions have been investigated including robust ones. We also propose a novel\nefficient data augmentation technique on the optical flow field, applicable to\nany network taking optical flow as input. In addition, our method is able by\ndesign to segment multiple motions. Our motion segmentation network was tested\non four benchmarks, DAVIS2016, SegTrackV2, FBMS59, and MoCA, and performed very\nwell, while being fast at test time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meunier_E/0/1/0/all/0/1\">Etienne Meunier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badoual_A/0/1/0/all/0/1\">Ana&#xef;s Badoual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouthemy_P/0/1/0/all/0/1\">Patrick Bouthemy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eye Know You Too: A DenseNet Architecture for End-to-end Eye Movement Biometrics. (arXiv:2201.02110v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.02110","description":"<p>Eye movement biometrics (EMB) is a relatively recent behavioral biometric\nmodality that may have the potential to become the primary authentication\nmethod in virtual- and augmented-reality devices due to their emerging use of\neye-tracking sensors to enable foveated rendering techniques. However, existing\nEMB models have yet to demonstrate levels of performance that would be\nacceptable for real-world use. Deep learning approaches to EMB have largely\nemployed plain convolutional neural networks (CNNs), but there have been many\nmilestone improvements to convolutional architectures over the years including\nresidual networks (ResNets) and densely connected convolutional networks\n(DenseNets). The present study employs a DenseNet architecture for end-to-end\nEMB and compares the proposed model against the most relevant prior works. The\nproposed technique not only outperforms the previous state of the art, but is\nalso the first to approach a level of authentication performance that would be\nacceptable for real-world use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lohr_D/0/1/0/all/0/1\">Dillon Lohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komogortsev_O/0/1/0/all/0/1\">Oleg V Komogortsev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Magnification with Multi-Layer Images. (arXiv:2201.05023v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05023","description":"<p>Representing scenes with multiple semi-transparent colored layers has been a\npopular and successful choice for real-time novel view synthesis. Existing\napproaches infer colors and transparency values over regularly-spaced layers of\nplanar or spherical shape. In this work, we introduce a new view synthesis\napproach based on multiple semi-transparent layers with scene-adapted geometry.\nOur approach infers such representations from stereo pairs in two stages. The\nfirst stage infers the geometry of a small number of data-adaptive layers from\na given pair of views. The second stage infers the color and the transparency\nvalues for these layers producing the final representation for novel view\nsynthesis. Importantly, both stages are connected through a differentiable\nrenderer and are trained in an end-to-end manner. In the experiments, we\ndemonstrate the advantage of the proposed approach over the use of\nregularly-spaced layers with no adaptation to scene geometry. Despite being\norders of magnitude faster during rendering, our approach also outperforms a\nrecently proposed IBRNet system based on implicit geometry representation. See\nresults at https://samsunglabs.github.io/StereoLayers .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khakhulin_T/0/1/0/all/0/1\">Taras Khakhulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korzhenkov_D/0/1/0/all/0/1\">Denis Korzhenkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Solovev_P/0/1/0/all/0/1\">Pavel Solovev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sterkin_G/0/1/0/all/0/1\">Gleb Sterkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ardelean_T/0/1/0/all/0/1\">Timotei Ardelean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lempitsky_V/0/1/0/all/0/1\">Victor Lempitsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Saliency Constrained Arbitrary Image Style Transfer using SIFT and DCNN. (arXiv:2201.05346v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.05346","description":"<p>This paper develops a new image synthesis approach to transfer an example\nimage (style image) to other images (content images) by using Deep\nConvolutional Neural Networks (DCNN) model. When common neural style transfer\nmethods are used, the textures and colors in the style image are usually\ntransferred imperfectly to the content image, or some visible errors are\ngenerated. This paper proposes a novel saliency constrained method to reduce or\navoid such effects. It first evaluates some existing saliency detection methods\nto select the most suitable one for use in our method. The selected saliency\ndetection method is used to detect the object in the style image, corresponding\nto the object of the content image with the same saliency. In addition, aim to\nsolve the problem that the size or resolution is different in the style image\nand content, the scale-invariant feature transform is used to generate a series\nof style images and content images which can be used to generate more feature\nmaps for patches matching. It then proposes a new loss function combining the\nsaliency loss, style loss and content loss, adding gradient of saliency\nconstraint into style transfer in iterations. Finally the source images and\nsaliency detection results are utilized as multichannel input to an improved\ndeep CNN framework for style transfer. The experiments show that the saliency\nmaps of source images can help find the correct matching and avoid artifacts.\nExperimental results on different kind of images demonstrate that our method\noutperforms nine representative methods from recent publications and has good\nrobustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">HuiHuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuhua Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SS-3DCapsNet: Self-supervised 3D Capsule Networks for Medical Segmentation on Less Labeled Data. (arXiv:2201.05905v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.05905","description":"<p>Capsule network is a recent new deep network architecture that has been\napplied successfully for medical image segmentation tasks. This work extends\ncapsule networks for volumetric medical image segmentation with self-supervised\nlearning. To improve on the problem of weight initialization compared to\nprevious capsule networks, we leverage self-supervised learning for capsule\nnetworks pre-training, where our pretext-task is optimized by\nself-reconstruction. Our capsule network, SS-3DCapsNet, has a UNet-based\narchitecture with a 3D Capsule encoder and 3D CNNs decoder. Our experiments on\nmultiple datasets including iSeg-2017, Hippocampus, and Cardiac demonstrate\nthat our 3D capsule network with self-supervised pre-training considerably\noutperforms previous capsule networks and 3D-UNets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ly_L/0/1/0/all/0/1\">Loi Ly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hua_B/0/1/0/all/0/1\">Binh-Son Hua</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can We Find Neurons that Cause Unrealistic Images in Deep Generative Networks?. (arXiv:2201.06346v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.06346","description":"<p>Even though image generation with Generative Adversarial Networks has been\nshowing remarkable ability to generate high-quality images, GANs do not always\nguarantee photorealistic images will be generated. Sometimes they generate\nimages that have defective or unnatural objects, which are referred to as\n'artifacts'. Research to determine why the artifacts emerge and how they can be\ndetected and removed has not been sufficiently carried out. To analyze this, we\nfirst hypothesize that rarely activated neurons and frequently activated\nneurons have different purposes and responsibilities for the progress of\ngenerating images. By analyzing the statistics and the roles for those neurons,\nwe empirically show that rarely activated neurons are related to failed results\nof making diverse objects and lead to artifacts. In addition, we suggest a\ncorrection method, called 'sequential ablation', to repair the defective part\nof the generated images without complex computational cost and manual efforts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hwanil Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Wonjoon Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jaesik Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShapeFormer: Transformer-based Shape Completion via Sparse Representation. (arXiv:2201.10326v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10326","description":"<p>We present ShapeFormer, a transformer-based network that produces a\ndistribution of object completions, conditioned on incomplete, and possibly\nnoisy, point clouds. The resultant distribution can then be sampled to generate\nlikely completions, each exhibiting plausible shape details while being\nfaithful to the input. To facilitate the use of transformers for 3D, we\nintroduce a compact 3D representation, vector quantized deep implicit function,\nthat utilizes spatial sparsity to represent a close approximation of a 3D shape\nby a short sequence of discrete variables. Experiments demonstrate that\nShapeFormer outperforms prior art for shape completion from ambiguous partial\ninputs in terms of both completion quality and diversity. We also show that our\napproach effectively handles a variety of shape types, incomplete patterns, and\nreal-world scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xingguang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1\">Niloy J. Mitra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1\">Dani Lischinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1\">Daniel Cohen-Or</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactron: Embodied Adaptive Object Detection. (arXiv:2202.00660v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.00660","description":"<p>Over the years various methods have been proposed for the problem of object\ndetection. Recently, we have witnessed great strides in this domain owing to\nthe emergence of powerful deep neural networks. However, there are typically\ntwo main assumptions common among these approaches. First, the model is trained\non a fixed training set and is evaluated on a pre-recorded test set. Second,\nthe model is kept frozen after the training phase, so no further updates are\nperformed after the training is finished. These two assumptions limit the\napplicability of these methods to real-world settings. In this paper, we\npropose Interactron, a method for adaptive object detection in an interactive\nsetting, where the goal is to perform object detection in images observed by an\nembodied agent navigating in different environments. Our idea is to continue\ntraining during inference and adapt the model at test time without any explicit\nsupervision via interacting with the environment. Our adaptive object detection\nmodel provides a 11.8 point improvement in AP (and 19.1 points in AP50) over\nDETR, a recent, high-performance object detector. Moreover, we show that our\nobject detection model adapts to environments with completely different\nappearance characteristics, and its performance is on par with a model trained\nwith full supervision for those environments. The code is available at:\nhttps://github.com/allenai/interactron .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ada-NETS: Face Clustering via Adaptive Neighbour Discovery in the Structure Space. (arXiv:2202.03800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.03800","description":"<p>Face clustering has attracted rising research interest recently to take\nadvantage of massive amounts of face images on the web. State-of-the-art\nperformance has been achieved by Graph Convolutional Networks (GCN) due to\ntheir powerful representation capacity. However, existing GCN-based methods\nbuild face graphs mainly according to kNN relations in the feature space, which\nmay lead to a lot of noise edges connecting two faces of different classes. The\nface features will be polluted when messages pass along these noise edges, thus\ndegrading the performance of GCNs. In this paper, a novel algorithm named\nAda-NETS is proposed to cluster faces by constructing clean graphs for GCNs. In\nAda-NETS, each face is transformed to a new structure space, obtaining robust\nfeatures by considering face features of the neighbour images. Then, an\nadaptive neighbour discovery strategy is proposed to determine a proper number\nof edges connecting to each face image. It significantly reduces the noise\nedges while maintaining the good ones to build a graph with clean yet rich\nedges for GCNs to cluster faces. Experiments on multiple public clustering\ndatasets show that Ada-NETS significantly outperforms current state-of-the-art\nmethods, proving its superiority and generalization. Code is available at\nhttps://github.com/damo-cv/Ada-NETS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaohua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yaobin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fangyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">YuQi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Senzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiuyu Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining the Silhouette and Skeleton Data for Gait Recognition. (arXiv:2202.10645v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.10645","description":"<p>Gait recognition, a promising long-distance biometric technology, has aroused\nintense interest in computer vision. Existing works on gait recognition can be\ndivided into appearance-based methods and model-based methods, which extract\nfeatures from silhouettes and skeleton data, respectively. However, since\nappearance-based methods are greatly affected by clothing changing and carrying\ncondition, and model-based methods are limited by the accuracy of pose\nestimation approaches, gait recognition remains challenging in practical\napplications. In order to integrate the advantages of such two approaches, a\ntwo-branch neural network (NN) is proposed in this paper. Our method contains\ntwo branches, namely a CNN-based branch taking silhouettes as input and a\nGCN-based branch taking skeletons as input. In addition, two new modules are\nproposed in the GCN-based branch for better gait representation. First, we\npresent a simple yet effective fully connected graph convolution operator to\nintegrate the multi-scale graph convolutions and alleviate the dependence on\nnatural human joint connections. Second, we deploy a multi-dimension attention\nmodule named STC-Att to learn spatial, temporal and channel-wise attention\nsimultaneously. We evaluated the proposed two-branch neural network on the\nCASIA-B dataset. The experimental results show that our method achieves\nstate-of-the-art performance in various conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Likai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Ruize Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Free Object Segments for Long-Tailed Instance Segmentation. (arXiv:2202.11124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11124","description":"<p>One fundamental challenge in building an instance segmentation model for a\nlarge number of classes in complex scenes is the lack of training examples,\nespecially for rare objects. In this paper, we explore the possibility to\nincrease the training examples without laborious data collection and\nannotation. We find that an abundance of instance segments can potentially be\nobtained freely from object-centric images, according to two insights: (i) an\nobject-centric image usually contains one salient object in a simple\nbackground; (ii) objects from the same class often share similar appearances or\nsimilar contrasts to the background. Motivated by these insights, we propose a\nsimple and scalable framework FreeSeg for extracting and leveraging these\n\"free\" object foreground segments to facilitate model training in long-tailed\ninstance segmentation. Concretely, we investigate the similarity among\nobject-centric images of the same class to propose candidate segments of\nforeground instances, followed by a novel ranking of segment quality. The\nresulting high-quality object segments can then be used to augment the existing\nlong-tailed datasets, e.g., by copying and pasting the segments onto the\noriginal training images. Extensive experiments show that FreeSeg yields\nsubstantial improvements on top of strong baselines and achieves\nstate-of-the-art accuracy for segmenting rare object categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1\">Tai-Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianle Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1\">Jike Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_W/0/1/0/all/0/1\">Wenjin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeachAugment: Data Augmentation Optimization Using Teacher Knowledge. (arXiv:2202.12513v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12513","description":"<p>Optimization of image transformation functions for the purpose of data\naugmentation has been intensively studied. In particular, adversarial data\naugmentation strategies, which search augmentation maximizing task loss, show\nsignificant improvement in the model generalization for many tasks. However,\nthe existing methods require careful parameter tuning to avoid excessively\nstrong deformations that take away image features critical for acquiring\ngeneralization. In this paper, we propose a data augmentation optimization\nmethod based on the adversarial strategy called TeachAugment, which can produce\ninformative transformed images to the model without requiring careful tuning by\nleveraging a teacher model. Specifically, the augmentation is searched so that\naugmented images are adversarial for the target model and recognizable for the\nteacher model. We also propose data augmentation using neural networks, which\nsimplifies the search space design and allows for updating of the data\naugmentation using the gradient method. We show that TeachAugment outperforms\nexisting methods in experiments of image classification, semantic segmentation,\nand unsupervised representation learning tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_T/0/1/0/all/0/1\">Teppei Suzuki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConvNeXt-backbone HoVerNet for nuclei segmentation and classification. (arXiv:2202.13560v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.13560","description":"<p>This manuscript gives a brief description of the algorithm used to\nparticipate in CoNIC Challenge 2022. After the baseline was made available, we\nfollow the method in it and replace the ResNet baseline with ConvNeXt one.\nMoreover, we propose to first convert RGB space to Haematoxylin-Eosin-DAB(HED)\nspace, then use Haematoxylin composition of origin image to smooth semantic one\nhot label. Afterwards, nuclei distribution of train and valid set are explored\nto select the best fold split for training model for final test phase\nsubmission. Results on validation set shows that even with channel of each\nstage smaller in number, HoVerNet with ConvNeXt-tiny backbone still improves\nthe mPQ+ by 0.04 and multi r2 by 0.0144\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Chixin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_B/0/1/0/all/0/1\">Banban Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zekun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion. (arXiv:2203.00838v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00838","description":"<p>A well-known challenge in applying deep-learning methods to omnidirectional\nimages is spherical distortion. In dense regression tasks such as depth\nestimation, where structural details are required, using a vanilla CNN layer on\nthe distorted 360 image results in undesired information loss. In this paper,\nwe propose a 360 monocular depth estimation pipeline, OmniFusion, to tackle the\nspherical distortion issue. Our pipeline transforms a 360 image into\nless-distorted perspective patches (i.e. tangent images) to obtain patch-wise\npredictions via CNN, and then merge the patch-wise results for final output. To\nhandle the discrepancy between patch-wise predictions which is a major issue\naffecting the merging quality, we propose a new framework with the following\nkey components. First, we propose a geometry-aware feature fusion mechanism\nthat combines 3D geometric features with 2D image features to compensate for\nthe patch-wise discrepancy. Second, we employ the self-attention-based\ntransformer architecture to conduct a global aggregation of patch-wise\ninformation, which further improves the consistency. Last, we introduce an\niterative depth refinement mechanism, to further refine the estimated depth\nbased on the more accurate geometric features. Experiments show that our method\ngreatly mitigates the distortion issue, and achieves state-of-the-art\nperformances on several 360 monocular depth estimation benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhixin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Ye Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liu Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DN-DETR: Accelerate DETR Training by Introducing Query DeNoising. (arXiv:2203.01305v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01305","description":"<p>We present in this paper a novel denoising training method to speedup DETR\n(DEtection TRansformer) training and offer a deepened understanding of the slow\nconvergence issue of DETR-like methods. We show that the slow convergence\nresults from the instability of bipartite graph matching which causes\ninconsistent optimization goals in early training stages. To address this\nissue, except for the Hungarian loss, our method additionally feeds\nground-truth bounding boxes with noises into Transformer decoder and trains the\nmodel to reconstruct the original boxes, which effectively reduces the\nbipartite graph matching difficulty and leads to a faster convergence. Our\nmethod is universal and can be easily plugged into any DETR-like methods by\nadding dozens of lines of code to achieve a remarkable improvement. As a\nresult, our DN-DETR results in a remarkable improvement ($+1.9$AP) under the\nsame setting and achieves the best result (AP $43.4$ and $48.6$ with $12$ and\n$50$ epochs of training respectively) among DETR-like methods with ResNet-$50$\nbackbone. Compared with the baseline under the same setting, DN-DETR achieves\ncomparable performance with $50\\%$ training epochs. Code is available at\n\\url{https://github.com/FengLi-ust/DN-DETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BatchFormer: Learning to Explore Sample Relationships for Robust Representation Learning. (arXiv:2203.01522v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01522","description":"<p>Despite the success of deep neural networks, there are still many challenges\nin deep representation learning due to the data scarcity issues such as data\nimbalance, unseen distribution, and domain shift. To address the\nabove-mentioned issues, a variety of methods have been devised to explore the\nsample relationships in a vanilla way (i.e., from the perspectives of either\nthe input or the loss function), failing to explore the internal structure of\ndeep neural networks for learning with sample relationships. Inspired by this,\nwe propose to enable deep neural networks themselves with the ability to learn\nthe sample relationships from each mini-batch. Specifically, we introduce a\nbatch transformer module or BatchFormer, which is then applied into the batch\ndimension of each mini-batch to implicitly explore sample relationships during\ntraining. By doing this, the proposed method enables the collaboration of\ndifferent samples, e.g., the head-class samples can also contribute to the\nlearning of the tail classes for long-tailed recognition. Furthermore, to\nmitigate the gap between training and testing, we share the classifier between\nwith or without the BatchFormer during training, which can thus be removed\nduring testing. We perform extensive experiments on over ten datasets and the\nproposed method achieves significant improvements on different data scarcity\napplications without any bells and whistles, including the tasks of long-tailed\nrecognition, compositional zero-shot learning, domain generalization, and\ncontrastive learning. Code will be made publicly available at\nhttps://github.com/zhihou7/BatchFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction. (arXiv:2203.01577v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01577","description":"<p>We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,\nto catalyze the research of category-level human-object interaction. HOI4D\nconsists of 3M RGB-D egocentric video frames over 5000 sequences collected by 9\nparticipants interacting with 1000 different object instances from 20\ncategories over 610 different indoor rooms. Frame-wise annotations for panoptic\nsegmentation, motion segmentation, 3D hand pose, category-level object pose and\nhand action have also been provided, together with reconstructed object meshes\nand scene point clouds. With HOI4D, we establish three benchmarking tasks to\npromote category-level HOI from 4D visual signals including semantic\nsegmentation of 4D dynamic point cloud sequences, category-level object pose\ntracking, and egocentric action segmentation with diverse interaction targets.\nIn-depth analysis shows HOI4D poses great challenges to existing methods and\nproduces great research opportunities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Che Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1\">Kangbo Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weikang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1\">Boqiang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhoujie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Style-ERD: Responsive and Coherent Online Motion Style Transfer. (arXiv:2203.02574v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.02574","description":"<p>Motion style transfer is a common method for enriching character animation.\nMotion style transfer algorithms are often designed for offline settings where\nmotions are processed in segments. However, for online animation applications,\nsuch as realtime avatar animation from motion capture, motions need to be\nprocessed as a stream with minimal latency. In this work, we realize a\nflexible, high-quality motion style transfer method for this setting. We\npropose a novel style transfer model, Style-ERD, to stylize motions in an\nonline manner with an Encoder-Recurrent-Decoder structure, along with a novel\ndiscriminator that combines feature attention and temporal attention. Our\nmethod stylizes motions into multiple target styles with a unified model.\nAlthough our method targets online settings, it outperforms previous offline\nmethods in motion realism and style expressiveness and provides significant\ngains in runtime efficiency\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianxin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xiaohang Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhongquan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panne_M/0/1/0/all/0/1\">Michiel van de Panne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection. (arXiv:2203.03605v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03605","description":"<p>We present DINO (\\textbf{D}ETR with \\textbf{I}mproved de\\textbf{N}oising\nanch\\textbf{O}r boxes), a state-of-the-art end-to-end object detector. % in\nthis paper. DINO improves over previous DETR-like models in performance and\nefficiency by using a contrastive way for denoising training, a mixed query\nselection method for anchor initialization, and a look forward twice scheme for\nbox prediction. DINO achieves $48.3$AP in $12$ epochs and $51.0$AP in $36$\nepochs on COCO with a ResNet-50 backbone and multi-scale features, yielding a\nsignificant improvement of $\\textbf{+4.9}$\\textbf{AP} and\n$\\textbf{+2.4}$\\textbf{AP}, respectively, compared to DN-DETR, the previous\nbest DETR-like model. DINO scales well in both model size and data size.\nWithout bells and whistles, after pre-training on the Objects365 dataset with a\nSwinL backbone, DINO obtains the best results on both COCO \\texttt{val2017}\n($\\textbf{63.2}$\\textbf{AP}) and \\texttt{test-dev}\n(\\textbf{$\\textbf{63.3}$AP}). Compared to other models on the leaderboard, DINO\nsignificantly reduces its model size and pre-training data size while achieving\nbetter results. Our code will be available at\n\\url{https://github.com/IDEACVR/DINO}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_L/0/1/0/all/0/1\">Lionel M. Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human-Aware Object Placement for Visual Environment Reconstruction. (arXiv:2203.03609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03609","description":"<p>Humans are in constant contact with the world as they move through it and\ninteract with it. This contact is a vital source of information for\nunderstanding 3D humans, 3D scenes, and the interactions between them. In fact,\nwe demonstrate that these human-scene interactions (HSIs) can be leveraged to\nimprove the 3D reconstruction of a scene from a monocular RGB video. Our key\nidea is that, as a person moves through a scene and interacts with it, we\naccumulate HSIs across multiple input images, and optimize the 3D scene to\nreconstruct a consistent, physically plausible and functional 3D scene layout.\nOur optimization-based approach exploits three types of HSI constraints: (1)\nhumans that move in a scene are occluded or occlude objects, thus, defining the\ndepth ordering of the objects, (2) humans move through free space and do not\ninterpenetrate objects, (3) when humans and objects are in contact, the contact\nsurfaces occupy the same place in space. Using these constraints in an\noptimization formulation across all observations, we significantly improve the\n3D scene layout reconstruction. Furthermore, we show that our scene\nreconstruction can be used to refine the initial 3D human pose and shape (HPS)\nestimation. We evaluate the 3D scene layout reconstruction and HPS estimation\nqualitatively and quantitatively using the PROX and PiGraphs datasets. The code\nand data are available for research purposes at https://mover.is.tue.mpg.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1\">Hongwei Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chun-Hao P. Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1\">Dimitrios Tzionas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Mohamed Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Rectangling for Image Stitching: A Learning Baseline. (arXiv:2203.03831v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03831","description":"<p>Stitched images provide a wide field-of-view (FoV) but suffer from unpleasant\nirregular boundaries. To deal with this problem, existing image rectangling\nmethods devote to searching an initial mesh and optimizing a target mesh to\nform the mesh deformation in two stages. Then rectangular images can be\ngenerated by warping stitched images. However, these solutions only work for\nimages with rich linear structures, leading to noticeable distortions for\nportraits and landscapes with non-linear objects. In this paper, we address\nthese issues by proposing the first deep learning solution to image\nrectangling. Concretely, we predefine a rigid target mesh and only estimate an\ninitial mesh to form the mesh deformation, contributing to a compact one-stage\nsolution. The initial mesh is predicted using a fully convolutional network\nwith a residual progressive regression strategy. To obtain results with high\ncontent fidelity, a comprehensive objective function is proposed to\nsimultaneously encourage the boundary rectangular, mesh shape-preserving, and\ncontent perceptually natural. Besides, we build the first image stitching\nrectangling dataset with a large diversity in irregular boundaries and scenes.\nExperiments demonstrate our superiority over traditional methods both\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuaicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChiTransformer:Towards Reliable Stereo from Cues. (arXiv:2203.04554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04554","description":"<p>Current stereo matching techniques are challenged by restricted searching\nspace, occluded regions and sheer size. While single image depth estimation is\nspared from these challenges and can achieve satisfactory results with the\nextracted monocular cues, the lack of stereoscopic relationship renders the\nmonocular prediction less reliable on its own, especially in highly dynamic or\ncluttered environments. To address these issues in both scenarios, we present\nan optic-chiasm-inspired self-supervised binocular depth estimation method,\nwherein a vision transformer (ViT) with gated positional cross-attention (GPCA)\nlayers is designed to enable feature-sensitive pattern retrieval between views\nwhile retaining the extensive context information aggregated through\nself-attentions. Monocular cues from a single view are thereafter conditionally\nrectified by a blending layer with the retrieved pattern pairs. This crossover\ndesign is biologically analogous to the optic-chasma structure in the human\nvisual system and hence the name, ChiTransformer. Our experiments show that\nthis architecture yields substantial improvements over state-of-the-art\nself-supervised stereo approaches by 11%, and can be used on both rectilinear\nand non-rectilinear (e.g., fisheye) images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1\">Qing Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Transitive Information Theory and its Application to Deep Generative Models. (arXiv:2203.05074v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2203.05074","description":"<p>Paradoxically, a Variational Autoencoder (VAE) could be pushed in two\nopposite directions, utilizing powerful decoder model for generating realistic\nimages but collapsing the learned representation, or increasing regularization\ncoefficient for disentangling representation but ultimately generating blurry\nexamples. Existing methods narrow the issues to the rate-distortion trade-off\nbetween compression and reconstruction. We argue that a good reconstruction\nmodel does learn high capacity latents that encode more details, however, its\nuse is hindered by two major issues: the prior is random noise which is\ncompletely detached from the posterior and allow no controllability in the\ngeneration; mean-field variational inference doesn't enforce hierarchy\nstructure which makes the task of recombining those units into plausible novel\noutput infeasible. As a result, we develop a system that learns a hierarchy of\ndisentangled representation together with a mechanism for recombining the\nlearned representation for generalization. This is achieved by introducing a\nminimal amount of inductive bias to learn controllable prior for the VAE. The\nidea is supported by here developed transitive information theory, that is, the\nmutual information between two target variables could alternately be maximized\nthrough the mutual information to the third variable, thus bypassing the\nrate-distortion bottleneck in VAE design. In particular, we show that our\nmodel, named SemafoVAE (inspired by the similar concept in computer science),\ncould generate high-quality examples in a controllable manner, perform smooth\ntraversals of the disentangled factors and intervention at a different level of\nrepresentation hierarchy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Trung Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laabid_N/0/1/0/all/0/1\">Najwa Laabid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinaniemi_M/0/1/0/all/0/1\">Merja Hein&#xe4;niemi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Visual-Prompt Temporal Answering Grounding in Medical Instructional Video. (arXiv:2203.06667v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.06667","description":"<p>The temporal answering grounding in the video (TAGV) is a new task naturally\nderived from temporal sentence grounding in the video (TSGV). Given an\nuntrimmed video and a text question, this task aims at locating the matching\nspan from the video that can semantically answer the question. Existing methods\ntend to formulate the TAGV task with a visual span-based question answering\n(QA) approach by matching the visual frame span queried by the text question.\nHowever, due to the weak correlations and huge gaps of the semantic features\nbetween the textual question and visual answer, existing methods adopting\nvisual span predictor perform poorly in the TAGV task. To bridge these gaps, we\npropose a visual-prompt text span localizing (VPTSL) method, which introduces\nthe timestamped subtitles as a passage to perform the text span localization\nfor the input text question, and prompts the visual highlight features into the\npre-trained language model (PLM) for enhancing the joint semantic\nrepresentations. Specifically, the context query attention is utilized to\nperform cross-modal interaction between the extracted textual and visual\nfeatures. Then, the highlight features are obtained through the video-text\nhighlighting for the visual prompt. To alleviate semantic differences between\ntextual and visual features, we design the text span predictor by encoding the\nquestion, the subtitles, and the prompted visual highlight features with the\nPLM. As a result, the TAGV task is formulated to predict the span of subtitles\nmatching the visual answer. Extensive experiments on the medical instructional\ndataset, namely MedVidQA, show that the proposed VPTSL outperforms the\nstate-of-the-art (SOTA) method by 28.36% in terms of mIOU with a large margin,\nwhich demonstrates the effectiveness of the proposed visual prompt and the text\nspan predictor.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1\">Yixuan Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Bin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shutao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning What Not to Segment: A New Perspective on Few-Shot Segmentation. (arXiv:2203.07615v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07615","description":"<p>Recently few-shot segmentation (FSS) has been extensively developed. Most\nprevious works strive to achieve generalization through the meta-learning\nframework derived from classification tasks; however, the trained models are\nbiased towards the seen classes instead of being ideally class-agnostic, thus\nhindering the recognition of new concepts. This paper proposes a fresh and\nstraightforward insight to alleviate the problem. Specifically, we apply an\nadditional branch (base learner) to the conventional FSS model (meta learner)\nto explicitly identify the targets of base classes, i.e., the regions that do\nnot need to be segmented. Then, the coarse results output by these two learners\nin parallel are adaptively integrated to yield precise segmentation prediction.\nConsidering the sensitivity of meta learner, we further introduce an adjustment\nfactor to estimate the scene differences between the input image pairs for\nfacilitating the model ensemble forecasting. The substantial performance gains\non PASCAL-5i and COCO-20i verify the effectiveness, and surprisingly, our\nversatile scheme sets a new state-of-the-art even with two plain learners.\nMoreover, in light of the unique nature of the proposed approach, we also\nextend it to a more realistic but challenging setting, i.e., generalized FSS,\nwhere the pixels of both base and novel classes are required to be determined.\nThe source code is available at github.com/chunbolang/BAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Chunbo Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_B/0/1/0/all/0/1\">Binfei Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junwei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Feature Decoupling with Depthwise Quantization. (arXiv:2203.08080v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08080","description":"<p>Quantization has been applied to multiple domains in Deep Neural Networks\n(DNNs). We propose Depthwise Quantization (DQ) where $\\textit{quantization}$ is\napplied to a decomposed sub-tensor along the $\\textit{feature axis}$ of weak\nstatistical dependence. The feature decomposition leads to an exponential\nincrease in $\\textit{representation capacity}$ with a linear increase in memory\nand parameter cost. In addition, DQ can be directly applied to existing\nencoder-decoder frameworks without modification of the DNN architecture. We use\nDQ in the context of Hierarchical Auto-Encoder and train end-to-end on an image\nfeature representation. We provide an analysis on cross-correlation between\nspatial and channel features and we propose a decomposition of the image\nfeature representation along the channel axis. The improved performance of the\ndepthwise operator is due to the increased representation capacity from\nimplicit feature decoupling. We evaluate DQ on the likelihood estimation task,\nwhere it outperforms the previous state-of-the-art on CIFAR-10, ImageNet-32 and\nImageNet-64. We progressively train with increasing image size a single\nhierarchical model that uses 69% less parameters and has a faster convergence\nthan the previous works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fostiropoulos_I/0/1/0/all/0/1\">Iordanis Fostiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boehm_B/0/1/0/all/0/1\">Barry Boehm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SocialVAE: Human Trajectory Prediction using Timewise Latents. (arXiv:2203.08207v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08207","description":"<p>Predicting pedestrian movement is critical for human behavior analysis and\nalso for safe and efficient human-agent interactions. However, despite\nsignificant advancements, it is still challenging for existing approaches to\ncapture the uncertainty and multimodality of human navigation decision making.\nIn this paper, we propose SocialVAE, a novel approach for human trajectory\nprediction. The core of SocialVAE is a timewise variational autoencoder\narchitecture that exploits stochastic recurrent neural networks to perform\nprediction, combined with a social attention mechanism and backward posterior\napproximation to allow for better extraction of pedestrian navigation\nstrategies. We show that SocialVAE improves current state-of-the-art\nperformance on several pedestrian trajectory prediction benchmarks, including\nthe ETH/UCY benchmark, the Stanford Drone Dataset and SportVU NBA movement\ndataset. Code is available at: https://github.com/xupei0610/SocialVAE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayet_J/0/1/0/all/0/1\">Jean-Bernard Hayet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karamouzas_I/0/1/0/all/0/1\">Ioannis Karamouzas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Flow: Cross-layer Graph Flow Distillation for Dual Efficient Medical Image Segmentation. (arXiv:2203.08667v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08667","description":"<p>With the development of deep convolutional neural networks, medical image\nsegmentation has achieved a series of breakthroughs in recent years. However,\nthe higher-performance convolutional neural networks always mean numerous\nparameters and high computation costs, which will hinder the applications in\nclinical scenarios. Meanwhile, the scarceness of large-scale annotated medical\nimage datasets further impedes the application of high-performance networks. To\ntackle these problems, we propose Graph Flow, a comprehensive knowledge\ndistillation framework, for both network-efficiency and annotation-efficiency\nmedical image segmentation. Specifically, our core Graph Flow Distillation\ntransfer the essence of cross-layer variations from a well-trained cumbersome\nteacher network to a non-trained compact student network. In addition, an\nunsupervised Paraphraser Module is designed to purify the knowledge of the\nteacher network, which is also beneficial for the stabilization of training\nprocedure. Furthermore, we build a unified distillation framework by\nintegrating the adversarial distillation and the vanilla logits distillation,\nwhich can further refine the final predictions of the compact network.\nExtensive experiments conducted on Gastric Cancer Segmentation Dataset and\nSynapse Multi-organ Segmentation Dataset demonstrate the prominent ability of\nour method which achieves state-of-the-art performance on these\ndifferent-modality and multi-category medical image datasets. Moreover, we\ndemonstrate the effectiveness of our Graph Flow through a new semi-supervised\nparadigm for dual efficient medical image segmentation. Our code will be\navailable at Graph Flow.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_W/0/1/0/all/0/1\">Wenxuan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionAug: Augmentation with Physical Correction for Human Motion Prediction. (arXiv:2203.09116v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09116","description":"<p>This paper presents a motion data augmentation scheme incorporating motion\nsynthesis encouraging diversity and motion correction imposing physical\nplausibility. This motion synthesis consists of our modified Variational\nAutoEncoder (VAE) and Inverse Kinematics (IK). In this VAE, our proposed\nsampling-near-samples method generates various valid motions even with\ninsufficient training motion data. Our IK-based motion synthesis method allows\nus to generate a variety of motions semi-automatically. Since these two schemes\ngenerate unrealistic artifacts in the synthesized motions, our motion\ncorrection rectifies them. This motion correction scheme consists of imitation\nlearning with physics simulation and subsequent motion debiasing. For this\nimitation learning, we propose the PD-residual force that significantly\naccelerates the training process. Furthermore, our motion debiasing\nsuccessfully offsets the motion bias induced by imitation learning to maximize\nthe effect of augmentation. As a result, our method outperforms previous\nnoise-based motion augmentation methods by a large margin on both Recurrent\nNeural Network-based and Graph Convolutional Network-based human motion\nprediction models. The code is available at\nhttps://github.com/meaten/MotionAug.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maeda_T/0/1/0/all/0/1\">Takahiro Maeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ukita_N/0/1/0/all/0/1\">Norimichi Ukita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localizing Visual Sounds the Easy Way. (arXiv:2203.09324v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09324","description":"<p>Unsupervised audio-visual source localization aims at localizing visible\nsound sources in a video without relying on ground-truth localization for\ntraining. Previous works often seek high audio-visual similarities for likely\npositive (sounding) regions and low similarities for likely negative regions.\nHowever, accurately distinguishing between sounding and non-sounding regions is\nchallenging without manual annotations. In this work, we propose a simple yet\neffective approach for Easy Visual Sound Localization, namely EZ-VSL, without\nrelying on the construction of positive and/or negative regions during\ntraining. Instead, we align audio and visual spaces by seeking audio-visual\nrepresentations that are aligned in, at least, one location of the associated\nimage, while not matching other images, at any location. We also introduce a\nnovel object guided localization scheme at inference time for improved\nprecision. Our simple and effective framework achieves state-of-the-art\nperformance on two popular benchmarks, Flickr SoundNet and VGG-Sound Source. In\nparticular, we improve the CIoU of the Flickr SoundNet test set from 76.80% to\n83.94%, and on the VGG-Sound Source dataset from 34.60% to 38.85%. The code is\navailable at https://github.com/stoneMo/EZ-VSL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1\">Shentong Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgado_P/0/1/0/all/0/1\">Pedro Morgado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELIC: Efficient Learned Image Compression with Unevenly Grouped Space-Channel Contextual Adaptive Coding. (arXiv:2203.10886v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10886","description":"<p>Recently, learned image compression techniques have achieved remarkable\nperformance, even surpassing the best manually designed lossy image coders.\nThey are promising to be large-scale adopted. For the sake of practicality, a\nthorough investigation of the architecture design of learned image compression,\nregarding both compression performance and running speed, is essential. In this\npaper, we first propose uneven channel-conditional adaptive coding, motivated\nby the observation of energy compaction in learned image compression. Combining\nthe proposed uneven grouping model with existing context models, we obtain a\nspatial-channel contextual adaptive model to improve the coding performance\nwithout damage to running speed. Then we study the structure of the main\ntransform and propose an efficient model, ELIC, to achieve state-of-the-art\nspeed and compression ability. With superior performance, the proposed model\nalso supports extremely fast preview decoding and progressive decoding, which\nmakes the coming application of learning-based image compression more\npromising.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dailan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1\">Weikun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Rui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1\">Hongwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MixFormer: End-to-End Tracking with Iterative Mixed Attention. (arXiv:2203.11082v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11082","description":"<p>Tracking often uses a multi-stage pipeline of feature extraction, target\ninformation integration, and bounding box estimation. To simplify this pipeline\nand unify the process of feature extraction and target information integration,\nwe present a compact tracking framework, termed as MixFormer, built upon\ntransformers. Our core design is to utilize the flexibility of attention\noperations, and propose a Mixed Attention Module (MAM) for simultaneous feature\nextraction and target information integration. This synchronous modeling scheme\nallows to extract target-specific discriminative features and perform extensive\ncommunication between target and search area. Based on MAM, we build our\nMixFormer tracking framework simply by stacking multiple MAMs with progressive\npatch embedding and placing a localization head on top. In addition, to handle\nmultiple target templates during online tracking, we devise an asymmetric\nattention scheme in MAM to reduce computational cost, and propose an effective\nscore prediction module to select high-quality templates. Our MixFormer sets a\nnew state-of-the-art performance on five tracking benchmarks, including LaSOT,\nTrackingNet, VOT2020, GOT-10k, and UAV123. In particular, our MixFormer-L\nachieves NP score of 79.9% on LaSOT, 88.9% on TrackingNet and EAO of 0.555 on\nVOT2020. We also perform in-depth ablation studies to demonstrate the\neffectiveness of simultaneous feature extraction and information integration.\nCode and trained models are publicly available at\nhttps://github.com/MCG-NJU/MixFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yutao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WuDaoMM: A large-scale Multi-Modal Dataset for Pre-training models. (arXiv:2203.11480v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11480","description":"<p>Compared with the domain-specific model, the vision-language pre-training\nmodels (VLPMs) have shown superior performance on downstream tasks with fast\nfine-tuning process. For example, ERNIE-ViL, Oscar and UNIMO trained VLPMs with\na uniform transformers stack architecture and large amounts of image-text\npaired data, achieving remarkable results on downstream tasks such as\nimage-text reference(IR and TR), vision question answering (VQA) and image\ncaptioning (IC) etc. During the training phase, VLPMs are always fed with a\ncombination of multiple public datasets to meet the demand of large-scare\ntraining data. However, due to the unevenness of data distribution including\nsize, task type and quality, using the mixture of multiple datasets for model\ntraining can be problematic. In this work, we introduce a large-scale\nmulti-modal corpora named WuDaoMM, totally containing more than 650M image-text\npairs. Specifically, about 600 million pairs of data are collected from\nmultiple webpages in which image and caption present weak correlation, and the\nother 50 million strong-related image-text pairs are collected from some\nhigh-quality graphic websites. We also release a base version of WuDaoMM with 5\nmillion strong-correlated image-text pairs, which is sufficient to support the\ncommon cross-modal model pre-training. Besides, we trained both an\nunderstanding and a generation vision-language (VL) model to test the dataset\neffectiveness. The results show that WuDaoMM can be applied as an efficient\ndataset for VLPMs, especially for the model in text-to-image generation task.\nThe data is released at https://data.wudaoai.cn\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1\">Sha Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_J/0/1/0/all/0/1\">Jiahong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1\">Zhao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peiyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1\">Zheng Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wayne Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Differential Privacy in Computer Vision. (arXiv:2203.11481v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.11481","description":"<p>We introduce AdaMix, an adaptive differentially private algorithm for\ntraining deep neural network classifiers using both private and public image\ndata. While pre-training language models on large public datasets has enabled\nstrong differential privacy (DP) guarantees with minor loss of accuracy, a\nsimilar practice yields punishing trade-offs in vision tasks. A few-shot or\neven zero-shot learning baseline that ignores private data can outperform\nfine-tuning on a large private dataset. AdaMix incorporates few-shot training,\nor cross-modal zero-shot learning, on public data prior to private fine-tuning,\nto improve the trade-off. AdaMix reduces the error increase from the\nnon-private upper bound from the 167-311\\% of the baseline, on average across 6\ndatasets, to 68-92\\% depending on the desired privacy level selected by the\nuser. AdaMix tackles the trade-off arising in visual classification, whereby\nthe most privacy sensitive data, corresponding to isolated points in\nrepresentation space, are also critical for high classification accuracy. In\naddition, AdaMix comes with strong theoretical privacy guarantees and\nconvergence analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1\">Alessandro Achille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kearns_M/0/1/0/all/0/1\">Michael Kearns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PlaneMVS: 3D Plane Reconstruction from Multi-View Stereo. (arXiv:2203.12082v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12082","description":"<p>We present a novel framework named PlaneMVS for 3D plane reconstruction from\nmultiple input views with known camera poses. Most previous learning-based\nplane reconstruction methods reconstruct 3D planes from single images, which\nhighly rely on single-view regression and suffer from depth scale ambiguity. In\ncontrast, we reconstruct 3D planes with a multi-view-stereo (MVS) pipeline that\ntakes advantage of multi-view geometry. We decouple plane reconstruction into a\nsemantic plane detection branch and a plane MVS branch. The semantic plane\ndetection branch is based on a single-view plane detection framework but with\ndifferences. The plane MVS branch adopts a set of slanted plane hypotheses to\nreplace conventional depth hypotheses to perform plane sweeping strategy and\nfinally learns pixel-level plane parameters and its planar depth map. We\npresent how the two branches are learned in a balanced way, and propose a\nsoft-pooling loss to associate the outputs of the two branches and make them\nbenefit from each other. Extensive experiments on various indoor datasets show\nthat PlaneMVS significantly outperforms state-of-the-art (SOTA) single-view\nplane reconstruction methods on both plane detection and 3D geometry metrics.\nOur method even outperforms a set of SOTA learning-based MVS methods thanks to\nthe learned plane priors. To the best of our knowledge, this is the first work\non 3D plane reconstruction within an end-to-end MVS framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_N/0/1/0/all/0/1\">Nitin Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Changjiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qingan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-time Object Detection for Streaming Perception. (arXiv:2203.12338v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12338","description":"<p>Autonomous driving requires the model to perceive the environment and (re)act\nwithin a low latency for safety. While past works ignore the inevitable changes\nin the environment after processing, streaming perception is proposed to\njointly evaluate the latency and accuracy into a single metric for video online\nperception. In this paper, instead of searching trade-offs between accuracy and\nspeed like previous works, we point out that endowing real-time models with the\nability to predict the future is the key to dealing with this problem. We build\na simple and effective framework for streaming perception. It equips a novel\nDualFlow Perception module (DFP), which includes dynamic and static flows to\ncapture the moving trend and basic detection feature for streaming prediction.\nFurther, we introduce a Trend-Aware Loss (TAL) combined with a trend factor to\ngenerate adaptive weights for objects with different moving speeds. Our simple\nmethod achieves competitive performance on Argoverse-HD dataset and improves\nthe AP by 4.9% compared to the strong baseline, validating its effectiveness.\nOur code will be made available at https://github.com/yancie-yjr/StreamYOLO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinrong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoping Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CroMo: Cross-Modal Learning for Monocular Depth Estimation. (arXiv:2203.12485v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12485","description":"<p>Learning-based depth estimation has witnessed recent progress in multiple\ndirections; from self-supervision using monocular video to supervised methods\noffering highest accuracy. Complementary to supervision, further boosts to\nperformance and robustness are gained by combining information from multiple\nsignals. In this paper we systematically investigate key trade-offs associated\nwith sensor and modality design choices as well as related model training\nstrategies. Our study leads us to a new method, capable of connecting\nmodality-specific advantages from polarisation, Time-of-Flight and\nstructured-light inputs. We propose a novel pipeline capable of estimating\ndepth from monocular polarisation for which we evaluate various training\nsignals. The inversion of differentiable analytic models thereby connects scene\ngeometry with polarisation and ToF signals and enables self-supervised and\ncross-modal learning. In the absence of existing multimodal datasets, we\nexamine our approach with a custom-made multi-modal camera rig and collect\nCroMo; the first dataset to consist of synchronized stereo polarisation,\nindirect ToF and structured-light depth, captured at video rates. Extensive\nexperiments on challenging video scenes confirm both qualitative and\nquantitative pipeline advantages where we are able to outperform competitive\nmonocular depth estimation method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verdie_Y/0/1/0/all/0/1\">Yannick Verdi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jifei Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mas_B/0/1/0/all/0/1\">Barnab&#xe9; Mas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1\">Benjamin Busam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1\">Ale&#x161; Leonardis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions. (arXiv:2203.12667v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12667","description":"<p>A long-term goal of AI research is to build intelligent agents that can\ncommunicate with humans in natural language, perceive the environment, and\nperform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental\nand interdisciplinary research topic towards this goal, and receives increasing\nattention from natural language processing, computer vision, robotics, and\nmachine learning communities. In this paper, we review contemporary studies in\nthe emerging field of VLN, covering tasks, evaluation metrics, methods, etc.\nThrough structured analysis of current progress and challenges, we highlight\nthe limitations of current VLN and opportunities for future work. This paper\nserves as a thorough reference for the VLN research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jing Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefani_E/0/1/0/all/0/1\">Eliana Stefani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1\">Jesse Thomason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to generate line drawings that convey geometry and semantics. (arXiv:2203.12691v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12691","description":"<p>This paper presents an unpaired method for creating line drawings from\nphotographs. Current methods often rely on high quality paired datasets to\ngenerate line drawings. However, these datasets often have limitations due to\nthe subjects of the drawings belonging to a specific domain, or in the amount\nof data collected. Although recent work in unsupervised image-to-image\ntranslation has shown much progress, the latest methods still struggle to\ngenerate compelling line drawings. We observe that line drawings are encodings\nof scene information and seek to convey 3D shape and semantic meaning. We build\nthese observations into a set of objectives and train an image translation to\nmap photographs into line drawings. We introduce a geometry loss which predicts\ndepth information from the image features of a line drawing, and a semantic\nloss which matches the CLIP features of a line drawing with its corresponding\nphotograph. Our approach outperforms state-of-the-art unpaired image\ntranslation and line drawing generation methods on creating line drawings from\narbitrary photographs. For code and demo visit our webpage\ncarolineec.github.io/informative_drawings\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Caroline Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1\">Fredo Durand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximum Spatial Perturbation Consistency for Unpaired Image-to-Image Translation. (arXiv:2203.12707v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12707","description":"<p>Unpaired image-to-image translation (I2I) is an ill-posed problem, as an\ninfinite number of translation functions can map the source domain distribution\nto the target distribution. Therefore, much effort has been put into designing\nsuitable constraints, e.g., cycle consistency (CycleGAN), geometry consistency\n(GCGAN), and contrastive learning-based constraints (CUTGAN), that help better\npose the problem. However, these well-known constraints have limitations: (1)\nthey are either too restrictive or too weak for specific I2I tasks; (2) these\nmethods result in content distortion when there is a significant spatial\nvariation between the source and target domains. This paper proposes a\nuniversal regularization technique called maximum spatial perturbation\nconsistency (MSPC), which enforces a spatial perturbation function (T ) and the\ntranslation operator (G) to be commutative (i.e., TG = GT ). In addition, we\nintroduce two adversarial training components for learning the spatial\nperturbation function. The first one lets T compete with G to achieve maximum\nperturbation. The second one lets G and T compete with discriminators to align\nthe spatial variations caused by the change of object size, object distortion,\nbackground interruptions, etc. Our method outperforms the state-of-the-art\nmethods on most I2I benchmarks. We also introduce a new benchmark, namely the\nfront face to profile face dataset, to emphasize the underlying challenges of\nI2I for real-world applications. We finally perform ablation experiments to\nstudy the sensitivity of our method to the severity of spatial perturbation and\nits effectiveness for distribution alignment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yanwu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shaoan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Mingming Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1\">Kayhan Batmanghelich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Random Forest Regression for continuous affect using Facial Action Units. (arXiv:2203.12818v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12818","description":"<p>In this paper we describe our approach to the arousal and valence track of\nthe 3rd Workshop and Competition on Affective Behavior Analysis in-the-wild\n(ABAW). We extracted facial features using OpenFace and used them to train a\nmultiple output random forest regressor. Our approach performed comparable to\nthe baseline approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hinduja_S/0/1/0/all/0/1\">Saurabh Hinduja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canavan_S/0/1/0/all/0/1\">Shaun Canavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jivnani_L/0/1/0/all/0/1\">Liza Jivnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannat_S/0/1/0/all/0/1\">Sk Rahatul Jannat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">V Sri Chakra Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Emotion Descriptors Estimation at the ABAW3 Challenge. (arXiv:2203.12845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12845","description":"<p>To describe complex emotional states, psychologists have proposed multiple\nemotion descriptors: sparse descriptors like facial action units; continuous\ndescriptors like valence and arousal; and discrete class descriptors like\nhappiness and anger. According to Ekman and Friesen, 1969, facial action units\nare sign vehicles that convey the emotion message, while discrete or continuous\nemotion descriptors are the messages perceived and expressed by human.\n</p>\n<p>In this paper, we designed an architecture for multiple emotion descriptors\nestimation in participating the ABAW3 Challenge. Based on the theory of Ekman\nand Friesen, 1969, we designed distinct architectures to measure the sign\nvehicles (i.e., facial action units) and the message (i.e., discrete emotions,\nvalence and arousal) given their different properties. The quantitative\nexperiments on the ABAW3 challenge dataset has shown the superior performance\nof our approach over two baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1\">Didan Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intrinsic Bias Identification on Medical Image Datasets. (arXiv:2203.12872v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12872","description":"<p>Machine learning based medical image analysis highly depends on datasets.\nBiases in the dataset can be learned by the model and degrade the\ngeneralizability of the applications. There are studies on debiased models.\nHowever, scientists and practitioners are difficult to identify implicit biases\nin the datasets, which causes lack of reliable unbias test datasets to valid\nmodels. To tackle this issue, we first define the data intrinsic bias\nattribute, and then propose a novel bias identification framework for medical\nimage datasets. The framework contains two major components, KlotskiNet and\nBias Discriminant Direction Analysis(bdda), where KlostkiNet is to build the\nmapping which makes backgrounds to distinguish positive and negative samples\nand bdda provides a theoretical solution on determining bias attributes.\nExperimental results on three datasets show the effectiveness of the bias\nattributes discovered by the framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lian Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">An-an Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Senhua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_D/0/1/0/all/0/1\">Dandan Tu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. (arXiv:2203.13009v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13009","description":"<p>Recently, significant progress has been made on image denoising with strong\nsupervision from large-scale datasets. However, obtaining well-aligned\nnoisy-clean training image pairs for each specific scenario is complicated and\ncostly in practice. Consequently, applying a conventional supervised denoising\nnetwork on in-the-wild noisy inputs is not straightforward. Although several\nstudies have challenged this problem without strong supervision, they rely on\nless practical assumptions and cannot be applied to practical situations\ndirectly. To address the aforementioned challenges, we propose a novel and\npowerful self-supervised denoising method called CVF-SID based on a Cyclic\nmulti-Variate Function (CVF) module and a self-supervised image disentangling\n(SID) framework. The CVF module can output multiple decomposed variables of the\ninput and take a combination of the outputs back as an input in a cyclic\nmanner. Our CVF-SID can disentangle a clean image and noise maps from the input\nby leveraging various self-supervised loss terms. Unlike several methods that\nonly consider the signal-independent noise models, we also deal with\nsignal-dependent noise components for real-world applications. Furthermore, we\ndo not rely on any prior assumptions about the underlying noise distribution,\nmaking CVF-SID more generalizable toward realistic noise. Extensive experiments\non real-world datasets show that CVF-SID achieves state-of-the-art\nself-supervised image denoising performance and is comparable to other existing\napproaches. The code is publicly available from\nhttps://github.com/Reyhanehne/CVF-SID_PyTorch .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neshatavar_R/0/1/0/all/0/1\">Reyhaneh Neshatavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavartanoo_M/0/1/0/all/0/1\">Mohsen Yavartanoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1\">Sanghyun Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyoung Mu Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation. (arXiv:2203.13254v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13254","description":"<p>Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is\na long-standing problem in computer vision. Driven by end-to-end deep learning,\nrecent studies suggest interpreting PnP as a differentiable layer, so that\n2D-3D point correspondences can be partly learned by backpropagating the\ngradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D\npoints from scratch fails to converge with existing approaches, since the\ndeterministic pose is inherently non-differentiable. In this paper, we propose\nthe EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation,\nwhich outputs a distribution of pose on the SE(3) manifold, essentially\nbringing categorical Softmax to the continuous domain. The 2D-3D coordinates\nand corresponding weights are treated as intermediate variables learned by\nminimizing the KL divergence between the predicted and target pose\ndistribution. The underlying principle unifies the existing approaches and\nresembles the attention mechanism. EPro-PnP significantly outperforms\ncompetitive baselines, closing the gap between PnP-based method and the\ntask-specific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D\nobject detection benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hansheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_W/0/1/0/all/0/1\">Wei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1\">Lu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Practical Blind Denoising via Swin-Conv-UNet and Data Synthesis. (arXiv:2203.13278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13278","description":"<p>While recent years have witnessed a dramatic upsurge of exploiting deep\nneural networks toward solving image denoising, existing methods mostly rely on\nsimple noise assumptions, such as additive white Gaussian noise (AWGN), JPEG\ncompression noise and camera sensor noise, and a general-purpose blind\ndenoising method for real images remains unsolved. In this paper, we attempt to\nsolve this problem from the perspective of network architecture design and\ntraining data synthesis. Specifically, for the network architecture design, we\npropose a swin-conv block to incorporate the local modeling ability of residual\nconvolutional layer and non-local modeling ability of swin transformer block,\nand then plug it as the main building block into the widely-used image-to-image\ntranslation UNet architecture. For the training data synthesis, we design a\npractical noise degradation model which takes into consideration different\nkinds of noise (including Gaussian, Poisson, speckle, JPEG compression, and\nprocessed camera sensor noises) and resizing, and also involves a random\nshuffle strategy and a double degradation strategy. Extensive experiments on\nAGWN removal and real image denoising demonstrate that the new network\narchitecture design achieves state-of-the-art performance and the new\ndegradation model can help to significantly improve the practicability. We\nbelieve our work can provide useful insights into current denoising research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yulun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous-Time Audiovisual Fusion with Recurrence vs. Attention for In-The-Wild Affect Recognition. (arXiv:2203.13285v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13285","description":"<p>In this paper, we present our submission to 3rd Affective Behavior Analysis\nin-the-wild (ABAW) challenge. Learningcomplex interactions among multimodal\nsequences is critical to recognise dimensional affect from in-the-wild\naudiovisual data. Recurrence and attention are the two widely used sequence\nmodelling mechanisms in the literature. To clearly understand the performance\ndifferences between recurrent and attention models in audiovisual affect\nrecognition, we present a comprehensive evaluation of fusion models based on\nLSTM-RNNs, self-attention and cross-modal attention, trained for valence and\narousal estimation. Particularly, we study the impact of some key design\nchoices: the modelling complexity of CNN backbones that provide features to the\nthe temporal models, with and without end-to-end learning. We trained the\naudiovisual affect recognition models on in-the-wild ABAW corpus by\nsystematically tuning the hyper-parameters involved in the network architecture\ndesign and training optimisation. Our extensive evaluation of the audiovisual\nfusion models shows that LSTM-RNNs can outperform the attention models when\ncoupled with low-complex CNN backbones and trained in an end-to-end fashion,\nimplying that attention models may not necessarily be the optimal choice for\ncontinuous-time multimodal emotion recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karas_V/0/1/0/all/0/1\">Vincent Karas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tellamekala_M/0/1/0/all/0/1\">Mani Kumar Tellamekala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallol_Ragolta_A/0/1/0/all/0/1\">Adria Mallol-Ragolta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valstar_M/0/1/0/all/0/1\">Michel Valstar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FaceVerse: a Fine-grained and Detail-controllable 3D Face Morphable Model from a Hybrid Dataset. (arXiv:2203.14057v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14057","description":"<p>We present FaceVerse, a fine-grained 3D Neural Face Model, which is built\nfrom hybrid East Asian face datasets containing 60K fused RGB-D images and 2K\nhigh-fidelity 3D head scan models. A novel coarse-to-fine structure is proposed\nto take better advantage of our hybrid dataset. In the coarse module, we\ngenerate a base parametric model from large-scale RGB-D images, which is able\nto predict accurate rough 3D face models in different genders, ages, etc. Then\nin the fine module, a conditional StyleGAN architecture trained with\nhigh-fidelity scan models is introduced to enrich elaborate facial geometric\nand texture details. Note that different from previous methods, our base and\ndetailed modules are both changeable, which enables an innovative application\nof adjusting both the basic attributes and the facial details of 3D face\nmodels. Furthermore, we propose a single-image fitting framework based on\ndifferentiable rendering. Rich experiments show that our method outperforms the\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chenguang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Selective Transformer for Semantic Image Segmentation. (arXiv:2203.14124v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14124","description":"<p>Recently, it has attracted more and more attentions to fuse multi-scale\nfeatures for semantic image segmentation. Various works were proposed to employ\nprogressive local or global fusion, but the feature fusions are not rich enough\nfor modeling multi-scale context features. In this work, we focus on fusing\nmulti-scale features from Transformer-based backbones for semantic\nsegmentation, and propose a Feature Selective Transformer (FeSeFormer), which\naggregates features from all scales (or levels) for each query feature.\nSpecifically, we first propose a Scale-level Feature Selection (SFS) module,\nwhich can choose an informative subset from the whole multi-scale feature set\nfor each scale, where those features that are important for the current scale\n(or level) are selected and the redundant are discarded. Furthermore, we\npropose a Full-scale Feature Fusion (FFF) module, which can adaptively fuse\nfeatures of all scales for queries. Based on the proposed SFS and FFF modules,\nwe develop a Feature Selective Transformer (FeSeFormer), and evaluate our\nFeSeFormer on four challenging semantic segmentation benchmarks, including\nPASCAL Context, ADE20K, COCO-Stuff 10K, and Cityscapes, outperforming the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fangjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Sitong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1\">Shengwei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1\">Guodong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audio-Adaptive Activity Recognition Across Video Domains. (arXiv:2203.14240v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14240","description":"<p>This paper strives for activity recognition under domain shift, for example\ncaused by change of scenery or camera viewpoint. The leading approaches reduce\nthe shift in activity appearance by adversarial training and self-supervised\nlearning. Different from these vision-focused works we leverage activity sounds\nfor domain adaptation as they have less variance across domains and can\nreliably indicate which activities are not happening. We propose an\naudio-adaptive encoder and associated learning methods that discriminatively\nadjust the visual feature representation as well as addressing shifts in the\nsemantic distribution. To further eliminate domain-specific features and\ninclude domain-invariant activity sounds for recognition, an audio-infused\nrecognizer is proposed, which effectively models the cross-modal interaction\nacross domains. We also introduce the new task of actor shift, with a\ncorresponding audio-visual dataset, to challenge our method with situations\nwhere the activity appearance changes dramatically. Experiments on this\ndataset, EPIC-Kitchens and CharadesEgo show the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunhua Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1\">Hazel Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SuperMVS: Non-Uniform Cost Volume For High-Resolution Multi-View Stereo. (arXiv:2203.14331v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14331","description":"<p>Different from most state-of-the-art~(SOTA) algorithms that use static and\nuniform sampling methods with a lot of hypothesis planes to get fine depth\nsampling. In this paper, we propose a free-moving hypothesis plane method for\ndynamic and non-uniform sampling in a wide depth range to build the cost\nvolume, which not only greatly reduces the number of planes but also finers\nsampling, for both of reducing computational cost and improving accuracy, named\nNon-Uniform Cost Volume. We present the SuperMVS network to implement\nMulti-View Stereo with Non-Uniform Cost Volume. SuperMVS is a coarse-to-fine\nframework with four cascade stages. It can output higher resolution and\naccurate depth map. Our SuperMVS achieves the SOTA results with low memory, low\nruntime, and fewer planes on the DTU datasets and Tanks \\&amp; Temples dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Locality-Aware Inter-and Intra-Video Reconstruction for Self-Supervised Correspondence Learning. (arXiv:2203.14333v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14333","description":"<p>Our target is to learn visual correspondence from unlabeled videos. We\ndevelop LIIR, a locality-aware inter-and intra-video reconstruction framework\nthat fills in three missing pieces, i.e., instance discrimination, location\nawareness, and spatial compactness, of self-supervised correspondence learning\npuzzle. First, instead of most existing efforts focusing on intra-video\nself-supervision only, we exploit cross video affinities as extra negative\nsamples within a unified, inter-and intra-video reconstruction scheme. This\nenables instance discriminative representation learning by contrasting desired\nintra-video pixel association against negative inter-video correspondence.\nSecond, we merge position information into correspondence matching, and design\na position shifting strategy to remove the side-effect of position encoding\nduring inter-video affinity computation, making our LIIR location-sensitive.\nThird, to make full use of the spatial continuity nature of video data, we\nimpose a compactness-based constraint on correspondence matching, yielding more\nsparse and reliable solutions. The learned representation surpasses\nself-supervised state-of-the-arts on label propagation tasks including objects,\nsemantic parts, and keypoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liulei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Hierarchical Semantic Segmentation. (arXiv:2203.14335v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14335","description":"<p>Humans are able to recognize structured relations in observation, allowing us\nto decompose complex scenes into simpler parts and abstract the visual world in\nmultiple levels. However, such hierarchical reasoning ability of human\nperception remains largely unexplored in current literature of semantic\nsegmentation. Existing work is often aware of flatten labels and predicts\ntarget classes exclusively for each pixel. In this paper, we instead address\nhierarchical semantic segmentation (HSS), which aims at structured, pixel-wise\ndescription of visual observation in terms of a class hierarchy. We devise\nHSSN, a general HSS framework that tackles two critical issues in this task: i)\nhow to efficiently adapt existing hierarchy-agnostic segmentation networks to\nthe HSS setting, and ii) how to leverage the hierarchy information to\nregularize HSS network learning. To address i), HSSN directly casts HSS as a\npixel-wise multi-label classification task, only bringing minimal architecture\nchange to current segmentation models. To solve ii), HSSN first explores\ninherent properties of the hierarchy as a training objective, which enforces\nsegmentation predictions to obey the hierarchy structure. Further, with\nhierarchy-induced margin constraints, HSSN reshapes the pixel embedding space,\nso as to generate well-structured pixel representations and improve\nsegmentation eventually. We conduct experiments on four semantic segmentation\ndatasets (i.e., Mapillary Vistas 2.0, Cityscapes, LIP, and PASCAL-Person-Part),\nwith different class hierarchies, segmentation network architectures and\nbackbones, showing the generalization and superiority of HSSN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liulei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MFSNet: A Multi Focus Segmentation Network for Skin Lesion Segmentation. (arXiv:2203.14341v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.14341","description":"<p>Segmentation is essential for medical image analysis to identify and localize\ndiseases, monitor morphological changes, and extract discriminative features\nfor further diagnosis. Skin cancer is one of the most common types of cancer\nglobally, and its early diagnosis is pivotal for the complete elimination of\nmalignant tumors from the body. This research develops an Artificial\nIntelligence (AI) framework for supervised skin lesion segmentation employing\nthe deep learning approach. The proposed framework, called MFSNet (Multi-Focus\nSegmentation Network), uses differently scaled feature maps for computing the\nfinal segmentation mask using raw input RGB images of skin lesions. In doing\nso, initially, the images are preprocessed to remove unwanted artifacts and\nnoises. The MFSNet employs the Res2Net backbone, a recently proposed\nconvolutional neural network (CNN), for obtaining deep features used in a\nParallel Partial Decoder (PPD) module to get a global map of the segmentation\nmask. In different stages of the network, convolution features and multi-scale\nmaps are used in two boundary attention (BA) modules and two reverse attention\n(RA) modules to generate the final segmentation output. MFSNet, when evaluated\non three publicly available datasets: $PH^2$, ISIC 2017, and HAM10000,\noutperforms state-of-the-art methods, justifying the reliability of the\nframework. The relevant codes for the proposed approach are accessible at\nhttps://github.com/Rohit-Kundu/MFSNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Basak_H/0/1/0/all/0/1\">Hritam Basak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kundu_R/0/1/0/all/0/1\">Rohit Kundu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkar_R/0/1/0/all/0/1\">Ram Sarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thin-Plate Spline Motion Model for Image Animation. (arXiv:2203.14367v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14367","description":"<p>Image animation brings life to the static object in the source image\naccording to the driving video. Recent works attempt to perform motion transfer\non arbitrary objects through unsupervised methods without using a priori\nknowledge. However, it remains a significant challenge for current unsupervised\nmethods when there is a large pose gap between the objects in the source and\ndriving images. In this paper, a new end-to-end unsupervised motion transfer\nframework is proposed to overcome such issue. Firstly, we propose thin-plate\nspline motion estimation to produce a more flexible optical flow, which warps\nthe feature maps of the source image to the feature domain of the driving\nimage. Secondly, in order to restore the missing regions more realistically, we\nleverage multi-resolution occlusion masks to achieve more effective feature\nfusion. Finally, additional auxiliary loss functions are designed to ensure\nthat there is a clear division of labor in the network modules, encouraging the\nnetwork to generate high-quality images. Our method can animate a variety of\nobjects, including talking faces, human bodies, and pixel animations.\nExperiments demonstrate that our method performs better on most benchmarks than\nthe state of the art with visible improvements in pose-related metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ARCS: Accurate Rotation and Correspondence Search. (arXiv:2203.14493v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.14493","description":"<p>This paper is about the old Wahba problem in its more general form, which we\ncall \"simultaneous rotation and correspondence search\". In this generalization\nwe need to find a rotation that best aligns two partially overlapping $3$D\npoint sets, of sizes $m$ and $n$ respectively with $m\\geq n$. We first propose\na solver, $\\texttt{ARCS}$, that i) assumes noiseless point sets in general\nposition, ii) requires only $2$ inliers, iii) uses $O(m\\log m)$ time and $O(m)$\nspace, and iv) can successfully solve the problem even with, e.g., $m,n\\approx\n10^6$ in about $0.1$ seconds. We next robustify $\\texttt{ARCS}$ to noise, for\nwhich we approximately solve consensus maximization problems using ideas from\nrobust subspace learning and interval stabbing. Thirdly, we refine the\napproximately found consensus set by a Riemannian subgradient descent approach\nover the space of unit quaternions, which we show converges globally to an\n$\\varepsilon$-stationary point in $O(\\varepsilon^{-4})$ iterations, or locally\nto the ground-truth at a linear rate in the absence of noise. We combine these\nalgorithms into $\\texttt{ARCS+}$, to simultaneously search for rotations and\ncorrespondences. Experiments show that $\\texttt{ARCS+}$ achieves\nstate-of-the-art performance on large-scale datasets with more than $10^6$\npoints with a $10^4$ time-speedup over alternative methods.\n\\url{https://github.com/liangzu/ARCS}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liangzu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakiris_M/0/1/0/all/0/1\">Manolis C. Tsakiris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Ren&#xe9; Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affordance Transfer Learning for Human-Object Interaction Detection. (arXiv:2104.02867v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2104.02867","description":"<p>Reasoning the human-object interactions (HOI) is essential for deeper scene\nunderstanding, while object affordances (or functionalities) are of great\nimportance for human to discover unseen HOIs with novel objects. Inspired by\nthis, we introduce an affordance transfer learning approach to jointly detect\nHOIs with novel objects and recognize affordances. Specifically, HOI\nrepresentations can be decoupled into a combination of affordance and object\nrepresentations, making it possible to compose novel interactions by combining\naffordance representations and novel object representations from additional\nimages, i.e. transferring the affordance to novel objects. With the proposed\naffordance transfer learning, the model is also capable of inferring the\naffordances of novel objects from known affordance representations. The\nproposed method can thus be used to 1) improve the performance of HOI\ndetection, especially for the HOIs with unseen objects; and 2) infer the\naffordances of novel objects. Experimental results on two datasets, HICO-DET\nand HOI-COCO (from V-COCO), demonstrate significant improvements over recent\nstate-of-the-art methods for HOI detection and object affordance detection.\nCode is available at https://github.com/zhihou7/HOI-CL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Human-Object Interaction Concepts via Self-Compositional Learning. (arXiv:2203.14272v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2203.14272","description":"<p>A comprehensive understanding of human-object interaction (HOI) requires\ndetecting not only a small portion of predefined HOI concepts (or categories)\nbut also other reasonable HOI concepts, while current approaches usually fail\nto explore a huge portion of unknown HOI concepts (i.e., unknown but reasonable\ncombinations of verbs and objects). In this paper, 1) we introduce a novel and\nchallenging task for a comprehensive HOI understanding, which is termed as HOI\nConcept Discovery; and 2) we devise a self-compositional learning framework (or\nSCL) for HOI concept discovery. Specifically, we maintain an online updated\nconcept confidence matrix during training: 1) we assign pseudo-labels for all\ncomposite HOI instances according to the concept confidence matrix for\nself-training; and 2) we update the concept confidence matrix using the\npredictions of all composite HOI instances. Therefore, the proposed method\nenables the learning on both known and unknown HOI concepts. We perform\nextensive experiments on several popular HOI datasets to demonstrate the\neffectiveness of the proposed method for HOI concept discovery, object\naffordance recognition and HOI detection. For example, the proposed\nself-compositional learning framework significantly improves the performance of\n1) HOI concept discovery by over 10% on HICO-DET and over 3% on V-COCO,\nrespectively; 2) object affordance recognition by over 9% mAP on MS-COCO and\nHICO-DET; and 3) rare-first and non-rare-first unknown HOI detection relatively\nover 30% and 20%, respectively. Code and models will be made publicly available\nat https://github.com/zhihou7/HOI-CL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhi Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/"}}]}]}