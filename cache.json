{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-18T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"FAMIE: A Fast Active Learning Framework for Multilingual Information Extraction. (arXiv:2202.08316v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08316","description":"<p>This paper presents FAMIE, a comprehensive and efficient active learning (AL)\ntoolkit for multilingual information extraction. FAMIE is designed to address a\nfundamental problem in existing AL frameworks where annotators need to wait for\na long time between annotation batches due to the time-consuming nature of\nmodel training and data selection at each AL iteration. This hinders the\nengagement, productivity, and efficiency of annotators. Based on the idea of\nusing a small proxy network for fast data selection, we introduce a novel\nknowledge distillation mechanism to synchronize the proxy network with the main\nlarge model (i.e., BERT-based) to ensure the appropriateness of the selected\nannotation examples for the main model. Our AL framework can support multiple\nlanguages. The experiments demonstrate the advantages of FAMIE in terms of\ncompetitive performance and time efficiency for sequence labeling with AL. We\npublicly release our code (\\url{https://github.com/nlp-uoregon/famie}) and demo\nwebsite (\\url{<a href=\"http://nlp.uoregon.edu:9000/\">this http URL</a>}). A demo video for FAMIE is\nprovided at: \\url{https://youtu.be/I2i8n_jAyrY}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_N/0/1/0/all/0/1\">Nghia Trung Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1\">Bonan Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thien Huu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text-Based Action-Model Acquisition for Planning. (arXiv:2202.08373v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08373","description":"<p>Although there have been approaches that are capable of learning action\nmodels from plan traces, there is no work on learning action models from\ntextual observations, which is pervasive and much easier to collect from\nreal-world applications compared to plan traces. In this paper we propose a\nnovel approach to learning action models from natural language texts by\nintegrating Constraint Satisfaction and Natural Language Processing techniques.\nSpecifically, we first build a novel language model to extract plan traces from\ntexts, and then build a set of constraints to generate action models based on\nthe extracted plan traces. After that, we iteratively improve the language\nmodel and constraints until we achieve the convergent language model and action\nmodels. We empirically exhibit that our approach is both effective and\nefficient.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_K/0/1/0/all/0/1\">Kebing Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaixun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_H/0/1/0/all/0/1\">Hankz Hankui Zhuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Training of Both Translation Models in the Back-Translation Framework. (arXiv:2202.08465v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08465","description":"<p>Semi-supervised learning algorithms in neural machine translation (NMT) have\nsignificantly improved translation quality compared to the supervised learning\nalgorithms by using additional monolingual corpora. Among them,\nback-translation is a theoretically well-structured and cutting-edge method.\nGiven two pre-trained NMT models between source and target languages, one\ntranslates a monolingual sentence as a latent sentence, and the other\nreconstructs the monolingual input sentence given the latent sentence.\nTherefore, previous works tried to apply the variational auto-encoder's (VAE)\ntraining framework to the back-translation framework. However, the discrete\nproperty of the latent sentence made it impossible to use backpropagation in\nthe framework. This paper proposes a categorical reparameterization trick that\ngenerates a differentiable sentence, with which we practically implement the\nVAE's training framework for the back-translation and train it by end-to-end\nbackpropagation. In addition, we propose several regularization techniques that\nare especially advantageous to this framework. In our experiments, we\ndemonstrate that our method makes backpropagation available through the latent\nsentences and improves the BLEU scores on the datasets of the WMT18 translation\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_D/0/1/0/all/0/1\">DongNyeong Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Heeyoul Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting the Evaluation Metrics of Paraphrase Generation. (arXiv:2202.08479v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08479","description":"<p>Paraphrase generation is an important NLP task that has achieved significant\nprogress recently. However, one crucial problem is overlooked, `how to evaluate\nthe quality of paraphrase?'. Most existing paraphrase generation models use\nreference-based metrics (e.g., BLEU) from neural machine translation (NMT) to\nevaluate their generated paraphrase. Such metrics' reliability is hardly\nevaluated, and they are only plausible when there exists a standard reference.\nTherefore, this paper first answers one fundamental question, `Are existing\nmetrics reliable for paraphrase generation?'. We present two conclusions that\ndisobey conventional wisdom in paraphrasing generation: (1) existing metrics\npoorly align with human annotation in system-level and segment-level paraphrase\nevaluation. (2) reference-free metrics outperform reference-based metrics,\nindicating that the standard references are unnecessary to evaluate the\nparaphrase's quality. Such empirical findings expose a lack of reliable\nautomatic evaluation metrics. Therefore, this paper proposes BBScore, a\nreference-free metric that can reflect the generated paraphrase's quality.\nBBScore consists of two sub-metrics: S3C score and SelfBLEU, which correspond\nto two criteria for paraphrase evaluation: semantic preservation and diversity.\nBy connecting two sub-metrics, BBScore significantly outperforms existing\nparaphrase evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lingfeng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AISHELL-NER: Named Entity Recognition from Chinese Speech. (arXiv:2202.08533v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08533","description":"<p>Named Entity Recognition (NER) from speech is among Spoken Language\nUnderstanding (SLU) tasks, aiming to extract semantic information from the\nspeech signal. NER from speech is usually made through a two-step pipeline that\nconsists of (1) processing the audio using an Automatic Speech Recognition\n(ASR) system and (2) applying an NER tagger to the ASR outputs. Recent works\nhave shown the capability of the End-to-End (E2E) approach for NER from English\nand French speech, which is essentially entity-aware ASR. However, due to the\nmany homophones and polyphones that exist in Chinese, NER from Chinese speech\nis effectively a more challenging task. In this paper, we introduce a new\ndataset AISEHLL-NER for NER from Chinese speech. Extensive experiments are\nconducted to explore the performance of several state-of-the-art methods. The\nresults demonstrate that the performance could be improved by combining\nentity-aware ASR and pretrained NER tagger, which can be easily applied to the\nmodern SLU pipeline. The dataset is publicly available at\ngithub.com/Alibaba-NLP/AISHELL-NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boli Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guangwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaobin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengjun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Term Rewriting Based On Set Automaton Matching. (arXiv:2202.08687v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08687","description":"<p>In previous work we have proposed an efficient pattern matching algorithm\nbased on the notion of set automaton. In this article we investigate how set\nautomata can be exploited to implement efficient term rewriting procedures.\nThese procedures interleave pattern matching steps and rewriting steps and thus\nsmoothly integrate redex discovery and subterm replacement. Concretely, we\npropose an optimised algorithm for outermost rewriting of left-linear term\nrewriting systems, prove its correctness, and present the results of some\nimplementation experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouwman_M/0/1/0/all/0/1\">Mark Bouwman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erkens_R/0/1/0/all/0/1\">Rick Erkens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mining On Alzheimer's Diseases Related Knowledge Graph to Identity Potential AD-related Semantic Triples for Drug Repurposing. (arXiv:2202.08712v1 [cs.AI])","link":"http://arxiv.org/abs/2202.08712","description":"<p>To date, there are no effective treatments for most neurodegenerative\ndiseases. Knowledge graphs can provide comprehensive and semantic\nrepresentation for heterogeneous data, and have been successfully leveraged in\nmany biomedical applications including drug repurposing. Our objective is to\nconstruct a knowledge graph from literature to study relations between\nAlzheimer's disease (AD) and chemicals, drugs and dietary supplements in order\nto identify opportunities to prevent or delay neurodegenerative progression. We\ncollected biomedical annotations and extracted their relations using SemRep via\nSemMedDB. We used both a BERT-based classifier and rule-based methods during\ndata preprocessing to exclude noise while preserving most AD-related semantic\ntriples. The 1,672,110 filtered triples were used to train with knowledge graph\ncompletion algorithms (i.e., TransE, DistMult, and ComplEx) to predict\ncandidates that might be helpful for AD treatment or prevention. Among three\nknowledge graph completion models, TransE outperformed the other two (MR =\n13.45, Hits@1 = 0.306). We leveraged the time-slicing technique to further\nevaluate the prediction results. We found supporting evidence for most highly\nranked candidates predicted by our model which indicates that our approach can\ninform reliable new knowledge. This paper shows that our graph mining model can\npredict reliable new relationships between AD and other entities (i.e., dietary\nsupplements, chemicals, and drugs). The knowledge graph constructed can\nfacilitate data-driven knowledge discoveries and the generation of novel\nhypotheses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nian_Y/0/1/0/all/0/1\">Yi Nian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xinyue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingna Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jingcheng Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Cui Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Knowledge-Intensive NLP with Pre-Trained Language Models. (arXiv:2202.08772v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08772","description":"<p>With the increasing of model capacity brought by pre-trained language models,\nthere emerges boosting needs for more knowledgeable natural language processing\n(NLP) models with advanced functionalities including providing and making\nflexible use of encyclopedic and commonsense knowledge. The mere pre-trained\nlanguage models, however, lack the capacity of handling such\nknowledge-intensive NLP tasks alone. To address this challenge, large numbers\nof pre-trained language models augmented with external knowledge sources are\nproposed and in rapid development. In this paper, we aim to summarize the\ncurrent progress of pre-trained language model-based knowledge-enhanced models\n(PLMKEs) by dissecting their three vital elements: knowledge sources,\nknowledge-intensive NLP tasks, and knowledge fusion methods. Finally, we\npresent the challenges of PLMKEs based on the discussion regarding the three\nelements and attempt to provide NLP practitioners with potential directions for\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"cosFormer: Rethinking Softmax in Attention. (arXiv:2202.08791v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08791","description":"<p>Transformer has shown great successes in natural language processing,\ncomputer vision, and audio processing. As one of its core components, the\nsoftmax attention helps to capture long-range dependencies yet prohibits its\nscale-up due to the quadratic space and time complexity to the sequence length.\nKernel methods are often adopted to reduce the complexity by approximating the\nsoftmax operator. Nevertheless, due to the approximation errors, their\nperformances vary in different tasks/corpus and suffer crucial performance\ndrops when compared with the vanilla softmax attention. In this paper, we\npropose a linear transformer called cosFormer that can achieve comparable or\nbetter accuracy to the vanilla transformer in both casual and cross attentions.\ncosFormer is based on two key properties of softmax attention: i).\nnon-negativeness of the attention matrix; ii). a non-linear re-weighting scheme\nthat can concentrate the distribution of the attention matrix. As its linear\nsubstitute, cosFormer fulfills these properties with a linear operator and a\ncosine-based distance re-weighting mechanism. Extensive experiments on language\nmodeling and text understanding tasks demonstrate the effectiveness of our\nmethod. We further examine our method on long sequences and achieve\nstate-of-the-art performance on the Long-Range Arena benchmark. The source code\nis available at https://github.com/OpenNLPLab/cosFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Weixuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1\">Hui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunshen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_B/0/1/0/all/0/1\">Baohong Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junjie Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingpeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08806","description":"<p>We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist\napproach toward learning a compositional and grounded meaning representation of\nlanguage from grounded data, such as paired images and texts. At the core of\nG2L2 is a collection of lexicon entries, which map each word to a tuple of a\nsyntactic type and a neuro-symbolic semantic program. For example, the word\nshiny has a syntactic type of adjective; its neuro-symbolic semantic program\nhas the symbolic form {\\lambda}x. filter(x, SHINY), where the concept SHINY is\nassociated with a neural network embedding, which will be used to classify\nshiny objects. Given an input sentence, G2L2 first looks up the lexicon entries\nassociated with each token. It then derives the meaning of the sentence as an\nexecutable neuro-symbolic program by composing lexical meanings based on\nsyntax. The recovered meaning programs can be executed on grounded inputs. To\nfacilitate learning in an exponentially-growing compositional space, we\nintroduce a joint parsing and expected execution algorithm, which does local\nmarginalization over derivations to reduce the training time. We evaluate G2L2\non two domains: visual reasoning and language-driven navigation. Results show\nthat G2L2 can generalize from small amounts of data to novel compositions of\nwords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiayuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoyue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Creative Inspiration with Fine-Grained Functional Aspects of Ideas. (arXiv:2102.09761v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2102.09761","description":"<p>Large repositories of products, patents and scientific papers offer an\nopportunity for building systems that scour millions of ideas and help users\ndiscover inspirations. However, idea descriptions are typically in the form of\nunstructured text, lacking key structure that is required for supporting\ncreative innovation interactions. Prior work has explored idea representations\nthat were either limited in expressivity, required significant manual effort\nfrom users, or dependent on curated knowledge bases with poor coverage. We\nexplore a novel representation that automatically breaks up products into\nfine-grained functional aspects capturing the purposes and mechanisms of ideas,\nand use it to support important creative innovation interactions: functional\nsearch for ideas, and exploration of the design space around a focal problem by\nviewing related problem perspectives pooled from across many products. In user\nstudies, our approach boosts the quality of creative search and inspirations,\nsubstantially outperforming strong baselines by 50-60%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamari_R/0/1/0/all/0/1\">Ronen Tamari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyeonsu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Joel Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1\">Aniket Kittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integration of Pre-trained Networks with Continuous Token Interface for End-to-End Spoken Language Understanding. (arXiv:2104.07253v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07253","description":"<p>Most End-to-End (E2E) SLU networks leverage the pre-trained ASR networks but\nstill lack the capability to understand the semantics of utterances, crucial\nfor the SLU task. To solve this, recently proposed studies use pre-trained NLU\nnetworks. However, it is not trivial to fully utilize both pre-trained\nnetworks; many solutions were proposed, such as Knowledge Distillation,\ncross-modal shared embedding, and network integration with Interface. We\npropose a simple and robust integration method for the E2E SLU network with\nnovel Interface, Continuous Token Interface (CTI), the junctional\nrepresentation of the ASR and NLU networks when both networks are pre-trained\nwith the same vocabulary. Because the only difference is the noise level, we\ndirectly feed the ASR network's output to the NLU network. Thus, we can train\nour SLU network in an E2E manner without additional modules, such as\nGumbel-Softmax. We evaluate our model using SLURP, a challenging SLU dataset\nand achieve state-of-the-art scores on both intent classification and slot\nfilling tasks. We also verify the NLU network, pre-trained with Masked Language\nModel, can utilize a noisy textual representation of CTI. Moreover, we show our\nmodel can be trained with multi-task learning from heterogeneous data even\nafter integration with CTI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seunghyun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bowon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Reptile: Inter-Task Gradient Alignment for Multilingual Learning. (arXiv:2110.02600v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02600","description":"<p>Multilingual models jointly pretrained on multiple languages have achieved\nremarkable performance on various multilingual downstream tasks. Moreover,\nmodels finetuned on a single monolingual downstream task have shown to\ngeneralize to unseen languages. In this paper, we first show that it is crucial\nfor those tasks to align gradients between them in order to maximize knowledge\ntransfer while minimizing negative transfer. Despite its importance, the\nexisting methods for gradient alignment either have a completely different\npurpose, ignore inter-task alignment, or aim to solve continual learning\nproblems in rather inefficient ways. As a result of the misaligned gradients\nbetween tasks, the model suffers from severe negative transfer in the form of\ncatastrophic forgetting of the knowledge acquired from the pretraining. To\novercome the limitations, we propose a simple yet effective method that can\nefficiently align gradients between tasks. Specifically, we perform each\ninner-optimization by sequentially sampling batches from all the tasks,\nfollowed by a Reptile outer update. Thanks to the gradients aligned between\ntasks by our method, the model becomes less vulnerable to negative transfer and\ncatastrophic forgetting. We extensively validate our method on various\nmulti-task learning and zero-shot cross-lingual transfer tasks, where our\nmethod largely outperforms all the relevant baselines we consider.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seanie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hae Beom Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Juho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LFPT5: A Unified Framework for Lifelong Few-shot Language Learning Based on Prompt Tuning of T5. (arXiv:2110.07298v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07298","description":"<p>Existing approaches to lifelong language learning rely on plenty of labeled\ndata for learning a new task, which is hard to obtain in most real scenarios.\nConsidering that humans can continually learn new tasks from a handful of\nexamples, we expect the models also to be able to generalize well on new\nfew-shot tasks without forgetting the previous ones. In this work, we define\nthis more challenging yet practical problem as Lifelong Few-shot Language\nLearning (LFLL) and propose a unified framework for it based on prompt tuning\nof T5. Our framework called LFPT5 takes full advantage of PT's strong few-shot\nlearning ability, and simultaneously trains the model as a task solver and a\ndata generator. Before learning a new domain of the same task type, LFPT5\ngenerates pseudo (labeled) samples of previously learned domains, and later\ngets trained on those samples to alleviate forgetting of previous knowledge as\nit learns the new domain. In addition, a KL divergence loss is minimized to\nachieve label consistency between the previous and the current model. While\nadapting to a new task type, LFPT5 includes and tunes additional prompt\nembeddings for the new task. With extensive experiments, we demonstrate that\nLFPT5 can be applied to various different types of tasks and significantly\noutperform previous methods in different LFLL settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chengwei Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't speak too fast: The impact of data bias on self-supervised speech models. (arXiv:2110.07957v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.07957","description":"<p>Self-supervised Speech Models (S3Ms) have been proven successful in many\nspeech downstream tasks, like ASR. However, how pre-training data affects S3Ms'\ndownstream behavior remains an unexplored issue. In this paper, we study how\npre-training data affects S3Ms by pre-training models on biased datasets\ntargeting different factors of speech, including gender, content, and prosody,\nand evaluate these pre-trained S3Ms on selected downstream tasks in SUPERB\nBenchmark. Our experiments show that S3Ms have tolerance toward gender bias.\nMoreover, we find that the content of speech has little impact on the\nperformance of S3Ms across downstream tasks, but S3Ms do show a preference\ntoward a slower speech rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_Y/0/1/0/all/0/1\">Yen Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chou_Y/0/1/0/all/0/1\">Yi-Hui Chou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1\">Andy T. Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DataWords: Getting Contrarian with Text, Structured Data and Explanations. (arXiv:2111.05384v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.05384","description":"<p>Our goal is to build classification models using a combination of free-text\nand structured data. To do this, we represent structured data by text\nsentences, DataWords, so that similar data items are mapped into the same\nsentence. This permits modeling a mixture of text and structured data by using\nonly text-modeling algorithms. Several examples illustrate that it is possible\nto improve text classification performance by first running extraction tools\n(named entity recognition), then converting the output to DataWords, and adding\nthe DataWords to the original text -- before model building and classification.\nThis approach also allows us to produce explanations for inferences in terms of\nboth free text and structured data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gallant_S/0/1/0/all/0/1\">Stephen I. Gallant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1\">Mirza Nasir Hossain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"User Response and Sentiment Prediction for Automatic Dialogue Evaluation. (arXiv:2111.08808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.08808","description":"<p>Automatic evaluation is beneficial for open-domain dialog system development.\nHowever, standard word-overlap metrics (BLEU, ROUGE) do not correlate well with\nhuman judgements of open-domain dialog systems. In this work we propose to use\nthe sentiment of the next user utterance for turn or dialog level evaluation.\nSpecifically we propose three methods: one that predicts the next sentiment\ndirectly, and two others that predict the next user utterance using an\nutterance or a feedback generator model and then classify its sentiment.\nExperiments show our model outperforming existing automatic evaluation metrics\non both written and spoken open-domain dialogue datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning music audio representations via weak language supervision. (arXiv:2112.04214v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2112.04214","description":"<p>Audio representations for music information retrieval are typically learned\nvia supervised learning in a task-specific fashion. Although effective at\nproducing state-of-the-art results, this scheme lacks flexibility with respect\nto the range of applications a model can have and requires extensively\nannotated datasets. In this work, we pose the question of whether it may be\npossible to exploit weakly aligned text as the only supervisory signal to learn\ngeneral-purpose music audio representations. To address this question, we\ndesign a multimodal architecture for music and language pre-training (MuLaP)\noptimised via a set of proxy tasks. Weak supervision is provided in the form of\nnoisy natural language descriptions conveying the overall musical content of\nthe track. After pre-training, we transfer the audio backbone of the model to a\nset of music audio classification and regression tasks. We demonstrate the\nusefulness of our approach by comparing the performance of audio\nrepresentations produced by the same audio backbone with different training\nstrategies and show that our pre-training method consistently achieves\ncomparable or higher scores on all tasks and datasets considered. Our\nexperiments also confirm that MuLaP effectively leverages audio-caption pairs\nto learn representations that are competitive with audio-only and cross-modal\nself-supervised methods in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manco_I/0/1/0/all/0/1\">Ilaria Manco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1\">Emmanouil Benetos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quinton_E/0/1/0/all/0/1\">Elio Quinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1\">Gyorgy Fazekas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Isometric MT: Neural Machine Translation for Automatic Dubbing. (arXiv:2112.08682v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08682","description":"<p>Automatic dubbing (AD) is among the machine translation (MT) use cases where\ntranslations should match a given length to allow for synchronicity between\nsource and target speech. For neural MT, generating translations of length\nclose to the source length (e.g. within +-10% in character count), while\npreserving quality is a challenging task. Controlling MT output length comes at\na cost to translation quality, which is usually mitigated with a two step\napproach of generating N-best hypotheses and then re-ranking based on length\nand quality. This work introduces a self-learning approach that allows a\ntransformer model to directly learn to generate outputs that closely match the\nsource length, in short Isometric MT. In particular, our approach does not\nrequire to generate multiple hypotheses nor any auxiliary ranking function. We\nreport results on four language pairs (English - French, Italian, German,\nSpanish) with a publicly available benchmark. Automatic and manual evaluations\nshow that our method for Isometric MT outperforms more complex approaches\nproposed in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel M. Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sublinear Time Approximation of Text Similarity Matrices. (arXiv:2112.09631v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.09631","description":"<p>We study algorithms for approximating pairwise similarity matrices that arise\nin natural language processing. Generally, computing a similarity matrix for\n$n$ data points requires $\\Omega(n^2)$ similarity computations. This quadratic\nscaling is a significant bottleneck, especially when similarities are computed\nvia expensive functions, e.g., via transformer models. Approximation methods\nreduce this quadratic complexity, often by using a small subset of exactly\ncomputed similarities to approximate the remainder of the complete pairwise\nsimilarity matrix.\n</p>\n<p>Significant work focuses on the efficient approximation of positive\nsemidefinite (PSD) similarity matrices, which arise e.g., in kernel methods.\nHowever, much less is understood about indefinite (non-PSD) similarity\nmatrices, which often arise in NLP. Motivated by the observation that many of\nthese matrices are still somewhat close to PSD, we introduce a generalization\nof the popular Nystr\\\"{o}m method to the indefinite setting. Our algorithm can\nbe applied to any similarity matrix and runs in sublinear time in the size of\nthe matrix, producing a rank-$s$ approximation with just $O(ns)$ similarity\ncomputations.\n</p>\n<p>We show that our method, along with a simple variant of CUR decomposition,\nperforms very well in approximating a variety of similarity matrices arising in\nNLP tasks. We demonstrate high accuracy of the approximated similarity matrices\nin the downstream tasks of document classification, sentence similarity, and\ncross-document coreference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Archan Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monath_N/0/1/0/all/0/1\">Nicholas Monath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1\">Cameron Musco</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regional Differences in Information Privacy Concerns After the Facebook-Cambridge Analytica Data Scandal. (arXiv:2202.07075v2 [cs.SI] UPDATED)","link":"http://arxiv.org/abs/2202.07075","description":"<p>While there is increasing global attention to data privacy, most of their\ncurrent theoretical understanding is based on research conducted in a few\ncountries. Prior work argues that people's cultural backgrounds might shape\ntheir privacy concerns; thus, we could expect people from different world\nregions to conceptualize them in diverse ways. We collected and analyzed a\nlarge-scale dataset of tweets about the #CambridgeAnalytica scandal in Spanish\nand English to start exploring this hypothesis. We employed word embeddings and\nqualitative analysis to identify which information privacy concerns are present\nand characterize language and regional differences in emphasis on these\nconcerns. Our results suggest that related concepts, such as regulations, can\nbe added to current information privacy frameworks. We also observe a greater\nemphasis on data collection in English than in Spanish. Additionally, data from\nNorth America exhibits a narrower focus on awareness compared to other regions\nunder study. Our results call for more diverse sources of data and nuanced\nanalysis of data privacy concerns around the globe.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Pizarro_F/0/1/0/all/0/1\">Felipe Gonz&#xe1;lez-Pizarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueroa_A/0/1/0/all/0/1\">Andrea Figueroa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1\">Claudia L&#xf3;pez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragon_C/0/1/0/all/0/1\">Cecilia Aragon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Speech Recognition By Learning Conversation-level Characteristics. (arXiv:2202.07855v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.07855","description":"<p>Conversational automatic speech recognition (ASR) is a task to recognize\nconversational speech including multiple speakers. Unlike sentence-level ASR,\nconversational ASR can naturally take advantages from specific characteristics\nof conversation, such as role preference and topical coherence. This paper\nproposes a conversational ASR model which explicitly learns conversation-level\ncharacteristics under the prevalent end-to-end neural framework. The highlights\nof the proposed model are twofold. First, a latent variational module (LVM) is\nattached to a conformer-based encoder-decoder ASR backbone to learn role\npreference and topical coherence. Second, a topic model is specifically adopted\nto bias the outputs of the decoder to words in the predicted topics.\nExperiments on two Mandarin conversational ASR tasks show that the proposed\nmodel achieves a maximum 12% relative character error rate (CER) reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1\">Kun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Sining Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Long Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Evaluation and Analysis of Different Aggregation and Hyperparameter Selection Methods for Federated Brain Tumor Segmentation. (arXiv:2202.08261v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08261","description":"<p>Availability of large, diverse, and multi-national datasets is crucial for\nthe development of effective and clinically applicable AI systems in the\nmedical imaging domain. However, forming a global model by bringing these\ndatasets together at a central location, comes along with various data privacy\nand ownership problems. To alleviate these problems, several recent studies\nfocus on the federated learning paradigm, a distributed learning approach for\ndecentralized data. Federated learning leverages all the available data without\nany need for sharing collaborators' data with each other or collecting them on\na central server. Studies show that federated learning can provide competitive\nperformance with conventional central training, while having a good\ngeneralization capability. In this work, we have investigated several federated\nlearning approaches on the brain tumor segmentation problem. We explore\ndifferent strategies for faster convergence and better performance which can\nalso work on strong Non-IID cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Isik_Polat_E/0/1/0/all/0/1\">Ece Isik-Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polat_G/0/1/0/all/0/1\">Gorkem Polat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocyigit_A/0/1/0/all/0/1\">Altan Kocyigit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1\">Alptekin Temizel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phase Aberration Robust Beamformer for Planewave US Using Self-Supervised Learning. (arXiv:2202.08262v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08262","description":"<p>Ultrasound (US) is widely used for clinical imaging applications thanks to\nits real-time and non-invasive nature. However, its lesion detectability is\noften limited in many applications due to the phase aberration artefact caused\nby variations in the speed of sound (SoS) within body parts. To address this,\nhere we propose a novel self-supervised 3D CNN that enables phase aberration\nrobust plane-wave imaging. Instead of aiming at estimating the SoS distribution\nas in conventional methods, our approach is unique in that the network is\ntrained in a self-supervised manner to robustly generate a high-quality image\nfrom various phase aberrated images by modeling the variation in the speed of\nsound as stochastic. Experimental results using real measurements from\ntissue-mimicking phantom and \\textit{in vivo} scans confirmed that the proposed\nmethod can significantly reduce the phase aberration artifacts and improve the\nvisual quality of deep scans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1\">Shujaat Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1\">Jaeyoung Huh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenKBP-Opt: An international and reproducible evaluation of 76 knowledge-based planning pipelines. (arXiv:2202.08303v1 [physics.med-ph])","link":"http://arxiv.org/abs/2202.08303","description":"<p>We establish an open framework for developing plan optimization models for\nknowledge-based planning (KBP) in radiotherapy. Our framework includes\nreference plans for 100 patients with head-and-neck cancer and high-quality\ndose predictions from 19 KBP models that were developed by different research\ngroups during the OpenKBP Grand Challenge. The dose predictions were input to\nfour optimization models to form 76 unique KBP pipelines that generated 7600\nplans. The predictions and plans were compared to the reference plans via: dose\nscore, which is the average mean absolute voxel-by-voxel difference in dose a\nmodel achieved; the deviation in dose-volume histogram (DVH) criterion; and the\nfrequency of clinical planning criteria satisfaction. We also performed a\ntheoretical investigation to justify our dose mimicking models. The range in\nrank order correlation of the dose score between predictions and their KBP\npipelines was 0.50 to 0.62, which indicates that the quality of the predictions\nis generally positively correlated with the quality of the plans. Additionally,\ncompared to the input predictions, the KBP-generated plans performed\nsignificantly better (P&lt;0.05; one-sided Wilcoxon test) on 18 of 23 DVH\ncriteria. Similarly, each optimization model generated plans that satisfied a\nhigher percentage of criteria than the reference plans. Lastly, our theoretical\ninvestigation demonstrated that the dose mimicking models generated plans that\nare also optimal for a conventional planning model. This was the largest\ninternational effort to date for evaluating the combination of KBP prediction\nand optimization models. In the interest of reproducibility, our data and code\nis freely available at https://github.com/ababier/open-kbp-opt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Babier_A/0/1/0/all/0/1\">Aaron Babier</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mahmood_R/0/1/0/all/0/1\">Rafid Mahmood</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_B/0/1/0/all/0/1\">Binghao Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Alves_V/0/1/0/all/0/1\">Victor G. L. Alves</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Barragan_Montero_A/0/1/0/all/0/1\">Ana Maria Barrag&#xe1;n-Montero</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beaudry_J/0/1/0/all/0/1\">Joel Beaudry</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Cardenas_C/0/1/0/all/0/1\">Carlos E. Cardenas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chang_Y/0/1/0/all/0/1\">Yankui Chang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_Z/0/1/0/all/0/1\">Zijie Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chun_J/0/1/0/all/0/1\">Jaehee Chun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Diaz_K/0/1/0/all/0/1\">Kelly Diaz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Eraso_H/0/1/0/all/0/1\">Harold David Eraso</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Faustmann_E/0/1/0/all/0/1\">Erik Faustmann</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gaj_S/0/1/0/all/0/1\">Sibaji Gaj</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gay_S/0/1/0/all/0/1\">Skylar Gay</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Gronberg_M/0/1/0/all/0/1\">Mary Gronberg</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Guo_B/0/1/0/all/0/1\">Bingqi Guo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+He_J/0/1/0/all/0/1\">Junjun He</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Heilemann_G/0/1/0/all/0/1\">Gerd Heilemann</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hira_S/0/1/0/all/0/1\">Sanchit Hira</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Huang_Y/0/1/0/all/0/1\">Yuliang Huang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ji_F/0/1/0/all/0/1\">Fuxin Ji</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Jiang_D/0/1/0/all/0/1\">Dashan Jiang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Giraldo_J/0/1/0/all/0/1\">Jean Carlo Jimenez Giraldo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lee_H/0/1/0/all/0/1\">Hoyeon Lee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lian_J/0/1/0/all/0/1\">Jun Lian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_S/0/1/0/all/0/1\">Shuolin Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Liu_K/0/1/0/all/0/1\">Keng-Chi Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Marrugo_J/0/1/0/all/0/1\">Jos&#xe9; Marrugo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Miki_K/0/1/0/all/0/1\">Kentaro Miki</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nakamura_K/0/1/0/all/0/1\">Kunio Nakamura</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Netherton_T/0/1/0/all/0/1\">Tucker Netherton</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nguyen_D/0/1/0/all/0/1\">Dan Nguyen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nourzadeh_H/0/1/0/all/0/1\">Hamidreza Nourzadeh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Osman_A/0/1/0/all/0/1\">Alexander F. I. Osman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Peng_Z/0/1/0/all/0/1\">Zhao Peng</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Munoz_J/0/1/0/all/0/1\">Jos&#xe9; Dar&#xed;o Quinto Mu&#xf1;oz</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ramsl_C/0/1/0/all/0/1\">Christian Ramsl</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rhee_D/0/1/0/all/0/1\">Dong Joo Rhee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rodriguez_J/0/1/0/all/0/1\">Juan David Rodriguez</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Siebers_J/0/1/0/all/0/1\">Jeffrey V. Siebers</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Soomro_M/0/1/0/all/0/1\">Mumtaz H. Soomro</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_K/0/1/0/all/0/1\">Kay Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hoyos_A/0/1/0/all/0/1\">Andr&#xe9;s Usuga Hoyos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Valderrama_C/0/1/0/all/0/1\">Carlos Valderrama</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Verbeek_R/0/1/0/all/0/1\">Rob Verbeek</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_E/0/1/0/all/0/1\">Enpei Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Willems_S/0/1/0/all/0/1\">Siri Willems</a>, et al. (10 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextualize differential privacy in image database: a lightweight image differential privacy approach based on principle component analysis inverse. (arXiv:2202.08309v1 [cs.CR])","link":"http://arxiv.org/abs/2202.08309","description":"<p>Differential privacy (DP) has been the de-facto standard to preserve\nprivacy-sensitive information in database. Nevertheless, there lacks a clear\nand convincing contextualization of DP in image database, where individual\nimages' indistinguishable contribution to a certain analysis can be achieved\nand observed when DP is exerted. As a result, the privacy-accuracy trade-off\ndue to integrating DP is insufficiently demonstrated in the context of\ndifferentially-private image database. This work aims at contextualizing DP in\nimage database by an explicit and intuitive demonstration of integrating\nconceptional differential privacy with images. To this end, we design a\nlightweight approach dedicating to privatizing image database as a whole and\npreserving the statistical semantics of the image database to an adjustable\nlevel, while making individual images' contribution to such statistics\nindistinguishable. The designed approach leverages principle component analysis\n(PCA) to reduce the raw image with large amount of attributes to a lower\ndimensional space whereby DP is performed, so as to decrease the DP load of\ncalculating sensitivity attribute-by-attribute. The DP-exerted image data,\nwhich is not visible in its privatized format, is visualized through PCA\ninverse such that both a human and machine inspector can evaluate the\nprivatization and quantify the privacy-accuracy trade-off in an analysis on the\nprivatized image database. Using the devised approach, we demonstrate the\ncontextualization of DP in images by two use cases based on deep learning\nmodels, where we show the indistinguishability of individual images induced by\nDP and the privatized images' retention of statistical semantics in deep\nlearning tasks, which is elaborated by quantitative analyses on the\nprivacy-accuracy trade-off under different privatization settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuehui Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1\">Hui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tengyuan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yajie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuzhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Augmentation Is Worth A Thousand Samples: Exact Quantification From Analytical Augmented Sample Moments. (arXiv:2202.08325v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08325","description":"<p>Data-Augmentation (DA) is known to improve performance across tasks and\ndatasets. We propose a method to theoretically analyze the effect of DA and\nstudy questions such as: how many augmented samples are needed to correctly\nestimate the information encoded by that DA? How does the augmentation policy\nimpact the final parameters of a model? We derive several quantities in\nclose-form, such as the expectation and variance of an image, loss, and model's\noutput under a given DA distribution. Those derivations open new avenues to\nquantify the benefits and limitations of DA. For example, we show that common\nDAs require tens of thousands of samples for the loss at hand to be correctly\nestimated and for the model training to converge. We show that for a training\nloss to be stable under DA sampling, the model's saliency map (gradient of the\nloss with respect to the model's input) must align with the smallest\neigenvector of the sample variance under the considered DA augmentation,\nhinting at a possible explanation on why models tend to shift their focus from\nedges to textures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1\">Randall Balestriero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CortexODE: Learning Cortical Surface Reconstruction by Neural ODEs. (arXiv:2202.08329v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08329","description":"<p>We present CortexODE, a deep learning framework for cortical surface\nreconstruction. CortexODE leverages neural ordinary different equations (ODEs)\nto deform an input surface into a target shape by learning a diffeomorphic\nflow. The trajectories of the points on the surface are modeled as ODEs, where\nthe derivatives of their coordinates are parameterized via a learnable\nLipschitz-continuous deformation network. This provides theoretical guarantees\nfor the prevention of self-intersections. CortexODE can be integrated to an\nautomatic learning-based pipeline, which reconstructs cortical surfaces\nefficiently in less than 6 seconds. The pipeline utilizes a 3D U-Net to predict\na white matter segmentation from brain Magnetic Resonance Imaging (MRI) scans,\nand further generates a signed distance function that represents an initial\nsurface. Fast topology correction is introduced to guarantee homeomorphism to a\nsphere. Following the isosurface extraction step, two CortexODE models are\ntrained to deform the initial surface to white matter and pial surfaces\nrespectively. The proposed pipeline is evaluated on large-scale neuroimage\ndatasets in various age groups including neonates (25-45 weeks), young adults\n(22-36 years) and elderly subjects (55-90 years). Our experiments demonstrate\nthat the CortexODE-based pipeline can achieve less than 0.2mm average geometric\nerror while being orders of magnitude faster compared to conventional\nprocessing pipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ma_Q/0/1/0/all/0/1\">Qiang Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1\">Liu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robinson_E/0/1/0/all/0/1\">Emma C. Robinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alansary_A/0/1/0/all/0/1\">Amir Alansary</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Developmentally-Inspired Examination of Shape versus Texture Bias in Machines. (arXiv:2202.08340v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08340","description":"<p>Early in development, children learn to extend novel category labels to\nobjects with the same shape, a phenomenon known as the shape bias. Inspired by\nthese findings, Geirhos et al. (2019) examined whether deep neural networks\nshow a shape or texture bias by constructing images with conflicting shape and\ntexture cues. They found that convolutional neural networks strongly preferred\nto classify familiar objects based on texture as opposed to shape, suggesting a\ntexture bias. However, there are a number of differences between how the\nnetworks were tested in this study versus how children are typically tested. In\nthis work, we re-examine the inductive biases of neural networks by adapting\nthe stimuli and procedure from Geirhos et al. (2019) to more closely follow the\ndevelopmental paradigm and test on a wide range of pre-trained neural networks.\nAcross three experiments, we find that deep neural networks exhibit a\npreference for shape rather than texture when tested under conditions that more\nclosely replicate the developmental procedure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tartaglini_A/0/1/0/all/0/1\">Alexa R. Tartaglini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vong_W/0/1/0/all/0/1\">Wai Keen Vong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lake_B/0/1/0/all/0/1\">Brenden M. Lake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anomalib: A Deep Learning Library for Anomaly Detection. (arXiv:2202.08341v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08341","description":"<p>This paper introduces anomalib, a novel library for unsupervised anomaly\ndetection and localization. With reproducibility and modularity in mind, this\nopen-source library provides algorithms from the literature and a set of tools\nto design custom anomaly detection algorithms via a plug-and-play approach.\nAnomalib comprises state-of-the-art anomaly detection algorithms that achieve\ntop performance on the benchmarks and that can be used off-the-shelf. In\naddition, the library provides components to design custom algorithms that\ncould be tailored towards specific needs. Additional tools, including\nexperiment trackers, visualizers, and hyper-parameter optimizers, make it\nsimple to design and implement anomaly detection models. The library also\nsupports OpenVINO model optimization and quantization for real-time deployment.\nOverall, anomalib is an extensive library for the design, implementation, and\ndeployment of unsupervised anomaly detection models from data to the edge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1\">Samet Akcay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ameln_D/0/1/0/all/0/1\">Dick Ameln</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1\">Ashwin Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakshmanan_B/0/1/0/all/0/1\">Barath Lakshmanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_N/0/1/0/all/0/1\">Nilesh Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genc_U/0/1/0/all/0/1\">Utku Genc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Smooth Neural Functions via Lipschitz Regularization. (arXiv:2202.08345v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08345","description":"<p>Neural implicit fields have recently emerged as a useful representation for\n3D shapes. These fields are commonly represented as neural networks which map\nlatent descriptors and 3D coordinates to implicit function values. The latent\ndescriptor of a neural field acts as a deformation handle for the 3D shape it\nrepresents. Thus, smoothness with respect to this descriptor is paramount for\nperforming shape-editing operations. In this work, we introduce a novel\nregularization designed to encourage smooth latent spaces in neural fields by\npenalizing the upper bound on the field's Lipschitz constant. Compared with\nprior Lipschitz regularized networks, ours is computationally fast, can be\nimplemented in four lines of code, and requires minimal hyperparameter tuning\nfor geometric applications. We demonstrate the effectiveness of our approach on\nshape interpolation and extrapolation as well as partial shape reconstruction\nfrom 3D point clouds, showing both qualitative and quantitative improvements\nover existing state-of-the-art and non-regularized baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hsueh-Ti Derek Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_F/0/1/0/all/0/1\">Francis Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobson_A/0/1/0/all/0/1\">Alec Jacobson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1\">Sanja Fidler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litany_O/0/1/0/all/0/1\">Or Litany</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision. (arXiv:2202.08360v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08360","description":"<p>Discriminative self-supervised learning allows training models on any random\ngroup of internet images, and possibly recover salient information that helps\ndifferentiate between the images. Applied to ImageNet, this leads to object\ncentric features that perform on par with supervised features on most\nobject-centric downstream tasks. In this work, we question if using this\nability, we can learn any salient and more representative information present\nin diverse unbounded set of images from across the globe. To do so, we train\nmodels on billions of random images without any data pre-processing or prior\nassumptions about what we want the model to learn. We scale our model size to\ndense 10 billion parameters to avoid underfitting on a large data size. We\nextensively study and validate our model performance on over 50 benchmarks\nincluding fairness, robustness to distribution shift, geographical diversity,\nfine grained recognition, image copy detection and many image classification\ndatasets. The resulting model, not only captures well semantic information, it\nalso captures information about artistic style and learns salient information\nsuch as geolocations and multilingual word embeddings based on visual content\nonly. More importantly, we discover that such model is more robust, more fair,\nless harmful and less biased than supervised models or models trained on object\ncentric datasets such as ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Priya Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duval_Q/0/1/0/all/0/1\">Quentin Duval</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seessel_I/0/1/0/all/0/1\">Isaac Seessel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Mannat Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagun_L/0/1/0/all/0/1\">Levent Sagun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fuzzy Pooling. (arXiv:2202.08372v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08372","description":"<p>Convolutional Neural Networks (CNNs) are artificial learning systems\ntypically based on two operations: convolution, which implements feature\nextraction through filtering, and pooling, which implements dimensionality\nreduction. The impact of pooling in the classification performance of the CNNs\nhas been highlighted in several previous works, and a variety of alternative\npooling operators have been proposed. However, only a few of them tackle with\nthe uncertainty that is naturally propagated from the input layer to the\nfeature maps of the hidden layers through convolutions. In this paper we\npresent a novel pooling operation based on (type-1) fuzzy sets to cope with the\nlocal imprecision of the feature maps, and we investigate its performance in\nthe context of image classification. Fuzzy pooling is performed by\nfuzzification, aggregation and defuzzification of feature map neighborhoods. It\nis used for the construction of a fuzzy pooling layer that can be applied as a\ndrop-in replacement of the current, crisp, pooling layers of CNN architectures.\nSeveral experiments using publicly available datasets show that the proposed\napproach can enhance the classification performance of a CNN. A comparative\nevaluation shows that it outperforms state-of-the-art pooling approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Diamantis_D/0/1/0/all/0/1\">Dimitrios E. Diamantis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iakovidis_D/0/1/0/all/0/1\">Dimitris K. Iakovidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How to Fill the Optimum Set? Population Gradient Descent with Harmless Diversity. (arXiv:2202.08376v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08376","description":"<p>Although traditional optimization methods focus on finding a single optimal\nsolution, most objective functions in modern machine learning problems,\nespecially those in deep learning, often have multiple or infinite numbers of\noptima. Therefore, it is useful to consider the problem of finding a set of\ndiverse points in the optimum set of an objective function. In this work, we\nframe this problem as a bi-level optimization problem of maximizing a diversity\nscore inside the optimum set of the main loss function, and solve it with a\nsimple population gradient descent framework that iteratively updates the\npoints to maximize the diversity score in a fashion that does not hurt the\noptimization of the main loss. We demonstrate that our method can efficiently\ngenerate diverse solutions on a variety of applications, including\ntext-to-image generation, text-to-mesh generation, molecular conformation\ngeneration and ensemble neural network training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lemeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Limitations of Neural Collapse for Understanding Generalization in Deep Learning. (arXiv:2202.08384v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08384","description":"<p>The recent work of Papyan, Han, &amp; Donoho (2020) presented an intriguing\n\"Neural Collapse\" phenomenon, showing a structural property of interpolating\nclassifiers in the late stage of training. This opened a rich area of\nexploration studying this phenomenon. Our motivation is to study the upper\nlimits of this research program: How far will understanding Neural Collapse\ntake us in understanding deep learning? First, we investigate its role in\ngeneralization. We refine the Neural Collapse conjecture into two separate\nconjectures: collapse on the train set (an optimization property) and collapse\non the test distribution (a generalization property). We find that while Neural\nCollapse often occurs on the train set, it does not occur on the test set. We\nthus conclude that Neural Collapse is primarily an optimization phenomenon,\nwith as-yet-unclear connections to generalization. Second, we investigate the\nrole of Neural Collapse in feature learning. We show simple, realistic\nexperiments where training longer leads to worse last-layer features, as\nmeasured by transfer-performance on a downstream task. This suggests that\nneural collapse is not always desirable for representation learning, as\npreviously claimed. Finally, we give preliminary evidence of a \"cascading\ncollapse\" phenomenon, wherein some form of Neural Collapse occurs not only for\nthe last layer, but in earlier layers as well. We hope our work encourages the\ncommunity to continue the rich line of Neural Collapse research, while also\nconsidering its inherent limitations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1\">Like Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1\">Mikhail Belkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1\">Preetum Nakkiran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shift-Memory Network for Temporal Scene Segmentation. (arXiv:2202.08399v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08399","description":"<p>Semantic segmentation has achieved great accuracy in understanding spatial\nlayout. For real-time tasks based on dynamic scenes, we extend semantic\nsegmentation in temporal domain to enhance the spatial accuracy with motion. We\nutilize a shift-mode network over streaming input to ensure zero-latency\noutput. For the data overlap under shifting network, this paper identifies\nrepeated computation in fixed periods across network layers. To avoid this\nredundancy, we derive a Shift-Memory Network (SMN) from encoding-decoding\nbaseline to reuse the network values without accuracy loss. Trained in\npatch-mode, the SMN extracts the network parameters for SMN to perform\ninference promptly in compact memory. We segment dynamic scenes from 1D\nscanning input and 2D video. The experiments of SMN achieve equivalent accuracy\nas shift-mode but in faster inference speeds and much smaller memory. This will\nfacilitate semantic segmentation in real-time application on edge devices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Guo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jiang Yu Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FPIC: A Novel Semantic Dataset for Optical PCB Assurance. (arXiv:2202.08414v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08414","description":"<p>The continued outsourcing of printed circuit board (PCB) fabrication to\noverseas venues necessitates increased hardware assurance capabilities. Toward\nthis end, several automated optical inspection (AOI) techniques have been\nproposed in the past exploring various aspects of PCB images acquired using\ndigital cameras. In this work, we review state-of-the-art AOI techniques and\nobserved the strong, rapid trend toward machine learning (ML) solutions. These\nrequire significant amounts of labeled ground truth data, which is lacking in\nthe publicly available PCB data space. We propose the FICS PBC Image Collection\n(FPIC) dataset to address this bottleneck in available large-volume, diverse,\nsemantic annotations. Additionally, this work covers the potential increase in\nhardware security capabilities and observed methodological distinctions\nhighlighted during data collection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jessurun_N/0/1/0/all/0/1\">Nathan Jessurun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dizon_Paradis_O/0/1/0/all/0/1\">Olivia P. Dizon-Paradis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1\">Jacob Harrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shajib Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tehranipoor_M/0/1/0/all/0/1\">Mark M. Tehranipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodard_D/0/1/0/all/0/1\">Damon L. Woodard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadizanjani_N/0/1/0/all/0/1\">Navid Asadizanjani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Marionette: Unsupervised Learning of Motion Skeleton and Latent Dynamics from Volumetric Video. (arXiv:2202.08418v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08418","description":"<p>We present Neural Marionette, an unsupervised approach that discovers the\nskeletal structure from a dynamic sequence and learns to generate diverse\nmotions that are consistent with the observed motion dynamics. Given a video\nstream of point cloud observation of an articulated body under arbitrary\nmotion, our approach discovers the unknown low-dimensional skeletal\nrelationship that can effectively represent the movement. Then the discovered\nstructure is utilized to encode the motion priors of dynamic sequences in a\nlatent structure, which can be decoded to the relative joint rotations to\nrepresent the full skeletal motion. Our approach works without any prior\nknowledge of the underlying motion or skeletal structure, and we demonstrate\nthat the discovered structure is even comparable to the hand-labeled ground\ntruth skeleton in representing a 4D sequence of motion. The skeletal structure\nembeds the general semantics of possible motion space that can generate motions\nfor diverse scenarios. We verify that the learned motion prior is generalizable\nto the multi-modal sequence generation, interpolation of two poses, and motion\nretargeting to a different skeletal structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bae_J/0/1/0/all/0/1\">Jinseok Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hojun Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_C/0/1/0/all/0/1\">Cheol-Hui Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hyungun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young Min Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AKB-48: A Real-World Articulated Object Knowledge Base. (arXiv:2202.08432v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08432","description":"<p>Human life is populated with articulated objects. A comprehensive\nunderstanding of articulated objects, namely appearance, structure, physics\nproperty, and semantics, will benefit many research communities. As current\narticulated object understanding solutions are usually based on synthetic\nobject dataset with CAD models without physics properties, which prevent\nsatisfied generalization from simulation to real-world applications in visual\nand robotics tasks. To bridge the gap, we present AKB-48: a large-scale\nArticulated object Knowledge Base which consists of 2,037 real-world 3D\narticulated object models of 48 categories. Each object is described by a\nknowledge graph ArtiKG. To build the AKB-48, we present a fast articulation\nknowledge modeling (FArM) pipeline, which can fulfill the ArtiKG for an\narticulated object within 10-15 minutes, and largely reduce the cost for object\nmodeling in the real world. Using our dataset, we propose AKBNet, a novel\nintegral pipeline for Category-level Visual Articulation Manipulation (C-VAM)\ntask, in which we benchmark three sub-tasks, namely pose estimation, object\nreconstruction and manipulation. Dataset, codes, and models will be publicly\navailable at https://liuliu66.github.io/articulationobjects/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Haoyuan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Sucheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yang Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PENCIL: Deep Learning with Noisy Labels. (arXiv:2202.08436v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08436","description":"<p>Deep learning has achieved excellent performance in various computer vision\ntasks, but requires a lot of training examples with clean labels. It is easy to\ncollect a dataset with noisy labels, but such noise makes networks overfit\nseriously and accuracies drop dramatically. To address this problem, we propose\nan end-to-end framework called PENCIL, which can update both network parameters\nand label estimations as label distributions. PENCIL is independent of the\nbackbone network structure and does not need an auxiliary clean dataset or\nprior information about noise, thus it is more general and robust than existing\nmethods and is easy to apply. PENCIL can even be used repeatedly to obtain\nbetter performance. PENCIL outperforms previous state-of-the-art methods by\nlarge margins on both synthetic and real-world datasets with different noise\ntypes and noise rates. And PENCIL is also effective in multi-label\nclassification tasks through adding a simple attention structure on backbone\nnetworks. Experiments show that PENCIL is robust on clean datasets, too.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kun Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guo-Hua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianxin Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual attention analysis of pathologists examining whole slide images of Prostate cancer. (arXiv:2202.08437v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08437","description":"<p>We study the attention of pathologists as they examine whole-slide images\n(WSIs) of prostate cancer tissue using a digital microscope. To the best of our\nknowledge, our study is the first to report in detail how pathologists navigate\nWSIs of prostate cancer as they accumulate information for their diagnoses. We\ncollected slide navigation data (i.e., viewport location, magnification level,\nand time) from 13 pathologists in 2 groups (5 genitourinary (GU) specialists\nand 8 general pathologists) and generated visual attention heatmaps and\nscanpaths. Each pathologist examined five WSIs from the TCGA PRAD dataset,\nwhich were selected by a GU pathology specialist. We examined and analyzed the\ndistributions of visual attention for each group of pathologists after each WSI\nwas examined. To quantify the relationship between a pathologist's attention\nand evidence for cancer in the WSI, we obtained tumor annotations from a\ngenitourinary specialist. We used these annotations to compute the overlap\nbetween the distribution of visual attention and annotated tumor region to\nidentify strong correlations. Motivated by this analysis, we trained a deep\nlearning model to predict visual attention on unseen WSIs. We find that the\nattention heatmaps predicted by our model correlate quite well with the ground\ntruth attention heatmap and tumor annotations on a test set of 17 WSIs by using\nvarious spatial and temporal evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chakraborty_S/0/1/0/all/0/1\">Souradeep Chakraborty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Ke Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_R/0/1/0/all/0/1\">Rajarsi Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Knudsen_B/0/1/0/all/0/1\">Beatrice Knudsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zelinsky_G/0/1/0/all/0/1\">Gregory J. Zelinsky</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saltz_J/0/1/0/all/0/1\">Joel H. Saltz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1\">Dimitris Samaras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"V2X-Sim: A Virtual Collaborative Perception Dataset for Autonomous Driving. (arXiv:2202.08449v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08449","description":"<p>Vehicle-to-everything (V2X), which denotes the collaboration between a\nvehicle and any entity in its surrounding, can fundamentally improve the\nperception in self-driving systems. As the individual perception rapidly\nadvances, collaborative perception has made little progress due to the shortage\nof public V2X datasets. In this work, we present the V2X-Sim dataset, the first\npublic large-scale collaborative perception dataset in autonomous driving.\nV2X-Sim provides: 1) well-synchronized recordings from roadside infrastructure\nand multiple vehicles at the intersection to enable collaborative perception,\n2) multi-modality sensor streams to facilitate multi-modality perception, 3)\ndiverse well-annotated ground truth to support various downstream tasks\nincluding detection, tracking, and segmentation. We seek to inspire research on\nmulti-agent multi-modality multi-task perception, and our virtual dataset is\npromising to promote the development of collaborative perception before\nrealistic datasets become widely available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Ziyan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCB Component Detection using Computer Vision for Hardware Assurance. (arXiv:2202.08452v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08452","description":"<p>Printed Circuit Board (PCB) assurance in the optical domain is a crucial\nfield of study. Though there are many existing PCB assurance methods using\nimage processing, computer vision (CV), and machine learning (ML), the PCB\nfield is complex and increasingly evolving so new techniques are required to\novercome the emerging problems. Existing ML-based methods outperform\ntraditional CV methods, however they often require more data, have low\nexplainability, and can be difficult to adapt when a new technology arises. To\novercome these challenges, CV methods can be used in tandem with ML methods. In\nparticular, human-interpretable CV algorithms such as those that extract color,\nshape, and texture features increase PCB assurance explainability. This allows\nfor incorporation of prior knowledge, which effectively reduce the number of\ntrainable ML parameters and thus, the amount of data needed to achieve high\naccuracy when training or retraining an ML model. Hence, this study explores\nthe benefits and limitations of a variety of common computer vision-based\nfeatures for the task of PCB component detection using semantic data. Results\nof this study indicate that color features demonstrate promising performance\nfor PCB component detection. The purpose of this paper is to facilitate\ncollaboration between the hardware assurance, computer vision, and machine\nlearning communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurudu_S/0/1/0/all/0/1\">Suprith Gurudu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taheri_S/0/1/0/all/0/1\">Shayan Taheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shajib Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sathiaseelan_M/0/1/0/all/0/1\">Mukhil Azhagan Mallaiyan Sathiaseelan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadizanjani_N/0/1/0/all/0/1\">Navid Asadizanjani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TraSeTR: Track-to-Segment Transformer with Contrastive Query for Instance-level Instrument Segmentation in Robotic Surgery. (arXiv:2202.08453v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08453","description":"<p>Surgical instrument segmentation -- in general a pixel classification task --\nis fundamentally crucial for promoting cognitive intelligence in robot-assisted\nsurgery (RAS). However, previous methods are struggling with discriminating\ninstrument types and instances. To address the above issues, we explore a mask\nclassification paradigm that produces per-segment predictions. We propose\nTraSeTR, a novel Track-to-Segment Transformer that wisely exploits tracking\ncues to assist surgical instrument segmentation. TraSeTR jointly reasons about\nthe instrument type, location, and identity with instance-level predictions\ni.e., a set of class-bbox-mask pairs, by decoding query embeddings.\nSpecifically, we introduce the prior query that encoded with previous temporal\nknowledge, to transfer tracking signals to current instances via identity\nmatching. A contrastive query learning strategy is further applied to reshape\nthe query feature space, which greatly alleviates the tracking difficulty\ncaused by large temporal variations. The effectiveness of our method is\ndemonstrated with state-of-the-art instrument type segmentation results on\nthree public datasets, including two RAS benchmarks from EndoVis Challenges and\none cataract surgery dataset CaDIS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zixu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and Grasping. (arXiv:2202.08471v1 [cs.RO])","link":"http://arxiv.org/abs/2202.08471","description":"<p>Transparent objects are common in our daily life and frequently handled in\nthe automated production line. Robust vision-based robotic grasping and\nmanipulation for these objects would be beneficial for automation. However, the\nmajority of current grasping algorithms would fail in this case since they\nheavily rely on the depth image, while ordinary depth sensors usually fail to\nproduce accurate depth information for transparent objects owing to the\nreflection and refraction of light. In this work, we address this issue by\ncontributing a large-scale real-world dataset for transparent object depth\ncompletion, which contains 57,715 RGB-D images from 130 different scenes. Our\ndataset is the first large-scale real-world dataset and provides the most\ncomprehensive annotation. Cross-domain experiments show that our dataset has a\ngreat generalization ability. Moreover, we propose an end-to-end depth\ncompletion network, which takes the RGB image and the inaccurate depth map as\ninputs and outputs a refined depth map. Experiments demonstrate superior\nefficacy, efficiency and robustness of our method over previous works, and it\nis able to process images of high resolutions under limited hardware resources.\nReal robot experiment shows that our method can also be applied to novel object\ngrasping robustly. The full dataset and our method are publicly available at\nwww.graspnet.net/transcg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hongjie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1\">Hao-Shu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Object Comprehension: A Framework For Evaluating Artificial Visual Perception. (arXiv:2202.08490v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08490","description":"<p>Augmented and Mixed Reality are emerging as likely successors to the mobile\ninternet. However, many technical challenges remain. One of the key\nrequirements of these systems is the ability to create a continuity between\nphysical and virtual worlds, with the user's visual perception as the primary\ninterface medium. Building this continuity requires the system to develop a\nvisual understanding of the physical world. While there has been significant\nrecent progress in computer vision and AI techniques such as image\nclassification and object detection, success in these areas has not yet led to\nthe visual perception required for these critical MR and AR applications. A\nsignificant issue is that current evaluation criteria are insufficient for\nthese applications. To motivate and evaluate progress in this emerging area,\nthere is a need for new metrics. In this paper we outline limitations of\ncurrent evaluation criteria and propose new criteria.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chin_S/0/1/0/all/0/1\">Scott Y.L. Chin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quinton_B/0/1/0/all/0/1\">Bradley R. Quinton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feels Bad Man: Dissecting Automated Hateful Meme Detection Through the Lens of Facebook's Challenge. (arXiv:2202.08492v1 [cs.CY])","link":"http://arxiv.org/abs/2202.08492","description":"<p>Internet memes have become a dominant method of communication; at the same\ntime, however, they are also increasingly being used to advocate extremism and\nfoster derogatory beliefs. Nonetheless, we do not have a firm understanding as\nto which perceptual aspects of memes cause this phenomenon. In this work, we\nassess the efficacy of current state-of-the-art multimodal machine learning\nmodels toward hateful meme detection, and in particular with respect to their\ngeneralizability across platforms. We use two benchmark datasets comprising\n12,140 and 10,567 images from 4chan's \"Politically Incorrect\" board (/pol/) and\nFacebook's Hateful Memes Challenge dataset to train the competition's\ntop-ranking machine learning models for the discovery of the most prominent\nfeatures that distinguish viral hateful memes from benign ones. We conduct\nthree experiments to determine the importance of multimodality on\nclassification performance, the influential capacity of fringe Web communities\non mainstream social platforms and vice versa, and the models' learning\ntransferability on 4chan memes. Our experiments show that memes' image\ncharacteristics provide a greater wealth of information than its textual\ncontent. We also find that current systems developed for online detection of\nhate speech in memes necessitate further concentration on its visual elements\nto improve their interpretation of underlying cultural connotations, implying\nthat multimodal models fail to adequately grasp the intricacies of hate speech\nin memes and generalize across social media platforms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jennifer_C/0/1/0/all/0/1\">Catherine Jennifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasbi_F/0/1/0/all/0/1\">Fatemeh Tahmasbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blackburn_J/0/1/0/all/0/1\">Jeremy Blackburn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stringhini_G/0/1/0/all/0/1\">Gianluca Stringhini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zannettou_S/0/1/0/all/0/1\">Savvas Zannettou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristofaro_E/0/1/0/all/0/1\">Emiliano De Cristofaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mirror-Yolo: An attention-based instance segmentation and detection model for mirrors. (arXiv:2202.08498v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08498","description":"<p>Mirrors can degrade the performance of computer vision models, however to\naccurately detect mirrors in images remains challenging. YOLOv4 achieves\nphenomenal results both in object detection accuracy and speed, nevertheless\nthe model often fails in detecting mirrors. In this paper, a novel mirror\ndetection method `Mirror-YOLO' is proposed, which mainly targets on mirror\ndetection. Based on YOLOv4, the proposed model embeds an attention mechanism\nfor better feature acquisition, and a hypercolumn-stairstep approach for\nfeature map fusion. Mirror-YOLO can also produce accurate bounding polygons for\ninstance segmentation. The effectiveness of our proposed model is demonstrated\nby our experiments, compared to the existing mirror detection methods, the\nproposed Mirror-YOLO achieves better performance in detection accuracy on the\nmirror image dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fengze Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jieming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhongbei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1\">Ji Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hai-Ning Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yungang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_T/0/1/0/all/0/1\">Tianxi Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLS: Cross Labeling Supervision for Semi-Supervised Learning. (arXiv:2202.08502v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08502","description":"<p>It is well known that the success of deep neural networks is greatly\nattributed to large-scale labeled datasets. However, it can be extremely\ntime-consuming and laborious to collect sufficient high-quality labeled data in\nmost practical applications. Semi-supervised learning (SSL) provides an\neffective solution to reduce the cost of labeling by simultaneously leveraging\nboth labeled and unlabeled data. In this work, we present Cross Labeling\nSupervision (CLS), a framework that generalizes the typical pseudo-labeling\nprocess. Based on FixMatch, where a pseudo label is generated from a\nweakly-augmented sample to teach the prediction on a strong augmentation of the\nsame input sample, CLS allows the creation of both pseudo and complementary\nlabels to support both positive and negative learning. To mitigate the\nconfirmation bias of self-labeling and boost the tolerance to false labels, two\ndifferent initialized networks with the same structure are trained\nsimultaneously. Each network utilizes high-confidence labels from the other\nnetwork as additional supervision signals. During the label generation phase,\nadaptive sample weights are assigned to artificial labels according to their\nprediction confidence. The sample weight plays two roles: quantify the\ngenerated labels' quality and reduce the disruption of inaccurate labels on\nnetwork training. Experimental results on the semi-supervised classification\ntask show that our framework outperforms existing approaches by large margins\non the CIFAR-10 and CIFAR-100 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Junyi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_B/0/1/0/all/0/1\">Bin Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Li Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CSCNet: Contextual Semantic Consistency Network for Trajectory Prediction in Crowded Spaces. (arXiv:2202.08506v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08506","description":"<p>Trajectory prediction aims to predict the movement trend of the agents like\npedestrians, bikers, vehicles. It is helpful to analyze and understand human\nactivities in crowded spaces and widely applied in many areas such as\nsurveillance video analysis and autonomous driving systems. Thanks to the\nsuccess of deep learning, trajectory prediction has made significant progress.\nThe current methods are dedicated to studying the agents' future trajectories\nunder the social interaction and the sceneries' physical constraints. Moreover,\nhow to deal with these factors still catches researchers' attention. However,\nthey ignore the \\textbf{Semantic Shift Phenomenon} when modeling these\ninteractions in various prediction sceneries. There exist several kinds of\nsemantic deviations inner or between social and physical interactions, which we\ncall the \"\\textbf{Gap}\". In this paper, we propose a \\textbf{C}ontextual\n\\textbf{S}emantic \\textbf{C}onsistency \\textbf{Net}work (\\textbf{CSCNet}) to\npredict agents' future activities with powerful and efficient context\nconstraints. We utilize a well-designed context-aware transfer to obtain the\nintermediate representations from the scene images and trajectories. Then we\neliminate the differences between social and physical interactions by aligning\nactivity semantics and scene semantics to cross the Gap. Experiments\ndemonstrate that CSCNet performs better than most of the current methods\nquantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Designing Compact Audio-Visual Wake Word Spotting System Based on Iterative Fine-Tuning in Neural Network Pruning. (arXiv:2202.08509v1 [cs.SD])","link":"http://arxiv.org/abs/2202.08509","description":"<p>Audio-only-based wake word spotting (WWS) is challenging under noisy\nconditions due to environmental interference in signal transmission. In this\npaper, we investigate on designing a compact audio-visual WWS system by\nutilizing visual information to alleviate the degradation. Specifically, in\norder to use visual information, we first encode the detected lips to\nfixed-size vectors with MobileNet and concatenate them with acoustic features\nfollowed by the fusion network for WWS. However, the audio-visual model based\non neural networks requires a large footprint and a high computational\ncomplexity. To meet the application requirements, we introduce a neural network\npruning strategy via the lottery ticket hypothesis in an iterative fine-tuning\nmanner (LTH-IF), to the single-modal and multi-modal models, respectively.\nTested on our in-house corpus for audio-visual WWS in a home TV scene, the\nproposed audio-visual system achieves significant performance improvements over\nthe single-modality (audio-only or video-only) system under different noisy\nconditions. Moreover, LTH-IF pruning can largely reduce the network parameters\nand computations with no degradation of WWS performance, leading to a potential\nproduct solution for the TV wake-up scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hengshun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jun Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_S/0/1/0/all/0/1\">Shifu Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chin-Hui Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A hybrid 2-stage vision transformer for AI-assisted 5 class pathologic diagnosis of gastric endoscopic biopsies. (arXiv:2202.08510v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08510","description":"<p>Gastric endoscopic screening is an effective way to decide appropriate\ngastric cancer (GC) treatment at an early stage, reducing GC-associated\nmortality rate. Although artificial intelligence (AI) has brought a great\npromise to assist pathologist to screen digitalized whole slide images,\nautomatic classification systems for guiding proper GC treatment based on\nclinical guideline are still lacking. Here, we propose an AI system classifying\n5 classes of GC histology, which can be perfectly matched to general treatment\nguidance. The AI system, mimicking the way pathologist understand slides\nthrough multi-scale self-attention mechanism using a 2-stage Vision\nTransformer, demonstrates clinical capability by achieving diagnostic\nsensitivity of above 85% for both internal and external cohort analysis.\nFurthermore, AI-assisted pathologists showed significantly improved diagnostic\nsensitivity by 10% within 18% saved screening time compared to human\npathologists. Our AI system has a great potential for providing presumptive\npathologic opinion for deciding proper treatment for early GC patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oh_Y/0/1/0/all/0/1\">Yujin Oh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_G/0/1/0/all/0/1\">Go Eun Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Hee Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yeo_M/0/1/0/all/0/1\">Min-Kyung Yeo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Ground Truth Construction as Faceted Classification. (arXiv:2202.08512v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08512","description":"<p>Recent work in Machine Learning and Computer Vision has provided evidence of\nsystematic design flaws in the development of major object recognition\nbenchmark datasets. One such example is ImageNet, wherein, for several\ncategories of images, there are incongruences between the objects they\nrepresent and the labels used to annotate them. The consequences of this\nproblem are major, in particular considering the large number of machine\nlearning applications, not least those based on Deep Neural Networks, that have\nbeen trained on these datasets. In this paper we posit the problem to be the\nlack of a knowledge representation (KR) methodology providing the foundations\nfor the construction of these ground truth benchmark datasets. Accordingly, we\npropose a solution articulated in three main steps: (i) deconstructing the\nobject recognition process in four ordered stages grounded in the philosophical\ntheory of teleosemantics; (ii) based on such stratification, proposing a novel\nfour-phased methodology for organizing objects in classification hierarchies\naccording to their visual properties; and (iii) performing such classification\naccording to the faceted classification paradigm. The key novelty of our\napproach lies in the fact that we construct the classification hierarchies from\nvisual properties exploiting visual genus-differentiae, and not from\nlinguistically grounded properties. The proposed approach is validated by a set\nof experiments on the ImageNet hierarchy of musical experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1\">Fausto Giunchiglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1\">Mayukh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1\">Xiaolei Diao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Survey on Self-supervised Representation Learning Using Image Transformations. (arXiv:2202.08514v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08514","description":"<p>Deep neural networks need huge amount of training data, while in real world\nthere is a scarcity of data available for training purposes. To resolve these\nissues, self-supervised learning (SSL) methods are used. SSL using geometric\ntransformations (GT) is a simple yet powerful technique used in unsupervised\nrepresentation learning. Although multiple survey papers have reviewed SSL\ntechniques, there is none that only focuses on those that use geometric\ntransformations. Furthermore, such methods have not been covered in depth in\npapers where they are reviewed. Our motivation to present this work is that\ngeometric transformations have shown to be powerful supervisory signals in\nunsupervised representation learning. Moreover, many such works have found\ntremendous success, but have not gained much attention. We present a concise\nsurvey of SSL approaches that use geometric transformations. We shortlist six\nrepresentative models that use image transformations including those based on\npredicting and autoencoding transformations. We review their architecture as\nwell as learning methodologies. We also compare the performance of these models\nin the object recognition task on CIFAR-10 and ImageNet datasets. Our analysis\nindicates the AETv2 performs the best in most settings. Rotation with feature\ndecoupling also performed well in some settings. We then derive insights from\nthe observed results. Finally, we conclude with a summary of the results and\ninsights as well as highlighting open problems to be addressed and indicating\nvarious future directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1\">Muhammad Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashim_S/0/1/0/all/0/1\">Sayed Hashim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TAFNet: A Three-Stream Adaptive Fusion Network for RGB-T Crowd Counting. (arXiv:2202.08517v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08517","description":"<p>In this paper, we propose a three-stream adaptive fusion network named\nTAFNet, which uses paired RGB and thermal images for crowd counting.\nSpecifically, TAFNet is divided into one main stream and two auxiliary streams.\nWe combine a pair of RGB and thermal images to constitute the input of main\nstream. Two auxiliary streams respectively exploit RGB image and thermal image\nto extract modality-specific features. Besides, we propose an Information\nImprovement Module (IIM) to fuse the modality-specific features into the main\nstream adaptively. Experiment results on RGBT-CC dataset show that our method\nachieves more than 20% improvement on mean average error and root mean squared\nerror compared with state-of-the-art method. The source code will be publicly\navailable at https://github.com/TANGHAIHAN/TAFNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haihan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1\">Lap-Pui Chau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point Cloud Generation with Continuous Conditioning. (arXiv:2202.08526v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08526","description":"<p>Generative models can be used to synthesize 3D objects of high quality and\ndiversity. However, there is typically no control over the properties of the\ngenerated object.This paper proposes a novel generative adversarial network\n(GAN) setup that generates 3D point cloud shapes conditioned on a continuous\nparameter. In an exemplary application, we use this to guide the generative\nprocess to create a 3D object with a custom-fit shape. We formulate this\ngeneration process in a multi-task setting by using the concept of auxiliary\nclassifier GANs. Further, we propose to sample the generator label input for\ntraining from a kernel density estimation (KDE) of the dataset. Our ablations\nshow that this leads to significant performance increase in regions with few\nsamples. Extensive quantitative and qualitative experiments show that we gain\nexplicit control over the object dimensions while maintaining good generation\nquality and diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1\">Larissa T. Triess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhler_A/0/1/0/all/0/1\">Andre B&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peter_D/0/1/0/all/0/1\">David Peter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flohr_F/0/1/0/all/0/1\">Fabian B. Flohr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1\">J. Marius Z&#xf6;llner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation for Underwater Image Enhancement via Content and Style Separation. (arXiv:2202.08537v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08537","description":"<p>Underwater image suffer from color cast, low contrast and hazy effect due to\nlight absorption, refraction and scattering, which degraded the high-level\napplication, e.g, object detection and object tracking. Recent learning-based\nmethods demonstrate astonishing performance on underwater image enhancement,\nhowever, most of these works use synthesis pair data for supervised learning\nand ignore the domain gap to real-world data. In this paper, we propose a\ndomain adaptation framework for underwater image enhancement via content and\nstyle separation, we assume image could be disentangled to content and style\nlatent, and image could be clustered to the sub-domain of associated style in\nlatent space, the goal is to build up the mapping between underwater style\nlatent and clean one. Different from prior works of domain adaptation for\nunderwater image enhancement, which target to minimize the latent discrepancy\nof synthesis and real-world data, we aim to distinguish style latent from\ndifferent sub-domains. To solve the problem of lacking pair real-world data, we\nleverage synthesis to real image-to-image translation to obtain pseudo real\nunderwater image pairs for supervised learning, and enhancement can be achieved\nby input content and clean style latent into generator. Our model provide a\nuser interact interface to adjust different enhanced level by latent\nmanipulation. Experiment on various public real-world underwater benchmarks\ndemonstrate that the proposed framework is capable to perform domain adaptation\nfor underwater image enhancement and outperform various state-of-the-art\nunderwater image enhancement algorithms in quantity and quality. The model and\nsource code are available at https://github.com/fordevoted/UIESS\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1\">Soo-Chang Pei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An overview of deep learning in medical imaging. (arXiv:2202.08546v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08546","description":"<p>Machine learning (ML) has seen enormous consideration during the most recent\ndecade. This success started in 2012 when an ML model accomplished a remarkable\ntriumph in the ImageNet Classification, the world's most famous competition for\ncomputer vision. This model was a kind of convolutional neural system (CNN)\ncalled deep learning (DL). Since then, researchers have started to participate\nefficiently in DL's fastest developing area of research. These days, DL systems\nare cutting-edge ML systems spanning a broad range of disciplines, from human\nlanguage processing to video analysis, and commonly used in the scholarly world\nand enterprise sector. Recent advances can bring tremendous improvement to the\nmedical field. Improved and innovative methods for data processing, image\nanalysis and can significantly improve the diagnostic technologies and\nmedicinal services gradually. A quick review of current developments with\nrelevant problems in the field of DL used for medical imaging has been\nprovided. The primary purposes of the review are four: (i) provide a brief\nprolog to DL by discussing different DL models, (ii) review of the DL usage for\nmedical image analysis (classification, detection, segmentation, and\nregistration), (iii) review seven main application fields of DL in medical\nimaging, (iv) give an initial stage to those keen on adding to the research\narea about DL in clinical imaging by providing links of some useful informative\nassets, such as freely available DL codes, public datasets Table 7, and medical\nimaging competition sources Table 8 and end our survey by outlining distinct\ncontinuous difficulties, lessons learned and future of DL in the field of\nmedical science.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Haq_I/0/1/0/all/0/1\">Imran Ul Haq</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EBHI:A New Enteroscope Biopsy Histopathological H&E Image Dataset for Image Classification Evaluation. (arXiv:2202.08552v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08552","description":"<p>Background and purpose: Colorectal cancer has become the third most common\ncancer worldwide, accounting for approximately 10% of cancer patients. Early\ndetection of the disease is important for the treatment of colorectal cancer\npatients. Histopathological examination is the gold standard for screening\ncolorectal cancer. However, the current lack of histopathological image\ndatasets of colorectal cancer, especially enteroscope biopsies, hinders the\naccurate evaluation of computer-aided diagnosis techniques. Methods: A new\npublicly available Enteroscope Biopsy Histopathological H&amp;E Image Dataset\n(EBHI) is published in this paper. To demonstrate the effectiveness of the EBHI\ndataset, we have utilized several machine learning, convolutional neural\nnetworks and novel transformer-based classifiers for experimentation and\nevaluation, using an image with a magnification of 200x. Results: Experimental\nresults show that the deep learning method performs well on the EBHI dataset.\nTraditional machine learning methods achieve maximum accuracy of 76.02% and\ndeep learning method achieves a maximum accuracy of 95.37%. Conclusion: To the\nbest of our knowledge, EBHI is the first publicly available colorectal\nhistopathology enteroscope biopsy dataset with four magnifications and five\ntypes of images of tumor differentiation stages, totaling 5532 images. We\nbelieve that EBHI could attract researchers to explore new classification\nalgorithms for the automated diagnosis of colorectal cancer, which could help\nphysicians and patients in clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Mamunur Rahaman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1\">Yudong Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_N/0/1/0/all/0/1\">Ning Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzegorze_M/0/1/0/all/0/1\">Marcin Grzegorze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-Aware Indoor Scene Synthesis with Depth Priors. (arXiv:2202.08553v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08553","description":"<p>Despite the recent advancement of Generative Adversarial Networks (GANs) in\nlearning 3D-aware image synthesis from 2D data, existing methods fail to model\nindoor scenes due to the large diversity of room layouts and the objects\ninside. We argue that indoor scenes do not have a shared intrinsic structure,\nand hence only using 2D images cannot adequately guide the model with the 3D\ngeometry. In this work, we fill in this gap by introducing depth as a 3D prior.\nCompared with other 3D data formats, depth better fits the convolution-based\ngeneration mechanism and is more easily accessible in practice. Specifically,\nwe propose a dual-path generator, where one path is responsible for depth\ngeneration, whose intermediate features are injected into the other path as the\ncondition for appearance rendering. Such a design eases the 3D-aware synthesis\nwith explicit geometry information. Meanwhile, we introduce a switchable\ndiscriminator both to differentiate real v.s. fake domains and to predict the\ndepth from a given input. In this way, the discriminator can take the spatial\narrangement into account and advise the generator to learn an appropriate depth\ncondition. Extensive experimental results suggest that our approach is capable\nof synthesizing indoor scenes with impressively good quality and 3D\nconsistency, significantly outperforming state-of-the-art alternatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zifan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yujun Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiapeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1\">Dit-Yan Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CADRE: A Cascade Deep Reinforcement Learning Framework for Vision-based Autonomous Urban Driving. (arXiv:2202.08557v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08557","description":"<p>Vision-based autonomous urban driving in dense traffic is quite challenging\ndue to the complicated urban environment and the dynamics of the driving\nbehaviors. Widely-applied methods either heavily rely on hand-crafted rules or\nlearn from limited human experience, which makes them hard to generalize to\nrare but critical scenarios. In this paper, we present a novel CAscade Deep\nREinforcement learning framework, CADRE, to achieve model-free vision-based\nautonomous urban driving. In CADRE, to derive representative latent features\nfrom raw observations, we first offline train a Co-attention Perception Module\n(CoPM) that leverages the co-attention mechanism to learn the\ninter-relationships between the visual and control information from a\npre-collected driving dataset. Cascaded by the frozen CoPM, we then present an\nefficient distributed proximal policy optimization framework to online learn\nthe driving policy under the guidance of particularly designed reward\nfunctions. We perform a comprehensive empirical study with the CARLA NoCrash\nbenchmark as well as specific obstacle avoidance scenarios in autonomous urban\ndriving tasks. The experimental results well justify the effectiveness of CADRE\nand its superiority over the state-of-the-art by a wide margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yinuo Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1\">Zhengping Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1\">Qi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Harold Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anatomically Parameterized Statistical Shape Model: Explaining Morphometry through Statistical Learning. (arXiv:2202.08580v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08580","description":"<p>Statistical shape models (SSMs) are a popular tool to conduct morphological\nanalysis of anatomical structures which is a crucial step in clinical\npractices. However, shape representations through SSMs are based on shape\ncoefficients and lack an explicit one-to-one relationship with anatomical\nmeasures of clinical relevance. While a shape coefficient embeds a combination\nof anatomical measures, a formalized approach to find the relationship between\nthem remains elusive in the literature. This limits the use of SSMs to\nsubjective evaluations in clinical practices. We propose a novel SSM controlled\nby anatomical parameters derived from morphometric analysis. The proposed\nanatomically parameterized SSM (ANAT-SSM) is based on learning a linear mapping\nbetween shape coefficients and selected anatomical parameters. This mapping is\nlearned from a synthetic population generated by the standard SSM. Determining\nthe pseudo-inverse of the mapping allows us to build the ANAT-SSM. We further\nimpose orthogonality constraints to the anatomical parameterization to obtain\nindependent shape variation patterns. The proposed contribution was evaluated\non two skeletal databases of femoral and scapular bone shapes using clinically\nrelevant anatomical parameters. Anatomical measures of the synthetically\ngenerated shapes exhibited realistic statistics. The learned matrices\ncorroborated well with the obtained statistical relationship, while the two\nSSMs achieved moderate to excellent performance in predicting anatomical\nparameters on unseen shapes. This study demonstrates the use of anatomical\nrepresentation for creating anatomically parameterized SSM and as a result,\nremoves the limited clinical interpretability of standard SSMs. The proposed\nmodels could help analyze differences in relevant bone morphometry between\npopulations, and be integrated in patient-specific pre-surgery planning or\nin-surgery assessment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Boutillon_A/0/1/0/all/0/1\">Arnaud Boutillon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salhi_A/0/1/0/all/0/1\">Asma Salhi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burdin_V/0/1/0/all/0/1\">Val&#xe9;rie Burdin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borotikar_B/0/1/0/all/0/1\">Bhushan Borotikar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point cloud completion on structured feature map with feedback network. (arXiv:2202.08583v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08583","description":"<p>In this paper, we tackle the challenging problem of point cloud completion\nfrom the perspective of feature learning. Our key observation is that to\nrecover the underlying structures as well as surface details given a partial\ninput, a fundamental component is a good feature representation that can\ncapture both global structure and local geometric details. Towards this end, we\nfirst propose FSNet, a feature structuring module that can adaptively aggregate\npoint-wise features into a 2D structured feature map by learning multiple\nlatent patterns from local regions. We then integrate FSNet into a\ncoarse-to-fine pipeline for point cloud completion. Specifically, a 2D\nconvolutional neural network is adopted to decode feature maps from FSNet into\na coarse and complete point cloud. Next, a point cloud upsampling network is\nused to generate dense point cloud from the partial input and the coarse\nintermediate output. To efficiently exploit the local structures and enhance\nthe point distribution uniformity, we propose IFNet, a point upsampling module\nwith self-correction mechanism that can progressively refine details of the\ngenerated dense point cloud. We conduct both qualitative and quantitative\nexperiments on ShapeNet, MVP, and KITTI datasets, which demonstrate that our\nmethod outperforms state-of-the-art point cloud completion approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zejia Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haibin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chongyang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruizhen Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single UHD Image Dehazing via Interpretable Pyramid Network. (arXiv:2202.08589v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08589","description":"<p>Currently, most single image dehazing models cannot run an\nultra-high-resolution (UHD) image with a single GPU shader in real-time. To\naddress the problem, we introduce the principle of infinite approximation of\nTaylor's theorem with the Laplace pyramid pattern to build a model which is\ncapable of handling 4K hazy images in real-time. The N branch networks of the\npyramid network correspond to the N constraint terms in Taylor's theorem.\nLow-order polynomials reconstruct the low-frequency information of the image\n(e.g. color, illumination). High-order polynomials regress the high-frequency\ninformation of the image (e.g. texture). In addition, we propose a Tucker\nreconstruction-based regularization term that acts on each branch network of\nthe pyramid model. It further constrains the generation of anomalous signals in\nthe feature space. Extensive experimental results demonstrate that our approach\ncan not only run 4K images with haze in real-time on a single GPU (80FPS) but\nalso has unparalleled interpretability.\n</p>\n<p>The developed method achieves state-of-the-art (SOTA) performance on two\nbenchmarks (O/I-HAZE) and our updated 4KID dataset while providing the reliable\ngroundwork for subsequent optimization schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Boxue Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuoran Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1\">Chen Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yunliang Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-Stage Architectural Fine-Tuning with Neural Architecture Search using Early-Stopping in Image Classification. (arXiv:2202.08604v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08604","description":"<p>Deep neural networks (NN) perform well in various tasks (e.g., computer\nvision) because of the convolutional neural networks (CNN). However, the\ndifficulty of gathering quality data in the industry field hinders the\npractical use of NN. To cope with this issue, the concept of transfer learning\n(TL) has emerged, which leverages the fine-tuning of NNs trained on large-scale\ndatasets in data-scarce situations. Therefore, this paper suggests a two-stage\narchitectural fine-tuning method for image classification, inspired by the\nconcept of neural architecture search (NAS). One of the main ideas of our\nproposed method is a mutation with base architectures, which reduces the search\ncost by using given architectural information. Moreover, an early-stopping is\nalso considered which directly reduces NAS costs. Experimental results verify\nthat our proposed method reduces computational and searching costs by up to\n28.2% and 22.3%, compared to existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngkee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_W/0/1/0/all/0/1\">Won Joon Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youn Kyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongheon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fourier PlenOctrees for Dynamic Radiance Field Rendering in Real-time. (arXiv:2202.08614v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08614","description":"<p>Implicit neural representations such as Neural Radiance Field (NeRF) have\nfocused mainly on modeling static objects captured under multi-view settings\nwhere real-time rendering can be achieved with smart data structures, e.g.,\nPlenOctree. In this paper, we present a novel Fourier PlenOctree (FPO)\ntechnique to tackle efficient neural modeling and real-time rendering of\ndynamic scenes captured under the free-view video (FVV) setting. The key idea\nin our FPO is a novel combination of generalized NeRF, PlenOctree\nrepresentation, volumetric fusion and Fourier transform. To accelerate FPO\nconstruction, we present a novel coarse-to-fine fusion scheme that leverages\nthe generalizable NeRF technique to generate the tree via spatial blending. To\ntackle dynamic scenes, we tailor the implicit network to model the Fourier\ncoefficients of timevarying density and color attributes. Finally, we construct\nthe FPO and train the Fourier coefficients directly on the leaves of a union\nPlenOctree structure of the dynamic sequence. We show that the resulting FPO\nenables compact memory overload to handle dynamic objects and supports\nefficient fine-tuning. Extensive experiments show that the proposed method is\n3000 times faster than the original NeRF and achieves over an order of\nmagnitude acceleration over SOTA while preserving high visual quality for the\nfree-viewpoint rendering of unseen dynamic scenes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiakai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xinhang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fuqiang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanshun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yingliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Minye Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantically Proportional Patchmix for Few-Shot Learning. (arXiv:2202.08647v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08647","description":"<p>Few-shot learning aims to classify unseen classes with only a limited number\nof labeled data. Recent works have demonstrated that training models with a\nsimple transfer learning strategy can achieve competitive results in few-shot\nclassification. Although excelling at distinguishing training data, these\nmodels are not well generalized to unseen data, probably due to insufficient\nfeature representations on evaluation. To tackle this issue, we propose\nSemantically Proportional Patchmix (SePPMix), in which patches are cut and\npasted among training images and the ground truth labels are mixed\nproportionally to the semantic information of the patches. In this way, we can\nimprove the generalization ability of the model by regional dropout effect\nwithout introducing severe label noise. To learn more robust representations of\ndata, we further take rotate transformation on the mixed images and predict\nrotations as a rule-based regularizer. Extensive experiments on prevalent\nfew-shot benchmarks have shown the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingquan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1\">Yu Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Randomization for Object Counting. (arXiv:2202.08670v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08670","description":"<p>Recently, the use of synthetic datasets based on game engines has been shown\nto improve the performance of several tasks in computer vision. However, these\ndatasets are typically only appropriate for the specific domains depicted in\ncomputer games, such as urban scenes involving vehicles and people. In this\npaper, we present an approach to generate synthetic datasets for object\ncounting for any domain without the need for photo-realistic techniques\nmanually generated by expensive teams of 3D artists. We introduce a domain\nrandomization approach for object counting based on synthetic datasets that are\nquick and inexpensive to generate. We deliberately avoid photorealism and\ndrastically increase the variability of the dataset, producing images with\nrandom textures and 3D transformations, which improves generalization.\nExperiments show that our method facilitates good performance on various real\nword object counting datasets for multiple domains: people, vehicles, penguins,\nand fruit. The source code is available at: https://github.com/enric1994/dr4oc\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreu_E/0/1/0/all/0/1\">Enric Moreu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortego_D/0/1/0/all/0/1\">Diego Ortego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic data for unsupervised polyp segmentation. (arXiv:2202.08680v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08680","description":"<p>Deep learning has shown excellent performance in analysing medical images.\nHowever, datasets are difficult to obtain due privacy issues, standardization\nproblems, and lack of annotations. We address these problems by producing\nrealistic synthetic images using a combination of 3D technologies and\ngenerative adversarial networks. We use zero annotations from medical\nprofessionals in our pipeline. Our fully unsupervised method achieves promising\nresults on five real polyp segmentation datasets. As a part of this study we\nrelease Synth-Colon, an entirely synthetic dataset that includes 20000\nrealistic colon images and additional details about depth and 3D geometry:\nhttps://enric1994.github.io/synth-colon\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Moreu_E/0/1/0/all/0/1\">Enric Moreu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McGuinness_K/0/1/0/all/0/1\">Kevin McGuinness</a>, <a href=\"http://arxiv.org/find/eess/1/au:+OConnor_N/0/1/0/all/0/1\">Noel E. O&#x27;Connor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Neuron Instance Segmentation based on Weakly Supervised Efficient UNet and Morphological Post-processing. (arXiv:2202.08682v1 [eess.IV])","link":"http://arxiv.org/abs/2202.08682","description":"<p>Recent studies have demonstrated the superiority of deep learning in medical\nimage analysis, especially in cell instance segmentation, a fundamental step\nfor many biological studies. However, the good performance of the neural\nnetworks requires training on large unbiased dataset and annotations, which is\nlabor-intensive and expertise-demanding. In this paper, we present an\nend-to-end weakly-supervised framework to automatically detect and segment NeuN\nstained neuronal cells on histological images using only point annotations. We\nintegrate the state-of-the-art network, EfficientNet, into our U-Net-like\narchitecture. Validation results show the superiority of our model compared to\nother recent methods. In addition, we investigated multiple post-processing\nschemes and proposed an original strategy to convert the probability map into\nsegmented instances using ultimate erosion and dynamic reconstruction. This\napproach is easy to configure and outperforms other classical post-processing\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_H/0/1/0/all/0/1\">Huaqian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Souedet_N/0/1/0/all/0/1\">Nicolas Souedet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jan_C/0/1/0/all/0/1\">Caroline Jan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Clouchoux_C/0/1/0/all/0/1\">C&#xe9;dric Clouchoux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delzescaux_T/0/1/0/all/0/1\">Thierry Delzescaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A study of deep perceptual metrics for image quality assessment. (arXiv:2202.08692v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08692","description":"<p>Several metrics exist to quantify the similarity between images, but they are\ninefficient when it comes to measure the similarity of highly distorted images.\nIn this work, we propose to empirically investigate perceptual metrics based on\ndeep neural networks for tackling the Image Quality Assessment (IQA) task. We\nstudy deep perceptual metrics according to different hyperparameters like the\nnetwork's architecture or training procedure. Finally, we propose our\nmulti-resolution perceptual metric (MR-Perceptual), that allows us to aggregate\nperceptual information at different resolutions and outperforms standard\nperceptual metrics on IQA tasks with varying image deformations. Our code is\navailable at https://github.com/ENSTA-U2IS/MR_perceptual\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kazmierczak_R/0/1/0/all/0/1\">R&#xe9;mi Kazmierczak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1\">Gianni Franchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belkhir_N/0/1/0/all/0/1\">Nacim Belkhir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzanera_A/0/1/0/all/0/1\">Antoine Manzanera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filliat_D/0/1/0/all/0/1\">David Filliat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting and Learning the Unknown in Semantic Segmentation. (arXiv:2202.08700v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08700","description":"<p>Semantic segmentation is a crucial component for perception in automated\ndriving. Deep neural networks (DNNs) are commonly used for this task and they\nare usually trained on a closed set of object classes appearing in a closed\noperational domain. However, this is in contrast to the open world assumption\nin automated driving that DNNs are deployed to. Therefore, DNNs necessarily\nface data that they have never encountered previously, also known as anomalies,\nwhich are extremely safety-critical to properly cope with. In this work, we\nfirst give an overview about anomalies from an information-theoretic\nperspective. Next, we review research in detecting semantically unknown objects\nin semantic segmentation. We demonstrate that training for high entropy\nresponses on anomalous objects outperforms other recent methods, which is in\nline with our theoretical findings. Moreover, we examine a method to assess the\noccurrence frequency of anomalies in order to select anomaly types to include\ninto a model's set of semantic categories. We demonstrate that these anomalies\ncan then be learned in an unsupervised fashion, which is particularly suitable\nin online applications based on deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Robin Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhlemeyer_S/0/1/0/all/0/1\">Svenja Uhlemeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Level set based particle filter driven by optical flow: an application to track the salt boundary from X-ray CT time-series. (arXiv:2202.08717v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08717","description":"<p>Image-based computational fluid dynamics have long played an important role\nin leveraging knowledge and understanding of several physical phenomena. In\nparticular, probabilistic computational methods have opened the way to\nmodelling the complex dynamics of systems in purely random turbulent motion. In\nthe field of structural geology, a better understanding of the deformation and\nstress state both within the salt and the surrounding rocks is of great\ninterest to characterize all kinds of subsurface long-terms energy-storage\nsystems. The objective of this research is to determine the non-linear\ndeformation of the salt boundary over time using a parallelized, stochastic\nfiltering approach from x-ray computed tomography (CT) image time series\ndepicting the evolution of salt structures triggered by gravity and under\ndifferential loading. This work represents a first step towards bringing\ntogether physical modeling and advanced stochastic image processing methods\nwhere model uncertainty is taken into account.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makki_K/0/1/0/all/0/1\">Karim Makki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecomte_J/0/1/0/all/0/1\">Jean Fran&#xe7;ois Lecomte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuchs_L/0/1/0/all/0/1\">Lukas Fuchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schueller_S/0/1/0/all/0/1\">Sylvie Schueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Memin_E/0/1/0/all/0/1\">Etienne M&#xe9;min</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Colonoscopy polyp detection with massive endoscopic images. (arXiv:2202.08730v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08730","description":"<p>We improved an existing end-to-end polyp detection model with better average\nprecision validated by different data sets with trivial cost on detection\nspeed. Previous work on detecting polyps within colonoscopy \\cite{Chen2018}\nprovided an efficient end-to-end solution to alleviate doctor's examination\noverhead. However, our later experiments found this framework is not as robust\nas before as the condition of polyp capturing varies. In this work, we\nconducted several studies on data set, identifying main issues that causes low\nprecision rate in the task of polyp detection. We used an optimized anchor\ngeneration methods to get better anchor box shape and more boxes are used for\ndetection as we believe this is necessary for small object detection. A\nalternative backbone is used to compensate the heavy time cost introduced by\ndense anchor box regression. With use of the attention gate module, our model\ncan achieve state-of-the-art polyp detection performance while still maintain\nreal-time detection speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jialin Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huogen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Ming Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniSyn: Synthesizing 360 Videos with Wide-baseline Panoramas. (arXiv:2202.08752v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08752","description":"<p>Immersive maps such as Google Street View and Bing Streetside provide\ntrue-to-life views with a massive collection of panoramas. However, these\npanoramas are only available at sparse intervals along the path they are taken,\nresulting in visual discontinuities during navigation. Prior art in view\nsynthesis is usually built upon a set of perspective images, a pair of\nstereoscopic images, or a monocular image, but barely examines wide-baseline\npanoramas, which are widely adopted in commercial platforms to optimize\nbandwidth and storage usage. In this paper, we leverage the unique\ncharacteristics of wide-baseline panoramas and present OmniSyn, a novel\npipeline for 360{\\deg} view synthesis between wide-baseline panoramas. OmniSyn\npredicts omnidirectional depth maps using a spherical cost volume and a\nmonocular skip connection, renders meshes in 360{\\deg} images, and synthesizes\nintermediate views with a fusion network. We demonstrate the effectiveness of\nOmniSyn via comprehensive experimental results including comparison with the\nstate-of-the-art methods on CARLA and Matterport datasets, ablation studies,\nand generalization studies on street views. We envision our work may inspire\nfuture research for this unheeded real-world task and eventually produce a\nsmoother experience for navigating immersive maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">David Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinda Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hane_C/0/1/0/all/0/1\">Christian H&#xe4;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Danhang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varshney_A/0/1/0/all/0/1\">Amitabh Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1\">Ruofei Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Wavelet-based Dual-stream Network for Underwater Image Enhancement. (arXiv:2202.08758v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08758","description":"<p>We present a wavelet-based dual-stream network that addresses color cast and\nblurry details in underwater images. We handle these artifacts separately by\ndecomposing an input image into multiple frequency bands using discrete wavelet\ntransform, which generates the downsampled structure image and detail images.\nThese sub-band images are used as input to our dual-stream network that\nincorporates two sub-networks: the multi-color space fusion network and the\ndetail enhancement network. The multi-color space fusion network takes the\ndecomposed structure image as input and estimates the color corrected output by\nemploying the feature representations from diverse color spaces of the input.\nThe detail enhancement network addresses the blurriness of the original\nunderwater image by improving the image details from high-frequency sub-bands.\nWe validate the proposed method on both real-world and synthetic underwater\ndatasets and show the effectiveness of our model in color correction and blur\nremoval with low computational complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziyin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1\">Changjae Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realistic Blur Synthesis for Learning Image Deblurring. (arXiv:2202.08771v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08771","description":"<p>Training learning-based deblurring methods demands a significant amount of\nblurred and sharp image pairs. Unfortunately, existing synthetic datasets are\nnot realistic enough, and existing real-world blur datasets provide limited\ndiversity of scenes and camera settings. As a result, deblurring models trained\non them still suffer from the lack of generalization ability for handling real\nblurred images. In this paper, we analyze various factors that introduce\ndifferences between real and synthetic blurred images, and present a novel blur\nsynthesis pipeline that can synthesize more realistic blur. We also present\nRSBlur, a novel dataset that contains real blurred images and the corresponding\nsequences of sharp images. The RSBlur dataset can be used for generating\nsynthetic blurred images to enable detailed analysis on the differences between\nreal and synthetic blur. With our blur synthesis pipeline and RSBlur dataset,\nwe reveal the effects of different factors in the blur synthesis. We also show\nthat our synthesis method can improve the deblurring performance on real\nblurred images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rim_J/0/1/0/all/0/1\">Jaesung Rim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geonung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jungeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seungyong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sunghyun Cho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar-Based Grounded Lexicon Learning. (arXiv:2202.08806v1 [cs.CL])","link":"http://arxiv.org/abs/2202.08806","description":"<p>We present Grammar-Based Grounded Lexicon Learning (G2L2), a lexicalist\napproach toward learning a compositional and grounded meaning representation of\nlanguage from grounded data, such as paired images and texts. At the core of\nG2L2 is a collection of lexicon entries, which map each word to a tuple of a\nsyntactic type and a neuro-symbolic semantic program. For example, the word\nshiny has a syntactic type of adjective; its neuro-symbolic semantic program\nhas the symbolic form {\\lambda}x. filter(x, SHINY), where the concept SHINY is\nassociated with a neural network embedding, which will be used to classify\nshiny objects. Given an input sentence, G2L2 first looks up the lexicon entries\nassociated with each token. It then derives the meaning of the sentence as an\nexecutable neuro-symbolic program by composing lexical meanings based on\nsyntax. The recovered meaning programs can be executed on grounded inputs. To\nfacilitate learning in an exponentially-growing compositional space, we\nintroduce a joint parsing and expected execution algorithm, which does local\nmarginalization over derivations to reduce the training time. We evaluate G2L2\non two domains: visual reasoning and language-driven navigation. Results show\nthat G2L2 can generalize from small amounts of data to novel compositions of\nwords.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiayuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoyue Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiajun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger P. Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1\">Joshua B. Tenenbaum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Cyclical Training of Neural Networks. (arXiv:2202.08835v1 [cs.LG])","link":"http://arxiv.org/abs/2202.08835","description":"<p>This paper describes the principle of \"General Cyclical Training\" in machine\nlearning, where training starts and ends with \"easy training\" and the \"hard\ntraining\" happens during the middle epochs. We propose several manifestations\nfor training neural networks, including algorithmic examples (via\nhyper-parameters and loss functions), data-based examples, and model-based\nexamples. Specifically, we introduce several novel techniques: cyclical weight\ndecay, cyclical batch size, cyclical focal loss, cyclical softmax temperature,\ncyclical data augmentation, cyclical gradient clipping, and cyclical\nsemi-supervised learning. In addition, we demonstrate that cyclical weight\ndecay, cyclical softmax temperature, and cyclical gradient clipping (as three\nexamples of this principle) are beneficial in the test accuracy performance of\na trained model. Furthermore, we discuss model-based examples (such as\npretraining and knowledge distillation) from the perspective of general\ncyclical training and recommend some changes to the typical training\nmethodology. In summary, this paper defines the general cyclical training\nconcept and discusses several specific ways in which this concept can be\napplied to training neural networks. In the spirit of reproducibility, the code\nused in our experiments is available at \\url{https://github.com/lnsmith54/CFL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1\">Leslie N. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adiabatic Quantum Computing for Multi Object Tracking. (arXiv:2202.08837v1 [cs.CV])","link":"http://arxiv.org/abs/2202.08837","description":"<p>Multi-Object Tracking (MOT) is most often approached in the\ntracking-by-detection paradigm, where object detections are associated through\ntime. The association step naturally leads to discrete optimization problems.\nAs these optimization problems are often NP-hard, they can only be solved\nexactly for small instances on current hardware. Adiabatic quantum computing\n(AQC) offers a solution for this, as it has the potential to provide a\nconsiderable speedup on a range of NP-hard optimization problems in the near\nfuture. However, current MOT formulations are unsuitable for quantum computing\ndue to their scaling properties. In this work, we therefore propose the first\nMOT formulation designed to be solved with AQC. We employ an Ising model that\nrepresents the quantum mechanical system implemented on the AQC. We show that\nour approach is competitive compared with state-of-the-art optimization-based\napproaches, even when using of-the-shelf integer programming solvers. Finally,\nwe demonstrate that our MOT problem is already solvable on the current\ngeneration of real quantum computers for small examples, and analyze the\nproperties of the measured solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaech_J/0/1/0/all/0/1\">Jan-Nico Zaech</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liniger_A/0/1/0/all/0/1\">Alexander Liniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danelljan_M/0/1/0/all/0/1\">Martin Danelljan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1\">Dengxin Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MRI Reconstruction Using Deep Bayesian Estimation. (arXiv:1909.01127v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1909.01127","description":"<p>Purpose: To develop a deep learning-based Bayesian inference for MRI\nreconstruction. Methods: We modeled the MRI reconstruction problem with Bayes's\ntheorem, following the recently proposed PixelCNN++ method. The image\nreconstruction from incomplete k-space measurement was obtained by maximizing\nthe posterior possibility. A generative network was utilized as the image\nprior, which was computationally tractable, and the k-space data fidelity was\nenforced by using an equality constraint. The stochastic backpropagation was\nutilized to calculate the descent gradient in the process of maximum a\nposterior, and a projected subgradient method was used to impose the equality\nconstraint. In contrast to the other deep learning reconstruction methods, the\nproposed one used the likelihood of prior as the training loss and the\nobjective function in reconstruction to improve the image quality. Results: The\nproposed method showed an improved performance in preserving image details and\nreducing aliasing artifacts, compared with GRAPPA, $\\ell_1$-ESPRiT, and MODL, a\nstate-of-the-art deep learning reconstruction method. The proposed method\ngenerally achieved more than 5 dB peak signal-to-noise ratio improvement for\ncompressed sensing and parallel imaging reconstructions compared with the other\nmethods. Conclusion: The Bayesian inference significantly improved the\nreconstruction performance, compared with the conventional $\\ell_1$-sparsity\nprior in compressed sensing reconstruction tasks. More importantly, the\nproposed reconstruction framework can be generalized for most MRI\nreconstruction scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">GuanXiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1\">Na Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_E/0/1/0/all/0/1\">Edward S. Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1\">Peng Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Feature Fusion for Mitosis Counting. (arXiv:2002.03781v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.03781","description":"<p>Each woman living in the United States has about 1 in 8 chance of developing\ninvasive breast cancer. The mitotic cell count is one of the most common tests\nto assess the aggressiveness or grade of breast cancer. In this prognosis,\nhistopathology images must be examined by a pathologist using high-resolution\nmicroscopes to count the cells. Unfortunately, this can be an exhaustive task\nwith poor reproducibility, especially for non-experts. Deep learning networks\nhave recently been adapted to medical applications which are able to\nautomatically localize these regions of interest. However, these region-based\nnetworks lack the ability to take advantage of the segmentation features\nproduced by a full image CNN which are often used as a sole method of\ndetection. Therefore, the proposed method leverages Faster RCNN for object\ndetection while fusing segmentation features generated by a UNet with RGB image\nfeatures to achieve an F-score of 0.508 on the MITOS-ATYPIA 2014 mitosis\ncounting challenge dataset, outperforming state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yancey_R/0/1/0/all/0/1\">Robin Elizabeth Yancey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation Learning. (arXiv:2003.05438v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.05438","description":"<p>The recently advanced unsupervised learning approaches use the siamese-like\nframework to compare two \"views\" from the same image for learning\nrepresentations. Making the two views distinctive is a core to guarantee that\nunsupervised methods can learn meaningful information. However, such frameworks\nare sometimes fragile on overfitting if the augmentations used for generating\ntwo views are not strong enough, causing the over-confident issue on the\ntraining data. This drawback hinders the model from learning subtle variance\nand fine-grained information. To address this, in this work we aim to involve\nthe distance concept on label space in the unsupervised learning and let the\nmodel be aware of the soft degree of similarity between positive or negative\npairs through mixing the input data space, to further work collaboratively for\nthe input and loss spaces. Despite its conceptual simplicity, we show\nempirically that with the solution -- Unsupervised image mixtures (Un-Mix), we\ncan learn subtler, more robust and generalized representations from the\ntransformed input and corresponding new label space. Extensive experiments are\nconducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard ImageNet\nwith popular unsupervised methods SimCLR, BYOL, MoCo V1&amp;V2, SwAV, etc. Our\nproposed image mixture and label assignment strategy can obtain consistent\nimprovement by 1~3% following exactly the same hyperparameters and training\nprocedures of the base methods. Code is publicly available at\nhttps://github.com/szq0214/Un-Mix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhiqiang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zechun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1\">Marios Savvides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wisdom of Committees: An Overlooked Approach To Faster and More Accurate Models. (arXiv:2012.01988v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01988","description":"<p>Committee-based models (ensembles or cascades) construct models by combining\nexisting pre-trained ones. While ensembles and cascades are well-known\ntechniques that were proposed before deep learning, they are not considered a\ncore building block of deep model architectures and are rarely compared to in\nrecent literature on developing efficient models. In this work, we go back to\nbasics and conduct a comprehensive analysis of the efficiency of\ncommittee-based models. We find that even the most simplistic method for\nbuilding committees from existing, independently pre-trained models can match\nor exceed the accuracy of state-of-the-art models while being drastically more\nefficient. These simple committee-based models also outperform sophisticated\nneural architecture search methods (e.g., BigNAS). These findings hold true for\nseveral tasks, including image classification, video classification, and\nsemantic segmentation, and various architecture families, such as ViT,\nEfficientNet, ResNet, MobileNetV2, and X3D. Our results show that an\nEfficientNet cascade can achieve a 5.4x speedup over B7 and a ViT cascade can\nachieve a 2.3x speedup over ViT-L-384 while being equally accurate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondratyuk_D/0/1/0/all/0/1\">Dan Kondratyuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiansen_E/0/1/0/all/0/1\">Eric Christiansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alon_Y/0/1/0/all/0/1\">Yair Alon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eban_E/0/1/0/all/0/1\">Elad Eban</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-path Bit Sharing for Automatic Loss-aware Model Compression. (arXiv:2101.04935v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.04935","description":"<p>Network pruning and quantization are proven to be effective ways for deep\nmodel compression. To obtain a highly compact model, most methods first perform\nnetwork pruning and then conduct network quantization based on the pruned\nmodel. However, this strategy may ignore that they would affect each other and\nthus performing them separately may lead to sub-optimal performance. To address\nthis, performing pruning and quantization jointly is essential. Nevertheless,\nhow to make a trade-off between pruning and quantization is non-trivial.\nMoreover, existing compression methods often rely on some pre-defined\ncompression configurations. Some attempts have been made to search for optimal\nconfigurations, which however may take unbearable optimization cost. To address\nthe above issues, we devise a simple yet effective method named Single-path Bit\nSharing (SBS). Specifically, we first consider network pruning as a special\ncase of quantization, which provides a unified view for pruning and\nquantization. We then introduce a single-path model to encode all candidate\ncompression configurations. In this way, the configuration search problem is\ntransformed into a subset selection problem, which significantly reduces the\nnumber of parameters, computational cost and optimization difficulty. Relying\non the single-path model, we further introduce learnable binary gates to encode\nthe choice of bitwidth. By jointly training the binary gates in conjunction\nwith network parameters, the compression configurations of each layer can be\nautomatically determined. Extensive experiments on both CIFAR-100 and ImageNet\nshow that SBS is able to significantly reduce computational cost while\nachieving promising performance. For example, our SBS compressed MobileNetV2\nachieves 22.6x Bit-Operation (BOP) reduction with only 0.1% drop in the Top-1\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Models as Distributions of Functions. (arXiv:2102.04776v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.04776","description":"<p>Generative models are typically trained on grid-like data such as images. As\na result, the size of these models usually scales directly with the underlying\ngrid resolution. In this paper, we abandon discretized grids and instead\nparameterize individual data points by continuous functions. We then build\ngenerative models by learning distributions over such functions. By treating\ndata points as functions, we can abstract away from the specific type of data\nwe train on and construct models that are agnostic to discretization. To train\nour model, we use an adversarial approach with a discriminator that acts on\ncontinuous signals. Through experiments on a wide variety of data modalities\nincluding images, 3D shapes and climate data, we demonstrate that our model can\nlearn rich distributions of functions independently of data type and\nresolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dupont_E/0/1/0/all/0/1\">Emilien Dupont</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1\">Yee Whye Teh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantitative Performance Assessment of CNN Units via Topological Entropy Calculation. (arXiv:2103.09716v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09716","description":"<p>Identifying the status of individual network units is critical for\nunderstanding the mechanism of convolutional neural networks (CNNs). However,\nit is still challenging to reliably give a general indication of unit status,\nespecially for units in different network models. To this end, we propose a\nnovel method for quantitatively clarifying the status of single unit in CNN\nusing algebraic topological tools. Unit status is indicated via the calculation\nof a defined topological-based entropy, called feature entropy, which measures\nthe degree of chaos of the global spatial pattern hidden in the unit for a\ncategory. In this way, feature entropy could provide an accurate indication of\nstatus for units in different networks with diverse situations like\nweight-rescaling operation. Further, we show that feature entropy decreases as\nthe layer goes deeper and shares almost simultaneous trend with loss during\ntraining. We show that by investigating the feature entropy of units on only\ntraining data, it could give discrimination between networks with different\ngeneralization ability from the view of the effectiveness of feature\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Versatile Neural Architectures by Propagating Network Codes. (arXiv:2103.13253v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13253","description":"<p>This work explores how to design a single neural network capable of adapting\nto multiple heterogeneous vision tasks, such as image segmentation, 3D\ndetection, and video recognition. This goal is challenging because both network\narchitecture search (NAS) spaces and methods in different tasks are\ninconsistent. We solve this challenge from both sides. We first introduce a\nunified design space for multiple tasks and build a multitask NAS benchmark\n(NAS-Bench-MR) on many widely used datasets, including ImageNet, Cityscapes,\nKITTI, and HMDB51. We further propose Network Coding Propagation (NCP), which\nback-propagates gradients of neural predictors to directly update architecture\ncodes along the desired gradient directions to solve various tasks. In this\nway, optimal architecture configurations can be found by NCP in our large\nsearch space in seconds.\n</p>\n<p>Unlike prior arts of NAS that typically focus on a single task, NCP has\nseveral unique benefits. (1) NCP transforms architecture optimization from\ndata-driven to architecture-driven, enabling joint search an architecture among\nmultitasks with different data distributions. (2) NCP learns from network codes\nbut not original data, enabling it to update the architecture efficiently\nacross datasets. (3) In addition to our NAS-Bench-MR, NCP performs well on\nother NAS benchmarks, such as NAS-Bench-201. (4) Thorough studies of NCP on\ninter-, cross-, and intra-tasks highlight the importance of cross-task neural\narchitecture design, i.e., multitask neural architectures and architecture\ntransferring between different tasks. Code is available at\nhttps://github.com/dingmyu/NCP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuqi Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haoyu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linjie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiwu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling a Powerful Student Model via Online Knowledge Distillation. (arXiv:2103.14473v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.14473","description":"<p>Existing online knowledge distillation approaches either adopt the student\nwith the best performance or construct an ensemble model for better holistic\nperformance. However, the former strategy ignores other students' information,\nwhile the latter increases the computational complexity during deployment. In\nthis paper, we propose a novel method for online knowledge distillation, termed\nFFSD, which comprises two key components: Feature Fusion and Self-Distillation,\ntowards solving the above problems in a unified framework. Different from\nprevious works, where all students are treated equally, the proposed FFSD\nsplits them into a leader student and a common student set. Then, the feature\nfusion module converts the concatenation of feature maps from all common\nstudents into a fused feature map. The fused representation is used to assist\nthe learning of the leader student. To enable the leader student to absorb more\ndiverse information, we design an enhancement strategy to increase the\ndiversity among students. Besides, a self-distillation module is adopted to\nconvert the feature map of deeper layers into a shallower one. Then, the\nshallower layers are encouraged to mimic the transformed feature maps of the\ndeeper layers, which helps the students to generalize better. After training,\nwe simply adopt the leader student, which achieves superior performance, over\nthe common students, without increasing the storage or inference cost.\nExtensive experiments on CIFAR-100 and ImageNet demonstrate the superiority of\nour FFSD over existing works. The code is available at\nhttps://github.com/SJLeo/FFSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaojie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GasHis-Transformer: A Multi-scale Visual Transformer Approach for Gastric Histopathology Image Classification. (arXiv:2104.14528v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.14528","description":"<p>Existing deep learning methods for diagnosis of gastric cancer commonly use\nconvolutional neural network. Recently, the Visual Transformer has attracted\ngreat attention because of its performance and efficiency, but its applications\nare mostly in the field of computer vision. In this paper, a multi-scale visual\ntransformer model, referred to as GasHis-Transformer, is proposed for Gastric\nHistopathological Image Classification (GHIC), which enables the automatic\nclassification of microscopic gastric images into abnormal and normal cases.\nThe GasHis-Transformer model consists of two key modules: A global information\nmodule and a local information module to extract histopathological features\neffectively. In our experiments, a public hematoxylin and eosin (H&amp;E) stained\ngastric histopathological dataset with 280 abnormal and normal images are\ndivided into training, validation and test sets by a ratio of 1 : 1 : 2. The\nGasHis-Transformer model is applied to estimate precision, recall, F1-score and\naccuracy on the test set of gastric histopathological dataset as 98.0%, 100.0%,\n96.0% and 98.0%, respectively. Furthermore, a critical study is conducted to\nevaluate the robustness of GasHis-Transformer, where ten different noises\nincluding four adversarial attack and six conventional image noises are added.\nIn addition, a clinically meaningful study is executed to test the\ngastrointestinal cancer identification performance of GasHis-Transformer with\n620 abnormal images and achieves 96.8% accuracy. Finally, a comparative study\nis performed to test the generalizability with both H&amp;E and immunohistochemical\nstained images on a lymphoma image dataset and a breast cancer dataset,\nproducing comparable F1-scores (85.6% and 82.8%) and accuracies (83.9% and\n89.4%), respectively. In conclusion, GasHisTransformer demonstrates high\nclassification performance and shows its significant potential in the GHIC\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Md Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hongzan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wanli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changhao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_S/0/1/0/all/0/1\">Shiliang Ai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Preservation of Global Knowledge by Not-True Distillation in Federated Learning. (arXiv:2106.03097v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03097","description":"<p>In federated learning, a strong global model is collaboratively learned by\naggregating clients' locally trained models. Although this precludes the need\nto access clients' data directly, the global model's convergence often suffers\nfrom data heterogeneity. This study starts from an analogy to continual\nlearning and suggests that forgetting could be the bottleneck of federated\nlearning. We observe that the global model forgets the knowledge from previous\nrounds, and the local training induces forgetting the knowledge outside of the\nlocal distribution. Based on our findings, we hypothesize that tackling down\nforgetting will relieve the data heterogeneity problem. To this end, we propose\na novel and effective algorithm, Federated Not-True Distillation (FedNTD),\nwhich preserves the global perspective on locally available data only for the\nnot-true classes. In the experiments, FedNTD shows state-of-the-art performance\non various setups without compromising data privacy or incurring additional\ncommunication costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gihun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minchan Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Yongjin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sangmin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoGG3D-Net: Locally Guided Global Descriptor Learning for 3D Place Recognition. (arXiv:2109.08336v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.08336","description":"<p>Retrieval-based place recognition is an efficient and effective solution for\nre-localization within a pre-built map, or global data association for\nSimultaneous Localization and Mapping (SLAM). The accuracy of such an approach\nis heavily dependent on the quality of the extracted scene-level\nrepresentation. While end-to-end solutions - which learn a global descriptor\nfrom input point clouds - have demonstrated promising results, such approaches\nare limited in their ability to enforce desirable properties at the local\nfeature level. In this paper, we introduce a local consistency loss to guide\nthe network towards learning local features which are consistent across\nrevisits, hence leading to more repeatable global descriptors resulting in an\noverall improvement in 3D place recognition performance. We formulate our\napproach in an end-to-end trainable architecture called LoGG3D-Net. Experiments\non two large-scale public benchmarks (KITTI and MulRan) show that our method\nachieves mean $F1_{max}$ scores of $0.939$ and $0.968$ on KITTI and MulRan\nrespectively, achieving state-of-the-art performance while operating in near\nreal-time. The open-source implementation is available at:\nhttps://github.com/csiro-robotics/LoGG3D-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vidanapathirana_K/0/1/0/all/0/1\">Kavisha Vidanapathirana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_M/0/1/0/all/0/1\">Milad Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1\">Peyman Moghadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Multiple Instance Learning with Attention Mechanisms. (arXiv:2111.00947v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.00947","description":"<p>Strongly supervised learning requires detailed knowledge of truth labels at\ninstance levels, and in many machine learning applications this is a major\ndrawback. Multiple instance learning (MIL) is a popular weakly supervised\nlearning method where truth labels are not available at instance level, but\nonly at bag-of-instances level. However, sometimes the nature of the problem\nrequires a more complex description, where a nested architecture of bag-of-bags\nat different levels can capture underlying relationships, like similar\ninstances grouped together. Predicting the latent labels of instances or\ninner-bags might be as important as predicting the final bag-of-bags label but\nis lost in a straightforward nested setting. We propose a Nested Multiple\nInstance with Attention (NMIA) model architecture combining the concept of\nnesting with attention mechanisms. We show that NMIA performs as conventional\nMIL in simple scenarios and can grasp a complex scenario providing insights to\nthe latent labels at different levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fuster_S/0/1/0/all/0/1\">Saul Fuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eftestol_T/0/1/0/all/0/1\">Trygve Eftest&#xf8;l</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engan_K/0/1/0/all/0/1\">Kjersti Engan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimizing Latent Space Directions For GAN-based Local Image Editing. (arXiv:2111.12583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.12583","description":"<p>Generative Adversarial Network (GAN) based localized image editing can suffer\nfrom ambiguity between semantic attributes. We thus present a novel objective\nfunction to evaluate the locality of an image edit. By introducing the\nsupervision from a pre-trained segmentation network and optimizing the\nobjective function, our framework, called Locally Effective Latent Space\nDirection (LELSD), is applicable to any dataset and GAN architecture. Our\nmethod is also computationally fast and exhibits a high extent of\ndisentanglement, which allows users to interactively perform a sequence of\nedits on an image. Our experiments on both GAN-generated and real images\nqualitatively demonstrate the high quality and advantages of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pajouheshgar_E/0/1/0/all/0/1\">Ehsan Pajouheshgar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1\">Sabine S&#xfc;sstrunk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploiting full Resolution Feature Context for Liver Tumor and Vessel Segmentation via Fusion Encoder: Application to Liver Tumor and Vessel 3D reconstruction. (arXiv:2111.13299v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.13299","description":"<p>Liver cancer is one of the most common malignant diseases in the world.\nSegmentation and labeling of liver tumors and blood vessels in CT images can\nprovide convenience for doctors in liver tumor diagnosis and surgical\nintervention. In the past decades, automatic CT segmentation methods based on\ndeep learning have received widespread attention in the medical field. Many\nstate-of-the-art segmentation algorithms appeared during this period. Yet, most\nof the existing segmentation methods only care about the local feature context\nand have a perception defect in the global relevance of medical images, which\nsignificantly affects the segmentation effect of liver tumors and blood\nvessels. We introduce a multi-scale feature context fusion network called\nTransFusionNet based on Transformer and SEBottleNet. This network can\naccurately detect and identify the details of the region of interest of the\nliver vessel, meanwhile it can improve the recognition of morphologic margins\nof liver tumors by exploiting the global information of CT images. Experiments\nshow that TransFusionNet is better than the state-of-the-art method on both the\npublic dataset LITS and 3Dircadb and our clinical dataset. Finally, we propose\nan automatic 3D reconstruction algorithm based on the trained model. The\nalgorithm can complete the reconstruction quickly and accurately in 1 second.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_X/0/1/0/all/0/1\">Xiangyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xudong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Gan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Ying Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_X/0/1/0/all/0/1\">Xin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1\">Huanhuan Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overview of the HECKTOR Challenge at MICCAI 2021: Automatic Head and Neck Tumor Segmentation and Outcome Prediction in PET/CT Images. (arXiv:2201.04138v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.04138","description":"<p>This paper presents an overview of the second edition of the HEad and neCK\nTumOR (HECKTOR) challenge, organized as a satellite event of the 24th\nInternational Conference on Medical Image Computing and Computer Assisted\nIntervention (MICCAI) 2021. The challenge is composed of three tasks related to\nthe automatic analysis of PET/CT images for patients with Head and Neck cancer\n(H&amp;N), focusing on the oropharynx region. Task 1 is the automatic segmentation\nof H&amp;N primary Gross Tumor Volume (GTVt) in FDG-PET/CT images. Task 2 is the\nautomatic prediction of Progression Free Survival (PFS) from the same\nFDG-PET/CT. Finally, Task 3 is the same as Task 2 with ground truth GTVt\nannotations provided to the participants. The data were collected from six\ncenters for a total of 325 images, split into 224 training and 101 testing\ncases. The interest in the challenge was highlighted by the important\nparticipation with 103 registered teams and 448 result submissions. The best\nmethods obtained a Dice Similarity Coefficient (DSC) of 0.7591 in the first\ntask, and a Concordance index (C-index) of 0.7196 and 0.6978 in Tasks 2 and 3,\nrespectively. In all tasks, simplicity of the approach was found to be key to\nensure generalization performance. The comparison of the PFS prediction\nperformance in Tasks 2 and 3 suggests that providing the GTVt contour was not\ncrucial to achieve best results, which indicates that fully automatic methods\ncan be used. This potentially obviates the need for GTVt contouring, opening\navenues for reproducible and large scale radiomics studies including thousands\npotential subjects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Andrearczyk_V/0/1/0/all/0/1\">Vincent Andrearczyk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Oreiller_V/0/1/0/all/0/1\">Valentin Oreiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boughdad_S/0/1/0/all/0/1\">Sarah Boughdad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rest_C/0/1/0/all/0/1\">Catherine Chez Le Rest</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elhalawani_H/0/1/0/all/0/1\">Hesham Elhalawani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jreige_M/0/1/0/all/0/1\">Mario Jreige</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prior_J/0/1/0/all/0/1\">John O. Prior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vallieres_M/0/1/0/all/0/1\">Martin Valli&#xe8;res</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visvikis_D/0/1/0/all/0/1\">Dimitris Visvikis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hatt_M/0/1/0/all/0/1\">Mathieu Hatt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Depeursinge_A/0/1/0/all/0/1\">Adrien Depeursinge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-rank features based double transformation matrices learning for image classification. (arXiv:2201.12351v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.12351","description":"<p>Linear regression is a supervised method that has been widely used in\nclassification tasks. In order to apply linear regression to classification\ntasks, a technique for relaxing regression targets was proposed. However,\nmethods based on this technique ignore the pressure on a single transformation\nmatrix due to the complex information contained in the data. A single\ntransformation matrix in this case is too strict to provide a flexible\nprojection, thus it is necessary to adopt relaxation on transformation matrix.\nThis paper proposes a double transformation matrices learning method based on\nlatent low-rank feature extraction. The core idea is to use double\ntransformation matrices for relaxation, and jointly projecting the learned\nprincipal and salient features from two directions into the label space, which\ncan share the pressure of a single transformation matrix. Firstly, the low-rank\nfeatures are learned by the latent low rank representation (LatLRR) method\nwhich processes the original data from two directions. In this process, sparse\nnoise is also separated, which alleviates its interference on projection\nlearning to some extent. Then, two transformation matrices are introduced to\nprocess the two features separately, and the information useful for the\nclassification is extracted. Finally, the two transformation matrices can be\neasily obtained by alternate optimization methods. Through such processing,\neven when a large amount of redundant information is contained in samples, our\nmethod can also obtain projection results that are easy to classify.\nExperiments on multiple data sets demonstrate the effectiveness of our approach\nfor classification, especially for complex scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yu-Hong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhe Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signing the Supermask: Keep, Hide, Invert. (arXiv:2201.13361v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.13361","description":"<p>The exponential growth in numbers of parameters of neural networks over the\npast years has been accompanied by an increase in performance across several\nfields. However, due to their sheer size, the networks not only became\ndifficult to interpret but also problematic to train and use in real-world\napplications, since hardware requirements increased accordingly. Tackling both\nissues, we present a novel approach that either drops a neural network's\ninitial weights or inverts their respective sign. Put simply, a network is\ntrained by weight selection and inversion without changing their absolute\nvalues. Our contribution extends previous work on masking by additionally\nsign-inverting the initial weights and follows the findings of the Lottery\nTicket Hypothesis. Through this extension and adaptations of initialization\nmethods, we achieve a pruning rate of up to 99%, while still matching or\nexceeding the performance of various baseline and previous models. Our approach\nhas two main advantages. First, and most notable, signed Supermask models\ndrastically simplify a model's structure, while still performing well on given\ntasks. Second, by reducing the neural network to its very foundation, we gain\ninsights into which weights matter for performance. The code is available on\nGitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koster_N/0/1/0/all/0/1\">Nils Koster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grothe_O/0/1/0/all/0/1\">Oliver Grothe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Decision boundaries and convex hulls in the feature space that deep learning functions learn from images. (arXiv:2202.04052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04052","description":"<p>The success of deep neural networks in image classification and learning can\nbe partly attributed to the features they extract from images. It is often\nspeculated about the properties of a low-dimensional manifold that models\nextract and learn from images. However, there is not sufficient understanding\nabout this low-dimensional space based on theory or empirical evidence. For\nimage classification models, their last hidden layer is the one where images of\neach class is separated from other classes and it also has the least number of\nfeatures. Here, we develop methods and formulations to study that feature space\nfor any model. We study the partitioning of the domain in feature space,\nidentify regions guaranteed to have certain classifications, and investigate\nits implications for the pixel space. We observe that geometric arrangements of\ndecision boundaries in feature space is significantly different compared to\npixel space, providing insights about adversarial vulnerabilities, image\nmorphing, extrapolation, ambiguity in classification, and the mathematical\nunderstanding of image classification models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousefzadeh_R/0/1/0/all/0/1\">Roozbeh Yousefzadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box Supervised Video Segmentation Proposal Network. (arXiv:2202.07025v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07025","description":"<p>Video Object Segmentation (VOS) has been targeted by various fully-supervised\nand self-supervised approaches. While fully-supervised methods demonstrate\nexcellent results, self-supervised ones, which do not use pixel-level ground\ntruth, attract much attention. However, self-supervised approaches pose a\nsignificant performance gap. Box-level annotations provide a balanced\ncompromise between labeling effort and result quality for image segmentation\nbut have not been exploited for the video domain. In this work, we propose a\nbox-supervised video object segmentation proposal network, which takes\nadvantage of intrinsic video properties. Our method incorporates object motion\nin the following way: first, motion is computed using a bidirectional temporal\ndifference and a novel bounding box-guided motion compensation. Second, we\nintroduce a novel motion-aware affinity loss that encourages the network to\npredict positive pixel pairs if they share similar motion and color. The\nproposed method outperforms the state-of-the-art self-supervised benchmark by\n16.4% and 6.9% $\\mathcal{J}$ &amp;$\\mathcal{F}$ score and the majority of fully\nsupervised methods on the DAVIS and Youtube-VOS dataset without imposing\nnetwork architectural specifications. We provide extensive tests and ablations\non the datasets, demonstrating the robustness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hannan_T/0/1/0/all/0/1\">Tanveer Hannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koner_R/0/1/0/all/0/1\">Rajat Koner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobold_J/0/1/0/all/0/1\">Jonathan Kobold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1\">Matthias Schubert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Natural Motion: Exploring Discontinuity for Video Frame Interpolation. (arXiv:2202.07291v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07291","description":"<p>Video interpolation is the task that synthesizes the intermediate frame given\ntwo consecutive frames. Most of the previous studies have focused on\nappropriate frame warping operations and refinement modules for the warped\nframes. These studies have been conducted on natural videos having only\ncontinuous motions. However, many practical videos contain a lot of\ndiscontinuous motions, such as chat windows, watermarks, GUI elements, or\nsubtitles. We propose three techniques to expand the concept of transition\nbetween two consecutive frames to address these issues. First is a new\narchitecture that can separate continuous and discontinuous motion areas. We\nalso propose a novel data augmentation strategy called figure-text mixing (FTM)\nto make our model learn more general scenarios. Finally, we propose loss\nfunctions to give supervisions of the discontinuous motion areas with the data\naugmentation. We collected a special dataset consisting of some mobile games\nand chatting videos. We show that our method significantly improves the\ninterpolation qualities of the videos on the special dataset. Moreover, our\nmodel outperforms the state-of-the-art methods for natural video datasets\ncontaining only continuous motions, such as DAVIS and UCF101.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hyeongmin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1\">Chajin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1\">Hanbin Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey of Semen Quality Evaluation in Microscopic Videos Using Computer Assisted Sperm Analysis. (arXiv:2202.07820v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.07820","description":"<p>The Computer Assisted Sperm Analysis (CASA) plays a crucial role in male\nreproductive health diagnosis and Infertility treatment. With the development\nof the computer industry in recent years, a great of accurate algorithms are\nproposed. With the assistance of those novel algorithms, it is possible for\nCASA to achieve a faster and higher quality result. Since image processing is\nthe technical basis of CASA, including pre-processing,feature extraction,\ntarget detection and tracking, these methods are important technical steps in\ndealing with CASA. The various works related to Computer Assisted Sperm\nAnalysis methods in the last 30 years (since 1988) are comprehensively\nintroduced and analysed in this survey. To facilitate understanding, the\nmethods involved are analysed in the sequence of general steps in sperm\nanalysis. In other words, the methods related to sperm detection (localization)\nare first analysed, and then the methods of sperm tracking are analysed. Beside\nthis, we analyse and prospect the present situation and future of CASA.\nAccording to our work, the feasible for applying in sperm microscopic video of\nmethods mentioned in this review is explained. Moreover, existing challenges of\nobject detection and tracking in microscope video are potential to be solved\ninspired by this survey.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhao_W/0/1/0/all/0/1\">Wenwei Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_P/0/1/0/all/0/1\">Pingli Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chen Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bu_X/0/1/0/all/0/1\">Xiaoning Bu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_S/0/1/0/all/0/1\">Shuojia Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_T/0/1/0/all/0/1\">Tao Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grzegorzek_M/0/1/0/all/0/1\">Marcin Grzegorzek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multi-reconstruction study of breast density estimation using Deep Learning. (arXiv:2202.08238v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.08238","description":"<p>Breast density estimation is one of the key tasks in recognizing individuals\npredisposed to breast cancer. It is often challenging because of low contrast\nand fluctuations in mammograms' fatty tissue background. Most of the time, the\nbreast density is estimated manually where a radiologist assigns one of the\nfour density categories decided by the Breast Imaging and Reporting Data\nSystems (BI-RADS). There have been efforts in the direction of automating a\nbreast density classification pipeline.\n</p>\n<p>Breast density estimation is one of the key tasks performed during a\nscreening exam. Dense breasts are more susceptible to breast cancer. The\ndensity estimation is challenging because of low contrast and fluctuations in\nmammograms' fatty tissue background. Traditional mammograms are being replaced\nby tomosynthesis and its other low radiation dose variants (for example\nHologic' Intelligent 2D and C-View). Because of the low-dose requirement,\nincreasingly more screening centers are favoring the Intelligent 2D view and\nC-View. Deep-learning studies for breast density estimation use only a single\nmodality for training a neural network. However, doing so restricts the number\nof images in the dataset. In this paper, we show that a neural network trained\non all the modalities at once performs better than a neural network trained on\nany single modality. We discuss these results using the area under the receiver\noperator characteristics curves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gupta_V/0/1/0/all/0/1\">Vikash Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demirer_M/0/1/0/all/0/1\">Mutlu Demirer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maxwell_R/0/1/0/all/0/1\">Robert W. Maxwell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+White_R/0/1/0/all/0/1\">Richard D. White</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erdal_B/0/1/0/all/0/1\">Barbaros Selnur Erdal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-17T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}