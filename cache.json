{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-04T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models. (arXiv:2207.00056v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00056","description":"<p>The promise of multimodal models for real-world applications has inspired\nresearch in visualizing and understanding their internal mechanics with the end\ngoal of empowering stakeholders to visualize model behavior, perform model\ndebugging, and promote trust in machine learning models. However, modern\nmultimodal models are typically black-box neural networks, which makes it\nchallenging to understand their internal mechanics. How can we visualize the\ninternal modeling of multimodal interactions in these models? Our paper aims to\nfill this gap by proposing MultiViz, a method for analyzing the behavior of\nmultimodal models by scaffolding the problem of interpretability into 4 stages:\n(1) unimodal importance: how each modality contributes towards downstream\nmodeling and prediction, (2) cross-modal interactions: how different modalities\nrelate with each other, (3) multimodal representations: how unimodal and\ncross-modal interactions are represented in decision-level features, and (4)\nmultimodal prediction: how decision-level features are composed to make a\nprediction. MultiViz is designed to operate on diverse modalities, models,\ntasks, and research areas. Through experiments on 8 trained models across 6\nreal-world tasks, we show that the complementary stages in MultiViz together\nenable users to (1) simulate model predictions, (2) assign interpretable\nconcepts to features, (3) perform error analysis on model misclassifications,\nand (4) use insights from error analysis to debug models. MultiViz is publicly\navailable, will be regularly updated with new interpretation tools and metrics,\nand welcomes inputs from the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nihal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Human-Agent Communication via the Information Bottleneck Principle. (arXiv:2207.00088v1 [cs.AI])","link":"http://arxiv.org/abs/2207.00088","description":"<p>Emergent communication research often focuses on optimizing task-specific\nutility as a driver for communication. However, human languages appear to\nevolve under pressure to efficiently compress meanings into communication\nsignals by optimizing the Information Bottleneck tradeoff between\ninformativeness and complexity. In this work, we study how trading off these\nthree factors -- utility, informativeness, and complexity -- shapes emergent\ncommunication, including compared to human communication. To this end, we\npropose Vector-Quantized Variational Information Bottleneck (VQ-VIB), a method\nfor training neural agents to compress inputs into discrete signals embedded in\na continuous space. We train agents via VQ-VIB and compare their performance to\npreviously proposed neural architectures in grounded environments and in a\nLewis reference game. Across all neural architectures and settings, taking into\naccount communicative informativeness benefits communication convergence rates,\nand penalizing communicative complexity leads to human-like lexicon sizes while\nmaintaining high utility. Additionally, we find that VQ-VIB outperforms other\ndiscrete communication methods. This work demonstrates how fundamental\nprinciples that are believed to characterize human language evolution may\ninform emergent communication in artificial agents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1\">Mycal Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1\">Julie Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1\">Roger Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaslavsky_N/0/1/0/all/0/1\">Noga Zaslavsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language model compression with weighted low-rank factorization. (arXiv:2207.00112v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00112","description":"<p>Factorizing a large matrix into small matrices is a popular strategy for\nmodel compression. Singular value decomposition (SVD) plays a vital role in\nthis compression strategy, approximating a learned matrix with fewer\nparameters. However, SVD minimizes the squared error toward reconstructing the\noriginal matrix without gauging the importance of the parameters, potentially\ngiving a larger reconstruction error for those who affect the task accuracy\nmore. In other words, the optimization objective of SVD is not aligned with the\ntrained model's task accuracy. We analyze this previously unexplored problem,\nmake observations, and address it by introducing Fisher information to weigh\nthe importance of parameters affecting the model prediction. This idea leads to\nour method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from\nour approach do not result in smaller reconstruction errors, we find that our\nresulting task accuracy is much closer to the original model's performance. We\nperform analysis with the transformer-based language models, showing our\nweighted SVD largely alleviates the mismatched optimization objectives and can\nmaintain model performance with a higher compression rate. Our method can\ndirectly compress a task-specific model while achieving better performance than\nother compact model strategies requiring expensive model pre-training.\nMoreover, the evaluation of compressing an already compact model shows our\nmethod can further reduce 9% to 30% parameters with an insignificant impact on\ntask accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1\">Ting Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sungen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lou_Q/0/1/0/all/0/1\">Qian Lou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Understanding-Oriented Robust Machine Reading Comprehension Model. (arXiv:2207.00187v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00187","description":"<p>Although existing machine reading comprehension models are making rapid\nprogress on many datasets, they are far from robust. In this paper, we propose\nan understanding-oriented machine reading comprehension model to address three\nkinds of robustness issues, which are over sensitivity, over stability and\ngeneralization. Specifically, we first use a natural language inference module\nto help the model understand the accurate semantic meanings of input questions\nso as to address the issues of over sensitivity and over stability. Then in the\nmachine reading comprehension module, we propose a memory-guided multi-head\nattention method that can further well understand the semantic meanings of\ninput questions and passages. Third, we propose a multilanguage learning\nmechanism to address the issue of generalization. Finally, these modules are\nintegrated with a multi-task learning based method. We evaluate our model on\nthree benchmark datasets that are designed to measure models robustness,\nincluding DuReader (robust) and two SQuAD-related datasets. Extensive\nexperiments show that our model can well address the mentioned three kinds of\nrobustness issues. And it achieves much better results than the compared\nstate-of-the-art models on all these datasets under different evaluation\nmetrics, even under some extreme and unfair evaluations. The source code of our\nwork is available at: https://github.com/neukg/RobustMRC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunchao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1\">Qi Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset. (arXiv:2207.00220v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00220","description":"<p>One concern with the rise of large language models lies with their potential\nfor significant harm, particularly from pretraining on biased, obscene,\ncopyrighted, and private information. Emerging ethical approaches have\nattempted to filter pretraining material, but such approaches have been ad hoc\nand failed to take into account context. We offer an approach to filtering\ngrounded in law, which has directly addressed the tradeoffs in filtering\nmaterial. First, we gather and make available the Pile of Law, a 256GB (and\ngrowing) dataset of open-source English-language legal and administrative data,\ncovering court opinions, contracts, administrative rules, and legislative\nrecords. Pretraining on the Pile of Law may potentially help with legal tasks\nthat have the promise to improve access to justice. Second, we distill the\nlegal norms that governments have developed to constrain the inclusion of toxic\nor private content into actionable lessons for researchers and discuss how our\ndataset reflects these norms. Third, we show how the Pile of Law offers\nresearchers the opportunity to learn such filtering rules directly from the\ndata, providing an exciting new research direction in model-based processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Henderson_P/0/1/0/all/0/1\">Peter Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krass_M/0/1/0/all/0/1\">Mark S. Krass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Lucia Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guha_N/0/1/0/all/0/1\">Neel Guha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_C/0/1/0/all/0/1\">Christopher D. Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00221","description":"<p>Vision-Language Pretraining (VLP) models have recently successfully\nfacilitated many cross-modal downstream tasks. Most existing works evaluated\ntheir systems by comparing the fine-tuned downstream task performance. However,\nonly average downstream task accuracy provides little information about the\npros and cons of each VLP method, let alone provides insights on how the\ncommunity can improve the systems in the future. Inspired by the CheckList for\ntesting natural language processing, we introduce VL-CheckList, a novel\nframework to understand the capabilities of VLP models. The proposed method\ndivides the image-texting ability of a VLP model into three categories:\nobjects, attributes, and relations, and uses a novel taxonomy to further break\ndown these three aspects. We conduct comprehensive studies to analyze seven\nrecently popular VLP models via the proposed framework. Results confirm the\neffectiveness of the proposed method by revealing fine-grained differences\namong the compared models that were not visible from downstream task-only\nevaluation. Further results show promising research direction in building\nbetter VLP models. Data and Code: https://github.com/om-ai-lab/VL-CheckList\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haozhan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianwei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-features based Semantic Augmentation Networks for Named Entity Recognition in Threat Intelligence. (arXiv:2207.00232v1 [cs.CR])","link":"http://arxiv.org/abs/2207.00232","description":"<p>Extracting cybersecurity entities such as attackers and vulnerabilities from\nunstructured network texts is an important part of security analysis. However,\nthe sparsity of intelligence data resulted from the higher frequency variations\nand the randomness of cybersecurity entity names makes it difficult for current\nmethods to perform well in extracting security-related concepts and entities.\nTo this end, we propose a semantic augmentation method which incorporates\ndifferent linguistic features to enrich the representation of input tokens to\ndetect and classify the cybersecurity names over unstructured text. In\nparticular, we encode and aggregate the constituent feature, morphological\nfeature and part of speech feature for each input token to improve the\nrobustness of the method. More than that, a token gets augmented semantic\ninformation from its most similar K words in cybersecurity domain corpus where\nan attentive module is leveraged to weigh differences of the words, and from\ncontextual clues based on a large-scale general field corpus. We have conducted\nexperiments on the cybersecurity datasets DNRTI and MalwareTextDB, and the\nresults demonstrate the effectiveness of the proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peipei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zuoguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yimo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongsong Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Affordance Extraction with an External Knowledge Database for Text-Based Simulated Environments. (arXiv:2207.00265v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00265","description":"<p>Text-based simulated environments have proven to be a valid testbed for\nmachine learning approaches. The process of affordance extraction can be used\nto generate possible actions for interaction within such an environment. In\nthis paper the capabilities and challenges for utilizing external knowledge\ndatabases (in particular ConceptNet) in the process of affordance extraction\nare studied. An algorithm for automated affordance extraction is introduced and\nevaluated on the Interactive Fiction (IF) platforms TextWorld and Jericho. For\nthis purpose, the collected affordances are translated into text commands for\nIF agents. To probe the quality of the automated evaluation process, an\nadditional human baseline study is conducted. The paper illustrates that,\ndespite some challenges, external databases can in principle be used for\naffordance extraction. The paper concludes with recommendations for further\nmodification and improvement of the process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gelhausen_P/0/1/0/all/0/1\">P. Gelhausen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">M. Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_G/0/1/0/all/0/1\">G. Peters</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vers la compr\\'ehension automatique de la parole bout-en-bout \\`a moindre effort. (arXiv:2207.00349v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00349","description":"<p>Recent advances in spoken language understanding benefited from\nSelf-Supervised models trained on large speech corpora. For French, the\nLeBenchmark project has made such models available and has led to impressive\nprogress on several tasks including spoken language understanding. These\nadvances have a non-negligible cost in terms of computation time and energy\nconsumption. In this paper, we compare several learning strategies aiming at\nreducing such cost while keeping competitive performances. The experiments are\nperformed on the MEDIA corpus, and show that it is possible to reduce the\nlearning cost while maintaining state-of-the-art performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naguib_M/0/1/0/all/0/1\">Marco Naguib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1\">Fran&#xe7;ois Portet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinarelli_M/0/1/0/all/0/1\">Marco Dinarelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Low-Cost End-to-End Spoken Language Understanding. (arXiv:2207.00352v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00352","description":"<p>Recent advances in spoken language understanding benefited from\nSelf-Supervised models trained on large speech corpora. For French, the\nLeBenchmark project has made such models available and has led to impressive\nprogress on several tasks including spoken language understanding. These\nadvances have a non-negligible cost in terms of computation time and energy\nconsumption. In this paper, we compare several learning strategies trying to\nreduce such cost while keeping competitive performance. At the same time we\npropose an extensive analysis where we measure the cost of our models in terms\nof training time and electric energy consumption, hopefully promoting a\ncomprehensive evaluation procedure. The experiments are performed on the FSC\nand MEDIA corpora, and show that it is possible to reduce the learning cost\nwhile maintaining state-of-the-art performance and using SSL models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dinarelli_M/0/1/0/all/0/1\">Marco Dinarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naguib_M/0/1/0/all/0/1\">Marco Naguib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portet_F/0/1/0/all/0/1\">Fran&#xe7;ois Portet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Generation with a Question-Answering Blueprint. (arXiv:2207.00397v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00397","description":"<p>The ability to convey relevant and faithful information is critical for many\ntasks in conditional generation and yet remains elusive for neural seq-to-seq\nmodels whose outputs often reveal hallucinations and fail to correctly cover\nimportant details. In this work, we advocate planning as a useful intermediate\nrepresentation for rendering conditional generation less opaque and more\ngrounded. Our work proposes a new conceptualization of text plans as a sequence\nof question-answer (QA) pairs. We enhance existing datasets (e.g., for\nsummarization) with a QA blueprint operating as a proxy for both content\nselection (i.e.,~what to say) and planning (i.e.,~in what order). We obtain\nblueprints automatically by exploiting state-of-the-art question generation\ntechnology and convert input-output pairs into input-blueprint-output tuples.\nWe develop Transformer-based models, each varying in how they incorporate the\nblueprint in the generated output (e.g., as a global plan or iteratively).\nEvaluation across metrics and datasets demonstrates that blueprint models are\nmore factual than alternatives which do not resort to planning and allow\ntighter control of the generation output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narayan_S/0/1/0/all/0/1\">Shashi Narayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maynez_J/0/1/0/all/0/1\">Joshua Maynez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amplayo_R/0/1/0/all/0/1\">Reinald Kim Amplayo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganchev_K/0/1/0/all/0/1\">Kuzman Ganchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Annie Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huot_F/0/1/0/all/0/1\">Fantine Huot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1\">Mirella Lapata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swiss German Speech to Text system evaluation. (arXiv:2207.00412v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00412","description":"<p>We present an in-depth evaluation of four commercially available\nSpeech-to-Text (STT) systems for Swiss German. The systems are anonymized and\nreferred to as system a-d in this report. We compare the four systems to our\nSTT model, referred to as FHNW from hereon after, and provide details on how we\ntrained our model. To evaluate the models, we use two STT datasets from\ndifferent domains. The Swiss Parliament Corpus (SPC) test set and a private\ndataset in the news domain with an even distribution across seven dialect\nregions. We provide a detailed error analysis to detect the three systems'\nstrengths and weaknesses. This analysis is limited by the characteristics of\nthe two test sets. Our model scored the highest bilingual evaluation understudy\n(BLEU) on both datasets. On the SPC test set, we obtain a BLEU score of 0.607,\nwhereas the best commercial system reaches a BLEU score of 0.509. On our\nprivate test set, we obtain a BLEU score of 0.722 and the best commercial\nsystem a BLEU score of 0.568.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schraner_Y/0/1/0/all/0/1\">Yanick Schraner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1\">Christian Scheller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluss_M/0/1/0/all/0/1\">Michel Pl&#xfc;ss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogel_M/0/1/0/all/0/1\">Manfred Vogel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How trial-to-trial learning shapes mappings in the mental lexicon: Modelling Lexical Decision with Linear Discriminative Learning. (arXiv:2207.00430v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00430","description":"<p>Priming and antipriming can be modelled with error-driven learning (Marsolek,\n2008), by assuming that the learning of the prime influences processing of the\ntarget stimulus. This implies that participants are continuously learning in\npriming studies, and predicts that they are also learning in each trial of\nother psycholinguistic experiments. This study investigates whether\ntrial-to-trial learning can be detected in lexical decision experiments. We\nused the Discriminative Lexicon Model (DLM; Baayen et al., 2019), a model of\nthe mental lexicon with meaning representations from distributional semantics,\nwhich models incremental learning with the Widrow-Hoff rule. We used data from\nthe British Lexicon Project (BLP; Keuleers et al., 2012) and simulated the\nlexical decision experiment with the DLM on a trial-by-trial basis for each\nsubject individually. Then, reaction times for words and nonwords were\npredicted with Generalised Additive Models, using measures derived from the DLM\nsimulations as predictors. Models were developed with the data of two subjects\nand tested on all other subjects. We extracted measures from two simulations\nfor each subject (one with learning updates between trials and one without),\nand used them as input to two GAMs. Learning-based models showed better model\nfit than the non-learning ones for the majority of subjects. Our measures also\nprovided insights into lexical processing and enabled us to explore individual\ndifferences with Linear Mixed Models. This demonstrates the potential of the\nDLM to model behavioural data and leads to the conclusion that trial-to-trial\nlearning can indeed be detected in psycholinguistic experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heitmeier_M/0/1/0/all/0/1\">Maria Heitmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1\">Yu-Ying Chuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reinforcement Learning of Multi-Domain Dialog Policies Via Action Embeddings. (arXiv:2207.00468v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00468","description":"<p>Learning task-oriented dialog policies via reinforcement learning typically\nrequires large amounts of interaction with users, which in practice renders\nsuch methods unusable for real-world applications. In order to reduce the data\nrequirements, we propose to leverage data from across different dialog domains,\nthereby reducing the amount of data required from each given domain. In\nparticular, we propose to learn domain-agnostic action embeddings, which\ncapture general-purpose structure that informs the system how to act given the\ncurrent dialog context, and are then specialized to a specific domain. We show\nhow this approach is capable of learning with significantly less interaction\nwith users, with a reduction of 35% in the number of dialogs required to learn,\nand to a higher level of proficiency than training separate policies for each\ndomain on a set of simulated domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendez_J/0/1/0/all/0/1\">Jorge A. Mendez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geramifard_A/0/1/0/all/0/1\">Alborz Geramifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Effects of Hyperparameters on Knowledge Graph Embedding Quality. (arXiv:2207.00473v1 [cs.AI])","link":"http://arxiv.org/abs/2207.00473","description":"<p>Embedding knowledge graphs into low-dimensional spaces is a popular method\nfor applying approaches, such as link prediction or node classification, to\nthese databases. This embedding process is very costly in terms of both\ncomputational time and space. Part of the reason for this is the optimisation\nof hyperparameters, which involves repeatedly sampling, by random, guided, or\nbrute-force selection, from a large hyperparameter space and testing the\nresulting embeddings for their quality. However, not all hyperparameters in\nthis search space will be equally important. In fact, with prior knowledge of\nthe relative importance of the hyperparameters, some could be eliminated from\nthe search altogether without significantly impacting the overall quality of\nthe outputted embeddings. To this end, we ran a Sobol sensitivity analysis to\nevaluate the effects of tuning different hyperparameters on the variance of\nembedding quality. This was achieved by performing thousands of embedding\ntrials, each time measuring the quality of embeddings produced by different\nhyperparameter configurations. We regressed the embedding quality on those\nhyperparameter configurations, using this model to generate Sobol sensitivity\nindices for each of the hyperparameters. By evaluating the correlation between\nSobol indices, we find substantial variability in the hyperparameter\nsensitivities between knowledge graphs with differing dataset characteristics\nas the probable cause of these inconsistencies. As an additional contribution\nof this work we identify several relations in the UMLS knowledge graph that may\ncause data leakage via inverse relations, and derive and present UMLS-43, a\nleakage-robust variant of that graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lloyd_O/0/1/0/all/0/1\">Oliver Lloyd</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaunt_T/0/1/0/all/0/1\">Tom Gaunt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panning for gold: Lessons learned from the platform-agnostic automated detection of political content in textual data. (arXiv:2207.00489v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00489","description":"<p>The growing availability of data about online information behaviour enables\nnew possibilities for political communication research. However, the volume and\nvariety of these data makes them difficult to analyse and prompts the need for\ndeveloping automated content approaches relying on a broad range of natural\nlanguage processing techniques (e.g. machine learning- or neural network-based\nones). In this paper, we discuss how these techniques can be used to detect\npolitical content across different platforms. Using three validation datasets,\nwhich include a variety of political and non-political textual documents from\nonline platforms, we systematically compare the performance of three groups of\ndetection techniques relying on dictionaries, supervised machine learning, or\nneural networks. We also examine the impact of different modes of data\npreprocessing (e.g. stemming and stopword removal) on the low-cost\nimplementations of these techniques using a large set (n = 66) of detection\nmodels. Our results show the limited impact of preprocessing on model\nperformance, with the best results for less noisy data being achieved by neural\nnetwork- and machine-learning-based models, in contrast to the more robust\nperformance of dictionary-based models on noisy data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1\">Mykola Makhortykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_E/0/1/0/all/0/1\">Ernesto de Le&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1\">Aleksandra Urman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christner_C/0/1/0/all/0/1\">Clara Christner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sydorova_M/0/1/0/all/0/1\">Maryna Sydorova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adam_S/0/1/0/all/0/1\">Silke Adam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_M/0/1/0/all/0/1\">Michaela Maier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gil_Lopez_T/0/1/0/all/0/1\">Teresa Gil-Lopez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reduce Indonesian Vocabularies with an Indonesian Sub-word Separator. (arXiv:2207.00552v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00552","description":"<p>Indonesian is an agglutinative language since it has a compounding process of\nword-formation. Therefore, the translation model of this language requires a\nmechanism that is even lower than the word level, referred to as the sub-word\nlevel. This compounding process leads to a rare word problem since the number\nof vocabulary explodes. We propose a strategy to address the unique word\nproblem of the neural machine translation (NMT) system, which uses Indonesian\nas a pair language. Our approach uses a rule-based method to transform a word\ninto its roots and accompanied affixes to retain its meaning and context. Using\na rule-based algorithm has more advantages: it does not require corpus data but\nonly applies the standard Indonesian rules. Our experiments confirm that this\nmethod is practical. It reduces the number of vocabulary significantly up to\n57\\%, and on the English to Indonesian translation, this strategy provides an\nimprovement of up to 5 BLEU points over a similar NMT system that does not use\nthis technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amien_M/0/1/0/all/0/1\">Mukhlis Amien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_F/0/1/0/all/0/1\">Feng Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heyan_H/0/1/0/all/0/1\">Huang Heyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Learning. (arXiv:2207.00555v1 [eess.AS])","link":"http://arxiv.org/abs/2207.00555","description":"<p>Large-scale speech self-supervised learning (SSL) has emerged to the main\nfield of speech processing, however, the problem of computational cost arising\nfrom its vast size makes a high entry barrier to academia. In addition,\nexisting distillation techniques of speech SSL models compress the model by\nreducing layers, which induces performance degradation in linguistic pattern\nrecognition tasks such as phoneme recognition (PR). In this paper, we propose\nFitHuBERT, which makes thinner in dimension throughout almost all model\ncomponents and deeper in layer compared to prior speech SSL distillation works.\nMoreover, we employ a time-reduction layer to speed up inference time and\npropose a method of hint-based distillation for less performance degradation.\nOur method reduces the model to 23.8% in size and 35.9% in inference time\ncompared to HuBERT. Also, we achieve 12.1% word error rate and 13.3% phoneme\nerror rate on the SUPERB benchmark which is superior than prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonghyeon Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jang_K/0/1/0/all/0/1\">Kangwook Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goo_J/0/1/0/all/0/1\">Jahyun Goo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_Y/0/1/0/all/0/1\">Youngmoon Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hoirin Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is neural language acquisition similar to natural? A chronological probing study. (arXiv:2207.00560v1 [cs.CL])","link":"http://arxiv.org/abs/2207.00560","description":"<p>The probing methodology allows one to obtain a partial representation of\nlinguistic phenomena stored in the inner layers of the neural network, using\nexternal classifiers and statistical analysis. Pre-trained transformer-based\nlanguage models are widely used both for natural language understanding (NLU)\nand natural language generation (NLG) tasks making them most commonly used for\ndownstream applications. However, little analysis was carried out, whether the\nmodels were pre-trained enough or contained knowledge correlated with\nlinguistic theory. We are presenting the chronological probing study of\ntransformer English models such as MultiBERT and T5. We sequentially compare\nthe information about the language learned by the models in the process of\ntraining on corpora. The results show that 1) linguistic information is\nacquired in the early stages of training 2) both language models demonstrate\ncapabilities to capture various features from various levels of language,\nincluding morphology, syntax, and even discourse, while they also can\ninconsistently fail on tasks that are perceived as easy. We also introduce the\nopen-source framework for chronological probing research, compatible with other\ntransformer-based models.\nhttps://github.com/EkaterinaVoloshina/chronological_probing\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voloshina_E/0/1/0/all/0/1\">Ekaterina Voloshina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serikov_O/0/1/0/all/0/1\">Oleg Serikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1\">Tatiana Shavrina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Document Keyphrase Extraction: Dataset, Baselines and Review. (arXiv:2110.01073v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01073","description":"<p>Keyphrase extraction has been extensively researched within the\nsingle-document setting, with an abundance of methods, datasets and\napplications. In contrast, multi-document keyphrase extraction has been\ninfrequently studied, despite its utility for describing sets of documents, and\nits use in summarization. Moreover, no prior dataset exists for multi-document\nkeyphrase extraction, hindering the progress of the task. Recent advances in\nmulti-text processing make the task an even more appealing challenge to pursue.\nTo stimulate this pursuit, we present here the first dataset for the task,\nMK-DUC-01, which can serve as a new benchmark, and test multiple keyphrase\nextraction baselines on our data. In addition, we provide a brief, yet\ncomprehensive, literature review of the task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shapira_O/0/1/0/all/0/1\">Ori Shapira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasunuru_R/0/1/0/all/0/1\">Ramakanth Pasunuru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amsterdamer_Y/0/1/0/all/0/1\">Yael Amsterdamer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepA2: A Modular Framework for Deep Argument Analysis with Pretrained Neural Text2Text Language Models. (arXiv:2110.01509v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01509","description":"<p>In this paper, we present and implement a multi-dimensional, modular\nframework for performing deep argument analysis (DeepA2) using current\npre-trained language models (PTLMs). ArgumentAnalyst -- a T5 model (Raffel et\nal. 2020) set up and trained within DeepA2 -- reconstructs argumentative texts,\nwhich advance an informal argumentation, as valid arguments: It inserts, e.g.,\nmissing premises and conclusions, formalizes inferences, and coherently links\nthe logical reconstruction to the source text. We create a synthetic corpus for\ndeep argument analysis, and evaluate ArgumentAnalyst on this new dataset as\nwell as on existing data, specifically EntailmentBank (Dalvi et al. 2021). Our\nempirical findings vindicate the overall framework and highlight the advantages\nof a modular design, in particular its ability to emulate established\nheuristics (such as hermeneutic cycles), to explore the model's uncertainty, to\ncope with the plurality of correct solutions (underdetermination), and to\nexploit higher-order evidence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Betz_G/0/1/0/all/0/1\">Gregor Betz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richardson_K/0/1/0/all/0/1\">Kyle Richardson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Waits for No One! Analysis and Challenges of Temporal Misalignment. (arXiv:2111.07408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.07408","description":"<p>When an NLP model is trained on text data from one time period and tested or\ndeployed on data from another, the resulting temporal misalignment can degrade\nend-task performance. In this work, we establish a suite of eight diverse tasks\nacross different domains (social media, science papers, news, and reviews) and\nperiods of time (spanning five years or more) to quantify the effects of\ntemporal misalignment. Our study is focused on the ubiquitous setting where a\npretrained model is optionally adapted through continued domain-specific\npretraining, followed by task-specific finetuning. We establish a suite of\ntasks across multiple domains to study temporal misalignment in modern NLP\nsystems. We find stronger effects of temporal misalignment on task performance\nthan have been previously reported. We also find that, while temporal\nadaptation through continued pretraining can help, these gains are small\ncompared to task-specific finetuning on data from the target time period. Our\nfindings motivate continued research to improve temporal robustness of NLP\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1\">Kelvin Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mandyam_K/0/1/0/all/0/1\">Karishma Mandyam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Degendering Resumes for Algorithmic Resume Screening. (arXiv:2112.08910v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08910","description":"<p>We investigate whether it is feasible to remove gendered information from\nresumes to mitigate potential bias in algorithmic resume screening. Using a\ncorpus of 709k resumes from IT firms, we first train a series of models to\nclassify the self-reported gender of the applicant, thereby measuring the\nextent and nature of gendered information encoded in resumes. We then conduct a\nseries of gender obfuscation experiments, where we iteratively remove gendered\ninformation from resumes. Finally, we train a resume screening algorithm and\ninvestigate the trade-off between gender obfuscation and screening algorithm\nperformance. Results show: (1) There is a significant amount of gendered\ninformation in resumes. (2) Lexicon-based gender obfuscation method (i.e.\nremoving tokens that are predictive of gender) can reduce the amount of\ngendered information to a large extent. However, after a certain point, the\nperformance of the resume screening algorithm starts suffering. (3)\nGeneral-purpose gender debiasing methods for NLP models such as removing gender\nsubspace from embeddings are not effective in obfuscating gender.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parasurama_P/0/1/0/all/0/1\">Prasanna Parasurama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization. (arXiv:2202.00443v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.00443","description":"<p>We present a novel benchmark and associated evaluation metrics for assessing\nthe performance of text anonymization methods. Text anonymization, defined as\nthe task of editing a text document to prevent the disclosure of personal\ninformation, currently suffers from a shortage of privacy-oriented annotated\ntext resources, making it difficult to properly evaluate the level of privacy\nprotection offered by various anonymization methods. This paper presents TAB\n(Text Anonymization Benchmark), a new, open-source annotated corpus developed\nto address this shortage. The corpus comprises 1,268 English-language court\ncases from the European Court of Human Rights (ECHR) enriched with\ncomprehensive annotations about the personal information appearing in each\ndocument, including their semantic category, identifier type, confidential\nattributes, and co-reference relations. Compared to previous work, the TAB\ncorpus is designed to go beyond traditional de-identification (which is limited\nto the detection of predefined semantic categories), and explicitly marks which\ntext spans ought to be masked in order to conceal the identity of the person to\nbe protected. Along with presenting the corpus and its annotation layers, we\nalso propose a set of evaluation metrics that are specifically tailored towards\nmeasuring the performance of text anonymization, both in terms of privacy\nprotection and utility preservation. We illustrate the use of the benchmark and\nthe proposed metrics by assessing the empirical performance of several baseline\ntext anonymization models. The full corpus along with its privacy-oriented\nannotation guidelines, evaluation scripts and baseline models are available on:\nhttps://github.com/NorskRegnesentral/text-anonymisation-benchmark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pilan_I/0/1/0/all/0/1\">Ildik&#xf3; Pil&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lison_P/0/1/0/all/0/1\">Pierre Lison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1\">Anthi Papadopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batet_M/0/1/0/all/0/1\">Montserrat Batet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v3 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.15081","description":"<p>We present a method for visually-grounded spoken term discovery. After\ntraining either a HuBERT or wav2vec2.0 model to associate spoken captions with\nnatural images, we show that powerful word segmentation and clustering\ncapability emerges within the model's self-attention heads. Our experiments\nreveal that this ability is not present to nearly the same extent in the base\nHuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a\ncrucial component of the word discovery capability we observe. We also evaluate\nour method on the Buckeye word segmentation and ZeroSpeech spoken term\ndiscovery tasks, where we outperform all currently published methods on several\nmetrics. Code and model weights are available at\nhttps://github.com/jasonppy/word-discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study on Speaker-attributed Automatic Speech Recognition in Multi-party Meetings. (arXiv:2203.16834v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.16834","description":"<p>In this paper, we conduct a comparative study on speaker-attributed automatic\nspeech recognition (SA-ASR) in the multi-party meeting scenario, a topic with\nincreasing attention in meeting rich transcription. Specifically, three\napproaches are evaluated in this study. The first approach, FD-SOT, consists of\na frame-level diarization model to identify speakers and a multi-talker ASR to\nrecognize utterances. The speaker-attributed transcriptions are obtained by\naligning the diarization results and recognized hypotheses. However, such an\nalignment strategy may suffer from erroneous timestamps due to the modular\nindependence, severely hindering the model performance. Therefore, we propose\nthe second approach, WD-SOT, to address alignment errors by introducing a\nword-level diarization model, which can get rid of such timestamp alignment\ndependency. To further mitigate the alignment issues, we propose the third\napproach, TS-ASR, which trains a target-speaker separation module and an ASR\nmodule jointly. By comparing various strategies for each SA-ASR approach,\nexperimental results on a real meeting scenario corpus, AliMeeting, reveal that\nthe WD-SOT approach achieves 10.7% relative reduction on averaged\nspeaker-dependent character error rate (SD-CER), compared with the FD-SOT\napproach. In addition, the TS-ASR approach also outperforms the FD-SOT approach\nand brings 16.5% relative average SD-CER reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1\">Zhihao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiliang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yuxiao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Building an ASR Error Robust Spoken Virtual Patient System in a Highly Class-Imbalanced Scenario Without Speech Data. (arXiv:2204.05183v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05183","description":"<p>A Virtual Patient (VP) is a powerful tool for training medical students to\ntake patient histories, where responding to a diverse set of spoken questions\nis essential to simulate natural conversations with a student. The performance\nof such a Spoken Language Understanding system (SLU) can be adversely affected\nby both the presence of Automatic Speech Recognition (ASR) errors in the test\ndata and a high degree of class imbalance in the SLU training data. While these\ntwo issues have been addressed separately in prior work, we develop a novel\ntwo-step training methodology that tackles both these issues effectively in a\nsingle dialog agent. As it is difficult to collect spoken data from users\nwithout a functioning SLU system, our method does not rely on spoken data for\ntraining, rather we use an ASR error predictor to \"speechify\" the text data.\nOur method shows significant improvements over strong baselines on the VP\nintent classification task at various word error rate settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sunder_V/0/1/0/all/0/1\">Vishal Sunder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serai_P/0/1/0/all/0/1\">Prashant Serai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosler_Lussier_E/0/1/0/all/0/1\">Eric Fosler-Lussier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tokenwise Contrastive Pretraining for Finer Speech-to-BERT Alignment in End-to-End Speech-to-Intent Systems. (arXiv:2204.05188v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05188","description":"<p>Recent advances in End-to-End (E2E) Spoken Language Understanding (SLU) have\nbeen primarily due to effective pretraining of speech representations. One such\npretraining paradigm is the distillation of semantic knowledge from\nstate-of-the-art text-based models like BERT to speech encoder neural networks.\nThis work is a step towards doing the same in a much more efficient and\nfine-grained manner where we align speech embeddings and BERT embeddings on a\ntoken-by-token basis. We introduce a simple yet novel technique that uses a\ncross-modal attention mechanism to extract token-level contextual embeddings\nfrom a speech encoder such that these can be directly compared and aligned with\nBERT based contextual embeddings. This alignment is performed using a novel\ntokenwise contrastive loss. Fine-tuning such a pretrained model to perform\nintent recognition using speech directly yields state-of-the-art performance on\ntwo widely used SLU datasets. Our model improves further when fine-tuned with\nadditional regularization using SpecAugment especially when speech is noisy,\ngiving an absolute improvement as high as 8% over previous results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sunder_V/0/1/0/all/0/1\">Vishal Sunder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fosler_Lussier_E/0/1/0/all/0/1\">Eric Fosler-Lussier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hong-Kwang J. Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Document-Level Relation Extraction. (arXiv:2205.02048v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2205.02048","description":"<p>We present FREDo, a few-shot document-level relation extraction (FSDLRE)\nbenchmark. As opposed to existing benchmarks which are built on sentence-level\nrelation extraction corpora, we argue that document-level corpora provide more\nrealism, particularly regarding none-of-the-above (NOTA) distributions.\nTherefore, we propose a set of FSDLRE tasks and construct a benchmark based on\ntwo existing supervised learning data sets, DocRED and sciERC. We adapt the\nstate-of-the-art sentence-level method MNAV to the document-level and develop\nit further for improved domain adaptation. We find FSDLRE to be a challenging\nsetting with interesting new characteristics such as the ability to sample NOTA\ninstances from the support set. The data, code, and trained models are\navailable online (https://github.com/nicpopovic/FREDo).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1\">Nicholas Popovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farber_M/0/1/0/all/0/1\">Michael F&#xe4;rber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Case for a Single Model that can Both Generate Continuations and Fill in the Blank. (arXiv:2206.04812v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.04812","description":"<p>The task of inserting text into a specified position in a passage, known as\nfill in the blank (FitB), is useful for a variety of applications where writers\ninteract with a natural language generation (NLG) system to craft text. While\nprevious work has tackled this problem with models trained specifically to do\nthe fill-in-the-blank task, a more useful model is one that can effectively\nperform _both_ FitB and continuation. In this work, we evaluate the feasibility\nof using a single model to do both tasks. We show that models pre-trained with\na FitB-style objective are capable of both tasks, while models pre-trained for\ncontinuation are not. Finally, we show how FitB models can be easily finetuned\nto allow for fine-grained control over the length and word choice of the\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dugan_L/0/1/0/all/0/1\">Liam Dugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1\">Emily Reif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Ann Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coenen_A/0/1/0/all/0/1\">Andy Coenen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Heuristics for AI-Generated Language Are Flawed. (arXiv:2206.07271v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07271","description":"<p>Human communication is increasingly intermixed with language generated by AI.\nAcross chat, email, and social media, AI systems produce smart replies,\nautocompletes, and translations. AI-generated language is often not identified\nas such but poses as human language, raising concerns about novel forms of\ndeception and manipulation. Here, we study how humans discern whether one of\nthe most personal and consequential forms of language - a self-presentation -\nwas generated by AI. In six experiments, participants (N = 4,600) tried to\ndetect self-presentations generated by state-of-the-art language models. Across\nprofessional, hospitality, and dating settings, we find that humans are unable\nto detect AI-generated self-presentations. Our findings show that human\njudgments of AI-generated language are handicapped by intuitive but flawed\nheuristics such as associating first-person pronouns, spontaneous wording, or\nfamily topics with humanity. We demonstrate that these heuristics make human\njudgment of generated language predictable and manipulable, allowing AI systems\nto produce language perceived as more human than human. We discuss solutions,\nsuch as AI accents, to reduce the deceptive potential of generated language,\nlimiting the subversion of human intuition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jakesch_M/0/1/0/all/0/1\">Maurice Jakesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hancock_J/0/1/0/all/0/1\">Jeffrey Hancock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naaman_M/0/1/0/all/0/1\">Mor Naaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Knowledge Selection for Grounded Dialogues via Document Semantic Graphs. (arXiv:2206.07296v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07296","description":"<p>Providing conversation models with background knowledge has been shown to\nmake open-domain dialogues more informative and engaging. Existing models treat\nknowledge selection as a sentence ranking or classification problem where each\nsentence is handled individually, ignoring the internal semantic connection\namong sentences in the background document. In this work, we propose to\nautomatically convert the background knowledge documents into document semantic\ngraphs and then perform knowledge selection over such graphs. Our document\nsemantic graphs preserve sentence-level information through the use of sentence\nnodes and provide concept connections between sentences. We jointly apply\nmulti-task learning for sentence-level and concept-level knowledge selection\nand show that it improves sentence-level selection. Our experiments show that\nour semantic graph-based knowledge selection improves over sentence selection\nbaselines for both the knowledge selection task and the end-to-end response\ngeneration task on HollE and improves generalization on unseen topics in WoW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namazifar_M/0/1/0/all/0/1\">Mahdi Namazifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Impact of Noises in Crowd-Sourced Data for Speech Translation. (arXiv:2206.13756v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.13756","description":"<p>Training speech translation (ST) models requires large and high-quality\ndatasets. MuST-C is one of the most widely used ST benchmark datasets. It\ncontains around 400 hours of speech-transcript-translation data for each of the\neight translation directions. This dataset passes several quality-control\nfilters during creation. However, we find that MuST-C still suffers from three\nmajor quality issues: audio-text misalignment, inaccurate translation, and\nunnecessary speaker's name. What are the impacts of these data quality issues\nfor model development and evaluation? In this paper, we propose an automatic\nmethod to fix or filter the above quality issues, using English-German (En-De)\ntranslation as an example. Our experiments show that ST models perform better\non clean test sets, and the rank of proposed models remains consistent across\ndifferent test sets. Besides, simply removing misaligned data points from the\ntraining set does not lead to a better ST model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siqi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is it possible not to cheat on the Turing Test: Exploring the potential and challenges for true natural language 'understanding' by computers. (arXiv:2206.14672v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14672","description":"<p>Recent hype surrounding the increasing sophistication of language processing\nmodels has renewed optimism regarding machines achieving a human-like command\nof natural language. The area of natural language understanding in artificial\nintelligence claims to have been making great strides in this area, however,\nthe lack of conceptual clarity in how 'understanding' is used in this and other\ndisciplines have made it difficult to discern how close we actually are. A\ncomprehensive, interdisciplinary overview of current approaches and remaining\nchallenges is yet to be carried out. Beyond linguistic knowledge, this requires\nconsidering our species-specific capabilities to categorize, memorize, label\nand communicate our (sufficiently similar) embodied and situated experiences.\nMoreover, gauging the practical constraints requires critically analyzing the\ntechnical capabilities of current models, as well as deeper philosophical\nreflection on theoretical possibilities and limitations. In this paper, I unite\nall of these perspectives -- the philosophical, cognitive-linguistic, and\ntechnical -- to unpack the challenges involved in reaching true (human-like)\nlanguage understanding. By unpacking the theoretical assumptions inherent in\ncurrent approaches, I hope to illustrate how far we actually are from achieving\nthis goal, if indeed it is the goal.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alberts_L/0/1/0/all/0/1\">Lize Alberts</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Solving Quantitative Reasoning Problems with Language Models. (arXiv:2206.14858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.14858","description":"<p>Language models have achieved remarkable performance on a wide range of tasks\nthat require natural language understanding. Nevertheless, state-of-the-art\nmodels have generally struggled with tasks that require quantitative reasoning,\nsuch as solving mathematics, science, and engineering problems at the college\nlevel. To help close this gap, we introduce Minerva, a large language model\npretrained on general natural language data and further trained on technical\ncontent. The model achieves state-of-the-art performance on technical\nbenchmarks without the use of external tools. We also evaluate our model on\nover two hundred undergraduate-level problems in physics, biology, chemistry,\neconomics, and other sciences that require quantitative reasoning, and find\nthat the model can correctly answer nearly a third of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lewkowycz_A/0/1/0/all/0/1\">Aitor Lewkowycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1\">David Dohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dyer_E/0/1/0/all/0/1\">Ethan Dyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramasesh_V/0/1/0/all/0/1\">Vinay Ramasesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slone_A/0/1/0/all/0/1\">Ambrose Slone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1\">Cem Anil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlag_I/0/1/0/all/0/1\">Imanol Schlag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutman_Solo_T/0/1/0/all/0/1\">Theo Gutman-Solo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuhuai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gur_Ari_G/0/1/0/all/0/1\">Guy Gur-Ari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_V/0/1/0/all/0/1\">Vedant Misra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"esCorpius: A Massive Spanish Crawling Corpus. (arXiv:2206.15147v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.15147","description":"<p>In the recent years, transformer-based models have lead to significant\nadvances in language modelling for natural language processing. However, they\nrequire a vast amount of data to be (pre-)trained and there is a lack of\ncorpora in languages other than English. Recently, several initiatives have\npresented multilingual datasets obtained from automatic web crawling. However,\nthe results in Spanish present important shortcomings, as they are either too\nsmall in comparison with other languages, or present a low quality derived from\nsub-optimal cleaning and deduplication. In this paper, we introduce esCorpius,\na Spanish crawling corpus obtained from near 1 Pb of Common Crawl data. It is\nthe most extensive corpus in Spanish with this level of quality in the\nextraction, purification and deduplication of web textual content. Our data\ncuration process involves a novel highly parallel cleaning pipeline and\nencompasses a series of deduplication mechanisms that together ensure the\nintegrity of both document and paragraph boundaries. Additionally, we maintain\nboth the source web page URL and the WARC shard origin URL in order to complain\nwith EU regulations. esCorpius has been released under CC BY-NC-ND 4.0 license\nand is available on HuggingFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Fernandez_D/0/1/0/all/0/1\">David P&#xe9;rez-Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griol_D/0/1/0/all/0/1\">David Griol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callejas_Z/0/1/0/all/0/1\">Zoraida Callejas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"MultiEarth 2022 -- The Champion Solution for Image-to-Image Translation Challenge via Generation Models. (arXiv:2207.00001v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00001","description":"<p>The MultiEarth 2022 Image-to-Image Translation challenge provides a\nwell-constrained test bed for generating the corresponding RGB Sentinel-2\nimagery with the given Sentinel-1 VV &amp; VH imagery. In this challenge, we\ndesigned various generation models and found the SPADE [1] and pix2pixHD [2]\nmodels could perform our best results. In our self-evaluation, the SPADE-2\nmodel with L1-loss can achieve 0.02194 MAE score and 31.092 PSNR dB. In our\nfinal submission, the best model can achieve 0.02795 MAE score ranked No.1 on\nthe leader board.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gou_Y/0/1/0/all/0/1\">Yuchuan Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bo Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jui-Hsin Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multi-stage Framework with Mean Subspace Computation and Recursive Feedback for Online Unsupervised Domain Adaptation. (arXiv:2207.00003v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00003","description":"<p>In this paper, we address the Online Unsupervised Domain Adaptation (OUDA)\nproblem and propose a novel multi-stage framework to solve real-world\nsituations when the target data are unlabeled and arriving online sequentially\nin batches. To project the data from the source and the target domains to a\ncommon subspace and manipulate the projected data in real-time, our proposed\nframework institutes a novel method, called an Incremental Computation of\nMean-Subspace (ICMS) technique, which computes an approximation of mean-target\nsubspace on a Grassmann manifold and is proven to be a close approximate to the\nKarcher mean. Furthermore, the transformation matrix computed from the\nmean-target subspace is applied to the next target data in the\nrecursive-feedback stage, aligning the target data closer to the source domain.\nThe computation of transformation matrix and the prediction of next-target\nsubspace leverage the performance of the recursive-feedback stage by\nconsidering the cumulative temporal dependency among the flow of the target\nsubspace on the Grassmann manifold. The labels of the transformed target data\nare predicted by the pre-trained source classifier, then the classifier is\nupdated by the transformed data and predicted labels. Extensive experiments on\nsix datasets were conducted to investigate in depth the effect and contribution\nof each stage in our proposed framework and its performance over previous\napproaches in terms of classification accuracy and computational speed. In\naddition, the experiments on traditional manifold-based learning models and\nneural-network-based learning models demonstrated the applicability of our\nproposed framework for various types of learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1\">Jihoon Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Debasmit Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">C. S. George Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correction Algorithm of Sampling Effect and Its Application. (arXiv:2207.00004v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2207.00004","description":"<p>The sampling effect of the imaging acquisition device is long considered to\nbe a modulation process of the input signal, introducing additional error into\nthe signal acquisition process. This paper proposes a correction algorithm for\nthe modulation process that solves the sampling effect with high accuracy. We\nexamine the algorithm with perfect continuous Gaussian images and selected\ndigitized images, which indicate an accuracy increase of 106 for Gaussian\nimages, 102 at 15 times of Shannon interpolation for digitized images, and 105\nat 101 times of Shannon interpolation for digitized images. The accuracy limit\nof the Gaussian image comes from the truncation error, while the accuracy limit\nof the digitized images comes from their finite resolution, which can be\nimproved by increasing the time of Shannon interpolation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_Y/0/1/0/all/0/1\">Yunqi Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zhou_J/0/1/0/all/0/1\">Jianfeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class Impression for Data-free Incremental Learning. (arXiv:2207.00005v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00005","description":"<p>Standard deep learning-based classification approaches require collecting all\nsamples from all classes in advance and are trained offline. This paradigm may\nnot be practical in real-world clinical applications, where new classes are\nincrementally introduced through the addition of new data. Class incremental\nlearning is a strategy allowing learning from such data. However, a major\nchallenge is catastrophic forgetting, i.e., performance degradation on previous\nclasses when adapting a trained model to new data. Prior methodologies to\nalleviate this challenge save a portion of training data require perpetual\nstorage of such data that may introduce privacy issues. Here, we propose a\nnovel data-free class incremental learning framework that first synthesizes\ndata from the model trained on previous classes to generate a \\ours.\nSubsequently, it updates the model by combining the synthesized data with new\nclass data. Furthermore, we incorporate a cosine normalized Cross-entropy loss\nto mitigate the adverse effects of the imbalance, a margin loss to increase\nseparation among previous classes and new ones, and an intra-domain contrastive\nloss to generalize the model trained on the synthesized data to real data. We\ncompare our proposed framework with state-of-the-art methods in class\nincremental learning, where we demonstrate improvement in accuracy for the\nclassification of 11,062 echocardiography cine series of patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayromlou_S/0/1/0/all/0/1\">Sana Ayromlou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abolmaesumi_P/0/1/0/all/0/1\">Purang Abolmaesumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_T/0/1/0/all/0/1\">Teresa Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxiao Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaserMix for Semi-Supervised LiDAR Semantic Segmentation. (arXiv:2207.00026v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00026","description":"<p>Densely annotating LiDAR point clouds is costly, which restrains the\nscalability of fully-supervised learning methods. In this work, we study the\nunderexplored semi-supervised learning (SSL) in LiDAR segmentation. Our core\nidea is to leverage the strong spatial cues of LiDAR point clouds to better\nexploit unlabeled data. We propose LaserMix to mix laser beams from different\nLiDAR scans, and then encourage the model to make consistent and confident\npredictions before and after mixing. Our framework has three appealing\nproperties: 1) Generic: LaserMix is agnostic to LiDAR representations (e.g.,\nrange view and voxel), and hence our SSL framework can be universally applied.\n2) Statistically grounded: We provide a detailed analysis to theoretically\nexplain the applicability of the proposed framework. 3) Effective:\nComprehensive experimental analysis on popular LiDAR segmentation datasets\n(nuScenes, SemanticKITTI, and ScribbleKITTI) demonstrates our effectiveness and\nsuperiority. Notably, we achieve competitive results over fully-supervised\ncounterparts with 2x to 5x fewer labels and improve the supervised-only\nbaseline significantly by 10.8% on average. We hope this concise yet\nhigh-performing framework could facilitate future research in semi-supervised\nLiDAR segmentation. Code will be publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1\">Lingdong Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jiawei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Ziwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Image Synthesis via Diffusion Models. (arXiv:2207.00050v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00050","description":"<p>Denoising Diffusion Probabilistic Models (DDPMs) have achieved remarkable\nsuccess in various image generation tasks compared with Generative Adversarial\nNets (GANs). Recent work on semantic image synthesis mainly follows the\n\\emph{de facto} GAN-based approaches, which may lead to unsatisfactory quality\nor diversity of generated images. In this paper, we propose a novel framework\nbased on DDPM for semantic image synthesis. Unlike previous conditional\ndiffusion model directly feeds the semantic layout and noisy image as input to\na U-Net structure, which may not fully leverage the information in the input\nsemantic mask, our framework processes semantic layout and noisy image\ndifferently. It feeds noisy image to the encoder of the U-Net structure while\nthe semantic layout to the decoder by multi-layer spatially-adaptive\nnormalization operators. To further improve the generation quality and semantic\ninterpretability in semantic image synthesis, we introduce the classifier-free\nguidance sampling strategy, which acknowledge the scores of an unconditional\nmodel for sampling process. Extensive experiments on three benchmark datasets\ndemonstrate the effectiveness of our proposed method, achieving\nstate-of-the-art performance in terms of fidelity~(FID) and diversity~(LPIPS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00052","description":"<p>A powerful paradigm for sensorimotor control is to predict actions from\nobservations directly. Training such an end-to-end system allows\nrepresentations that are useful for the downstream tasks to emerge\nautomatically. In visual navigation, an agent can learn to navigate without any\nmanual designs by correlating how its views change with the actions being\ntaken. However, the lack of inductive bias makes this system data-inefficient\nand impractical in scenarios like search and rescue, where interacting with the\nenvironment to collect data is costly. We hypothesize a sufficient\nrepresentation of the current view and the goal view for a navigation policy\ncan be learned by predicting the location and size of a crop of the current\nview that corresponds to the goal. We further show that training such random\ncrop prediction in a self-supervised fashion purely on random noise images\ntransfers well to natural home images. The learned representation can then be\nbootstrapped to learn a navigation policy efficiently with little interaction\ndata. Code is available at https://github.com/yanweiw/noise2ptz.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1\">Ching-Yun Ko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models. (arXiv:2207.00056v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00056","description":"<p>The promise of multimodal models for real-world applications has inspired\nresearch in visualizing and understanding their internal mechanics with the end\ngoal of empowering stakeholders to visualize model behavior, perform model\ndebugging, and promote trust in machine learning models. However, modern\nmultimodal models are typically black-box neural networks, which makes it\nchallenging to understand their internal mechanics. How can we visualize the\ninternal modeling of multimodal interactions in these models? Our paper aims to\nfill this gap by proposing MultiViz, a method for analyzing the behavior of\nmultimodal models by scaffolding the problem of interpretability into 4 stages:\n(1) unimodal importance: how each modality contributes towards downstream\nmodeling and prediction, (2) cross-modal interactions: how different modalities\nrelate with each other, (3) multimodal representations: how unimodal and\ncross-modal interactions are represented in decision-level features, and (4)\nmultimodal prediction: how decision-level features are composed to make a\nprediction. MultiViz is designed to operate on diverse modalities, models,\ntasks, and research areas. Through experiments on 8 trained models across 6\nreal-world tasks, we show that the complementary stages in MultiViz together\nenable users to (1) simulate model predictions, (2) assign interpretable\nconcepts to features, (3) perform error analysis on model misclassifications,\nand (4) use insights from error analysis to debug models. MultiViz is publicly\navailable, will be regularly updated with new interpretation tools and metrics,\nand welcomes inputs from the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1\">Yiwei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nihal Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1\">Zihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1\">Louis-Philippe Morency</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Unsupervised Domain Adaptation for Semantic Segmentation. (arXiv:2207.00067v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00067","description":"<p>Unsupervised domain adaptation (UDA) adapts a model trained on one domain to\na novel domain using only unlabeled data. So many studies have been conducted,\nespecially for semantic segmentation due to its high annotation cost. The\nexisting studies stick to the basic assumption that no labeled sample is\navailable for the new domain. However, this assumption has several issues.\nFirst, it is pretty unrealistic, considering the standard practice of ML to\nconfirm the model's performance before its deployment; the confirmation needs\nlabeled data. Second, any UDA method will have a few hyper-parameters, needing\na certain amount of labeled data. To rectify this misalignment with reality, we\nrethink UDA from a data-centric point of view. Specifically, we start with the\nassumption that we do have access to a minimum level of labeled data. Then, we\nask how many labeled samples are necessary for finding satisfactory\nhyper-parameters of existing UDA methods. How well does it work if we use the\nsame data to train the model, e.g., finetuning? We conduct experiments to\nanswer these questions with popular scenarios, {GTA5,\nSYNTHIA}$\\rightarrow$Cityscapes. Our findings are as follows: i) for some UDA\nmethods, good hyper-parameters can be found with only a few labeled samples\n(i.e., images), e.g., five, but this does not apply to others, and ii)\nfinetuning outperforms most existing UDA methods with only ten labeled images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Periodic Systolic Dataflow for Lowering Latency and Power Dissipation of Convolutional Neural Network Accelerators. (arXiv:2207.00068v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00068","description":"<p>This paper introduces the sparse periodic systolic (SPS) dataflow, which\nadvances the state-of-the-art hardware accelerator for supporting lightweight\nneural networks. Specifically, the SPS dataflow enables a novel hardware design\napproach unlocked by an emergent pruning scheme, periodic pattern-based\nsparsity (PPS). By exploiting the regularity of PPS, our sparsity-aware\ncompiler optimally reorders the weights and uses a simple indexing unit in\nhardware to create matches between the weights and activations. Through the\ncompiler-hardware codesign, SPS dataflow enjoys higher degrees of parallelism\nwhile being free of the high indexing overhead and without model accuracy loss.\nEvaluated on popular benchmarks such as VGG and ResNet, the SPS dataflow and\naccompanying neural network compiler outperform prior work in convolutional\nneural network (CNN) accelerator designs targeting FPGA devices. Against other\nsparsity-supporting weight storage formats, SPS results in 4.49x energy\nefficiency gain while lowering storage requirements by 3.67x for total weight\nstorage (non-pruned weights plus indexing) and 22,044x for indexing memory.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Jung Hwan Heo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fayyazi_A/0/1/0/all/0/1\">Arash Fayyazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esmaili_A/0/1/0/all/0/1\">Amirhossein Esmaili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1\">Massoud Pedram</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stain-free, rapid, and quantitative viral plaque assay using deep learning and holography. (arXiv:2207.00089v1 [physics.ins-det])","link":"http://arxiv.org/abs/2207.00089","description":"<p>Plaque assay is the gold standard method for quantifying the concentration of\nreplication-competent lytic virions. Expediting and automating viral plaque\nassays will significantly benefit clinical diagnosis, vaccine development, and\nthe production of recombinant proteins or antiviral agents. Here, we present a\nrapid and stain-free quantitative viral plaque assay using lensfree holographic\nimaging and deep learning. This cost-effective, compact, and automated device\nsignificantly reduces the incubation time needed for traditional plaque assays\nwhile preserving their advantages over other virus quantification methods. This\ndevice captures ~0.32 Giga-pixel/hour phase information of the objects per test\nwell, covering an area of ~30x30 mm^2, in a label-free manner, eliminating\nstaining entirely. We demonstrated the success of this computational method\nusing Vero E6 cells and vesicular stomatitis virus. Using a neural network,\nthis stain-free device automatically detected the first cell lysing events due\nto the viral replication as early as 5 hours after the incubation, and achieved\n&gt;90% detection rate for the plaque-forming units (PFUs) with 100% specificity\nin &lt;20 hours, providing major time savings compared to the traditional plaque\nassays that take ~48 hours or more. This data-driven plaque assay also offers\nthe capability of quantifying the infected area of the cell monolayer,\nperforming automated counting and quantification of PFUs and virus-infected\nareas over a 10-fold larger dynamic range of virus concentration than standard\nviral plaque assays. This compact, low-cost, automated PFU quantification\ndevice can be broadly used in virology research, vaccine development, and\nclinical applications\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Liu_T/0/1/0/all/0/1\">Tairan Liu</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1\">Yuzhu Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Koydemir_H/0/1/0/all/0/1\">Hatice Ceylan Koydemir</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Zhang_Y/0/1/0/all/0/1\">Yijie Zhang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_E/0/1/0/all/0/1\">Ethan Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Wang_H/0/1/0/all/0/1\">Hongda Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Li_J/0/1/0/all/0/1\">Jingxi Li</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Bai_B/0/1/0/all/0/1\">Bijie Bai</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ozcan_A/0/1/0/all/0/1\">Aydogan Ozcan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology. (arXiv:2207.00095v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00095","description":"<p>Current approaches for classification of whole slide images (WSI) in digital\npathology predominantly utilize a two-stage learning pipeline. The first stage\nidentifies areas of interest (e.g. tumor tissue), while the second stage\nprocesses cropped tiles from these areas in a supervised fashion. During\ninference, a large number of tiles are combined into a unified prediction for\nthe entire slide. A major drawback of such approaches is the requirement for\ntask-specific auxiliary labels which are not acquired in clinical routine. We\npropose a novel learning pipeline for WSI classification that is trainable\nend-to-end and does not require any auxiliary annotations. We apply our\napproach to predict molecular alterations for a number of different use-cases,\nincluding detection of microsatellite instability in colorectal tumors and\nprediction of specific mutations for colon, lung, and breast cancer cases from\nThe Cancer Genome Atlas. Results reach AUC scores of up to 94% and are shown to\nbe competitive with state of the art two-stage pipelines. We believe our\napproach can facilitate future research in digital pathology and contribute to\nsolve a large range of problems around the prediction of cancer phenotypes,\nhopefully enabling personalized therapies for more patients in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Teichmann_M/0/1/0/all/0/1\">Marvin Teichmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aichert_A/0/1/0/all/0/1\">Andre Aichert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bohnenberger_H/0/1/0/all/0/1\">Hanibal Bohnenberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Strobel_P/0/1/0/all/0/1\">Philipp Str&#xf6;bel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heimann_T/0/1/0/all/0/1\">Tobias Heimann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation. (arXiv:2207.00106v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00106","description":"<p>Parkinson's disease (PD) is a neurological disorder that has a variety of\nobservable motor-related symptoms such as slow movement, tremor, muscular\nrigidity, and impaired posture. PD is typically diagnosed by evaluating the\nseverity of motor impairments according to scoring systems such as the Movement\nDisorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS).\nAutomated severity prediction using video recordings of individuals provides a\npromising route for non-intrusive monitoring of motor impairments. However, the\nlimited size of PD gait data hinders model ability and clinical potential.\nBecause of this clinical data scarcity and inspired by the recent advances in\nself-supervised large-scale language models like GPT-3, we use human motion\nforecasting as an effective self-supervised pre-training task for the\nestimation of motor impairment severity. We introduce GaitForeMer, Gait\nForecasting and impairment estimation transforMer, which is first pre-trained\non public datasets to forecast gait movements and then applied to clinical data\nto predict MDS-UPDRS gait impairment severity. Our method outperforms previous\napproaches that rely solely on clinical data by a large margin, achieving an F1\nscore of 0.76, precision of 0.79, and recall of 0.75. Using GaitForeMer, we\nshow how public human movement data repositories can assist clinical use cases\nthrough learning universal motion representations. The code is available at\nhttps://github.com/markendo/GaitForeMer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Endo_M/0/1/0/all/0/1\">Mark Endo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poston_K/0/1/0/all/0/1\">Kathleen L. Poston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches. (arXiv:2207.00113v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00113","description":"<p>Surgical captioning plays an important role in surgical instruction\nprediction and report generation. However, the majority of captioning models\nstill rely on the heavy computational object detector or feature extractor to\nextract regional features. In addition, the detection model requires additional\nbounding box annotation which is costly and needs skilled annotators. These\nlead to inference delay and limit the captioning model to deploy in real-time\nrobotic surgery. For this purpose, we design an end-to-end detector and feature\nextractor-free captioning model by utilizing the patch-based shifted window\ntechnique. We propose Shifted Window-Based Multi-Layer Perceptrons Transformer\nCaptioning model (SwinMLP-TranCAP) with faster inference speed and less\ncomputation. SwinMLP-TranCAP replaces the multi-head attention module with\nwindow-based multi-head MLP. Such deployments primarily focus on image\nunderstanding tasks, but very few works investigate the caption generation\ntask. SwinMLP-TranCAP is also extended into a video version for video\ncaptioning tasks using 3D patches and windows. Compared with previous\ndetector-based or feature extractor-based models, our models greatly simplify\nthe architecture design while maintaining performance on two surgical datasets.\nThe code is publicly available at\nhttps://github.com/XuMengyaAmy/SwinMLP_TranCAP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mengya Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Mobarakol Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Hongliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProSelfLC: Progressive Self Label Correction Towards A Low-Temperature Entropy State. (arXiv:2207.00118v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00118","description":"<p>To train robust deep neural networks (DNNs), we systematically study several\ntarget modification approaches, which include output regularisation, self and\nnon-self label correction (LC). Three key issues are discovered: (1) Self LC is\nthe most appealing as it exploits its own knowledge and requires no extra\nmodels. However, how to automatically decide the trust degree of a learner as\ntraining goes is not well answered in the literature. (2) Some methods penalise\nwhile the others reward low-entropy predictions, prompting us to ask which one\nis better. (3) Using the standard training setting, a trained network is of low\nconfidence when severe noise exists, making it hard to leverage its\nhigh-entropy self knowledge.\n</p>\n<p>To resolve the issue (1), taking two well-accepted propositions--deep neural\nnetworks learn meaningful patterns before fitting noise and minimum entropy\nregularisation principle--we propose a novel end-to-end method named ProSelfLC,\nwhich is designed according to learning time and entropy. Specifically, given a\ndata point, we progressively increase trust in its predicted label distribution\nversus its annotated one if a model has been trained for enough time and the\nprediction is of low entropy (high confidence). For the issue (2), according to\nProSelfLC, we empirically prove that it is better to redefine a meaningful\nlow-entropy status and optimise the learner toward it. This serves as a defence\nof entropy minimisation. To address the issue (3), we decrease the entropy of\nself knowledge using a low temperature before exploiting it to correct labels,\nso that the revised labels redefine a low-entropy target state.\n</p>\n<p>We demonstrate the effectiveness of ProSelfLC through extensive experiments\nin both clean and noisy settings, and on both image and protein datasets.\nFurthermore, our source code is available at\nhttps://github.com/XinshaoAmosWang/ProSelfLC-AT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinshao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yang Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodirov_E/0/1/0/all/0/1\">Elyor Kodirov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Sankha Subhra Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1\">David A. Clifton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1\">Neil M. Robertson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos. (arXiv:2207.00141v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00141","description":"<p>Breast lesion detection in ultrasound is critical for breast cancer\ndiagnosis. Existing methods mainly rely on individual 2D ultrasound images or\ncombine unlabeled video and labeled 2D images to train models for breast lesion\ndetection. In this paper, we first collect and annotate an ultrasound video\ndataset (188 videos) for breast lesion detection. Moreover, we propose a\nclip-level and video-level feature aggregated network (CVA-Net) for addressing\nbreast lesion detection in ultrasound videos by aggregating video-level lesion\nclassification features and clip-level temporal features. The clip-level\ntemporal features encode local temporal information of ordered video frames and\nglobal temporal information of shuffled video frames. In our CVA-Net, an\ninter-video fusion module is devised to fuse local features from original video\nframes and global features from shuffled video frames, and an intra-video\nfusion module is devised to learn the temporal information among adjacent video\nframes. Moreover, we learn video-level features to classify the breast lesions\nof the original video as benign or malignant lesions to further enhance the\nfinal breast lesion detection performance in ultrasound videos. Experimental\nresults on our annotated dataset demonstrate that our CVA-Net clearly\noutperforms state-of-the-art methods. The corresponding code and dataset are\npublicly available at \\url{https://github.com/jhl-Det/CVA-Net}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zhi Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Junhao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1\">Lei Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jing Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Liansheng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ChrSNet: Chromosome Straightening using Self-attention Guided Networks. (arXiv:2207.00147v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00147","description":"<p>Karyotyping is an important procedure to assess the possible existence of\nchromosomal abnormalities. However, because of the non-rigid nature,\nchromosomes are usually heavily curved in microscopic images and such deformed\nshapes hinder the chromosome analysis for cytogeneticists. In this paper, we\npresent a self-attention guided framework to erase the curvature of\nchromosomes. The proposed framework extracts spatial information and local\ntextures to preserve banding patterns in a regression module. With\ncomplementary information from the bent chromosome, a refinement module is\ndesigned to further improve fine details. In addition, we propose two dedicated\ngeometric constraints to maintain the length and restore the distortion of\nchromosomes. To train our framework, we create a synthetic dataset where curved\nchromosomes are generated from the real-world straight chromosomes by\ngrid-deformation. Quantitative and qualitative experiments are conducted on\nsynthetic and real-world data. Experimental results show that our proposed\nmethod can effectively straighten bent chromosomes while keeping banding\ndetails and length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingxiong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shui_Z/0/1/0/all/0/1\">Zhongyi Shui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenglu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunlong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pingyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models. (arXiv:2207.00156v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00156","description":"<p>We aim to quantitatively measure the practical usability of medical image\nsegmentation models: to what extent, how often, and on which samples a model's\npredictions can be used/trusted. We first propose a measure,\nCorrectness-Confidence Rank Correlation (CCRC), to capture how predictions'\nconfidence estimates correlate with their correctness scores in rank. A model\nwith a high value of CCRC means its prediction confidences reliably suggest\nwhich samples' predictions are more likely to be correct. Since CCRC does not\ncapture the actual prediction correctness, it alone is insufficient to indicate\nwhether a prediction model is both accurate and reliable to use in practice.\nTherefore, we further propose another method, Usable Region Estimate (URE),\nwhich simultaneously quantifies predictions' correctness and reliability of\nconfidence assessments in one estimate. URE provides concrete information on to\nwhat extent a model's predictions are usable. In addition, the sizes of usable\nregions (UR) can be utilized to compare models: A model with a larger UR can be\ntaken as a more usable and hence better model. Experiments on six datasets\nvalidate that the proposed evaluation methods perform well, providing a\nconcrete and concise measure for the practical usability of medical image\nsegmentation models. Code is made available at\nhttps://github.com/yizhezhang2000/ure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mishra_S/0/1/0/all/0/1\">Suraj Mishra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_P/0/1/0/all/0/1\">Peixian Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_D/0/1/0/all/0/1\">Danny Z. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Disease Classification Performance and Explainability of Deep Learning Models in Radiology with Heatmap Generators. (arXiv:2207.00157v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00157","description":"<p>As deep learning is widely used in the radiology field, the explainability of\nsuch models is increasingly becoming essential to gain clinicians' trust when\nusing the models for diagnosis. In this research, three experiment sets were\nconducted with a U-Net architecture to improve the classification performance\nwhile enhancing the heatmaps corresponding to the model's focus through\nincorporating heatmap generators during training. All of the experiments used\nthe dataset that contained chest radiographs, associated labels from one of the\nthree conditions (\"normal\", \"congestive heart failure (CHF)\", and \"pneumonia\"),\nand numerical information regarding a radiologist's eye-gaze coordinates on the\nimages. The paper (A. Karargyris and Moradi, 2021) that introduced this dataset\ndeveloped a U-Net model, which was treated as the baseline model for this\nresearch, to show how the eye-gaze data can be used in multi-modal training for\nexplainability improvement. To compare the classification performances, the 95%\nconfidence intervals (CI) of the area under the receiver operating\ncharacteristic curve (AUC) were measured. The best method achieved an AUC of\n0.913 (CI: 0.860-0.966). The greatest improvements were for the \"pneumonia\" and\n\"CHF\" classes, which the baseline model struggled most to classify, resulting\nin AUCs of 0.859 (CI: 0.732-0.957) and 0.962 (CI: 0.933-0.989), respectively.\nThe proposed method's decoder was also able to produce probability masks that\nhighlight the determining image parts in model classifications, similarly as\nthe radiologist's eye-gaze data. Hence, this work showed that incorporating\nheatmap generators and eye-gaze information into training can simultaneously\nimprove disease classification and provide explainable visuals that align well\nwith how the radiologist viewed the chest radiographs when making diagnosis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_A/0/1/0/all/0/1\">Akino Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ketabi_S/0/1/0/all/0/1\">Sara Ketabi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khashayar/0/1/0/all/0/1\">Khashayar</a> (Ernest) <a href=\"http://arxiv.org/find/eess/1/au:+Namdar/0/1/0/all/0/1\">Namdar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khalvati_F/0/1/0/all/0/1\">Farzad Khalvati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Presentation Attack using DCGAN and Deep CNN. (arXiv:2207.00161v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00161","description":"<p>Biometric based authentication is currently playing an essential role over\nconventional authentication system; however, the risk of presentation attacks\nsubsequently rising. Our research aims at identifying the areas where\npresentation attack can be prevented even though adequate biometric image\nsamples of users are limited. Our work focusses on generating photorealistic\nsynthetic images from the real image sets by implementing Deep Convolution\nGenerative Adversarial Net (DCGAN). We have implemented the temporal and\nspatial augmentation during the fake image generation. Our work detects the\npresentation attacks on facial and iris images using our deep CNN, inspired by\nVGGNet [1]. We applied the deep neural net techniques on three different\nbiometric image datasets, namely MICHE I [2], VISOB [3], and UBIPr [4]. The\ndatasets, used in this research, contain images that are captured both in\ncontrolled and uncontrolled environment along with different resolutions and\nsizes. We obtained the best test accuracy of 97% on UBI-Pr [4] Iris datasets.\nFor MICHE-I [2] and VISOB [3] datasets, we achieved the test accuracies of 95%\nand 96% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddiqui_N/0/1/0/all/0/1\">Nyle Siddiqui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dave_R/0/1/0/all/0/1\">Rushit Dave</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Optical Coding Design in Computational Imaging. (arXiv:2207.00164v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00164","description":"<p>Computational optical imaging (COI) systems leverage optical coding elements\n(CE) in their setups to encode a high-dimensional scene in a single or multiple\nsnapshots and decode it by using computational algorithms. The performance of\nCOI systems highly depends on the design of its main components: the CE pattern\nand the computational method used to perform a given task. Conventional\napproaches rely on random patterns or analytical designs to set the\ndistribution of the CE. However, the available data and algorithm capabilities\nof deep neural networks (DNNs) have opened a new horizon in CE data-driven\ndesigns that jointly consider the optical encoder and computational decoder.\nSpecifically, by modeling the COI measurements through a fully differentiable\nimage formation model that considers the physics-based propagation of light and\nits interaction with the CEs, the parameters that define the CE and the\ncomputational decoder can be optimized in an end-to-end (E2E) manner. Moreover,\nby optimizing just CEs in the same framework, inference tasks can be performed\nfrom pure optics. This work surveys the recent advances on CE data-driven\ndesign and provides guidelines on how to parametrize different optical elements\nto include them in the E2E framework. Since the E2E framework can handle\ndifferent inference applications by changing the loss function and the DNN, we\npresent low-level tasks such as spectral imaging reconstruction or high-level\ntasks such as pose estimation with privacy preserving enhanced by using optimal\ntask-based optical architectures. Finally, we illustrate classification and 3D\nobject recognition applications performed at the speed of the light using\nall-optics DNN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arguello_H/0/1/0/all/0/1\">Henry Arguello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacca_J/0/1/0/all/0/1\">Jorge Bacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariyawasam_H/0/1/0/all/0/1\">Hasindu Kariyawasam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_E/0/1/0/all/0/1\">Edwin Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marquez_M/0/1/0/all/0/1\">Miguel Marquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hettiarachchi_R/0/1/0/all/0/1\">Ramith Hettiarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_H/0/1/0/all/0/1\">Hans Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herath_K/0/1/0/all/0/1\">Kithmini Herath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haputhanthri_U/0/1/0/all/0/1\">Udith Haputhanthri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahluwalia_B/0/1/0/all/0/1\">Balpreet Singh Ahluwalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+So_P/0/1/0/all/0/1\">Peter So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadduwage_D/0/1/0/all/0/1\">Dushan N. Wadduwage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edussooriya_C/0/1/0/all/0/1\">Chamira U. S. Edussooriya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An adaptive bi-objective optimization algorithm for the satellite image data downlink scheduling problem considering request split. (arXiv:2207.00168v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00168","description":"<p>The satellite image data downlink scheduling problem (SIDSP) is well studied\nin literature for traditional satellites. With recent developments in satellite\ntechnology, SIDSP for modern satellites became more complicated, adding new\ndimensions of complexities and additional opportunities for the effective use\nof the satellite. In this paper, we introduce the dynamic two-phase satellite\nimage data downlink scheduling problem (D-SIDSP) which combines two interlinked\noperations of image data segmentation and image data downlink, in a dynamic\nway, and thereby offering additional modelling flexibility and renewed\ncapabilities. D-SIDSP is formulated as a bi-objective problem of optimizing the\nimage data transmission rate and the service-balance degree. Harnessing the\npower of an adaptive large neighborhood search algorithm (ALNS) with a\nnondominated sorting genetic algorithm II (NSGA-II), an adaptive bi-objective\nmemetic algorithm, ALNS+NSGA-II, is developed to solve D-SIDSP. Results of\nextensive computational experiments carried out using benchmark instances are\nalso presented. Our experimental results disclose that the algorithm\nALNS+NSGA-II is a viable alternative to solve D-SIDSP more efficiently and\ndemonstrates superior outcomes based on various performance metrics. The paper\nalso offers new benchmark instances for D-SIDSP that can be used in future\nresearch works on the topic.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1\">Zhongxiang Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punnen_A/0/1/0/all/0/1\">Abraham P. Punnen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhongbao Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TENET: Transformer Encoding Network for Effective Temporal Flow on Motion Prediction. (arXiv:2207.00170v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00170","description":"<p>This technical report presents an effective method for motion prediction in\nautonomous driving. We develop a Transformer-based method for input encoding\nand trajectory prediction. Besides, we propose the Temporal Flow Header to\nenhance the trajectory encoding. In the end, an efficient K-means ensemble\nmethod is used. Using our Transformer network and ensemble method, we win the\nfirst place of Argoverse 2 Motion Forecasting Challenge with the\nstate-of-the-art brier-minFDE score of 1.90.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hangning Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhigang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1\">Chen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Huadong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chaofei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yizhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhenting Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shiyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuefeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Ziyao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Turbo: Opportunistic Enhancement for Edge Video Analytics. (arXiv:2207.00172v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00172","description":"<p>Edge computing is being widely used for video analytics. To alleviate the\ninherent tension between accuracy and cost, various video analytics pipelines\nhave been proposed to optimize the usage of GPU on edge nodes. Nonetheless, we\nfind that GPU compute resources provisioned for edge nodes are commonly\nunder-utilized due to video content variations, subsampling and filtering at\ndifferent places of a pipeline. As opposed to model and pipeline optimization,\nin this work, we study the problem of opportunistic data enhancement using the\nnon-deterministic and fragmented idle GPU resources. In specific, we propose a\ntask-specific discrimination and enhancement module and a model-aware\nadversarial training mechanism, providing a way to identify and transform\nlow-quality images that are specific to a video pipeline in an accurate and\nefficient manner. A multi-exit model structure and a resource-aware scheduler\nis further developed to make online enhancement decisions and fine-grained\ninference execution under latency and GPU resource constraints. Experiments\nacross multiple video analytics pipelines and datasets reveal that by\njudiciously allocating a small amount of idle resources on frames that tend to\nyield greater marginal benefits from enhancement, our system boosts DNN object\ndetection accuracy by $7.3-11.3\\%$ without incurring any latency costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shiqi Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1\">Ting Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yuanchao Shu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end cell recognition by point annotation. (arXiv:2207.00176v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00176","description":"<p>Reliable quantitative analysis of immunohistochemical staining images\nrequires accurate and robust cell detection and classification. Recent\nweakly-supervised methods usually estimate probability density maps for cell\nrecognition. However, in dense cell scenarios, their performance can be limited\nby pre- and post-processing as it is impossible to find a universal parameter\nsetting. In this paper, we introduce an end-to-end framework that applies\ndirect regression and classification for preset anchor points. Specifically, we\npropose a pyramidal feature aggregation strategy to combine low-level features\nand high-level semantics simultaneously, which provides accurate cell\nrecognition for our purely point-based model. In addition, an optimized cost\nfunction is designed to adapt our multi-task learning framework by matching\nground truth and predicted points. The experimental results demonstrate the\nsuperior accuracy and efficiency of the proposed method, which reveals the high\npotentiality in assisting pathologist assessments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shui_Z/0/1/0/all/0/1\">Zhongyi Shui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shichuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenglu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingchuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pingyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sunyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Motion Network for Freehand 3D Ultrasound Reconstruction. (arXiv:2207.00177v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00177","description":"<p>Freehand 3D ultrasound (US) has important clinical value due to its low cost\nand unrestricted field of view. Recently deep learning algorithms have removed\nits dependence on bulky and expensive external positioning devices. However,\nimproving reconstruction accuracy is still hampered by difficult elevational\ndisplacement estimation and large cumulative drift. In this context, we propose\na novel deep motion network (MoNet) that integrates images and a lightweight\nsensor known as the inertial measurement unit (IMU) from a velocity perspective\nto alleviate the obstacles mentioned above. Our contribution is two-fold.\nFirst, we introduce IMU acceleration for the first time to estimate elevational\ndisplacements outside the plane. We propose a temporal and multi-branch\nstructure to mine the valuable information of low signal-to-noise ratio (SNR)\nacceleration. Second, we propose a multi-modal online self-supervised strategy\nthat leverages IMU information as weak labels for adaptive optimization to\nreduce drift errors and further ameliorate the impacts of acceleration noise.\nExperiments show that our proposed method achieves the superior reconstruction\nperformance, exceeding state-of-the-art methods across the board.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongzhang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1\">Liwei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Monocular Disparity Estimation for Single-View Reconstruction. (arXiv:2207.00182v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00182","description":"<p>We present a fine-tuning method to improve the appearance of 3D geometries\nreconstructed from single images. We leverage advances in monocular depth\nestimation to obtain disparity maps and present a novel approach to\ntransforming 2D normalized disparity maps into 3D point clouds by solving an\noptimization on the relevant camera parameters, After creating a 3D point cloud\nfrom disparity, we introduce a method to combine the new point cloud with\nexisting information to form a more faithful and detailed final geometry. We\ndemonstrate the efficacy of our approach with multiple experiments on both\nsynthetic and real images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chanlatte_M/0/1/0/all/0/1\">Marissa Ramirez de Chanlatte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadelha_M/0/1/0/all/0/1\">Matheus Gadelha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groueix_T/0/1/0/all/0/1\">Thibault Groueix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mech_R/0/1/0/all/0/1\">Radomir Mech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MMFN: Multi-Modal-Fusion-Net for End-to-End Driving. (arXiv:2207.00186v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00186","description":"<p>Inspired by the fact that humans use diverse sensory organs to perceive the\nworld, sensors with different modalities are deployed in end-to-end driving to\nobtain the global context of the 3D scene. In previous works, camera and LiDAR\ninputs are fused through transformers for better driving performance. These\ninputs are normally further interpreted as high-level map information to assist\nnavigation tasks. Nevertheless, extracting useful information from the complex\nmap input is challenging, for redundant information may mislead the agent and\nnegatively affect driving performance. We propose a novel approach to\nefficiently extract features from vectorized High-Definition (HD) maps and\nutilize them in the end-to-end driving tasks. In addition, we design a new\nexpert to further enhance the model performance by considering multi-road\nrules. Experimental results prove that both of the proposed improvements enable\nour agent to achieve superior performance compared with other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Mingkai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1\">Ruoyu Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feiyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_R/0/1/0/all/0/1\">Ren Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lujia Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Query-Key Pairwise Interactions in Vision Transformers. (arXiv:2207.00188v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00188","description":"<p>Vision Transformers have achieved state-of-the-art performance in many visual\ntasks. Due to the quadratic computational and memory complexities of\nself-attention, recent works either apply attention only to low-resolution\ninputs or restrict the receptive field to a small local region. To overcome\nthese limitations, we propose key-only attention, which excludes query-key\npairwise interactions and uses a compute-efficient saliency-gate to obtain\nattention weights, modeling local-global interactions in all stages. Key-only\nattention has linear computational and memory complexities w.r.t input size. We\nuse alternate layout to hybridize convolution and attention layers instead of\ngrafting which is suggested by previous works, so that all stages can benefit\nfrom both spatial attentions and convolutions. We leverage these improvements\nto develop a new self-attention model family, LinGlos, which reach\nstate-of-the-art accuracies on the parameter-limited setting of ImageNet\nclassification benchmark, and outperform baselines significantly in downstream\ntasks, e.g., COCO object detection and ADE20K semantic segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yangxin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data generation using simulation technology to improve perception mechanism of autonomous vehicles. (arXiv:2207.00191v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00191","description":"<p>Recent advancements in computer graphics technology allow more realistic\nren-dering of car driving environments. They have enabled self-driving car\nsimulators such as DeepGTA-V and CARLA (Car Learning to Act) to generate large\namounts of synthetic data that can complement the existing real-world dataset\nin training autonomous car perception. Furthermore, since self-driving car\nsimulators allow full control of the environment, they can generate dangerous\ndriving scenarios that the real-world dataset lacks such as bad weather and\naccident scenarios. In this paper, we will demonstrate the effectiveness of\ncombining data gathered from the real world with data generated in the\nsimulated world to train perception systems on object detection and\nlocalization task. We will also propose a multi-level deep learning perception\nframework that aims to emulate a human learning experience in which a series of\ntasks from the simple to more difficult ones are learned in a certain domain.\nThe autonomous car perceptron can learn from easy-to-drive scenarios to more\nchallenging ones customized by simulation software.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1\">Minh Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_R/0/1/0/all/0/1\">Ramin Ramezani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reading and Writing: Discriminative and Generative Modeling for Self-Supervised Text Recognition. (arXiv:2207.00193v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00193","description":"<p>Existing text recognition methods usually need large-scale training data.\nMost of them rely on synthetic training data due to the lack of annotated real\nimages. However, there is a domain gap between the synthetic data and real\ndata, which limits the performance of the text recognition models. Recent\nself-supervised text recognition methods attempted to utilize unlabeled real\nimages by introducing contrastive learning, which mainly learns the\ndiscrimination of the text images. Inspired by the observation that humans\nlearn to recognize the texts through both reading and writing, we propose to\nlearn discrimination and generation by integrating contrastive learning and\nmasked image modeling in our self-supervised method. The contrastive learning\nbranch is adopted to learn the discrimination of text images, which imitates\nthe reading behavior of humans. Meanwhile, masked image modeling is firstly\nintroduced for text recognition to learn the context generation of the text\nimages, which is similar to the writing behavior. The experimental results show\nthat our method outperforms previous self-supervised text recognition methods\nby 10.2%-20.2% on irregular scene text recognition datasets. Moreover, our\nproposed text recognizer exceeds previous state-of-the-art text recognition\nmethods by averagely 5.3% on 11 benchmarks, with similar model size. We also\ndemonstrate that our pre-trained model can be easily applied to other\ntext-related tasks with obvious performance gain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Mingkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_M/0/1/0/all/0/1\">Minghui Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1\">Pu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Shenggao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hualin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying the impact of magnitude pruning on contrastive learning methods. (arXiv:2207.00200v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00200","description":"<p>We study the impact of different pruning techniques on the representation\nlearned by deep neural networks trained with contrastive loss functions. Our\nwork finds that at high sparsity levels, contrastive learning results in a\nhigher number of misclassified examples relative to models trained with\ntraditional cross-entropy loss. To understand this pronounced difference, we\nuse metrics such as the number of PIEs (Hooker et al., 2019), Q-Score (Kalibhat\net al., 2022), and PD-Score (Baldock et al., 2021) to measure the impact of\npruning on the learned representation quality. Our analysis suggests the\nschedule of the pruning method implementation matters. We find that the\nnegative impact of sparsity on the quality of the learned representation is the\nhighest when pruning is introduced early on in the training phase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corti_F/0/1/0/all/0/1\">Francesco Corti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Entezari_R/0/1/0/all/0/1\">Rahim Entezari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saukh_O/0/1/0/all/0/1\">Olga Saukh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"e-CLIP: Large-Scale Vision-Language Representation Learning in E-commerce. (arXiv:2207.00208v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00208","description":"<p>Understanding vision and language representations of product content is vital\nfor search and recommendation applications in e-commerce. As a backbone for\nonline shopping platforms and inspired by the recent success in representation\nlearning research, we propose a contrastive learning framework that aligns\nlanguage and visual models using unlabeled raw product text and images. We\npresent techniques we used to train large-scale representation learning models\nand share solutions that address domain-specific challenges. We study the\nperformance using our pre-trained model as backbones for diverse downstream\ntasks, including category classification, attribute extraction, product\nmatching, product clustering, and adult product recognition. Experimental\nresults show that our proposed method outperforms the baseline in each\ndownstream task regarding both single modality and multiple modalities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1\">Wonyoung Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jonghun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_T/0/1/0/all/0/1\">Taekang Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Yongwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1\">Kwangjin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1\">Hwanjun Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Parameterization for Dynamic Human Head Editing. (arXiv:2207.00210v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00210","description":"<p>Implicit radiance functions emerged as a powerful scene representation for\nreconstructing and rendering photo-realistic views of a 3D scene. These\nrepresentations, however, suffer from poor editability. On the other hand,\nexplicit representations such as polygonal meshes allow easy editing but are\nnot as suitable for reconstructing accurate details in dynamic human heads,\nsuch as fine facial features, hair, teeth, and eyes. In this work, we present\nNeural Parameterization (NeP), a hybrid representation that provides the\nadvantages of both implicit and explicit methods. NeP is capable of\nphoto-realistic rendering while allowing fine-grained editing of the scene\ngeometry and appearance. We first disentangle the geometry and appearance by\nparameterizing the 3D geometry into 2D texture space. We enable geometric\neditability by introducing an explicit linear deformation blending layer. The\ndeformation is controlled by a set of sparse key points, which can be\nexplicitly and intuitively displaced to edit the geometry. For appearance, we\ndevelop a hybrid 2D texture consisting of an explicit texture map for easy\nediting and implicit view and time-dependent residuals to model temporal and\nview variations. We compare our method to several reconstruction and editing\nbaselines. The results show that the NeP achieves almost the same level of\nrendering accuracy while maintaining high editability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Li Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1\">Jing Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sander_P/0/1/0/all/0/1\">Pedro Sander</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polarized Color Image Denoising using Pocoformer. (arXiv:2207.00215v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00215","description":"<p>Polarized color photography provides both visual textures and object\nsurficial information in one single snapshot. However, the use of the\ndirectional polarizing filter array causes extremely lower photon count and SNR\ncompared to conventional color imaging. Thus, the feature essentially leads to\nunpleasant noisy images and destroys polarization analysis performance. It is a\nchallenge for traditional image processing pipelines owing to the fact that the\nphysical constraints exerted implicitly in the channels are excessively\ncomplicated. To address this issue, we propose a learning-based approach to\nsimultaneously restore clean signals and precise polarization information. A\nreal-world polarized color image dataset of paired raw short-exposed noisy and\nlong-exposed reference images are captured to support the learning-based\npipeline. Moreover, we embrace the development of vision Transformer and\npropose a hybrid transformer model for the Polarized Color image denoising,\nnamely PoCoformer, for a better restoration performance. Abundant experiments\ndemonstrate the effectiveness of proposed method and key factors that affect\nresults are analyzed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhuoxiao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_H/0/1/0/all/0/1\">Haiyang Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VL-CheckList: Evaluating Pre-trained Vision-Language Models with Objects, Attributes and Relations. (arXiv:2207.00221v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00221","description":"<p>Vision-Language Pretraining (VLP) models have recently successfully\nfacilitated many cross-modal downstream tasks. Most existing works evaluated\ntheir systems by comparing the fine-tuned downstream task performance. However,\nonly average downstream task accuracy provides little information about the\npros and cons of each VLP method, let alone provides insights on how the\ncommunity can improve the systems in the future. Inspired by the CheckList for\ntesting natural language processing, we introduce VL-CheckList, a novel\nframework to understand the capabilities of VLP models. The proposed method\ndivides the image-texting ability of a VLP model into three categories:\nobjects, attributes, and relations, and uses a novel taxonomy to further break\ndown these three aspects. We conduct comprehensive studies to analyze seven\nrecently popular VLP models via the proposed framework. Results confirm the\neffectiveness of the proposed method by revealing fine-grained differences\namong the compared models that were not visible from downstream task-only\nevaluation. Further results show promising research direction in building\nbetter VLP models. Data and Code: https://github.com/om-ai-lab/VL-CheckList\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiancheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1\">Mingwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Haozhan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kyusong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianwei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keeping Less is More: Point Sparsification for Visual SLAM. (arXiv:2207.00225v1 [cs.RO])","link":"http://arxiv.org/abs/2207.00225","description":"<p>When adapting Simultaneous Mapping and Localization (SLAM) to real-world\napplications, such as autonomous vehicles, drones, and augmented reality\ndevices, its memory footprint and computing cost are the two main factors\nlimiting the performance and the range of applications. In sparse feature based\nSLAM algorithms, one efficient way for this problem is to limit the map point\nsize by selecting the points potentially useful for local and global bundle\nadjustment (BA). This study proposes an efficient graph optimization for\nsparsifying map points in SLAM systems. Specifically, we formulate a maximum\npose-visibility and maximum spatial diversity problem as a minimum-cost\nmaximum-flow graph optimization problem. The proposed method works as an\nadditional step in existing SLAM systems, so it can be used in both\nconventional or learning based SLAM systems. By extensive experimental\nevaluations we demonstrate the proposed method achieves even more accurate\ncamera poses with approximately 1/3 of the map points and 1/2 of the\ncomputation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1\">Yeonsoo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Soohyun Bae</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Transformer Meets CutMix for Improved Accuracy, Communication Efficiency, and Data Privacy in Split Learning. (arXiv:2207.00234v1 [cs.LG])","link":"http://arxiv.org/abs/2207.00234","description":"<p>This article seeks for a distributed learning solution for the visual\ntransformer (ViT) architectures. Compared to convolutional neural network (CNN)\narchitectures, ViTs often have larger model sizes, and are computationally\nexpensive, making federated learning (FL) ill-suited. Split learning (SL) can\ndetour this problem by splitting a model and communicating the hidden\nrepresentations at the split-layer, also known as smashed data.\nNotwithstanding, the smashed data of ViT are as large as and as similar as the\ninput data, negating the communication efficiency of SL while violating data\nprivacy. To resolve these issues, we propose a new form of CutSmashed data by\nrandomly punching and compressing the original smashed data. Leveraging this,\nwe develop a novel SL framework for ViT, coined CutMixSL, communicating\nCutSmashed data. CutMixSL not only reduces communication costs and privacy\nleakage, but also inherently involves the CutMix data augmentation, improving\naccuracy and scalability. Simulations corroborate that CutMixSL outperforms\nbaselines such as parallelized SL and SplitFed that integrates FL with SL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1\">Sihun Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jihong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vepakomma_P/0/1/0/all/0/1\">Praneeth Vepakomma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raskar_R/0/1/0/all/0/1\">Ramesh Raskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1\">Mehdi Bennis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seong-Lyun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance. (arXiv:2207.00251v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00251","description":"<p>Although deep learning algorithms have been intensively developed for\ncomputer-aided tuberculosis diagnosis (CTD), they mainly depend on carefully\nannotated datasets, leading to much time and resource consumption. Weakly\nsupervised learning (WSL), which leverages coarse-grained labels to accomplish\nfine-grained tasks, has the potential to solve this problem. In this paper, we\nfirst propose a new large-scale tuberculosis (TB) chest X-ray dataset, namely\nthe tuberculosis chest X-ray attribute dataset (TBX-Att), and then establish an\nattribute-assisted weakly-supervised framework to classify and localize TB by\nleveraging the attribute information to overcome the insufficiency of\nsupervision in WSL scenarios. Specifically, first, the TBX-Att dataset contains\n2000 X-ray images with seven kinds of attributes for TB relational reasoning,\nwhich are annotated by experienced radiologists. It also includes the public\nTBX11K dataset with 11200 X-ray images to facilitate weakly supervised\ndetection. Second, we exploit a multi-scale feature interaction model for TB\narea classification and detection with attribute relational reasoning. The\nproposed model is evaluated on the TBX-Att dataset and will serve as a solid\nbaseline for future research. The code and data will be available at\nhttps://github.com/GangmingZhao/tb-attribute-weak-localization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1\">Chengwei Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Gangming Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1\">Junjie Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_B/0/1/0/all/0/1\">Baolian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1\">Chaowei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dingwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yizhou Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trajectory Forecasting on Temporal Graphs. (arXiv:2207.00255v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00255","description":"<p>Predicting future locations of agents in the scene is an important problem in\nself-driving. In recent years, there has been a significant progress in\nrepresenting the scene and the agents in it. The interactions of agents with\nthe scene and with each other are typically modeled with a Graph Neural\nNetwork. However, the graph structure is mostly static and fails to represent\nthe temporal changes in highly dynamic scenes. In this work, we propose a\ntemporal graph representation to better capture the dynamics in traffic scenes.\nWe complement our representation with two types of memory modules; one focusing\non the agent of interest and the other on the entire scene. This allows us to\nlearn temporally-aware representations that can achieve good results even with\nsimple regression of multiple futures. When combined with goal-conditioned\nprediction, we show better results that can reach the state-of-the-art\nperformance on the Argoverse benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aydemir_G/0/1/0/all/0/1\">G&#xf6;rkay Aydemir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1\">Adil Kaan Akan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1\">Fatma G&#xfc;ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised High-Resolution Portrait Gaze Correction and Animation. (arXiv:2207.00256v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00256","description":"<p>This paper proposes a gaze correction and animation method for\nhigh-resolution, unconstrained portrait images, which can be trained without\nthe gaze angle and the head pose annotations. Common gaze-correction methods\nusually require annotating training data with precise gaze, and head pose\ninformation. Solving this problem using an unsupervised method remains an open\nproblem, especially for high-resolution face images in the wild, which are not\neasy to annotate with gaze and head pose labels. To address this issue, we\nfirst create two new portrait datasets: CelebGaze and high-resolution\nCelebHQGaze. Second, we formulate the gaze correction task as an image\ninpainting problem, addressed using a Gaze Correction Module (GCM) and a Gaze\nAnimation Module (GAM). Moreover, we propose an unsupervised training strategy,\ni.e., Synthesis-As-Training, to learn the correlation between the eye region\nfeatures and the gaze angle. As a result, we can use the learned latent space\nfor gaze animation with semantic interpolation in this space. Moreover, to\nalleviate both the memory and the computational costs in the training and the\ninference stage, we propose a Coarse-to-Fine Module (CFM) integrated with GCM\nand GAM. Extensive experiments validate the effectiveness of our method for\nboth the gaze correction and the gaze animation tasks in both low and\nhigh-resolution face datasets in the wild and demonstrate the superiority of\nour method with respect to the state of the arts. Code is available at\nhttps://github.com/zhangqianhui/GazeAnimationV2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jichao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingjing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1\">Enver Sangineto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Peng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1\">Nicu Sebe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Covid-19 detection using transfer learning approach from computed temography images. (arXiv:2207.00259v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00259","description":"<p>Our main goal in this study is to propose a transfer learning based method\nfor COVID-19 detection from Computed Tomography (CT) images. The transfer\nlearning model used for the task is a pretrained Xception model. Both model\narchitecture and pre-trained weights on ImageNet were used. The resulting\nmodified model was trained with 128 batch size and 224x224, 3 channeled input\nimages, converted from original 512x512, grayscale images. The dataset used is\na the COV19-CT-DB. Labels in the dataset include COVID-19 cases and\nNon-COVID-19 cases for COVID-1919 detection. Firstly, a accuracy and loss on\nthe validation partition of the dataset as well as precision recall and macro\nF1 score were used to measure the performance of the proposed method. The\nresulting Macro F1 score on the validation set exceeded the baseline model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1\">Kenan Morani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balikci_M/0/1/0/all/0/1\">Muhammet Fatih Balikci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Altuntas_T/0/1/0/all/0/1\">Tayfun Yigit Altuntas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1\">Devrim Unay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Two-view 6D Object Pose Estimation: A Comparative Study on Fusion Strategy. (arXiv:2207.00260v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00260","description":"<p>Current RGB-based 6D object pose estimation methods have achieved noticeable\nperformance on datasets and real world applications. However, predicting 6D\npose from single 2D image features is susceptible to disturbance from changing\nof environment and textureless or resemblant object surfaces. Hence, RGB-based\nmethods generally achieve less competitive results than RGBD-based methods,\nwhich deploy both image features and 3D structure features. To narrow down this\nperformance gap, this paper proposes a framework for 6D object pose estimation\nthat learns implicit 3D information from 2 RGB images. Combining the learned 3D\ninformation and 2D image features, we establish more stable correspondence\nbetween the scene and the object models. To seek for the methods best utilizing\n3D information from RGB inputs, we conduct an investigation on three different\napproaches, including Early- Fusion, Mid-Fusion, and Late-Fusion. We ascertain\nthe Mid- Fusion approach is the best approach to restore the most precise 3D\nkeypoints useful for object pose estimation. The experiments show that our\nmethod outperforms state-of-the-art RGB-based methods, and achieves comparable\nresults with RGBD-based methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lilu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1\">Rong Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wavelet leader based formalism to compute multifractal features for classifying lung nodules in X-ray images. (arXiv:2207.00262v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00262","description":"<p>This paper presents and validates a novel lung nodule classification\nalgorithm that uses multifractal features found in X-ray images. The proposed\nmethod includes a pre-processing step where two enhancement techniques are\napplied: histogram equalization and a combination of wavelet decomposition and\nmorphological operations. As a novelty, multifractal features using wavelet\nleader based formalism are used with Support Vector Machine classifier; other\nclassical texture features were also included. Best results were obtained when\nusing multifractal features in combination with classical texture features,\nwith a maximum ROC AUC of 75\\%. The results show improvements when using data\naugmentation technique, and parameter optimization. The proposed method proved\nto be more efficient and accurate than Modulus Maxima Wavelet Formalism in both\ncomputational cost and accuracy when compared in a similar experimental set up.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sierra_Ponce_I/0/1/0/all/0/1\">Isabella Mar&#xed;a Sierra-Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leon_Mecias_A/0/1/0/all/0/1\">Angela Mireya Le&#xf3;n-Mec&#xed;as</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valdes_Santiago_D/0/1/0/all/0/1\">Damian Vald&#xe9;s-Santiago</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BadHash: Invisible Backdoor Attacks against Deep Hashing with Clean Label. (arXiv:2207.00278v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00278","description":"<p>Due to its powerful feature learning capability and high efficiency, deep\nhashing has achieved great success in large-scale image retrieval. Meanwhile,\nextensive works have demonstrated that deep neural networks (DNNs) are\nsusceptible to adversarial examples, and exploring adversarial attack against\ndeep hashing has attracted many research efforts. Nevertheless, backdoor\nattack, another famous threat to DNNs, has not been studied for deep hashing\nyet. Although various backdoor attacks have been proposed in the field of image\nclassification, existing approaches failed to realize a truly imperceptive\nbackdoor attack that enjoys invisible triggers and clean label setting\nsimultaneously, and they also cannot meet the intrinsic demand of image\nretrieval backdoor.\n</p>\n<p>In this paper, we propose BadHash, the first generative-based imperceptible\nbackdoor attack against deep hashing, which can effectively generate invisible\nand input-specific poisoned images with clean label. Specifically, we first\npropose a new conditional generative adversarial network (cGAN) pipeline to\neffectively generate poisoned samples. For any given benign image, it seeks to\ngenerate a natural-looking poisoned counterpart with a unique invisible\ntrigger. In order to improve the attack effectiveness, we introduce a\nlabel-based contrastive learning network LabCLN to exploit the semantic\ncharacteristics of different labels, which are subsequently used for confusing\nand misleading the target model to learn the embedded trigger. We finally\nexplore the mechanism of backdoor attacks on image retrieval in the hash space.\nExtensive experiments on multiple benchmark datasets verify that BadHash can\ngenerate imperceptible poisoned samples with strong attack ability and\ntransferability over state-of-the-art deep hashing schemes. Primary Subject\nArea: [Engagement] Multimedia Search and Recommendation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengshan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yechao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Leo Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yifeng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+HE_Y/0/1/0/all/0/1\">Yuanyuan HE</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hai Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"(Un)likelihood Training for Interpretable Embedding. (arXiv:2207.00282v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00282","description":"<p>Cross-modal representation learning has become a new normal for bridging the\nsemantic gap between text and visual data. Learning modality agnostic\nrepresentations in a continuous latent space, however, is often treated as a\nblack-box data-driven training process. It is well-known that the effectiveness\nof representation learning depends heavily on the quality and scale of training\ndata. For video representation learning, having a complete set of labels that\nannotate the full spectrum of video content for training is highly difficult if\nnot impossible. These issues, black-box training and dataset bias, make\nrepresentation learning practically challenging to be deployed for video\nunderstanding due to unexplainable and unpredictable results. In this paper, we\npropose two novel training objectives, likelihood and unlikelihood functions,\nto unroll semantics behind embeddings while addressing the label sparsity\nproblem in training. The likelihood training aims to interpret semantics of\nembeddings beyond training labels, while the unlikelihood training leverages\nprior knowledge for regularization to ensure semantically coherent\ninterpretation. With both training objectives, a new encoder-decoder network,\nwhich learns interpretable cross-modal representation, is proposed for ad-hoc\nvideo search. Extensive experiments on TRECVid and MSR-VTT datasets show the\nproposed network outperforms several state-of-the-art retrieval models with a\nstatistically significant performance margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">Wing-Kwong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1\">Zhijian Hou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DALG: Deep Attentive Local and Global Modeling for Image Retrieval. (arXiv:2207.00287v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00287","description":"<p>Deeply learned representations have achieved superior image retrieval\nperformance in a retrieve-then-rerank manner. Recent state-of-the-art single\nstage model, which heuristically fuses local and global features, achieves\npromising trade-off between efficiency and effectiveness. However, we notice\nthat efficiency of existing solutions is still restricted because of their\nmulti-scale inference paradigm. In this paper, we follow the single stage art\nand obtain further complexity-effectiveness balance by successfully getting rid\nof multi-scale testing. To achieve this goal, we abandon the widely-used\nconvolution network giving its limitation in exploring diverse visual patterns,\nand resort to fully attention based framework for robust representation\nlearning motivated by the success of Transformer. Besides applying Transformer\nfor global feature extraction, we devise a local branch composed of\nwindow-based multi-head attention and spatial attention to fully exploit local\nimage patterns. Furthermore, we propose to combine the hierarchical local and\nglobal features via a cross-attention module, instead of using heuristically\nfusion as previous art does. With our Deep Attentive Local and Global modeling\nframework (DALG), extensive experimental results show that efficiency can be\nsignificantly improved while maintaining competitive results with the state of\nthe arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yuxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1\">Ruolin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongliang He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to segment from object sizes. (arXiv:2207.00289v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00289","description":"<p>Deep learning has proved particularly useful for semantic segmentation, a\nfundamental image analysis task. However, the standard deep learning methods\nneed many training images with ground-truth pixel-wise annotations, which are\nusually laborious to obtain and, in some cases (e.g., medical images), require\ndomain expertise. Therefore, instead of pixel-wise annotations, we focus on\nimage annotations that are significantly easier to acquire but still\ninformative, namely the size of foreground objects. We define the object size\nas the maximum distance between a foreground pixel and the background. We\npropose an algorithm for training a deep segmentation network from a dataset of\na few pixel-wise annotated images and many images with known object sizes. The\nalgorithm minimizes a discrete (non-differentiable) loss function defined over\nthe object sizes by sampling the gradient and then using the standard\nback-propagation algorithm. We study the performance of our approach in terms\nof training time and generalization error.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barucic_D/0/1/0/all/0/1\">Denis Baru&#x10d;i&#x107;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Kybic_J/0/1/0/all/0/1\">Jan Kybic</a> (1) ((1) Czech Technical University in Prague, Czech Republic)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Graph Matching Algorithms in Computer Vision. (arXiv:2207.00291v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00291","description":"<p>The graph matching optimization problem is an essential component for many\ntasks in computer vision, such as bringing two deformable objects in\ncorrespondence. Naturally, a wide range of applicable algorithms have been\nproposed in the last decades. Since a common standard benchmark has not been\ndeveloped, their performance claims are often hard to verify as evaluation on\ndiffering problem instances and criteria make the results incomparable. To\naddress these shortcomings, we present a comparative study of graph matching\nalgorithms. We create a uniform benchmark where we collect and categorize a\nlarge set of existing and publicly available computer vision graph matching\nproblems in a common format. At the same time we collect and categorize the\nmost popular open-source implementations of graph matching algorithms. Their\nperformance is evaluated in a way that is in line with the best practices for\ncomparing optimization algorithms. The study is designed to be reproducible and\nextensible to serve as a valuable resource in the future.\n</p>\n<p>Our study provides three notable insights:\n</p>\n<p>1.) popular problem instances are exactly solvable in substantially less than\n1 second and, therefore, are insufficient for future empirical evaluations;\n</p>\n<p>2.) the most popular baseline methods are highly inferior to the best\navailable methods;\n</p>\n<p>3.) despite the NP-hardness of the problem, instances coming from vision\napplications are often solvable in a few seconds even for graphs with more than\n500 vertices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haller_S/0/1/0/all/0/1\">Stefan Haller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feineis_L/0/1/0/all/0/1\">Lorenz Feineis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hutschenreiter_L/0/1/0/all/0/1\">Lisa Hutschenreiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1\">Florian Bernard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rother_C/0/1/0/all/0/1\">Carsten Rother</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainmuller_D/0/1/0/all/0/1\">Dagmar Kainm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savchynskyy_B/0/1/0/all/0/1\">Bogdan Savchynskyy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offset equivariant networks and their applications. (arXiv:2207.00292v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00292","description":"<p>In this paper we present a framework for the design and implementation of\noffset equivariant networks, that is, neural networks that preserve in their\noutput uniform increments in the input. In a suitable color space this kind of\nnetworks achieves equivariance with respect to the photometric transformations\nthat characterize changes in the lighting conditions. We verified the framework\non three different problems: image recognition, illuminant estimation, and\nimage inpainting. Our experiments show that the performance of offset\nequivariant networks are comparable to those in the state of the art on regular\ndata. Differently from conventional networks, however, equivariant networks do\nbehave consistently well when the color of the illuminant changes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cotogni_M/0/1/0/all/0/1\">Marco Cotogni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cusano_C/0/1/0/all/0/1\">Claudio Cusano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopicFM: Robust and Interpretable Feature Matching with Topic-assisted. (arXiv:2207.00328v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00328","description":"<p>Finding correspondences across images is an important task in many visual\napplications. Recent state-of-the-art methods focus on end-to-end\nlearning-based architectures designed in a coarse-to-fine manner. They use a\nvery deep CNN or multi-block Transformer to learn robust representation, which\nrequires high computation power. Moreover, these methods learn features without\nreasoning about objects, shapes inside images, thus lacks of interpretability.\nIn this paper, we propose an architecture for image matching which is\nefficient, robust, and interpretable. More specifically, we introduce a novel\nfeature matching module called TopicFM which can roughly organize same spatial\nstructure across images into a topic and then augment the features inside each\ntopic for accurate matching. To infer topics, we first learn global embedding\nof topics and then use a latent-variable model to detect-then-assign the image\nstructures into topics. Our method can only perform matching in co-visibility\nregions to reduce computations. Extensive experiments in both outdoor and\nindoor datasets show that our method outperforms the recent methods in terms of\nmatching performance and computational efficiency. The code is available at\nhttps://github.com/TruongKhang/TopicFM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Giang_K/0/1/0/all/0/1\">Khang Truong Giang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Soohwan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Sungho Jo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Literature on Hand GESTURE Recognition using Graph based methods. (arXiv:2207.00329v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00329","description":"<p>Skeleton based recognition systems are gaining popularity and machine\nlearning models focusing on points or joints in a skeleton have proved to be\ncomputationally effective and application in many areas like Robotics. It is\neasy to track points and thereby preserving spatial and temporal information,\nwhich plays an important role in abstracting the required information,\nclassification becomes an easy task. In this paper, we aim to study these\npoints but using a cloud mechanism, where we define a cloud as collection of\npoints. However, when we add temporal information, it may not be possible to\nretrieve the coordinates of a point in each frame and hence instead of focusing\non a single point, we can use k-neighbors to retrieve the state of the point\nunder discussion. Our focus is to gather such information using weight sharing\nbut making sure that when we try to retrieve the information from neighbors, we\ndo not carry noise with it. LSTM which has capability of long-term modelling\nand can carry both temporal and spatial information. In this article we tried\nto summarise graph based gesture recognition method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baranwal_N/0/1/0/all/0/1\">Neha Baranwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_V/0/1/0/all/0/1\">Varun Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Correlation Loss for Regression. (arXiv:2207.00347v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00347","description":"<p>Regression learning is classic and fundamental for medical image analysis. It\nprovides the continuous mapping for many critical applications, like the\nattribute estimation, object detection, segmentation and non-rigid\nregistration. However, previous studies mainly took the case-wise criteria,\nlike the mean square errors, as the optimization objectives. They ignored the\nvery important population-wise correlation criterion, which is exactly the\nfinal evaluation metric in many tasks. In this work, we propose to revisit the\nclassic regression tasks with novel investigations on directly optimizing the\nfine-grained correlation losses. We mainly explore two complementary\ncorrelation indexes as learnable losses: Pearson linear correlation (PLC) and\nSpearman rank correlation (SRC). The contributions of this paper are two folds.\nFirst, for the PLC on global level, we propose a strategy to make it robust\nagainst the outliers and regularize the key distribution factors. These efforts\nsignificantly stabilize the learning and magnify the efficacy of PLC. Second,\nfor the SRC on local level, we propose a coarse-to-fine scheme to ease the\nlearning of the exact ranking order among samples. Specifically, we convert the\nlearning for the ranking of samples into the learning of similarity\nrelationships among samples. We extensively validate our method on two typical\nultrasound image regression tasks, including the image quality assessment and\nbio-metric measurement. Experiments prove that, with the fine-grained guidance\nin directly optimizing the correlation, the regression performances are\nsignificantly improved. Our proposed correlation losses are general and can be\nextended to more important applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yankai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiduo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinrui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yinyu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuang_X/0/1/0/all/0/1\">Xue Shuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Juzheng Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting the Mean Teacher for keypoint-based lung registration under geometric domain shifts. (arXiv:2207.00371v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00371","description":"<p>Recent deep learning-based methods for medical image registration achieve\nresults that are competitive with conventional optimization algorithms at\nreduced run times. However, deep neural networks generally require plenty of\nlabeled training data and are vulnerable to domain shifts between training and\ntest data. While typical intensity shifts can be mitigated by keypoint-based\nregistration, these methods still suffer from geometric domain shifts, for\ninstance, due to different fields of view. As a remedy, in this work, we\npresent a novel approach to geometric domain adaptation for image registration,\nadapting a model from a labeled source to an unlabeled target domain. We build\non a keypoint-based registration model, combining graph convolutions for\ngeometric feature learning with loopy belief optimization, and propose to\nreduce the domain shift through self-ensembling. To this end, we embed the\nmodel into the Mean Teacher paradigm. We extend the Mean Teacher to this\ncontext by 1) adapting the stochastic augmentation scheme and 2) combining\nlearned feature extraction with differentiable optimization. This enables us to\nguide the learning process in the unlabeled target domain by enforcing\nconsistent predictions of the learning student and the temporally averaged\nteacher model. We evaluate the method for exhale-to-inhale lung CT registration\nunder two challenging adaptation scenarios (DIR-Lab 4D CT to COPD, COPD to\nLearn2Reg). Our method consistently improves on the baseline model by 50%/47%\nwhile even matching the accuracy of models trained on target data. Source code\nis available at\nhttps://github.com/multimodallearning/registration-da-mean-teacher.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bigalke_A/0/1/0/all/0/1\">Alexander Bigalke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_M/0/1/0/all/0/1\">Mattias P. Heinrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReLER@ZJU-Alibaba Submission to the Ego4D Natural Language Queries Challenge 2022. (arXiv:2207.00383v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00383","description":"<p>In this report, we present the ReLER@ZJU-Alibaba submission to the Ego4D\nNatural Language Queries (NLQ) Challenge in CVPR 2022. Given a video clip and a\ntext query, the goal of this challenge is to locate a temporal moment of the\nvideo clip where the answer to the query can be obtained. To tackle this task,\nwe propose a multi-scale cross-modal transformer and a video frame-level\ncontrastive loss to fully uncover the correlation between language queries and\nvideo clips. Besides, we propose two data augmentation strategies to increase\nthe diversity of training samples. The experimental results demonstrate the\neffectiveness of our method. The final submission ranked first on the\nleaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Naiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaohan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yueting Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WNet: A data-driven dual-domain denoising model for sparse-view computed tomography with a trainable reconstruction layer. (arXiv:2207.00400v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00400","description":"<p>Deep learning based solutions are being succesfully implemented for a wide\nvariety of applications. Most notably, clinical use-cases have gained an\nincreased interest and have been the main driver behind some of the\ncutting-edge data-driven algorithms proposed in the last years. For\napplications like sparse-view tomographic reconstructions, where the amount of\nmeasurement data is small in order to keep acquisition times short and\nradiation dose low, reduction of the streaking artifacts has prompted the\ndevelopment of data-driven denoising algorithms with the main goal of obtaining\ndiagnostically viable images with only a subset of a full-scan data. We propose\nWNet, a data-driven dual-domain denoising model which contains a trainable\nreconstruction layer for sparse-view artifact denoising. Two encoder-decoder\nnetworks perform denoising in both sinogram- and reconstruction-domain\nsimultaneously, while a third layer implementing the Filtered Backprojection\nalgorithm is sandwiched between the first two and takes care of the\nreconstruction operation. We investigate the performance of the network on\nsparse-view chest CT scans, and we highlight the added benefit of having a\ntrainable reconstruction layer over the more conventional fixed ones. We train\nand test our network on two clinically relevant datasets and we compare the\nobtained results with three different types of sparse-view CT denoising and\nreconstruction algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Cheslerean_Boghiu_T/0/1/0/all/0/1\">Theodor Cheslerean-Boghiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hofmann_F/0/1/0/all/0/1\">Felix C. Hofmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schultheiss_M/0/1/0/all/0/1\">Manuel Schulthei&#xdf;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pfeiffer_F/0/1/0/all/0/1\">Franz Pfeiffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pfeiffer_D/0/1/0/all/0/1\">Daniela Pfeiffer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1\">Tobias Lasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Intraluminal Navigation of a Soft Robot using Deep-Learning-based Visual Servoing. (arXiv:2207.00401v1 [cs.RO])","link":"http://arxiv.org/abs/2207.00401","description":"<p>Navigation inside luminal organs is an arduous task that requires\nnon-intuitive coordination between the movement of the operator's hand and the\ninformation obtained from the endoscopic video. The development of tools to\nautomate certain tasks could alleviate the physical and mental load of doctors\nduring interventions, allowing them to focus on diagnosis and decision-making\ntasks. In this paper, we present a synergic solution for intraluminal\nnavigation consisting of a 3D printed endoscopic soft robot that can move\nsafely inside luminal structures. Visual servoing, based on Convolutional\nNeural Networks (CNNs) is used to achieve the autonomous navigation task. The\nCNN is trained with phantoms and in-vivo data to segment the lumen, and a\nmodel-less approach is presented to control the movement in constrained\nenvironments. The proposed robot is validated in anatomical phantoms in\ndifferent path configurations. We analyze the movement of the robot using\ndifferent metrics such as task completion time, smoothness, error in the\nsteady-state, and mean and maximum error. We show that our method is suitable\nto navigate safely in hollow environments and conditions which are different\nthan the ones the network was originally trained on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazo_J/0/1/0/all/0/1\">Jorge F. Lazo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Chun-Feng Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosa_B/0/1/0/all/0/1\">Benoit Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catellani_M/0/1/0/all/0/1\">Michele Catellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathelin_M/0/1/0/all/0/1\">Michel de Mathelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrigno_G/0/1/0/all/0/1\">Giancarlo Ferrigno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breedveld_P/0/1/0/all/0/1\">Paul Breedveld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dankelman_J/0/1/0/all/0/1\">Jenny Dankelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momi_E/0/1/0/all/0/1\">Elena De Momi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Supervised Learning for Videos: A Survey. (arXiv:2207.00419v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00419","description":"<p>The remarkable success of deep learning in various domains relies on the\navailability of large-scale annotated datasets. However, the use of\nhuman-generated annotations leads to models with biased learning, poor domain\ngeneralization, and poor robustness. Obtaining annotations is also expensive\nand requires great effort, which is especially challenging for videos. As an\nalternative, self-supervised learning provides a way for representation\nlearning which does not require annotations and has shown promise in both image\nand video domains. Different from the image domain, learning video\nrepresentations are more challenging due to the temporal dimension, bringing in\nmotion and other environmental dynamics. This also provides opportunities for\nexclusive ideas which can advance self-supervised learning in the video and\nmultimodal domain. In this survey, we provide a review of existing approaches\non self-supervised learning focusing on the video domain. We summarize these\nmethods into three different categories based on their learning objectives:\npre-text tasks, generative modeling, and contrastive learning. These approaches\nalso differ in terms of the modality which are being used: video, video-audio,\nvideo-text, and video-audio-text. We further introduce the commonly used\ndatasets, downstream evaluation tasks, insights into the limitations of\nexisting works, and the potential future directions in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1\">Madeline C. Schiappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S. Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Showcases: Generating Multi-Modal Explanations for Recommendations. (arXiv:2207.00422v1 [cs.IR])","link":"http://arxiv.org/abs/2207.00422","description":"<p>Existing explanation models generate only text for recommendations but still\nstruggle to produce diverse contents. In this paper, to further enrich\nexplanations, we propose a new task named personalized showcases, in which we\nprovide both textual and visual information to explain our recommendations.\nSpecifically, we first select a personalized image set that is the most\nrelevant to a user's interest toward a recommended item. Then, natural language\nexplanations are generated accordingly given our selected images. For this new\ntask, we collect a large-scale dataset from Google Local (i.e.,~maps) and\nconstruct a high-quality subset for generating multi-modal explanations. We\npropose a personalized multi-modal framework which can generate diverse and\nvisually-aligned explanations via contrastive learning. Experiments show that\nour framework benefits from different modalities as inputs, and is able to\nproduce more diverse and expressive explanations compared to previous methods\non a variety of evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_A/0/1/0/all/0/1\">An Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhankui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiacheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stain Isolation-based Guidance for Improved Stain Translation. (arXiv:2207.00431v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00431","description":"<p>Unsupervised and unpaired domain translation using generative adversarial\nneural networks, and more precisely CycleGAN, is state of the art for the stain\ntranslation of histopathology images. It often, however, suffers from the\npresence of cycle-consistent but non structure-preserving errors. We propose an\nalternative approach to the set of methods which, relying on segmentation\nconsistency, enable the preservation of pathology structures. Focusing on\nimmunohistochemistry (IHC) and multiplexed immunofluorescence (mIF), we\nintroduce a simple yet effective guidance scheme as a loss function that\nleverages the consistency of stain translation with stain isolation.\nQualitative and quantitative experiments show the ability of the proposed\napproach to improve translation between the two domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brieu_N/0/1/0/all/0/1\">Nicolas Brieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segerer_F/0/1/0/all/0/1\">Felix J. Segerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapil_A/0/1/0/all/0/1\">Ansh Kapil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wortmann_P/0/1/0/all/0/1\">Philipp Wortmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_G/0/1/0/all/0/1\">Guenter Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PROTOtypical Logic Tensor Networks (PROTO-LTN) for Zero Shot Learning. (arXiv:2207.00433v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00433","description":"<p>Semantic image interpretation can vastly benefit from approaches that combine\nsub-symbolic distributed representation learning with the capability to reason\nat a higher level of abstraction. Logic Tensor Networks (LTNs) are a class of\nneuro-symbolic systems based on a differentiable, first-order logic grounded\ninto a deep neural network. LTNs replace the classical concept of training set\nwith a knowledge base of fuzzy logical axioms. By defining a set of\ndifferentiable operators to approximate the role of connectives, predicates,\nfunctions and quantifiers, a loss function is automatically specified so that\nLTNs can learn to satisfy the knowledge base. We focus here on the subsumption\nor \\texttt{isOfClass} predicate, which is fundamental to encode most semantic\nimage interpretation tasks. Unlike conventional LTNs, which rely on a separate\npredicate for each class (e.g., dog, cat), each with its own set of learnable\nweights, we propose a common \\texttt{isOfClass} predicate, whose level of truth\nis a function of the distance between an object embedding and the corresponding\nclass prototype. The PROTOtypical Logic Tensor Networks (PROTO-LTN) extend the\ncurrent formulation by grounding abstract concepts as parametrized class\nprototypes in a high-dimensional embedding space, while reducing the number of\nparameters required to ground the knowledge base. We show how this architecture\ncan be effectively trained in the few and zero-shot learning scenarios.\nExperiments on Generalized Zero Shot Learning benchmarks validate the proposed\nimplementation as a competitive alternative to traditional embedding-based\napproaches. The proposed formulation opens up new opportunities in zero shot\nlearning settings, as the LTN formalism allows to integrate background\nknowledge in the form of logical axioms to compensate for the lack of labelled\nexamples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martone_S/0/1/0/all/0/1\">Simone Martone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manigrasso_F/0/1/0/all/0/1\">Francesco Manigrasso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabrizio_L/0/1/0/all/0/1\">Lamberti Fabrizio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morra_L/0/1/0/all/0/1\">Lia Morra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dissecting Self-Supervised Learning Methods for Surgical Computer Vision. (arXiv:2207.00449v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00449","description":"<p>The field of surgical computer vision has undergone considerable\nbreakthroughs in recent years with the rising popularity of deep neural\nnetwork-based methods. However, standard fully-supervised approaches for\ntraining such models require vast amounts of annotated data, imposing a\nprohibitively high cost; especially in the clinical domain. Self-Supervised\nLearning (SSL) methods, which have begun to gain traction in the general\ncomputer vision community, represent a potential solution to these annotation\ncosts, allowing to learn useful representations from only unlabeled data.\nStill, the effectiveness of SSL methods in more complex and impactful domains,\nsuch as medicine and surgery, remains limited and unexplored. In this work, we\naddress this critical need by investigating four state-of-the-art SSL methods\n(MoCo v2, SimCLR, DINO, SwAV) in the context of surgical computer vision. We\npresent an extensive analysis of the performance of these methods on the\nCholec80 dataset for two fundamental and popular tasks in surgical context\nunderstanding, phase recognition and tool presence detection. We examine their\nparameterization, then their behavior with respect to training data quantities\nin semi-supervised settings. Correct transfer of these methods to surgery, as\ndescribed and conducted in this work, leads to substantial performance gains\nover generic uses of SSL - up to 7% on phase recognition and 20% on tool\npresence detection - as well as state-of-the-art semi-supervised phase\nrecognition approaches by up to 14%. The code will be made available at\nhttps://github.com/CAMMA-public/SelfSupSurg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_S/0/1/0/all/0/1\">Sanat Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastav_V/0/1/0/all/0/1\">Vinkle Srivastav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alapatt_D/0/1/0/all/0/1\">Deepak Alapatt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murali_A/0/1/0/all/0/1\">Aditya Murali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sestini_L/0/1/0/all/0/1\">Luca Sestini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1\">Chinedu Innocent Nwoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamoud_I/0/1/0/all/0/1\">Idris Hamoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleurentin_A/0/1/0/all/0/1\">Antoine Fleurentin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Exarchakis_G/0/1/0/all/0/1\">Georgios Exarchakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karargyris_A/0/1/0/all/0/1\">Alexandros Karargyris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1\">Nicolas Padoy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors. (arXiv:2207.00458v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00458","description":"<p>Optical coherence tomography (OCT) is a non-invasive 3D modality widely used\nin ophthalmology for imaging the retina. Achieving automated, anatomically\ncoherent retinal layer segmentation on OCT is important for the detection and\nmonitoring of different retinal diseases, like Age-related Macular Disease\n(AMD) or Diabetic Retinopathy. However, the majority of state-of-the-art layer\nsegmentation methods are based on purely supervised deep-learning, requiring a\nlarge amount of pixel-level annotated data that is expensive and hard to\nobtain. With this in mind, we introduce a semi-supervised paradigm into the\nretinal layer segmentation task that makes use of the information present in\nlarge-scale unlabeled datasets as well as anatomical priors. In particular, a\nnovel fully differentiable approach is used for converting surface position\nregression into a pixel-wise structured segmentation, allowing to use both 1D\nsurface and 2D layer representations in a coupled fashion to train the model.\nIn particular, these 2D segmentations are used as anatomical factors that,\ntogether with learned style factors, compose disentangled representations used\nfor reconstructing the input image. In parallel, we propose a set of anatomical\npriors to improve network training when a limited amount of labeled data is\navailable. We demonstrate on the real-world dataset of scans with intermediate\nand wet-AMD that our method outperforms state-of-the-art when using our full\ntraining set, but more importantly largely exceeds state-of-the-art when it is\ntrained with a fraction of the labeled data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fazekas_B/0/1/0/all/0/1\">Botond Fazekas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aresta_G/0/1/0/all/0/1\">Guilherme Aresta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lachinov_D/0/1/0/all/0/1\">Dmitrii Lachinov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riedl_S/0/1/0/all/0/1\">Sophie Riedl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Julia Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1\">Ursula Schmidt-Erfurth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the solution space of linear inverse problems with GAN latent geometry. (arXiv:2207.00460v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00460","description":"<p>Inverse problems consist in reconstructing signals from incomplete sets of\nmeasurements and their performance is highly dependent on the quality of the\nprior knowledge encoded via regularization. While traditional approaches focus\non obtaining a unique solution, an emerging trend considers exploring multiple\nfeasibile solutions. In this paper, we propose a method to generate multiple\nreconstructions that fit both the measurements and a data-driven prior learned\nby a generative adversarial network. In particular, we show that, starting from\nan initial solution, it is possible to find directions in the latent space of\nthe generative model that are null to the forward operator, and thus keep\nconsistency with the measurements, while inducing significant perceptual\nchange. Our exploration approach allows to generate multiple solutions to the\ninverse problem an order of magnitude faster than existing approaches; we show\nresults on image super-resolution and inpainting problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Montanaro_A/0/1/0/all/0/1\">Antonio Montanaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valsesia_D/0/1/0/all/0/1\">Diego Valsesia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Magli_E/0/1/0/all/0/1\">Enrico Magli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling. (arXiv:2207.00474v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00474","description":"<p>Ultrasound (US) is widely used for its advantages of real-time imaging,\nradiation-free and portability. In clinical practice, analysis and diagnosis\noften rely on US sequences rather than a single image to obtain dynamic\nanatomical information. This is challenging for novices to learn because\npracticing with adequate videos from patients is clinically unpractical. In\nthis paper, we propose a novel framework to synthesize high-fidelity US videos.\nSpecifically, the synthesis videos are generated by animating source content\nimages based on the motion of given driving videos. Our highlights are\nthree-fold. First, leveraging the advantages of self- and fully-supervised\nlearning, our proposed system is trained in weakly-supervised manner for\nkeypoint detection. These keypoints then provide vital information for handling\ncomplex high dynamic motions in US videos. Second, we decouple content and\ntexture learning using the dual decoders to effectively reduce the model\nlearning difficulty. Last, we adopt the adversarial training strategy with GAN\nlosses for further improving the sharpness of the generated videos, narrowing\nthe gap between real and synthesis videos. We validate our method on a large\nin-house pelvic dataset with high dynamic motion. Extensive evaluation metrics\nand user study prove the effectiveness of our proposed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiamin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinrui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Huanjia Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanji Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound. (arXiv:2207.00475v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00475","description":"<p>Standard plane (SP) localization is essential in routine clinical ultrasound\n(US) diagnosis. Compared to 2D US, 3D US can acquire multiple view planes in\none scan and provide complete anatomy with the addition of coronal plane.\nHowever, manually navigating SPs in 3D US is laborious and biased due to the\norientation variability and huge search space. In this study, we introduce a\nnovel reinforcement learning (RL) framework for automatic SP localization in 3D\nUS. Our contribution is three-fold. First, we formulate SP localization in 3D\nUS as a tangent-point-based problem in RL to restructure the action space and\nsignificantly reduce the search space. Second, we design an auxiliary task\nlearning strategy to enhance the model's ability to recognize subtle\ndifferences crossing Non-SPs and SPs in plane search. Finally, we propose a\nspatial-anatomical reward to effectively guide learning trajectories by\nexploiting spatial and anatomical information simultaneously. We explore the\nefficacy of our approach on localizing four SPs on uterus and fetal brain\ndatasets. The experiments indicate that our approach achieves a high\nlocalization accuracy as well as robust performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuxin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jikuan Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhen_C/0/1/0/all/0/1\">Chaojiong Zhen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaodan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1\">Nishant Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guoqiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Weijun Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1\">Alejandro F. Frangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Reflective Learning for Robust Medical Image Segmentation. (arXiv:2207.00476v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00476","description":"<p>Deep segmentation models often face the failure risks when the testing image\npresents unseen distributions. Improving model robustness against these risks\nis crucial for the large-scale clinical application of deep models. In this\nstudy, inspired by human learning cycle, we propose a novel online reflective\nlearning framework (RefSeg) to improve segmentation robustness. Based on the\nreflection-on-action conception, our RefSeg firstly drives the deep model to\ntake action to obtain semantic segmentation. Then, RefSeg triggers the model to\nreflect itself. Because making deep models realize their segmentation failures\nduring testing is challenging, RefSeg synthesizes a realistic proxy image from\nthe semantic mask to help deep models build intuitive and effective\nreflections. This proxy translates and emphasizes the segmentation flaws. By\nmaximizing the structural similarity between the raw input and the proxy, the\nreflection-on-action loop is closed with segmentation robustness improved.\nRefSeg runs in the testing phase and is general for segmentation models.\nExtensive validation on three medical image segmentation tasks with a public\ncardiac MR dataset and two in-house large ultrasound datasets show that our\nRefSeg remarkably improves model robustness and reports state-of-the-art\nperformance over strong competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaoqiong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiamin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinrui Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1\">Haoran Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision-based Conflict Detection within Crowds based on High-Resolution Human Pose Estimation for Smart and Safe Airport. (arXiv:2207.00477v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00477","description":"<p>Future airports are becoming more complex and congested with the increasing\nnumber of travellers. While the airports are more likely to become hotspots for\npotential conflicts to break out which can cause serious delays to flights and\nseveral safety issues. An intelligent algorithm which renders security\nsurveillance more effective in detecting conflicts would bring many benefits to\nthe passengers in terms of their safety, finance, and travelling efficiency.\nThis paper details the development of a machine learning model to classify\nconflicting behaviour in a crowd. HRNet is used to segment the images and then\ntwo approaches are taken to classify the poses of people in the frame via\nmultiple classifiers. Among them, it was found that the support vector machine\n(SVM) achieved the most performant achieving precision of 94.37%. Where the\nmodel falls short is against ambiguous behaviour such as a hug or losing track\nof a subject in the frame. The resulting model has potential for deployment\nwithin an airport if improvements are made to cope with the vast number of\npotential passengers in view as well as training against further ambiguous\nbehaviours which will arise in an airport setting. In turn, will provide the\ncapability to enhance security surveillance and improve airport safety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kheta_K/0/1/0/all/0/1\">Karan Kheta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Delgove_C/0/1/0/all/0/1\">Claire Delgove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Ruolin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aderogba_A/0/1/0/all/0/1\">Adeola Aderogba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokam_M/0/1/0/all/0/1\">Marc-Olivier Pokam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unal_M/0/1/0/all/0/1\">Muhammed Mehmet Unal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yang Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weisi Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound. (arXiv:2207.00496v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00496","description":"<p>Over the past decades, the incidence of thyroid cancer has been increasing\nglobally. Accurate and early diagnosis allows timely treatment and helps to\navoid over-diagnosis. Clinically, a nodule is commonly evaluated from both\ntransverse and longitudinal views using thyroid ultrasound. However, the\nappearance of the thyroid gland and lesions can vary dramatically across\nindividuals. Identifying key diagnostic information from both views requires\nspecialized expertise. Furthermore, finding an optimal way to integrate\nmulti-view information also relies on the experience of clinicians and adds\nfurther difficulty to accurate diagnosis. To address these, we propose a\npersonalized diagnostic tool that can customize its decision-making process for\ndifferent patients. It consists of a multi-view classification module for\nfeature extraction and a personalized weighting allocation network that\ngenerates optimal weighting for different views. It is also equipped with a\nself-supervised view-aware contrastive loss to further improve the model\nrobustness towards different patient groups. Experimental results show that the\nproposed framework can better utilize multi-view information and outperform the\ncompeting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Han Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yijie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xiaohong Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jianqiao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MotionMixer: MLP-based 3D Human Body Pose Forecasting. (arXiv:2207.00499v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00499","description":"<p>In this work, we present MotionMixer, an efficient 3D human body pose\nforecasting model based solely on multi-layer perceptrons (MLPs). MotionMixer\nlearns the spatial-temporal 3D body pose dependencies by sequentially mixing\nboth modalities. Given a stacked sequence of 3D body poses, a spatial-MLP\nextracts fine grained spatial dependencies of the body joints. The interaction\nof the body joints over time is then modelled by a temporal MLP. The\nspatial-temporal mixed features are finally aggregated and decoded to obtain\nthe future motion. To calibrate the influence of each time step in the pose\nsequence, we make use of squeeze-and-excitation (SE) blocks. We evaluate our\napproach on Human3.6M, AMASS, and 3DPW datasets using the standard evaluation\nprotocols. For all evaluations, we demonstrate state-of-the-art performance,\nwhile having a model with a smaller number of parameters. Our code is available\nat: https://github.com/MotionMLP/MotionMixer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bouazizi_A/0/1/0/all/0/1\">Arij Bouazizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzbock_A/0/1/0/all/0/1\">Adrian Holzbock</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kressel_U/0/1/0/all/0/1\">Ulrich Kressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietmayer_K/0/1/0/all/0/1\">Klaus Dietmayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1\">Vasileios Belagiannis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification. (arXiv:2207.00501v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00501","description":"<p>Diagnosing hematological malignancies requires identification and\nclassification of white blood cells in peripheral blood smears. Domain shifts\ncaused by different lab procedures, staining, illumination, and microscope\nsettings hamper the re-usability of recently developed machine learning methods\non data collected from different sites. Here, we propose a cross-domain adapted\nautoencoder to extract features in an unsupervised manner on three different\ndatasets of single white blood cells scanned from peripheral blood smears. The\nautoencoder is based on an R-CNN architecture allowing it to focus on the\nrelevant white blood cell and eliminate artifacts in the image. To evaluate the\nquality of the extracted features we use a simple random forest to classify\nsingle cells. We show that thanks to the rich features extracted by the\nautoencoder trained on only one of the datasets, the random forest classifier\nperforms satisfactorily on the unseen datasets, and outperforms published\noracle networks in the cross-domain task. Our results suggest the possibility\nof employing this unsupervised approach in more complicated diagnosis and\nprognosis tasks without the need to add expensive expert labels to unseen data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_R/0/1/0/all/0/1\">Raheleh Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadafi_A/0/1/0/all/0/1\">Ario Sadafi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruber_A/0/1/0/all/0/1\">Armin Gruber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienemann_P/0/1/0/all/0/1\">Peter Lienemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1\">Shadi Albarqouni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1\">Carsten Marr</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Far Can I Go ? : A Self-Supervised Approach for Deterministic Video Depth Forecasting. (arXiv:2207.00506v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00506","description":"<p>In this paper we present a novel self-supervised method to anticipate the\ndepth estimate for a future, unobserved real-world urban scene. This work is\nthe first to explore self-supervised learning for estimation of monocular depth\nof future unobserved frames of a video. Existing works rely on a large number\nof annotated samples to generate the probabilistic prediction of depth for\nunseen frames. However, this makes it unrealistic due to its requirement for\nlarge amount of annotated depth samples of video. In addition, the\nprobabilistic nature of the case, where one past can have multiple future\noutcomes often leads to incorrect depth estimates. Unlike previous methods, we\nmodel the depth estimation of the unobserved frame as a view-synthesis problem,\nwhich treats the depth estimate of the unseen video frame as an auxiliary task\nwhile synthesizing back the views using learned pose. This approach is not only\ncost effective - we do not use any ground truth depth for training (hence\npractical) but also deterministic (a sequence of past frames map to an\nimmediate future). To address this task we first develop a novel depth\nforecasting network DeFNet which estimates depth of unobserved future by\nforecasting latent features. Second, we develop a channel-attention based pose\nestimation network that estimates the pose of the unobserved frame. Using this\nlearned pose, estimated depth map is reconstructed back into the image domain,\nthus forming a self-supervised solution. Our proposed approach shows\nsignificant improvements in Abs Rel metric compared to state-of-the-art\nalternatives on both short and mid-term forecasting setting, benchmarked on\nKITTI and Cityscapes. Code is available at\nhttps://github.com/sauradip/depthForecasting\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Suaradip Nag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nisarg Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_A/0/1/0/all/0/1\">Anran Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ray-Space Motion Compensation for Lenslet Plenoptic Video Coding. (arXiv:2207.00522v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00522","description":"<p>Plenoptic images and videos bearing rich information demand a tremendous\namount of data storage and high transmission cost. While there has been much\nstudy on plenoptic image coding, investigations into plenoptic video coding\nhave been very limited. We investigate the motion compensation for plenoptic\nvideo coding from a slightly different perspective by looking at the problem in\nthe ray-space domain instead of in the conventional pixel domain. Here, we\ndevelop a novel motion compensation scheme for lenslet video under two\nsub-cases of ray-space motion, that is, integer ray-space motion and fractional\nray-space motion. The proposed new scheme of light field motion-compensated\nprediction is designed such that it can be easily integrated into well-known\nvideo coding techniques such as HEVC. Experimental results compared to relevant\nexisting methods have shown remarkable compression efficiency with an average\ngain of 19.63% and a peak gain of 29.1%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huu_T/0/1/0/all/0/1\">Thuc Nguyen Huu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_V/0/1/0/all/0/1\">Vinh Van Duong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yim_J/0/1/0/all/0/1\">Jonghoon Yim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jeon_B/0/1/0/all/0/1\">Byeungwoo Jeon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Autoencoders for Self-Supervised Learning on Automotive Point Clouds. (arXiv:2207.00531v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00531","description":"<p>Masked autoencoding has become a successful pre-training paradigm for\nTransformer models for text, images, and recently, point clouds. Raw automotive\ndatasets are a suitable candidate for self-supervised pre-training as they\ngenerally are cheap to collect compared to annotations for tasks like 3D object\ndetection (OD). However, development of masked autoencoders for point clouds\nhas focused solely on synthetic and indoor data. Consequently, existing methods\nhave tailored their representations and models toward point clouds which are\nsmall, dense and have homogeneous point density. In this work, we study masked\nautoencoding for point clouds in an automotive setting, which are sparse and\nfor which the point density can vary drastically among objects in the same\nscene. To this end, we propose Voxel-MAE, a simple masked autoencoding\npre-training scheme designed for voxel representations. We pre-train the\nbackbone of a Transformer-based 3D object detector to reconstruct masked voxels\nand to distinguish between empty and non-empty voxels. Our method improves the\n3D OD performance by 1.75 mAP points and 1.05 NDS on the challenging nuScenes\ndataset. Compared to existing self-supervised methods for automotive data,\nVoxel-MAE displays up to $2\\times$ performance increase. Further, we show that\nby pre-training with Voxel-MAE, we require only 40% of the annotated data to\noutperform a randomly initialized equivalent. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1\">Georg Hess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaxing_J/0/1/0/all/0/1\">Johan Jaxing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_E/0/1/0/all/0/1\">Elias Svensson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hagerman_D/0/1/0/all/0/1\">David Hagerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_C/0/1/0/all/0/1\">Christoffer Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1\">Lennart Svensson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transforming Image Generation from Scene Graphs. (arXiv:2207.00545v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00545","description":"<p>Generating images from semantic visual knowledge is a challenging task, that\ncan be useful to condition the synthesis process in complex, subtle, and\nunambiguous ways, compared to alternatives such as class labels or text\ndescriptions. Although generative methods conditioned by semantic\nrepresentations exist, they do not provide a way to control the generation\nprocess aside from the specification of constraints between objects. As an\nexample, the possibility to iteratively generate or modify images by manually\nadding specific items is a desired property that, to our knowledge, has not\nbeen fully investigated in the literature. In this work we propose a\ntransformer-based approach conditioned by scene graphs that, conversely to\nrecent transformer-based methods, also employs a decoder to autoregressively\ncompose images, making the synthesis process more effective and controllable.\nThe proposed architecture is composed by three modules: 1) a graph\nconvolutional network, to encode the relationships of the input graph; 2) an\nencoder-decoder transformer, which autoregressively composes the output image;\n3) an auto-encoder, employed to generate representations used as input/output\nof each generation step by the transformer. Results obtained on CIFAR10 and\nMNIST images show that our model is able to satisfy semantic constraints\ndefined by a scene graph and to model relations between visual objects in the\nscene by taking into account a user-provided partial rendering of the desired\ntarget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sortino_R/0/1/0/all/0/1\">Renato Sortino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palazzo_S/0/1/0/all/0/1\">Simone Palazzo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spampinato_C/0/1/0/all/0/1\">Concetto Spampinato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How can spherical CNNs benefit ML-based diffusion MRI parameter estimation?. (arXiv:2207.00572v1 [eess.IV])","link":"http://arxiv.org/abs/2207.00572","description":"<p>This paper demonstrates spherical convolutional neural networks (S-CNN) offer\ndistinct advantages over conventional fully-connected networks (FCN) at\nestimating scalar parameters of tissue microstructure from diffusion MRI\n(dMRI). Such microstructure parameters are valuable for identifying pathology\nand quantifying its extent. However, current clinical practice commonly\nacquires dMRI data consisting of only 6 diffusion weighted images (DWIs),\nlimiting the accuracy and precision of estimated microstructure indices.\nMachine learning (ML) has been proposed to address this challenge. However,\nexisting ML-based methods are not robust to differing dMRI gradient sampling\nschemes, nor are they rotation equivariant. Lack of robustness to sampling\nschemes requires a new network to be trained for each scheme, complicating the\nanalysis of data from multiple sources. A possible consequence of the lack of\nrotational equivariance is that the training dataset must contain a diverse\nrange of microstucture orientations. Here, we show spherical CNNs represent a\ncompelling alternative that is robust to new sampling schemes as well as\noffering rotational equivariance. We show the latter can be leveraged to\ndecrease the number of training datapoints required.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goodwin_Allcock_T/0/1/0/all/0/1\">Tobias Goodwin-Allcock</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McEwen_J/0/1/0/all/0/1\">Jason McEwen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gray_R/0/1/0/all/0/1\">Robert Gray</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nachev_P/0/1/0/all/0/1\">Parashkev Nachev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video + CLIP Baseline for Ego4D Long-term Action Anticipation. (arXiv:2207.00579v1 [cs.CV])","link":"http://arxiv.org/abs/2207.00579","description":"<p>In this report, we introduce our adaptation of image-text models for\nlong-term action anticipation. Our Video + CLIP framework makes use of a\nlarge-scale pre-trained paired image-text model: CLIP and a video encoder\nSlowfast network. The CLIP embedding provides fine-grained understanding of\nobjects relevant for an action whereas the slowfast network is responsible for\nmodeling temporal information within a video clip of few frames. We show that\nthe features obtained from both encoders are complementary to each other, thus\noutperforming the baseline on Ego4D for the task of long-term action\nanticipation. Our code is available at\ngithub.com/srijandas07/clip_baseline_LTA_Ego4d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Srijan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kernelized Similarity Learning and Embedding for Dynamic Texture Synthesis. (arXiv:1911.04254v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.04254","description":"<p>Dynamic texture (DT) exhibits statistical stationarity in the spatial domain\nand stochastic repetitiveness in the temporal dimension, indicating that\ndifferent frames of DT possess a high similarity correlation that is critical\nprior knowledge. However, existing methods cannot effectively learn a promising\nsynthesis model for high-dimensional DT from a small number of training data.\nIn this paper, we propose a novel DT synthesis method, which makes full use of\nsimilarity prior knowledge to address this issue. Our method bases on the\nproposed kernel similarity embedding, which not only can mitigate the\nhigh-dimensionality and small sample issues, but also has the advantage of\nmodeling nonlinear feature relationship. Specifically, we first raise two\nhypotheses that are essential for DT model to generate new frames using\nsimilarity correlation. Then, we integrate kernel learning and extreme learning\nmachine into a unified synthesis model to learn kernel similarity embedding for\nrepresenting DT. Extensive experiments on DT videos collected from the internet\nand two benchmark datasets, i.e., Gatech Graphcut Textures and Dyntex,\ndemonstrate that the learned kernel similarity embedding can effectively\nexhibit the discriminative representation for DT. Accordingly, our method is\ncapable of preserving the long-term temporal continuity of the synthesized DT\nsequences with excellent sustainability and generalization. Meanwhile, it\neffectively generates realistic DT videos with fast speed and low computation,\ncompared with the state-of-the-art methods. The code and more synthesis videos\nare available at our project page\nhttps://shiming-chen.github.io/Similarity-page/Similarit.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zehong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology-Aware Network Pruning using Multi-stage Graph Embedding and Reinforcement Learning. (arXiv:2102.03214v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.03214","description":"<p>Model compression is an essential technique for deploying deep neural\nnetworks (DNNs) on power and memory-constrained resources. However, existing\nmodel-compression methods often rely on human expertise and focus on\nparameters' local importance, ignoring the rich topology information within\nDNNs. In this paper, we propose a novel multi-stage graph embedding technique\nbased on graph neural networks (GNNs) to identify DNN topologies and use\nreinforcement learning (RL) to find a suitable compression policy. We performed\nresource-constrained (i.e., FLOPs) channel pruning and compared our approach\nwith state-of-the-art model compression methods. We evaluated our method on\nvarious models from typical to mobile-friendly networks, such as ResNet family,\nVGG-16, MobileNet-v1/v2, and ShuffleNet. Results show that our method can\nachieve higher compression ratios with a minimal fine-tuning cost yet yields\noutstanding and competitive performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sixing Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazaheri_A/0/1/0/all/0/1\">Arya Mazaheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1\">Ali Jannesari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M4Depth: Monocular depth estimation for autonomous vehicles in unseen environments. (arXiv:2105.09847v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.09847","description":"<p>Estimating the distance to objects is crucial for autonomous vehicles when\nusing depth sensors is not possible. In this case, the distance has to be\nestimated from on-board mounted RGB cameras, which is a complex task especially\nin environments such as natural outdoor landscapes. In this paper, we present a\nnew method named M4Depth for depth estimation. First, we establish a bijective\nrelationship between depth and the visual disparity of two consecutive frames\nand show how to exploit it to perform motion-invariant pixel-wise depth\nestimation. Then, we detail M4Depth which is based on a pyramidal convolutional\nneural network architecture where each level refines an input disparity map\nestimate by using two customized cost volumes. We use these cost volumes to\nleverage the visual spatio-temporal constraints imposed by motion and to make\nthe network robust for varied scenes. We benchmarked our approach both in test\nand generalization modes on public datasets featuring synthetic camera\ntrajectories recorded in a wide variety of outdoor scenes. Results show that\nour network outperforms the state of the art on these datasets, while also\nperforming well on a standard depth estimation benchmark. The code of our\nmethod is publicly available at https://github.com/michael-fonder/M4Depth.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fonder_M/0/1/0/all/0/1\">Micha&#xeb;l Fonder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSN: Multi-Style Network for Trajectory Prediction. (arXiv:2107.00932v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00932","description":"<p>Trajectory prediction aims to forecast agents' possible future locations\nconsidering their observations along with the video context. It is strongly\nneeded for many autonomous platforms like tracking, detection, robot\nnavigation, and self-driving cars. Whether it is agents' internal personality\nfactors, interactive behaviors with the neighborhood, or the influence of\nsurroundings, all of them might represent impacts on agents' future plannings.\nHowever, many previous methods model and predict agents' behaviors with the\nsame strategy or feature distribution, making them challenging to give\npredictions with sufficient style differences. This manuscript proposes the\nMulti-Style Network (MSN), which utilizes style proposal and stylized\nprediction two sub-networks, to give agents multi-style predictions in a novel\ncategorical way adaptively. The proposed network contains a series of style\nchannels, and each channel is bound to a unique and specific behavior style. In\ndetail, we use agents' end-point plannings and their interaction context as the\nbasis for the behavior classification, so as to adaptively learn multiple\ndiverse behavior styles through these channels. Then, we assume that the target\nagents will plan their future behaviors according to each of these categorized\nstyles, thus utilizing different style channels to give potential predictions\nwith significant style differences in parallel. Experiments show that MSN\noutperforms current state-of-the-art methods up to 10\\% quantitatively on two\nwidely used datasets, and presents better multi-style characteristics\nqualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Wei Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02234","description":"<p>In this paper, we propose a novel hand-based person recognition method for\nthe purpose of criminal investigations since the hand image is often the only\navailable information in cases of serious crime such as sexual abuse. Our\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\nboth channel and spatial attention modules in branches in addition to a global\n(without attention) branch to capture global structural information for\ndiscriminative feature learning. The attention modules focus on the relevant\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. Extensive\nevaluations on two large multi-ethnic and publicly available hand datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nsurpassing the existing hand-based identification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Caption Generation on Scenes with Seen and Unseen Object Categories. (arXiv:2108.06165v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06165","description":"<p>Image caption generation is one of the most challenging problems at the\nintersection of vision and language domains. In this work, we propose a\nrealistic captioning task where the input scenes may incorporate visual objects\nwith no corresponding visual or textual training examples. For this problem, we\npropose a detection-driven approach that consists of a single-stage generalized\nzero-shot detection model to recognize and localize instances of both seen and\nunseen classes, and a template-based captioning model that transforms\ndetections into sentences. To improve the generalized zero-shot detection\nmodel, which provides essential information for captioning, we define effective\nclass representations in terms of class-to-class semantic similarities, and\nleverage their special structure to construct an effective unseen/seen class\nconfidence score calibration mechanism. We also propose a novel evaluation\nmetric that provides additional insights for the captioning outputs by\nseparately measuring the visual and non-visual contents of generated sentences.\nOur experiments highlight the importance of studying captioning in the proposed\nzero-shot setting, and verify the effectiveness of the proposed\ndetection-driven zero-shot captioning approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1\">Berkan Demirel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation. (arXiv:2111.13300v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.13300","description":"<p>We propose a Transformer architecture for volumetric segmentation, a\nchallenging task that requires keeping a complex balance in encoding local and\nglobal spatial cues, and preserving information along all axes of the volume.\nEncoder of the proposed design benefits from self-attention mechanism to\nsimultaneously encode local and global cues, while the decoder employs a\nparallel self and cross attention formulation to capture fine details for\nboundary refinement. Empirically, we show that the proposed design choices\nresult in a computationally efficient model, with competitive and promising\nresults on the Medical Segmentation Decathlon (MSD) brain tumor segmentation\n(BraTS) Task. We further show that the representations learned by our model are\nrobust against data corruptions.\n\\href{https://github.com/himashi92/VT-UNet}{Our code implementation is publicly\navailable}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peiris_H/0/1/0/all/0/1\">Himashi Peiris</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaolin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egan_G/0/1/0/all/0/1\">Gary Egan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Semi-Supervised Learning for Video Action Detection. (arXiv:2203.04251v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04251","description":"<p>In this work, we focus on semi-supervised learning for video action detection\nwhich utilizes both labeled as well as unlabeled data. We propose a simple\nend-to-end consistency based approach which effectively utilizes the unlabeled\ndata. Video action detection requires both, action class prediction as well as\na spatio-temporal localization of actions. Therefore, we investigate two types\nof constraints, classification consistency, and spatio-temporal consistency.\nThe presence of predominant background and static regions in a video makes it\nchallenging to utilize spatio-temporal consistency for action detection. To\naddress this, we propose two novel regularization constraints for\nspatio-temporal consistency; 1) temporal coherency, and 2) gradient smoothness.\nBoth these aspects exploit the temporal continuity of action in videos and are\nfound to be effective for utilizing unlabeled videos for action detection. We\ndemonstrate the effectiveness of the proposed approach on two different action\ndetection benchmark datasets, UCF101-24 and JHMDB-21. In addition, we also show\nthe effectiveness of the proposed approach for video object segmentation on the\nYoutube-VOS which demonstrates its generalization capability The proposed\napproach achieves competitive performance by using merely 20% of annotations on\nUCF101-24 when compared with recent fully supervised methods. On UCF101-24, it\nimproves the score by +8.9% and +11% at 0.5 f-mAP and v-mAP respectively,\ncompared to supervised approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Akash Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh Singh Rawat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Quality Assessment for Magnetic Resonance Imaging. (arXiv:2203.07809v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2203.07809","description":"<p>Image quality assessment (IQA) algorithms aim to reproduce the human's\nperception of the image quality. The growing popularity of image enhancement,\ngeneration, and recovery models instigated the development of many methods to\nassess their performance. However, most IQA solutions are designed to predict\nimage quality in the general domain, with the applicability to specific areas,\nsuch as medical imaging, remaining questionable. Moreover, the selection of\nthese IQA metrics for a specific task typically involves intentionally induced\ndistortions, such as manually added noise or artificial blurring; yet, the\nchosen metrics are then used to judge the output of real-life computer vision\nmodels. In this work, we aspire to fill these gaps by carrying out the most\nextensive IQA evaluation study for Magnetic Resonance Imaging (MRI) to date\n(14,700 subjective scores). We use outputs of neural network models trained to\nsolve problems relevant to MRI, including image reconstruction in the scan\nacceleration, motion correction, and denoising. Our emphasis is on reflecting\nthe radiologist's perception of the reconstructed images, gauging the most\ndiagnostically influential criteria for the quality of MRI scans:\nsignal-to-noise ratio, contrast-to-noise ratio, and the presence of artifacts.\nSeven trained radiologists assess these distorted images, with their verdicts\nthen correlated with 35 different image quality metrics (full-reference,\nno-reference, and distribution-based metrics considered). The top performers --\nDISTS, HaarPSI, VSI, and FID-VGG16 -- are found to be efficient across three\nproposed quality criteria, for all considered anatomies and the target tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kastryulin_S/0/1/0/all/0/1\">Segrey Kastryulin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zakirov_J/0/1/0/all/0/1\">Jamil Zakirov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pezzotti_N/0/1/0/all/0/1\">Nicola Pezzotti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1\">Dmitry V. Dylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Reasoning Meets Visual Representation Learning: A Prospective Study. (arXiv:2204.12037v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.12037","description":"<p>Visual representation learning is ubiquitous in various real-world\napplications, including visual comprehension, video understanding, multi-modal\nanalysis, human-computer interaction, and urban computing. Due to the emergence\nof huge amounts of multi-modal heterogeneous spatial/temporal/spatial-temporal\ndata in big data era, the lack of interpretability, robustness, and\nout-of-distribution generalization are becoming the challenges of the existing\nvisual models. The majority of the existing methods tend to fit the original\ndata/variable distributions and ignore the essential causal relations behind\nthe multi-modal knowledge, which lacks an unified guidance and analysis about\nwhy modern visual representation learning methods are easily collapse into data\nbias and have limited generalization and cognitive abilities. Inspired by the\nstrong inference ability of human-level agents, recent years have therefore\nwitnessed great effort in developing causal reasoning paradigms to realize\nrobust representation and model learning with good cognitive ability. In this\npaper, we conduct a comprehensive review of existing causal reasoning methods\nfor visual representation learning, covering fundamental theories, models, and\ndatasets. The limitations of current methods and datasets are also discussed.\nMoreover, we propose some prospective challenges, opportunities, and future\nresearch directions for benchmarking causal reasoning algorithms in visual\nrepresentation learning. This paper aims to provide a comprehensive overview of\nthis emerging field, attract attention, encourage discussions, bring to the\nforefront the urgency of developing novel causal reasoning methods, publicly\navailable benchmarks, and consensus-building standards for reliable visual\nrepresentation learning and related real-world applications more efficiently.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yushen Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations. (arXiv:2205.06779v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.06779","description":"<p>Recently, weakly-supervised image segmentation using weak annotations like\nscribbles has gained great attention, since such annotations are much easier to\nobtain compared to time-consuming and label-intensive labeling at the\npixel/voxel level. However, because scribbles lack structure information of\nregion of interest (ROI), existing scribble-based methods suffer from poor\nboundary localization. Furthermore, most current methods are designed for 2D\nimage segmentation, which do not fully leverage the volumetric information if\ndirectly applied to image slices. In this paper, we propose a scribble-based\nvolumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image\nsegmentation and improves boundary prediction. To achieve this, we augment a\n2.5D attention UNet with a proposed label propagation module to extend semantic\ninformation from scribbles and a combination of static and active boundary\nprediction to learn ROI's boundary and regularize its shape. Extensive\nexperiments on three public datasets demonstrate Scribble2D5 significantly\noutperforms current scribble-based methods and approaches the performance of\nfully-supervised ones. Our code is available online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiuhui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yi Hong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoLink: Self-supervised Learning of Human Skeletons and Object Outlines by Linking Keypoints. (arXiv:2205.10636v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.10636","description":"<p>Structured representations such as keypoints are widely used in pose\ntransfer, conditional image generation, animation, and 3D reconstruction.\nHowever, their supervised learning requires expensive annotation for each\ntarget domain. We propose a self-supervised method that learns to disentangle\nobject structure from the appearance with a graph of 2D keypoints linked by\nstraight edges. Both the keypoint location and their pairwise edge weights are\nlearned, given only a collection of images depicting the same object class. The\ngraph is interpretable, for example, AutoLink recovers the human skeleton\ntopology when applied to images showing people. Our key ingredients are i) an\nencoder that predicts keypoint locations in an input image, ii) a shared graph\nas a latent variable that links the same pairs of keypoints in every image,\niii) an intermediate edge map that combines the latent graph edge weights and\nkeypoint locations in a soft, differentiable manner, and iv) an inpainting\nobjective on randomly masked images. Although simpler, AutoLink outperforms\nexisting self-supervised methods on the established keypoint and pose\nestimation benchmarks and paves the way for structure-conditioned generative\nmodels on more diverse datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingzhe He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mask2Hand: Learning to Predict the 3D Hand Pose and Shape from Shadow. (arXiv:2205.15553v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.15553","description":"<p>We present a self-trainable method, Mask2Hand, which learns to solve the\nchallenging task of predicting 3D hand pose and shape from a 2D binary mask of\nhand silhouette/shadow without additional manually-annotated data. Given the\nintrinsic camera parameters and the parametric hand model in the camera space,\nwe adopt the differentiable rendering technique to project 3D estimations onto\nthe 2D binary silhouette space. By applying a tailored combination of losses\nbetween the rendered silhouette and the input binary mask, we are able to\nintegrate the self-guidance mechanism into our end-to-end optimization process\nfor constraining global mesh registration and hand pose estimation. The\nexperiments show that our method, which takes a single binary mask as the\ninput, can achieve comparable prediction accuracy on both unaligned and aligned\nsettings as state-of-the-art methods that require RGB or depth inputs. Our code\nis available at https://github.com/lijenchang/Mask2Hand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_L/0/1/0/all/0/1\">Li-Jen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yu-Cheng Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Hui Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BANet: Motion Forecasting with Boundary Aware Network. (arXiv:2206.07934v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.07934","description":"<p>We propose a motion forecasting model called BANet, which means\nBoundary-Aware Network, and it is a variant of LaneGCN. We believe that it is\nnot enough to use only the lane centerline as input to obtain the embedding\nfeatures of the vector map nodes. The lane centerline can only provide the\ntopology of the lanes, and other elements of the vector map also contain rich\ninformation. For example, the lane boundary can provide traffic rule constraint\ninformation such as whether it is possible to change lanes which is very\nimportant. Therefore, we achieved better performance by encoding more vector\nmap elements in the motion forecasting model.We report our results on the 2022\nArgoverse2 Motion Forecasting challenge and rank 1st on the test leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Honglin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning. (arXiv:2206.12596v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.12596","description":"<p>Deformable image registration is a crucial step in medical image analysis for\nfinding a non-linear spatial transformation between a pair of fixed and moving\nimages. Deep registration methods based on Convolutional Neural Networks (CNNs)\nhave been widely used as they can perform image registration in a fast and\nend-to-end manner. However, these methods usually have limited performance for\nimage pairs with large deformations. Recently, iterative deep registration\nmethods have been used to alleviate this limitation, where the transformations\nare iteratively learned in a coarse-to-fine manner. However, iterative methods\ninevitably prolong the registration runtime, and tend to learn separate image\nfeatures for each iteration, which hinders the features from being leveraged to\nfacilitate the registration at later iterations. In this study, we propose a\nNon-Iterative Coarse-to-finE registration Network (NICE-Net) for deformable\nimage registration. In the NICE-Net, we propose: (i) a Single-pass Deep\nCumulative Learning (SDCL) decoder that can cumulatively learn coarse-to-fine\ntransformations within a single pass (iteration) of the network, and (ii) a\nSelectively-propagated Feature Learning (SFL) encoder that can learn common\nimage features for the whole coarse-to-fine registration process and\nselectively propagate the features as needed. Extensive experiments on six\npublic datasets of 3D brain Magnetic Resonance Imaging (MRI) show that our\nproposed NICE-Net can outperform state-of-the-art iterative deep registration\nmethods while only requiring similar runtime to non-iterative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingyuan Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1\">Lei Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1\">Dagan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinman Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning for Action Recognition. (arXiv:2206.13559v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.13559","description":"<p>Capitalizing on large pre-trained models for various downstream tasks of\ninterest have recently emerged with promising performance. Due to the\never-growing model size, the standard full fine-tuning based task adaptation\nstrategy becomes prohibitively costly in terms of model training and storage.\nThis has led to a new research direction in parameter-efficient transfer\nlearning. However, existing attempts typically focus on downstream tasks from\nthe same modality (e.g., image understanding) of the pre-trained model. This\ncreates a limit because in some specific modalities, (e.g., video\nunderstanding) such a strong pre-trained model with sufficient knowledge is\nless or not available. In this work, we investigate such a novel cross-modality\ntransfer learning setting, namely parameter-efficient image-to-video transfer\nlearning. To solve this problem, we propose a new Spatio-Temporal Adapter\n(ST-Adapter) for parameter-efficient fine-tuning per video task. With a\nbuilt-in spatio-temporal reasoning capability in a compact design, ST-Adapter\nenables a pre-trained image model without temporal knowledge to reason about\ndynamic video content at a small (~8%) per-task parameter cost, requiring\napproximately 20 times fewer updated parameters compared to previous work.\nExtensive experiments on video action recognition tasks show that our\nST-Adapter can match or even outperform the strong full fine-tuning strategy\nand state-of-the-art video models, whilst enjoying the advantage of parameter\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Junting Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Ziyi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CTrGAN: Cycle Transformers GAN for Gait Transfer. (arXiv:2206.15248v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15248","description":"<p>We attempt for the first time to address the problem of gait transfer. In\ncontrast to motion transfer, the objective here is not to imitate the source's\nnormal motions, but rather to transform the source's motion into a typical gait\npattern for the target. Using gait recognition models, we demonstrate that\nexisting techniques yield a discrepancy that can be easily detected. We\nintroduce a novel model, Cycle Transformers GAN (CTrGAN), that can successfully\ngenerate the target's natural gait. CTrGAN's generators consist of a decoder\nand encoder, both Transformers, where the attention is on the temporal domain\nbetween complete images rather than the spatial domain between patches. While\nrecent Transformer studies in computer vision mainly focused on discriminative\ntasks, we introduce an architecture that can be applied to synthesis tasks.\nUsing a widely-used gait recognition dataset, we demonstrate that our approach\nis capable of producing over an order of magnitude more realistic personalized\ngaits than existing methods, even when used with sources that were not\navailable during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahpod_S/0/1/0/all/0/1\">Shahar Mahpod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaash_N/0/1/0/all/0/1\">Noam Gaash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Artzi_G/0/1/0/all/0/1\">G. Ben-Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PolarFormer: Multi-camera 3D Object Detection with Polar Transformers. (arXiv:2206.15398v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.15398","description":"<p>3D object detection in autonomous driving aims to reason \"what\" and \"where\"\nthe objects of interest present in a 3D world. Following the conventional\nwisdom of previous 2D object detection, existing methods often adopt the\ncanonical Cartesian coordinate system with perpendicular axis. However, we\nconjugate that this does not fit the nature of the ego car's perspective, as\neach onboard camera perceives the world in shape of wedge intrinsic to the\nimaging geometry with radical (non-perpendicular) axis. Hence, in this paper we\nadvocate the exploitation of the Polar coordinate system and propose a new\nPolar Transformer (PolarFormer) for more accurate 3D object detection in the\nbird's-eye-view (BEV) taking as input only multi-camera 2D images.\nSpecifically, we design a cross attention based Polar detection head without\nrestriction to the shape of input structure to deal with irregular Polar grids.\nFor tackling the unconstrained object scale variations along Polar's distance\ndimension, we further introduce a multi-scalePolar representation learning\nstrategy. As a result, our model can make best use of the Polar representation\nrasterized via attending to the corresponding image observation in a\nsequence-to-sequence fashion subject to the geometric constraints. Thorough\nexperiments on the nuScenes dataset demonstrate that our PolarFormer\noutperforms significantly state-of-the-art 3D object detection alternatives, as\nwell as yielding competitive performance on BEV semantic segmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yanqin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1\">Zhenwei Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiatian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu-Gang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image features of a splashing drop on a solid surface extracted using a feedforward neural network. (arXiv:2201.09541v1 [physics.flu-dyn] CROSS LISTED)","link":"http://arxiv.org/abs/2201.09541","description":"<p>This article reports nonintuitive characteristic of a splashing drop on a\nsolid surface discovered through extracting image features using a feedforward\nneural network (FNN). Ethanol of area-equivalent radius about 1.29 mm was\ndropped from impact heights ranging from 4 cm to 60 cm (splashing threshold 20\ncm) and impacted on a hydrophilic surface. The images captured when half of the\ndrop impacted the surface were labeled according to their outcome, splashing or\nnonsplashing, and were used to train an FNN. A classification accuracy higher\nthan 96% was achieved. To extract the image features identified by the FNN for\nclassification, the weight matrix of the trained FNN for identifying splashing\ndrops was visualized. Remarkably, the visualization showed that the trained FNN\nidentified the contour height of the main body of the impacting drop as an\nimportant characteristic differentiating between splashing and nonsplashing\ndrops, which has not been reported in previous studies. This feature was found\nthroughout the impact, even when one and three-quarters of the drop impacted\nthe surface. To confirm the importance of this image feature, the FNN was\nretrained to classify using only the main body without checking for the\npresence of ejected secondary droplets. The accuracy was still higher than 82%,\nconfirming that the contour height is an important feature distinguishing\nsplashing from nonsplashing drops. Several aspects of drop impact are analyzed\nand discussed with the aim of identifying the possible mechanism underlying the\ndifference in contour height between splashing and nonsplashing drops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Yee_J/0/1/0/all/0/1\">Jingzu Yee</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yamanaka_A/0/1/0/all/0/1\">Akinori Yamanaka</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tagawa_Y/0/1/0/all/0/1\">Yoshiyuki Tagawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-03T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}