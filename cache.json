{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-07-07T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Putting the Con in Context: Identifying Deceptive Actors in the Game of Mafia. (arXiv:2207.02253v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02253","description":"<p>While neural networks demonstrate a remarkable ability to model linguistic\ncontent, capturing contextual information related to a speaker's conversational\nrole is an open area of research. In this work, we analyze the effect of\nspeaker role on language use through the game of Mafia, in which participants\nare assigned either an honest or a deceptive role. In addition to building a\nframework to collect a dataset of Mafia game records, we demonstrate that there\nare differences in the language produced by players with different roles. We\nconfirm that classification models are able to rank deceptive players as more\nsuspicious than honest ones based only on their use of language. Furthermore,\nwe show that training models on two auxiliary tasks outperforms a standard\nBERT-based text classification approach. We also present methods for using our\ntrained models to identify features that distinguish between player roles,\nwhich could be used to assist players during the Mafia game.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibraheem_S/0/1/0/all/0/1\">Samee Ibraheem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Gaoyue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DeNero_J/0/1/0/all/0/1\">John DeNero</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving the Faithfulness of Abstractive Summarization via Entity Coverage Control. (arXiv:2207.02263v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02263","description":"<p>Abstractive summarization systems leveraging pre-training language models\nhave achieved superior results on benchmark datasets. However, such models have\nbeen shown to be more prone to hallucinate facts that are unfaithful to the\ninput context. In this paper, we propose a method to remedy entity-level\nextrinsic hallucinations with Entity Coverage Control (ECC). We first compute\nentity coverage precision and prepend the corresponding control code for each\ntraining example, which implicitly guides the model to recognize faithfulness\ncontents in the training phase. We further extend our method via intermediate\nfine-tuning on large but noisy data extracted from Wikipedia to unlock\nzero-shot summarization. We show that the proposed method leads to more\nfaithful and salient abstractive summarization in supervised fine-tuning and\nzero-shot settings according to our experimental results on three benchmark\ndatasets XSum, Pubmed, and SAMSum of very different domains and styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1\">Wojciech Kryscinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pretraining on Interactions for Learning Grounded Affordance Representations. (arXiv:2207.02272v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02272","description":"<p>Lexical semantics and cognitive science point to affordances (i.e. the\nactions that objects support) as critical for understanding and representing\nnouns and verbs. However, study of these semantic features has not yet been\nintegrated with the \"foundation\" models that currently dominate language\nrepresentation research. We hypothesize that predictive modeling of object\nstate over time will result in representations that encode object affordance\ninformation \"for free\". We train a neural network to predict objects'\ntrajectories in a simulated interaction and show that our network's latent\nrepresentations differentiate between both observed and unobserved affordances.\nWe find that models trained using 3D simulations from our SPATIAL dataset\noutperform conventional 2D computer vision models trained on a similar task,\nand, on initial inspection, that differences between concepts correspond to\nexpected features (e.g., roll entails rotation). Our results suggest a way in\nwhich modern deep learning approaches to grounded language learning can be\nintegrated with traditional formal semantic notions of lexical representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merullo_J/0/1/0/all/0/1\">Jack Merullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebert_D/0/1/0/all/0/1\">Dylan Ebert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1\">Carsten Eickhoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-shot Cross-Linguistic Learning of Event Semantics. (arXiv:2207.02356v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02356","description":"<p>Typologically diverse languages offer systems of lexical and grammatical\naspect that allow speakers to focus on facets of event structure in ways that\ncomport with the specific communicative setting and discourse constraints they\nface. In this paper, we look specifically at captions of images across Arabic,\nChinese, Farsi, German, Russian, and Turkish and describe a computational model\nfor predicting lexical aspects. Despite the heterogeneity of these languages,\nand the salient invocation of distinctive linguistic resources across their\ncaption corpora, speakers of these languages show surprising similarities in\nthe ways they frame image content. We leverage this observation for zero-shot\ncross-lingual learning and show that lexical aspects can be predicted for a\ngiven language despite not having observed any annotated data for this language\nat all.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kober_T/0/1/0/all/0/1\">Thomas Kober</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alhafni_B/0/1/0/all/0/1\">Bashar Alhafni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert Inan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_E/0/1/0/all/0/1\">Elizabeth Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raji_S/0/1/0/all/0/1\">Shahab Raji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compute Cost Amortized Transformer for Streaming ASR. (arXiv:2207.02393v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02393","description":"<p>We present a streaming, Transformer-based end-to-end automatic speech\nrecognition (ASR) architecture which achieves efficient neural inference\nthrough compute cost amortization. Our architecture creates sparse computation\npathways dynamically at inference time, resulting in selective use of compute\nresources throughout decoding, enabling significant reductions in compute with\nminimal impact on accuracy. The fully differentiable architecture is trained\nend-to-end with an accompanying lightweight arbitrator mechanism operating at\nthe frame-level to make dynamic decisions on each input while a tunable loss\nfunction is used to regularize the overall level of compute against predictive\nperformance. We report empirical results from experiments using the compute\namortized Transformer-Transducer (T-T) model conducted on LibriSpeech data. Our\nbest model can achieve a 60% compute cost reduction with only a 3% relative\nword error rate (WER) increase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macoskey_J/0/1/0/all/0/1\">Jonathan Macoskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1\">Martin Radfar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_F/0/1/0/all/0/1\">Feng-Ju Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1\">Brian King</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mouchtaris_A/0/1/0/all/0/1\">Athanasios Mouchtaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strimel_G/0/1/0/all/0/1\">Grant P. Strimel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BioTABQA: Instruction Learning for Biomedical Table Question Answering. (arXiv:2207.02419v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02419","description":"<p>Table Question Answering (TQA) is an important but under-explored task. Most\nof the existing QA datasets are in unstructured text format and only few of\nthem use tables as the context. To the best of our knowledge, none of TQA\ndatasets exist in the biomedical domain where tables are frequently used to\npresent information. In this paper, we first curate a table question answering\ndataset, BioTABQA, using 22 templates and the context from a biomedical\ntextbook on differential diagnosis. BioTABQA can not only be used to teach a\nmodel how to answer questions from tables but also evaluate how a model\ngeneralizes to unseen questions, an important scenario for biomedical\napplications. To achieve the generalization evaluation, we divide the templates\ninto 17 training and 5 cross-task evaluations. Then, we develop two baselines\nusing single and multi-tasks learning on BioTABQA. Furthermore, we explore\ninstructional learning, a recent technique showing impressive generalizing\nperformance. Experimental results show that our instruction-tuned model\noutperforms single and multi-task baselines on an average by ~23% and ~6%\nacross various evaluation settings, and more importantly, instruction-tuned\nmodel outperforms baselines by ~5% on cross-tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Man Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Sharad Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aspect-Based Sentiment Analysis using Local Context Focus Mechanism with DeBERTa. (arXiv:2207.02424v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02424","description":"<p>Text sentiment analysis, also known as opinion mining, is research on the\ncalculation of people's views, evaluations, attitude and emotions expressed by\nentities. Text sentiment analysis can be divided into text-level sentiment\nanalysis, sen-tence-level sentiment analysis and aspect-level sentiment\nanalysis. Aspect-Based Sentiment Analysis (ABSA) is a fine-grained task in the\nfield of sentiment analysis, which aims to predict the polarity of aspects. The\nresearch of pre-training neural model has significantly improved the\nperformance of many natural language processing tasks. In recent years, pre\ntraining model (PTM) has been applied in ABSA. Therefore, there has been a\nquestion, which is whether PTMs contain sufficient syntactic information for\nABSA. In this paper, we explored the recent DeBERTa model (Decoding-enhanced\nBERT with disentangled attention) to solve Aspect-Based Sentiment Analysis\nproblem. DeBERTa is a kind of neural language model based on transformer, which\nuses self-supervised learning to pre-train on a large number of original text\ncorpora. Based on the Local Context Focus (LCF) mechanism, by integrating\nDeBERTa model, we purpose a multi-task learning model for aspect-based\nsentiment analysis. The experiments result on the most commonly used the laptop\nand restaurant datasets of SemEval-2014 and the ACL twitter dataset show that\nLCF mechanism with DeBERTa has significant improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Junping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Ang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Zeli Guan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EEPT: Early Discovery of Emerging Entities in Twitter with Semantic Similarity. (arXiv:2207.02434v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02434","description":"<p>Some events which happen in the future could be important for companies,\ngovernments, and even our personal life. Prediction of these events before\ntheir establishment is helpful for efficient decision-making. We call such\nevents emerging entities. They have not taken place yet, and there is no\ninformation about them in KB. However, some clues exist in different areas,\nespecially on social media. Thus, retrieving these type of entities are\npossible. This paper proposes a method of early discovery of emerging entities.\nWe use semantic clustering of short messages. To evaluate the performance of\nour proposal, we devise and utilize a performance evaluation metric. The\nresults show that our proposed method finds those emerging entities of which\nTwitter trends are not always capable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yousefi_S/0/1/0/all/0/1\">Shahin Yousefi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooshmand_M/0/1/0/all/0/1\">Mohsen Hooshmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afsharchi_M/0/1/0/all/0/1\">Mohsen Afsharchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Brain-inspired probabilistic generative model for double articulation analysis of spoken language. (arXiv:2207.02457v1 [q-bio.NC])","link":"http://arxiv.org/abs/2207.02457","description":"<p>The human brain, among its several functions, analyzes the double\narticulation structure in spoken language, i.e., double articulation analysis\n(DAA). A hierarchical structure in which words are connected to form a sentence\nand words are composed of phonemes or syllables is called a double articulation\nstructure. Where and how DAA is performed in the human brain has not been\nestablished, although some insights have been obtained. In addition, existing\ncomputational models based on a probabilistic generative model (PGM) do not\nincorporate neuroscientific findings, and their consistency with the brain has\nnot been previously discussed. This study compared, mapped, and integrated\nthese existing computational models with neuroscientific findings to bridge\nthis gap, and the findings are relevant for future applications and further\nresearch. This study proposes a PGM for a DAA hypothesis that can be realized\nin the brain based on the outcomes of several neuroscientific surveys. The\nstudy involved (i) investigation and organization of anatomical structures\nrelated to spoken language processing, and (ii) design of a PGM that matches\nthe anatomy and functions of the region of interest. Therefore, this study\nprovides novel insights that will be foundational to further exploring DAA in\nthe brain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Taniguchi_A/0/1/0/all/0/1\">Akira Taniguchi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Muro_M/0/1/0/all/0/1\">Maoko Muro</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yamakawa_H/0/1/0/all/0/1\">Hiroshi Yamakawa</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Taniguchi_T/0/1/0/all/0/1\">Tadahiro Taniguchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gender Biases and Where to Find Them: Exploring Gender Bias in Pre-Trained Transformer-based Language Models Using Movement Pruning. (arXiv:2207.02463v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02463","description":"<p>Language model debiasing has emerged as an important field of study in the\nNLP community. Numerous debiasing techniques were proposed, but bias ablation\nremains an unaddressed issue. We demonstrate a novel framework for inspecting\nbias in pre-trained transformer-based language models via movement pruning.\nGiven a model and a debiasing objective, our framework finds a subset of the\nmodel containing less bias than the original model. We implement our framework\nby pruning the model while fine-tuning it on the debiasing objective. Optimized\nare only the pruning scores - parameters coupled with the model's weights that\nact as gates. We experiment with pruning attention heads, an important building\nblock of transformers: we prune square blocks, as well as establish a new way\nof pruning the entire heads. Lastly, we demonstrate the usage of our framework\nusing gender bias, and based on our findings, we propose an improvement to an\nexisting debiasing method. Additionally, we re-discover a bias-performance\ntrade-off: the better the model performs, the more bias it contains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joniak_P/0/1/0/all/0/1\">Przemyslaw Joniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1\">Akiko Aizawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compositional Generalization in Grounded Language Learning via Induced Model Sparsity. (arXiv:2207.02518v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02518","description":"<p>We provide a study of how induced model sparsity can help achieve\ncompositional generalization and better sample efficiency in grounded language\nlearning problems. We consider simple language-conditioned navigation problems\nin a grid world environment with disentangled observations. We show that\nstandard neural architectures do not always yield compositional generalization.\nTo address this, we design an agent that contains a goal identification module\nthat encourages sparse correlations between words in the instruction and\nattributes of objects, composing them together to find the goal. The output of\nthe goal identification module is the input to a value iteration network\nplanner. Our agent maintains a high level of performance on goals containing\nnovel combinations of properties even when learning from a handful of\ndemonstrations. We examine the internal representations of our agent and find\nthe correct correspondences between words in its dictionary and attributes in\nthe environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spilsbury_S/0/1/0/all/0/1\">Sam Spilsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1\">Alexander Ilin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Role of Complex NLP in Transformers for Text Ranking?. (arXiv:2207.02522v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02522","description":"<p>Even though term-based methods such as BM25 provide strong baselines in\nranking, under certain conditions they are dominated by large pre-trained\nmasked language models (MLMs) such as BERT. To date, the source of their\neffectiveness remains unclear. Is it their ability to truly understand the\nmeaning through modeling syntactic aspects? We answer this by manipulating the\ninput order and position information in a way that destroys the natural\nsequence order of query and passage and shows that the model still achieves\ncomparable performance. Overall, our results highlight that syntactic aspects\ndo not play a critical role in the effectiveness of re-ranking with BERT. We\npoint to other mechanisms such as query-passage cross-attention and richer\nembeddings that capture word meanings based on aggregated context regardless of\nthe word order for being the main attributions for its superior performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rau_D/0/1/0/all/0/1\">David Rau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamps_J/0/1/0/all/0/1\">Jaap Kamps</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Diversify for Product Question Generation. (arXiv:2207.02534v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02534","description":"<p>We address the product question generation task. For a given product\ndescription, our goal is to generate questions that reflect potential user\ninformation needs that are either missing or not well covered in the\ndescription. Moreover, we wish to cover diverse user information needs that may\nspan a multitude of product types. To this end, we first show how the T5\npre-trained Transformer encoder-decoder model can be fine-tuned for the task.\nYet, while the T5 generated questions have a reasonable quality compared to the\nstate-of-the-art method for the task (KPCNet), many of such questions are still\ntoo general, resulting in a sub-optimal global question diversity. As an\nalternative, we propose a novel learning-to-diversify (LTD) fine-tuning\napproach that allows to enrich the language learned by the underlying\nTransformer model. Our empirical evaluation shows that, using our approach\nsignificantly improves the global diversity of the underlying Transformer\nmodel, while preserves, as much as possible, its generation relevance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roitman_H/0/1/0/all/0/1\">Haggai Roitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singer_U/0/1/0/all/0/1\">Uriel Singer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshel_Y/0/1/0/all/0/1\">Yotam Eshel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nus_A/0/1/0/all/0/1\">Alexander Nus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiperwasser_E/0/1/0/all/0/1\">Eliyahu Kiperwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing Earlier what Right Means to You: A Comprehensive VQA Dataset for Grounding Relative Directions via Multi-Task Learning. (arXiv:2207.02624v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02624","description":"<p>Spatial reasoning poses a particular challenge for intelligent agents and is\nat the same time a prerequisite for their successful interaction and\ncommunication in the physical world. One such reasoning task is to describe the\nposition of a target object with respect to the intrinsic orientation of some\nreference object via relative directions. In this paper, we introduce\nGRiD-A-3D, a novel diagnostic visual question-answering (VQA) dataset based on\nabstract objects. Our dataset allows for a fine-grained analysis of end-to-end\nVQA models' capabilities to ground relative directions. At the same time, model\ntraining requires considerably fewer computational resources compared with\nexisting datasets, yet yields a comparable or even higher performance. Along\nwith the new dataset, we provide a thorough evaluation based on two widely\nknown end-to-end VQA architectures trained on GRiD-A-3D. We demonstrate that\nwithin a few epochs, the subtasks required to reason over relative directions,\nsuch as recognizing and locating objects in a scene and estimating their\nintrinsic orientations, are learned in the order in which relative directions\nare intuitively processed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahrens_K/0/1/0/all/0/1\">Kyra Ahrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1\">Matthias Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Challenge on Semi-Supervised and Reinforced Task-Oriented Dialog Systems. (arXiv:2207.02657v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02657","description":"<p>A challenge on Semi-Supervised and Reinforced Task-Oriented Dialog Systems,\nCo-located with EMNLP2022 SereTOD Workshop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ou_Z/0/1/0/all/0/1\">Zhijian Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Junlan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yakun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiangjiang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Kaggle Competition: Cantonese Audio-Visual Speech Recognition for In-car Commands. (arXiv:2207.02663v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02663","description":"<p>With the rise of deep learning and intelligent vehicles, the smart assistant\nhas become an essential in-car component to facilitate driving and provide\nextra functionalities. In-car smart assistants should be able to process\ngeneral as well as car-related commands and perform corresponding actions,\nwhich eases driving and improves safety. However, in this research field, most\ndatasets are in major languages, such as English and Chinese. There is a huge\ndata scarcity issue for low-resource languages, hindering the development of\nresearch and applications for broader communities. Therefore, it is crucial to\nhave more benchmarks to raise awareness and motivate the research in\nlow-resource languages. To mitigate this problem, we collect a new dataset,\nnamely Cantonese In-car Audio-Visual Speech Recognition (CI-AVSR), for in-car\nspeech recognition in the Cantonese language with video and audio data.\nTogether with it, we propose Cantonese Audio-Visual Speech Recognition for\nIn-car Commands as a new challenge for the community to tackle low-resource\nspeech recognition under in-car scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1\">Samuel Cahyawijaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barezi_E/0/1/0/all/0/1\">Elham J Barezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the Value of Gazetteer in Chinese Named Entity Recognition. (arXiv:2207.02802v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02802","description":"<p>Gazetteer is widely used in Chinese named entity recognition (NER) to enhance\nspan boundary detection and type classification. However, to further understand\nthe generalizability and effectiveness of gazetteers, the NLP community still\nlacks a systematic analysis of the gazetteer-enhanced NER model. In this paper,\nwe first re-examine the effectiveness several common practices of the\ngazetteer-enhanced NER models and carry out a series of detailed analysis to\nevaluate the relationship between the model performance and the gazetteer\ncharacteristics, which can guide us to build a more suitable gazetteer. The\nfindings of this paper are as follows: (1) the gazetteer has improved the most\nsituations where the dataset is difficult to learn well for the conventional\nNER model. (2) the performance of model greatly benefits from the high-quality\npre-trained lexeme embeddings. (3) a good gazetteer should cover more entities\nthat can be matched in both the training set and testing set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qianglong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangji Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiangang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bojia Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Strong Heuristics for Named Entity Linking. (arXiv:2207.02824v1 [cs.CL])","link":"http://arxiv.org/abs/2207.02824","description":"<p>Named entity linking (NEL) in news is a challenging endeavour due to the\nfrequency of unseen and emerging entities, which necessitates the use of\nunsupervised or zero-shot methods. However, such methods tend to come with\ncaveats, such as no integration of suitable knowledge bases (like Wikidata) for\nemerging entities, a lack of scalability, and poor interpretability. Here, we\nconsider person disambiguation in Quotebank, a massive corpus of\nspeaker-attributed quotations from the news, and investigate the suitability of\nintuitive, lightweight, and scalable heuristics for NEL in web-scale corpora.\nOur best performing heuristic disambiguates 94% and 63% of the mentions on\nQuotebank and the AIDA-CoNLL benchmark, respectively. Additionally, the\nproposed heuristics compare favourably to the state-of-the-art unsupervised and\nzero-shot methods, Eigenthemes and mGENRE, respectively, thereby serving as\nstrong baselines for unsupervised and zero-shot entity linking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Culjak_M/0/1/0/all/0/1\">Marko &#x10c;uljak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spitz_A/0/1/0/all/0/1\">Andreas Spitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Akhil Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric. (arXiv:2110.05376v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.05376","description":"<p>Measuring automatic speech recognition (ASR) system quality is critical for\ncreating user-satisfying voice-driven applications. Word Error Rate (WER) has\nbeen traditionally used to evaluate ASR system quality; however, it sometimes\ncorrelates poorly with user perception/judgement of transcription quality. This\nis because WER weighs every word equally and does not consider semantic\ncorrectness which has a higher impact on user perception. In this work, we\npropose evaluating ASR output hypotheses quality with SemDist that can measure\nsemantic correctness by using the distance between the semantic vectors of the\nreference and hypothesis extracted from a pre-trained language model. Our\nexperimental results of 71K and 36K user annotated ASR output quality show that\nSemDist achieves higher correlation with user perception than WER. We also show\nthat SemDist has higher correlation with downstream Natural Language\nUnderstanding (NLU) tasks than WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Suyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Weiyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_T/0/1/0/all/0/1\">Tarun Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Abhinav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1\">Xiaoyu Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuegen_C/0/1/0/all/0/1\">Christian Fuegen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHAS: Approaching optimal Segmentation for End-to-End Speech Translation. (arXiv:2202.04774v3 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2202.04774","description":"<p>Speech translation models are unable to directly process long audios, like\nTED talks, which have to be split into shorter segments. Speech translation\ndatasets provide manual segmentations of the audios, which are not available in\nreal-world scenarios, and existing segmentation methods usually significantly\nreduce translation quality at inference time. To bridge the gap between the\nmanual segmentation of training and the automatic one at inference, we propose\nSupervised Hybrid Audio Segmentation (SHAS), a method that can effectively\nlearn the optimal segmentation from any manually segmented speech corpus.\nFirst, we train a classifier to identify the included frames in a segmentation,\nusing speech representations from a pre-trained wav2vec 2.0. The optimal\nsplitting points are then found by a probabilistic Divide-and-Conquer algorithm\nthat progressively splits at the frame of lowest probability until all segments\nare below a pre-specified length. Experiments on MuST-C and mTEDx show that the\ntranslation of the segments produced by our method approaches the quality of\nthe manual segmentation on 5 language pairs. Namely, SHAS retains 95-98% of the\nmanual segmentation's BLEU score, compared to the 87-93% of the best existing\nmethods. Our method is additionally generalizable to different domains and\nachieves high zero-shot performance in unseen languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Large-Scale Hate Speech Detection with Cross-Domain Transfer. (arXiv:2203.01111v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.01111","description":"<p>The performance of hate speech detection models relies on the datasets on\nwhich the models are trained. Existing datasets are mostly prepared with a\nlimited number of instances or hate domains that define hate topics. This\nhinders large-scale analysis and transfer learning with respect to hate\ndomains. In this study, we construct large-scale tweet datasets for hate speech\ndetection in English and a low-resource language, Turkish, consisting of\nhuman-labeled 100k tweets per each. Our datasets are designed to have equal\nnumber of tweets distributed over five domains. The experimental results\nsupported by statistical tests show that Transformer-based language models\noutperform conventional bag-of-words and neural models by at least 5% in\nEnglish and 10% in Turkish for large-scale hate speech detection. The\nperformance is also scalable to different training sizes, such that 98% of\nperformance in English, and 97% in Turkish, are recovered when 20% of training\ninstances are used. We further examine the generalization ability of\ncross-domain transfer among hate domains. We show that 96% of the performance\nof a target domain in average is recovered by other domains for English, and\n92% for Turkish. Gender and religion are more successful to generalize to other\ndomains, while sports fail most.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahinuc_F/0/1/0/all/0/1\">Furkan &#x15e;ahinu&#xe7;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Eyup Halit Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2203.17255","description":"<p>This theoretical article examines how to construct human-like working memory\nand thought processes within a computer. There should be two working memory\nstores, one analogous to sustained firing in association cortex, and one\nanalogous to synaptic potentiation in the cerebral cortex. These stores must be\nconstantly updated with new representations that arise from either\nenvironmental stimulation or internal processing. They should be updated\ncontinuously, and in an iterative fashion, meaning that, in the next state,\nsome items in the set of coactive items should always be retained. Thus, the\nset of concepts coactive in working memory will evolve gradually and\nincrementally over time. This makes each state is a revised iteration of the\npreceding state and causes successive states to overlap and blend with respect\nto the set of representations they contain. As new representations are added\nand old ones are subtracted, some remain active for several seconds over the\ncourse of these changes. This persistent activity, similar to that used in\nartificial recurrent neural networks, is used to spread activation energy\nthroughout the global workspace to search for the next associative update. The\nresult is a chain of associatively linked intermediate states that are capable\nof advancing toward a solution or goal. Iterative updating is conceptualized\nhere as an information processing strategy, a computational and\nneurophysiological determinant of the stream of thought, and an algorithm for\ndesigning and programming artificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1\">Jared Edward Reser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology Reuse: the Real Test of Ontological Design. (arXiv:2205.02892v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2205.02892","description":"<p>Reusing ontologies in practice is still very challenging, especially when\nmultiple ontologies are (jointly) involved. Moreover, despite recent advances,\nthe realization of systematic ontology quality assurance remains a difficult\nproblem. In this work, the quality of thirty biomedical ontologies, and the\nComputer Science Ontology are investigated, from the perspective of a practical\nuse case. Special scrutiny is given to cross-ontology references, which are\nvital for combining ontologies. Diverse methods to detect potential issues are\nproposed, including natural language processing and network analysis. Moreover,\nseveral suggestions for improving ontologies and their quality assurance\nprocesses are presented. It is argued that while the advancing automatic tools\nfor ontology quality assurance are crucial for ontology improvement, they will\nnot solve the problem entirely. It is ontology reuse that is the ultimate\nmethod for continuously verifying and improving ontology quality, as well as\nfor guiding its future development. Specifically, multiple issues can be found\nand fixed primarily through practical and diverse ontology reuse scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sowinski_P/0/1/0/all/0/1\">Piotr Sowinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasielewska_Michniewska_K/0/1/0/all/0/1\">Katarzyna Wasielewska-Michniewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganzha_M/0/1/0/all/0/1\">Maria Ganzha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paprzycki_M/0/1/0/all/0/1\">Marcin Paprzycki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badica_C/0/1/0/all/0/1\">Costin Badica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciTweets -- A Dataset and Annotation Framework for Detecting Scientific Online Discourse. (arXiv:2206.07360v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2206.07360","description":"<p>Scientific topics, claims and resources are increasingly debated as part of\nonline discourse, where prominent examples include discourse related to\nCOVID-19 or climate change. This has led to both significant societal impact\nand increased interest in scientific online discourse from various disciplines.\nFor instance, communication studies aim at a deeper understanding of biases,\nquality or spreading pattern of scientific information whereas computational\nmethods have been proposed to extract, classify or verify scientific claims\nusing NLP and IR techniques. However, research across disciplines currently\nsuffers from both a lack of robust definitions of the various forms of\nscience-relatedness as well as appropriate ground truth data for distinguishing\nthem. In this work, we contribute (a) an annotation framework and corresponding\ndefinitions for different forms of scientific relatedness of online discourse\nin Tweets, (b) an expert-annotated dataset of 1261 tweets obtained through our\nlabeling framework reaching an average Fleiss Kappa $\\kappa$ of 0.63, (c) a\nmulti-label classifier trained on our data able to detect science-relatedness\nwith 89% F1 and also able to detect distinct forms of scientific knowledge\n(claims, references). With this work we aim to lay the foundation for\ndeveloping and evaluating robust methods for analysing science as part of\nlarge-scale online discourse.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hafid_S/0/1/0/all/0/1\">Salim Hafid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schellhammer_S/0/1/0/all/0/1\">Sebastian Schellhammer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bringay_S/0/1/0/all/0/1\">Sandra Bringay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorov_K/0/1/0/all/0/1\">Konstantin Todorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dietze_S/0/1/0/all/0/1\">Stefan Dietze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MIA 2022 Shared Task Submission: Leveraging Entity Representations, Dense-Sparse Hybrids, and Fusion-in-Decoder for Cross-Lingual Question Answering. (arXiv:2207.01940v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2207.01940","description":"<p>We describe our two-stage system for the Multilingual Information Access\n(MIA) 2022 Shared Task on Cross-Lingual Open-Retrieval Question Answering. The\nfirst stage consists of multilingual passage retrieval with a hybrid dense and\nsparse retrieval strategy. The second stage consists of a reader which outputs\nthe answer from the top passages returned by the first stage. We show the\nefficacy of using entity representations, sparse retrieval signals to help\ndense retrieval, and Fusion-in-Decoder. On the development set, we obtain 43.46\nF1 on XOR-TyDi QA and 21.99 F1 on MKQA, for an average F1 score of 32.73. On\nthe test set, we obtain 40.93 F1 on XOR-TyDi QA and 22.29 F1 on MKQA, for an\naverage F1 score of 31.61. We improve over the official baseline by over 4 F1\npoints on both the development and test sets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1\">Zhucheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padmanabhan_S/0/1/0/all/0/1\">Sarguna Janani Padmanabhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Guiding Machine Perception with Psychophysics. (arXiv:2207.02241v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02241","description":"<p>{G}{ustav} Fechner's 1860 delineation of psychophysics, the measurement of\nsensation in relation to its stimulus, is widely considered to be the advent of\nmodern psychological science. In psychophysics, a researcher parametrically\nvaries some aspects of a stimulus, and measures the resulting changes in a\nhuman subject's experience of that stimulus; doing so gives insight to the\ndetermining relationship between a sensation and the physical input that evoked\nit. This approach is used heavily in perceptual domains, including signal\ndetection, threshold measurement, and ideal observer analysis. Scientific\nfields like vision science have always leaned heavily on the methods and\nprocedures of psychophysics, but there is now growing appreciation of them by\nmachine learning researchers, sparked by widening overlap between biological\nand artificial perception \\cite{rojas2011automatic,\nscheirer2014perceptual,escalera2014chalearn,zhang2018agil,\ngrieggs2021measuring}. Machine perception that is guided by behavioral\nmeasurements, as opposed to guidance restricted to arbitrarily assigned human\nlabels, has significant potential to fuel further progress in artificial\nintelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dulay_J/0/1/0/all/0/1\">Justin Dulay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poltoratski_S/0/1/0/all/0/1\">Sonia Poltoratski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_T/0/1/0/all/0/1\">Till S. Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anthony_S/0/1/0/all/0/1\">Samuel E. Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheirer_W/0/1/0/all/0/1\">Walter J. Scheirer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based Surgical Skills Assessment using Long term Tool Tracking. (arXiv:2207.02247v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02247","description":"<p>Mastering the technical skills required to perform surgery is an extremely\nchallenging task. Video-based assessment allows surgeons to receive feedback on\ntheir technical skills to facilitate learning and development. Currently, this\nfeedback comes primarily from manual video review, which is time-intensive and\nlimits the feasibility of tracking a surgeon's progress over many cases. In\nthis work, we introduce a motion-based approach to automatically assess\nsurgical skills from surgical case video feed. The proposed pipeline first\ntracks surgical tools reliably to create motion trajectories and then uses\nthose trajectories to predict surgeon technical skill levels. The tracking\nalgorithm employs a simple yet effective re-identification module that improves\nID-switch compared to other state-of-the-art methods. This is critical for\ncreating reliable tool trajectories when instruments regularly move on- and\noff-screen or are periodically obscured. The motion-based classification model\nemploys a state-of-the-art self-attention transformer network to capture short-\nand long-term motion patterns that are essential for skill evaluation. The\nproposed method is evaluated on an in-vivo (Cholec80) dataset where an\nexpert-rated GOALS skill assessment of the Calot Triangle Dissection is used as\na quantitative skill measure. We compare transformer-based skill assessment\nwith traditional machine learning approaches using the proposed and\nstate-of-the-art tracking. Our result suggests that using motion trajectories\nfrom reliable tracking methods is beneficial for assessing surgeon skills based\nsolely on video streams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fathollahi_M/0/1/0/all/0/1\">Mona Fathollahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarhan_M/0/1/0/all/0/1\">Mohammad Hasan Sarhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pena_R/0/1/0/all/0/1\">Ramon Pena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DiMonte_L/0/1/0/all/0/1\">Lela DiMonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Anshu Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataliwala_A/0/1/0/all/0/1\">Aishani Ataliwala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barker_J/0/1/0/all/0/1\">Jocelyn Barker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Array Camera Image Fusion using Physics-Aware Transformers. (arXiv:2207.02250v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02250","description":"<p>We demonstrate a physics-aware transformer for feature-based data fusion from\ncameras with diverse resolution, color spaces, focal planes, focal lengths, and\nexposure. We also demonstrate a scalable solution for synthetic training data\ngeneration for the transformer using open-source computer graphics software. We\ndemonstrate image synthesis on arrays with diverse spectral responses,\ninstantaneous field of view and frame rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1\">Minghao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brady_D/0/1/0/all/0/1\">David Jones Brady</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSFormer: One-Stage Camouflaged Instance Segmentation with Transformers. (arXiv:2207.02255v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02255","description":"<p>We present OSFormer, the first one-stage transformer framework for\ncamouflaged instance segmentation (CIS). OSFormer is based on two key designs.\nFirst, we design a location-sensing transformer (LST) to obtain the location\nlabel and instance-aware parameters by introducing the location-guided queries\nand the blend-convolution feedforward network. Second, we develop a\ncoarse-to-fine fusion (CFF) to merge diverse context information from the LST\nencoder and CNN backbone. Coupling these two components enables OSFormer to\nefficiently blend local features and long-range context dependencies for\npredicting camouflaged instances. Compared with two-stage frameworks, our\nOSFormer reaches 41% AP and achieves good convergence efficiency without\nrequiring enormous training data, i.e., only 3,040 samples under 60 epochs.\nCode link: https://github.com/PJLallen/OSFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jialun Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Tianyang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">He Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chuanbo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenLDN: Learning to Discover Novel Classes for Open-World Semi-Supervised Learning. (arXiv:2207.02261v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02261","description":"<p>Semi-supervised learning (SSL) is one of the dominant approaches to address\nthe annotation bottleneck of supervised learning. Recent SSL methods can\neffectively leverage a large repository of unlabeled data to improve\nperformance while relying on a small set of labeled data. One common assumption\nin most SSL methods is that the labeled and unlabeled data are from the same\nunderlying data distribution. However, this is hardly the case in many\nreal-world scenarios, which limits their applicability. In this work, instead,\nwe attempt to solve the recently proposed challenging open-world SSL problem\nthat does not make such an assumption. In the open-world SSL problem, the\nobjective is to recognize samples of known classes, and simultaneously detect\nand cluster samples belonging to novel classes present in unlabeled data. This\nwork introduces OpenLDN that utilizes a pairwise similarity loss to discover\nnovel classes. Using a bi-level optimization rule this pairwise similarity loss\nexploits the information available in the labeled set to implicitly cluster\nnovel class samples, while simultaneously recognizing samples from known\nclasses. After discovering novel classes, OpenLDN transforms the open-world SSL\nproblem into a standard SSL problem to achieve additional performance gains\nusing existing SSL methods. Our extensive experiments demonstrate that OpenLDN\noutperforms the current state-of-the-art methods on multiple popular\nclassification benchmarks while providing a better accuracy/training time\ntrade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Realistic Semi-Supervised Learning. (arXiv:2207.02269v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02269","description":"<p>Deep learning is pushing the state-of-the-art in many computer vision\napplications. However, it relies on large annotated data repositories, and\ncapturing the unconstrained nature of the real-world data is yet to be solved.\nSemi-supervised learning (SSL) complements the annotated training data with a\nlarge corpus of unlabeled data to reduce annotation cost. The standard SSL\napproach assumes unlabeled data are from the same distribution as annotated\ndata. Recently, ORCA [9] introduce a more realistic SSL problem, called\nopen-world SSL, by assuming that the unannotated data might contain samples\nfrom unknown classes. This work proposes a novel approach to tackle SSL in\nopen-world setting, where we simultaneously learn to classify known and unknown\nclasses. At the core of our method, we utilize sample uncertainty and\nincorporate prior knowledge about class distribution to generate reliable\npseudo-labels for unlabeled data belonging to both known and unknown classes.\nOur extensive experimentation showcases the effectiveness of our approach on\nseveral benchmark datasets, where it substantially outperforms the existing\nstate-of-the-art on seven diverse datasets including CIFAR-100 (17.6%),\nImageNet-100 (5.7%), and Tiny ImageNet (9.9%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rizve_M/0/1/0/all/0/1\">Mamshad Nayeem Rizve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1\">Navid Kardan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Trajectory Prediction for Pedestrian Video Anomaly Detection. (arXiv:2207.02279v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02279","description":"<p>Video anomaly detection is a core problem in vision. Correctly detecting and\nidentifying anomalous behaviors in pedestrians from video data will enable\nsafety-critical applications such as surveillance, activity monitoring, and\nhuman-robot interaction. In this paper, we propose to leverage trajectory\nlocalization and prediction for unsupervised pedestrian anomaly event\ndetection. Different than previous reconstruction-based approaches, our\nproposed framework rely on the prediction errors of normal and abnormal\npedestrian trajectories to detect anomalies spatially and temporally. We\npresent experimental results on real-world benchmark datasets on varying\ntimescales and show that our proposed trajectory-predictor-based anomaly\ndetection pipeline is effective and efficient at identifying anomalous\nactivities of pedestrians in videos. Code will be made available at\nhttps://github.com/akanuasiegbu/Leveraging-Trajectory-Prediction-for-Pedestrian-Video-Anomaly-Detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanu_Asiegbu_A/0/1/0/all/0/1\">Asiegbu Miracle Kanu-Asiegbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_R/0/1/0/all/0/1\">Ram Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoxiao Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiPOCO: Bi-Directional Trajectory Prediction with Pose Constraints for Pedestrian Anomaly Detection. (arXiv:2207.02281v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02281","description":"<p>We present BiPOCO, a Bi-directional trajectory predictor with POse\nCOnstraints, for detecting anomalous activities of pedestrians in videos. In\ncontrast to prior work based on feature reconstruction, our work identifies\npedestrian anomalous events by forecasting their future trajectories and\ncomparing the predictions with their expectations. We introduce a set of novel\ncompositional pose-based losses with our predictor and leverage prediction\nerrors of each body joint for pedestrian anomaly detection. Experimental\nresults show that our BiPOCO approach can detect pedestrian anomalous\nactivities with a high detection rate (up to 87.0%) and incorporating pose\nconstraints helps distinguish normal and anomalous poses in prediction. This\nwork extends current literature of using prediction-based methods for anomaly\ndetection and can benefit safety-critical applications such as autonomous\ndriving and surveillance. Code is available at\nhttps://github.com/akanuasiegbu/BiPOCO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kanu_Asiegbu_A/0/1/0/all/0/1\">Asiegbu Miracle Kanu-Asiegbu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasudevan_R/0/1/0/all/0/1\">Ram Vasudevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xiaoxiao Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effectivity of super resolution convolutional neural network for the enhancement of land cover classification from medium resolution satellite images. (arXiv:2207.02301v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02301","description":"<p>In the modern world, satellite images play a key role in forest management\nand degradation monitoring. For a precise quantification of forest land cover\nchanges, the availability of spatially fine resolution data is a necessity.\nSince 1972, NASAs LANDSAT Satellites are providing terrestrial images covering\nevery corner of the earth, which have been proved to be a highly useful\nresource for terrestrial change analysis and have been used in numerous other\nsectors. However, freely accessible satellite images are, generally, of medium\nto low resolution which is a major hindrance to the precision of the analysis.\nHence, we performed a comprehensive study to prove our point that, enhancement\nof resolution by Super-Resolution Convolutional Neural Network (SRCNN) will\nlessen the chance of misclassification of pixels, even under the established\nrecognition methods. We tested the method on original LANDSAT-7 images of\ndifferent regions of Sundarbans and their upscaled versions which were produced\nby bilinear interpolation, bicubic interpolation, and SRCNN respectively and it\nwas discovered that SRCNN outperforms the others by a significant amount.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bose_P/0/1/0/all/0/1\">Pritom Bose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halder_D/0/1/0/all/0/1\">Debolina Halder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_O/0/1/0/all/0/1\">Oliur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pial_T/0/1/0/all/0/1\">Turash Haque Pial</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Ensemble Learning Approach to Lung CT Segmentation for COVID-19 Severity Assessment. (arXiv:2207.02322v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02322","description":"<p>We present a novel deep learning approach to categorical segmentation of lung\nCTs of COVID-19 patients. Specifically, we partition the scans into healthy\nlung tissues, non-lung regions, and two different, yet visually similar,\npathological lung tissues, namely, ground-glass opacity and consolidation. This\nis accomplished via a unique, end-to-end hierarchical network architecture and\nensemble learning, which contribute to the segmentation and provide a measure\nfor segmentation uncertainty. The proposed framework achieves competitive\nresults and outstanding generalization capabilities for three COVID-19\ndatasets. Our method is ranked second in a public Kaggle competition for\nCOVID-19 CT images segmentation. Moreover, segmentation uncertainty regions are\nshown to correspond to the disagreements between the manual annotations of two\ndifferent radiologists. Finally, preliminary promising correspondence results\nare shown for our private dataset when comparing the patients' COVID-19\nseverity scores (based on clinical measures), and the segmented lung\npathologies. Code and data are available at our repository:\nhttps://github.com/talbenha/covid-seg\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ben_Haim_T/0/1/0/all/0/1\">Tal Ben-Haim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sofer_R/0/1/0/all/0/1\">Ron Moshe Sofer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ben_Arie_G/0/1/0/all/0/1\">Gal Ben-Arie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shelef_I/0/1/0/all/0/1\">Ilan Shelef</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riklin_Raviv_T/0/1/0/all/0/1\">Tammy Riklin-Raviv</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers. (arXiv:2207.02327v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02327","description":"<p>Diffusion MRI tractography is an advanced imaging technique for quantitative\nmapping of the brain's structural connectivity. Whole brain tractography (WBT)\ndata contains over hundreds of thousands of individual fiber streamlines\n(estimated brain connections), and this data is usually parcellated to create\ncompact representations for data analysis applications such as disease\nclassification. In this paper, we propose a novel parcellation-free WBT\nanalysis framework, TractoFormer, that leverages tractography information at\nthe level of individual fiber streamlines and provides a natural mechanism for\ninterpretation of results using the attention mechanism of transformers.\nTractoFormer includes two main contributions. First, we propose a novel and\nsimple 2D image representation of WBT, TractoEmbedding, to encode 3D fiber\nspatial relationships and any feature of interest that can be computed from\nindividual fibers (such as FA or MD). Second, we design a network based on\nvision transformers (ViTs) that includes: 1) data augmentation to overcome\nmodel overfitting on small datasets, 2) identification of discriminative fibers\nfor interpretation of results, and 3) ensemble learning to leverage fiber\ninformation from different brain regions. In a synthetic data experiment,\nTractoFormer successfully identifies discriminative fibers with simulated group\ndifferences. In a disease classification experiment comparing several methods,\nTractoFormer achieves the highest accuracy in classifying schizophrenia vs\ncontrol. Discriminative fibers are identified in left hemispheric frontal and\nparietal superficial white matter regions, which have previously been shown to\nbe affected in schizophrenia patients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_T/0/1/0/all/0/1\">Tengfei Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Westin_C/0/1/0/all/0/1\">Carl-Fredrik Westin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Grounding for VQA in Vision-Language Transformers. (arXiv:2207.02334v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02334","description":"<p>Transformers for visual-language representation learning have been getting a\nlot of interest and shown tremendous performance on visual question answering\n(VQA) and grounding. But most systems that show good performance of those tasks\nstill rely on pre-trained object detectors during training, which limits their\napplicability to the object classes available for those detectors. To mitigate\nthis limitation, the following paper focuses on the problem of weakly\nsupervised grounding in context of visual question answering in transformers.\nThe approach leverages capsules by grouping each visual token in the visual\nencoder and uses activations from language self-attention layers as a\ntext-guided selection module to mask those capsules before they are forwarded\nto the next layer. We evaluate our approach on the challenging GQA as well as\nVQA-HAT dataset for VQA grounding. Our experiments show that: while removing\nthe information of masked objects from standard transformer architectures leads\nto a significant drop in performance, the integration of capsules significantly\nimproves the grounding ability of such systems and provides new\nstate-of-the-art results compared to other approaches in the field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Aisha Urooj Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1\">Chuang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_N/0/1/0/all/0/1\">Niels Da Vitoria Lobo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Label Retinal Disease Classification using Transformers. (arXiv:2207.02335v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02335","description":"<p>Early detection of retinal diseases is one of the most important means of\npreventing partial or permanent blindness in patients. In this research, a\nnovel multi-label classification system is proposed for the detection of\nmultiple retinal diseases, using fundus images collected from a variety of\nsources. First, a new multi-label retinal disease dataset, the MuReD dataset,\nis constructed, using a number of publicly available datasets for fundus\ndisease classification. Next, a sequence of post-processing steps is applied to\nensure the quality of the image data and the range of diseases, present in the\ndataset. For the first time in fundus multi-label disease classification, a\ntransformer-based model optimized through extensive experimentation is used for\nimage analysis and decision making. Numerous experiments are performed to\noptimize the configuration of the proposed system. It is shown that the\napproach performs better than state-of-the-art works on the same task by 7.9%\nand 8.1% in terms of AUC score for disease detection and disease\nclassification, respectively. The obtained results further support the\npotential applications of transformer-based architectures in the medical\nimaging field.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_M/0/1/0/all/0/1\">M. A. Rodriguez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlMarzouqi_H/0/1/0/all/0/1\">H. AlMarzouqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liatsis_P/0/1/0/all/0/1\">P. Liatsis</a> (Department of Electrical Engineering and Computer Science, Khalifa University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Federated and Transfer Learning: A Survey on Adversaries and Defense Mechanisms. (arXiv:2207.02337v1 [cs.LG])","link":"http://arxiv.org/abs/2207.02337","description":"<p>The advent of federated learning has facilitated large-scale data exchange\namongst machine learning models while maintaining privacy. Despite its brief\nhistory, federated learning is rapidly evolving to make wider use more\npractical. One of the most significant advancements in this domain is the\nincorporation of transfer learning into federated learning, which overcomes\nfundamental constraints of primary federated learning, particularly in terms of\nsecurity. This chapter performs a comprehensive survey on the intersection of\nfederated and transfer learning from a security point of view. The main goal of\nthis study is to uncover potential vulnerabilities and defense mechanisms that\nmight compromise the privacy and performance of systems that use federated and\ntransfer learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hallaji_E/0/1/0/all/0/1\">Ehsan Hallaji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_Far_R/0/1/0/all/0/1\">Roozbeh Razavi-Far</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saif_M/0/1/0/all/0/1\">Mehrdad Saif</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalization to translation shifts: a study in architectures and augmentations. (arXiv:2207.02349v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02349","description":"<p>We provide a detailed evaluation of various image classification\narchitectures (convolutional, vision transformer, and fully connected MLP\nnetworks) and data augmentation techniques towards generalization to large\nspacial translation shifts. We make the following observations: (a) In the\nabsence of data augmentation, all architectures, including convolutional\nnetworks suffer degradation in performance when evaluated on translated test\ndistributions. Understandably, both the in-distribution accuracy as well as\ndegradation to shifts is significantly worse for non-convolutional\narchitectures. (b) Across all architectures, even a minimal augmentation of $4$\npixel random crop improves the robustness of performance to much larger\nmagnitude shifts of up to $1/4$ of image size ($8$-$16$ pixels) in the test\ndata -- suggesting a form of meta generalization from augmentation. For\nnon-convolutional architectures, while the absolute accuracy is still low, we\nsee dramatic improvements in robustness to large translation shifts. (c) With\nsufficiently advanced augmentation ($4$ pixel\ncrop+RandAugmentation+Erasing+MixUp) pipeline all architectures can be trained\nto have competitive performance, both in terms of in-distribution accuracy as\nwell as generalization to large translation shifts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SNeRF: Stylized Neural Implicit Representations for 3D Scenes. (arXiv:2207.02363v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02363","description":"<p>This paper presents a stylized novel view synthesis method. Applying\nstate-of-the-art stylization methods to novel views frame by frame often causes\njittering artifacts due to the lack of cross-view consistency. Therefore, this\npaper investigates 3D scene stylization that provides a strong inductive bias\nfor consistent novel view synthesis. Specifically, we adopt the emerging neural\nradiance fields (NeRF) as our choice of 3D scene representation for their\ncapability to render high-quality novel views for a variety of scenes. However,\nas rendering a novel view from a NeRF requires a large number of samples,\ntraining a stylized NeRF requires a large amount of GPU memory that goes beyond\nan off-the-shelf GPU capacity. We introduce a new training method to address\nthis problem by alternating the NeRF and stylization optimization steps. Such a\nmethod enables us to make full use of our hardware memory capacity to both\ngenerate images at higher resolution and adopt more expressive image style\ntransfer methods. Our experiments show that our method produces stylized NeRFs\nfor a wide range of content, including indoor, outdoor and dynamic scenes, and\nsynthesizes high-quality novel views with cross-view consistency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Phuoc_T/0/1/0/all/0/1\">Thu Nguyen-Phuoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Lei Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Learning for Human Sensing Using Radio Signals. (arXiv:2207.02370v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02370","description":"<p>There is a growing literature demonstrating the feasibility of using Radio\nFrequency (RF) signals to enable key computer vision tasks in the presence of\nocclusions and poor lighting. It leverages that RF signals traverse walls and\nocclusions to deliver through-wall pose estimation, action recognition, scene\ncaptioning, and human re-identification. However, unlike RGB datasets which can\nbe labeled by human workers, labeling RF signals is a daunting task because\nsuch signals are not human interpretable. Yet, it is fairly easy to collect\nunlabelled RF signals. It would be highly beneficial to use such unlabeled RF\ndata to learn useful representations in an unsupervised manner. Thus, in this\npaper, we explore the feasibility of adapting RGB-based unsupervised\nrepresentation learning to RF signals. We show that while contrastive learning\nhas emerged as the main technique for unsupervised representation learning from\nimages and videos, such methods produce poor performance when applied to\nsensing humans using RF signals. In contrast, predictive unsupervised learning\nmethods learn high-quality representations that can be used for multiple\ndownstream RF-based sensing tasks. Our empirical results show that this\napproach outperforms state-of-the-art RF-based human sensing on various tasks,\nopening the possibility of unsupervised representation learning from this novel\nmodality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianhong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Lijie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yuan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1\">Dina Katabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Video Segmentation via Temporal Pseudo Supervision. (arXiv:2207.02372v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02372","description":"<p>Video semantic segmentation has achieved great progress under the supervision\nof large amounts of labelled training data. However, domain adaptive video\nsegmentation, which can mitigate data labelling constraints by adapting from a\nlabelled source domain toward an unlabelled target domain, is largely\nneglected. We design temporal pseudo supervision (TPS), a simple and effective\nmethod that explores the idea of consistency training for learning effective\nrepresentations from unlabelled target videos. Unlike traditional consistency\ntraining that builds consistency in spatial space, we explore consistency\ntraining in spatiotemporal space by enforcing model consistency across\naugmented video frames which helps learn from more diverse target data.\nSpecifically, we design cross-frame pseudo labelling to provide pseudo\nsupervision from previous video frames while learning from the augmented\ncurrent video frames. The cross-frame pseudo labelling encourages the network\nto produce high-certainty predictions, which facilitates consistency training\nwith cross-frame augmentation effectively. Extensive experiments over multiple\npublic datasets show that TPS is simpler to implement, much more stable to\ntrain, and achieves superior video segmentation accuracy as compared with the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1\">Yun Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DG-STFM: 3D Geometric Guided Student-Teacher Feature Matching. (arXiv:2207.02375v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02375","description":"<p>We tackle the essential task of finding dense visual correspondences between\na pair of images. This is a challenging problem due to various factors such as\npoor texture, repetitive patterns, illumination variation, and motion blur in\npractical scenarios. In contrast to methods that use dense correspondence\nground-truths as direct supervision for local feature matching training, we\ntrain 3DG-STFM: a multi-modal matching model (Teacher) to enforce the depth\nconsistency under 3D dense correspondence supervision and transfer the\nknowledge to 2D unimodal matching model (Student). Both teacher and student\nmodels consist of two transformer-based matching modules that obtain dense\ncorrespondences in a coarse-to-fine manner. The teacher model guides the\nstudent model to learn RGB-induced depth information for the matching purpose\non both coarse and fine branches. We also evaluate 3DG-STFM on a model\ncompression task. To the best of our knowledge, 3DG-STFM is the first\nstudent-teacher learning method for the local feature matching task. The\nexperiments show that our method outperforms state-of-the-art methods on indoor\nand outdoor camera pose estimations, and homography estimation problems. Code\nis available at: https://github.com/Ryan-prime/3DG-STFM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_R/0/1/0/all/0/1\">Runyu Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1\">Chen Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1\">Yatong An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fengqing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comprehensive Review on Deep Supervision: Theories and Applications. (arXiv:2207.02376v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02376","description":"<p>Deep supervision, or known as 'intermediate supervision' or 'auxiliary\nsupervision', is to add supervision at hidden layers of a neural network. This\ntechnique has been increasingly applied in deep neural network learning systems\nfor various computer vision applications recently. There is a consensus that\ndeep supervision helps improve neural network performance by alleviating the\ngradient vanishing problem, as one of the many strengths of deep supervision.\nBesides, in different computer vision applications, deep supervision can be\napplied in different ways. How to make the most use of deep supervision to\nimprove network performance in different applications has not been thoroughly\ninvestigated. In this paper, we provide a comprehensive in-depth review of deep\nsupervision in both theories and applications. We propose a new classification\nof different deep supervision networks, and discuss advantages and limitations\nof current deep supervision networks in computer vision applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Renjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wenli Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kaining Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiaotong Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_S/0/1/0/all/0/1\">Son N. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1\">Saurabh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alty_J/0/1/0/all/0/1\">Jane Alty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Q/0/1/0/all/0/1\">Quan Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising. (arXiv:2207.02377v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02377","description":"<p>The acquisition conditions for low-dose and high-dose CT images are usually\ndifferent, so that the shifts in the CT numbers often occur. Accordingly,\nunsupervised deep learning-based approaches, which learn the target image\ndistribution, often introduce CT number distortions and result in detrimental\neffects in diagnostic performance. To address this, here we propose a novel\nunsupervised learning approach for lowdose CT reconstruction using patch-wise\ndeep metric learning. The key idea is to learn embedding space by pulling the\npositive pairs of image patches which shares the same anatomical structure, and\npushing the negative pairs which have same noise level each other. Thereby, the\nnetwork is trained to suppress the noise level, while retaining the original\nglobal CT number distributions even after the image translation. Experimental\nresults confirm that our deep metric learning plays a critical role in\nproducing high quality denoised images without CT number shift.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jung_C/0/1/0/all/0/1\">Chanyong Jung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Joonhyung Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_S/0/1/0/all/0/1\">Sunkyoung You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1\">Jong Chul Ye</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI. (arXiv:2207.02390v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02390","description":"<p>Fast MRI aims to reconstruct a high fidelity image from partially observed\nmeasurements. Exuberant development in fast MRI using deep learning has been\nwitnessed recently. Meanwhile, novel deep learning paradigms, e.g., Transformer\nbased models, are fast-growing in natural language processing and promptly\ndeveloped for computer vision and medical image analysis due to their prominent\nperformance. Nevertheless, due to the complexity of the Transformer, the\napplication of fast MRI may not be straightforward. The main obstacle is the\ncomputational cost of the self-attention layer, which is the core part of the\nTransformer, can be expensive for high resolution MRI inputs. In this study, we\npropose a new Transformer architecture for solving fast MRI that coupled\nShifted Windows Transformer with U-Net to reduce the network complexity. We\nincorporate deformable attention to construe the explainability of our\nreconstruction model. We empirically demonstrate that our method achieves\nconsistently superior performance on the fast MRI task. Besides, compared to\nstate-of-the-art Transformer models, our method has fewer network parameters\nwhile revealing explainability. The code is publicly available at\nhttps://github.com/ayanglab/SDAUT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiahao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1\">Xiaodan Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1\">Zhifan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Efficient Adversarial Attack Based on Latin Hypercube Sampling. (arXiv:2207.02391v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02391","description":"<p>In order to be applicable in real-world scenario, Boundary Attacks (BAs) were\nproposed and ensured one hundred percent attack success rate with only decision\ninformation. However, existing BA methods craft adversarial examples by\nleveraging a simple random sampling (SRS) to estimate the gradient, consuming a\nlarge number of model queries. To overcome the drawback of SRS, this paper\nproposes a Latin Hypercube Sampling based Boundary Attack (LHS-BA) to save\nquery budget. Compared with SRS, LHS has better uniformity under the same\nlimited number of random samples. Therefore, the average on these random\nsamples is closer to the true gradient than that estimated by SRS. Various\nexperiments are conducted on benchmark datasets including MNIST, CIFAR, and\nImageNet-1K. Experimental results demonstrate the superiority of the proposed\nLHS-BA over the state-of-the-art BA methods in terms of query efficiency. The\nsource codes are publicly available at https://github.com/GZHU-DVL/LHS-BA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiayu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan-Gen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoSpeed: A Linked Autoencoder Approach for Pulse-Echo Speed-of-Sound Imaging for Medical Ultrasound. (arXiv:2207.02392v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02392","description":"<p>Quantitative ultrasound, e.g., speed-of-sound (SoS) in tissues, provides\ninformation about tissue properties that have diagnostic value. Recent studies\nshowed the possibility of extracting SoS information from pulse-echo ultrasound\nraw data (a.k.a. RF data) using deep neural networks that are fully trained on\nsimulated data. These methods take sensor domain data, i.e., RF data, as input\nand train a network in an end-to-end fashion to learn the implicit mapping\nbetween the RF data domain and SoS domain. However, such networks are prone to\noverfitting to simulated data which results in poor performance and instability\nwhen tested on measured data. We propose a novel method for SoS mapping\nemploying learned representations from two linked autoencoders. We test our\napproach on simulated and measured data acquired from human breast mimicking\nphantoms. We show that SoS mapping is possible using linked autoencoders. The\nproposed method has a Mean Absolute Percentage Error (MAPE) of 2.39% on the\nsimulated data. On the measured data, the predictions of the proposed method\nare close to the expected values with MAPE of 1.1%. Compared to an end-to-end\ntrained network, the proposed method shows higher stability and\nreproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jush_F/0/1/0/all/0/1\">Farnaz Khun Jush</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biele_M/0/1/0/all/0/1\">Markus Biele</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dueppenbecker_P/0/1/0/all/0/1\">Peter M. Dueppenbecker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Hybrid Endoscopic Dataset for Evaluating Machine Learning-based Photometric Image Enhancement Models. (arXiv:2207.02396v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02396","description":"<p>Endoscopy is the most widely used medical technique for cancer and polyp\ndetection inside hollow organs. However, images acquired by an endoscope are\nfrequently affected by illumination artefacts due to the enlightenment source\norientation. There exist two major issues when the endoscope's light source\npose suddenly changes: overexposed and underexposed tissue areas are produced.\nThese two scenarios can result in misdiagnosis due to the lack of information\nin the affected zones or hamper the performance of various computer vision\nmethods (e.g., SLAM, structure from motion, optical flow) used during the non\ninvasive examination. The aim of this work is two-fold: i) to introduce a new\nsynthetically generated data-set generated by a generative adversarial\ntechniques and ii) and to explore both shallow based and deep learning-based\nimage-enhancement methods in overexposed and underexposed lighting conditions.\nBest quantitative results (i.e., metric based results), were obtained by the\ndeep-learnnig-based LMSPEC method,besides a running time around 7.6 fps)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Garcia_Vega_A/0/1/0/all/0/1\">Axel Garcia-Vega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Espinosa_R/0/1/0/all/0/1\">Ricardo Espinosa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1\">Gilberto Ochoa-Ruiz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bazin_T/0/1/0/all/0/1\">Thomas Bazin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Falcon_Morales_L/0/1/0/all/0/1\">Luis Eduardo Falcon-Morales</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lamarque_D/0/1/0/all/0/1\">Dominique Lamarque</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Daul_C/0/1/0/all/0/1\">Christian Daul</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Transformation for Image Composition via Correspondence Learning. (arXiv:2207.02398v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02398","description":"<p>When using cut-and-paste to acquire a composite image, the geometry\ninconsistency between foreground and background may severely harm its fidelity.\nTo address the geometry inconsistency in composite images, several existing\nworks learned to warp the foreground object for geometric correction. However,\nthe absence of annotated dataset results in unsatisfactory performance and\nunreliable evaluation. In this work, we contribute a Spatial TRAnsformation for\nvirtual Try-on (STRAT) dataset covering three typical application scenarios.\nMoreover, previous works simply concatenate foreground and background as input\nwithout considering their mutual correspondence. Instead, we propose a novel\ncorrespondence learning network (CorrelNet) to model the correspondence between\nforeground and background using cross-attention maps, based on which we can\npredict the target coordinate that each source coordinate of foreground should\nbe mapped to on the background. Then, the warping parameters of foreground\nobject can be derived from pairs of source and target coordinates.\nAdditionally, we learn a filtering mask to eliminate noisy pairs of coordinates\nto estimate more accurate warping parameters. Extensive experiments on our\nSTRAT dataset demonstrate that our proposed CorrelNet performs more favorably\nagainst previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kaixin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Apparent Diffusion Coefficient Maps from Undersampled Radial k-Space Diffusion-Weighted MRI in Mice using a Deep CNN-Transformer Model in Conjunction with a Monoexponential Model. (arXiv:2207.02399v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02399","description":"<p>Purpose: To accelerate radially sampled diffusion weighted spin-echo\n(Rad-DW-SE) acquisition method for generating high quality of apparent\ndiffusion coefficient (ADC) maps. Methods: A deep learning method was developed\nto generate accurate ADC map reconstruction from undersampled DWI data acquired\nwith the Rad-DW-SE method. The deep learning method integrates convolutional\nneural networks (CNNs) with vison transformers to generate high quality ADC\nmaps from undersampled DWI data, regularized by a monoexponential ADC model\nfitting term. A model was trained on DWI data of 147 mice and evaluated on DWI\ndata of 36 mice, with undersampling rates of 4x and 8x. Results: Ablation\nstudies and experimental results have demonstrated that the proposed deep\nlearning model can generate high quality ADC maps from undersampled DWI data,\nbetter than alternative deep learning methods under comparison, with their\nperformance quantified on different levels of images, tumors, kidneys, and\nmuscles. Conclusions: The deep learning method with integrated CNNs and\ntransformers provides an effective means to accurately compute ADC maps from\nundersampled DWI data acquired with the Rad-DW-SE method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuemeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_H/0/1/0/all/0/1\">Hee Kwon Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joaquim_M/0/1/0/all/0/1\">Miguel Romanello Joaquim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pickup_S/0/1/0/all/0/1\">Stephen Pickup</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_R/0/1/0/all/0/1\">Rong Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1\">Yong Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chairs Can be Stood on: Overcoming Object Bias in Human-Object Interaction Detection. (arXiv:2207.02400v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02400","description":"<p>Detecting Human-Object Interaction (HOI) in images is an important step\ntowards high-level visual comprehension. Existing work often shed light on\nimproving either human and object detection, or interaction recognition.\nHowever, due to the limitation of datasets, these methods tend to fit well on\nfrequent interactions conditioned on the detected objects, yet largely ignoring\nthe rare ones, which is referred to as the object bias problem in this paper.\nIn this work, we for the first time, uncover the problem from two aspects:\nunbalanced interaction distribution and biased model learning. To overcome the\nobject bias problem, we propose a novel plug-and-play Object-wise Debiasing\nMemory (ODM) method for re-balancing the distribution of interactions under\ndetected objects. Equipped with carefully designed read and write strategies,\nthe proposed ODM allows rare interaction instances to be more frequently\nsampled for training, thereby alleviating the object bias induced by the\nunbalanced interaction distribution. We apply this method to three advanced\nbaselines and conduct experiments on the HICO-DET and HOI-COCO datasets. To\nquantitatively study the object bias problem, we advocate a new protocol for\nevaluating model performance. As demonstrated in the experimental results, our\nmethod brings consistent and significant improvements over baselines,\nespecially on rare interactions under each object. In addition, when evaluating\nunder the conventional standard setting, our method achieves new\nstate-of-the-art on the two benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guangzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1\">Yongkang Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1\">Mohan Kankanhalli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"White Matter Tracts are Point Clouds: Neuropsychological Score Prediction and Critical Region Localization via Geometric Deep Learning. (arXiv:2207.02402v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02402","description":"<p>White matter tract microstructure has been shown to influence\nneuropsychological scores of cognitive performance. However, prediction of\nthese scores from white matter tract data has not been attempted. In this\npaper, we propose a deep-learning-based framework for neuropsychological score\nprediction using microstructure measurements estimated from diffusion magnetic\nresonance imaging (dMRI) tractography, focusing on predicting performance on a\nreceptive vocabulary assessment task based on a critical fiber tract for\nlanguage, the arcuate fasciculus (AF). We directly utilize information from all\npoints in a fiber tract, without the need to average data along the fiber as is\ntraditionally required by diffusion MRI tractometry methods. Specifically, we\nrepresent the AF as a point cloud with microstructure measurements at each\npoint, enabling adoption of point-based neural networks. We improve prediction\nperformance with the proposed Paired-Siamese Loss that utilizes information\nabout differences between continuous neuropsychological scores. Finally, we\npropose a Critical Region Localization (CRL) algorithm to localize informative\nanatomical regions containing points with strong contributions to the\nprediction results. Our method is evaluated on data from 806 subjects from the\nHuman Connectome Project dataset. Results demonstrate superior\nneuropsychological score prediction performance compared to baseline methods.\nWe discover that critical regions in the AF are strikingly consistent across\nsubjects, with the highest number of strongly contributing points located in\nfrontal cortical regions (i.e., the rostral middle frontal, pars opercularis,\nand pars triangularis), which are strongly implicated as critical areas for\nlanguage processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuqian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Tengfei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zekelman_L/0/1/0/all/0/1\">Leo R. Zekelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jianzhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1\">Nikos Makris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1\">Yogesh Rathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golby_A/0/1/0/all/0/1\">Alexandra J. Golby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1\">Lauren J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Model for Partial Multi-Label Image Classification with Curriculum Based Disambiguation. (arXiv:2207.02410v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02410","description":"<p>In this paper, we study the partial multi-label (PML) image classification\nproblem, where each image is annotated with a candidate label set consists of\nmultiple relevant labels and other noisy labels. Existing PML methods typically\ndesign a disambiguation strategy to filter out noisy labels by utilizing prior\nknowledge with extra assumptions, which unfortunately is unavailable in many\nreal tasks. Furthermore, because the objective function for disambiguation is\nusually elaborately designed on the whole training set, it can be hardly\noptimized in a deep model with SGD on mini-batches. In this paper, for the\nfirst time we propose a deep model for PML to enhance the representation and\ndiscrimination ability. On one hand, we propose a novel curriculum based\ndisambiguation strategy to progressively identify ground-truth labels by\nincorporating the varied difficulties of different classes. On the other hand,\na consistency regularization is introduced for model retraining to balance\nfitting identified easy labels and exploiting potential relevant labels.\nExtensive experimental results on the commonly used benchmark datasets show the\nproposed method significantly outperforms the SOTA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Feng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Ming-Kun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng-Jun Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Constrained Inference Optimization on Structural Groups for Human Pose Estimation. (arXiv:2207.02425v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02425","description":"<p>We observe that human poses exhibit strong group-wise structural correlation\nand spatial coupling between keypoints due to the biological constraints of\ndifferent body parts. This group-wise structural correlation can be explored to\nimprove the accuracy and robustness of human pose estimation. In this work, we\ndevelop a self-constrained prediction-verification network to characterize and\nlearn the structural correlation between keypoints during training. During the\ninference stage, the feedback information from the verification network allows\nus to perform further optimization of pose prediction, which significantly\nimproves the performance of human pose estimation. Specifically, we partition\nthe keypoints into groups according to the biological structure of human body.\nWithin each group, the keypoints are further partitioned into two subsets,\nhigh-confidence base keypoints and low-confidence terminal keypoints. We\ndevelop a self-constrained prediction-verification network to perform forward\nand backward predictions between these keypoint subsets. One fundamental\nchallenge in pose estimation, as well as in generic prediction tasks, is that\nthere is no mechanism for us to verify if the obtained pose estimation or\nprediction results are accurate or not, since the ground truth is not\navailable. Once successfully learned, the verification network serves as an\naccuracy verification module for the forward pose prediction. During the\ninference stage, it can be used to guide the local optimization of the pose\nestimation results of low-confidence keypoints with the self-constrained loss\non high-confidence keypoints as the objective function. Our extensive\nexperimental results on benchmark MS COCO and CrowdPose datasets demonstrate\nthat the proposed method can significantly improve the pose estimation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kan_Z/0/1/0/all/0/1\">Zhehan Kan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuoshuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhihai He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCT-Net: Domain-Calibrated Translation for Portrait Stylization. (arXiv:2207.02426v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02426","description":"<p>This paper introduces DCT-Net, a novel image translation architecture for\nfew-shot portrait stylization. Given limited style exemplars ($\\sim$100), the\nnew architecture can produce high-quality style transfer results with advanced\nability to synthesize high-fidelity contents and strong generality to handle\ncomplicated scenes (e.g., occlusions and accessories). Moreover, it enables\nfull-body image translation via one elegant evaluation network trained by\npartial observations (i.e., stylized heads). Few-shot learning based style\ntransfer is challenging since the learned model can easily become overfitted in\nthe target domain, due to the biased distribution formed by only a few training\nexamples. This paper aims to handle the challenge by adopting the key idea of\n\"calibration first, translation later\" and exploring the augmented global\nstructure with locally-focused translation. Specifically, the proposed DCT-Net\nconsists of three modules: a content adapter borrowing the powerful prior from\nsource photos to calibrate the content distribution of target samples; a\ngeometry expansion module using affine transformations to release spatially\nsemantic constraints; and a texture translation module leveraging samples\nproduced by the calibrated distribution to learn a fine-grained conversion.\nExperimental results demonstrate the proposed method's superiority over the\nstate of the art in head stylization and its effectiveness on full image\ntranslation with adaptive deformations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Men_Y/0/1/0/all/0/1\">Yifang Men</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1\">Zhouhui Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAMa: Cross-view Video Geo-localization. (arXiv:2207.02431v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02431","description":"<p>The existing work in cross-view geo-localization is based on images where a\nground panorama is matched to an aerial image. In this work, we focus on ground\nvideos instead of images which provides additional contextual cues which are\nimportant for this task. There are no existing datasets for this problem,\ntherefore we propose GAMa dataset, a large-scale dataset with ground videos and\ncorresponding aerial images. We also propose a novel approach to solve this\nproblem. At clip-level, a short video clip is matched with corresponding aerial\nimage and is later used to get video-level geo-localization of a long video.\nMoreover, we propose a hierarchical approach to further improve the clip-level\ngeolocalization. It is a challenging dataset, unaligned and limited field of\nview, and our proposed method achieves a Top-1 recall rate of 19.4% and 45.1%\n@1.0mile. Code and dataset are available at following link:\nhttps://github.com/svyas23/GAMa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Mubarak Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Complementary Bi-directional Feature Compression for Indoor 360{\\deg} Semantic Segmentation with Self-distillation. (arXiv:2207.02437v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02437","description":"<p>Recently, horizontal representation-based panoramic semantic segmentation\napproaches outperform projection-based solutions, because the distortions can\nbe effectively removed by compressing the spherical data in the vertical\ndirection. However, these methods ignore the distortion distribution prior and\nare limited to unbalanced receptive fields, e.g., the receptive fields are\nsufficient in the vertical direction and insufficient in the horizontal\ndirection. Differently, a vertical representation compressed in another\ndirection can offer implicit distortion prior and enlarge horizontal receptive\nfields. In this paper, we combine the two different representations and propose\na novel 360{\\deg} semantic segmentation solution from a complementary\nperspective. Our network comprises three modules: a feature extraction module,\na bi-directional compression module, and an ensemble decoding module. First, we\nextract multi-scale features from a panorama. Then, a bi-directional\ncompression module is designed to compress features into two complementary\nlow-dimensional representations, which provide content perception and\ndistortion prior. Furthermore, to facilitate the fusion of bi-directional\nfeatures, we design a unique self distillation strategy in the ensemble\ndecoding module to enhance the interaction of different features and further\nimprove the performance. Experimental results show that our approach\noutperforms the state-of-the-art solutions with at least 10\\% improvement on\nquantitative evaluations while displaying the best performance on visual\nappearance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zishuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhijie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GLENet: Boosting 3D Object Detectors with Generative Label Uncertainty Estimation. (arXiv:2207.02466v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02466","description":"<p>The inherent ambiguity in ground-truth annotations of 3D bounding boxes\ncaused by occlusions, signal missing, or manual annotation errors can confuse\ndeep 3D object detectors during training, thus deteriorating the detection\naccuracy. However, existing methods overlook such issues to some extent and\ntreat the labels as deterministic. In this paper, we propose GLENet, a\ngenerative label uncertainty estimation framework adapted from conditional\nvariational autoencoders, to model the one-to-many relationship between a\ntypical 3D object and its potential ground-truth bounding boxes with latent\nvariables. The label uncertainty generated by GLENet is a plug-and-play module\nand can be conveniently integrated into existing deep 3D detectors to build\nprobabilistic detectors and supervise the learning of the localization\nuncertainty. Besides, we propose an uncertainty-aware quality estimator\narchitecture in probabilistic detectors to guide the training of IoU-branch\nwith predicted localization uncertainty. We incorporate the proposed methods\ninto various popular base 3D detectors and observe that their performance is\nsignificantly boosted to the current state-of-the-art over the Waymo Open\ndataset and KITTI dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qijian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhiyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yixuan Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-area Target Individual Detection with Free Drawing on Video. (arXiv:2207.02467v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02467","description":"<p>This paper has provided a novel design idea and some implementation methods\nto make a real time detection of multi-areas with multiple detecting areas that\nare generated by the real time drawing on the screen display of the video. The\ndrawing on the video will remain the output as polylines, and the colors of the\noutlines will change when the stage of drawing or detecting is changed. The\nshape of the drawn area is free to be customized and real-time effective. The\nconfiguration of the drawn areas can be renewed and the detecting areas are\nworking individually. The detection result should be shown with a GUI designed\nby Tkinter. The object recognition model was developed on YOLOv5 but can be\nchanged to others, which means the core design and implementation idea of this\npaper is model-independent. With PIL and OpenCV and Tkinter, the drawing effect\nis real time and efficient. The design and code of this research is basic and\ncan be extended to be implemented in numerous monitoring and detecting\nsituations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jinwei Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Contrast MRI Segmentation Trained on Synthetic Images. (arXiv:2207.02469v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02469","description":"<p>In our comprehensive experiments and evaluations, we show that it is possible\nto generate multiple contrast (even all synthetically) and use synthetically\ngenerated images to train an image segmentation engine. We showed promising\nsegmentation results tested on real multi-contrast MRI scans when delineating\nmuscle, fat, bone and bone marrow, all trained on synthetic images. Based on\nsynthetic image training, our segmentation results were as high as 93.91\\%,\n94.11\\%, 91.63\\%, 95.33\\%, for muscle, fat, bone, and bone marrow delineation,\nrespectively. Results were not significantly different from the ones obtained\nwhen real images were used for segmentation training: 94.68\\%, 94.67\\%,\n95.91\\%, and 96.82\\%, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Irmakci_I/0/1/0/all/0/1\">Ismail Irmakci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unel_Z/0/1/0/all/0/1\">Zeki Emre Unel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ikizler_Cinbis_N/0/1/0/all/0/1\">Nazli Ikizler-Cinbis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1\">Ulas Bagci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Two-stage Decision Improves Open-Set Panoptic Segmentation. (arXiv:2207.02504v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02504","description":"<p>Open-set panoptic segmentation (OPS) problem is a new research direction\naiming to perform segmentation for both \\known classes and \\unknown classes,\ni.e., the objects (\"things\") that are never annotated in the training set. The\nmain challenges of OPS are twofold: (1) the infinite possibility of the\n\\unknown object appearances makes it difficult to model them from a limited\nnumber of training data. (2) at training time, we are only provided with the\n\"void\" category, which essentially mixes the \"unknown thing\" and \"background\"\nclasses. We empirically find that directly using \"void\" category to supervise\n\\known class or \"background\" without screening will not lead to a satisfied OPS\nresult. In this paper, we propose a divide-and-conquer scheme to develop a\ntwo-stage decision process for OPS. We show that by properly combining a \\known\nclass discriminator with an additional class-agnostic object prediction head,\nthe OPS performance can be significantly improved. Specifically, we first\npropose to create a classifier with only \\known categories and let the \"void\"\nclass proposals achieve low prediction probability from those categories. Then\nwe distinguish the \"unknown things\" from the background by using the additional\nobject prediction head. To further boost performance, we introduce \"unknown\nthings\" pseudo-labels generated from up-to-date models and a heuristic rule to\nenrich the training set. Our extensive experimental evaluation shows that our\napproach significantly improves \\unknown class panoptic quality, with more than\n30\\% relative improvements than the existing best-performed method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hai-Ming Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yufei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Mitigating Flaws of Deep Perceptual Similarity Metrics. (arXiv:2207.02512v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02512","description":"<p>Measuring the similarity of images is a fundamental problem to computer\nvision for which no universal solution exists. While simple metrics such as the\npixel-wise L2-norm have been shown to have significant flaws, they remain\npopular. One group of recent state-of-the-art metrics that mitigates some of\nthose flaws are Deep Perceptual Similarity (DPS) metrics, where the similarity\nis evaluated as the distance in the deep features of neural networks. However,\nDPS metrics themselves have been less thoroughly examined for their benefits\nand, especially, their flaws. This work investigates the most common DPS\nmetric, where deep features are compared by spatial position, along with\nmetrics comparing the averaged and sorted deep features. The metrics are\nanalyzed in-depth to understand the strengths and weaknesses of the metrics by\nusing images designed specifically to challenge them. This work contributes\nwith new insights into the flaws of DPS, and further suggests improvements to\nthe metrics. An implementation of this work is available online:\nhttps://github.com/guspih/deep_perceptual_similarity_analysis/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sjogren_O/0/1/0/all/0/1\">Oskar Sj&#xf6;gren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pihlgren_G/0/1/0/all/0/1\">Gustav Grund Pihlgren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandin_F/0/1/0/all/0/1\">Fredrik Sandin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1\">Marcus Liwicki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lightweight Encoder-Decoder Architecture for Foot Ulcer Segmentation. (arXiv:2207.02515v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02515","description":"<p>Continuous monitoring of foot ulcer healing is needed to ensure the efficacy\nof a given treatment and to avoid any possibility of deterioration. Foot ulcer\nsegmentation is an essential step in wound diagnosis. We developed a model that\nis similar in spirit to the well-established encoder-decoder and residual\nconvolution neural networks. Our model includes a residual connection along\nwith a channel and spatial attention integrated within each convolution block.\nA simple patch-based approach for model training, test time augmentations, and\nmajority voting on the obtained predictions resulted in superior performance.\nOur model did not leverage any readily available backbone architecture,\npre-training on a similar external dataset, or any of the transfer learning\ntechniques. The total number of network parameters being around 5 million made\nit a significantly lightweight model as compared with the available\nstate-of-the-art models used for the foot ulcer segmentation task. Our\nexperiments presented results at the patch-level and image-level. Applied on\npublicly available Foot Ulcer Segmentation (FUSeg) Challenge dataset from\nMICCAI 2021, our model achieved state-of-the-art image-level performance of\n88.22% in terms of Dice similarity score and ranked second in the official\nchallenge leaderboard. We also showed an extremely simple solution that could\nbe compared against the more advanced architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Shahzad Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mahmood_A/0/1/0/all/0/1\">Arif Mahmood</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1\">Soon Ki Jung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Perspective Decoupled Heatmaps for 3D Robot Pose Estimation from Depth Maps. (arXiv:2207.02519v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02519","description":"<p>Knowing the exact 3D location of workers and robots in a collaborative\nenvironment enables several real applications, such as the detection of unsafe\nsituations or the study of mutual interactions for statistical and social\npurposes. In this paper, we propose a non-invasive and light-invariant\nframework based on depth devices and deep neural networks to estimate the 3D\npose of robots from an external camera. The method can be applied to any robot\nwithout requiring hardware access to the internal states. We introduce a novel\nrepresentation of the predicted pose, namely Semi-Perspective Decoupled\nHeatmaps (SPDH), to accurately compute 3D joint locations in world coordinates\nadapting efficient deep networks designed for the 2D Human Pose Estimation. The\nproposed approach, which takes as input a depth representation based on XYZ\ncoordinates, can be trained on synthetic depth data and applied to real-world\nsettings without the need for domain adaptation techniques. To this end, we\npresent the SimBa dataset, based on both synthetic and real depth images, and\nuse it for the experimental evaluation. Results show that the proposed\napproach, made of a specific depth map representation and the SPDH, overcomes\nthe current state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simoni_A/0/1/0/all/0/1\">Alessandro Simoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pini_S/0/1/0/all/0/1\">Stefano Pini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borghi_G/0/1/0/all/0/1\">Guido Borghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vezzani_R/0/1/0/all/0/1\">Roberto Vezzani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation through Shape Modeling for Medical Image Segmentation. (arXiv:2207.02529v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02529","description":"<p>Shape information is a strong and valuable prior in segmenting organs in\nmedical images. However, most current deep learning based segmentation\nalgorithms have not taken shape information into consideration, which can lead\nto bias towards texture. We aim at modeling shape explicitly and using it to\nhelp medical image segmentation. Previous methods proposed Variational\nAutoencoder (VAE) based models to learn the distribution of shape for a\nparticular organ and used it to automatically evaluate the quality of a\nsegmentation prediction by fitting it into the learned shape distribution.\nBased on which we aim at incorporating VAE into current segmentation pipelines.\nSpecifically, we propose a new unsupervised domain adaptation pipeline based on\na pseudo loss and a VAE reconstruction loss under a teacher-student learning\nparadigm. Both losses are optimized simultaneously and, in return, boost the\nsegmentation task performance. Extensive experiments on three public Pancreas\nsegmentation datasets as well as two in-house Pancreas segmentation datasets\nshow consistent improvements with at least 2.8 points gain in the Dice score,\ndemonstrating the effectiveness of our method in challenging unsupervised\ndomain adaptation scenarios for medical image segmentation. We hope this work\nwill advance shape analysis and geometric learning in medical imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengze Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yongyi Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Regularized Multi-Scale Feature Flow for High Dynamic Range Imaging. (arXiv:2207.02539v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02539","description":"<p>Reconstructing ghosting-free high dynamic range (HDR) images of dynamic\nscenes from a set of multi-exposure images is a challenging task, especially\nwith large object motion and occlusions, leading to visible artifacts using\nexisting methods. To address this problem, we propose a deep network that tries\nto learn multi-scale feature flow guided by the regularized loss. It first\nextracts multi-scale features and then aligns features from non-reference\nimages. After alignment, we use residual channel attention blocks to merge the\nfeatures from different images. Extensive qualitative and quantitative\ncomparisons show that our approach achieves state-of-the-art performance and\nproduces excellent results where color artifacts and geometric distortions are\nsignificantly reduced.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1\">Qian Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1\">Masanori Suganuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1\">Takayuki Okatani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Teacher: Dense Pseudo-Labels for Semi-supervised Object Detection. (arXiv:2207.02541v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02541","description":"<p>To date, the most powerful semi-supervised object detectors (SS-OD) are based\non pseudo-boxes, which need a sequence of post-processing with fine-tuned\nhyper-parameters. In this work, we propose replacing the sparse pseudo-boxes\nwith the dense prediction as a united and straightforward form of pseudo-label.\nCompared to the pseudo-boxes, our Dense Pseudo-Label (DPL) does not involve any\npost-processing method, thus retaining richer information. We also introduce a\nregion selection technique to highlight the key information while suppressing\nthe noise carried by dense labels. We name our proposed SS-OD algorithm that\nleverages the DPL as Dense Teacher. On COCO and VOC, Dense Teacher shows\nsuperior performance under various settings compared with the pseudo-box-based\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zheng Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Songtao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1\">Weixin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zeming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Haiyan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound. (arXiv:2207.02549v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02549","description":"<p>Accurate and consistent predictions of echocardiography parameters are\nimportant for cardiovascular diagnosis and treatment. In particular,\nsegmentations of the left ventricle can be used to derive ventricular volume,\nejection fraction (EF) and other relevant measurements. In this paper we\npropose a new automated method called EchoGraphs for predicting ejection\nfraction and segmenting the left ventricle by detecting anatomical keypoints.\nModels for direct coordinate regression based on Graph Convolutional Networks\n(GCNs) are used to detect the keypoints. GCNs can learn to represent the\ncardiac shape based on local appearance of each keypoint, as well as global\nspatial and temporal structures of all keypoints combined. We evaluate our\nEchoGraphs model on the EchoNet benchmark dataset. Compared to semantic\nsegmentation, GCNs show accurate segmentation and improvements in robustness\nand inference runtime. EF is computed simultaneously to segmentations and our\nmethod also obtains state-of-the-art ejection fraction estimation. Source code\nis available online: https://github.com/guybenyosef/EchoGraphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Sarina Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1\">Andrew Gilbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ben_Yosef_G/0/1/0/all/0/1\">Guy Ben-Yosef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is the U-Net Directional-Relationship Aware?. (arXiv:2207.02574v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02574","description":"<p>CNNs are often assumed to be capable of using contextual information about\ndistinct objects (such as their directional relations) inside their receptive\nfield. However, the nature and limits of this capacity has never been explored\nin full. We explore a specific type of relationship~-- directional~-- using a\nstandard U-Net trained to optimize a cross-entropy loss function for\nsegmentation. We train this network on a pretext segmentation task requiring\ndirectional relation reasoning for success and state that, with enough data and\na sufficiently large receptive field, it succeeds to learn the proposed task.\nWe further explore what the network has learned by analysing scenarios where\nthe directional relationships are perturbed, and show that the network has\nlearned to reason using these relationships.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riva_M/0/1/0/all/0/1\">Mateus Riva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1\">Pietro Gori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yger_F/0/1/0/all/0/1\">Florian Yger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloch_I/0/1/0/all/0/1\">Isabelle Bloch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PIC 4th Challenge: Semantic-Assisted Multi-Feature Encoding and Multi-Head Decoding for Dense Video Captioning. (arXiv:2207.02583v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02583","description":"<p>The task of Dense Video Captioning (DVC) aims to generate captions with\ntimestamps for multiple events in one video. Semantic information plays an\nimportant role for both localization and description of DVC. We present a\nsemantic-assisted dense video captioning model based on the encoding-decoding\nframework. In the encoding stage, we design a concept detector to extract\nsemantic information, which is then fused with multi-modal visual features to\nsufficiently represent the input video. In the decoding stage, we design a\nclassification head, paralleled with the localization and captioning heads, to\nprovide semantic supervision. Our method achieves significant improvements on\nthe YouMakeup dataset under DVC evaluation metrics and achieves high\nperformance in the Makeup Dense Video Captioning (MDVC) task of PIC 4th\nChallenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yifan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chunfeng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Weiming Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FAST-VQA: Efficient End-to-end Video Quality Assessment with Fragment Sampling. (arXiv:2207.02595v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02595","description":"<p>Current deep video quality assessment (VQA) methods are usually with high\ncomputational costs when evaluating high-resolution videos. This cost hinders\nthem from learning better video-quality-related representations via end-to-end\ntraining. Existing approaches typically consider naive sampling to reduce the\ncomputational cost, such as resizing and cropping. However, they obviously\ncorrupt quality-related information in videos and are thus not optimal for\nlearning good representations for VQA. Therefore, there is an eager need to\ndesign a new quality-retained sampling scheme for VQA. In this paper, we\npropose Grid Mini-patch Sampling (GMS), which allows consideration of local\nquality by sampling patches at their raw resolution and covers global quality\nwith contextual relations via mini-patches sampled in uniform grids. These\nmini-patches are spliced and aligned temporally, named as fragments. We further\nbuild the Fragment Attention Network (FANet) specially designed to accommodate\nfragments as inputs. Consisting of fragments and FANet, the proposed FrAgment\nSample Transformer for VQA (FAST-VQA) enables efficient end-to-end deep VQA and\nlearns effective video-quality-related representations. It improves\nstate-of-the-art accuracy by around 10% while reducing 99.5% FLOPs on 1080P\nhigh-resolution videos. The newly learned video-quality-related representations\ncan also be transferred into smaller VQA datasets, boosting performance in\nthese scenarios. Extensive experiments show that FAST-VQA has good performance\non inputs of various resolutions while retaining high efficiency. We publish\nour code at https://github.com/timothyhtimothy/FAST-VQA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haoning Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chaofeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Jingwen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Liang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Annan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wenxiu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weisi Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting is not Understanding: Recognizing and Addressing Underspecification in Machine Learning. (arXiv:2207.02598v1 [cs.LG])","link":"http://arxiv.org/abs/2207.02598","description":"<p>Machine learning (ML) models are typically optimized for their accuracy on a\ngiven dataset. However, this predictive criterion rarely captures all desirable\nproperties of a model, in particular how well it matches a domain expert's\nunderstanding of a task. Underspecification refers to the existence of multiple\nmodels that are indistinguishable in their in-domain accuracy, even though they\ndiffer in other desirable properties such as out-of-distribution (OOD)\nperformance. Identifying these situations is critical for assessing the\nreliability of ML models.\n</p>\n<p>We formalize the concept of underspecification and propose a method to\nidentify and partially address it. We train multiple models with an\nindependence constraint that forces them to implement different functions. They\ndiscover predictive features that are otherwise ignored by standard empirical\nrisk minimization (ERM), which we then distill into a global model with\nsuperior OOD performance. Importantly, we constrain the models to align with\nthe data manifold to ensure that they discover meaningful features. We\ndemonstrate the method on multiple datasets in computer vision (collages,\nWILDS-Camelyon17, GQA) and discuss general implications of underspecification.\nMost notably, in-domain performance cannot serve for OOD model selection\nwithout additional assumptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Teney_D/0/1/0/all/0/1\">Damien Teney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GFNet: Geometric Flow Network for 3D Point Cloud Semantic Segmentation. (arXiv:2207.02605v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02605","description":"<p>Point cloud semantic segmentation from projected views, such as range-view\n(RV) and bird's-eye-view (BEV), has been intensively investigated. Different\nviews capture different information of point clouds and thus are complementary\nto each other. However, recent projection-based methods for point cloud\nsemantic segmentation usually utilize a vanilla late fusion strategy for the\npredictions of different views, failing to explore the complementary\ninformation from a geometric perspective during the representation learning. In\nthis paper, we introduce a geometric flow network (GFNet) to explore the\ngeometric correspondence between different views in an align-before-fuse\nmanner. Specifically, we devise a novel geometric flow module (GFM) to\nbidirectionally align and propagate the complementary information across\ndifferent views according to geometric relationships under the end-to-end\nlearning scheme. We perform extensive experiments on two widely used benchmark\ndatasets, SemanticKITTI and nuScenes, to demonstrate the effectiveness of our\nGFNet for project-based point cloud semantic segmentation. Concretely, GFNet\nnot only significantly boosts the performance of each individual view but also\nachieves state-of-the-art results over all existing projection-based models.\nCode is available at \\url{https://github.com/haibo-qiu/GFNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Haibo Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DenseHybrid: Hybrid Anomaly Detection for Dense Open-set Recognition. (arXiv:2207.02606v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02606","description":"<p>Anomaly detection can be conceived either through generative modelling of\nregular training data or by discriminating with respect to negative training\ndata. These two approaches exhibit different failure modes. Consequently,\nhybrid algorithms present an attractive research goal. Unfortunately, dense\nanomaly detection requires translational equivariance and very large input\nresolutions. These requirements disqualify all previous hybrid approaches to\nthe best of our knowledge. We therefore design a novel hybrid algorithm based\non reinterpreting discriminative logits as a logarithm of the unnormalized\njoint distribution $\\hat{p}(\\mathbf{x}, \\mathbf{y})$. Our model builds on a\nshared convolutional representation from which we recover three dense\npredictions: i) the closed-set class posterior $P(\\mathbf{y}|\\mathbf{x})$, ii)\nthe dataset posterior $P(d_{in}|\\mathbf{x})$, iii) unnormalized data likelihood\n$\\hat{p}(\\mathbf{x})$. The latter two predictions are trained both on the\nstandard training data and on a generic negative dataset. We blend these two\npredictions into a hybrid anomaly score which allows dense open-set recognition\non large natural images. We carefully design a custom loss for the data\nlikelihood in order to avoid backpropagation through the untractable\nnormalizing constant $Z(\\theta)$. Experiments evaluate our contributions on\nstandard dense anomaly detection benchmarks as well as in terms of open-mIoU -\na novel metric for dense open-set performance. Our submissions achieve\nstate-of-the-art performance despite neglectable computational overhead over\nthe standard semantic segmentation baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1\">Matej Grci&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bevandic_P/0/1/0/all/0/1\">Petra Bevandi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VMRF: View Matching Neural Radiance Fields. (arXiv:2207.02621v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02621","description":"<p>Neural Radiance Fields (NeRF) have demonstrated very impressive performance\nin novel view synthesis via implicitly modelling 3D representations from\nmulti-view 2D images. However, most existing studies train NeRF models with\neither reasonable camera pose initialization or manually-crafted camera pose\ndistributions which are often unavailable or hard to acquire in various\nreal-world data. We design VMRF, an innovative view matching NeRF that enables\neffective NeRF training without requiring prior knowledge in camera poses or\ncamera pose distributions. VMRF introduces a view matching scheme, which\nexploits unbalanced optimal transport to produce a feature transport plan for\nmapping a rendered image with randomly initialized camera pose to the\ncorresponding real image. With the feature transport plan as the guidance, a\nnovel pose calibration technique is designed which rectifies the initially\nrandomized camera poses by predicting relative pose transformations between the\npair of rendered and real images. Extensive experiments over a number of\nsynthetic and real datasets show that the proposed VMRF outperforms the\nstate-of-the-art qualitatively and quantitatively by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bai Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowing Earlier what Right Means to You: A Comprehensive VQA Dataset for Grounding Relative Directions via Multi-Task Learning. (arXiv:2207.02624v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02624","description":"<p>Spatial reasoning poses a particular challenge for intelligent agents and is\nat the same time a prerequisite for their successful interaction and\ncommunication in the physical world. One such reasoning task is to describe the\nposition of a target object with respect to the intrinsic orientation of some\nreference object via relative directions. In this paper, we introduce\nGRiD-A-3D, a novel diagnostic visual question-answering (VQA) dataset based on\nabstract objects. Our dataset allows for a fine-grained analysis of end-to-end\nVQA models' capabilities to ground relative directions. At the same time, model\ntraining requires considerably fewer computational resources compared with\nexisting datasets, yet yields a comparable or even higher performance. Along\nwith the new dataset, we provide a thorough evaluation based on two widely\nknown end-to-end VQA architectures trained on GRiD-A-3D. We demonstrate that\nwithin a few epochs, the subtasks required to reason over relative directions,\nsuch as recognizing and locating objects in a scene and estimating their\nintrinsic orientations, are learned in the order in which relative directions\nare intuitively processed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahrens_K/0/1/0/all/0/1\">Kyra Ahrens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1\">Matthias Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae Hee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1\">Cornelius Weber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Difference in Euclidean Norm Can Cause Semantic Divergence in Batch Normalization. (arXiv:2207.02625v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02625","description":"<p>In this paper, we show that the difference in Euclidean norm of samples can\nmake a contribution to the semantic divergence and even confusion, after the\nspatial translation and scaling transformation in batch normalization. To\naddress this issue, we propose an intuitive but effective method to equalize\nthe Euclidean norms of sample vectors. Concretely, we $l_2$-normalize each\nsample vector before batch normalization, and therefore the sample vectors are\nof the same magnitude. Since the proposed method combines the $l_2$\nnormalization and batch normalization, we name our method as $L_2$BN. The\n$L_2$BN can strengthen the compactness of intra-class features and enlarge the\ndiscrepancy of inter-class features. In addition, it can help the gradient\nconverge to a stable scale. The $L_2$BN is easy to implement and can exert its\neffect without any additional parameters and hyper-parameters. Therefore, it\ncan be used as a basic normalization method for neural networks. We evaluate\nthe effectiveness of $L_2$BN through extensive experiments with various models\non image classification and acoustic scene classification tasks. The\nexperimental results demonstrate that the $L_2$BN is able to boost the\ngeneralization ability of various neural network models and achieve\nconsiderable performance improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhennan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kehan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Runyi Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_P/0/1/0/all/0/1\">Pengchong Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1\">Guoli Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Context Sensing Attention Network for Video-based Person Re-identification. (arXiv:2207.02631v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02631","description":"<p>Video-based person re-identification (ReID) is challenging due to the\npresence of various interferences in video frames. Recent approaches handle\nthis problem using temporal aggregation strategies. In this work, we propose a\nnovel Context Sensing Attention Network (CSA-Net), which improves both the\nframe feature extraction and temporal aggregation steps. First, we introduce\nthe Context Sensing Channel Attention (CSCA) module, which emphasizes responses\nfrom informative channels for each frame. These informative channels are\nidentified with reference not only to each individual frame, but also to the\ncontent of the entire sequence. Therefore, CSCA explores both the individuality\nof each frame and the global context of the sequence. Second, we propose the\nContrastive Feature Aggregation (CFA) module, which predicts frame weights for\ntemporal aggregation. Here, the weight for each frame is determined in a\ncontrastive manner: i.e., not only by the quality of each individual frame, but\nalso by the average quality of the other frames in a sequence. Therefore, it\neffectively promotes the contribution of relatively good frames. Extensive\nexperimental results on four datasets show that CSA-Net consistently achieves\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jianxin Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiangmin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network Pruning via Feature Shift Minimization. (arXiv:2207.02632v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02632","description":"<p>Channel pruning is widely used to reduce the complexity of deep network\nmodels. Recent pruning methods usually identify which parts of the network to\ndiscard by proposing a channel importance criterion. However, recent studies\nhave shown that these criteria do not work well in all conditions. In this\npaper, we propose a novel Feature Shift Minimization (FSM) method to compress\nCNN models, which evaluates the feature shift by converging the information of\nboth features and filters. Specifically, we first investigate the compression\nefficiency with some prevalent methods in different layer-depths and then\npropose the feature shift concept. Then, we introduce an approximation method\nto estimate the magnitude of the feature shift, since it is difficult to\ncompute it directly. Besides, we present a distribution-optimization algorithm\nto compensate for the accuracy loss and improve the network compression\nefficiency. The proposed method yields state-of-the-art performance on various\nbenchmark networks and datasets, verified by extensive experiments. The codes\ncan be available at \\url{https://github.com/lscgx/FSM}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuanzhi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaofang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Peng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1\">Shukai Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness of Visual Dialog. (arXiv:2207.02639v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02639","description":"<p>Adversarial robustness evaluates the worst-case performance scenario of a\nmachine learning model to ensure its safety and reliability. This study is the\nfirst to investigate the robustness of visually grounded dialog models towards\ntextual attacks. These attacks represent a worst-case scenario where the input\nquestion contains a synonym which causes the previously correct model to return\na wrong answer. Using this scenario, we first aim to understand how multimodal\ninput components contribute to model robustness. Our results show that models\nwhich encode dialog history are more robust, and when launching an attack on\nhistory, model prediction becomes more uncertain. This is in contrast to prior\nwork which finds that dialog history is negligible for model performance on\nthis task. We also evaluate how to generate adversarial test examples which\nsuccessfully fool the model but remain undetected by the user/software\ndesigner. We find that the textual, as well as the visual context are important\nto generate plausible worst-case scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gaze-Vergence-Controlled See-Through Vision in Augmented Reality. (arXiv:2207.02645v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02645","description":"<p>Augmented Reality (AR) see-through vision is an interesting research topic\nsince it enables users to see through a wall and see the occluded objects. Most\nexisting research focuses on the visual effects of see-through vision, while\nthe interaction method is less studied. However, we argue that using common\ninteraction modalities, e.g., midair click and speech, may not be the optimal\nway to control see-through vision. This is because when we want to see through\nsomething, it is physically related to our gaze depth/vergence and thus should\nbe naturally controlled by the eyes. Following this idea, this paper proposes a\nnovel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since\ngaze depth is needed, we build a gaze tracking module with two infrared cameras\nand the corresponding algorithm and assemble it into the Microsoft HoloLens 2\nto achieve gaze depth estimation. We then propose two different GVC modes for\nsee-through vision to fit different scenarios. Extensive experimental results\ndemonstrate that our gaze depth estimation is efficient and accurate. By\ncomparing with conventional interaction modalities, our GVC techniques are also\nshown to be superior in terms of efficiency and more preferred by users.\nFinally, we present four example applications of gaze-vergence-controlled\nsee-through vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuxin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Quality Assessment of Omnidirectional Images. (arXiv:2207.02674v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02674","description":"<p>Omnidirectional images and videos can provide immersive experience of\nreal-world scenes in Virtual Reality (VR) environment. We present a perceptual\nomnidirectional image quality assessment (IQA) study in this paper since it is\nextremely important to provide a good quality of experience under the VR\nenvironment. We first establish an omnidirectional IQA (OIQA) database, which\nincludes 16 source images and 320 distorted images degraded by 4 commonly\nencountered distortion types, namely JPEG compression, JPEG2000 compression,\nGaussian blur and Gaussian noise. Then a subjective quality evaluation study is\nconducted on the OIQA database in the VR environment. Considering that humans\ncan only see a part of the scene at one movement in the VR environment, visual\nattention becomes extremely important. Thus we also track head and eye movement\ndata during the quality rating experiments. The original and distorted\nomnidirectional images, subjective quality ratings, and the head and eye\nmovement data together constitute the OIQA database. State-of-the-art\nfull-reference (FR) IQA measures are tested on the OIQA database, and some new\nobservations different from traditional IQA are made.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1\">Huiyu Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1\">Guangtao Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1\">Xiongkuo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yucheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team PKU-WICT-MIPL PIC Makeup Temporal Video Grounding Challenge 2022 Technical Report. (arXiv:2207.02687v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02687","description":"<p>In this technical report, we briefly introduce the solutions of our team\n`PKU-WICT-MIPL' for the PIC Makeup Temporal Video Grounding (MTVG) Challenge in\nACM-MM 2022. Given an untrimmed makeup video and a step query, the MTVG aims to\nlocalize a temporal moment of the target makeup step in the video. To tackle\nthis task, we propose a phrase relationship mining framework to exploit the\ntemporal localization relationship relevant to the fine-grained phrase and the\nwhole sentence. Besides, we propose to constrain the localization results of\ndifferent step sentence queries to not overlap with each other through a\ndynamic programming algorithm. The experimental results demonstrate the\neffectiveness of our method. Our final submission ranked 2nd on the\nleaderboard, with only a 0.55\\% gap from the first.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1\">Minghang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dejie Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zhongjie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_T/0/1/0/all/0/1\">Ting Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yuxin Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. (arXiv:2207.02696v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02696","description":"<p>YOLOv7 surpasses all known object detectors in both speed and accuracy in the\nrange from 5 FPS to 160 FPS and has the highest accuracy 56.8% AP among all\nknown real-time object detectors with 30 FPS or higher on GPU V100. YOLOv7-E6\nobject detector (56 FPS V100, 55.9% AP) outperforms both transformer-based\ndetector SWIN-L Cascade-Mask R-CNN (9.2 FPS A100, 53.9% AP) by 509% in speed\nand 2% in accuracy, and convolutional-based detector ConvNeXt-XL Cascade-Mask\nR-CNN (8.6 FPS A100, 55.2% AP) by 551% in speed and 0.7% AP in accuracy, as\nwell as YOLOv7 outperforms: YOLOR, YOLOX, Scaled-YOLOv4, YOLOv5, DETR,\nDeformable DETR, DINO-5scale-R50, ViT-Adapter-B and many other object detectors\nin speed and accuracy. Moreover, we train YOLOv7 only on MS COCO dataset from\nscratch without using any other datasets or pre-trained weights. Source code is\nreleased in https://github.com/WongKinYiu/yolov7.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chien-Yao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bochkovskiy_A/0/1/0/all/0/1\">Alexey Bochkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hong-Yuan Mark Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spike Calibration: Fast and Accurate Conversion of Spiking Neural Network for Object Detection and Segmentation. (arXiv:2207.02702v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02702","description":"<p>Spiking neural network (SNN) has been attached to great importance due to the\nproperties of high biological plausibility and low energy consumption on\nneuromorphic hardware. As an efficient method to obtain deep SNN, the\nconversion method has exhibited high performance on various large-scale\ndatasets. However, it typically suffers from severe performance degradation and\nhigh time delays. In particular, most of the previous work focuses on simple\nclassification tasks while ignoring the precise approximation to ANN output. In\nthis paper, we first theoretically analyze the conversion errors and derive the\nharmful effects of time-varying extremes on synaptic currents. We propose the\nSpike Calibration (SpiCalib) to eliminate the damage of discrete spikes to the\noutput distribution and modify the LIPooling to allow conversion of the\narbitrary MaxPooling layer losslessly. Moreover, Bayesian optimization for\noptimal normalization parameters is proposed to avoid empirical settings. The\nexperimental results demonstrate the state-of-the-art performance on\nclassification, object detection, and segmentation tasks. To the best of our\nknowledge, this is the first time to obtain SNN comparable to ANN on these\ntasks simultaneously. Moreover, we only need 1/50 inference time of the\nprevious work on the detection task and can achieve the same performance under\n0.492$\\times$ energy consumption of ANN on the segmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yiting Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1\">Qingqun Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Histopathology DatasetGAN: Synthesizing Large-Resolution Histopathology Datasets. (arXiv:2207.02712v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02712","description":"<p>Self-supervised learning (SSL) methods are enabling an increasing number of\ndeep learning models to be trained on image datasets in domains where labels\nare difficult to obtain. These methods, however, struggle to scale to the high\nresolution of medical imaging datasets, where they are critical for achieving\ngood generalization on label-scarce medical image datasets. In this work, we\npropose the Histopathology DatasetGAN (HDGAN) framework, an extension of the\nDatasetGAN semi-supervised framework for image generation and segmentation that\nscales well to large-resolution histopathology images. We make several\nadaptations from the original framework, including updating the generative\nbackbone, selectively extracting latent features from the generator, and\nswitching to memory-mapped arrays. These changes reduce the memory consumption\nof the framework, improving its applicability to medical imaging domains. We\nevaluate HDGAN on a thrombotic microangiopathy high-resolution tile dataset,\ndemonstrating strong performance on the high-resolution image-annotation\ngeneration task. We hope that this work enables more application of deep\nlearning models to medical datasets, in addition to encouraging more\nexploration of self-supervised frameworks within the medical imaging domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rizvi_S/0/1/0/all/0/1\">S. A. Rizvi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cicalese_P/0/1/0/all/0/1\">P. Cicalese</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seshan_S/0/1/0/all/0/1\">S. V. Seshan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sciascia_S/0/1/0/all/0/1\">S. Sciascia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Becker_J/0/1/0/all/0/1\">J. U.Becker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1\">H.V. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open- and Closed-Loop Neural Network Verification using Polynomial Zonotopes. (arXiv:2207.02715v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02715","description":"<p>We present a novel approach to efficiently compute tight non-convex\nenclosures of the image through neural networks with ReLU, sigmoid, or\nhyperbolic tangent activation functions. In particular, we abstract the\ninput-output relation of each neuron by a polynomial approximation, which is\nevaluated in a set-based manner using polynomial zonotopes. Our proposed method\nis especially well suited for reachability analysis of neural network\ncontrolled systems since polynomial zonotopes are able to capture the\nnon-convexity in both, the image through the neural network as well as the\nreachable set. We demonstrate the superior performance of our approach compared\nto other state of the art methods on various benchmark systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kochdumper_N/0/1/0/all/0/1\">Niklas Kochdumper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1\">Christian Schilling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Althoff_M/0/1/0/all/0/1\">Matthias Althoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bak_S/0/1/0/all/0/1\">Stanley Bak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning approach for Classifying Trusses and Runners of Strawberries. (arXiv:2207.02721v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02721","description":"<p>The use of artificial intelligence in the agricultural sector has been\ngrowing at a rapid rate to automate farming activities. Emergent farming\ntechnologies focus on mapping and classification of plants, fruits, diseases,\nand soil types. Although, assisted harvesting and pruning applications using\ndeep learning algorithms are in the early development stages, there is a demand\nfor solutions to automate such processes. This paper proposes the use of Deep\nLearning for the classification of trusses and runners of strawberry plants\nusing semantic segmentation and dataset augmentation. The proposed approach is\nbased on the use of noises (i.e. Gaussian, Speckle, Poisson and\nSalt-and-Pepper) to artificially augment the dataset and compensate the low\nnumber of data samples and increase the overall classification performance. The\nresults are evaluated using mean average of precision, recall and F1 score. The\nproposed approach achieved 91\\%, 95\\% and 92\\% on precision, recall and F1\nscore, respectively, for truss detection using the ResNet101 with dataset\naugmentation utilising Salt-and-Pepper noise; and 83\\%, 53\\% and 65\\% on\nprecision, recall and F1 score, respectively, for truss detection using the\nResNet50 with dataset augmentation utilising Poisson noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pomykala_J/0/1/0/all/0/1\">Jakub Pomykala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lemos_F/0/1/0/all/0/1\">Francisco de Lemos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihianle_I/0/1/0/all/0/1\">Isibor Kennedy Ihianle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adama_D/0/1/0/all/0/1\">David Ada Adama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Gesture Recognition with Virtual Glove Markers. (arXiv:2207.02729v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02729","description":"<p>Due to the universal non-verbal natural communication approach that allows\nfor effective communication between humans, gesture recognition technology has\nbeen steadily developing over the previous few decades. Many different\nstrategies have been presented in research articles based on gesture\nrecognition to try to create an effective system to send non-verbal natural\ncommunication information to computers, using both physical sensors and\ncomputer vision. Hyper accurate real-time systems, on the other hand, have only\nrecently began to occupy the study field, with each adopting a range of\nmethodologies due to past limits such as usability, cost, speed, and accuracy.\nA real-time computer vision-based human-computer interaction tool for gesture\nrecognition applications that acts as a natural user interface is proposed.\nVirtual glove markers on users hands will be created and used as input to a\ndeep learning model for the real-time recognition of gestures. The results\nobtained show that the proposed system would be effective in real-time\napplications including social interaction through telepresence and\nrehabilitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McKinnon_F/0/1/0/all/0/1\">Finlay McKinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adama_D/0/1/0/all/0/1\">David Ada Adama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1\">Pedro Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihianle_I/0/1/0/all/0/1\">Isibor Kennedy Ihianle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STVGFormer: Spatio-Temporal Video Grounding with Static-Dynamic Cross-Modal Understanding. (arXiv:2207.02756v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02756","description":"<p>In this technical report, we introduce our solution to human-centric\nspatio-temporal video grounding task. We propose a concise and effective\nframework named STVGFormer, which models spatiotemporal visual-linguistic\ndependencies with a static branch and a dynamic branch. The static branch\nperforms cross-modal understanding in a single frame and learns to localize the\ntarget object spatially according to intra-frame visual cues like object\nappearances. The dynamic branch performs cross-modal understanding across\nmultiple frames. It learns to predict the starting and ending time of the\ntarget moment according to dynamic visual cues like motions. Both the static\nand dynamic branches are designed as cross-modal transformers. We further\ndesign a novel static-dynamic interaction block to enable the static and\ndynamic branches to transfer useful and complementary information from each\nother, which is shown to be effective to improve the prediction on hard cases.\nOur proposed method achieved 39.6% vIoU and won the first place in the HC-STVG\ntrack of the 4th Person in Context Challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zihang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chaolei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian-Fang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhi Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tiancai Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wei-Shi Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Relighting of Real Scenes. (arXiv:2207.02774v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02774","description":"<p>We introduce the task of local relighting, which changes a photograph of a\nscene by switching on and off the light sources that are visible within the\nimage. This new task differs from the traditional image relighting problem, as\nit introduces the challenge of detecting light sources and inferring the\npattern of light that emanates from them. We propose an approach for local\nrelighting that trains a model without supervision of any novel image dataset\nby using synthetically generated image pairs from another model. Concretely, we\ncollect paired training images from a stylespace-manipulated GAN; then we use\nthese images to train a conditional image-to-image model. To benchmark local\nrelighting, we introduce Lonoff, a collection of 306 precisely aligned images\ntaken in indoor spaces with different combinations of lights switched on. We\nshow that our method significantly outperforms baseline methods based on GAN\ninversion. Finally, we demonstrate extensions of our method that control\ndifferent light sources separately. We invite the community to tackle this new\ntask of local relighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_A/0/1/0/all/0/1\">Audrey Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanian_A/0/1/0/all/0/1\">Ali Jahanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lapedriza_A/0/1/0/all/0/1\">Agata Lapedriza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdizadehaghdam_S/0/1/0/all/0/1\">Shahin Mahdizadehaghdam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rohit Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1\">David Bau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-receptive Focused Inference Network for Lightweight Image Super-Resolution. (arXiv:2207.02796v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02796","description":"<p>With the development of deep learning, single image super-resolution (SISR)\nhas achieved significant breakthroughs. Recently, methods to enhance the\nperformance of SISR networks based on global feature interactions have been\nproposed. However, the capabilities of neurons that need to adjust their\nfunction in response to the context dynamically are neglected. To address this\nissue, we propose a lightweight Cross-receptive Focused Inference Network\n(CFIN), a hybrid network composed of a Convolutional Neural Network (CNN) and a\nTransformer. Specifically, a novel Cross-receptive Field Guide Transformer\n(CFGT) is designed to adaptively modify the network weights by using modulated\nconvolution kernels combined with local representative semantic information. In\naddition, a CNN-based Cross-scale Information Aggregation Module (CIAM) is\nproposed to make the model better focused on potentially practical information\nand improve the efficiency of the Transformer stage. Extensive experiments show\nthat our proposed CFIN is a lightweight and efficient SISR model, which can\nachieve a good balance between computational cost and model performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juncheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Guangwei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jiantao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guo-Jun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning. (arXiv:2207.02797v1 [eess.IV])","link":"http://arxiv.org/abs/2207.02797","description":"<p>The manifold hypothesis is a core mechanism behind the success of deep\nlearning, so understanding the intrinsic manifold structure of image data is\ncentral to studying how neural networks learn from the data. Intrinsic dataset\nmanifolds and their relationship to learning difficulty have recently begun to\nbe studied for the common domain of natural images, but little such research\nhas been attempted for radiological images. We address this here. First, we\ncompare the intrinsic manifold dimensionality of radiological and natural\nimages. We also investigate the relationship between intrinsic dimensionality\nand generalization ability over a wide range of datasets. Our analysis shows\nthat natural image datasets generally have a higher number of intrinsic\ndimensions than radiological images. However, the relationship between\ngeneralization ability and intrinsic dimensionality is much stronger for\nmedical images, which could be explained as radiological images having\nintrinsic features that are more difficult to learn. These results give a more\nprincipled underpinning for the intuition that radiological images can be more\nchallenging to apply deep learning to than natural image datasets common to\nmachine learning research. We believe rather than directly applying models\ndeveloped for natural images to the radiological imaging domain, more care\nshould be taken to developing architectures and algorithms that are more\ntailored to the specific characteristics of this domain. The research shown in\nour paper, demonstrating these characteristics and the differences from natural\nimages, is an important first step in this direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Konz_N/0/1/0/all/0/1\">Nicholas Konz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_H/0/1/0/all/0/1\">Hanxue Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delving into Sequential Patches for Deepfake Detection. (arXiv:2207.02803v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02803","description":"<p>Recent advances in face forgery techniques produce nearly visually\nuntraceable deepfake videos, which could be leveraged with malicious\nintentions. As a result, researchers have been devoted to deepfake detection.\nPrevious studies has identified the importance of local low-level cues and\ntemporal information in pursuit to generalize well across deepfake methods,\nhowever, they still suffer from robustness problem against post-processings. In\nthis work, we propose the Local- &amp; Temporal-aware Transformer-based Deepfake\nDetection (LTTD) framework, which adopts a local-to-global learning protocol\nwith a particular focus on the valuable temporal information within local\nsequences. Specifically, we propose a Local Sequence Transformer (LST), which\nmodels the temporal consistency on sequences of restricted spatial regions,\nwhere low-level information is hierarchically enhanced with shallow layers of\nlearned 3D filters. Based on the local temporal embeddings, we then achieve the\nfinal classification in a global contrastive way. Extensive experiments on\npopular datasets validate that our approach effectively spots local forgery\ncues and achieves state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiazhi Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Zhibin Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_C/0/1/0/all/0/1\">Chengbin Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Youjian Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DPODv2: Dense Correspondence-Based 6 DoF Pose Estimation. (arXiv:2207.02805v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02805","description":"<p>We propose a three-stage 6 DoF object detection method called DPODv2 (Dense\nPose Object Detector) that relies on dense correspondences. We combine a 2D\nobject detector with a dense correspondence estimation network and a multi-view\npose refinement method to estimate a full 6 DoF pose. Unlike other deep\nlearning methods that are typically restricted to monocular RGB images, we\npropose a unified deep learning network allowing different imaging modalities\nto be used (RGB or Depth). Moreover, we propose a novel pose refinement method,\nthat is based on differentiable rendering. The main concept is to compare\npredicted and rendered correspondences in multiple views to obtain a pose which\nis consistent with predicted correspondences in all views. Our proposed method\nis evaluated rigorously on different data modalities and types of training data\nin a controlled setup. The main conclusions is that RGB excels in\ncorrespondence estimation, while depth contributes to the pose accuracy if good\n3D-3D correspondences are available. Naturally, their combination achieves the\noverall best performance. We perform an extensive evaluation and an ablation\nstudy to analyze and validate the results on several challenging datasets.\nDPODv2 achieves excellent results on all of them while still remaining fast and\nscalable independent of the used data modality and the type of training data\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shugurov_I/0/1/0/all/0/1\">Ivan Shugurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-View Object Pose Refinement With Differentiable Renderer. (arXiv:2207.02811v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02811","description":"<p>This paper introduces a novel multi-view 6 DoF object pose refinement\napproach focusing on improving methods trained on synthetic data. It is based\non the DPOD detector, which produces dense 2D-3D correspondences between the\nmodel vertices and the image pixels in each frame. We have opted for the use of\nmultiple frames with known relative camera transformations, as it allows\nintroduction of geometrical constraints via an interpretable ICP-like loss\nfunction. The loss function is implemented with a differentiable renderer and\nis optimized iteratively. We also demonstrate that a full detection and\nrefinement pipeline, which is trained solely on synthetic data, can be used for\nauto-labeling real data. We perform quantitative evaluation on LineMOD,\nOcclusion, Homebrewed and YCB-V datasets and report excellent performance in\ncomparison to the state-of-the-art methods trained on the synthetic and real\ndata. We demonstrate empirically that our approach requires only a few frames\nand is robust to close camera locations and noise in extrinsic camera\ncalibration, making its practical usage easier and more ubiquitous.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shugurov_I/0/1/0/all/0/1\">Ivan Shugurov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlov_I/0/1/0/all/0/1\">Ivan Pavlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1\">Sergey Zakharov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilic_S/0/1/0/all/0/1\">Slobodan Ilic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Counterfactual Image Manipulation via CLIP. (arXiv:2207.02812v1 [cs.CV])","link":"http://arxiv.org/abs/2207.02812","description":"<p>Leveraging StyleGAN's expressivity and its disentangled latent codes,\nexisting methods can achieve realistic editing of different visual attributes\nsuch as age and gender of facial images. An intriguing yet challenging problem\narises: Can generative models achieve counterfactual editing against their\nlearnt priors? Due to the lack of counterfactual samples in natural datasets,\nwe investigate this problem in a text-driven manner with\nContrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic\nknowledge even for various counterfactual concepts. Different from in-domain\nmanipulation, counterfactual manipulation requires more comprehensive\nexploitation of semantic knowledge encapsulated in CLIP as well as more\ndelicate handling of editing directions for avoiding being stuck in local\nminimum or undesired editing. To this end, we design a novel contrastive loss\nthat exploits predefined CLIP-space directions to guide the editing toward\ndesired directions from different perspectives. In addition, we design a simple\nyet effective scheme that explicitly maps CLIP embeddings (of target text) to\nthe latent space and fuses them with latent codes for effective latent code\noptimization and accurate editing. Extensive experiments show that our design\nachieves accurate and realistic editing while driving by target texts with\nvarious counterfactual concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Miaomiao Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Localization Uncertainty Estimation for Anchor-Free Object Detection. (arXiv:2006.15607v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.15607","description":"<p>Since many safety-critical systems, such as surgical robots and autonomous\ndriving cars operate in unstable environments with sensor noise and incomplete\ndata, it is desirable for object detectors to take the localization uncertainty\ninto account. However, there are several limitations of the existing\nuncertainty estimation methods for anchor-based object detection. 1) They model\nthe uncertainty of the heterogeneous object properties with different\ncharacteristics and scales, such as location (center point) and scale (width,\nheight), which could be difficult to estimate. 2) They model box offsets as\nGaussian distributions, which is not compatible with the ground truth bounding\nboxes that follow the Dirac delta distribution. 3) Since anchor-based methods\nare sensitive to anchor hyper-parameters, their localization uncertainty could\nalso be highly sensitive to the choice of hyper-parameters. To tackle these\nlimitations, we propose a new localization uncertainty estimation method called\nUAD for anchor-free object detection. Our method captures the uncertainty in\nfour directions of box offsets (left, right, top, bottom) that are homogeneous,\nso that it can tell which direction is uncertain, and provide a quantitative\nvalue of uncertainty in [0, 1]. To enable such uncertainty estimation, we\ndesign a new uncertainty loss, negative power log-likelihood loss, to measure\nthe localization uncertainty by weighting the likelihood loss by its IoU, which\nalleviates the model misspecification problem. Furthermore, we propose an\nuncertainty-aware focal loss for reflecting the estimated uncertainty to the\nclassification score. Experimental results on COCO datasets demonstrate that\nour method significantly improves FCOS, by up to 1.8 points, without\nsacrificing computational efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Joong-won Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyung-Il Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1\">Kimin Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1\">Yongjin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_Y/0/1/0/all/0/1\">Yuseok Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Contrastive Patch-Based Subspace Learning for Camera Image Signal Processing. (arXiv:2104.00253v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2104.00253","description":"<p>Camera Image Signal Processing(ISP) pipelines, including deep learning\ntrained versions, can get appealing results in different image signal\nprocessing tasks. However, most if not all of these methods tend to apply a\nsingle filter that is homogeneous over the entire image. This is also\nparticularly true when an encoder-decoder type deep architecture is trained for\nthe task. However, it is natural to view a camera image as heterogeneous, as\nthe color intensity and the artificial noise are distributed vastly different,\neven across the two dimensional domain of a single image. Varied Moire ringing,\nmotion-blur, color-bleaching or lens based projection distortions can all\npotentially lead to a heterogeneous image artifact filtering problem. In this\npaper, we present a specific patch-based, local subspace deep neural network\nthat improves Camera ISP to be robust to heterogeneous artifacts (especially\nimage denoising). We call our three-fold deep trained model the Patch Subspace\nLearning Autoencoder (PSL-AE). PSL-AE does not necessarily assume uniform image\ndistortion levels nor repeated nor similar artifact types within the image.\nRather, PSL-AE first diagnostically encodes patches extracted from noisy and\nclean image pairs, with different artifact type and distortion levels, by\ncontrastive learning. Then, each image's patches are encoded into soft-clusters\nin their appropriate latent sub-space, using a prior mixture model. Lastly, the\ndecoders of the PSL-AE are also trained in an unsupervised manner customized\nfor the image patches in each soft-cluster. Our experimental results\ndemonstrates the flexibility and performance that one can achieve through\nimproved heterogeneous filtering, both from synthesized artifacts but also\nrealistic SIDD image pairs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yunhao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuhan Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bajaj_C/0/1/0/all/0/1\">Chandrajit Bajaj</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Detransformation Autoencoder for Representation Learning in Open Set Recognition. (arXiv:2105.13557v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.13557","description":"<p>The objective of Open set recognition (OSR) is to learn a classifier that can\nreject the unknown samples while classifying the known classes accurately. In\nthis paper, we propose a self-supervision method, Detransformation Autoencoder\n(DTAE), for the OSR problem. This proposed method engages in learning\nrepresentations that are invariant to the transformations of the input data.\nExperiments on several standard image datasets indicate that the pre-training\nprocess significantly improves the model performance in the OSR tasks.\nMeanwhile, our proposed self-supervision method achieves significant gains in\ndetecting the unknown class and classifying the known classes. Moreover, our\nanalysis indicates that DTAE can yield representations that contain more target\nclass information and less transformation information than RotNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jingyun Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_P/0/1/0/all/0/1\">Philip K. Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Object Wake-up: 3D Object Rigging from a Single Image. (arXiv:2108.02708v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02708","description":"<p>Given a single image of a general object such as a chair, could we also\nrestore its articulated 3D shape similar to human modeling, so as to animate\nits plausible articulations and diverse motions? This is an interesting new\nquestion that may have numerous downstream augmented reality and virtual\nreality applications. Comparing with previous efforts on object manipulation,\nour work goes beyond 2D manipulation and rigid deformation, and involves\narticulated manipulation. To achieve this goal, we propose an automated\napproach to build such 3D generic objects from single images and embed\narticulated skeletons in them. Specifically, our framework starts by\nreconstructing the 3D object from an input image. Afterwards, to extract\nskeletons for generic 3D objects, we develop a novel skeleton prediction method\nwith a multi-head structure for skeleton probability field estimation by\nutilizing the deep implicit functions. A dataset of generic 3D objects with\nground-truth annotated skeletons is collected. Empirically our approach is\ndemonstrated with satisfactory performance on public datasets as well as our\nin-house dataset; our results surpass those of the state-of-the-arts by a\nnoticeable margin on both 3D reconstruction and skeleton prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Ji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xinxin Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhenbo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1\">Minglun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DexMV: Imitation Learning for Dexterous Manipulation from Human Videos. (arXiv:2108.05877v5 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.05877","description":"<p>While significant progress has been made on understanding hand-object\ninteractions in computer vision, it is still very challenging for robots to\nperform complex dexterous manipulation. In this paper, we propose a new\nplatform and pipeline DexMV (Dexterous Manipulation from Videos) for imitation\nlearning. We design a platform with: (i) a simulation system for complex\ndexterous manipulation tasks with a multi-finger robot hand and (ii) a computer\nvision system to record large-scale demonstrations of a human hand conducting\nthe same tasks. In our novel pipeline, we extract 3D hand and object poses from\nvideos, and propose a novel demonstration translation method to convert human\nmotion to robot demonstrations. We then apply and benchmark multiple imitation\nlearning algorithms with the demonstrations. We show that the demonstrations\ncan indeed improve robot learning by a large margin and solve the complex tasks\nwhich reinforcement learning alone cannot solve. More details can be found in\nthe project page: https://yzqin.github.io/dexmv\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yueh-Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shaowei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hanwen Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruihan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks. (arXiv:2110.05668v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.05668","description":"<p>Most existing neural architecture search (NAS) benchmarks and algorithms\nprioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet.\nThis makes the performance of NAS approaches in more diverse areas poorly\nunderstood. In this paper, we present NAS-Bench-360, a benchmark suite to\nevaluate methods on domains beyond those traditionally studied in architecture\nsearch, and use it to address the following question: do state-of-the-art NAS\nmethods perform well on diverse tasks? To construct the benchmark, we curate\nten tasks spanning a diverse array of application domains, dataset sizes,\nproblem dimensionalities, and learning objectives. Each task is carefully\nchosen to interoperate with modern CNN-based search methods while possibly\nbeing far-afield from its original development domain. To speed up and reduce\nthe cost of NAS research, for two of the tasks we release the precomputed\nperformance of 15,625 architectures comprising a standard CNN search space.\nExperimentally, we show the need for more robust NAS evaluation of the kind\nNAS-Bench-360 enables by showing that several modern NAS procedures perform\ninconsistently across the ten tasks, with many catastrophically poor results.\nWe also demonstrate how NAS-Bench-360 and its associated precomputed results\nwill enable future scientific discoveries by testing whether several recent\nhypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360\nis hosted at https://nb360.ml.cmu.edu.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Renbo Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1\">Nicholas Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khodak_M/0/1/0/all/0/1\">Mikhail Khodak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Junhong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1\">Frederic Sala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Survey on Anomaly, Novelty, Open-Set, and Out-of-Distribution Detection: Solutions and Future Challenges. (arXiv:2110.14051v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.14051","description":"<p>Machine learning models often encounter samples that are diverged from the\ntraining distribution. Failure to recognize an out-of-distribution (OOD)\nsample, and consequently assign that sample to an in-class label significantly\ncompromises the reliability of a model. The problem has gained significant\nattention due to its importance for safety deploying models in open-world\nsettings. Detecting OOD samples is challenging due to the intractability of\nmodeling all possible unknown distributions. To date, several research domains\ntackle the problem of detecting unfamiliar samples, including anomaly\ndetection, novelty detection, one-class learning, open set recognition, and\nout-of-distribution detection. Despite having similar and shared concepts,\nout-of-distribution, open-set, and anomaly detection have been investigated\nindependently. Accordingly, these research avenues have not cross-pollinated,\ncreating research barriers. While some surveys intend to provide an overview of\nthese approaches, they seem to only focus on a specific domain without\nexamining the relationship between different domains. This survey aims to\nprovide a cross-domain and comprehensive review of numerous eminent works in\nrespective areas while identifying their commonalities. Researchers can benefit\nfrom the overview of research advances in different fields and develop future\nmethodology synergistically. Furthermore, to the best of our knowledge, while\nthere are surveys in anomaly detection or one-class learning, there is no\ncomprehensive or up-to-date survey on out-of-distribution detection, which our\nsurvey covers extensively. Finally, having a unified cross-domain perspective,\nwe discuss and shed light on future lines of research, intending to bring these\nfields closer together.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1\">Mohammadreza Salehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirzaei_H/0/1/0/all/0/1\">Hossein Mirzaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Mask: Real-World Universal Adversarial Attack on Face Recognition Models. (arXiv:2111.10759v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10759","description":"<p>Deep learning-based facial recognition (FR) models have demonstrated\nstate-of-the-art performance in the past few years, even when wearing\nprotective medical face masks became commonplace during the COVID-19 pandemic.\nGiven the outstanding performance of these models, the machine learning\nresearch community has shown increasing interest in challenging their\nrobustness. Initially, researchers presented adversarial attacks in the digital\ndomain, and later the attacks were transferred to the physical domain. However,\nin many cases, attacks in the physical domain are conspicuous, and thus may\nraise suspicion in real-world environments (e.g., airports). In this paper, we\npropose Adversarial Mask, a physical universal adversarial perturbation (UAP)\nagainst state-of-the-art FR models that is applied on face masks in the form of\na carefully crafted pattern. In our experiments, we examined the\ntransferability of our adversarial mask to a wide range of FR model\narchitectures and datasets. In addition, we validated our adversarial mask's\neffectiveness in real-world experiments (CCTV use case) by printing the\nadversarial pattern on a fabric face mask. In these experiments, the FR system\nwas only able to identify 3.34% of the participants wearing the mask (compared\nto a minimum of 83.34% with other evaluated masks). A demo of our experiments\ncan be found at: https://youtu.be/_TXkDO5z11w.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zolfi_A/0/1/0/all/0/1\">Alon Zolfi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avidan_S/0/1/0/all/0/1\">Shai Avidan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1\">Asaf Shabtai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Based Automated COVID-19 Classification from Computed Tomography Images. (arXiv:2111.11191v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.11191","description":"<p>The paper presents a method of a Convolutional Neural Networks (CNN) model\nfor image classification with image preprocessing and hyperparameters tuning,\naiming at increasing the predictive performance for COVID-19 diagnosis while\navoiding deeper and thus more complex alternatives. Firstly, the CNN model\nincludes four similar convolutional layers followed by a flattening and two\ndense layers. This work proposes a less complex solution based on simply\nclassifying 2D slices of CT scans using a CNN model. Despite the simplicity in\narchitecture, the proposed CNN model showed improved quantitative results\nexceeding state-of-the-art on the dataset of images, in terms of the macro F1\nscore. The results were achieved on the original CT slices of the dataset.\nSecondly, the original dataset was processed via anatomy-relevant masking of\nslice, removing none-representative slices from the CT volume, and\nhyperparameters tuning. For slice processing, a fixed-sized rectangular area\nwas used for cropping an anatomy-relevant region-of-interest in the images, and\na threshold based on the number of white pixels in binarized slices was\nemployed to remove none-representative slices from the 3D-CT scans. The CNN\nmodel with a learning rate schedule and an exponential decay and slice flipping\ntechniques was deployed on the processed slices. The proposed method was used\nto make predictions on the 2D slices. For final diagnosis at patient level,\nmajority voting was applied on the slices of each CT scan to take the\ndiagnosis. The macro F1 score of the proposed method well-exceeded the baseline\napproach and other alternatives on the validation set as well as on a test\npartition of previously unseen images from COV-19CT-DB dataset partitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1\">Kenan Morani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Unay_D/0/1/0/all/0/1\">Devrim Unay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Discriminative Shrinkage Deep Networks for Image Deconvolution. (arXiv:2111.13876v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13876","description":"<p>Most existing methods usually formulate the non-blind deconvolution problem\ninto a maximum-a-posteriori framework and address it by manually designing\nkinds of regularization terms and data terms of the latent clear images.\nHowever, explicitly designing these two terms is quite challenging and usually\nleads to complex optimization problems which are difficult to solve. In this\npaper, we propose an effective non-blind deconvolution approach by learning\ndiscriminative shrinkage functions to implicitly model these terms. In contrast\nto most existing methods that use deep convolutional neural networks (CNNs) or\nradial basis functions to simply learn the regularization term, we formulate\nboth the data term and regularization term and split the deconvolution model\ninto data-related and regularization-related sub-problems according to the\nalternating direction method of multipliers. We explore the properties of the\nMaxout function and develop a deep CNN model with a Maxout layer to learn\ndiscriminative shrinkage functions to directly approximate the solutions of\nthese two sub-problems. Moreover, given the fast-Fourier-transform-based image\nrestoration usually leads to ringing artifacts while conjugate-gradient-based\napproach is time-consuming, we develop the Conjugate Gradient Network to\nrestore the latent clear images effectively and efficiently. Experimental\nresults show that the proposed method performs favorably against the\nstate-of-the-art ones in terms of efficiency and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_P/0/1/0/all/0/1\">Pin-Hung Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jinshan Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Minimal Misalignment at Minimal Cost in One-Stage and Anchor-Free Object Detection. (arXiv:2112.08902v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08902","description":"<p>Common object detection models consist of classification and regression\nbranches, due to different task drivers, these two branches have different\nsensibility to the features from the same scale level and the same spatial\nlocation. The point-based prediction method, which is based on the assumption\nthat the high classification confidence point has the high regression quality,\nleads to the misalignment problem. Our analysis shows, the problem is further\ncomposed of scale misalignment and spatial misalignment specifically. We aim to\nresolve the phenomenon at minimal cost: a minor adjustment of the head network\nand a new label assignment method replacing the rigid one. Our experiments show\nthat, compared to the baseline FCOS, a one-stage and anchor-free object\ndetection model, our model consistently get around 3 AP improvement with\ndifferent backbones, demonstrating both simplicity and efficiency of our\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Shuaizheng Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongzhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ningwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Cheng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Masking for Self-Supervised Learning. (arXiv:2201.13100v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.13100","description":"<p>We propose ADIOS, a masked image model (MIM) framework for self-supervised\nlearning, which simultaneously learns a masking function and an image encoder\nusing an adversarial objective. The image encoder is trained to minimise the\ndistance between representations of the original and that of a masked image.\nThe masking function, conversely, aims at maximising this distance. ADIOS\nconsistently improves on state-of-the-art self-supervised learning (SSL)\nmethods on a variety of tasks and datasets -- including classification on\nImageNet100 and STL10, transfer learning on CIFAR10/100, Flowers102 and\niNaturalist, as well as robustness evaluated on the backgrounds challenge (Xiao\net al., 2021) -- while generating semantically meaningful masks. Unlike modern\nMIM models such as MAE, BEiT and iBOT, ADIOS does not rely on the image-patch\ntokenisation construction of Vision Transformers, and can be implemented with\nconvolutional backbones. We further demonstrate that the masks learned by ADIOS\nare more effective in improving representation learning of SSL methods than\nmasking schemes used in popular MIM models. Code is available at\nhttps://github.com/YugeTen/adios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosiorek_A/0/1/0/all/0/1\">Adam R. Kosiorek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Neighbor Consistency for Noisy Labels. (arXiv:2202.02200v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.02200","description":"<p>Recent advances in deep learning have relied on large, labelled datasets to\ntrain high-capacity models. However, collecting large datasets in a time- and\ncost-efficient manner often results in label noise. We present a method for\nlearning from noisy labels that leverages similarities between training\nexamples in feature space, encouraging the prediction of each example to be\nsimilar to its nearest neighbours. Compared to training algorithms that use\nmultiple models or distinct stages, our approach takes the form of a simple,\nadditional regularization term. It can be interpreted as an inductive version\nof the classical, transductive label propagation algorithm. We thoroughly\nevaluate our method on datasets evaluating both synthetic (CIFAR-10, CIFAR-100)\nand realistic (mini-WebVision, WebVision, Clothing1M, mini-ImageNet-Red) noise,\nand achieve competitive or state-of-the-art accuracies across all of them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iscen_A/0/1/0/all/0/1\">Ahmet Iscen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valmadre_J/0/1/0/all/0/1\">Jack Valmadre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GroupViT: Semantic Segmentation Emerges from Text Supervision. (arXiv:2202.11094v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11094","description":"<p>Grouping and recognition are important components of visual scene\nunderstanding, e.g., for object detection and semantic segmentation. With\nend-to-end deep learning systems, grouping of image regions usually happens\nimplicitly via top-down supervision from pixel-level recognition labels.\nInstead, in this paper, we propose to bring back the grouping mechanism into\ndeep networks, which allows semantic segments to emerge automatically with only\ntext supervision. We propose a hierarchical Grouping Vision Transformer\n(GroupViT), which goes beyond the regular grid structure representation and\nlearns to group image regions into progressively larger arbitrary-shaped\nsegments. We train GroupViT jointly with a text encoder on a large-scale\nimage-text dataset via contrastive losses. With only text supervision and\nwithout any pixel-level annotations, GroupViT learns to group together semantic\nregions and successfully transfers to the task of semantic segmentation in a\nzero-shot manner, i.e., without any further fine-tuning. It achieves a\nzero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on\nPASCAL Context datasets, and performs competitively to state-of-the-art\ntransfer-learning methods requiring greater levels of supervision. We\nopen-source our code at https://github.com/NVlabs/GroupViT .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiarui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mello_S/0/1/0/all/0/1\">Shalini De Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sifei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byeon_W/0/1/0/all/0/1\">Wonmin Byeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Breuel_T/0/1/0/all/0/1\">Thomas Breuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kautz_J/0/1/0/all/0/1\">Jan Kautz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FUNQUE: Fusion of Unified Quality Evaluators. (arXiv:2202.11241v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.11241","description":"<p>Fusion-based quality assessment has emerged as a powerful method for\ndeveloping high-performance quality models from quality models that\nindividually achieve lower performances. A prominent example of such an\nalgorithm is VMAF, which has been widely adopted as an industry standard for\nvideo quality prediction along with SSIM. In addition to advancing the\nstate-of-the-art, it is imperative to alleviate the computational burden\npresented by the use of a heterogeneous set of quality models. In this paper,\nwe unify \"atom\" quality models by computing them on a common transform domain\nthat accounts for the Human Visual System, and we propose FUNQUE, a quality\nmodel that fuses unified quality evaluators. We demonstrate that in comparison\nto the state-of-the-art, FUNQUE offers significant improvements in both\ncorrelation against subjective scores and efficiency, due to computation\nsharing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_A/0/1/0/all/0/1\">Abhinau K. Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stejerean_C/0/1/0/all/0/1\">Cosmin Stejerean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AssistQ: Affordance-centric Question-driven Task Completion for Egocentric Assistant. (arXiv:2203.04203v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04203","description":"<p>A long-standing goal of intelligent assistants such as AR glasses/robots has\nbeen to assist users in affordance-centric real-world scenarios, such as \"how\ncan I run the microwave for 1 minute?\". However, there is still no clear task\ndefinition and suitable benchmarks. In this paper, we define a new task called\nAffordance-centric Question-driven Task Completion, where the AI assistant\nshould learn from instructional videos and scripts to guide the user\nstep-by-step. To support the task, we constructed AssistQ, a new dataset\ncomprising 531 question-answer samples derived from 100 newly filmed\nfirst-person videos. Each question should be completed with multi-step\nguidances by inferring from visual details (e.g., buttons' position) and\ntextural details (e.g., actions like press/turn). To address this unique task,\nwe developed a Question-to-Actions (Q2A) model that significantly outperforms\nseveral baseline methods while still having large room for improvement. We\nexpect our task and dataset to advance Egocentric AI Assistant's development.\nOur project page is available at: https://showlab.github.io/assistq\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wong_B/0/1/0/all/0/1\">Benita Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Joya Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1\">Stan Weixian Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_D/0/1/0/all/0/1\">Dongxing Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Difei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1\">Mike Zheng Shou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"City-wide Street-to-Satellite Image Geolocalization of a Mobile Ground Agent. (arXiv:2203.05612v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05612","description":"<p>Cross-view image geolocalization provides an estimate of an agent's global\nposition by matching a local ground image to an overhead satellite image\nwithout the need for GPS. It is challenging to reliably match a ground image to\nthe correct satellite image since the images have significant viewpoint\ndifferences. Existing works have demonstrated localization in constrained\nscenarios over small areas but have not demonstrated wider-scale localization.\nOur approach, called Wide-Area Geolocalization (WAG), combines a neural network\nwith a particle filter to achieve global position estimates for agents moving\nin GPS-denied environments, scaling efficiently to city-scale regions. WAG\nintroduces a trinomial loss function for a Siamese network to robustly match\nnon-centered image pairs and thus enables the generation of a smaller satellite\nimage database by coarsely discretizing the search area. A modified particle\nfilter weighting scheme is also presented to improve localization accuracy and\nconvergence. Taken together, WAG's network training and particle filter\nweighting approach achieves city-scale position estimation accuracies on the\norder of 20 meters, a 98% reduction compared to a baseline training and\nweighting approach. Applied to a smaller-scale testing area, WAG reduces the\nfinal position estimation error by 64% compared to a state-of-the-art baseline\nfrom the literature. WAG's search space discretization additionally\nsignificantly reduces storage and processing requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Downes_L/0/1/0/all/0/1\">Lena M. Downes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Ki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_T/0/1/0/all/0/1\">Ted J. Steiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08344","description":"<p>We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Jhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Normalized Density Map (SNDM) for Counting Microbiological Objects. (arXiv:2203.09474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09474","description":"<p>The statistical properties of the density map (DM) approach to counting\nmicrobiological objects on images are studied in detail. The DM is given by\nU$^2$-Net. Two statistical methods for deep neural networks are utilized: the\nbootstrap and the Monte Carlo (MC) dropout. The detailed analysis of the\nuncertainties for the DM predictions leads to a deeper understanding of the DM\nmodel's deficiencies. Based on our investigation, we propose a\nself-normalization module in the network. The improved network model, called\n\\textit{Self-Normalized Density Map} (SNDM), can correct its output density map\nby itself to accurately predict the total number of objects in the image. The\nSNDM architecture outperforms the original model. Moreover, both statistical\nframeworks -- bootstrap and MC dropout -- have consistent statistical results\nfor SNDM, which were not observed in the original model. The SNDM efficiency is\ncomparable with the detector-base models, such as Faster and Cascade R-CNN\ndetectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graczyk_K/0/1/0/all/0/1\">Krzysztof M. Graczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pawlowski_J/0/1/0/all/0/1\">Jaroslaw Pawlowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1\">Tomasz Golan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09855","description":"<p>In this paper, we formulate a potentially valuable panoramic depth completion\n(PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing\ndata in complex scenes. Its goal is to recover dense panoramic depths from raw\nsparse ones and panoramic RGB images. To deal with the PDC task, we train a\ndeep network that takes both depth and image as inputs for the dense panoramic\ndepth recovery. However, it needs to face a challenging optimization problem of\nthe network parameters due to its non-convex objective function. To address\nthis problem, we propose a simple yet effective approach termed M{^3}PT:\nmulti-modal masked pre-training. Specifically, during pre-training, we\nsimultaneously cover up patches of the panoramic RGB image and sparse depth by\nshared random mask, then reconstruct the sparse depth in the masked regions. To\nour best knowledge, it is the first time that we show the effectiveness of\nmasked pre-training in a multi-modal vision task, instead of the single-modal\ntask resolved by masked autoencoders (MAE). Different from MAE where\nfine-tuning completely discards the decoder part of pre-training, there is no\narchitectural difference between the pre-training and fine-tuning stages in our\nM$^{3}$PT as they only differ in the prediction density, which potentially\nmakes the transfer learning more convenient and effective. Extensive\nexperiments verify the effectiveness of M{^3}PT on three panoramic datasets.\nNotably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,\n51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Serves Traffic Safety Analysis: A Forward-looking Review. (arXiv:2203.10939v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.10939","description":"<p>This paper explores Deep Learning (DL) methods that are used or have the\npotential to be used for traffic video analysis, emphasizing driving safety for\nboth Autonomous Vehicles (AVs) and human-operated vehicles. We present a\ntypical processing pipeline, which can be used to understand and interpret\ntraffic videos by extracting operational safety metrics and providing general\nhints and guidelines to improve traffic safety. This processing framework\nincludes several steps, including video enhancement, video stabilization,\nsemantic and incident segmentation, object detection and classification,\ntrajectory extraction, speed estimation, event analysis, modeling and anomaly\ndetection. Our main goal is to guide traffic analysts to develop their own\ncustom-built processing frameworks by selecting the best choices for each step\nand offering new designs for the lacking modules by providing a comparative\nanalysis of the most successful conventional and DL-based algorithms proposed\nfor each step. We also review existing open-source tools and public datasets\nthat can help train DL models. To be more specific, we review exemplary traffic\nproblems and mentioned requires steps for each problem. Besides, we investigate\nconnections to the closely related research areas of drivers' cognition\nevaluation, Crowd-sourcing-based monitoring systems, Edge Computing in roadside\ninfrastructures, Automated Driving Systems (ADS)-equipped vehicles, and\nhighlight the missing gaps. Finally, we review commercial implementations of\ntraffic monitoring systems, their future outlook, and open problems and\nremaining challenges for widespread use of such systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1\">Abolfazl Razi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huayu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_B/0/1/0/all/0/1\">Brendan Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hongbin Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cell segmentation from telecentric bright-field transmitted light microscopy images using a Residual Attention U-Net: a case study on HeLa line. (arXiv:2203.12290v3 [q-bio.QM] UPDATED)","link":"http://arxiv.org/abs/2203.12290","description":"<p>Living cell segmentation from bright-field light microscopy images is\nchallenging due to the image complexity and temporal changes in the living\ncells. Recently developed deep learning (DL)-based methods became popular in\nmedical and microscopy image segmentation tasks due to their success and\npromising outcomes. The main objective of this paper is to develop a deep\nlearning, U-Net-based method to segment the living cells of the HeLa line in\nbright-field transmitted light microscopy. To find the most suitable\narchitecture for our datasets, a residual attention U-Net was proposed and\ncompared with an attention and a simple U-Net architecture.\n</p>\n<p>The attention mechanism highlights the remarkable features and suppresses\nactivations in the irrelevant image regions. The residual mechanism overcomes\nwith vanishing gradient problem. The Mean-IoU score for our datasets reaches\n0.9505, 0.9524, and 0.9530 for the simple, attention, and residual attention\nU-Net, respectively. The most accurate semantic segmentation results was\nachieved in the Mean-IoU and Dice metrics by applying the residual and\nattention mechanisms together. The watershed method applied to this best --\nResidual Attention -- semantic segmentation result gave the segmentation with\nthe specific information for each cell.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Ghaznavi_A/0/1/0/all/0/1\">Ali Ghaznavi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Rychtarikova_R/0/1/0/all/0/1\">Renata Rychtarikova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Saberioon_M/0/1/0/all/0/1\">Mohammadmehdi Saberioon</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stys_D/0/1/0/all/0/1\">Dalibor Stys</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Architecture for Machine Consciousness and Artificial Superintelligence: Updating Working Memory Iteratively. (arXiv:2203.17255v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2203.17255","description":"<p>This theoretical article examines how to construct human-like working memory\nand thought processes within a computer. There should be two working memory\nstores, one analogous to sustained firing in association cortex, and one\nanalogous to synaptic potentiation in the cerebral cortex. These stores must be\nconstantly updated with new representations that arise from either\nenvironmental stimulation or internal processing. They should be updated\ncontinuously, and in an iterative fashion, meaning that, in the next state,\nsome items in the set of coactive items should always be retained. Thus, the\nset of concepts coactive in working memory will evolve gradually and\nincrementally over time. This makes each state is a revised iteration of the\npreceding state and causes successive states to overlap and blend with respect\nto the set of representations they contain. As new representations are added\nand old ones are subtracted, some remain active for several seconds over the\ncourse of these changes. This persistent activity, similar to that used in\nartificial recurrent neural networks, is used to spread activation energy\nthroughout the global workspace to search for the next associative update. The\nresult is a chain of associatively linked intermediate states that are capable\nof advancing toward a solution or goal. Iterative updating is conceptualized\nhere as an information processing strategy, a computational and\nneurophysiological determinant of the stream of thought, and an algorithm for\ndesigning and programming artificial intelligence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Reser_J/0/1/0/all/0/1\">Jared Edward Reser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expression-preserving face frontalization improves visually assisted speech processing. (arXiv:2204.02810v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.02810","description":"<p>Face frontalization consists of synthesizing a frontally-viewed face from an\narbitrarily-viewed one. The main contribution of this paper is a frontalization\nmethodology that preserves non-rigid facial deformations in order to boost the\nperformance of visually assisted speech communication. The method alternates\nbetween the estimation of (i)~the rigid transformation (scale, rotation, and\ntranslation) and (ii)~the non-rigid deformation between an arbitrarily-viewed\nface and a face model. The method has two important merits: it can deal with\nnon-Gaussian errors in the data and it incorporates a dynamical face\ndeformation model. For that purpose, we use the generalized Student\nt-distribution in combination with a linear dynamic system in order to account\nfor both rigid head motions and time-varying facial deformations caused by\nspeech production. We propose to use the zero-mean normalized cross-correlation\n(ZNCC) score to evaluate the ability of the method to preserve facial\nexpressions. The method is thoroughly evaluated and compared with several state\nof the art methods, either based on traditional geometric models or on deep\nlearning. Moreover, we show that the method, when incorporated into deep\nlearning pipelines, namely lip reading and speech enhancement, improves word\nrecognition and speech intelligibilty scores by a considerable margin.\nSupplemental material is accessible at\nhttps://team.inria.fr/robotlearn/research/facefrontalization-benchmark/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhiqi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mostafa Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From 2D Images to 3D Model:Weakly Supervised Multi-View Face Reconstruction with Deep Fusion. (arXiv:2204.03842v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03842","description":"<p>We consider the problem of Multi-view 3D Face Reconstruction (MVR) with\nweakly supervised learning that leverages a limited number of 2D face images\n(e.g. 3) to generate a high-quality 3D face model with very light annotation.\nDespite their encouraging performance, present MVR methods simply concatenate\nmulti-view image features and pay less attention to critical areas (e.g. eye,\nbrow, nose, and mouth). To this end, we propose a novel model called Deep\nFusion MVR (DF-MVR) and design a multi-view encoding to a single decoding\nframework with skip connections, able to extract, integrate, and compensate\ndeep features with attention from multi-view images. In addition, we develop a\nmulti-view face parse network to learn, identify, and emphasize the critical\ncommon face area. Finally, though our model is trained with a few 2D images, it\ncan reconstruct an accurate 3D model even if one single 2D image is input. We\nconduct extensive experiments to evaluate various multi-view 3D face\nreconstruction methods. Experiments on Pixel-Face and Bosphorus datasets\nindicate the superiority of our model. Without 3D landmarks annotation, DF-MVR\nachieves 5.2% and 3.0% RMSE improvements over the existing best weakly\nsupervised MVRs respectively on Pixel-Face and Bosphorus datasets; with 3D\nlandmarks annotation, DF-MVR attains superior performance particularly on\nPixel-Face dataset, leading to 13.4% RMSE improvement over the best weakly\nsupervised MVR model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Weiguang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chaolong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianan Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yuyao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaizhu Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2022: pothole and crack detection in the road pavement using images and RGB-D data. (arXiv:2205.13326v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.13326","description":"<p>This paper describes the methods submitted for evaluation to the SHREC 2022\ntrack on pothole and crack detection in the road pavement. A total of 7\ndifferent runs for the semantic segmentation of the road surface are compared,\n6 from the participants plus a baseline method. All methods exploit Deep\nLearning techniques and their performance is tested using the same environment\n(i.e.: a single Jupyter notebook). A training set, composed of 3836 semantic\nsegmentation image/mask pairs and 797 RGB-D video clips collected with the\nlatest depth cameras was made available to the participants. The methods are\nthen evaluated on the 496 image/mask pairs in the validation set, on the 504\npairs in the test set and finally on 8 video clips. The analysis of the results\nis based on quantitative metrics for image segmentation and qualitative\nanalysis of the video clips. The participation and the results show that the\nscenario is of great interest and that the use of RGB-D data is still\nchallenging in this context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thompson_E/0/1/0/all/0/1\">Elia Moscoso Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranieri_A/0/1/0/all/0/1\">Andrea Ranieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biasotti_S/0/1/0/all/0/1\">Silvia Biasotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chicchon_M/0/1/0/all/0/1\">Miguel Chicchon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sipiran_I/0/1/0/all/0/1\">Ivan Sipiran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1\">Minh-Khoi Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_Ho_T/0/1/0/all/0/1\">Thang-Long Nguyen-Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hai-Dang Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation. (arXiv:2205.14141v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2205.14141","description":"<p>Masked image modeling (MIM) learns representations with remarkably good\nfine-tuning performances, overshadowing previous prevalent pre-training\napproaches such as image classification, instance contrastive learning, and\nimage-text alignment. In this paper, we show that the inferior fine-tuning\nperformance of these pre-training approaches can be significantly improved by a\nsimple post-processing in the form of feature distillation (FD). The feature\ndistillation converts the old representations to new representations that have\na few desirable properties just like those representations produced by MIM.\nThese properties, which we aggregately refer to as optimization friendliness,\nare identified and analyzed by a set of attention- and optimization-related\ndiagnosis tools. With these properties, the new representations show strong\nfine-tuning performance. Specifically, the contrastive self-supervised learning\nmethods are made as competitive in fine-tuning as the state-of-the-art masked\nimage modeling (MIM) algorithms. The CLIP models' fine-tuning performance is\nalso significantly improved, with a CLIP ViT-L model reaching \\textbf{89.0%}\ntop-1 accuracy on ImageNet-1K classification. On the 3-billion-parameter\nSwinV2-G model, the fine-tuning accuracy on ADE20K semantic segmentation is\nimproved by +1.5 mIoU to \\textbf{61.4 mIoU}, creating a new record. More\nimportantly, our work provides a way for the future research to focus more\neffort on the generality and scalability of the learnt representations without\nbeing pre-occupied with optimization friendliness since it can be enhanced\nrather easily. The code will be available at\nhttps://github.com/SwinTransformer/Feature-Distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yixuan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1\">Zhenda Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yue Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision GNN: An Image is Worth Graph of Nodes. (arXiv:2206.00272v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.00272","description":"<p>Network architecture plays a key role in the deep learning-based computer\nvision system. The widely-used convolutional neural network and transformer\ntreat the image as a grid or sequence structure, which is not flexible to\ncapture irregular and complex objects. In this paper, we propose to represent\nthe image as a graph structure and introduce a new Vision GNN (ViG)\narchitecture to extract graph-level feature for visual tasks. We first split\nthe image to a number of patches which are viewed as nodes, and construct a\ngraph by connecting the nearest neighbors. Based on the graph representation of\nimages, we build our ViG model to transform and exchange information among all\nthe nodes. ViG consists of two basic modules: Grapher module with graph\nconvolution for aggregating and updating graph information, and FFN module with\ntwo linear layers for node feature transformation. Both isotropic and pyramid\narchitectures of ViG are built with different model sizes. Extensive\nexperiments on image recognition and object detection tasks demonstrate the\nsuperiority of our ViG architecture. We hope this pioneering study of GNN on\ngeneral visual tasks will provide useful inspiration and experience for future\nresearch. The PyTorch code is available at\nhttps://github.com/huawei-noah/Efficient-AI-Backbones and the MindSpore code is\navailable at https://gitee.com/mindspore/models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jianyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1\">Enhua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-aware Panoptic Segmentation. (arXiv:2206.14554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2206.14554","description":"<p>Reliable scene understanding is indispensable for modern autonomous systems.\nCurrent learning-based methods typically try to maximize their performance\nbased on segmentation metrics that only consider the quality of the\nsegmentation. However, for the safe operation of a system in the real world it\nis crucial to consider the uncertainty in the prediction as well. In this work,\nwe introduce the novel task of uncertainty-aware panoptic segmentation, which\naims to predict per-pixel semantic and instance segmentations, together with\nper-pixel uncertainty estimates. We define two novel metrics to facilitate its\nquantitative analysis, the uncertainty-aware Panoptic Quality (uPQ) and the\npanoptic Expected Calibration Error (pECE). We further propose the novel\ntop-down Evidential Panoptic Segmentation Network (EvPSNet) to solve this task.\nOur architecture employs a simple yet effective probabilistic fusion module\nthat leverages the predicted uncertainties. Additionally, we propose a new\nLov\\'asz evidential loss function to optimize the IoU for the segmentation\nutilizing the probabilities provided by deep evidential learning. Furthermore,\nwe provide several strong baselines combining state-of-the-art panoptic\nsegmentation networks with sampling-free uncertainty estimation techniques.\nExtensive evaluations show that our EvPSNet achieves the new state-of-the-art\nfor the standard Panoptic Quality (PQ), as well as for our uncertainty-aware\npanoptic metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sirohi_K/0/1/0/all/0/1\">Kshitij Sirohi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marvi_S/0/1/0/all/0/1\">Sajad Marvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buscher_D/0/1/0/all/0/1\">Daniel B&#xfc;scher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgard_W/0/1/0/all/0/1\">Wolfram Burgard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Latent Replay for efficient Generative Rehearsal. (arXiv:2207.01562v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01562","description":"<p>We introduce a new method for internal replay that modulates the frequency of\nrehearsal based on the depth of the network. While replay strategies mitigate\nthe effects of catastrophic forgetting in neural networks, recent works on\ngenerative replay show that performing the rehearsal only on the deeper layers\nof the network improves the performance in continual learning. However, the\ngenerative approach introduces additional computational overhead, limiting its\napplications. Motivated by the observation that earlier layers of neural\nnetworks forget less abruptly, we propose to update network layers with varying\nfrequency using intermediate-level features during replay. This reduces the\ncomputational burden by omitting computations for both deeper layers of the\ngenerator and earlier layers of the main model. We name our method Progressive\nLatent Replay and show that it outperforms Internal Replay while using\nsignificantly fewer resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pawlak_S/0/1/0/all/0/1\">Stanis&#x142;aw Pawlak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szatkowski_F/0/1/0/all/0/1\">Filip Szatkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bortkiewicz_M/0/1/0/all/0/1\">Micha&#x142; Bortkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubinski_J/0/1/0/all/0/1\">Jan Dubi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Fine-Grained Sketch-Based Image Retrieval. (arXiv:2207.01723v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01723","description":"<p>The recent focus on Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) has\nshifted towards generalising a model to new categories without any training\ndata from them. In real-world applications, however, a trained FG-SBIR model is\noften applied to both new categories and different human sketchers, i.e.,\ndifferent drawing styles. Although this complicates the generalisation problem,\nfortunately, a handful of examples are typically available, enabling the model\nto adapt to the new category/style. In this paper, we offer a novel perspective\n-- instead of asking for a model that generalises, we advocate for one that\nquickly adapts, with just very few samples during testing (in a few-shot\nmanner). To solve this new problem, we introduce a novel model-agnostic\nmeta-learning (MAML) based framework with several key modifications: (1) As a\nretrieval task with a margin-based contrastive loss, we simplify the MAML\ntraining in the inner loop to make it more stable and tractable. (2) The margin\nin our contrastive loss is also meta-learned with the rest of the model. (3)\nThree additional regularisation losses are introduced in the outer loop, to\nmake the meta-learned FG-SBIR model more effective for category/style\nadaptation. Extensive experiments on public datasets suggest a large gain over\ngeneralisation and zero-shot based approaches, and a few strong few-shot\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhunia_A/0/1/0/all/0/1\">Ayan Kumar Bhunia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sain_A/0/1/0/all/0/1\">Aneeshan Sain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Parth Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Animesh Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1\">Pinaki Nath Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian approaches for Quantifying Clinicians' Variability in Medical Image Quantification. (arXiv:2207.01868v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2207.01868","description":"<p>Medical imaging, including MRI, CT, and Ultrasound, plays a vital role in\nclinical decisions. Accurate segmentation is essential to measure the structure\nof interest from the image. However, manual segmentation is highly\noperator-dependent, which leads to high inter and intra-variability of\nquantitative measurements. In this paper, we explore the feasibility that\nBayesian predictive distribution parameterized by deep neural networks can\ncapture the clinicians' inter-intra variability. By exploring and analyzing\nrecently emerged approximate inference schemes, we evaluate whether approximate\nBayesian deep learning with the posterior over segmentations can learn\ninter-intra rater variability both in segmentation and clinical measurements.\nThe experiments are performed with two different imaging modalities: MRI and\nultrasound. We empirically demonstrated that Bayesian predictive distribution\nparameterized by deep neural networks could approximate the clinicians'\ninter-intra variability. We show a new perspective in analyzing medical images\nquantitatively by providing clinical measurement uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeon_J/0/1/0/all/0/1\">Jaeik Jeon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1\">Yeonggul Jang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1\">Youngtaek Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shim_H/0/1/0/all/0/1\">Hackjoon Shim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sekeun Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latents2Segments: Disentangling the Latent Space of Generative Models for Semantic Segmentation of Face Images. (arXiv:2207.01871v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.01871","description":"<p>With the advent of an increasing number of Augmented and Virtual Reality\napplications that aim to perform meaningful and controlled style edits on\nimages of human faces, the impetus for the task of parsing face images to\nproduce accurate and fine-grained semantic segmentation maps is more than ever\nbefore. Few State of the Art (SOTA) methods which solve this problem, do so by\nincorporating priors with respect to facial structure or other face attributes\nsuch as expression and pose in their deep classifier architecture. Our\nendeavour in this work is to do away with the priors and complex pre-processing\noperations required by SOTA multi-class face segmentation models by reframing\nthis operation as a downstream task post infusion of disentanglement with\nrespect to facial semantic regions of interest (ROIs) in the latent space of a\nGenerative Autoencoder model. We present results for our model's performance on\nthe CelebAMask-HQ and HELEN datasets. The encoded latent space of our model\nachieves significantly higher disentanglement with respect to semantic ROIs\nthan that of other SOTA works. Moreover, it achieves a 13% faster inference\nrate and comparable accuracy with respect to the publicly available SOTA for\nthe downstream task of semantic segmentation of face images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomar_S/0/1/0/all/0/1\">Snehal Singh Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A.N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Robustness Analysis Against Language and Visual Perturbations. (arXiv:2207.02159v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2207.02159","description":"<p>Joint visual and language modeling on large-scale datasets has recently shown\na good progress in multi-modal tasks when compared to single modal learning.\nHowever, robustness of these approaches against real-world perturbations has\nnot been studied. In this work, we perform the first extensive robustness study\nof such models against various real-world perturbations focusing on video and\nlanguage. We focus on text-to-video retrieval and propose two large-scale\nbenchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual\nand 35 different textual perturbations. The study reveals some interesting\nfindings: 1) The studied models are more robust when text is perturbed versus\nwhen video is perturbed 2) The transformer text encoder is more robust on\nnon-semantic changing text perturbations and visual perturbations compared to\nword embedding approaches. 3) Using two-branch encoders in isolation is\ntypically more robust than when architectures use cross-attention. We hope this\nstudy will serve as a benchmark and guide future research in robust multimodal\nlearning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1\">Madeline C. Schiappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vyas_S/0/1/0/all/0/1\">Shruti Vyas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1\">Hamid Palangi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1\">Yogesh S. Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1\">Vibhav Vineet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-07-06T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}