{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.6","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-10-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Law Smells: Defining and Detecting Problematic Patterns in Legal Drafting. (arXiv:2110.11984v1 [cs.IR])","link":"http://arxiv.org/abs/2110.11984","description":"<p>Building on the computer science concept of code smells, we initiate the\nstudy of law smells, i.e., patterns in legal texts that pose threats to the\ncomprehensibility and maintainability of the law. With five intuitive law\nsmells as running examples - namely, duplicated phrase, long element, large\nreference tree, ambiguous syntax, and natural language obsession -, we develop\na comprehensive law smell taxonomy. This taxonomy classifies law smells by when\nthey can be detected, which aspects of law they relate to, and how they can be\ndiscovered. We introduce text-based and graph-based methods to identify\ninstances of law smells, confirming their utility in practice using the United\nStates Code as a test case. Our work demonstrates how ideas from software\nengineering can be leveraged to assess and improve the quality of legal code,\nthus drawing attention to an understudied area in the intersection of law and\ncomputer science and highlighting the potential of computational legal\ndrafting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coupette_C/0/1/0/all/0/1\">Corinna Coupette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartung_D/0/1/0/all/0/1\">Dirk Hartung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beckedorf_J/0/1/0/all/0/1\">Janis Beckedorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bother_M/0/1/0/all/0/1\">Maximilian B&#xf6;ther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_D/0/1/0/all/0/1\">Daniel Martin Katz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Embracing advanced AI/ML to help investors achieve success: Vanguard Reinforcement Learning for Financial Goal Planning. (arXiv:2110.12003v1 [q-fin.ST])","link":"http://arxiv.org/abs/2110.12003","description":"<p>In the world of advice and financial planning, there is seldom one right\nanswer. While traditional algorithms have been successful in solving linear\nproblems, its success often depends on choosing the right features from a\ndataset, which can be a challenge for nuanced financial planning scenarios.\nReinforcement learning is a machine learning approach that can be employed with\ncomplex data sets where picking the right features can be nearly impossible. In\nthis paper, we will explore the use of machine learning for financial\nforecasting, predicting economic indicators, and creating a savings strategy.\nVanguard ML algorithm for goals-based financial planning is based on deep\nreinforcement learning that identifies optimal savings rates across multiple\ngoals and sources of income to help clients achieve financial success. Vanguard\nlearning algorithms are trained to identify market indicators and behaviors too\ncomplex to capture with formulas and rules, instead, it works to model the\nfinancial success trajectory of investors and their investment outcomes as a\nMarkov decision process. We believe that reinforcement learning can be used to\ncreate value for advisors and end-investors, creating efficiency, more\npersonalized plans, and data to enable customized solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Mohammed_S/0/1/0/all/0/1\">Shareefuddin Mohammed</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Bealer_R/0/1/0/all/0/1\">Rusty Bealer</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Cohen_J/0/1/0/all/0/1\">Jason Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClimateBert: A Pretrained Language Model for Climate-Related Text. (arXiv:2110.12010v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12010","description":"<p>Over the recent years, large pretrained language models (LM) have\nrevolutionized the field of natural language processing (NLP). However, while\npretraining on general language has been shown to work very well for common\nlanguage, it has been observed that niche language poses problems. In\nparticular, climate-related texts include specific language that common LMs can\nnot represent accurately. We argue that this shortcoming of today's LMs limits\nthe applicability of modern NLP to the broad field of text processing of\nclimate-related texts. As a remedy, we propose ClimateBert, a transformer-based\nlanguage model that is further pretrained on over 1.6 million paragraphs of\nclimate-related texts, crawled from various sources such as common news,\nresearch articles, and climate reporting of companies. We find that\nClimateBertleads to a 46% improvement on a masked language model objective\nwhich, in turn, leads to lowering error rates by 3.57% to 35.71% for various\nclimate-related downstream tasks like text classification, sentiment analysis,\nand fact-checking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Webersinke_N/0/1/0/all/0/1\">Nicolas Webersinke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kraus_M/0/1/0/all/0/1\">Mathias Kraus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bingler_J/0/1/0/all/0/1\">Julia Anna Bingler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leippold_M/0/1/0/all/0/1\">Markus Leippold</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhoMT: A High-Quality and Large-Scale Benchmark Dataset for Vietnamese-English Machine Translation. (arXiv:2110.12199v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12199","description":"<p>We introduce a high-quality and large-scale Vietnamese-English parallel\ndataset of 3.02M sentence pairs, which is 2.9M pairs larger than the benchmark\nVietnamese-English machine translation corpus IWSLT15. We conduct experiments\ncomparing strong neural baselines and well-known automatic translation engines\non our dataset and find that in both automatic and human evaluations: the best\nperformance is obtained by fine-tuning the pre-trained sequence-to-sequence\ndenoising auto-encoder mBART. To our best knowledge, this is the first\nlarge-scale Vietnamese-English machine translation study. We hope our publicly\navailable dataset and study can serve as a starting point for future research\nand applications on Vietnamese-English machine translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doan_L/0/1/0/all/0/1\">Long Doan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Linh The Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nguyen Luong Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Thai Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate and Offensive Speech Detection in Hindi and Marathi. (arXiv:2110.12200v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12200","description":"<p>Sentiment analysis is the most basic NLP task to determine the polarity of\ntext data. There has been a significant amount of work in the area of\nmultilingual text as well. Still hate and offensive speech detection faces a\nchallenge due to inadequate availability of data, especially for Indian\nlanguages like Hindi and Marathi. In this work, we consider hate and offensive\nspeech detection in Hindi and Marathi texts. The problem is formulated as a\ntext classification task using the state of the art deep learning approaches.\nWe explore different deep learning architectures like CNN, LSTM, and variations\nof BERT like multilingual BERT, IndicBERT, and monolingual RoBERTa. The basic\nmodels based on CNN and LSTM are augmented with fast text word embeddings. We\nuse the HASOC 2021 Hindi and Marathi hate speech datasets to compare these\nalgorithms. The Marathi dataset consists of binary labels and the Hindi dataset\nconsists of binary as well as more-fine grained labels. We show that the\ntransformer-based models perform the best and even the basic models along with\nFastText embeddings give a competitive performance. Moreover, with normal\nhyper-parameter tuning, the basic models perform better than BERT-based models\non the fine-grained Hindi dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Velankar_A/0/1/0/all/0/1\">Abhishek Velankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_H/0/1/0/all/0/1\">Hrushikesh Patil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gore_A/0/1/0/all/0/1\">Amol Gore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salunke_S/0/1/0/all/0/1\">Shubham Salunke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Raviraj Joshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spanish Legalese Language Model and Corpora. (arXiv:2110.12201v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12201","description":"<p>There are many Language Models for the English language according to its\nworldwide relevance. However, for the Spanish language, even if it is a widely\nspoken language, there are very few Spanish Language Models which result to be\nsmall and too general. Legal slang could be think of a Spanish variant on its\nown as it is very complicated in vocabulary, semantics and phrase\nunderstanding. For this work we gathered legal-domain corpora from different\nsources, generated a model and evaluated against Spanish general domain tasks.\nThe model provides reasonable results in those tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PASTRIE: A Corpus of Prepositions Annotated with Supersense Tags in Reddit International English. (arXiv:2110.12243v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12243","description":"<p>We present the Prepositions Annotated with Supersense Tags in Reddit\nInternational English (\"PASTRIE\") corpus, a new dataset containing manually\nannotated preposition supersenses of English data from presumed speakers of\nfour L1s: English, French, German, and Spanish. The annotations are\ncomprehensive, covering all preposition types and tokens in the sample. Along\nwith the corpus, we provide analysis of distributional patterns across the\nincluded L1s and a discussion of the influence of L1s on L2 preposition choice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kranzlein_M/0/1/0/all/0/1\">Michael Kranzlein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manning_E/0/1/0/all/0/1\">Emma Manning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Siyao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wein_S/0/1/0/all/0/1\">Shira Wein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Aryaman Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salen_B/0/1/0/all/0/1\">Bradford Salen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoVA: Context-aware Visual Attention for Webpage Information Extraction. (arXiv:2110.12320v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12320","description":"<p>Webpage information extraction (WIE) is an important step to create knowledge\nbases. For this, classical WIE methods leverage the Document Object Model (DOM)\ntree of a website. However, use of the DOM tree poses significant challenges as\ncontext and appearance are encoded in an abstract manner. To address this\nchallenge we propose to reformulate WIE as a context-aware Webpage Object\nDetection task. Specifically, we develop a Context-aware Visual Attention-based\n(CoVA) detection pipeline which combines appearance features with syntactical\nstructure from the DOM tree. To study the approach we collect a new large-scale\ndataset of e-commerce websites for which we manually annotate every web element\nwith four labels: product price, product title, product image and background.\nOn this dataset we show that the proposed CoVA approach is a new challenging\nbaseline which improves upon prior state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurendra Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morabia_K/0/1/0/all/0/1\">Keval Morabia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Traditional Poetry Generating System Based on Deep Learning. (arXiv:2110.12335v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12335","description":"<p>Chinese traditional poetry is an important intangible cultural heritage of\nChina and an artistic carrier of thought, culture, spirit and emotion. However,\ndue to the strict rules of ancient poetry, it is very difficult to write poetry\nby machine. This paper proposes an automatic generation method of Chinese\ntraditional poetry based on deep learning technology, which extracts keywords\nfrom each poem and matches them with the previous text to make the poem conform\nto the theme, and when a user inputs a paragraph of text, the machine obtains\nthe theme and generates poem sentence by sentence. Using the classic word2vec\nmodel as the preprocessing model, the Chinese characters which are not\nunderstood by the computer are transformed into matrix for processing.\nBi-directional Long Short-Term Memory is used as the neural network model to\ngenerate Chinese characters one by one and make the meaning of Chinese\ncharacters as accurate as possible. At the same time, TF-IDF and TextRank are\nused to extract keywords. Using the attention mechanism based encoding-decoding\nmodel, we can solve practical problems by transforming the model, and\nstrengthen the important information of long-distance information, so as to\ngrasp the key points without losing important information. In the aspect of\nemotion judgment, Long Short-Term Memory network is used. The final result\nshows that it can get good poetry outputs according to the user input text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Chenlei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lican Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable knowledge base completion with superposition memories. (arXiv:2110.12341v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12341","description":"<p>We present Harmonic Memory Networks (HMem), a neural architecture for\nknowledge base completion that models entities as weighted sums of pairwise\nbindings between an entity's neighbors and corresponding relations. Since\nentities are modeled as aggregated neighborhoods, representations of unseen\nentities can be generated on the fly. We demonstrate this with two new\ndatasets: WNGen and FBGen. Experiments show that the model is SOTA on\nbenchmarks, and flexible enough to evolve without retraining as the knowledge\ngraph grows.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lalisse_M/0/1/0/all/0/1\">Matthias Lalisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosen_E/0/1/0/all/0/1\">Eric Rosen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolensky_P/0/1/0/all/0/1\">Paul Smolensky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributed neural encoding of binding to thematic roles. (arXiv:2110.12342v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12342","description":"<p>A framework and method are proposed for the study of constituent composition\nin fMRI. The method produces estimates of neural patterns encoding complex\nlinguistic structures, under the assumption that the contributions of\nindividual constituents are additive. Like usual techniques for modeling\ncompositional structure in fMRI, the proposed method employs pattern\nsuperposition to synthesize complex structures from their parts. Unlike these\ntechniques, superpositions are sensitive to the structural positions of\nconstituents, making them irreducible to structure-indiscriminate\n(\"bag-of-words\") models of composition. Reanalyzing data from a study by\nFrankland and Greene (2015), it is shown that comparison of neural predictive\nmodels with differing specifications can illuminate aspects of neural\nrepresentational contents that are not apparent when composition is not\nmodelled. The results indicate that the neural instantiations of the binding of\nfillers to thematic roles in a sentence are non-orthogonal, and therefore\nspatially overlapping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lalisse_M/0/1/0/all/0/1\">Matthias Lalisse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smolensky_P/0/1/0/all/0/1\">Paul Smolensky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Think about it! Improving defeasible reasoning by first modeling the question scenario. (arXiv:2110.12349v1 [cs.AI])","link":"http://arxiv.org/abs/2110.12349","description":"<p>Defeasible reasoning is the mode of reasoning where conclusions can be\noverturned by taking into account new evidence. Existing cognitive science\nliterature on defeasible reasoning suggests that a person forms a mental model\nof the problem scenario before answering questions. Our research goal asks\nwhether neural models can similarly benefit from envisioning the question\nscenario before answering a defeasible query. Our approach is, given a\nquestion, to have a model first create a graph of relevant influences, and then\nleverage that graph as an additional input when answering the question. Our\nsystem, CURIOUS, achieves a new state-of-the-art on three different defeasible\nreasoning datasets. This result is significant as it illustrates that\nperformance can be improved by guiding a system to \"think about\" a question and\nexplicitly model the scenario, rather than answering reflexively. Code, data,\nand pre-trained models are located at https://github.com/madaan/thinkaboutit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1\">Aman Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tandon_N/0/1/0/all/0/1\">Niket Tandon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yiming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Team Enigma at ArgMining-EMNLP 2021: Leveraging Pre-trained Language Models for Key Point Matching. (arXiv:2110.12370v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12370","description":"<p>We present the system description for our submission towards the Key Point\nAnalysis Shared Task at ArgMining 2021. Track 1 of the shared task requires\nparticipants to develop methods to predict the match score between each pair of\narguments and keypoints, provided they belong to the same topic under the same\nstance. We leveraged existing state of the art pre-trained language models\nalong with incorporating additional data and features extracted from the inputs\n(topics, key points, and arguments) to improve performance. We were able to\nachieve mAP strict and mAP relaxed score of 0.872 and 0.966 respectively in the\nevaluation phase, securing 5th place on the leaderboard. In the post evaluation\nphase, we achieved a mAP strict and mAP relaxed score of 0.921 and 0.982\nrespectively. All the codes to generate reproducible results on our models are\navailable on Github.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kapadnis_M/0/1/0/all/0/1\">Manav Nitin Kapadnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patnaik_S/0/1/0/all/0/1\">Sohan Patnaik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panigrahi_S/0/1/0/all/0/1\">Siba Smarak Panigrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_V/0/1/0/all/0/1\">Varun Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1\">Abhilash Nandy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transliterating Kurdish texts in Latin into Persian-Arabic script. (arXiv:2110.12374v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12374","description":"<p>Kurdish is written in different scripts. The two most popular scripts are\nLatin and Persian-Arabic. However, not all Kurdish readers are familiar with\nboth mentioned scripts that could be resolved by automatic transliterators. So\nfar, the developed tools mostly transliterate Persian-Arabic scripts into\nLatin. We present a transliterator to transliterate Kurdish texts in Latin into\nPersian-Arabic script. We also discuss the issues that should be considered in\nthe transliteration process. The tool is a part of Kurdish BLARK, and it is\npublicly available for non-commercial use\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hossein Hassani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Extraction of Sentencing Decisions from Court Cases in the Hebrew Language. (arXiv:2110.12383v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12383","description":"<p>We present the task of Automated Punishment Extraction (APE) in sentencing\ndecisions from criminal court cases in Hebrew. Addressing APE will enable the\nidentification of sentencing patterns and constitute an important stepping\nstone for many follow up legal NLP applications in Hebrew, including the\nprediction of sentencing decisions. We curate a dataset of sexual assault\nsentencing decisions and a manually-annotated evaluation dataset, and implement\nrule-based and supervised models. We find that while supervised models can\nidentify the sentence containing the punishment with good accuracy, rule-based\napproaches outperform them on the full APE task. We conclude by presenting a\nfirst analysis of sentencing patterns in our dataset and analyze common models'\nerrors, indicating avenues for future work, such as distinguishing between\nprobation and actual imprisonment punishment. We will make all our resources\navailable upon request, including data, annotation, and first benchmark models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_M/0/1/0/all/0/1\">Mohr Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalir_T/0/1/0/all/0/1\">Tom Kalir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berger_N/0/1/0/all/0/1\">Noga Berger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalamish_C/0/1/0/all/0/1\">Carmit Chalamish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keydar_R/0/1/0/all/0/1\">Renana Keydar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Goal Oriented Dialogue via Utterance Generation and Look Ahead. (arXiv:2110.12412v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12412","description":"<p>Goal oriented dialogue systems have become a prominent customer-care\ninteraction channel for most businesses. However, not all interactions are\nsmooth, and customer intent misunderstanding is a major cause of dialogue\nfailure. We show that intent prediction can be improved by training a deep\ntext-to-text neural model to generate successive user utterances from unlabeled\ndialogue data. For that, we define a multi-task training regime that utilizes\nsuccessive user-utterance generation to improve the intent prediction. Our\napproach achieves the reported improvement due to two complementary factors:\nFirst, it uses a large amount of unlabeled dialogue data for an auxiliary\ngeneration task. Second, it uses the generated user utterance as an additional\nsignal for the intent prediction model. Lastly, we present a novel look-ahead\napproach that uses user utterance generation to improve intent prediction in\ninference time. Specifically, we generate counterfactual successive user\nutterances for conversations with ambiguous predicted intents, and disambiguate\nthe prediction by reassessing the concatenated sequence of available and\ngenerated utterances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1\">Eyal Ben-David</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carmeli_B/0/1/0/all/0/1\">Boaz Carmeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anaby_Tavor_A/0/1/0/all/0/1\">Ateret Anaby-Tavor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Punctuation for Collaborative Commentary Generation in Esports Live-Streaming. (arXiv:2110.12416v1 [cs.CL])","link":"http://arxiv.org/abs/2110.12416","description":"<p>To solve the existing sentence punctuation problem for collaborative\ncommentary generation in Esports live-streaming, this paper presents two\nstrategies for sentence punctuation for text sequences of game commentary, that\nis, punctuating sentences by two or three text sequence(s) originally\npunctuated by Youtube to obtain a complete sentence of commentary. We conducted\ncomparative experiments utilizing and fine-tuning a state-of-the-art\npre-trained generative language model among two strategies and the baseline to\ngenerate collaborative commentary. Both objective evaluations by automatic\nmetrics and subjective analyses showed that our strategy of punctuating\nsentences by two text sequences outperformed the baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junjie H. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiaoling Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliyawan_P/0/1/0/all/0/1\">Pujana Paliyawan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Dialog Management: Recent Advances and Challenges. (arXiv:2005.02233v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.02233","description":"<p>Dialog management (DM) is a crucial component in a task-oriented dialog\nsystem. Given the dialog history, DM predicts the dialog state and decides the\nnext action that the dialog agent should take. Recently, dialog policy learning\nhas been widely formulated as a Reinforcement Learning (RL) problem, and more\nworks focus on the applicability of DM. In this paper, we survey recent\nadvances and challenges within three critical topics for DM: (1) improving\nmodel scalability to facilitate dialog system modeling in new scenarios, (2)\ndealing with the data scarcity problem for dialog policy learning, and (3)\nenhancing the training efficiency to achieve better task-completion performance\n. We believe that this survey can shed a light on future research in dialog\nmanagement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Huihua Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yixuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1\">Chengguang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Analyses of Social Biases in Wikipedia Bios. (arXiv:2101.00078v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00078","description":"<p>Social biases on Wikipedia, a widely-read global platform, could greatly\ninfluence public opinion. While prior research has examined man/woman gender\nbias in biography articles, possible influences of other demographic attributes\nlimit conclusions. In this work, we present a methodology for analyzing\nWikipedia pages about people that isolates dimensions of interest (e.g.,\ngender), from other attributes (e.g., occupation). Given a target corpus for\nanalysis (e.g. biographies about women), we present a method for constructing a\ncomparison corpus that matches the target corpus in as many attributes as\npossible, except the target one. We develop evaluation metrics to measure how\nwell the comparison corpus aligns with the target corpus and then examine how\narticles about gender and racial minorities (cis. women, non-binary people,\ntransgender women, and transgender men; African American, Asian American, and\nHispanic/Latinx American people) differ from other articles. In addition to\nidentifying suspect social biases, our results show that failing to control for\ncovariates can result in different conclusions and veil biases. Our\ncontributions include methodology that facilitates further analyses of bias in\nWikipedia articles, findings that can aid Wikipedia editors in reducing biases,\nand a framework and evaluation metrics to guide future work in this area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Field_A/0/1/0/all/0/1\">Anjalie Field</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chan Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1\">Kevin Z. Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VeeAlign: Multifaceted Context Representation using Dual Attention for Ontology Alignment. (arXiv:2102.04081v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04081","description":"<p>Ontology Alignment is an important research problem applied to various fields\nsuch as data integration, data transfer, data preparation, etc.\nState-of-the-art (SOTA) Ontology Alignment systems typically use naive\ndomain-dependent approaches with handcrafted rules or domain-specific\narchitectures, making them unscalable and inefficient. In this work, we propose\nVeeAlign, a Deep Learning based model that uses a novel dual-attention\nmechanism to compute the contextualized representation of a concept which, in\nturn, is used to discover alignments. By doing this, not only is our approach\nable to exploit both syntactic and semantic information encoded in ontologies,\nit is also, by design, flexible and scalable to different domains with minimal\neffort. We evaluate our model on four different datasets from different domains\nand languages, and establish its superiority through these results as well as\ndetailed ablation studies. The code and datasets used are available at\nhttps://github.com/Remorax/VeeAlign.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1\">Vivek Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Arvind Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_H/0/1/0/all/0/1\">Harshit Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XeroAlign: Zero-Shot Cross-lingual Transformer Alignment. (arXiv:2105.02472v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02472","description":"<p>The introduction of pretrained cross-lingual language models brought decisive\nimprovements to multilingual NLP tasks. However, the lack of labelled task data\nnecessitates a variety of methods aiming to close the gap to high-resource\nlanguages. Zero-shot methods in particular, often use translated task data as a\ntraining signal to bridge the performance gap between the source and target\nlanguage(s). We introduce XeroAlign, a simple method for task-specific\nalignment of cross-lingual pretrained transformers such as XLM-R. XeroAlign\nuses translated task data to encourage the model to generate similar sentence\nembeddings for different languages. The XeroAligned XLM-R, called XLM-RA, shows\nstrong improvements over the baseline models to achieve state-of-the-art\nzero-shot results on three multilingual natural language understanding tasks.\nXLM-RA's text classification accuracy exceeds that of XLM-R trained with\nlabelled data and performs on par with state-of-the-art models on a\ncross-lingual adversarial paraphrasing task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gritta_M/0/1/0/all/0/1\">Milan Gritta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Generative Augmentation for Visual Question Answering. (arXiv:2105.04780v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04780","description":"<p>Data augmentation has been shown to effectively improve the performance of\nmultimodal machine learning models. This paper introduces a generative model\nfor data augmentation by leveraging the correlations among multiple modalities.\nDifferent from conventional data augmentation approaches that apply low-level\noperations with deterministic heuristics, our method learns a generator that\ngenerates samples of the target modality conditioned on observed modalities in\nthe variational auto-encoder framework. Additionally, the proposed model is\nable to quantify the confidence of augmented data by its generative\nprobability, and can be jointly optimised with a downstream task. Experiments\non Visual Question Answering as downstream task demonstrate the effectiveness\nof the proposed generative model, which is able to improve strong UpDn-based\nmodels to achieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Yes, BM25 is a Strong Baseline for Legal Case Retrieval. (arXiv:2105.05686v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2105.05686","description":"<p>We describe our single submission to task 1 of COLIEE 2021. Our vanilla BM25\ngot second place, well above the median of submissions. Code is available at\nhttps://github.com/neuralmind-ai/coliee.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Guilherme Moraes Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_R/0/1/0/all/0/1\">Ruan Chaves Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A cost-benefit analysis of cross-lingual transfer methods. (arXiv:2105.06813v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.06813","description":"<p>An effective method for cross-lingual transfer is to fine-tune a bilingual or\nmultilingual model on a supervised dataset in one language and evaluating it on\nanother language in a zero-shot manner. Translating examples at training time\nor inference time are also viable alternatives. However, there are costs\nassociated with these methods that are rarely addressed in the literature. In\nthis work, we analyze cross-lingual methods in terms of their effectiveness\n(e.g., accuracy), development and deployment costs, as well as their latencies\nat inference time. Our experiments on three tasks indicate that the best\ncross-lingual method is highly task-dependent. Finally, by combining zero-shot\nand translation methods, we achieve the state-of-the-art in two of the three\ndatasets used in this work. Based on these results, we question the need for\nmanually labeled training data in a target language. Code, models and\ntranslated datasets are available at\nhttps://github.com/unicamp-dl/cross-lingual-analysis\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1\">Guilherme Moraes Rosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Souza_L/0/1/0/all/0/1\">Leandro Rodrigues de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Specializing Multilingual Language Models: An Empirical Study. (arXiv:2106.09063v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.09063","description":"<p>Pretrained multilingual language models have become a common tool in\ntransferring NLP capabilities to low-resource languages, often with\nadaptations. In this work, we study the performance, extensibility, and\ninteraction of two such adaptations: vocabulary augmentation and script\ntransliteration. Our evaluations on part-of-speech tagging, universal\ndependency parsing, and named entity recognition in nine diverse low-resource\nlanguages uphold the viability of these approaches while raising new questions\naround how to optimally adapt multilingual models to low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chau_E/0/1/0/all/0/1\">Ethan C. Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01076","description":"<p>Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n</p>\n<p>In this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EchoEA: Echo Information between Entities and Relations for Entity Alignment. (arXiv:2107.03054v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03054","description":"<p>Entity alignment (EA) plays an important role in automatically integrating\nknowledge graphs (KGs) from multiple sources. Recent approaches based on Graph\nNeural Network (GNN) obtain entity representation from relation information and\nhave achieved promising results. Besides, more and more methods introduce\nsemi-supervision to ask for more labeled training data. However, two challenges\nstill exist in GNN-based EA methods: (1) Deeper GNN Encoder: The GNN encoder of\ncurrent methods has limited depth (usually 2-layers). (2) Low-quality\nBootstrapping: The generated semi-supervised data is of low quality. In this\npaper, we propose a novel framework, Echo Entity Alignment (EchoEA), which\nleverages 4-levels self-attention mechanism to spread entity information to\nrelations and echo back to entities. Furthermore, we propose attribute-combined\nbi-directional global-filtered strategy (ABGS) to improve bootstrapping, reduce\nfalse samples and generate high-quality training data. The experimental results\non three real-world cross-lingual datasets are stable at around 96\\% at hits@1\non average, showing that our approach not only significantly outperforms the\nstate-of-the-art GNN-based methods, but also is universal and transferable for\nexisting EA methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xueyuan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E_H/0/1/0/all/0/1\">Haihong E</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1\">Wenyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haoran Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.06314","description":"<p>Time is an important dimension in our physical world. Lots of facts can\nevolve with respect to time. For example, the U.S. President might change every\nfour years. Therefore, it is important to consider the time dimension and\nempower the existing QA models to reason over time. However, the existing QA\ndatasets contain rather few time-sensitive questions, hence not suitable for\ndiagnosing or benchmarking the model's temporal reasoning capability. In order\nto promote research in this direction, we propose to construct a time-sensitive\nQA dataset. The dataset is constructed by 1) mining time-evolving facts from\nWikiData and aligning them to their corresponding Wikipedia page, 2) employing\ncrowd workers to verify and calibrate these noisy facts, 3) generating\nquestion-answer pairs based on the annotated time-sensitive facts. Our dataset\nposes challenges in the aspect of both temporal understanding and temporal\nreasoning. We evaluate different SoTA long-document QA systems like BigBird and\nFiD on our dataset. The best-performing model FiD can only achieve 46\\%\naccuracy, still far behind the human performance of 87\\%. We demonstrate that\nthese models are still lacking the ability to perform consistent temporal\nreasoning. Therefore, we believe that our dataset could serve as a benchmark to\ndevelop NLP models more sensitive to temporal shifts. The dataset and code are\nreleased in~\\url{https://github.com/wenhuchen/Time-Sensitive-QA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mMARCO: A Multilingual Version of the MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13897","description":"<p>The MS MARCO ranking dataset has been widely used for training deep learning\nmodels for IR tasks, achieving considerable effectiveness on diverse zero-shot\nscenarios. However, this type of resource is scarce in other languages than\nEnglish. In this work we present mMARCO, a multilingual version of the MS MARCO\npassage ranking dataset comprising 8 languages that was created using machine\ntranslation. We evaluated mMARCO by fine-tuning mono and multilingual\nre-ranking models on it. Experimental results demonstrate that multilingual\nmodels fine-tuned on our translated dataset achieve superior effectiveness than\nmodels fine-tuned on the original English version alone. Also, our distilled\nmultilingual re-ranker is competitive with non-distilled models while having\n5.4 times fewer parameters. The translated datasets as well as fine-tuned\nmodels are available at https://github.com/unicamp-dl/mMARCO.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campiotti_I/0/1/0/all/0/1\">Israel Campiotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeronymo_V/0/1/0/all/0/1\">Vitor Jeronymo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the ability of monolingual models to learn language-agnostic representations. (arXiv:2109.01942v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01942","description":"<p>Pretrained multilingual models have become a de facto default approach for\nzero-shot cross-lingual transfer. Previous work has shown that these models are\nable to achieve cross-lingual representations when pretrained on two or more\nlanguages with shared parameters. In this work, we provide evidence that a\nmodel can achieve language-agnostic representations even when pretrained on a\nsingle language. That is, we find that monolingual models pretrained and\nfinetuned on different languages achieve competitive performance compared to\nthe ones that use the same target language. Surprisingly, the models show a\nsimilar performance on a same task regardless of the pretraining language. For\nexample, models pretrained on distant languages such as German and Portuguese\nperform similarly on English tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_L/0/1/0/all/0/1\">Leandro Rodrigues de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing Length Divergence Bias of Textual Matching Models. (arXiv:2109.02431v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02431","description":"<p>Despite the remarkable success deep models have achieved in Textual Matching\n(TM), their robustness issue is still a topic of concern. In this work, we\npropose a new perspective to study this issue -- via the length divergence bias\nof TM models. We conclude that this bias stems from two parts: the label bias\nof existing TM datasets and the sensitivity of TM models to superficial\ninformation. We critically examine widely used TM datasets, and find that all\nof them follow specific length divergence distributions by labels, providing\ndirect cues for predictions. As for the TM models, we conduct adversarial\nevaluation and show that all models' performances drop on the\nout-of-distribution adversarial test sets we construct, which demonstrates that\nthey are all misled by biased training sets. This is also confirmed by the\n\\textit{SentLen} probing task that all models capture rich length information\nduring training to facilitate their performances. Finally, to alleviate the\nlength divergence bias in TM models, we propose a practical adversarial\ntraining method using bias-free training data. Our experiments indicate that we\nsuccessfully improve the robustness and generalization ability of models at the\nsame time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Lan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_T/0/1/0/all/0/1\">Tianshu Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1\">Chong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1\">Xiaoyong Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dawei Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04513","description":"<p>We present models which complete missing text given transliterations of\nancient Mesopotamian documents, originally written on cuneiform clay tablets\n(2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on\ncontextual cues to manually fill in missing parts in the text in a subjective\nand time-consuming process. We identify that this challenge can be formulated\nas a masked language modelling task, used mostly as a pretraining objective for\ncontextualized language models. Following, we develop several architectures\nfocusing on the Akkadian language, the lingua franca of the time. We find that\ndespite data scarcity (1M tokens) we can achieve state of the art performance\non missing tokens prediction (89% hit@5) using a greedy decoding scheme and\npretraining on data from other languages and different time periods. Finally,\nwe conduct human evaluations showing the applicability of our models in\nassisting experts to transcribe texts in extinct languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saret_B/0/1/0/all/0/1\">Benny Saret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1\">Asaf Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horowitz_W/0/1/0/all/0/1\">Wayne Horowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserman_N/0/1/0/all/0/1\">Nathan Wasserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05473","description":"<p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by\nlearning with merely a handful of annotated instances. Meta-learning has been\nwidely adopted for such a task, which trains on randomly generated few-shot\ntasks to learn generic data representations. Despite impressive results\nachieved, existing models still perform suboptimally when handling hard FSRE\ntasks, where the relations are fine-grained and similar to each other. We argue\nthis is largely because existing models do not distinguish hard tasks from easy\nones in the learning process. In this paper, we introduce a novel approach\nbased on contrastive learning that learns better representations by exploiting\nrelation label information. We further design a method that allows the model to\nadaptively learn how to focus on hard tasks. Experiments on two standard\ndatasets demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiale Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.08270","description":"<p>Language models (LMs) are sentence-completion engines trained on massive\ncorpora. LMs have emerged as a significant breakthrough in natural-language\nprocessing, providing capabilities that go far beyond sentence completion\nincluding question answering, summarization, and natural-language inference.\nWhile many of these capabilities have potential application to cognitive\nsystems, exploiting language models as a source of task knowledge, especially\nfor task learning, offers significant, near-term benefits. We introduce\nlanguage models and the various tasks to which they have been applied and then\nreview methods of knowledge extraction from language models. The resulting\nanalysis outlines both the challenges and opportunities for using language\nmodels as a new knowledge source for cognitive systems. It also identifies\npossible ways to improve knowledge extraction from language models using the\ncapabilities provided by cognitive systems. Central to success will be the\nability of a cognitive agent to itself learn an abstract model of the knowledge\nimplicit in the LM as well as methods to extract high-quality knowledge\neffectively and efficiently. To illustrate, we introduce a hypothetical robot\nagent and describe how language models could extend its task knowledge and\nimprove its performance and the kinds of knowledge and methods the agent can\nuse to exploit the knowledge within a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray, III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AraT5: Text-to-Text Transformers for Arabic Language Understanding and Generation. (arXiv:2109.12068v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12068","description":"<p>Transfer learning with a unified Transformer framework (T5) that converts all\nlanguage problems into a text-to-text format has recently been proposed as a\nsimple, yet effective, transfer learning approach. Although a multilingual\nversion of the T5 model (mT5) has been introduced, it is not clear how well it\ncan fare on non-English tasks involving diverse data. To investigate this\nquestion, we apply mT5 on a language with a wide variety of dialects--Arabic.\nFor evaluation, we use an existing benchmark for Arabic language understanding\nand introduce a new benchmark for Arabic language generation (ARGEN). We also\npre-train three powerful Arabic-specific text-to-text Transformer based models\nand evaluate them on the two benchmarks. Our new models perform significantly\nbetter than mT5 and exceed MARBERT, the current state-of-the-art Arabic\nBERT-based model, on Arabic language understanding. The models also set new\nSOTA on the generation benchmark. Our new models and are publicly released at\nhttps://github.com/UBC-NLP/araT5 and ARGEN will be released through the same\nrepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1\">El Moatez Billah Nagoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1\">AbdelRahim Elmadany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LawSum: A weakly supervised approach for Indian Legal Document Summarization. (arXiv:2110.01188v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.01188","description":"<p>Unlike the courts in western countries, public records of Indian judiciary\nare completely unstructured and noisy. No large scale publicly available\nannotated datasets of Indian legal documents exist till date. This limits the\nscope for legal analytics research. In this work, we propose a new dataset\nconsisting of over 10,000 judgements delivered by the supreme court of India\nand their corresponding hand written summaries. The proposed dataset is\npre-processed by normalising common legal abbreviations, handling spelling\nvariations in named entities, handling bad punctuations and accurate sentence\ntokenization. Each sentence is tagged with their rhetorical roles. We also\nannotate each judgement with several attributes like date, names of the\nplaintiffs, defendants and the people representing them, judges who delivered\nthe judgement, acts/statutes that are cited and the most common citations used\nto refer the judgement. Further, we propose an automatic labelling technique\nfor identifying sentences which have summary worthy information. We demonstrate\nthat this auto labeled data can be used effectively to train a weakly\nsupervised sentence extractor with high accuracy. Some possible applications of\nthis dataset besides legal document summarization can be in retrieval, citation\nanalysis and prediction of decisions by a particular judge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parikh_V/0/1/0/all/0/1\">Vedant Parikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_V/0/1/0/all/0/1\">Vidit Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_P/0/1/0/all/0/1\">Parth Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_N/0/1/0/all/0/1\">Namita Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_P/0/1/0/all/0/1\">Prasenjit Majumder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion Parameter Pretraining. (arXiv:2110.03888v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03888","description":"<p>Recent expeditious developments in deep learning algorithms, distributed\ntraining, and even hardware design for large models have enabled training\nextreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of\nbillions or even trillions of parameters. However, under limited resources,\nextreme-scale model training that requires enormous amounts of computes and\nmemory footprint suffers from frustratingly low efficiency in model\nconvergence. In this paper, we propose a simple training strategy called\n\"Pseudo-to-Real\" for high-memory-footprint-required large models.\nPseudo-to-Real is compatible with large models with architecture of sequential\nlayers. We demonstrate a practice of pretraining unprecedented\n10-trillion-parameter model, an order of magnitude larger than the\nstate-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the\napplication of Pseudo-to-Real, we also provide a technique, Granular CPU\noffloading, to manage CPU memory for training large model and maintain high GPU\nutilities. Fast training of extreme-scale models on a decent amount of\nresources can bring much smaller carbon footprint and contribute to greener AI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junyang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1\">An Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Jinze Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Le Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xianyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1\">Ang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Inductive Bias of In-Context Learning: Rethinking Pretraining Example Design. (arXiv:2110.04541v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04541","description":"<p>Pretraining Neural Language Models (NLMs) over a large corpus involves\nchunking the text into training examples, which are contiguous text segments of\nsizes processable by the neural architecture. We highlight a bias introduced by\nthis common practice: we prove that the pretrained NLM can model much stronger\ndependencies between text segments that appeared in the same training example,\nthan it can between text segments that appeared in different training examples.\nThis intuitive result has a twofold role. First, it formalizes the motivation\nbehind a broad line of recent successful NLM training heuristics, proposed for\nthe pretraining and fine-tuning stages, which do not necessarily appear related\nat first glance. Second, our result clearly indicates further improvements to\nbe made in NLM pretraining for the benefit of Natural Language Understanding\ntasks. As an example, we propose \"kNN-Pretraining\": we show that including\nsemantically related non-neighboring sentences in the same pretraining example\nyields improved sentence representations and open domain question answering\nabilities. This theoretically motivated degree of freedom for \"pretraining\nexample design\" indicates new training schemes for self-improving\nrepresentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levine_Y/0/1/0/all/0/1\">Yoav Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1\">Noam Wies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannai_D/0/1/0/all/0/1\">Daniel Jannai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_D/0/1/0/all/0/1\">Dan Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Masking for Temporal Language Models. (arXiv:2110.06366v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06366","description":"<p>Our world is constantly evolving, and so is the content on the web.\nConsequently, our languages, often said to mirror the world, are dynamic in\nnature. However, most current contextual language models are static and cannot\nadapt to changes over time. In this work, we propose a temporal contextual\nlanguage model called TempoBERT, which uses time as an additional context of\ntexts. Our technique is based on modifying texts with temporal information and\nperforming time masking - specific masking for the supplementary time\ninformation. We leverage our approach for the tasks of semantic change\ndetection and sentence time prediction, experimenting on diverse datasets in\nterms of time, size, genre, and language. Our extensive evaluation shows that\nboth tasks benefit from exploiting time masking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosin_G/0/1/0/all/0/1\">Guy D. Rosin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guy_I/0/1/0/all/0/1\">Ido Guy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radinsky_K/0/1/0/all/0/1\">Kira Radinsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-tuning in ASR systems for efficient domain-adaptation. (arXiv:2110.06502v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06502","description":"<p>Automatic Speech Recognition (ASR) systems have found their use in numerous\nindustrial applications in very diverse domains. Since domain-specific systems\nperform better than their generic counterparts on in-domain evaluation, the\nneed for memory and compute-efficient domain adaptation is obvious.\nParticularly, adapting parameter-heavy transformer-based language models used\nfor rescoring ASR hypothesis is challenging. In this work, we overcome the\nproblem using prompt-tuning, a methodology that trains a small number of domain\ntoken embedding parameters to prime a transformer-based LM to a particular\ndomain. With just a handful of extra parameters per domain, we achieve much\nbetter perplexity scores over the baseline of using an unadapted LM. Despite\nbeing parameter-efficient, these improvements are comparable to those of\nfully-fine-tuned models with hundreds of millions of parameters. We replicate\nour findings in perplexity numbers to Word Error Rate in a domain-specific ASR\nsystem for one such domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dingliwal_S/0/1/0/all/0/1\">Saket Dingliwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1\">Ashish Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1\">Sravan Bodapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1\">Ravi Teja Gadde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1\">Katrin Kirchhoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Transfer Learning & Beyond: Transformer Language Models in Information Systems Research. (arXiv:2110.08975v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.08975","description":"<p>AI is widely thought to be poised to transform business, yet current\nperceptions of the scope of this transformation may be myopic. Recent progress\nin natural language processing involving transformer language models (TLMs)\noffers a potential avenue for AI-driven business and societal transformation\nthat is beyond the scope of what most currently foresee. We review this recent\nprogress as well as recent literature utilizing text mining in top IS journals\nto develop an outline for how future IS research can benefit from these new\ntechniques. Our review of existing IS literature reveals that suboptimal text\nmining techniques are prevalent and that the more advanced TLMs could be\napplied to enhance and increase IS research involving text data, and to enable\nnew IS research topics, thus creating more value for the research community.\nThis is possible because these techniques make it easier to develop very\npowerful custom systems and their performance is superior to existing methods\nfor a wide range of tasks and applications. Further, multilingual language\nmodels make possible higher quality text analytics for research in multiple\nlanguages. We also identify new avenues for IS research, like language user\ninterfaces, that may offer even greater potential for future IS research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gruetzemacher_R/0/1/0/all/0/1\">Ross Gruetzemacher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paradice_D/0/1/0/all/0/1\">David Paradice</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Aspect-guided Explanation Generation for Explainable Recommendation. (arXiv:2110.10358v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.10358","description":"<p>Explainable recommendation systems provide explanations for recommendation\nresults to improve their transparency and persuasiveness. The existing\nexplainable recommendation methods generate textual explanations without\nexplicitly considering the user's preferences on different aspects of the item.\nIn this paper, we propose a novel explanation generation framework, named\nHierarchical Aspect-guided explanation Generation (HAG), for explainable\nrecommendation. Specifically, HAG employs a review-based syntax graph to\nprovide a unified view of the user/item details. An aspect-guided graph pooling\noperator is proposed to extract the aspect-relevant information from the\nreview-based syntax graphs to model the user's preferences on an item at the\naspect level. Then, a hierarchical explanation decoder is developed to generate\naspects and aspect-relevant explanations based on the attention mechanism. The\nexperimental results on three real datasets indicate that HAG outperforms\nstate-of-the-art explanation generation methods in both single-aspect and\nmulti-aspect explanation generation tasks, and also achieves comparable or even\nbetter preference prediction accuracy than strong baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yidan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Gongqi Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yuan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting Deep Learning Models in Natural Language Processing: A Review. (arXiv:2110.10470v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.10470","description":"<p>Neural network models have achieved state-of-the-art performances in a wide\nrange of natural language processing (NLP) tasks. However, a long-standing\ncriticism against neural network models is the lack of interpretability, which\nnot only reduces the reliability of neural NLP systems but also limits the\nscope of their applications in areas where interpretability is essential (e.g.,\nhealth care applications). In response, the increasing interest in interpreting\nneural NLP models has spurred a diverse array of interpretation methods over\nrecent years. In this survey, we provide a comprehensive review of various\ninterpretation methods for neural models in NLP. We first stretch out a\nhigh-level taxonomy for interpretation methods in NLP, i.e., training-based\napproaches, test-based approaches, and hybrid approaches. Next, we describe\nsub-categories in each category in detail, e.g., influence-function based\nmethods, KNN-based methods, attention-based models, saliency-based methods,\nperturbation-based methods, etc. We point out deficiencies of current methods\nand suggest some avenues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Han Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciCap: Generating Captions for Scientific Figures. (arXiv:2110.11624v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.11624","description":"<p>Researchers use figures to communicate rich, complex information in\nscientific papers. The captions of these figures are critical to conveying\neffective messages. However, low-quality figure captions commonly occur in\nscientific articles and may decrease understanding. In this paper, we propose\nan end-to-end neural framework to automatically generate informative,\nhigh-quality captions for scientific figures. To this end, we introduce SCICAP,\na large-scale figure-caption dataset based on computer science arXiv papers\npublished between 2010 and 2020. After pre-processing - including figure-type\nclassification, sub-figure identification, text normalization, and caption text\nselection - SCICAP contained more than two million figures extracted from over\n290,000 papers. We then established baseline models that caption graph plots,\nthe dominant (19.2%) figure type. The experimental results showed both\nopportunities and steep challenges of generating captions for scientific\nfigures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Ting-Yao Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1\">C. Lee Giles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"PhotoWCT$^2$: Compact Autoencoder for Photorealistic Style Transfer Resulting from Blockwise Training and Skip Connections of High-Frequency Residuals. (arXiv:2110.11995v1 [eess.IV])","link":"http://arxiv.org/abs/2110.11995","description":"<p>Photorealistic style transfer is an image editing task with the goal to\nmodify an image to match the style of another image while ensuring the result\nlooks like a real photograph. A limitation of existing models is that they have\nmany parameters, which in turn prevents their use for larger image resolutions\nand leads to slower run-times. We introduce two mechanisms that enable our\ndesign of a more compact model that we call PhotoWCT$^2$, which preserves\nstate-of-art stylization strength and photorealism. First, we introduce\nblockwise training to perform coarse-to-fine feature transformations that\nenable state-of-art stylization strength in a single autoencoder in place of\nthe inefficient cascade of four autoencoders used in PhotoWCT. Second, we\nintroduce skip connections of high-frequency residuals in order to preserve\nimage quality when applying the sequential coarse-to-fine feature\ntransformations. Our PhotoWCT$^2$ model requires fewer parameters (e.g., 30.3\\%\nfewer) while supporting higher resolution images (e.g., 4K) and achieving\nfaster stylization than existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chiu_T/0/1/0/all/0/1\">Tai-Yin Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Semantic Segmentation of Vessel Images using Leaking Perturbations. (arXiv:2110.11998v1 [eess.IV])","link":"http://arxiv.org/abs/2110.11998","description":"<p>Semantic segmentation based on deep learning methods can attain appealing\naccuracy provided large amounts of annotated samples. However, it remains a\nchallenging task when only limited labelled data are available, which is\nespecially common in medical imaging. In this paper, we propose to use Leaking\nGAN, a GAN-based semi-supervised architecture for retina vessel semantic\nsegmentation. Our key idea is to pollute the discriminator by leaking\ninformation from the generator. This leads to more moderate generations that\nbenefit the training of GAN. As a result, the unlabelled examples can be better\nutilized to boost the learning of the discriminator, which eventually leads to\nstronger classification performance. In addition, to overcome the variations in\nmedical images, the mean-teacher mechanism is utilized as an auxiliary\nregularization of the discriminator. Further, we modify the focal loss to fit\nit as the consistency objective for mean-teacher regularizer. Extensive\nexperiments demonstrate that the Leaking GAN framework achieves competitive\nperformance compared to the state-of-the-art methods when evaluated on\nbenchmark datasets including DRIVE, STARE and CHASE\\_DB1, using as few as 8\nlabelled images in the semi-supervised setting. It also outperforms existing\nalgorithms on cross-domain segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1\">Jinyong Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ding_X/0/1/0/all/0/1\">Xuejie Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_J/0/1/0/all/0/1\">Jeremiah D. Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When to Prune? A Policy towards Early Structural Pruning. (arXiv:2110.12007v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12007","description":"<p>Pruning enables appealing reductions in network memory footprint and time\ncomplexity. Conventional post-training pruning techniques lean towards\nefficient inference while overlooking the heavy computation for training.\nRecent exploration of pre-training pruning at initialization hints on training\ncost reduction via pruning, but suffers noticeable performance degradation. We\nattempt to combine the benefits of both directions and propose a policy that\nprunes as early as possible during training without hurting performance.\nInstead of pruning at initialization, our method exploits initial dense\ntraining for few epochs to quickly guide the architecture, while constantly\nevaluating dominant sub-networks via neuron importance ranking. This unveils\ndominant sub-networks whose structures turn stable, allowing conventional\npruning to be pushed earlier into the training. To do this early, we further\nintroduce an Early Pruning Indicator (EPI) that relies on sub-network\narchitectural similarity and quickly triggers pruning when the sub-network's\narchitecture stabilizes. Through extensive experiments on ImageNet, we show\nthat EPI empowers a quick tracking of early training epochs suitable for\npruning, offering same efficacy as an otherwise ``oracle'' grid-search that\nscans through epochs and requires orders of magnitude more compute. Our method\nyields $1.4\\%$ top-1 accuracy boost over state-of-the-art pruning counterparts,\ncuts down training cost on GPU by $2.4\\times$, hence offers a new\nefficiency-accuracy boundary for network pruning during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_M/0/1/0/all/0/1\">Maying Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molchanov_P/0/1/0/all/0/1\">Pavlo Molchanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongxu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local-Global Associative Frame Assemble in Video Re-ID. (arXiv:2110.12018v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12018","description":"<p>Noisy and unrepresentative frames in automatically generated object bounding\nboxes from video sequences cause significant challenges in learning\ndiscriminative representations in video re-identification (Re-ID). Most\nexisting methods tackle this problem by assessing the importance of video\nframes according to either their local part alignments or global appearance\ncorrelations separately. However, given the diverse and unknown sources of\nnoise which usually co-exist in captured video data, existing methods have not\nbeen effective satisfactorily. In this work, we explore jointly both local\nalignments and global correlations with further consideration of their mutual\npromotion/reinforcement so to better assemble complementary discriminative\nRe-ID information within all the relevant frames in video tracklets.\nSpecifically, we concurrently optimise a local aligned quality (LAQ) module\nthat distinguishes the quality of each frame based on local alignments, and a\nglobal correlated quality (GCQ) module that estimates global appearance\ncorrelations. With the help of a local-assembled global appearance prototype,\nwe associate LAQ and GCQ to exploit their mutual complement. Extensive\nexperiments demonstrate the superiority of the proposed model against\nstate-of-the-art methods on five Re-ID benchmarks, including MARS, Duke-Video,\nDuke-SI, iLIDS-VID, and PRID2011.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qilei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiabo Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1\">Shaogang Gong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Prototype-Oriented Framework for Unsupervised Domain Adaptation. (arXiv:2110.12024v1 [cs.LG])","link":"http://arxiv.org/abs/2110.12024","description":"<p>Existing methods for unsupervised domain adaptation often rely on minimizing\nsome statistical distance between the source and target samples in the latent\nspace. To avoid the sampling variability, class imbalance, and data-privacy\nconcerns that often plague these methods, we instead provide a memory and\ncomputation-efficient probabilistic framework to extract class prototypes and\nalign the target features with them. We demonstrate the general applicability\nof our method on a wide range of scenarios, including single-source,\nmulti-source, class-imbalance, and source-private domain adaptation. Requiring\nno additional model parameters and having a moderate increase in computation\nover the source model alone, the proposed method achieves competitive\nperformance with state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanwisuth_K/0/1/0/all/0/1\">Korawat Tanwisuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xinjie Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Baseline for Low-Budget Active Learning. (arXiv:2110.12033v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12033","description":"<p>Active learning focuses on choosing a subset of unlabeled data to be labeled.\nHowever, most such methods assume that a large subset of the data can be\nannotated. We are interested in low-budget active learning where only a small\nsubset (e.g., 0.2% of ImageNet) can be annotated. Instead of proposing a new\nquery strategy to iteratively sample batches of unlabeled data given an initial\npool, we learn rich features by an off-the-shelf self-supervised learning\nmethod only once and then study the effectiveness of different sampling\nstrategies given a low budget on a variety of datasets as well as ImageNet\ndataset. We show that although the state-of-the-art active learning methods\nwork well given a large budget of data labeling, a simple k-means clustering\nalgorithm can outperform them on low budgets. We believe this method can be\nused as a simple baseline for low-budget active learning on image\nclassification. Code is available at:\nhttps://github.com/UCDvision/low-budget-al\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourahmadi_K/0/1/0/all/0/1\">Kossar Pourahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nooralinejad_P/0/1/0/all/0/1\">Parsa Nooralinejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pirsiavash_H/0/1/0/all/0/1\">Hamed Pirsiavash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Networks for Non-Raytraced Global Illumination on Older GPU Hardware. (arXiv:2110.12039v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12039","description":"<p>We give an overview of the different rendering methods and we demonstrate\nthat the use of a Generative Adversarial Networks (GAN) for Global Illumination\n(GI) gives a superior quality rendered image to that of a rasterisations image.\nWe utilise the Pix2Pix architecture and specify the hyper-parameters and\nmethodology used to mimic ray-traced images from a set of input features. We\nalso demonstrate that the GANs quality is comparable to the quality of the\nray-traced images, but is able to produce the image, at a fraction of the time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Harris_Dewey_J/0/1/0/all/0/1\">Jared Harris-Dewey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_R/0/1/0/all/0/1\">Richard Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of Semantic Web-based Imaging Database for Biological Morphome. (arXiv:2110.12058v1 [q-bio.QM])","link":"http://arxiv.org/abs/2110.12058","description":"<p>We introduce the RIKEN Microstructural Imaging Metadatabase, a semantic\nweb-based imaging database in which image metadata are described using the\nResource Description Framework (RDF) and detailed biological properties\nobserved in the images can be represented as Linked Open Data. The metadata are\nused to develop a large-scale imaging viewer that provides a straightforward\ngraphical user interface to visualise a large microstructural tiling image at\nthe gigabyte level. We applied the database to accumulate comprehensive\nmicrostructural imaging data produced by automated scanning electron\nmicroscopy. As a result, we have successfully managed vast numbers of images\nand their metadata, including the interpretation of morphological phenotypes\noccurring in sub-cellular components and biosamples captured in the images. We\nalso discuss advanced utilisation of morphological imaging data that can be\npromoted by this database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Kume_S/0/1/0/all/0/1\">Satoshi Kume</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Masuya_H/0/1/0/all/0/1\">Hiroshi Masuya</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Maeda_M/0/1/0/all/0/1\">Mitsuyo Maeda</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Suga_M/0/1/0/all/0/1\">Mitsuo Suga</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kataoka_Y/0/1/0/all/0/1\">Yosky Kataoka</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kobayashi_N/0/1/0/all/0/1\">Norio Kobayashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CD&S Dataset: Handheld Imagery Dataset Acquired Under Field Conditions for Corn Disease Identification and Severity Estimation. (arXiv:2110.12084v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12084","description":"<p>Accurate disease identification and its severity estimation is an important\nconsideration for disease management. Deep learning-based solutions for disease\nmanagement using imagery datasets are being increasingly explored by the\nresearch community. However, most reported studies have relied on imagery\ndatasets that were acquired under controlled lab conditions. As a result, such\nmodels lacked the ability to identify diseases in the field. Therefore, to\ntrain a robust deep learning model for field use, an imagery dataset was\ncreated using raw images acquired under field conditions using a handheld\nsensor and augmented images with varying backgrounds. The Corn Disease and\nSeverity (CD&amp;S) dataset consisted of 511, 524, and 562, field acquired raw\nimages, corresponding to three common foliar corn diseases, namely Northern\nLeaf Blight (NLB), Gray Leaf Spot (GLS), and Northern Leaf Spot (NLS),\nrespectively. For training disease identification models, half of the imagery\ndata for each disease was annotated using bounding boxes and also used to\ngenerate 2343 additional images through augmentation using three different\nbackgrounds. For severity estimation, an additional 515 raw images for NLS were\nacquired and categorized into severity classes ranging from 1 (resistant) to 5\n(susceptible). Overall, the CD&amp;S dataset consisted of 4455 total images\ncomprising of 2112 field images and 2343 augmented images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1\">Aanis Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saraswat_D/0/1/0/all/0/1\">Dharmendra Saraswat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gamal_A/0/1/0/all/0/1\">Aly El Gamal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johal_G/0/1/0/all/0/1\">Gurmukh Johal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Circle Representation for Medical Object Detection. (arXiv:2110.12093v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12093","description":"<p>Box representation has been extensively used for object detection in computer\nvision. Such representation is efficacious but not necessarily optimized for\nbiomedical objects (e.g., glomeruli), which play an essential role in renal\npathology. In this paper, we propose a simple circle representation for medical\nobject detection and introduce CircleNet, an anchor-free detection framework.\nCompared with the conventional bounding box representation, the proposed\nbounding circle representation innovates in three-fold: (1) it is optimized for\nball-shaped biomedical objects; (2) The circle representation reduced the\ndegree of freedom compared with box representation; (3) It is naturally more\nrotation invariant. When detecting glomeruli and nuclei on pathological images,\nthe proposed circle representation achieved superior detection performance and\nbe more rotation-invariant, compared with the bounding box. The code has been\nmade publicly available: https://github.com/hrlblab/CircleNet\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1\">Ethan H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuzhe Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roland_J/0/1/0/all/0/1\">Joseph T. Roland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Le Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1\">Bennett A. Landman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fogo_A/0/1/0/all/0/1\">Agnes B. Fogo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MTGLS: Multi-Task Gaze Estimation with Limited Supervision. (arXiv:2110.12100v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12100","description":"<p>Robust gaze estimation is a challenging task, even for deep CNNs, due to the\nnon-availability of large-scale labeled data. Moreover, gaze annotation is a\ntime-consuming process and requires specialized hardware setups. We propose\nMTGLS: a Multi-Task Gaze estimation framework with Limited Supervision, which\nleverages abundantly available non-annotated facial image data. MTGLS distills\nknowledge from off-the-shelf facial image analysis models, and learns strong\nfeature representations of human eyes, guided by three complementary auxiliary\nsignals: (a) the line of sight of the pupil (i.e. pseudo-gaze) defined by the\nlocalized facial landmarks, (b) the head-pose given by Euler angles, and (c)\nthe orientation of the eye patch (left/right eye). To overcome inherent noise\nin the supervisory signals, MTGLS further incorporates a noise distribution\nmodelling approach. Our experimental results show that MTGLS learns highly\ngeneralized representations which consistently perform well on a range of\ndatasets. Our proposed framework outperforms the unsupervised state-of-the-art\non CAVE (by 6.43%) and even supervised state-of-the-art methods on Gaze360 (by\n6.59%) datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shreya Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayat_M/0/1/0/all/0/1\">Munawar Hayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhall_A/0/1/0/all/0/1\">Abhinav Dhall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knibbe_J/0/1/0/all/0/1\">Jarrod Knibbe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConformalLayers: A non-linear sequential neural network with associative layers. (arXiv:2110.12108v1 [cs.LG])","link":"http://arxiv.org/abs/2110.12108","description":"<p>Convolutional Neural Networks (CNNs) have been widely applied. But as the\nCNNs grow, the number of arithmetic operations and memory footprint also\nincrease. Furthermore, typical non-linear activation functions do not allow\nassociativity of the operations encoded by consecutive layers, preventing the\nsimplification of intermediate steps by combining them. We present a new\nactivation function that allows associativity between sequential layers of\nCNNs. Even though our activation function is non-linear, it can be represented\nby a sequence of linear operations in the conformal model for Euclidean\ngeometry. In this domain, operations like, but not limited to, convolution,\naverage pooling, and dropout remain linear. We take advantage of associativity\nto combine all the \"conformal layers\" and make the cost of inference constant\nregardless of the depth of the network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sousa_E/0/1/0/all/0/1\">Eduardo Vera Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_L/0/1/0/all/0/1\">Leandro A. F. Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1\">Cristina Nader Vasconcelos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense Dual-Attention Network for Light Field Image Super-Resolution. (arXiv:2110.12114v1 [eess.IV])","link":"http://arxiv.org/abs/2110.12114","description":"<p>Light field (LF) images can be used to improve the performance of image\nsuper-resolution (SR) because both angular and spatial information is\navailable. It is challenging to incorporate distinctive information from\ndifferent views for LF image SR. Moreover, the long-term information from the\nprevious layers can be weakened as the depth of network increases. In this\npaper, we propose a dense dual-attention network for LF image SR. Specifically,\nwe design a view attention module to adaptively capture discriminative features\nacross different views and a channel attention module to selectively focus on\ninformative information across all channels. These two modules are fed to two\nbranches and stacked separately in a chain structure for adaptive fusion of\nhierarchical features and distillation of valid information. Meanwhile, a dense\nconnection is used to fully exploit multi-level information. Extensive\nexperiments demonstrate that our dense dual-attention mechanism can capture\ninformative information across views and channels to improve SR performance.\nComparative results show the advantage of our method over state-of-the-art\nmethods on public datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mo_Y/0/1/0/all/0/1\">Yu Mo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yingqian Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_C/0/1/0/all/0/1\">Chao Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jungang Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+An_W/0/1/0/all/0/1\">Wei An</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RCNet: Reverse Feature Pyramid and Cross-scale Shift Network for Object Detection. (arXiv:2110.12130v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12130","description":"<p>Feature pyramid networks (FPN) are widely exploited for multi-scale feature\nfusion in existing advanced object detection frameworks. Numerous previous\nworks have developed various structures for bidirectional feature fusion, all\nof which are shown to improve the detection performance effectively. We observe\nthat these complicated network structures require feature pyramids to be\nstacked in a fixed order, which introduces longer pipelines and reduces the\ninference speed. Moreover, semantics from non-adjacent levels are diluted in\nthe feature pyramid since only features at adjacent pyramid levels are merged\nby the local fusion operation in a sequence manner. To address these issues, we\npropose a novel architecture named RCNet, which consists of Reverse Feature\nPyramid (RevFP) and Cross-scale Shift Network (CSN). RevFP utilizes local\nbidirectional feature fusion to simplify the bidirectional pyramid inference\npipeline. CSN directly propagates representations to both adjacent and\nnon-adjacent levels to enable multi-scale features more correlative. Extensive\nexperiments on the MS COCO dataset demonstrate RCNet can consistently bring\nsignificant improvements over both one-stage and two-stage detectors with\nsubtle extra computational overhead. In particular, RetinaNet is boosted to\n40.2 AP, which is 3.7 points higher than baseline, by replacing FPN with our\nproposed model. On COCO test-dev, RCNet can achieve very competitive\nperformance with a single-model single-scale 50.5 AP. Codes will be made\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1\">Zhuofan Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1\">Qianggang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_B/0/1/0/all/0/1\">Biao Leng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Study of Multimodal Person Verification Using Audio-Visual-Thermal Data. (arXiv:2110.12136v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12136","description":"<p>In this paper, we study an approach to multimodal person verification using\naudio, visual, and thermal modalities. The combination of audio and visual\nmodalities has already been shown to be effective for robust person\nverification. From this perspective, we investigate the impact of further\nincreasing the number of modalities by supplementing thermal images. In\nparticular, we implemented unimodal, bimodal, and trimodal verification systems\nusing the state-of-the-art deep learning architectures and compared their\nperformance under clean and noisy conditions. We also compared two popular\nfusion approaches based on simple score averaging and soft attention mechanism.\nThe experiment conducted on the SpeakingFaces dataset demonstrates the\nsuperiority of the trimodal verification system over both unimodal and bimodal\nsystems. To enable the reproducibility of the experiment and facilitate\nresearch into multimodal person verification, we make our code, pretrained\nmodels and preprocessed dataset freely available in our GitHub repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdrakhmanova_M/0/1/0/all/0/1\">Madina Abdrakhmanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abushakimova_S/0/1/0/all/0/1\">Saniya Abushakimova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khassanov_Y/0/1/0/all/0/1\">Yerbolat Khassanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_H/0/1/0/all/0/1\">Huseyin Atakan Varol</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatio-Temporal Graph Complementary Scattering Networks. (arXiv:2110.12150v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12150","description":"<p>Spatio-temporal graph signal analysis has a significant impact on a wide\nrange of applications, including hand/body pose action recognition. To achieve\neffective analysis, spatio-temporal graph convolutional networks (ST-GCN)\nleverage the powerful learning ability to achieve great empirical successes;\nhowever, those methods need a huge amount of high-quality training data and\nlack theoretical interpretation. To address this issue, the spatio-temporal\ngraph scattering transform (ST-GST) was proposed to put forth a theoretically\ninterpretable framework; however, the empirical performance of this approach is\nconstrainted by the fully mathematical design. To benefit from both sides, this\nwork proposes a novel complementary mechanism to organically combine the\nspatio-temporal graph scattering transform and neural networks, resulting in\nthe proposed spatio-temporal graph complementary scattering networks (ST-GCSN).\nThe essence is to leverage the mathematically designed graph wavelets with\npruning techniques to cover major information and use trainable networks to\ncapture complementary information. The empirical experiments on hand pose\naction recognition show that the proposed ST-GCSN outperforms both ST-GCN and\nST-GST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zida Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Siheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectrum-to-Kernel Translation for Accurate Blind Image Super-Resolution. (arXiv:2110.12151v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12151","description":"<p>Deep-learning based Super-Resolution (SR) methods have exhibited promising\nperformance under non-blind setting where blur kernel is known. However, blur\nkernels of Low-Resolution (LR) images in different practical applications are\nusually unknown. It may lead to significant performance drop when degradation\nprocess of training images deviates from that of real images. In this paper, we\npropose a novel blind SR framework to super-resolve LR images degraded by\narbitrary blur kernel with accurate kernel estimation in frequency domain. To\nour best knowledge, this is the first deep learning method which conducts blur\nkernel estimation in frequency domain. Specifically, we first demonstrate that\nfeature representation in frequency domain is more conducive for blur kernel\nreconstruction than in spatial domain. Next, we present a Spectrum-to-Kernel\n(S$2$K) network to estimate general blur kernels in diverse forms. We use a\nConditional GAN (CGAN) combined with SR-oriented optimization target to learn\nthe end-to-end translation from degraded images' spectra to unknown kernels.\nExtensive experiments on both synthetic and real-world images demonstrate that\nour proposed method sufficiently reduces blur kernel estimation error, thus\nenables the off-the-shelf non-blind SR methods to work under blind setting\neffectively, and achieves superior performance over state-of-the-art blind SR\nmethods, averagely by 1.39dB, 0.48dB on commom blind SR setting (with Gaussian\nkernels) for scales $2\\times$ and $4\\times$, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1\">Guangpin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaozhong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenzhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chuming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Donghao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vertebrae segmentation, identification and localization using a graph optimization and a synergistic cycle. (arXiv:2110.12177v1 [eess.IV])","link":"http://arxiv.org/abs/2110.12177","description":"<p>This paper considers the segmentation, identification and localization of\nvertebrae in CT images. Although these three tasks are related, they face\nspecific problems that add up when they are addressed together. For example\nneighboring vertebrae with similar shapes perturb the identification and\nvertebrae with complex or even pathological morphologies impact the\nsegmentation. Consequently, the three tasks tend to be approached\nindependently, e.g. labelling (localization and identification) or segmenting\nonly, or, when treated globally, a sequential strategy is used. Sequential\nmethods however are prone to accumulate errors as they are not able to recover\nfrom mistakes of the previous module. In this work, we propose to combine all\nthree tasks and leverage their interdependence: locations ease the\nsegmentation, the segmentations in turn improve the locations and they all\ncontribute and benefit from the identification task. To this purpose we propose\na virtuous cycle to enforce coherence between the three tasks. Within such a\ncycle, the tasks interoperate and are iterated until a global consistency\ncriterion is satisfied. Our experiments validate this strategy with\nanatomically coherent results that outperform the state of the art on the\nVerSe20 challenge benchmark. Our code and model are openly available for\nresearch purposes at https://gitlab.inria.fr/spine/vertebrae_segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Di Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mohammed_E/0/1/0/all/0/1\">Eslam Mohammed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boyer_E/0/1/0/all/0/1\">Edmond Boyer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pujades_S/0/1/0/all/0/1\">Sergi Pujades</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An attention-driven hierarchical multi-scale representation for visual recognition. (arXiv:2110.12178v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12178","description":"<p>Convolutional Neural Networks (CNNs) have revolutionized the understanding of\nvisual content. This is mainly due to their ability to break down an image into\nsmaller pieces, extract multi-scale localized features and compose them to\nconstruct highly expressive representations for decision making. However, the\nconvolution operation is unable to capture long-range dependencies such as\narbitrary relations between pixels since it operates on a fixed-size window.\nTherefore, it may not be suitable for discriminating subtle changes (e.g.\nfine-grained visual recognition). To this end, our proposed method captures the\nhigh-level long-range dependencies by exploring Graph Convolutional Networks\n(GCNs), which aggregate information by establishing relationships among\nmulti-scale hierarchical regions. These regions consist of smaller (closer\nlook) to larger (far look), and the dependency between regions is modeled by an\ninnovative attention-driven message propagation, guided by the graph structure\nto emphasize the neighborhoods of a given region. Our approach is simple yet\nextremely effective in solving both the fine-grained and generic visual\nclassification problems. It outperforms the state-of-the-arts with a\nsignificant margin on three and is very competitive on other two datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wharton_Z/0/1/0/all/0/1\">Zachary Wharton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_A/0/1/0/all/0/1\">Ardhendu Behera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Asish Bera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MisMatch: Learning to Change Predictive Confidences with Attention for Consistency-Based, Semi-Supervised Medical Image Segmentation. (arXiv:2110.12179v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12179","description":"<p>The lack of labels is one of the fundamental constraints in deep learning\nbased methods for image classification and segmentation, especially in\napplications such as medical imaging. Semi-supervised learning (SSL) is a\npromising method to address the challenge of labels carcity. The\nstate-of-the-art SSL methods utilise consistency regularisation to learn\nunlabelled predictions which are invariant to perturbations on the prediction\nconfidence. However, such SSL approaches rely on hand-crafted augmentation\ntechniques which could be sub-optimal. In this paper, we propose MisMatch, a\nnovel consistency based semi-supervised segmentation method. MisMatch\nautomatically learns to produce paired predictions with increasedand decreased\nconfidences. MisMatch consists of an encoder and two decoders. One decoder\nlearns positive attention for regions of interest (RoI) on unlabelled data\nthereby generating higher confidence predictions of RoI. The other decoder\nlearns negative attention for RoI on the same unlabelled data thereby\ngenerating lower confidence predictions. We then apply a consistency\nregularisation between the paired predictions of the decoders. For evaluation,\nwe first perform extensive cross-validation on a CT-based pulmonary vessel\nsegmentation task and show that MisMatch statistically outperforms\nstate-of-the-art semi-supervised methods when only 6.25% of the total labels\nare used. Furthermore MisMatch performance using 6.25% ofthe total labels is\ncomparable to state-of-the-art methodsthat utilise all available labels. In a\nsecond experiment, MisMatch outperforms state-of-the-art methods on an\nMRI-based brain tumour segmentation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mou-Cheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu-Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1\">Chen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blumberg_S/0/1/0/all/0/1\">Stefano B. Blumberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_F/0/1/0/all/0/1\">Frederick J. Wilson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groot_M/0/1/0/all/0/1\">Marius De Groot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oxtoby_N/0/1/0/all/0/1\">Neil P. Oxtoby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1\">Daniel C. Alexander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1\">Joseph Jacob</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attend and Guide (AG-Net): A Keypoints-driven Attention-based Deep Network for Image Recognition. (arXiv:2110.12183v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12183","description":"<p>This paper presents a novel keypoints-based attention mechanism for visual\nrecognition in still images. Deep Convolutional Neural Networks (CNNs) for\nrecognizing images with distinctive classes have shown great success, but their\nperformance in discriminating fine-grained changes is not at the same level. We\naddress this by proposing an end-to-end CNN model, which learns meaningful\nfeatures linking fine-grained changes using our novel attention mechanism. It\ncaptures the spatial structures in images by identifying semantic regions (SRs)\nand their spatial distributions, and is proved to be the key to modelling\nsubtle changes in images. We automatically identify these SRs by grouping the\ndetected keypoints in a given image. The ``usefulness'' of these SRs for image\nrecognition is measured using our innovative attentional mechanism focusing on\nparts of the image that are most relevant to a given task. This framework\napplies to traditional and fine-grained image recognition tasks and does not\nrequire manually annotated regions (e.g. bounding-box of body parts, objects,\netc.) for learning and prediction. Moreover, the proposed keypoints-driven\nattention mechanism can be easily integrated into the existing CNN models. The\nframework is evaluated on six diverse benchmark datasets. The model outperforms\nthe state-of-the-art approaches by a considerable margin using Distracted\nDriver V1 (Acc: 3.39%), Distracted Driver V2 (Acc: 6.58%), Stanford-40 Actions\n(mAP: 2.15%), People Playing Musical Instruments (mAP: 16.05%), Food-101 (Acc:\n6.30%) and Caltech-256 (Acc: 2.59%) datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1\">Asish Bera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wharton_Z/0/1/0/all/0/1\">Zachary Wharton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yonghuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bessis_N/0/1/0/all/0/1\">Nik Bessis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Behera_A/0/1/0/all/0/1\">Ardhendu Behera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group-disentangled Representation Learning with Weakly-Supervised Regularization. (arXiv:2110.12185v1 [cs.LG])","link":"http://arxiv.org/abs/2110.12185","description":"<p>Learning interpretable and human-controllable representations that uncover\nfactors of variation in data remains an ongoing key challenge in representation\nlearning. We investigate learning group-disentangled representations for groups\nof factors with weak supervision. Existing techniques to address this challenge\nmerely constrain the approximate posterior by averaging over observations of a\nshared group. As a result, observations with a common set of variations are\nencoded to distinct latent representations, reducing their capacity to\ndisentangle and generalize to downstream tasks. In contrast to previous works,\nwe propose GroupVAE, a simple yet effective Kullback-Leibler (KL)\ndivergence-based regularization across shared latent representations to enforce\nconsistent and disentangled representations. We conduct a thorough evaluation\nand demonstrate that our GroupVAE significantly improves group disentanglement.\nFurther, we demonstrate that learning group-disentangled representations\nimprove upon downstream tasks, including fair classification and 3D\nshape-related tasks such as reconstruction, classification, and transfer\nlearning, and is competitive to supervised methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1\">Linh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khasahmadi_A/0/1/0/all/0/1\">Amir Hosein Khasahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1\">Aditya Sanghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asgari_S/0/1/0/all/0/1\">Saeid Asgari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Shape Guided Segmentation Network for Organs-at-Risk in Head and Neck CT Images. (arXiv:2110.12192v1 [eess.IV])","link":"http://arxiv.org/abs/2110.12192","description":"<p>The accurate segmentation of organs-at-risk (OARs) in head and neck CT images\nis a critical step for radiation therapy of head and neck cancer patients.\nHowever, manual delineation for numerous OARs is time-consuming and laborious,\neven for expert oncologists. Moreover, manual delineation results are\nsusceptible to high intra- and inter-variability. To this end, we propose a\nnovel dual shape guided network (DSGnet) to automatically delineate nine\nimportant OARs in head and neck CT images. To deal with the large shape\nvariation and unclear boundary of OARs in CT images, we represent the organ\nshape using an organ-specific unilateral inverse-distance map (UIDM) and guide\nthe segmentation task from two different perspectives: direct shape guidance by\nfollowing the segmentation prediction and across shape guidance by sharing the\nsegmentation feature. In the direct shape guidance, the segmentation prediction\nis not only supervised by the true label mask, but also by the true UIDM, which\nis implemented through a simple yet effective encoder-decoder mapping from the\nlabel space to the distance space. In the across shape guidance, UIDM is used\nto facilitate the segmentation by optimizing the shared feature maps. For the\nexperiments, we build a large head and neck CT dataset with a total of 699\nimages from different volunteers, and conduct comprehensive experiments and\ncomparisons with other state-of-the-art methods to justify the effectiveness\nand efficiency of our proposed method. The overall Dice Similarity Coefficient\n(DSC) value of 0.842 across the nine important OARs demonstrates great\npotential applications in improving the delineation quality and reducing the\ntime cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yanagihara_T/0/1/0/all/0/1\">Theodore Yanagihara</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chera_B/0/1/0/all/0/1\">Bhishamjit Chera</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Colette Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lian_J/0/1/0/all/0/1\">Jun Lian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RPT++: Customized Feature Representation for Siamese Visual Tracking. (arXiv:2110.12194v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12194","description":"<p>While recent years have witnessed remarkable progress in the feature\nrepresentation of visual tracking, the problem of feature misalignment between\nthe classification and regression tasks is largely overlooked. The approaches\nof feature extraction make no difference for these two tasks in most of\nadvanced trackers. We argue that the performance gain of visual tracking is\nlimited since features extracted from the salient area provide more\nrecognizable visual patterns for classification, while these around the\nboundaries contribute to accurately estimating the target state.\n</p>\n<p>We address this problem by proposing two customized feature extractors, named\npolar pooling and extreme pooling to capture task-specific visual patterns.\nPolar pooling plays the role of enriching information collected from the\nsemantic keypoints for stronger classification, while extreme pooling\nfacilitates explicit visual patterns of the object boundary for accurate target\nstate estimation. We demonstrate the effectiveness of the task-specific feature\nrepresentation by integrating it into the recent and advanced tracker RPT.\nExtensive experiments on several benchmarks show that our Customized Features\nbased RPT (RPT++) achieves new state-of-the-art performances on OTB-100,\nVOT2018, VOT2019, GOT-10k, TrackingNet and LaSOT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Ziang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Linyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jun Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Robust Differentiable Architecture Search under Label Noise. (arXiv:2110.12197v1 [cs.LG])","link":"http://arxiv.org/abs/2110.12197","description":"<p>Neural Architecture Search (NAS) is the game changer in designing robust\nneural architectures. Architectures designed by NAS outperform or compete with\nthe best manual network designs in terms of accuracy, size, memory footprint\nand FLOPs. That said, previous studies focus on developing NAS algorithms for\nclean high quality data, a restrictive and somewhat unrealistic assumption. In\nthis paper, focusing on the differentiable NAS algorithms, we show that vanilla\nNAS algorithms suffer from a performance loss if class labels are noisy. To\ncombat this issue, we make use of the principle of information bottleneck as a\nregularizer. This leads us to develop a noise injecting operation that is\nincluded during the learning process, preventing the network from learning from\nnoisy samples. Our empirical evaluations show that the noise injecting\noperation does not degrade the performance of the NAS algorithm if the data is\nindeed clean. In contrast, if the data is noisy, the architecture learned by\nour algorithm comfortably outperforms algorithms specifically equipped with\nsophisticated mechanisms to learn in the presence of label noise. In contrast\nto many algorithms designed to work in the presence of noisy labels, prior\nknowledge about the properties of the noise and its characteristics are not\nrequired for our algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Simon_C/0/1/0/all/0/1\">Christian Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1\">Mehrtash Harandi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascading Feature Extraction for Fast Point Cloud Registration. (arXiv:2110.12204v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12204","description":"<p>We propose a method for speeding up a 3D point cloud registration through a\ncascading feature extraction. The current approach with the highest accuracy is\nrealized by iteratively executing feature extraction and registration using\ndeep features. However, iterative feature extraction takes time. Our proposed\nmethod significantly reduces the computational cost using cascading shallow\nlayers. Our idea is to omit redundant computations that do not always\ncontribute to the final accuracy. The proposed approach is approximately three\ntimes faster than the existing methods without a loss of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hisadome_Y/0/1/0/all/0/1\">Yoichiro Hisadome</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsui_Y/0/1/0/all/0/1\">Yusuke Matsui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Domain Incremental Learning for Semantic Segmentation. (arXiv:2110.12205v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12205","description":"<p>Recent efforts in multi-domain learning for semantic segmentation attempt to\nlearn multiple geographical datasets in a universal, joint model. A simple\nfine-tuning experiment performed sequentially on three popular road scene\nsegmentation datasets demonstrates that existing segmentation frameworks fail\nat incrementally learning on a series of visually disparate geographical\ndomains. When learning a new domain, the model catastrophically forgets\npreviously learned knowledge. In this work, we pose the problem of multi-domain\nincremental learning for semantic segmentation. Given a model trained on a\nparticular geographical domain, the goal is to (i) incrementally learn a new\ngeographical domain, (ii) while retaining performance on the old domain, (iii)\ngiven that the previous domain's dataset is not accessible. We propose a\ndynamic architecture that assigns universally shared, domain-invariant\nparameters to capture homogeneous semantic features present in all domains,\nwhile dedicated domain-specific parameters learn the statistics of each domain.\nOur novel optimization strategy helps achieve a good balance between retention\nof old knowledge (stability) and acquiring new knowledge (plasticity). We\ndemonstrate the effectiveness of our proposed solution on domain incremental\nsettings pertaining to real-world driving scenes from roads of Germany\n(Cityscapes), the United States (BDD100k), and India (IDD).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_P/0/1/0/all/0/1\">Prachi Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saluja_R/0/1/0/all/0/1\">Rohit Saluja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1\">Vineeth N Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1\">Chetan Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramanian_A/0/1/0/all/0/1\">Anbumani Subramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1\">C.V. Jawahar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MaskSplit: Self-supervised Meta-learning for Few-shot Semantic Segmentation. (arXiv:2110.12207v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12207","description":"<p>Just like other few-shot learning problems, few-shot segmentation aims to\nminimize the need for manual annotation, which is particularly costly in\nsegmentation tasks. Even though the few-shot setting reduces this cost for\nnovel test classes, there is still a need to annotate the training data. To\nalleviate this need, we propose a self-supervised training approach for\nlearning few-shot segmentation models. We first use unsupervised saliency\nestimation to obtain pseudo-masks on images. We then train a simple prototype\nbased model over different splits of pseudo masks and augmentations of images.\nOur extensive experiments show that the proposed approach achieves promising\nresults, highlighting the potential of self-supervised training. To the best of\nour knowledge this is the first work that addresses unsupervised few-shot\nsegmentation problem on natural images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amac_M/0/1/0/all/0/1\">Mustafa Sercan Amac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sencan_A/0/1/0/all/0/1\">Ahmet Sencan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baran_O/0/1/0/all/0/1\">Orhun Bugra Baran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikizler_Cinbis_N/0/1/0/all/0/1\">Nazli Ikizler-Cinbis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinbis_R/0/1/0/all/0/1\">Ramazan Gokberk Cinbis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks. (arXiv:2110.12211v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12211","description":"<p>With event-driven algorithms, especially the spiking neural networks (SNNs),\nachieving continuous improvement in neuromorphic vision processing, a more\nchallenging event-stream-dataset is urgently needed. However, it is well known\nthat creating an ES-dataset is a time-consuming and costly task with\nneuromorphic cameras like dynamic vision sensors (DVS). In this work, we\npropose a fast and effective algorithm termed Omnidirectional Discrete Gradient\n(ODG) to convert the popular computer vision dataset ILSVRC2012 into its\nevent-stream (ES) version, generating about 1,300,000 frame-based images into\nES-samples in 1000 categories. In this way, we propose an ES-dataset called\nES-ImageNet, which is dozens of times larger than other neuromorphic\nclassification datasets at present and completely generated by the software.\nThe ODG algorithm implements an image motion to generate local value changes\nwith discrete gradient information in different directions, providing a\nlow-cost and high-speed way for converting frame-based images into event\nstreams, along with Edge-Integral to reconstruct the high-quality images from\nevent streams. Furthermore, we analyze the statistics of the ES-ImageNet in\nmultiple ways, and a performance benchmark of the dataset is also provided\nusing both famous deep neural network algorithms and spiking neural network\nalgorithms. We believe that this work shall provide a new large-scale benchmark\ndataset for SNNs and neuromorphic vision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiang_S/0/1/0/all/0/1\">Shaohua Qiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Lei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guoqi Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptation for Rare Classes Augmented with Synthetic Samples. (arXiv:2110.12216v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12216","description":"<p>To alleviate lower classification performance on rare classes in imbalanced\ndatasets, a possible solution is to augment the underrepresented classes with\nsynthetic samples. Domain adaptation can be incorporated in a classifier to\ndecrease the domain discrepancy between real and synthetic samples. While\ndomain adaptation is generally applied on completely synthetic source domains\nand real target domains, we explore how domain adaptation can be applied when\nonly a single rare class is augmented with simulated samples. As a testbed, we\nuse a camera trap animal dataset with a rare deer class, which is augmented\nwith synthetic deer samples. We adapt existing domain adaptation methods to two\nnew methods for the single rare class setting: DeerDANN, based on the\nDomain-Adversarial Neural Network (DANN), and DeerCORAL, based on deep\ncorrelation alignment (Deep CORAL) architectures. Experiments show that\nDeerDANN has the highest improvement in deer classification accuracy of 24.0%\nversus 22.4% improvement of DeerCORAL when compared to the baseline. Further,\nboth methods require fewer than 10k synthetic samples, as used by the baseline,\nto achieve these higher accuracies. DeerCORAL requires the least number of\nsynthetic samples (2k deer), followed by DeerDANN (8k deer).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1\">Tuhin Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruintjes_R/0/1/0/all/0/1\">Robert-Jan Bruintjes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lengyel_A/0/1/0/all/0/1\">Attila Lengyel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Variational Linear Units (PVLUs) in Deep Convolutional Networks. (arXiv:2110.12246v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12246","description":"<p>The Rectified Linear Unit is currently a state-of-the-art activation function\nin deep convolutional neural networks. To combat ReLU's dying neuron problem,\nwe propose the Parametric Variational Linear Unit (PVLU), which adds a\nsinusoidal function with trainable coefficients to ReLU. Along with introducing\nnonlinearity and non-zero gradients across the entire real domain, PVLU allows\nfor increased model generalization and robustness when implemented in the\ncontext of transfer learning. On a simple, non-transfer sequential CNN, PVLU\nled to relative error decrease of 16.3% and 11.3% without and with data\naugmentation, relative to ReLU. PVLU is also tested on transfer learning\nproblems. The VGG-16 and VGG-19 models experience relative error reductions of\n9.5% and 10.7% on CIFAR-10, respectively, after the substitution of ReLU with\nPVLU. When training on Gaussian-filtered CIFAR-10 images, similar improvements\nare noted for the VGG models. Most notably, PVLU fine tuning allows for\nrelative error reductions up to and exceeding 10% on near state-of-the-art\nResNet models for both CIFAR-10 and CIFAR-100.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aarush Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Shikhar Ahuja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Confidence-Aware Active Feedback for Efficient Instance Search. (arXiv:2110.12255v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12255","description":"<p>Relevance feedback is widely used in instance search (INS) tasks to further\nrefine imperfect ranking results, but it often comes with low interaction\nefficiency. Active learning (AL) technique has achieved great success in\nimproving annotation efficiency in classification tasks. However, considering\nirrelevant samples' diversity and class imbalance in INS tasks, existing AL\nmethods cannot always select the most suitable feedback candidates for INS\nproblems. In addition, they are often too computationally complex to be applied\nin interactive INS scenario. To address the above problems, we propose a\nconfidence-aware active feedback (CAAF) method that can efficiently select the\nmost valuable feedback candidates to improve the re-ranking performance.\nSpecifically, inspired by the explicit sample difficulty modeling in self-paced\nlearning, we utilize a pairwise manifold ranking loss to evaluate the ranking\nconfidence of each unlabeled sample, and formulate the INS process as a\nconfidence-weighted manifold ranking problem. Furthermore, we introduce an\napproximate optimization scheme to simplify the solution from QP problems with\nconstraints to closed-form expressions, and selects only the top-K samples in\nthe initial ranking list for INS, so that CAAF is able to handle large-scale\nINS tasks in a short period of time. Extensive experiments on both image and\nvideo INS tasks demonstrate the effectiveness of the proposed CAAF method. In\nparticular, CAAF outperforms the first-place record in the public large-scale\nvideo INS evaluation of TRECVID 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Longxiang Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"espiownage: Tracking Transients in Steelpan Drum Strikes Using Surveillance Technology. (arXiv:2110.12261v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12261","description":"<p>We present an improvement in the ability to meaningfully track features in\nhigh speed videos of Caribbean steelpan drums illuminated by Electronic Speckle\nPattern Interferometry (ESPI). This is achieved through the use of up-to-date\ncomputer vision libraries for object detection and image segmentation as well\nas a significant effort toward cleaning the dataset previously used to train\nsystems for this application. Besides improvements on previous metric scores by\n10% or more, noteworthy in this project are the introduction of a\nsegmentation-regression map for the entire drum surface yielding interference\nfringe counts comparable to those obtained via object detection, as well as the\naccelerated workflow for coordinating the data-cleaning-and-model-training\nfeedback loop for rapid iteration allowing this project to be conducted on a\ntimescale of only 18 days.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hawley_S/0/1/0/all/0/1\">Scott H. Hawley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morrison_A/0/1/0/all/0/1\">Andrew C. Morrison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_G/0/1/0/all/0/1\">Grant S. Morgan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking of Lightweight Deep Learning Architectures for Skin Cancer Classification using ISIC 2017 Dataset. (arXiv:2110.12270v1 [eess.IV])","link":"http://arxiv.org/abs/2110.12270","description":"<p>Skin cancer is one of the deadly types of cancer and is common in the world.\nRecently, there has been a huge jump in the rate of people getting skin cancer.\nFor this reason, the number of studies on skin cancer classification with deep\nlearning are increasing day by day. For the growth of work in this area, the\nInternational Skin Imaging Collaboration (ISIC) organization was established\nand they created an open dataset archive. In this study, images were taken from\nISIC 2017 Challenge. The skin cancer images taken were preprocessed and data\naugmented. Later, these images were trained with transfer learning and\nfine-tuning approach and deep learning models were created in this way. 3\ndifferent mobile deep learning models and 3 different batch size values were\ndetermined for each, and a total of 9 models were created. Among these models,\nthe NASNetMobile model with 16 batch size got the best result. The accuracy\nvalue of this model is 82.00%, the precision value is 81.77% and the F1 score\nvalue is 0.8038. Our method is to benchmark mobile deep learning models which\nhave few parameters and compare the results of the models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yilmaz_A/0/1/0/all/0/1\">Abdurrahim Yilmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kalebasi_M/0/1/0/all/0/1\">Mucahit Kalebasi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Samoylenko_Y/0/1/0/all/0/1\">Yegor Samoylenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guvenilir_M/0/1/0/all/0/1\">Mehmet Erhan Guvenilir</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uvet_H/0/1/0/all/0/1\">Huseyin Uvet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Validation: Early Stopping for Single-Instance Deep Generative Priors. (arXiv:2110.12271v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12271","description":"<p>Recent works have shown the surprising effectiveness of deep generative\nmodels in solving numerous image reconstruction (IR) tasks, even without\ntraining data. We call these models, such as deep image prior and deep decoder,\ncollectively as single-instance deep generative priors (SIDGPs). The successes,\nhowever, often hinge on appropriate early stopping (ES), which by far has\nlargely been handled in an ad-hoc manner. In this paper, we propose the first\nprincipled method for ES when applying SIDGPs to IR, taking advantage of the\ntypical bell trend of the reconstruction quality. In particular, our method is\nbased on collaborative training and self-validation: the primal reconstruction\nprocess is monitored by a deep autoencoder, which is trained online with the\nhistoric reconstructed images and used to validate the reconstruction quality\nconstantly. Experimentally, on several IR problems and different SIDGPs, our\nself-validation method is able to reliably detect near-peak performance and\nsignal good ES points. Our code is available at\nhttps://sun-umn.github.io/Self-Validation/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Taihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hengyue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Le Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hengkang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Ju Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"One-Shot\" Reduction of Additive Artifacts in Medical Images. (arXiv:2110.12274v1 [eess.IV])","link":"http://arxiv.org/abs/2110.12274","description":"<p>Medical images may contain various types of artifacts with different patterns\nand mixtures, which depend on many factors such as scan setting, machine\ncondition, patients' characteristics, surrounding environment, etc. However,\nexisting deep-learning-based artifact reduction methods are restricted by their\ntraining set with specific predetermined artifact types and patterns. As such,\nthey have limited clinical adoption. In this paper, we introduce One-Shot\nmedical image Artifact Reduction (OSAR), which exploits the power of deep\nlearning but without using pre-trained general networks. Specifically, we train\na light-weight image-specific artifact reduction network using data synthesized\nfrom the input image at test-time. Without requiring any prior large training\ndata set, OSAR can work with almost any medical images that contain varying\nadditive artifacts which are not in any existing data sets. In addition,\nComputed Tomography (CT) and Magnetic Resonance Imaging (MRI) are used as\nvehicles and show that the proposed method can reduce artifacts better than\nstate-of-the-art both qualitatively and quantitatively using shorter test time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yu-Jen Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_Y/0/1/0/all/0/1\">Yen-Jung Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wen_S/0/1/0/all/0/1\">Shao-Cheng Wen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_T/0/1/0/all/0/1\">Tsung-Yi Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_H/0/1/0/all/0/1\">Haiyun Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Signal to Noise Ratio Loss Function. (arXiv:2110.12275v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12275","description":"<p>This work proposes a new loss function targeting classification problems,\nutilizing a source of information overlooked by cross entropy loss. First, we\nderive a series of the tightest upper and lower bounds for the probability of a\nrandom variable in a given interval. Second, a lower bound is proposed for the\nprobability of a true positive for a parametric classification problem, where\nthe form of probability density function (pdf) of data is given. A closed form\nfor finding the optimal function of unknowns is derived to maximize the\nprobability of true positives. Finally, for the case that the pdf of data is\nunknown, we apply the proposed boundaries to find the lower bound of the\nprobability of true positives and upper bound of the probability of false\npositives and optimize them using a loss function which is given by combining\nthe boundaries. We demonstrate that the resultant loss function is a function\nof the signal to noise ratio both within and across logits. We empirically\nevaluate our proposals to show their benefit for classification problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghobadzadeh_A/0/1/0/all/0/1\">Ali Ghobadzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lashkari_A/0/1/0/all/0/1\">Amir Lashkari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perineural Invasion Detection in Multiple Organ Cancer Based on Deep Convolutional Neural Network. (arXiv:2110.12283v1 [eess.IV])","link":"http://arxiv.org/abs/2110.12283","description":"<p>Perineural invasion (PNI) by malignant tumor cells has been reported as an\nindependent indicator of poor prognosis in various cancers. Assessment of PNI\nin small nerves on glass slides is a labor-intensive task. In this study, we\npropose an algorithm to detect the perineural invasions in colon, prostate, and\npancreas cancers based on a convolutional neural network (CNN).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Nateghi_R/0/1/0/all/0/1\">Ramin Nateghi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pourakpour_F/0/1/0/all/0/1\">Fattaneh Pourakpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face sketch to photo translation using generative adversarial networks. (arXiv:2110.12290v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12290","description":"<p>Translating face sketches to photo-realistic faces is an interesting and\nessential task in many applications like law enforcement and the digital\nentertainment industry. One of the most important challenges of this task is\nthe inherent differences between the sketch and the real image such as the lack\nof color and details of the skin tissue in the sketch. With the advent of\nadversarial generative models, an increasing number of methods have been\nproposed for sketch-to-image synthesis. However, these models still suffer from\nlimitations such as the large number of paired data required for training, the\nlow resolution of the produced images, or the unrealistic appearance of the\ngenerated images. In this paper, we propose a method for converting an input\nfacial sketch to a colorful photo without the need for any paired dataset. To\ndo so, we use a pre-trained face photo generating model to synthesize\nhigh-quality natural face photos and employ an optimization procedure to keep\nhigh-fidelity to the input sketch. We train a network to map the facial\nfeatures extracted from the input sketch to a vector in the latent space of the\nface generating model. Also, we study different optimization criteria and\ncompare the results of the proposed model with those of the state-of-the-art\nmodels quantitatively and qualitatively. The proposed model achieved 0.655 in\nthe SSIM index and 97.59% rank-1 face recognition rate with higher quality of\nthe produced images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farid_N/0/1/0/all/0/1\">Nastaran Moradzadeh Farid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fard_M/0/1/0/all/0/1\">Maryam Saeedi Fard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickabadi_A/0/1/0/all/0/1\">Ahmad Nickabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Layer-wise Adversarial-aware Quantization Optimization for Improving Robustness. (arXiv:2110.12308v1 [cs.LG])","link":"http://arxiv.org/abs/2110.12308","description":"<p>Neural networks are getting better accuracy with higher energy and\ncomputational cost. After quantization, the cost can be greatly saved, and the\nquantized models are more hardware friendly with acceptable accuracy loss. On\nthe other hand, recent research has found that neural networks are vulnerable\nto adversarial attacks, and the robustness of a neural network model can only\nbe improved with defense methods, such as adversarial training. In this work,\nwe find that adversarially-trained neural networks are more vulnerable to\nquantization loss than plain models. To minimize both the adversarial and the\nquantization losses simultaneously and to make the quantized model robust, we\npropose a layer-wise adversarial-aware quantization method, using the Lipschitz\nconstant to choose the best quantization parameter settings for a neural\nnetwork. We theoretically derive the losses and prove the consistency of our\nmetric selection. The experiment results show that our method can effectively\nand efficiently improve the robustness of quantized adversarially-trained\nneural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Riya Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hai Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoVA: Context-aware Visual Attention for Webpage Information Extraction. (arXiv:2110.12320v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12320","description":"<p>Webpage information extraction (WIE) is an important step to create knowledge\nbases. For this, classical WIE methods leverage the Document Object Model (DOM)\ntree of a website. However, use of the DOM tree poses significant challenges as\ncontext and appearance are encoded in an abstract manner. To address this\nchallenge we propose to reformulate WIE as a context-aware Webpage Object\nDetection task. Specifically, we develop a Context-aware Visual Attention-based\n(CoVA) detection pipeline which combines appearance features with syntactical\nstructure from the DOM tree. To study the approach we collect a new large-scale\ndataset of e-commerce websites for which we manually annotate every web element\nwith four labels: product price, product title, product image and background.\nOn this dataset we show that the proposed CoVA approach is a new challenging\nbaseline which improves upon prior state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurendra Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morabia_K/0/1/0/all/0/1\">Keval Morabia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingjin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kevin Chen-Chuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADC: Adversarial attacks against object Detection that evade Context consistency checks. (arXiv:2110.12321v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12321","description":"<p>Deep Neural Networks (DNNs) have been shown to be vulnerable to adversarial\nexamples, which are slightly perturbed input images which lead DNNs to make\nwrong predictions. To protect from such examples, various defense strategies\nhave been proposed. A very recent defense strategy for detecting adversarial\nexamples, that has been shown to be robust to current attacks, is to check for\nintrinsic context consistencies in the input data, where context refers to\nvarious relationships (e.g., object-to-object co-occurrence relationships) in\nimages. In this paper, we show that even context consistency checks can be\nbrittle to properly crafted adversarial examples and to the best of our\nknowledge, we are the first to do so. Specifically, we propose an adaptive\nframework to generate examples that subvert such defenses, namely, Adversarial\nattacks against object Detection that evade Context consistency checks (ADC).\nIn ADC, we formulate a joint optimization problem which has two attack goals,\nviz., (i) fooling the object detector and (ii) evading the context consistency\ncheck system, at the same time. Experiments on both PASCAL VOC and MS COCO\ndatasets show that examples generated with ADC fool the object detector with a\nsuccess rate of over 85% in most cases, and at the same time evade the recently\nproposed context consistency checks, with a bypassing rate of over 80% in most\ncases. Our results suggest that how to robustly model context and check its\nconsistency, is still an open problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1\">Mingjun Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shasha Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chengyu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1\">M. Salman Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1\">Amit K. Roy-Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnamurthy_S/0/1/0/all/0/1\">Srikanth V. Krishnamurthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A methodology for detection and localization of fruits in apples orchards from aerial images. (arXiv:2110.12331v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12331","description":"<p>Computer vision methods based on convolutional neural networks (CNNs) have\npresented promising results on image-based fruit detection at ground-level for\ndifferent crops. However, the integration of the detections found in different\nimages, allowing accurate fruit counting and yield prediction, have received\nless attention. This work presents a methodology for automated fruit counting\nemploying aerial-images. It includes algorithms based on multiple view geometry\nto perform fruits tracking, not just avoiding double counting but also locating\nthe fruits in the 3-D space. Preliminary assessments show correlations above\n0.8 between fruit counting and true yield for apples. The annotated dataset\nemployed on CNN training is publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_T/0/1/0/all/0/1\">Thiago T. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebler_L/0/1/0/all/0/1\">Luciano Gebler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOLVER: Scene-Object Interrelated Visual Emotion Reasoning Network. (arXiv:2110.12334v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12334","description":"<p>Visual Emotion Analysis (VEA) aims at finding out how people feel emotionally\ntowards different visual stimuli, which has attracted great attention recently\nwith the prevalence of sharing images on social networks. Since human emotion\ninvolves a highly complex and abstract cognitive process, it is difficult to\ninfer visual emotions directly from holistic or regional features in affective\nimages. It has been demonstrated in psychology that visual emotions are evoked\nby the interactions between objects as well as the interactions between objects\nand scenes within an image. Inspired by this, we propose a novel Scene-Object\ninterreLated Visual Emotion Reasoning network (SOLVER) to predict emotions from\nimages. To mine the emotional relationships between distinct objects, we first\nbuild up an Emotion Graph based on semantic concepts and visual features. Then,\nwe conduct reasoning on the Emotion Graph using Graph Convolutional Network\n(GCN), yielding emotion-enhanced object features. We also design a Scene-Object\nFusion Module to integrate scenes and objects, which exploits scene features to\nguide the fusion process of object features with the proposed scene-based\nattention mechanism. Extensive experiments and comparisons are conducted on\neight public visual emotion datasets, and the results demonstrate that the\nproposed SOLVER consistently outperforms the state-of-the-art methods by a\nlarge margin. Ablation studies verify the effectiveness of our method and\nvisualizations prove its interpretability, which also bring new insight to\nexplore the mysteries in VEA. Notably, we further discuss SOLVER on three other\npotential datasets with extended experiments, where we validate the robustness\nof our method and notice some limitations of it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingyuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xinbo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Leida Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiumei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1\">Jinshan Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quality Map Fusion for Adversarial Learning. (arXiv:2110.12338v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12338","description":"<p>Generative adversarial models that capture salient low-level features which\nconvey visual information in correlation with the human visual system (HVS)\nstill suffer from perceptible image degradations. The inability to convey such\nhighly informative features can be attributed to mode collapse, convergence\nfailure and vanishing gradients. In this paper, we improve image quality\nadversarially by introducing a novel quality map fusion technique that\nharnesses image features similar to the HVS and the perceptual properties of a\ndeep convolutional neural network (DCNN). We extend the widely adopted l2\nWasserstein distance metric to other preferable quality norms derived from\nBanach spaces that capture richer image properties like structure, luminance,\ncontrast and the naturalness of images. We also show that incorporating a\nperceptual attention mechanism (PAM) that extracts global feature embeddings\nfrom the network bottleneck with aggregated perceptual maps derived from\nstandard image quality metrics translate to a better image quality. We also\ndemonstrate impressive performance over other methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osahor_U/0/1/0/all/0/1\">Uche Osahor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Closer Look at Few-Shot Video Classification: A New Baseline and Benchmark. (arXiv:2110.12358v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12358","description":"<p>The existing few-shot video classification methods often employ a\nmeta-learning paradigm by designing customized temporal alignment module for\nsimilarity calculation. While significant progress has been made, these methods\nfail to focus on learning effective representations, and heavily rely on the\nImageNet pre-training, which might be unreasonable for the few-shot recognition\nsetting due to semantics overlap. In this paper, we aim to present an in-depth\nstudy on few-shot video classification by making three contributions. First, we\nperform a consistent comparative study on the existing metric-based methods to\nfigure out their limitations in representation learning. Accordingly, we\npropose a simple classifier-based baseline without any temporal alignment that\nsurprisingly outperforms the state-of-the-art meta-learning based methods.\nSecond, we discover that there is a high correlation between the novel action\nclass and the ImageNet object class, which is problematic in the few-shot\nrecognition setting. Our results show that the performance of training from\nscratch drops significantly, which implies that the existing benchmarks cannot\nprovide enough base data. Finally, we present a new benchmark with more base\ndata to facilitate future few-shot video classification without pre-training.\nThe code will be made available at https://github.com/MCG-NJU/FSL-Video.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenxi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Limin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Sheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gangshan Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CvT-ASSD: Convolutional vision-Transformer Based Attentive Single Shot MultiBox Detector. (arXiv:2110.12364v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12364","description":"<p>Due to the success of Bidirectional Encoder Representations from Transformers\n(BERT) in natural language process (NLP), the multi-head attention transformer\nhas been more and more prevalent in computer-vision researches (CV). However,\nit still remains a challenge for researchers to put forward complex tasks such\nas vision detection and semantic segmentation. Although multiple\nTransformer-Based architectures like DETR and ViT-FRCNN have been proposed to\ncomplete object detection task, they inevitably decreases discrimination\naccuracy and brings down computational efficiency caused by the enormous\nlearning parameters and heavy computational complexity incurred by the\ntraditional self-attention operation. In order to alleviate these issues, we\npresent a novel object detection architecture, named Convolutional vision\nTransformer Based Attentive Single Shot MultiBox Detector (CvT-ASSD), that\nbuilt on the top of Convolutional vision Transormer (CvT) with the efficient\nAttentive Single Shot MultiBox Detector (ASSD). We provide comprehensive\nempirical evidence showing that our model CvT-ASSD can leads to good system\nefficiency and performance while being pretrained on large-scale detection\ndatasets such as PASCAL VOC and MS COCO. Code has been released on public\ngithub repository at https://github.com/albert-jin/CvT-ASSD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1\">Weiqiang Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hang Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AuxAdapt: Stable and Efficient Test-Time Adaptation for Temporally Consistent Video Semantic Segmentation. (arXiv:2110.12369v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12369","description":"<p>In video segmentation, generating temporally consistent results across frames\nis as important as achieving frame-wise accuracy. Existing methods rely either\non optical flow regularization or fine-tuning with test data to attain temporal\nconsistency. However, optical flow is not always avail-able and reliable.\nBesides, it is expensive to compute. Fine-tuning the original model in test\ntime is cost sensitive.\n</p>\n<p>This paper presents an efficient, intuitive, and unsupervised online\nadaptation method, AuxAdapt, for improving the temporal consistency of most\nneural network models. It does not require optical flow and only takes one pass\nof the video. Since inconsistency mainly arises from the model's uncertainty in\nits output, we propose an adaptation scheme where the model learns from its own\nsegmentation decisions as it streams a video, which allows producing more\nconfident and temporally consistent labeling for similarly-looking pixels\nacross frames. For stability and efficiency, we leverage a small auxiliary\nsegmentation network (AuxNet) to assist with this adaptation. More\nspecifically, AuxNet readjusts the decision of the original segmentation\nnetwork (Main-Net) by adding its own estimations to that of MainNet. At every\nframe, only AuxNet is updated via back-propagation while keeping MainNet fixed.\nWe extensively evaluate our test-time adaptation approach on standard video\nbenchmarks, including Cityscapes, CamVid, and KITTI. The results demonstrate\nthat our approach provides label-wise accurate, temporally consistent, and\ncomputationally efficient adaptation (5+ folds overhead reduction comparing to\nstate-of-the-art test-time adaptation methods).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borse_S/0/1/0/all/0/1\">Shubhankar Borse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Lung Nodule Segmentation with Multiple Annotations. (arXiv:2110.12372v1 [eess.IV])","link":"http://arxiv.org/abs/2110.12372","description":"<p>Since radiologists have different training and clinical experience, they may\nprovide various segmentation maps for a lung nodule. As a result, for a\nspecific lung nodule, some regions have a higher chance of causing segmentation\nuncertainty, which brings difficulty for lung nodule segmentation with multiple\nannotations. To address this problem, this paper proposes an Uncertainty-Aware\nSegmentation Network (UAS-Net) based on multi-branch U-Net, which can learn the\nvaluable visual features from the regions that may cause segmentation\nuncertainty and contribute to a better segmentation result. Meanwhile, this\nnetwork can provide a Multi-Confidence Mask (MCM) simultaneously, pointing out\nregions with different segmentation uncertainty levels. We introduce a\nFeature-Aware Concatenation structure for different learning targets and let\neach branch have a specific learning preference. Moreover, a joint adversarial\nlearning process is also adopted to help learn discriminative features of\ncomplex structures. Experimental results show that our method can predict the\nreasonable regions with higher uncertainty and improve lung nodule segmentation\nperformance in LIDC-IDRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qiuli Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Han Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1\">Lu Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Mengke Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Consistency in Video Segmentation. (arXiv:2110.12385v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12385","description":"<p>In this paper, we present a novel perceptual consistency perspective on video\nsemantic segmentation, which can capture both temporal consistency and\npixel-wise correctness. Given two nearby video frames, perceptual consistency\nmeasures how much the segmentation decisions agree with the pixel\ncorrespondences obtained via matching general perceptual features. More\nspecifically, for each pixel in one frame, we find the most perceptually\ncorrelated pixel in the other frame. Our intuition is that such a pair of\npixels are highly likely to belong to the same class. Next, we assess how much\nthe segmentation agrees with such perceptual correspondences, based on which we\nderive the perceptual consistency of the segmentation maps across these two\nframes. Utilizing perceptual consistency, we can evaluate the temporal\nconsistency of video segmentation by measuring the perceptual consistency over\nconsecutive pairs of segmentation maps in a video. Furthermore, given a\nsparsely labeled test video, perceptual consistency can be utilized to aid with\npredicting the pixel-wise correctness of the segmentation on an unlabeled\nframe. More specifically, by measuring the perceptual consistency between the\npredicted segmentation and the available ground truth on a nearby frame and\ncombining it with the segmentation confidence, we can accurately assess the\nclassification correctness on each pixel. Our experiments show that the\nproposed perceptual consistency can more accurately evaluate the temporal\nconsistency of video segmentation as compared to flow-based measures.\nFurthermore, it can help more confidently predict segmentation accuracy on\nunlabeled test frames, as compared to using classification confidence alone.\nFinally, our proposed measure can be used as a regularizer during the training\nof segmentation models, which leads to more temporally consistent video\nsegmentation while maintaining accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borse_S/0/1/0/all/0/1\">Shubhankar Borse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Ying Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1\">Ning Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoyun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Motion History Images with 3D Convolutional Networks in Isolated Sign Language Recognition. (arXiv:2110.12396v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12396","description":"<p>Sign language recognition using computational models is a challenging problem\nthat requires simultaneous spatio-temporal modeling of the multiple sources,\ni.e. faces, hands, body etc. In this paper, we propose an isolated sign\nlanguage recognition model based on a model trained using Motion History Images\n(MHI) that are generated from RGB video frames. RGB-MHI images represent\nspatio-temporal summary of each sign video effectively in a single RGB image.\nWe propose two different approaches using this model. In the first approach, we\nuse RGB-MHI model as a motion-based spatial attention module integrated in a\n3D-CNN architecture. In the second approach, we use RGB-MHI model features\ndirectly with a late fusion technique with the features of a 3D-CNN model. We\nperform extensive experiments on two recently released large-scale isolated\nsign language datasets, namely AUTSL and BosphorusSign22k datasets. Our\nexperiments show that our models, which use only RGB data, can compete with the\nstate-of-the-art models in the literature that use multi-modal data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sincan_O/0/1/0/all/0/1\">Ozge Mercanoglu Sincan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keles_H/0/1/0/all/0/1\">Hacer Yalim Keles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IQNAS: Interpretable Integer Quadratic Programming Neural Architecture Search. (arXiv:2110.12399v1 [cs.LG])","link":"http://arxiv.org/abs/2110.12399","description":"<p>Realistic use of neural networks often requires adhering to multiple\nconstraints on latency, energy and memory among others. A popular approach to\nfind fitting networks is through constrained Neural Architecture Search (NAS).\nHowever, previous methods use complicated predictors for the accuracy of the\nnetwork. Those predictors are hard to interpret and sensitive to many\nhyperparameters to be tuned, hence, the resulting accuracy of the generated\nmodels is often harmed. In this work we resolve this by introducing\nInterpretable Integer Quadratic programming Neural Architecture Search (IQNAS),\nthat is based on an accurate and simple quadratic formulation of both the\naccuracy predictor and the expected resource requirement, together with a\nscalable search method with theoretical guarantees. The simplicity of our\nproposed predictor together with the intuitive way it is constructed bring\ninterpretability through many insights about the contribution of different\ndesign choices. For example, we find that in the examined search space, adding\ndepth and width is more effective at deeper stages of the network and at the\nbeginning of each resolution stage. Our experiments show that IQNAS generates\ncomparable to or better architectures than other state-of-the-art NAS methods\nwithin a reduced search cost for each additional generated network, while\nstrictly satisfying the resource constraints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nayman_N/0/1/0/all/0/1\">Niv Nayman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aflalo_Y/0/1/0/all/0/1\">Yonathan Aflalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1\">Asaf Noy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1\">Lihi Zelnik-Manor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic Keypoints Selection Network for 6DoF Pose Estimation. (arXiv:2110.12401v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12401","description":"<p>6 DoF poses estimation problem aims to estimate the rotation and translation\nparameters between two coordinates, such as object world coordinate and camera\nworld coordinate. Although some advances are made with the help of deep\nlearning, how to full use scene information is still a problem. Prior works\ntackle the problem by pixel-wise feature fusion but need to randomly selecte\nnumerous points from images, which can not satisfy the demands of fast\ninference simultaneously and accurate pose estimation. In this work, we present\na novel deep neural network based on dynamic keypoints selection designed for\n6DoF pose estimation from a single RGBD image. Our network includes three\nparts, instance semantic segmentation, edge points detection and 6DoF pose\nestimation. Given an RGBD image, our network is trained to predict pixel\ncategory and the translation to edge points and center points. Then, a\nleast-square fitting manner is applied to estimate the 6DoF pose parameters.\nSpecifically, we propose a dynamic keypoints selection algorithm to choose\nkeypoints from the foreground feature map. It allows us to leverage geometric\nand appearance information. During 6DoF pose estimation, we utilize the\ninstance semantic segmentation result to filter out background points and only\nuse foreground points to finish edge points detection and 6DoF pose estimation.\nExperiments on two commonly used 6DoF estimation benchmark datasets, YCB-Video\nand LineMoD, demonstrate that our method outperforms the state-of-the-art\nmethods and achieves significant improvements over other same category methods\ntime efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haowen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Taiyong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAS-FCOS: Efficient Search for Object Detection Architectures. (arXiv:2110.12423v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12423","description":"<p>Neural Architecture Search (NAS) has shown great potential in effectively\nreducing manual effort in network design by automatically discovering optimal\narchitectures. What is noteworthy is that as of now, object detection is less\ntouched by NAS algorithms despite its significant importance in computer\nvision. To the best of our knowledge, most of the recent NAS studies on object\ndetection tasks fail to satisfactorily strike a balance between performance and\nefficiency of the resulting models, let alone the excessive amount of\ncomputational resources cost by those algorithms. Here we propose an efficient\nmethod to obtain better object detectors by searching for the feature pyramid\nnetwork (FPN) as well as the prediction head of a simple anchor-free object\ndetector, namely, FCOS [36], using a tailored reinforcement learning paradigm.\nWith carefully designed search space, search algorithms, and strategies for\nevaluating network quality, we are able to find top-performing detection\narchitectures within 4 days using 8 V100 GPUs. The discovered architectures\nsurpass state-of-the-art object detection models (such as Faster R-CNN,\nRetina-Net and, FCOS) by 1.0% to 5.4% points in AP on the COCO dataset, with\ncomparable computation complexity and memory footprint, demonstrating the\nefficacy of the proposed NAS method for object detection. Code is available at\nhttps://github.com/Lausannen/NAS-FCOS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanning Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Based CLIP-Guided Essence Transfer. (arXiv:2110.12427v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12427","description":"<p>CLIP is trained on a large corpus of matched images and text captions and is,\ntherefore, much richer semantically than networks that perform multiclass\nclassification for a limited number of classes only. It has been shown to be\nextremely suitable for zero-shot computer vision tasks; here, we demonstrate\nits ability to support semantic blending. While the StyleGAN space already\nperforms reasonable blending for images of, e.g., two children, it struggles\nwhen blending images with different attributes. On the other hand, CLIP by\nitself struggles to maintain identity when blending. The combination of the two\nseems to provide a powerful blending technique, which enjoys the benefits of\nboth representations. This is enabled through a novel method, which assumes\nadditivity in the first latent space and ensures additivity in the second\nthrough optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1\">Hila Chefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paiss_R/0/1/0/all/0/1\">Roni Paiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WARPd: A linearly convergent first-order method for inverse problems with approximate sharpness conditions. (arXiv:2110.12437v1 [math.NA])","link":"http://arxiv.org/abs/2110.12437","description":"<p>Reconstruction of signals from undersampled and noisy measurements is a topic\nof considerable interest. Sharpness conditions directly control the recovery\nperformance of restart schemes for first-order methods without the need for\nrestrictive assumptions such as strong convexity. However, they are challenging\nto apply in the presence of noise or approximate model classes (e.g.,\napproximate sparsity). We provide a first-order method: Weighted, Accelerated\nand Restarted Primal-dual (WARPd), based on primal-dual iterations and a novel\nrestart-reweight scheme. Under a generic approximate sharpness condition, WARPd\nachieves stable linear convergence to the desired vector. Many problems of\ninterest fit into this framework. For example, we analyze sparse recovery in\ncompressed sensing, low-rank matrix recovery, matrix completion, TV\nregularization, minimization of $\\|Bx\\|_{l^1}$ under constraints\n($l^1$-analysis problems for general $B$), and mixed regularization problems.\nWe show how several quantities controlling recovery performance also provide\nexplicit approximate sharpness constants. Numerical experiments show that WARPd\ncompares favorably with specialized state-of-the-art methods and is ideally\nsuited for solving large-scale problems. We also present a noise-blind variant\nbased on the Square-Root LASSO decoder. Finally, we show how to unroll WARPd as\nneural networks. This approximation theory result provides lower bounds for\nstable and accurate neural networks for inverse problems and sheds light on\narchitecture choices. Code and a gallery of examples are made available online\nas a MATLAB package.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/math/1/au:+Colbrook_M/0/1/0/all/0/1\">Matthew J. Colbrook</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bangla Image Caption Generation through CNN-Transformer based Encoder-Decoder Network. (arXiv:2110.12442v1 [cs.CV])","link":"http://arxiv.org/abs/2110.12442","description":"<p>Automatic Image Captioning is the never-ending effort of creating\nsyntactically and validating the accuracy of textual descriptions of an image\nin natural language with context. The encoder-decoder structure used throughout\nexisting Bengali Image Captioning (BIC) research utilized abstract image\nfeature vectors as the encoder's input. We propose a novel transformer-based\narchitecture with an attention mechanism with a pre-trained ResNet-101 model\nimage encoder for feature extraction from images. Experiments demonstrate that\nthe language decoder in our technique captures fine-grained information in the\ncaption and, then paired with image features, produces accurate and diverse\ncaptions on the BanglaLekhaImageCaptions dataset. Our approach outperforms all\nexisting Bengali Image Captioning work and sets a new benchmark by scoring\n0.694 on BLEU-1, 0.630 on BLEU-2, 0.582 on BLEU-3, and 0.337 on METEOR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palash_M/0/1/0/all/0/1\">Md Aminul Haque Palash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sourav Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Afrin_F/0/1/0/all/0/1\">Faria Afrin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallik_R/0/1/0/all/0/1\">Raisa Mallik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samiappan_S/0/1/0/all/0/1\">Sathishkumar Samiappan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Edge Detection with Diverse Deep Supervision. (arXiv:1804.02864v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1804.02864","description":"<p>Semantic edge detection (SED), which aims at jointly extracting edges as well\nas their category information, has far-reaching applications in domains such as\nsemantic segmentation, object proposal generation, and object recognition. SED\nnaturally requires achieving two distinct supervision targets: locating fine\ndetailed edges and identifying high-level semantics. Our motivation comes from\nthe hypothesis that such distinct targets prevent state-of-the-art SED methods\nfrom effectively using deep supervision to improve results. To this end, we\npropose a novel fully convolutional neural network using diverse deep\nsupervision (DDS) within a multi-task framework where bottom layers aim at\ngenerating category-agnostic edges, while top layers are responsible for the\ndetection of category-aware semantic edges. To overcome the hypothesized\nsupervision challenge, a novel information converter unit is introduced, whose\neffectiveness has been extensively evaluated on SBD and Cityscapes datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Ming-Ming Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Le Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1\">JiaWang Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Recognize Once: Towards Fast Video Text Spotting. (arXiv:1903.03299v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1903.03299","description":"<p>Video text spotting is still an important research topic due to its various\nreal-applications. Previous approaches usually fall into the four-staged\npipeline: text detection in individual images, framewisely recognizing\nlocalized text regions, tracking text streams and generating final results with\ncomplicated post-processing skills, which might suffer from the huge\ncomputational cost as well as the interferences of low-quality text. In this\npaper, we propose a fast and robust video text spotting framework by only\nrecognizing the localized text one-time instead of frame-wisely recognition.\nSpecifically, we first obtain text regions in videos with a well-designed\nspatial-temporal detector. Then we concentrate on developing a novel text\nrecommender for selecting the highest-quality text from text streams and only\nrecognizing the selected ones. Here, the recommender assembles text tracking,\nquality scoring and recognition into an end-to-end trainable module, which not\nonly avoids the interferences from low-quality text but also dramatically\nspeeds up the video text spotting process. In addition, we collect a larger\nscale video text dataset (LSVTD) for promoting the video text spotting\ncommunity, which contains 100 text videos from 22 different real-life\nscenarios. Extensive experiments on two public benchmarks show that our method\ngreatly speeds up the recognition process averagely by 71 times compared with\nthe frame-wise manner, and also achieves the remarkable state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuigeng Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Image Classification Using Coupled Dictionary Embedding. (arXiv:1906.10509v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1906.10509","description":"<p>Zero-shot learning (ZSL) is a framework to classify images belonging to\nunseen classes based on solely semantic information about these unseen classes.\nIn this paper, we propose a new ZSL algorithm using coupled dictionary\nlearning. The core idea is that the visual features and the semantic attributes\nof an image can share the same sparse representation in an intermediate space.\nWe use images from seen classes and semantic attributes from seen and unseen\nclasses to learn two dictionaries that can represent sparsely the visual and\nsemantic feature vectors of an image. In the ZSL testing stage and in the\nabsence of labeled data, images from unseen classes can be mapped into the\nattribute space by finding the joint sparse representation using solely the\nvisual data. The image is then classified in the attribute space given semantic\ndescriptions of unseen classes. We also provide an attribute-aware formulation\nto tackle domain shift and hubness problems in ZSL. Extensive experiments are\nprovided to demonstrate the superior performance of our approach against the\nstate of the art ZSL algorithms on benchmark ZSL datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolouri_S/0/1/0/all/0/1\">Soheil Kolouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murez_Z/0/1/0/all/0/1\">Zak Murez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Owekcho_Y/0/1/0/all/0/1\">Yuri Owekcho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1\">Eric Eaton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kuyngnam Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PhIT-Net: Photo-consistent Image Transform for Robust Illumination Invariant Matching. (arXiv:1911.12641v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1911.12641","description":"<p>We propose a new and completely data-driven approach for generating a\nphoto-consistent image transform. We show that simple classical algorithms\nwhich operate in the transform domain become extremely resilient to\nillumination changes. This considerably improves matching accuracy,\noutperforming the use of state-of-the-art invariant representations as well as\nnew matching methods based on deep features. The transform is obtained by\ntraining a neural network with a specialized triplet loss, designed to\nemphasize actual scene changes while attenuating illumination changes. The\ntransform yields an illumination invariant representation, structured as an\nimage map, which is highly flexible and can be easily used for various tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaliroff_D/0/1/0/all/0/1\">Damian Kaliroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gilboa_G/0/1/0/all/0/1\">Guy Gilboa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Malware Makeover: Breaking ML-based Static Analysis by Modifying Executable Bytes. (arXiv:1912.09064v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/1912.09064","description":"<p>Motivated by the transformative impact of deep neural networks (DNNs) in\nvarious domains, researchers and anti-virus vendors have proposed DNNs for\nmalware detection from raw bytes that do not require manual feature\nengineering. In this work, we propose an attack that interweaves\nbinary-diversification techniques and optimization frameworks to mislead such\nDNNs while preserving the functionality of binaries. Unlike prior attacks, ours\nmanipulates instructions that are a functional part of the binary, which makes\nit particularly challenging to defend against. We evaluated our attack against\nthree DNNs in white- and black-box settings, and found that it often achieved\nsuccess rates near 100%. Moreover, we found that our attack can fool some\ncommercial anti-viruses, in certain cases with a success rate of 85%. We\nexplored several defenses, both new and old, and identified some that can foil\nover 80% of our evasion attempts. However, these defenses may still be\nsusceptible to evasion by attacks, and so we advocate for augmenting\nmalware-detection systems with methods that do not rely on machine learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lucas_K/0/1/0/all/0/1\">Keane Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharif_M/0/1/0/all/0/1\">Mahmood Sharif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_L/0/1/0/all/0/1\">Lujo Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_M/0/1/0/all/0/1\">Michael K. Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shintre_S/0/1/0/all/0/1\">Saurabh Shintre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn to Predict Sets Using Feed-Forward Neural Networks. (arXiv:2001.11845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.11845","description":"<p>This paper addresses the task of set prediction using deep feed-forward\nneural networks. A set is a collection of elements which is invariant under\npermutation and the size of a set is not fixed in advance. Many real-world\nproblems, such as image tagging and object detection, have outputs that are\nnaturally expressed as sets of entities. This creates a challenge for\ntraditional deep neural networks which naturally deal with structured outputs\nsuch as vectors, matrices or tensors. We present a novel approach for learning\nto predict sets with unknown permutation and cardinality using deep neural\nnetworks. In our formulation we define a likelihood for a set distribution\nrepresented by a) two discrete distributions defining the set cardinally and\npermutation variables, and b) a joint distribution over set elements with a\nfixed cardinality. Depending on the problem under consideration, we define\ndifferent training models for set prediction using deep neural networks. We\ndemonstrate the validity of our set formulations on relevant vision problems\nsuch as: 1) multi-label image classification where we outperform the other\ncompeting methods on the PASCAL VOC and MS COCO datasets, 2) object detection,\nfor which our formulation outperforms popular state-of-the-art detectors, and\n3) a complex CAPTCHA test, where we observe that, surprisingly, our set-based\nnetwork acquired the ability of mimicking arithmetics without any rules being\ncoded.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1\">Tianyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaskman_R/0/1/0/all/0/1\">Roman Kaskman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlagh_F/0/1/0/all/0/1\">Farbod T. Motlagh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qinfeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milan_A/0/1/0/all/0/1\">Anton Milan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1\">Daniel Cremers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text Perceptron: Towards End-to-End Arbitrary-Shaped Text Spotting. (arXiv:2002.06820v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.06820","description":"<p>Many approaches have recently been proposed to detect irregular scene text\nand achieved promising results. However, their localization results may not\nwell satisfy the following text recognition part mainly because of two reasons:\n1) recognizing arbitrary shaped text is still a challenging task, and 2)\nprevalent non-trainable pipeline strategies between text detection and text\nrecognition will lead to suboptimal performances. To handle this\nincompatibility problem, in this paper we propose an end-to-end trainable text\nspotting approach named Text Perceptron. Concretely, Text Perceptron first\nemploys an efficient segmentation-based text detector that learns the latent\ntext reading order and boundary information. Then a novel Shape Transform\nModule (abbr. STM) is designed to transform the detected feature regions into\nregular morphologies without extra parameters. It unites text detection and the\nfollowing recognition part into a whole framework, and helps the whole network\nachieve global optimization. Experiments show that our method achieves\ncompetitive performance on two standard text benchmarks, i.e., ICDAR 2013 and\nICDAR 2015, and also obviously outperforms existing methods on irregular text\nbenchmarks SCUT-CTW1500 and Total-Text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sanli Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Causality-Aware Inferring: A Sequential Discriminative Approach for Medical Automatic Diagnosis. (arXiv:2003.06534v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.06534","description":"<p>Through learning from the patient simulator built on the collected\npatient-doctor dialogues records, medical automatic diagnosis (MAD) aims to\nbuild an interactive diagnostic agent to sequentially inquire about symptoms\nfor discriminating diseases. However, due to some task-unrelated and non-causal\nassociations in these collected data, e.g., the preference of the collectors,\nthe simulator is probably biased against the disease-symptom causality and the\ndiagnostic agent might be hindered from capturing the transportable knowledge.\nThis work attempts to address these critical issues in MAD by taking advantage\nof the structural causal model (SCM) to identify and resolve two representative\nnon-causal biases, i.e., (i) default-answer bias and (ii) distributional\ninquiry bias, from the aspects of the data usage and the agent design,\nrespectively. Specifically, Bias (i) originates from that the patient simulator\ntries to answer unrecorded inquiries with default answers, which cannot be\nresolved by feeding more data [1]. Suffering from the biased simulator,\nprevious MAD methods cannot fully demonstrate their advantages. To eliminate\nthis bias and inspired by the propensity score matching technique with SCM, we\npropose a propensity-based patient simulator to effectively answer unrecorded\ninquiry by drawing knowledge from the other records; Bias (ii) inherently comes\nalong with the passive manner of collecting MAD data. To this end, we propose a\nprogressive assurance agent, which includes the dual processes accounting for\nsymptom inquiry and disease diagnosis. The inquiry process is driven by the\ndiagnosis process in a top-down manner to inquire about symptoms for enhancing\ndiagnostic confidence. The diagnosis process can reason within that mental\nrepresentation by intervening with imaginary questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Junfan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Keze Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ziliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Night-time Scene Parsing with a Large Real Dataset. (arXiv:2003.06883v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2003.06883","description":"<p>Although huge progress has been made on scene analysis in recent years, most\nexisting works assume the input images to be in day-time with good lighting\nconditions. In this work, we aim to address the night-time scene parsing (NTSP)\nproblem, which has two main challenges: 1) labeled night-time data are scarce,\nand 2) over- and under-exposures may co-occur in the input night-time images\nand are not explicitly modeled in existing pipelines. To tackle the scarcity of\nnight-time data, we collect a novel labeled dataset, named {\\it NightCity}, of\n4,297 real night-time images with ground truth pixel-level semantic\nannotations. To our knowledge, NightCity is the largest dataset for NTSP. In\naddition, we also propose an exposure-aware framework to address the NTSP\nproblem through augmenting the segmentation process with explicitly learned\nexposure features. Extensive experiments show that training on NightCity can\nsignificantly improve NTSP performances and that our exposure-aware model\noutperforms the state-of-the-art methods, yielding top performances on our\ndataset as well as existing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Ying Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yiheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolution-Weight-Distribution Assumption: Rethinking the Criteria of Channel Pruning. (arXiv:2004.11627v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2004.11627","description":"<p>Channel pruning is a popular technique for compressing convolutional neural\nnetworks (CNNs), where various pruning criteria have been proposed to remove\nthe redundant filters. From our comprehensive experiments, we found two blind\nspots in the study of pruning criteria: (1) Similarity: There are some strong\nsimilarities among several primary pruning criteria that are widely cited and\ncompared. According to these criteria, the ranks of filters'Importance Score\nare almost identical, resulting in similar pruned structures. (2)\nApplicability: The filters'Importance Score measured by some pruning criteria\nare too close to distinguish the network redundancy well. In this paper, we\nanalyze these two blind spots on different types of pruning criteria with\nlayer-wise pruning or global pruning. The analyses are based on the empirical\nexperiments and our assumption (Convolutional Weight Distribution Assumption)\nthat the well-trained convolutional filters each layer approximately follow a\nGaussian-alike distribution. This assumption has been verified through\nsystematic and extensive statistical tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhongzhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wenqi Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinjiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPIN: Structure-Preserving Inner Offset Network for Scene Text Recognition. (arXiv:2005.13117v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.13117","description":"<p>Arbitrary text appearance poses a great challenge in scene text recognition\ntasks. Existing works mostly handle with the problem in consideration of the\nshape distortion, including perspective distortions, line curvature or other\nstyle variations. Therefore, methods based on spatial transformers are\nextensively studied. However, chromatic difficulties in complex scenes have not\nbeen paid much attention on. In this work, we introduce a new learnable\ngeometric-unrelated module, the Structure-Preserving Inner Offset Network\n(SPIN), which allows the color manipulation of source data within the network.\nThis differentiable module can be inserted before any recognition architecture\nto ease the downstream tasks, giving neural networks the ability to actively\ntransform input intensity rather than the existing spatial rectification. It\ncan also serve as a complementary module to known spatial transformations and\nwork in both independent and collaborative ways with them. Extensive\nexperiments show that the use of SPIN results in a significant improvement on\nmultiple text recognition benchmarks compared to the state-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chengwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_F/0/1/0/all/0/1\">Futai Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TRIE: End-to-End Text Reading and Information Extraction for Document Understanding. (arXiv:2005.13118v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2005.13118","description":"<p>Since real-world ubiquitous documents (e.g., invoices, tickets, resumes and\nleaflets) contain rich information, automatic document image understanding has\nbecome a hot topic. Most existing works decouple the problem into two separate\ntasks, (1) text reading for detecting and recognizing texts in images and (2)\ninformation extraction for analyzing and extracting key elements from\npreviously extracted plain text. However, they mainly focus on improving\ninformation extraction task, while neglecting the fact that text reading and\ninformation extraction are mutually correlated. In this paper, we propose a\nunified end-to-end text reading and information extraction network, where the\ntwo tasks can reinforce each other. Specifically, the multimodal visual and\ntextual features of text reading are fused for information extraction and in\nturn, the semantics in information extraction contribute to the optimization of\ntext reading. On three real-world datasets with diverse document images (from\nfixed layout to variable layout, from structured text to semi-structured text),\nour proposed method significantly outperforms the state-of-the-art methods in\nboth efficiency and accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Spatio-temporal Latent Feature Clustering for Multiple-object Tracking and Segmentation. (arXiv:2007.07175v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.07175","description":"<p>Assigning consistent temporal identifiers to multiple moving objects in a\nvideo sequence is a challenging problem. A solution to that problem would have\nimmediate ramifications in multiple object tracking and segmentation problems.\nWe propose a strategy that treats the temporal identification task as a\nspatio-temporal clustering problem. We propose an unsupervised learning\napproach using a convolutional and fully connected autoencoder, which we call\ndeep heterogeneous autoencoder, to learn discriminative features from\nsegmentation masks and detection bounding boxes. We extract masks and their\ncorresponding bounding boxes from a pretrained instance segmentation network\nand train the autoencoders jointly using task-dependent uncertainty weights to\ngenerate common latent features. We then construct constraints graphs that\nencourage associations among objects that satisfy a set of known temporal\nconditions. The feature vectors and the constraints graphs are then provided to\nthe kmeans clustering algorithm to separate the corresponding data points in\nthe latent space. We evaluate the performance of our method using challenging\nsynthetic and real-world multiple-object video datasets. Our results show that\nour technique outperforms several state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siddique_A/0/1/0/all/0/1\">Abubakar Siddique</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mozhdehi_R/0/1/0/all/0/1\">Reza Jalil Mozhdehi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medeiros_H/0/1/0/all/0/1\">Henry Medeiros</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CenterNet3D: An Anchor Free Object Detector for Point Cloud. (arXiv:2007.07214v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2007.07214","description":"<p>Accurate and fast 3D object detection from point clouds is a key task in\nautonomous driving. Existing one-stage 3D object detection methods can achieve\nreal-time performance, however, they are dominated by anchor-based detectors\nwhich are inefficient and require additional post-processing. In this paper, we\neliminate anchors and model an object as a single point--the center point of\nits bounding box. Based on the center point, we propose an anchor-free\nCenterNet3D network that performs 3D object detection without anchors. Our\nCenterNet3D uses keypoint estimation to find center points and directly\nregresses 3D bounding boxes. However, because inherent sparsity of point\nclouds, 3D object center points are likely to be in empty space which makes it\ndifficult to estimate accurate boundaries. To solve this issue, we propose an\nextra corner attention module to enforce the CNN backbone to pay more attention\nto object boundaries. Besides, considering that one-stage detectors suffer from\nthe discordance between the predicted bounding boxes and corresponding\nclassification confidences, we develop an efficient keypoint-sensitive warping\noperation to align the confidences to the predicted bounding boxes. Our\nproposed CenterNet3D is non-maximum suppression free which makes it more\nefficient and simpler. We evaluate CenterNet3D on the widely used KITTI dataset\nand more challenging nuScenes dataset. Our method outperforms all\nstate-of-the-art anchor-based one-stage methods and has comparable performance\nto two-stage methods as well. It has an inference speed of 20 FPS and achieves\nthe best speed and accuracy trade-off. Our source code will be released at\nhttps://github.com/wangguojun2018/CenterNet3d.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guojun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1\">Bin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1\">Siyu Teng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongpu Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Face Image Quality Assessment: A Literature Survey. (arXiv:2009.01103v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.01103","description":"<p>The performance of face analysis and recognition systems depends on the\nquality of the acquired face data, which is influenced by numerous factors.\nAutomatically assessing the quality of face data in terms of biometric utility\ncan thus be useful to detect low-quality data and make decisions accordingly.\nThis survey provides an overview of the face image quality assessment\nliterature, which predominantly focuses on visible wavelength face image input.\nA trend towards deep learning based methods is observed, including notable\nconceptual differences among the recent approaches, such as the integration of\nquality assessment into face recognition models. Besides image selection, face\nimage quality assessment can also be used in a variety of other application\nscenarios, which are discussed herein. Open issues and challenges are pointed\nout, i.a. highlighting the importance of comparability for algorithm\nevaluations, and the challenge for future work to create deep learning\napproaches that are interpretable in addition to providing accurate utility\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schlett_T/0/1/0/all/0/1\">Torsten Schlett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henniger_O/0/1/0/all/0/1\">Olaf Henniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1\">Javier Galbally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1\">Julian Fierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360-Degree Gaze Estimation in the Wild Using Multiple Zoom Scales. (arXiv:2009.06924v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.06924","description":"<p>Gaze estimation involves predicting where the person is looking at within an\nimage or video. Technically, the gaze information can be inferred from two\ndifferent magnification levels: face orientation and eye orientation. The\ninference is not always feasible for gaze estimation in the wild, given the\nlack of clear eye patches in conditions like extreme left/right gazes or\nocclusions. In this work, we design a model that mimics humans' ability to\nestimate the gaze by aggregating from focused looks, each at a different\nmagnification level of the face area. The model avoids the need to extract\nclear eye patches and at the same time addresses another important issue of\nface-scale variation for gaze estimation in the wild. We further extend the\nmodel to handle the challenging task of 360-degree gaze estimation by encoding\nthe backward gazes in the polar representation along with a robust averaging\nscheme. Experiment results on the ETH-XGaze dataset, which does not contain\nscale-varying faces, demonstrate the model's effectiveness to assimilate\ninformation from multiple scales. For other benchmark datasets with many\nscale-varying faces (Gaze360 and RT-GENE), the proposed model achieves\nstate-of-the-art performance for gaze estimation when using either images or\nvideos. Our code and pretrained models can be accessed at\nhttps://github.com/ashesh-0/MultiZoomGaze.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ashesh/0/1/0/all/0/1\">Ashesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chu-Song Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsuan-Tien Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weak-shot Fine-grained Classification via Similarity Transfer. (arXiv:2009.09197v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.09197","description":"<p>Recognizing fine-grained categories remains a challenging task, due to the\nsubtle distinctions among different subordinate categories, which results in\nthe need of abundant annotated samples. To alleviate the data-hungry problem,\nwe consider the problem of learning novel categories from web data with the\nsupport of a clean set of base categories, which is referred to as weak-shot\nlearning. In this setting, we propose a method called SimTrans to transfer\npairwise semantic similarity from base categories to novel categories.\nSpecifically, we firstly train a similarity net on clean data, and then\nleverage the transferred similarity to denoise web training data using two\nsimple yet effective strategies. In addition, we apply adversarial loss on\nsimilarity net to enhance the transferability of similarity. Comprehensive\nexperiments demonstrate the effectiveness of our weak-shot setting and our\nSimTrans method. Datasets and codes are available at\nhttps://github.com/bcmi/SimTrans-Weak-Shot-Classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1\">Li Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liqing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty Sets for Image Classifiers using Conformal Prediction. (arXiv:2009.14193v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.14193","description":"<p>Convolutional image classifiers can achieve high predictive accuracy, but\nquantifying their uncertainty remains an unresolved challenge, hindering their\ndeployment in consequential settings. Existing uncertainty quantification\ntechniques, such as Platt scaling, attempt to calibrate the network's\nprobability estimates, but they do not have formal guarantees. We present an\nalgorithm that modifies any classifier to output a predictive set containing\nthe true label with a user-specified probability, such as 90%. The algorithm is\nsimple and fast like Platt scaling, but provides a formal finite-sample\ncoverage guarantee for every model and dataset. Our method modifies an existing\nconformal prediction algorithm to give more stable predictive sets by\nregularizing the small scores of unlikely classes after Platt scaling. In\nexperiments on both Imagenet and Imagenet-V2 with ResNet-152 and other\nclassifiers, our scheme outperforms existing approaches, achieving coverage\nwith sets that are often factors of 5 to 10 smaller than a stand-alone Platt\nscaling baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1\">Anastasios Angelopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1\">Stephen Bates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Back to the Future: Cycle Encoding Prediction for Self-supervised Contrastive Video Representation Learning. (arXiv:2010.07217v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.07217","description":"<p>In this paper we show that learning video feature spaces in which temporal\ncycles are maximally predictable benefits action classification. In particular,\nwe propose a novel learning approach termed Cycle Encoding Prediction (CEP)\nthat is able to effectively represent high-level spatio-temporal structure of\nunlabelled video content. CEP builds a latent space wherein the concept of\nclosed forward-backward as well as backward-forward temporal loops is\napproximately preserved. As a self-supervision signal, CEP leverages the\nbi-directional temporal coherence of the video stream and applies loss\nfunctions that encourage both temporal cycle closure as well as contrastive\nfeature separation. Architecturally, the underpinning network structure\nutilises a single feature encoder for all video snippets, adding two predictive\nmodules that learn temporal forward and backward transitions. We apply our\nframework for pretext training of networks for action recognition tasks. We\nreport significantly improved results for the standard datasets UCF101 and\nHMDB51. Detailed ablation studies support the effectiveness of the proposed\ncomponents. We publish source code for the CEP components in full with this\npaper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirmehdi_M/0/1/0/all/0/1\">Majid Mirmehdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intriguing Properties of Contrastive Losses. (arXiv:2011.02803v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2011.02803","description":"<p>We study three intriguing properties of contrastive learning. First, we\ngeneralize the standard contrastive loss to a broader family of losses, and we\nfind that various instantiations of the generalized loss perform similarly\nunder the presence of a multi-layer non-linear projection head. Second, we\nstudy if instance-based contrastive learning (with a global image\nrepresentation) can learn well on images with multiple objects present. We find\nthat meaningful hierarchical local features can be learned despite the fact\nthat these objectives operate on global instance-level features. Finally, we\nstudy the phenomenon of feature suppression among competing features shared\nacross augmented views, such as \"color distribution\" vs \"object class\". We\nconstruct datasets with explicit and controllable competing features, and show\nthat, for contrastive learning, a few bits of easy-to-learn shared features can\nsuppress, and even fully prevent, the learning of other sets of competing\nfeatures. In scenarios where there are multiple objects in an image, the\ndominant object would suppress the learning of smaller objects. Existing\ncontrastive learning methods critically rely on data augmentation to favor\ncertain sets of features over others, and could suffer from learning saturation\nfor scenarios where existing augmentations cannot fully address the feature\nsuppression. This poses open challenges to existing contrastive learning\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1\">Calvin Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lala Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale 2D Temporal Adjacent Networks for Moment Localization with Natural Language. (arXiv:2012.02646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02646","description":"<p>We address the problem of retrieving a specific moment from an untrimmed\nvideo by natural language. It is a challenging problem because a target moment\nmay take place in the context of other temporal moments in the untrimmed video.\nExisting methods cannot tackle this challenge well since they do not fully\nconsider the temporal contexts between temporal moments. In this paper, we\nmodel the temporal context between video moments by a set of predefined\ntwo-dimensional maps under different temporal scales. For each map, one\ndimension indicates the starting time of a moment and the other indicates the\nduration. These 2D temporal maps can cover diverse video moments with different\nlengths, while representing their adjacent contexts at different temporal\nscales. Based on the 2D temporal maps, we propose a Multi-Scale Temporal\nAdjacent Network (MS-2D-TAN), a single-shot framework for moment localization.\nIt is capable of encoding the adjacent temporal contexts at each scale, while\nlearning discriminative features for matching video moments with referring\nexpressions. We evaluate the proposed MS-2D-TAN on three challenging\nbenchmarks, i.e., Charades-STA, ActivityNet Captions, and TACoS, where our\nMS-2D-TAN outperforms the state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jiebo Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MANGO: A Mask Attention Guided One-Stage Scene Text Spotter. (arXiv:2012.04350v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04350","description":"<p>Recently end-to-end scene text spotting has become a popular research topic\ndue to its advantages of global optimization and high maintainability in real\napplications. Most methods attempt to develop various region of interest (RoI)\noperations to concatenate the detection part and the sequence recognition part\ninto a two-stage text spotting framework. However, in such framework, the\nrecognition part is highly sensitive to the detected results (e.g.), the\ncompactness of text contours). To address this problem, in this paper, we\npropose a novel Mask AttentioN Guided One-stage text spotting framework named\nMANGO, in which character sequences can be directly recognized without RoI\noperation. Concretely, a position-aware mask attention module is developed to\ngenerate attention weights on each text instance and its characters. It allows\ndifferent text instances in an image to be allocated on different feature map\nchannels which are further grouped as a batch of instance features. Finally, a\nlightweight sequence decoder is applied to generate the character sequences. It\nis worth noting that MANGO inherently adapts to arbitrary-shaped text spotting\nand can be trained end-to-end with only coarse position information (e.g.),\nrectangular bounding box) and text annotations. Experimental results show that\nthe proposed method achieves competitive and even new state-of-the-art\nperformance on both regular and irregular text spotting benchmarks, i.e., ICDAR\n2013, ICDAR 2015, Total-Text, and SCUT-CTW1500.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spectral Analysis for Semantic Segmentation with Applications on Feature Truncation and Weak Annotation. (arXiv:2012.14123v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.14123","description":"<p>We propose spectral analysis to investigate the correlation between the\naccuracy and the resolution of segmentation maps for semantic segmentation. The\ncurrent networks predict segmentation maps on the down-sampled grid of images\nto alleviate the computational cost. Moreover, these networks can be trained by\nweak annotations that utilize only the coarse contour of segmentation maps.\nDespite the successful achievement of these works utilizing the low-frequency\ninformation of segmentation maps, however, the accuracy of resultant\nsegmentation maps may also be degraded in the regions near object boundaries.\nIt is yet unclear for a theoretical guideline to determine an optimal\ndown-sampled grid to strike the balance between the cost and the accuracy of\nsegmentation. We analyze the objective function (cross-entropy) and network\nback-propagation process in frequency domain. We discover that cross-entropy\nand key features of CNN are mainly contributed by the low-frequency components\nof segmentation maps. This further provides us quantitative results to\ndetermine the efficacy of down-sampled grid of segmentation maps. The analysis\nis then validated on the two applications: the feature truncation method and\nthe block-wise annotation that limit the high-frequency components of the CNN\nfeatures and annotation, respectively. The results agree with our analysis.\nThus the success of the existing work utilizing low-frequency information of\nsegmentation maps now has theoretical foundation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Li-Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chin-Tien Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surprisingly Simple Semi-Supervised Domain Adaptation with Pretraining and Consistency. (arXiv:2101.12727v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.12727","description":"<p>Most modern unsupervised domain adaptation (UDA) approaches are rooted in\ndomain alignment, i.e., learning to align source and target features to learn a\ntarget domain classifier using source labels. In semi-supervised domain\nadaptation (SSDA), when the learner can access few target domain labels, prior\napproaches have followed UDA theory to use domain alignment for learning. We\nshow that the case of SSDA is different and a good target classifier can be\nlearned without needing alignment. We use self-supervised pretraining (via\nrotation prediction) and consistency regularization to achieve well separated\ntarget clusters, aiding in learning a low error target classifier. With our\nPretraining and Consistency (PAC) approach, we achieve state of the art target\naccuracy on this semi-supervised domain adaptation task, surpassing multiple\nadversarial domain alignment methods, across multiple datasets. PAC, while\nusing simple techniques, performs remarkably well on large and challenging SSDA\nbenchmarks like DomainNet and Visda-17, often outperforming recent state of the\nart by sizeable margins. Code for our experiments can be found at\nhttps://github.com/venkatesh-saligrama/PAC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Samarth Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1\">Kate Saenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saligrama_V/0/1/0/all/0/1\">Venkatesh Saligrama</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge. (arXiv:2102.02711v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.02711","description":"<p>Dynamic imaging is a beneficial tool for interventions to assess\nphysiological changes. Nonetheless during dynamic MRI, while achieving a high\ntemporal resolution, the spatial resolution is compromised. To overcome this\nspatio-temporal trade-off, this research presents a super-resolution (SR) MRI\nreconstruction with prior knowledge based fine-tuning to maximise spatial\ninformation while reducing the required scan-time for dynamic MRIs. An U-Net\nbased network with perceptual loss is trained on a benchmark dataset and\nfine-tuned using one subject-specific static high resolution MRI as prior\nknowledge to obtain high resolution dynamic images during the inference stage.\n3D dynamic data for three subjects were acquired with different parameters to\ntest the generalisation capabilities of the network. The method was tested for\ndifferent levels of in-plane undersampling for dynamic MRI. The reconstructed\ndynamic SR results after fine-tuning showed higher similarity with the high\nresolution ground-truth, while quantitatively achieving statistically\nsignificant improvement. The average SSIM of the lowest resolution experimented\nduring this research (6.25~\\% of the k-space) before and after fine-tuning were\n0.939 $\\pm$ 0.008 and 0.957 $\\pm$ 0.006 respectively. This could theoretically\nresult in an acceleration factor of 16, which can potentially be acquired in\nless than half a second. The proposed approach shows that the super-resolution\nMRI reconstruction with prior-information can alleviate the spatio-temporal\ntrade-off in dynamic MRI, even for high acceleration factors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sarasaen_C/0/1/0/all/0/1\">Chompunuch Sarasaen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Breitkopf_M/0/1/0/all/0/1\">Mario Breitkopf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery Ticket Perspective. (arXiv:2103.00397v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.00397","description":"<p>Training generative adversarial networks (GANs) with limited real image data\ngenerally results in deteriorated performance and collapsed models. To conquer\nthis challenge, we are inspired by the latest observation, that one can\ndiscover independently trainable and highly sparse subnetworks (a.k.a., lottery\ntickets) from GANs. Treating this as an inductive prior, we suggest a brand-new\nangle towards data-efficient GAN training: by first identifying the lottery\nticket from the original GAN using the small training set of real images; and\nthen focusing on training that sparse subnetwork by re-using the same set. We\nfind our coordinated framework to offer orthogonal gains to existing real image\ndata augmentation methods, and we additionally present a new feature-level\naugmentation that can be applied together with them. Comprehensive experiments\nendorse the effectiveness of our proposed framework, across various GAN\narchitectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10,\nCIFAR-100, Tiny-ImageNet, ImageNet, and multiple few-shot generation datasets).\nCodes are available at:\nhttps://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPICE: Semantic Pseudo-labeling for Image Clustering. (arXiv:2103.09382v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09382","description":"<p>The similarity among samples and the discrepancy between clusters are two\ncrucial aspects of image clustering. However, current deep clustering methods\nsuffer from the inaccurate estimation of either feature similarity or semantic\ndiscrepancy. In this paper, we present a Semantic Pseudo-labeling-based Image\nClustEring (SPICE) framework, which divides the clustering network into a\nfeature model for measuring the instance-level similarity and a clustering head\nfor identifying the cluster-level discrepancy. We design two semantics-aware\npseudo-labeling algorithms, prototype pseudo-labeling, and reliable\npseudo-labeling, which enable accurate and reliable self-supervision over\nclustering. Without using any ground-truth label, we optimize the clustering\nnetwork in three stages: 1) train the feature model through contrastive\nlearning to measure the instance similarity, 2) train the clustering head with\nthe prototype pseudo-labeling algorithm to identify cluster semantics, and 3)\njointly train the feature model and clustering head with the reliable\npseudo-labeling algorithm to improve the clustering performance. Extensive\nexperimental results demonstrate that SPICE achieves significant improvements\n(~10%) over existing methods and establishes the new state-of-the-art\nclustering results on six image benchmark datasets in terms of three popular\nmetrics. Importantly, SPICE significantly reduces the gap between unsupervised\nand fully-supervised classification; e.g., there is only a 2% (91.8% vs 93.8%)\naccuracy difference on CIFAR-10. Our code has been made publically available at\nhttps://github.com/niuchuangnn/SPICE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_C/0/1/0/all/0/1\">Chuang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Hongming Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harmonic Beltrami Signature: A Novel 2D Shape Representation for Object Classification. (arXiv:2103.16411v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16411","description":"<p>There is a growing interest in shape analysis in recent years. We present a\nnovel shape signature for 2D bounded simply-connected domains, named the\nHarmonic Beltrami signature (HBS). The proposed signature is based on the\nharmonic extension of the conformal welding map of a unit circle and its\nBeltrami coefficient. We show that there is a one-to-one correspondence between\nthe quotient space of HBS and the space of 2D simply-connected shapes up to a\ntranslation, rotation and scaling. With a suitable normalization, each\nequivalence class in the quotient space of HBS is associated to a unique\nrepresentative. It gets rid of the conformal ambiguity. As such, each shape is\nassociated to a unique HBS. Conversely, the associated shape of a HBS can be\nreconstructed based on quasiconformal Teichmuller theories, which is uniquely\ndetermined up to a translation, rotation and scaling. The HBS is thus an\neffective fingerprint to represent a 2D shape. The robustness of HBS is studied\nboth theoretically and experimentally. With the HBS, simple metric, such as L2,\ncan be used to measure geometric dissimilarity between shapes. Experiments have\nbeen carried out to classify shapes in different classes using HBS. Results\nshow good classification performance, which demonstrate the efficacy of our\nproposed shape signature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chenran Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Investigation of Critical Issues in Bias Mitigation Techniques. (arXiv:2104.00170v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.00170","description":"<p>A critical problem in deep learning is that systems learn inappropriate\nbiases, resulting in their inability to perform well on minority groups. This\nhas led to the creation of multiple algorithms that endeavor to mitigate bias.\nHowever, it is not clear how effective these methods are. This is because study\nprotocols differ among papers, systems are tested on datasets that fail to test\nmany forms of bias, and systems have access to hidden knowledge or are tuned\nspecifically to the test set. To address this, we introduce an improved\nevaluation protocol, sensible metrics, and a new dataset, which enables us to\nask and answer critical questions about bias mitigation algorithms. We evaluate\nseven state-of-the-art algorithms using the same network architecture and\nhyperparameter selection policy across three benchmark datasets. We introduce a\nnew dataset called Biased MNIST that enables assessment of robustness to\nmultiple bias sources. We use Biased MNIST and a visual question answering\n(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning\nto the test set distribution, we study robustness across different tuning\ndistributions, which is critical because for many applications the test\ndistribution may not be known during development. We find that algorithms\nexploit hidden biases, are unable to scale to multiple forms of bias, and are\nhighly sensitive to the choice of tuning set. Based on our findings, we implore\nthe community to adopt more rigorous assessment of future bias mitigation\nmethods. All data, code, and results are publicly available at:\nhttps://github.com/erobic/bias-mitigators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shrestha_R/0/1/0/all/0/1\">Robik Shrestha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kafle_K/0/1/0/all/0/1\">Kushal Kafle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training Deep Neural Networks via Branch-and-Bound. (arXiv:2104.01730v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01730","description":"<p>In this paper, we propose BPGrad, a novel approximate algorithm for deep\nnueral network training, based on adaptive estimates of feasible region via\nbranch-and-bound. The method is based on the assumption of Lipschitz continuity\nin objective function, and as a result, it can adaptively determine the step\nsize for the current gradient given the history of previous updates. We prove\nthat, by repeating such a branch-and-pruning procedure, it can achieve the\noptimal solution within finite iterations. A computationally efficient solver\nbased on BPGrad has been proposed to train the deep neural networks. Empirical\nresults demonstrate that BPGrad solver works well in practice and compares\nfavorably to other stochastic optimization methods in the tasks of object\nrecognition, detection, and segmentation. The code is available at\n\\url{https://github.com/RyanCV/BPGrad}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuanwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Permutation Equivariant Structure from Motion. (arXiv:2104.06703v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.06703","description":"<p>Existing deep methods produce highly accurate 3D reconstructions in stereo\nand multiview stereo settings, i.e., when cameras are both internally and\nexternally calibrated. Nevertheless, the challenge of simultaneous recovery of\ncamera poses and 3D scene structure in multiview settings with deep networks is\nstill outstanding. Inspired by projective factorization for Structure from\nMotion (SFM) and by deep matrix completion techniques, we propose a neural\nnetwork architecture that, given a set of point tracks in multiple images of a\nstatic scene, recovers both the camera parameters and a (sparse) scene\nstructure by minimizing an unsupervised reprojection loss. Our network\narchitecture is designed to respect the structure of the problem: the sought\noutput is equivariant to permutations of both cameras and scene points.\nNotably, our method does not require initialization of camera parameters or 3D\npoint locations. We test our architecture in two setups: (1) single scene\nreconstruction and (2) learning from multiple scenes. Our experiments,\nconducted on a variety of datasets in both internally calibrated and\nuncalibrated settings, indicate that our method accurately recovers pose and\nstructure, on par with classical state of the art methods. Additionally, we\nshow that a pre-trained network can be used to reconstruct novel scenes using\ninexpensive fine-tuning with no loss of accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moran_D/0/1/0/all/0/1\">Dror Moran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koslowsky_H/0/1/0/all/0/1\">Hodaya Koslowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1\">Yoni Kasten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maron_H/0/1/0/all/0/1\">Haggai Maron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1\">Meirav Galun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1\">Ronen Basri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M3DeTR: Multi-representation, Multi-scale, Mutual-relation 3D Object Detection with Transformers. (arXiv:2104.11896v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11896","description":"<p>We present a novel architecture for 3D object detection, M3DeTR, which\ncombines different point cloud representations (raw, voxels, bird-eye view)\nwith different feature scales based on multi-scale feature pyramids. M3DeTR is\nthe first approach that unifies multiple point cloud representations, feature\nscales, as well as models mutual relationships between point clouds\nsimultaneously using transformers. We perform extensive ablation experiments\nthat highlight the benefits of fusing representation and scale, and modeling\nthe relationships. Our method achieves state-of-the-art performance on the\nKITTI 3D object detection dataset and Waymo Open Dataset. Results show that\nM3DeTR improves the baseline significantly by 1.48% mAP for all classes on\nWaymo Open Dataset. In particular, our approach ranks 1st on the well-known\nKITTI 3D Detection Benchmark for both car and cyclist classes, and ranks 1st on\nWaymo Open Dataset with single frame point cloud input. Our code is available\nat: https://github.com/rayguan97/M3DETR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_T/0/1/0/all/0/1\">Tianrui Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1\">Shiyi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zuxuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Synergistic Attention for Light Field Salient Object Detection. (arXiv:2104.13916v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.13916","description":"<p>We propose a novel Synergistic Attention Network (SA-Net) to address the\nlight field salient object detection by establishing a synergistic effect\nbetween multi-modal features with advanced attention mechanisms. Our SA-Net\nexploits the rich information of focal stacks via 3D convolutional neural\nnetworks, decodes the high-level features of multi-modal light field data with\ntwo cascaded synergistic attention modules, and predicts the saliency map using\nan effective feature fusion module in a progressive manner. Extensive\nexperiments on three widely-used benchmark datasets show that our SA-Net\noutperforms 28 state-of-the-art models, sufficiently demonstrating its\neffectiveness and superiority. Our code is available at\nhttps://github.com/PanoAsh/SA-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yujia Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Generative Augmentation for Visual Question Answering. (arXiv:2105.04780v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04780","description":"<p>Data augmentation has been shown to effectively improve the performance of\nmultimodal machine learning models. This paper introduces a generative model\nfor data augmentation by leveraging the correlations among multiple modalities.\nDifferent from conventional data augmentation approaches that apply low-level\noperations with deterministic heuristics, our method learns a generator that\ngenerates samples of the target modality conditioned on observed modalities in\nthe variational auto-encoder framework. Additionally, the proposed model is\nable to quantify the confidence of augmented data by its generative\nprobability, and can be jointly optimised with a downstream task. Experiments\non Visual Question Answering as downstream task demonstrate the effectiveness\nof the proposed generative model, which is able to improve strong UpDn-based\nmodels to achieve state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1\">Yishu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGPMA: Complicated Table Structure Recognition with Local and Global Pyramid Mask Alignment. (arXiv:2105.06224v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06224","description":"<p>Table structure recognition is a challenging task due to the various\nstructures and complicated cell spanning relations. Previous methods handled\nthe problem starting from elements in different granularities (rows/columns,\ntext regions), which somehow fell into the issues like lossy heuristic rules or\nneglect of empty cell division. Based on table structure characteristics, we\nfind that obtaining the aligned bounding boxes of text region can effectively\nmaintain the entire relevant range of different cells. However, the aligned\nbounding boxes are hard to be accurately predicted due to the visual\nambiguities. In this paper, we aim to obtain more reliable aligned bounding\nboxes by fully utilizing the visual information from both text regions in\nproposed local features and cell relations in global features. Specifically, we\npropose the framework of Local and Global Pyramid Mask Alignment, which adopts\nthe soft pyramid mask learning mechanism in both the local and global feature\nmaps. It allows the predicted boundaries of bounding boxes to break through the\nlimitation of original proposals. A pyramid mask re-scoring module is then\nintegrated to compromise the local and global information and refine the\npredicted boundaries. Finally, we propose a robust table structure recovery\npipeline to obtain the final structure, in which we also effectively solve the\nproblems of empty cells locating and division. Experimental results show that\nthe proposed method achieves competitive and even new state-of-the-art\nperformance on several public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1\">Liang Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zaisheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reciprocal Feature Learning via Explicit and Implicit Tasks in Scene Text Recognition. (arXiv:2105.06229v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.06229","description":"<p>Text recognition is a popular topic for its broad applications. In this work,\nwe excavate the implicit task, character counting within the traditional text\nrecognition, without additional labor annotation cost. The implicit task plays\nas an auxiliary branch for complementing the sequential recognition. We design\na two-branch reciprocal feature learning framework in order to adequately\nutilize the features from both the tasks. Through exploiting the complementary\neffect between explicit and implicit tasks, the feature is reliably enhanced.\nExtensive experiments on 7 benchmarks show the advantages of the proposed\nmethods in both text recognition and the new-built character counting tasks. In\naddition, it is convenient yet effective to equip with variable networks and\ntasks. We offer abundant ablation studies, generalizing experiments with deeper\nunderstanding on the tasks. Code is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hui Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yunlu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhanzhan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1\">Shiliang Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yi Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Boombox: Visual Reconstruction from Acoustic Vibrations. (arXiv:2105.08052v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08052","description":"<p>Interacting with bins and containers is a fundamental task in robotics,\nmaking state estimation of the objects inside the bin critical. While robots\noften use cameras for state estimation, the visual modality is not always ideal\ndue to occlusions and poor illumination. We introduce The Boombox, a container\nthat uses sound to estimate the state of the contents inside a box. Based on\nthe observation that the collision between objects and its containers will\ncause an acoustic vibration, we present a convolutional network for learning to\nreconstruct visual scenes. Although we use low-cost and low-power contact\nmicrophones to detect the vibrations, our results show that learning from\nmultimodal data enables state estimation from affordable audio sensors. Due to\nthe many ways that robots use containers, we believe the box will have a number\nof applications in robotics. Our project website is at: boombox.cs.columbia.edu\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiquier_M/0/1/0/all/0/1\">Mia Chiquier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1\">Hod Lipson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SSCAP: Self-supervised Co-occurrence Action Parsing for Unsupervised Temporal Action Segmentation. (arXiv:2105.14158v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14158","description":"<p>Temporal action segmentation is a task to classify each frame in the video\nwith an action label. However, it is quite expensive to annotate every frame in\na large corpus of videos to construct a comprehensive supervised training\ndataset. Thus in this work we propose an unsupervised method, namely SSCAP,\nthat operates on a corpus of unlabeled videos and predicts a likely set of\ntemporal segments across the videos. SSCAP leverages Self-Supervised learning\nto extract distinguishable features and then applies a novel Co-occurrence\nAction Parsing algorithm to not only capture the correlation among sub-actions\nunderlying the structure of activities, but also estimate the temporal path of\nthe sub-actions in an accurate and general way. We evaluate on both classic\ndatasets (Breakfast, 50Salads) and the emerging fine-grained action dataset\n(FineGym) with more complex activity structures and similar sub-actions.\nResults show that SSCAP achieves state-of-the-art performance on all datasets\nand can even outperform some weakly-supervised approaches, demonstrating its\neffectiveness and generalizability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chunhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1\">Yuanjun Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1\">Joseph Tighe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowlkes_C/0/1/0/all/0/1\">Charless Fowlkes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Pay Less Attention in Vision Transformers. (arXiv:2105.14217v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14217","description":"<p>Transformers have become one of the dominant architectures in deep learning,\nparticularly as a powerful alternative to convolutional neural networks (CNNs)\nin computer vision. However, Transformer training and inference in previous\nworks can be prohibitively expensive due to the quadratic complexity of\nself-attention over a long sequence of representations, especially for\nhigh-resolution dense prediction tasks. To this end, we present a novel Less\nattention vIsion Transformer (LIT), building upon the fact that the early\nself-attention layers in Transformers still focus on local patterns and bring\nminor benefits in recent hierarchical vision Transformers. Specifically, we\npropose a hierarchical Transformer where we use pure multi-layer perceptrons\n(MLPs) to encode rich local patterns in the early stages while applying\nself-attention modules to capture longer dependencies in deeper layers.\nMoreover, we further propose a learned deformable token merging module to\nadaptively fuse informative patches in a non-uniform manner. The proposed LIT\nachieves promising performance on image recognition tasks, including image\nclassification, object detection and instance segmentation, serving as a strong\nbackbone for many vision tasks. Code is available at:\nhttps://github.com/MonashAI/LIT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Isotropy Maximization Loss: Seamless and High-Performance Out-of-Distribution Detection Simply Replacing the SoftMax Loss. (arXiv:2105.14399v8 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2105.14399","description":"<p>Current out-of-distribution detection approaches usually present special\nrequirements (e.g., collecting outlier data and hyperparameter validation) and\nproduce side effects (e.g., classification accuracy drop and slow/inefficient\ninferences). Recently, entropic out-of-distribution detection has been proposed\nas a seamless approach (i.e., a solution that avoids all previously mentioned\ndrawbacks). The entropic out-of-distribution detection solution uses the IsoMax\nloss for training and the entropic score for out-of-distribution detection. The\nIsoMax loss works as a drop-in replacement of the SoftMax loss (i.e., the\ncombination of the output linear layer, the SoftMax activation, and the\ncross-entropy loss) because swapping the SoftMax loss with the IsoMax loss\nrequires no changes in the model's architecture or training hyperparameters. In\nthis paper, we perform what we call an isometrization of the distances used in\nthe IsoMax loss. Additionally, we propose replacing the entropic score with the\nminimum distance score. Experiments showed that these simple modifications\nsignificantly increase out-of-distribution detection performance while keeping\nthe solution seamless. Besides being competitive with or outperforming all\nmajor current approaches, the proposed solution avoids all their current\nlimitations in addition to being much easier to use because only a simple loss\nreplacement for training the neural network is required. The code to replace\nthe SoftMax loss with the IsoMax+ loss and reproduce the results is available\nhttps://github.com/dlmacedo/entropic-out-of-distribution-detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1\">David Mac&#xea;do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1\">Teresa Ludermir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Aligning Pretraining for Detection via Object-Level Contrastive Learning. (arXiv:2106.02637v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02637","description":"<p>Image-level contrastive representation learning has proven to be highly\neffective as a generic model for transfer learning. Such generality for\ntransfer learning, however, sacrifices specificity if we are interested in a\ncertain downstream task. We argue that this could be sub-optimal and thus\nadvocate a design principle which encourages alignment between the\nself-supervised pretext task and the downstream task. In this paper, we follow\nthis principle with a pretraining method specifically designed for the task of\nobject detection. We attain alignment in the following three aspects: 1)\nobject-level representations are introduced via selective search bounding boxes\nas object proposals; 2) the pretraining network architecture incorporates the\nsame dedicated modules used in the detection pipeline (e.g. FPN); 3) the\npretraining is equipped with object detection properties such as object-level\ntranslation invariance and scale invariance. Our method, called Selective\nObject COntrastive learning (SoCo), achieves state-of-the-art results for\ntransfer performance on COCO detection using a Mask R-CNN framework. Code is\navailable at https://github.com/hologerry/SoCo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Fangyun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Han Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Stephen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combinatorial Optimization for Panoptic Segmentation: A Fully Differentiable Approach. (arXiv:2106.03188v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03188","description":"<p>We propose a fully differentiable architecture for simultaneous semantic and\ninstance segmentation (a.k.a. panoptic segmentation) consisting of a\nconvolutional neural network and an asymmetric multiway cut problem solver. The\nlatter solves a combinatorial optimization problem that elegantly incorporates\nsemantic and boundary predictions to produce a panoptic labeling. Our\nformulation allows to directly maximize a smooth surrogate of the panoptic\nquality metric by backpropagating the gradient through the optimization\nproblem. Experimental evaluation shows improvement by backpropagating through\nthe optimization problem w.r.t. comparable approaches on Cityscapes and COCO\ndatasets. Overall, our approach shows the utility of using combinatorial\noptimization in tandem with deep learning in a challenging large scale\nreal-world problem and showcases benefits and insights into training such an\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1\">Ahmed Abbas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1\">Paul Swoboda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Semantic Hallucination for Domain Generalized Semantic Segmentation. (arXiv:2106.04144v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04144","description":"<p>Convolutional neural networks may perform poorly when the test and train data\nare from different domains. While this problem can be mitigated by using the\ntarget domain data to align the source and target domain feature\nrepresentations, the target domain data may be unavailable due to privacy\nconcerns. Consequently, there is a need for methods that generalize well\nwithout access to target domain data during training. In this work, we propose\nan adversarial hallucination approach, which combines a class-wise\nhallucination module and a semantic segmentation module. Since the segmentation\nperformance varies across different classes, we design a semantic-conditioned\nstyle hallucination layer to adaptively stylize each class. The classwise\nstylization parameters are generated from the semantic knowledge in the\nsegmentation probability maps of the source domain image. Both modules compete\nadversarially, with the hallucination module generating increasingly\n'difficult' style images to challenge the segmentation module. In response, the\nsegmentation module improves its performance as it is trained with generated\nsamples at an appropriate class-wise difficulty level. Experiments on state of\nthe art domain adaptation work demonstrate the efficacy of our proposed method\nwhen no target domain data are available for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tjio_G/0/1/0/all/0/1\">Gabriel Tjio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Joey Tianyi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_R/0/1/0/all/0/1\">Rick Siow Mong Goh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chasing Sparsity in Vision Transformers: An End-to-End Exploration. (arXiv:2106.04533v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04533","description":"<p>Vision transformers (ViTs) have recently received explosive popularity, but\ntheir enormous model sizes and training costs remain daunting. Conventional\npost-training pruning often incurs higher training budgets. In contrast, this\npaper aims to trim down both the training memory overhead and the inference\ncomplexity, without sacrificing the achievable accuracy. We carry out the\nfirst-of-its-kind comprehensive exploration, on taking a unified approach of\nintegrating sparsity in ViTs \"from end to end\". Specifically, instead of\ntraining full ViTs, we dynamically extract and train sparse subnetworks, while\nsticking to a fixed small parameter budget. Our approach jointly optimizes\nmodel parameters and explores connectivity throughout training, ending up with\none sparse network as the final output. The approach is seamlessly extended\nfrom unstructured to structured sparsity, the latter by considering to guide\nthe prune-and-grow of self-attention heads inside ViTs. We further co-explore\ndata and architecture sparsity for additional efficiency gains by plugging in a\nnovel learnable token selector to adaptively determine the currently most vital\npatches. Extensive results on ImageNet with diverse ViT backbones validate the\neffectiveness of our proposals which obtain significantly reduced computational\ncost and almost unimpaired generalization. Perhaps most surprisingly, we find\nthat the proposed sparse (co-)training can sometimes improve the ViT accuracy\nrather than compromising it, making sparsity a tantalizing \"free lunch\". For\nexample, our sparsified DeiT-Small at (5%, 50%) sparsity for (data,\narchitecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32% FLOPs\nand 4.40% running time savings. Our codes are available at\nhttps://github.com/VITA-Group/SViTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers. (arXiv:2106.05392v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05392","description":"<p>In video transformers, the time dimension is often treated in the same way as\nthe two spatial dimensions. However, in a scene where objects or the camera may\nmove, a physical point imaged at one location in frame $t$ may be entirely\nunrelated to what is found at that location in frame $t+k$. These temporal\ncorrespondences should be modeled to facilitate learning about dynamic scenes.\nTo this end, we propose a new drop-in block for video transformers --\ntrajectory attention -- that aggregates information along implicitly determined\nmotion paths. We additionally propose a new method to address the quadratic\ndependence of computation and memory on the input size, which is particularly\nimportant for high resolution or long videos. While these ideas are useful in a\nrange of settings, we apply them to the specific task of video action\nrecognition with a transformer model and obtain state-of-the-art results on the\nKinetics, Something--Something V2, and Epic-Kitchens datasets. Code and models\nare available at: https://github.com/facebookresearch/Motionformer\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patrick_M/0/1/0/all/0/1\">Mandela Patrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1\">Dylan Campbell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1\">Yuki M. Asano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feichtenhofer_C/0/1/0/all/0/1\">Christoph Feichtenhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1\">Jo&#xe3;o F. Henriques</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MST: Masked Self-Supervised Transformer for Visual Representation. (arXiv:2106.05656v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05656","description":"<p>Transformer has been widely used for self-supervised pre-training in Natural\nLanguage Processing (NLP) and achieved great success. However, it has not been\nfully explored in visual self-supervised learning. Meanwhile, previous methods\nonly consider the high-level feature and learning representation from a global\nperspective, which may fail to transfer to the downstream dense prediction\ntasks focusing on local features. In this paper, we present a novel Masked\nSelf-supervised Transformer approach named MST, which can explicitly capture\nthe local context of an image while preserving the global semantic information.\nSpecifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose\na masked token strategy based on the multi-head self-attention map, which\ndynamically masks some tokens of local patches without damaging the crucial\nstructure for self-supervised learning. More importantly, the masked tokens\ntogether with the remaining tokens are further recovered by a global image\ndecoder, which preserves the spatial information of the image and is more\nfriendly to the downstream dense prediction tasks. The experiments on multiple\ndatasets demonstrate the effectiveness and generality of the proposed method.\nFor instance, MST achieves Top-1 accuracy of 76.9% with DeiT-S only using\n300-epoch pre-training by linear evaluation, which outperforms supervised\nmethods with the same epoch by 0.4% and its comparable variant DINO by 1.0\\%.\nFor dense prediction tasks, MST also achieves 42.7% mAP on MS COCO object\ndetection and 74.04% mIoU on Cityscapes segmentation only with 100-epoch\npre-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yousong Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chaoyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Rui Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Rui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partial success in closing the gap between human and machine vision. (arXiv:2106.07411v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07411","description":"<p>A few years ago, the first CNN surpassed human performance on ImageNet.\nHowever, it soon became clear that machines lack robustness on more challenging\ntest cases, a major obstacle towards deploying machines \"in the wild\" and\ntowards obtaining better computational models of human visual perception. Here\nwe ask: Are we making progress in closing the gap between human and machine\nvision? To answer this question, we tested human observers on a broad range of\nout-of-distribution (OOD) datasets, recording 85,120 psychophysical trials\nacross 90 participants. We then investigated a range of promising machine\nlearning developments that crucially deviate from standard supervised CNNs\nalong three axes: objective function (self-supervised, adversarially trained,\nCLIP language-image training), architecture (e.g. vision transformers), and\ndataset size (ranging from 1M to 1B).\n</p>\n<p>Our findings are threefold. (1.) The longstanding distortion robustness gap\nbetween humans and CNNs is closing, with the best models now exceeding human\nfeedforward performance on most of the investigated OOD datasets. (2.) There is\nstill a substantial image-level consistency gap, meaning that humans make\ndifferent errors than models. In contrast, most models systematically agree in\ntheir categorisation errors, even substantially different ones like contrastive\nself-supervised vs. standard supervised models. (3.) In many cases,\nhuman-to-model consistency improves when training dataset size is increased by\none to three orders of magnitude. Our results give reason for cautious\noptimism: While there is still much room for improvement, the behavioural\ndifference between human and machine vision is narrowing. In order to measure\nfuture progress, 17 OOD datasets with image-level human behavioural data and\nevaluation code are provided as a toolbox and benchmark at:\nhttps://github.com/bethgelab/model-vs-human/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1\">Robert Geirhos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanappa_K/0/1/0/all/0/1\">Kantharaju Narayanappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitzkus_B/0/1/0/all/0/1\">Benjamin Mitzkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thieringer_T/0/1/0/all/0/1\">Tizian Thieringer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1\">Felix A. Wichmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastAno: Fast Anomaly Detection via Spatio-temporal Patch Transformation. (arXiv:2106.08613v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.08613","description":"<p>Video anomaly detection has gained significant attention due to the\nincreasing requirements of automatic monitoring for surveillance videos.\nEspecially, the prediction based approach is one of the most studied methods to\ndetect anomalies by predicting frames that include abnormal events in the test\nset after learning with the normal frames of the training set. However, a lot\nof prediction networks are computationally expensive owing to the use of\npre-trained optical flow networks, or fail to detect abnormal situations\nbecause of their strong generative ability to predict even the anomalies. To\naddress these shortcomings, we propose spatial rotation transformation (SRT)\nand temporal mixing transformation (TMT) to generate irregular patch cuboids\nwithin normal frame cuboids in order to enhance the learning of normal\nfeatures. Additionally, the proposed patch transformation is used only during\nthe training phase, allowing our model to detect abnormal frames at fast speed\nduring inference. Our model is evaluated on three anomaly detection benchmarks,\nachieving competitive accuracy and surpassing all the previous works in terms\nof speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chaewon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1\">MyeongAh Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minhyeok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sangyoun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Anytime Learning at Macroscale. (arXiv:2106.09563v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.09563","description":"<p>Classical machine learning frameworks assume access to a possibly large\ndataset in order to train a predictive model. In many practical applications\nhowever, data does not arrive all at once, but in batches over time. This\ncreates a natural trade-off between accuracy of a model and time to obtain such\na model. A greedy predictor could produce non-trivial predictions by\nimmediately training on batches as soon as these become available but, it may\nalso make suboptimal use of future data. On the other hand, a tardy predictor\ncould wait for a long time to aggregate several batches into a larger dataset,\nbut ultimately deliver a much better performance. In this work, we consider\nsuch a streaming learning setting, which we dub anytime learning at macroscale}\n(ALMA). It is an instance of anytime learning applied not at the level of a\nsingle chunk of data, but at the level of the entire sequence of large batches.\nWe first formalize this learning setting, we then introduce metrics to assess\nhow well learners perform on the given task for a given memory and compute\nbudget, and finally we test about thirty baseline approaches on three standard\nbenchmarks repurposed for anytime learning at macroscale. Our findings indicate\nthat no model strikes the best trade-off across the board. While replay-based\nmethods attain the lowest error rate, they also incur in a 5 to 10 times\nincrease of compute. Approaches that grow capacity over time do offer better\nscaling in terms of training flops, but they also underperform simpler\nensembling methods in terms of error rate. Overall, ALMA offers both a good\nabstraction of the typical learning setting faced everyday by practitioners,\nand a set of unsolved modeling problems for those interested in efficient\nlearning of dynamic models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_M/0/1/0/all/0/1\">Myle Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranzato_M/0/1/0/all/0/1\">Marc&#x27;Aurelio Ranzato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denoyer_L/0/1/0/all/0/1\">Ludovic Denoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Training via Boosting Pruning Plasticity with Neuroregeneration. (arXiv:2106.10404v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.10404","description":"<p>Works on lottery ticket hypothesis (LTH) and single-shot network pruning\n(SNIP) have raised a lot of attention currently on post-training pruning\n(iterative magnitude pruning), and before-training pruning (pruning at\ninitialization). The former method suffers from an extremely large computation\ncost and the latter usually struggles with insufficient performance. In\ncomparison, during-training pruning, a class of pruning methods that\nsimultaneously enjoys the training/inference efficiency and the comparable\nperformance, temporarily, has been less explored. To better understand\nduring-training pruning, we quantitatively study the effect of pruning\nthroughout training from the perspective of pruning plasticity (the ability of\nthe pruned networks to recover the original performance). Pruning plasticity\ncan help explain several other empirical observations about neural network\npruning in literature. We further find that pruning plasticity can be\nsubstantially improved by injecting a brain-inspired mechanism called\nneuroregeneration, i.e., to regenerate the same number of connections as\npruned. We design a novel gradual magnitude pruning (GMP) method, named gradual\npruning with zero-cost neuroregeneration (\\textbf{GraNet}), that advances state\nof the art. Perhaps most impressively, its sparse-to-sparse version for the\nfirst time boosts the sparse-to-sparse training performance over various\ndense-to-sparse methods with ResNet-50 on ImageNet without extending the\ntraining time. We release all codes in\nhttps://github.com/Shiweiliuiiiiiii/GraNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1\">Zahra Atashgahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_L/0/1/0/all/0/1\">Lu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_H/0/1/0/all/0/1\">Huanyu Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Li Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1\">Decebal Constantin Mocanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Million Scenes for Autonomous Driving: ONCE Dataset. (arXiv:2106.11037v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.11037","description":"<p>Current perception models in autonomous driving have become notorious for\ngreatly relying on a mass of annotated data to cover unseen cases and address\nthe long-tail problem. On the other hand, learning from unlabeled large-scale\ncollected data and incrementally self-training powerful recognition models have\nreceived increasing attention and may become the solutions of next-generation\nindustry-level powerful and robust perception models in autonomous driving.\nHowever, the research community generally suffered from data inadequacy of\nthose essential real-world scene data, which hampers the future exploration of\nfully/semi/self-supervised methods for 3D perception. In this paper, we\nintroduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the\nautonomous driving scenario. The ONCE dataset consists of 1 million LiDAR\nscenes and 7 million corresponding camera images. The data is selected from 144\ndriving hours, which is 20x longer than the largest 3D autonomous driving\ndataset available (e.g. nuScenes and Waymo), and it is collected across a range\nof different areas, periods and weather conditions. To facilitate future\nresearch on exploiting unlabeled data for 3D detection, we additionally provide\na benchmark in which we reproduce and evaluate a variety of self-supervised and\nsemi-supervised methods on the ONCE dataset. We conduct extensive analyses on\nthose methods and provide valuable observations on their performance related to\nthe scale of used data. Data, code, and more information are available at\nhttps://once-for-auto-driving.github.io/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1\">Jiageng Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1\">Minzhe Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chenhan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hanxue Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yamin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chaoqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jie Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.15610","description":"<p>Unsupervised disentanglement has been shown to be theoretically impossible\nwithout inductive biases on the models and the data. As an alternative\napproach, recent methods rely on limited supervision to disentangle the factors\nof variation and allow their identifiability. While annotating the true\ngenerative factors is only required for a limited number of observations, we\nargue that it is infeasible to enumerate all the factors of variation that\ndescribe a real-world image distribution. To this end, we propose a method for\ndisentangling a set of factors which are only partially labeled, as well as\nseparating the complementary set of residual factors that are never explicitly\nspecified. Our success in this challenging setting, demonstrated on synthetic\nbenchmarks, gives rise to leveraging off-the-shelf image descriptors to\npartially annotate a subset of attributes in real image domains (e.g. of human\nfaces) with minimal manual effort. Specifically, we use a recent language-image\nembedding model (CLIP) to annotate a set of attributes of interest in a\nzero-shot manner and demonstrate state-of-the-art disentangled image\nmanipulation results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking. (arXiv:2106.16100v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.16100","description":"<p>Association, aiming to link bounding boxes of the same identity in a video\nsequence, is a central component in multi-object tracking (MOT). To train\nassociation modules, e.g., parametric networks, real video data are usually\nused. However, annotating person tracks in consecutive video frames is\nexpensive, and such real data, due to its inflexibility, offer us limited\nopportunities to evaluate the system performance w.r.t changing tracking\nscenarios. In this paper, we study whether 3D synthetic data can replace\nreal-world videos for association training. Specifically, we introduce a\nlarge-scale synthetic data engine named MOTX, where the motion characteristics\nof cameras and objects are manually configured to be similar to those in\nreal-world datasets. We show that compared with real data, association\nknowledge obtained from synthetic data can achieve very similar performance on\nreal-world test sets without domain adaption techniques. Our intriguing\nobservation is credited to two factors. First and foremost, 3D engines can well\nsimulate motion factors such as camera movement, camera view and object\nmovement, so that the simulated videos can provide association modules with\neffective motion features. Second, experimental results show that the\nappearance domain gap hardly harms the learning of association knowledge. In\naddition, the strong customization ability of MOTX allows us to quantitatively\nassess the impact of motion factors on MOT, which brings new insights to the\ncommunity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiangxin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.00166","description":"<p>There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Supervision Learning for Pathology Whole Slide Image Classification. (arXiv:2107.00934v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00934","description":"<p>Weak supervision learning on classification labels has demonstrated high\nperformance in various tasks, while a few pixel-level fine annotations are also\naffordable. Naturally a question comes to us that whether the combination of\npixel-level (e.g., segmentation) and image level (e.g., classification)\nannotation can introduce further improvement. However in computational\npathology this is a difficult task for this reason: High resolution of whole\nslide images makes it difficult to do end-to-end classification model training,\nwhich is challenging to research of weak or hybrid supervision learning in the\npast. To handle this problem, we propose a hybrid supervision learning\nframework for this kind of high resolution images with sufficient image-level\ncoarse annotations and a few pixel-level fine labels. This framework, when\napplied in training patch model, can carefully make use of coarse image-level\nlabels to refine generated pixel-level pseudo labels. Complete strategy is\nproposed to suppress pixel-level false positives and false negatives. A large\nhybrid annotated dataset is used to evaluate the effectiveness of hybrid\nsupervision learning. By extracting pixel-level pseudo labels in initially\nimage-level labeled samples, we achieve 5.2% higher specificity than purely\ntraining on existing labels while retaining 100% sensitivity, in the task of\nimage-level classification to be positive or negative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1\">Qi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Inter-intra Variant Dual Representations forSelf-supervised Video Recognition. (arXiv:2107.01194v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01194","description":"<p>Contrastive learning applied to self-supervised representation learning has\nseen a resurgence in deep models. In this paper, we find that existing\ncontrastive learning based solutions for self-supervised video recognition\nfocus on inter-variance encoding but ignore the intra-variance existing in\nclips within the same video. We thus propose to learn dual representations for\neach clip which (\\romannumeral 1) encode intra-variance through a shuffle-rank\npretext task; (\\romannumeral 2) encode inter-variance through a temporal\ncoherent contrastive loss. Experiment results show that our method plays an\nessential role in balancing inter and intra variances and brings consistent\nperformance gains on multiple backbones and contrastive learning frameworks.\nIntegrated with SimCLR and pretrained on Kinetics-400, our method achieves\n$\\textbf{82.0\\%}$ and $\\textbf{51.2\\%}$ downstream classification accuracy on\nUCF101 and HMDB51 test sets respectively and $\\textbf{46.1\\%}$ video retrieval\naccuracy on UCF101, outperforming both pretext-task based and contrastive\nlearning based counterparts. Our code is available at\n\\href{https://github.com/lzhangbj/DualVar}{https://github.com/lzhangbj/DualVar}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhengyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Object Behavioral Feature Extraction for Potential Risk Analysis based on Video Sensor. (arXiv:2107.03554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.03554","description":"<p>Pedestrians are exposed to risk of death or serious injuries on roads,\nespecially unsignalized crosswalks, for a variety of reasons. To date, an\nextensive variety of studies have reported on vision based traffic safety\nsystem. However, many studies required manual inspection of the volumes of\ntraffic video to reliably obtain traffic related objects behavioral factors. In\nthis paper, we propose an automated and simpler system for effectively\nextracting object behavioral features from video sensors deployed on the road.\nWe conduct basic statistical analysis on these features, and show how they can\nbe useful for monitoring the traffic behavior on the road. We confirm the\nfeasibility of the proposed system by applying our prototype to two\nunsignalized crosswalks in Osan city, South Korea. To conclude, we compare\nbehaviors of vehicles and pedestrians in those two areas by simple statistical\nanalysis. This study demonstrates the potential for a network of connected\nvideo sensors to provide actionable data for smart cities to improve pedestrian\nsafety in dangerous road environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noh_B/0/1/0/all/0/1\">Byeongjoon Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_W/0/1/0/all/0/1\">Wonjun Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">David Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1\">Hwasoo Yeo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDMapNet: A Local Semantic Map Learning and Evaluation Framework. (arXiv:2107.06307v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06307","description":"<p>Estimating local semantics from sensory inputs is a central component for\nhigh-definition map constructions in autonomous driving. However, traditional\npipelines require a vast amount of human efforts and resources in annotating\nand maintaining the semantics in the map, which limits its scalability. In this\npaper, we introduce the problem of local semantic map learning, which\ndynamically constructs the vectorized semantics based on onboard sensor\nobservations. Meanwhile, we introduce a local semantic map learning method,\ndubbed HDMapNet. HDMapNet encodes image features from surrounding cameras\nand/or point clouds from LiDAR, and predicts vectorized map elements in the\nbird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all\nsettings, it performs better than baseline methods. Of note, our fusion-based\nHDMapNet outperforms existing methods by more than 50% in all metrics. In\naddition, we develop semantic-level and instance-level metrics to evaluate the\nmap learning performance. Finally, we showcase our method is capable of\npredicting a locally consistent map. By introducing the method and metrics, we\ninvite the community to study this novel map learning problem. Code and\nevaluation kit will be released to facilitate future development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FetalNet: Multi-task deep learning framework for fetal ultrasound biometric measurements. (arXiv:2107.06943v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06943","description":"<p>In this paper, we propose an end-to-end multi-task neural network called\nFetalNet with an attention mechanism and stacked module for spatio-temporal\nfetal ultrasound scan video analysis. Fetal biometric measurement is a standard\nexamination during pregnancy used for the fetus growth monitoring and\nestimation of gestational age and fetal weight. The main goal in fetal\nultrasound scan video analysis is to find proper standard planes to measure the\nfetal head, abdomen and femur. Due to natural high speckle noise and shadows in\nultrasound data, medical expertise and sonographic experience are required to\nfind the appropriate acquisition plane and perform accurate measurements of the\nfetus. In addition, existing computer-aided methods for fetal US biometric\nmeasurement address only one single image frame without considering temporal\nfeatures. To address these shortcomings, we propose an end-to-end multi-task\nneural network for spatio-temporal ultrasound scan video analysis to\nsimultaneously localize, classify and measure the fetal body parts. We propose\na new encoder-decoder segmentation architecture that incorporates a\nclassification branch. Additionally, we employ an attention mechanism with a\nstacked module to learn salient maps to suppress irrelevant US regions and\nefficient scan plane localization. We trained on the fetal ultrasound video\ncomes from routine examinations of 700 different patients. Our method called\nFetalNet outperforms existing state-of-the-art methods in both classification\nand segmentation in fetal ultrasound video recordings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plotka_S/0/1/0/all/0/1\">Szymon P&#x142;otka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wlodarczyk_T/0/1/0/all/0/1\">Tomasz W&#x142;odarczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klasa_A/0/1/0/all/0/1\">Adam Klasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipa_M/0/1/0/all/0/1\">Micha&#x142; Lipa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sitek_A/0/1/0/all/0/1\">Arkadiusz Sitek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzci&#x144;ski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Flood Segmentation on Sentinel-1 SAR Imagery with Semi-Supervised Learning. (arXiv:2107.08369v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.08369","description":"<p>Floods wreak havoc throughout the world, causing billions of dollars in\ndamages, and uprooting communities, ecosystems and economies. The NASA Impact\nFlood Detection competition tasked participants with predicting flooded pixels\nafter training with synthetic aperture radar (SAR) images in a supervised\nsetting. We propose a semi-supervised learning pseudo-labeling scheme that\nderives confidence estimates from U-Net ensembles, progressively improving\naccuracy. Concretely, we use a cyclical approach involving multiple stages (1)\ntraining an ensemble model of multiple U-Net architectures with the provided\nhigh confidence hand-labeled data and, generated pseudo labels or low\nconfidence labels on the entire unlabeled test dataset, and then, (2) filter\nout quality generated labels and, (3) combine the generated labels with the\npreviously available high confidence hand-labeled dataset. This assimilated\ndataset is used for the next round of training ensemble models and the cyclical\nprocess is repeated until the performance improvement plateaus. We post process\nour results with Conditional Random Fields. Our approach sets a new\nstate-of-the-art on the Sentinel-1 dataset with 0.7654 IoU, an impressive\nimprovement over the 0.60 IoU baseline. Our method, which we release with all\nthe code and models, can also be used as an open science benchmark for the\nSentinel-1 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sayak Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1\">Siddha Ganju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANFIC: Image Compression Using Augmented Normalizing Flows. (arXiv:2107.08470v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.08470","description":"<p>This paper introduces an end-to-end learned image compression system, termed\nANFIC, based on Augmented Normalizing Flows (ANF). ANF is a new type of flow\nmodel, which stacks multiple variational autoencoders (VAE) for greater model\nexpressiveness. The VAE-based image compression has gone mainstream, showing\npromising compression performance. Our work presents the first attempt to\nleverage VAE-based compression in a flow-based framework. ANFIC advances\nfurther compression efficiency by stacking and extending hierarchically\nmultiple VAE's. The invertibility of ANF, together with our training\nstrategies, enables ANFIC to support a wide range of quality levels without\nchanging the encoding and decoding networks. Extensive experimental results\nshow that in terms of PSNR-RGB, ANFIC performs comparably to or better than the\nstate-of-the-art learned image compression. Moreover, it performs close to VVC\nintra coding, from low-rate compression up to nearly-lossless compression. In\nparticular, ANFIC achieves the state-of-the-art performance, when extended with\nconditional convolution for variable rate compression with a single model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ho_Y/0/1/0/all/0/1\">Yung-Han Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_C/0/1/0/all/0/1\">Chih-Chun Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peng_W/0/1/0/all/0/1\">Wen-Hsiao Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hang_H/0/1/0/all/0/1\">Hsueh-Ming Hang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Domanski_M/0/1/0/all/0/1\">Marek Domanski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Proximal Unrolling Network for Compressive Imaging. (arXiv:2107.11007v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.11007","description":"<p>Compressive imaging aims to recover a latent image from under-sampled\nmeasurements, suffering from a serious ill-posed inverse problem. Recently,\ndeep neural networks have been applied to this problem with superior results,\nowing to the learned advanced image priors. These approaches, however, require\ntraining separate models for different imaging modalities and sampling ratios,\nleading to overfitting to specific settings. In this paper, a dynamic proximal\nunrolling network (dubbed DPUNet) was proposed, which can handle a variety of\nmeasurement matrices via one single model without retraining. Specifically,\nDPUNet can exploit both the embedded observation model via gradient descent and\nimposed image priors by learned dynamic proximal operators, achieving joint\nreconstruction. A key component of DPUNet is a dynamic proximal mapping module,\nwhose parameters can be dynamically adjusted at the inference stage and make it\nadapt to different imaging settings. Experimental results demonstrate that the\nproposed DPUNet can effectively handle multiple compressive imaging modalities\nunder varying sampling ratios and noise levels via only one trained model, and\noutperform the state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yixiao Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1\">Ran Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1\">Kaixuan Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Ying Fu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bridging Gap between Image Pixels and Semantics via Supervision: A Survey. (arXiv:2107.13757v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13757","description":"<p>The fact that there exists a gap between low-level features and semantic\nmeanings of images, called the semantic gap, is known for decades. Resolution\nof the semantic gap is a long standing problem. The semantic gap problem is\nreviewed and a survey on recent efforts in bridging the gap is made in this\nwork. Most importantly, we claim that the semantic gap is primarily bridged\nthrough supervised learning today. Experiences are drawn from two application\ndomains to illustrate this point: 1) object detection and 2) metric learning\nfor content-based image retrieval (CBIR). To begin with, this paper offers a\nhistorical retrospective on supervision, makes a gradual transition to the\nmodern data-driven methodology and introduces commonly used datasets. Then, it\nsummarizes various supervision methods to bridge the semantic gap in the\ncontext of object detection and metric learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1\">Jiali Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuo_C/0/1/0/all/0/1\">C.-C. Jay Kuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Road Scenes Segmentation Across Different Domains by Disentangling Latent Representations. (arXiv:2108.03021v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03021","description":"<p>Deep learning models obtain impressive accuracy in road scenes understanding,\nhowever they need a large quantity of labeled samples for their training.\nAdditionally, such models do not generalize well to environments where the\nstatistical properties of data do not perfectly match those of training scenes,\nand this can be a significant problem for intelligent vehicles. Hence, domain\nadaptation approaches have been introduced to transfer knowledge acquired on a\nlabel-abundant source domain to a related label-scarce target domain. In this\nwork, we design and carefully analyze multiple latent space-shaping\nregularization strategies that work together to reduce the domain shift. More\nin detail, we devise a feature clustering strategy to increase domain\nalignment, a feature perpendicularity constraint to space apart features\nbelonging to different semantic classes, including those not present in the\ncurrent batch, and a feature norm alignment strategy to separate active and\ninactive channels. In addition, we propose a novel evaluation metric to capture\nthe relative performance of an adapted model with respect to supervised\ntraining. We validate our framework in driving scenarios, considering both\nsynthetic-to-real and real-to-real adaptation, outperforming previous\nfeature-level state-of-the-art methods on multiple road scenes benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1\">Francesco Barbato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1\">Umberto Michieli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1\">Marco Toldo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributional Depth-Based Estimation of Object Articulation Models. (arXiv:2108.05875v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2108.05875","description":"<p>We propose a method that efficiently learns distributions over articulation\nmodel parameters directly from depth images without the need to know\narticulation model categories a priori. By contrast, existing methods that\nlearn articulation models from raw observations typically only predict point\nestimates of the model parameters, which are insufficient to guarantee the safe\nmanipulation of articulated objects. Our core contributions include a novel\nrepresentation for distributions over rigid body transformations and\narticulation model parameters based on screw theory, von Mises-Fisher\ndistributions, and Stiefel manifolds. Combining these concepts allows for an\nefficient, mathematically sound representation that implicitly satisfies the\nconstraints that rigid body transformations and articulations must adhere to.\nLeveraging this representation, we introduce a novel deep learning based\napproach, DUST-net, that performs category-independent articulation model\nestimation while also providing model uncertainties. We evaluate our approach\non several benchmarking datasets and real-world objects and compare its\nperformance with two current state-of-the-art methods. Our results demonstrate\nthat DUST-net can successfully learn distributions over articulation models for\nnovel objects across articulation model categories, which generate point\nestimates with better accuracy than state-of-the-art methods and effectively\ncapture the uncertainty over predicted model parameters due to noisy inputs.\nProject webpage: https://pearl-utexas.github.io/DUST-net/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ajinkya Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giguere_S/0/1/0/all/0/1\">Stephen Giguere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1\">Rudolf Lioutikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning. (arXiv:2108.06098v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2108.06098","description":"<p>In this work, we propose a communication-efficient parameterization, FedPara,\nfor federated learning (FL) to overcome the burdens on frequent model uploads\nand downloads. Our method re-parameterizes weight parameters of layers using\nlow-rank weights followed by the Hadamard product. Compared to the conventional\nlow-rank parameterization, our FedPara method is not restricted to low-rank\nconstraints, and thereby it has a far larger capacity. This property enables to\nachieve comparable performance while requiring 3 to 10 times lower\ncommunication costs than the model with the original layers, which is not\nachievable by the traditional low-rank methods. The efficiency of our method\ncan be further improved by combining with other efficient FL optimizers. In\naddition, we extend our method to a personalized FL application, pFedPara,\nwhich separates parameters into global and local ones. We show that pFedPara\noutperforms competing personalized FL methods with more than three times fewer\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hyeon_Woo_N/0/1/0/all/0/1\">Nam Hyeon-Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Bin_M/0/1/0/all/0/1\">Moon Ye-Bin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_T/0/1/0/all/0/1\">Tae-Hyun Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Anchored Unsigned Distance Functions with Gradient Direction Alignment for Single-view Garment Reconstruction. (arXiv:2108.08478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.08478","description":"<p>While single-view 3D reconstruction has made significant progress benefiting\nfrom deep shape representations in recent years, garment reconstruction is\nstill not solved well due to open surfaces, diverse topologies and complex\ngeometric details. In this paper, we propose a novel learnable Anchored\nUnsigned Distance Function (AnchorUDF) representation for 3D garment\nreconstruction from a single image. AnchorUDF represents 3D shapes by\npredicting unsigned distance fields (UDFs) to enable open garment surface\nmodeling at arbitrary resolution. To capture diverse garment topologies,\nAnchorUDF not only computes pixel-aligned local image features of query points,\nbut also leverages a set of anchor points located around the surface to enrich\n3D position features for query points, which provides stronger 3D space context\nfor the distance function. Furthermore, in order to obtain more accurate point\nprojection direction at inference, we explicitly align the spatial gradient\ndirection of AnchorUDF with the ground-truth direction to the surface during\ntraining. Extensive experiments on two public 3D garment datasets, i.e., MGN\nand Deep Fashion3D, demonstrate that AnchorUDF achieves the state-of-the-art\nperformance on single-view garment reconstruction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Synthesis-Based Approach for Thermal-to-Visible Face Verification. (arXiv:2108.09558v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09558","description":"<p>In recent years, visible-spectrum face verification systems have been shown\nto match the performance of experienced forensic examiners. However, such\nsystems are ineffective in low-light and nighttime conditions. Thermal face\nimagery, which captures body heat emissions, effectively augments the visible\nspectrum, capturing discriminative facial features in scenes with limited\nillumination. Due to the increased cost and difficulty of obtaining diverse,\npaired thermal and visible spectrum datasets, not many algorithms and\nlarge-scale benchmarks for low-light recognition are available. This paper\npresents an algorithm that achieves state-of-the-art performance on both the\nARL-VTF and TUFTS multi-spectral face datasets. Importantly, we study the\nimpact of face alignment, pixel-level correspondence, and identity\nclassification with label smoothing for multi-spectral face synthesis and\nverification. We show that our proposed method is widely applicable, robust,\nand highly effective. In addition, we show that the proposed method\nsignificantly outperforms face frontalization methods on profile-to-frontal\nverification. Finally, we present MILAB-VTF(B), a challenging multi-spectral\nface dataset that is composed of paired thermal and visible videos. To the best\nof our knowledge, with face data from 400 subjects, this dataset represents the\nmost extensive collection of publicly available indoor and long-range outdoor\nthermal-visible face imagery. Lastly, we show that our end-to-end\nthermal-to-visible face verification system provides strong performance on the\nMILAB-VTF(B) dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peri_N/0/1/0/all/0/1\">Neehar Peri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bourlai_T/0/1/0/all/0/1\">Thirimachos Bourlai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Superpixel-guided Discriminative Low-rank Representation of Hyperspectral Images for Classification. (arXiv:2108.11172v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11172","description":"<p>In this paper, we propose a novel classification scheme for the remotely\nsensed hyperspectral image (HSI), namely SP-DLRR, by comprehensively exploring\nits unique characteristics, including the local spatial information and\nlow-rankness. SP-DLRR is mainly composed of two modules, i.e., the\nclassification-guided superpixel segmentation and the discriminative low-rank\nrepresentation, which are iteratively conducted. Specifically, by utilizing the\nlocal spatial information and incorporating the predictions from a typical\nclassifier, the first module segments pixels of an input HSI (or its\nrestoration generated by the second module) into superpixels. According to the\nresulting superpixels, the pixels of the input HSI are then grouped into\nclusters and fed into our novel discriminative low-rank representation model\nwith an effective numerical solution. Such a model is capable of increasing the\nintra-class similarity by suppressing the spectral variations locally while\npromoting the inter-class discriminability globally, leading to a restored HSI\nwith more discriminative pixels. Experimental results on three benchmark\ndatasets demonstrate the significant superiority of SP-DLRR over\nstate-of-the-art methods, especially for the case with an extremely limited\nnumber of training pixels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shujun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Junhui Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yuheng Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1\">Shaohui Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Riemannian Framework for Analysis of Human Body Surface. (arXiv:2108.11449v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11449","description":"<p>We propose a novel framework for comparing 3D human shapes under the change\nof shape and pose. This problem is challenging since 3D human shapes vary\nsignificantly across subjects and body postures. We solve this problem by using\na Riemannian approach. Our core contribution is the mapping of the human body\nsurface to the space of metrics and normals. We equip this space with a family\nof Riemannian metrics, called Ebin (or DeWitt) metrics. We treat a human body\nsurface as a point in a \"shape space\" equipped with a family of Riemannian\nmetrics. The family of metrics is invariant under rigid motions and\nreparametrizations; hence it induces a metric on the \"shape space\" of surfaces.\nUsing the alignment of human bodies with a given template, we show that this\nfamily of metrics allows us to distinguish the changes in shape and pose. The\nproposed framework has several advantages. First, we define a family of metrics\nwith desired invariance properties for the comparison of human shape. Second,\nwe present an efficient framework to compute geodesic paths between human shape\ngiven the chosen metric. Third, this framework provides some basic tools for\nstatistical shape analysis of human body surfaces. Finally, we demonstrate the\nutility of the proposed framework in pose and shape retrieval of human body.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pierson_E/0/1/0/all/0/1\">Emery Pierson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daoudi_M/0/1/0/all/0/1\">Mohamed Daoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tumpach_A/0/1/0/all/0/1\">Alice-Barbara Tumpach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PoissonSeg: Semi-Supervised Few-Shot Medical Image Segmentation via Poisson Learning. (arXiv:2108.11694v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11694","description":"<p>The application of deep learning to medical image segmentation has been\nhampered due to the lack of abundant pixel-level annotated data. Few-shot\nSemantic Segmentation (FSS) is a promising strategy for breaking the deadlock.\nHowever, a high-performing FSS model still requires sufficient pixel-level\nannotated classes for training to avoid overfitting, which leads to its\nperformance bottleneck in medical image segmentation due to the unmet need for\nannotations. Thus, semi-supervised FSS for medical images is accordingly\nproposed to utilize unlabeled data for further performance improvement.\nNevertheless, existing semi-supervised FSS methods has two obvious defects: (1)\nneglecting the relationship between the labeled and unlabeled data; (2) using\nunlabeled data directly for end-to-end training leads to degenerated\nrepresentation learning. To address these problems, we propose a novel\nsemi-supervised FSS framework for medical image segmentation. The proposed\nframework employs Poisson learning for modeling data relationship and\npropagating supervision signals, and Spatial Consistency Calibration for\nencouraging the model to learn more coherent representations. In this process,\nunlabeled samples do not involve in end-to-end training, but provide\nsupervisory information for query image segmentation through graph-based\nlearning. We conduct extensive experiments on three medical image segmentation\ndatasets (i.e. ISIC skin lesion segmentation, abdominal organs segmentation for\nMRI and abdominal organs segmentation for CT) to demonstrate the\nstate-of-the-art performance and broad applicability of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xiaoang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guokai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huilin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jihao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianwei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Ye Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAENet: A Progressive Attention-Enhanced Network for 3D to 2D Retinal Vessel Segmentation. (arXiv:2108.11695v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.11695","description":"<p>3D to 2D retinal vessel segmentation is a challenging problem in Optical\nCoherence Tomography Angiography (OCTA) images. Accurate retinal vessel\nsegmentation is important for the diagnosis and prevention of ophthalmic\ndiseases. However, making full use of the 3D data of OCTA volumes is a vital\nfactor for obtaining satisfactory segmentation results. In this paper, we\npropose a Progressive Attention-Enhanced Network (PAENet) based on attention\nmechanisms to extract rich feature representation. Specifically, the framework\nconsists of two main parts, the three-dimensional feature learning path and the\ntwo-dimensional segmentation path. In the three-dimensional feature learning\npath, we design a novel Adaptive Pooling Module (APM) and propose a new\nQuadruple Attention Module (QAM). The APM captures dependencies along the\nprojection direction of volumes and learns a series of pooling coefficients for\nfeature fusion, which efficiently reduces feature dimension. In addition, the\nQAM reweights the features by capturing four-group cross-dimension\ndependencies, which makes maximum use of 4D feature tensors. In the\ntwo-dimensional segmentation path, to acquire more detailed information, we\npropose a Feature Fusion Module (FFM) to inject 3D information into the 2D\npath. Meanwhile, we adopt the Polarized Self-Attention (PSA) block to model the\nsemantic interdependencies in spatial and channel dimensions respectively.\nExperimentally, our extensive experiments on the OCTA-500 dataset show that our\nproposed algorithm achieves state-of-the-art performance compared with previous\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Z/0/1/0/all/0/1\">Zhuojie Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A realistic approach to generate masked faces applied on two novel masked face recognition data sets. (arXiv:2109.01745v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01745","description":"<p>The COVID-19 pandemic raises the problem of adapting face recognition systems\nto the new reality, where people may wear surgical masks to cover their noses\nand mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for\ntraining these systems were released before the pandemic, so they now seem\nunsuited due to the lack of examples of people wearing masks. We propose a\nmethod for enhancing data sets containing faces without masks by creating\nsynthetic masks and overlaying them on faces in the original images. Our method\nrelies on SparkAR Studio, a developer program made by Facebook that is used to\ncreate Instagram face filters. In our approach, we use 9 masks of different\ncolors, shapes and fabrics. We employ our method to generate a number of\n445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254\n(96.8%) masks for the CelebA data set, releasing the mask images at\nhttps://github.com/securifai/masked_faces. We show that our method produces\nsignificantly more realistic training examples of masks overlaid on faces by\nasking volunteers to qualitatively compare it to other methods or data sets\ndesigned for the same task. We also demonstrate the usefulness of our method by\nevaluating state-of-the-art face recognition systems (FaceNet, VGG-face,\nArcFace) trained on our enhanced data sets and showing that they outperform\nequivalent systems trained on original data sets (containing faces without\nmasks) or competing data sets (containing masks generated by related methods),\nwhen the test benchmarks contain masked faces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mare_T/0/1/0/all/0/1\">Tudor Mare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duta_G/0/1/0/all/0/1\">Georgian Duta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgescu_M/0/1/0/all/0/1\">Mariana-Iuliana Georgescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandru_A/0/1/0/all/0/1\">Adrian Sandru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1\">Bogdan Alexe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Out-of-distribution Generalization of Probabilistic Image Modelling. (arXiv:2109.02639v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.02639","description":"<p>Out-of-distribution (OOD) detection and lossless compression constitute two\nproblems that can be solved by the training of probabilistic models on a first\ndataset with subsequent likelihood evaluation on a second dataset, where data\ndistributions differ. By defining the generalization of probabilistic models in\nterms of likelihood we show that, in the case of image models, the OOD\ngeneralization ability is dominated by local features. This motivates our\nproposal of a Local Autoregressive model that exclusively models local image\nfeatures towards improving OOD performance. We apply the proposed model to OOD\ndetection tasks and achieve state-of-the-art unsupervised OOD detection\nperformance without the introduction of additional data. Additionally, we\nemploy our model to build a new lossless image compressor: NeLLoC (Neural Local\nLossless Compressor) and report state-of-the-art compression rates and model\nsize.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mingtian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Andi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Autism Spectrum Disorder Based on Individual-Aware Down-Sampling and Multi-Modal Learning. (arXiv:2109.09129v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.09129","description":"<p>Autism Spectrum Disorder(ASD) is a set of neurodevelopmental conditions that\naffect patients' social abilities. In recent years, many studies have employed\ndeep learning to diagnose this brain dysfunction through functional MRI (fMRI).\nHowever, existing approaches solely focused on the abnormal brain functional\nconnections but ignored the impact of regional activities. Due to this biased\nprior knowledge, previous diagnosis models suffered from inter-site measurement\nheterogeneity and inter-individual phenotypic differences. To address this\nissue, we propose a novel feature extraction method for fMRI that can learn a\npersonalized lower-resolution representation of the entire brain networking\nregarding both the functional connections and regional activities.\nSpecifically, we abstract the brain imaging as a graph structure and\nstraightforwardly downsample it to substructures by hierarchical graph pooling.\nTo further recalibrate the distribution of the extracted features under\nphenotypic information, we subsequently embed the sparse feature vectors into a\npopulation graph, where the hidden inter-subject heterogeneity and homogeneity\nare explicitly expressed as inter- and intra-community connectivity\ndifferences, and utilize Graph Convolutional Networks to learn the node\nembeddings. By these means, our framework can extract features directly and\nefficiently from the entire fMRI and be aware of implicit inter-individual\nvariance. We have evaluated our framework on the ABIDE-I dataset with 10-fold\ncross-validation. The present model has achieved a mean classification accuracy\nof 87.62\\% and a mean AUC of 0.92, better than the state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Pan_L/0/1/0/all/0/1\">Li Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jundong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_M/0/1/0/all/0/1\">Mingqin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_C/0/1/0/all/0/1\">Chi Wah Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_K/0/1/0/all/0/1\">Kei Hang Katie Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skin Deep Unlearning: Artefact and Instrument Debiasing in the Context of Melanoma Classification. (arXiv:2109.09818v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.09818","description":"<p>Convolutional Neural Networks have demonstrated dermatologist-level\nperformance in the classification of melanoma and other skin lesions, but\nprediction irregularities due to biases seen within the training data are an\nissue that should be addressed before widespread deployment is possible. In\nthis work, we robustly remove bias and spurious variation from an automated\nmelanoma classification pipeline using two leading bias unlearning techniques.\nWe show that the biases introduced by surgical markings and rulers presented in\nprevious studies can be reasonably mitigated using these bias removal methods.\nWe also demonstrate the generalisation benefits of unlearning spurious\nvariation relating to the imaging instrument used to capture lesion images.\nContributions of this work include the application of different debiasing\ntechniques for artefact bias removal and the concept of instrument bias\nunlearning for domain generalisation in melanoma detection. Our experimental\nresults provide evidence that the effects of each of the aforementioned biases\nare notably reduced, with different debiasing techniques excelling at different\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bevan_P/0/1/0/all/0/1\">Peter J. Bevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atapour_Abarghouei_A/0/1/0/all/0/1\">Amir Atapour-Abarghouei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Data-Free Domain Generalization. (arXiv:2110.04545v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.04545","description":"<p>In this work, we investigate the unexplored intersection of domain\ngeneralization and data-free learning. In particular, we address the question:\nHow can knowledge contained in models trained on different source data domains\ncan be merged into a single model that generalizes well to unseen target\ndomains, in the absence of source and target domain data? Machine learning\nmodels that can cope with domain shift are essential for for real-world\nscenarios with often changing data distributions. Prior domain generalization\nmethods typically rely on using source domain data, making them unsuitable for\nprivate decentralized data. We define the novel problem of Data-Free Domain\nGeneralization (DFDG), a practical setting where models trained on the source\ndomains separately are available instead of the original datasets, and\ninvestigate how to effectively solve the domain generalization problem in that\ncase. We propose DEKAN, an approach that extracts and fuses domain-specific\nknowledge from the available teacher models into a student model robust to\ndomain shift. Our empirical evaluation demonstrates the effectiveness of our\nmethod which achieves first state-of-the-art results in DFDG by significantly\noutperforming ensemble and data-free knowledge distillation baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Frikha_A/0/1/0/all/0/1\">Ahmed Frikha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haokun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krompass_D/0/1/0/all/0/1\">Denis Krompa&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Runkler_T/0/1/0/all/0/1\">Thomas Runkler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modality-Guided Subnetwork for Salient Object Detection. (arXiv:2110.04904v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.04904","description":"<p>Recent RGBD-based models for saliency detection have attracted research\nattention. The depth clues such as boundary clues, surface normal, shape\nattribute, etc., contribute to the identification of salient objects with\ncomplicated scenarios. However, most RGBD networks require multi-modalities\nfrom the input side and feed them separately through a two-stream design, which\ninevitably results in extra costs on depth sensors and computation. To tackle\nthese inconveniences, we present in this paper a novel fusion design named\nmodality-guided subnetwork (MGSnet). It has the following superior designs: 1)\nOur model works for both RGB and RGBD data, and dynamically estimating depth if\nnot available. Taking the inner workings of depth-prediction networks into\naccount, we propose to estimate the pseudo-geometry maps from RGB input -\nessentially mimicking the multi-modality input. 2) Our MGSnet for RGB SOD\nresults in real-time inference but achieves state-of-the-art performance\ncompared to other RGB models. 3) The flexible and lightweight design of MGS\nfacilitates the integration into RGBD two-streaming models. The introduced\nfusion design enables a cross-modality interaction to enable further progress\nbut with a minimal cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zongwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allibert_G/0/1/0/all/0/1\">Guillaume Allibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolz_C/0/1/0/all/0/1\">Christophe Stolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demonceaux_C/0/1/0/all/0/1\">C&#xe9;dric Demonceaux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Fine-grained Layout Analysis for the Historical Tibetan Document Based on the Instance Segmentation. (arXiv:2110.08164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08164","description":"<p>Accurate layout analysis without subsequent text-line segmentation remains an\nongoing challenge, especially when facing the Kangyur, a kind of historical\nTibetan document featuring considerable touching components and mottled\nbackground. Aiming at identifying different regions in document images, layout\nanalysis is indispensable for subsequent procedures such as character\nrecognition. However, there was only a little research being carried out to\nperform line-level layout analysis which failed to deal with the Kangyur. To\nobtain the optimal results, a fine-grained sub-line level layout analysis\napproach is presented. Firstly, we introduced an accelerated method to build\nthe dataset which is dynamic and reliable. Secondly, enhancement had been made\nto the SOLOv2 according to the characteristics of the Kangyur. Then, we fed the\nenhanced SOLOv2 with the prepared annotation file during the training phase.\nOnce the network is trained, instances of the text line, sentence, and titles\ncan be segmented and identified during the inference stage. The experimental\nresults show that the proposed method delivers a decent 72.7% AP on our\ndataset. In general, this preliminary research provides insights into the\nfine-grained sub-line level layout analysis and testifies the SOLOv2-based\napproaches. We also believe that the proposed methods can be adopted on other\nlanguage documents with various layouts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Penghai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weilan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhengqi Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guowei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuqi Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparing Human and Machine Bias in Face Recognition. (arXiv:2110.08396v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08396","description":"<p>Much recent research has uncovered and discussed serious concerns of bias in\nfacial analysis technologies, finding performance disparities between groups of\npeople based on perceived gender, skin type, lighting condition, etc. These\naudits are immensely important and successful at measuring algorithmic bias but\nhave two major challenges: the audits (1) use facial recognition datasets which\nlack quality metadata, like LFW and CelebA, and (2) do not compare their\nobserved algorithmic bias to the biases of their human alternatives. In this\npaper, we release improvements to the LFW and CelebA datasets which will enable\nfuture researchers to obtain measurements of algorithmic bias that are not\ntainted by major flaws in the dataset (e.g. identical images appearing in both\nthe gallery and test set). We also use these new data to develop a series of\nchallenging facial identification and verification questions that we\nadministered to various algorithms and a large, balanced sample of human\nreviewers. We find that both computer models and human survey participants\nperform significantly better at the verification task, generally obtain lower\naccuracy rates on dark-skinned or female subjects for both tasks, and obtain\nhigher accuracy rates when their demographics match that of the question.\nComputer models are observed to achieve a higher level of accuracy than the\nsurvey participants on both tasks and exhibit bias to similar degrees as the\nhuman survey participants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dooley_S/0/1/0/all/0/1\">Samuel Dooley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downing_R/0/1/0/all/0/1\">Ryan Downing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_G/0/1/0/all/0/1\">George Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_N/0/1/0/all/0/1\">Nathan Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thymes_B/0/1/0/all/0/1\">Bradon Thymes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorkelsdottir_G/0/1/0/all/0/1\">Gudrun Thorkelsdottir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_Miott_T/0/1/0/all/0/1\">Tiye Kurtz-Miott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattson_R/0/1/0/all/0/1\">Rachel Mattson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obiwumi_O/0/1/0/all/0/1\">Olufemi Obiwumi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherepanova_V/0/1/0/all/0/1\">Valeriia Cherepanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1\">John P Dickerson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation. (arXiv:2110.08733v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.08733","description":"<p>Deep learning approaches have shown promising results in remote sensing high\nspatial resolution (HSR) land-cover mapping. However, urban and rural scenes\ncan show completely different geographical landscapes, and the inadequate\ngeneralizability of these algorithms hinders city-level or national-level\nmapping. Most of the existing HSR land-cover datasets mainly promote the\nresearch of learning semantic representation, thereby ignoring the model\ntransferability. In this paper, we introduce the Land-cOVEr Domain Adaptive\nsemantic segmentation (LoveDA) dataset to advance semantic and transferable\nlearning. The LoveDA dataset contains 5987 HSR images with 166768 annotated\nobjects from three different cities. Compared to the existing datasets, the\nLoveDA dataset encompasses two domains (urban and rural), which brings\nconsiderable challenges due to the: 1) multi-scale objects; 2) complex\nbackground samples; and 3) inconsistent class distributions. The LoveDA dataset\nis suitable for both land-cover semantic segmentation and unsupervised domain\nadaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on\neleven semantic segmentation methods and eight UDA methods. Some exploratory\nstudies including multi-scale architectures and strategies, additional\nbackground supervision, and pseudo-label analysis were also carried out to\naddress these challenges. The code and data are available at\nhttps://github.com/Junjue-Wang/LoveDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zhuo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1\">Ailong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaoyan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yanfei Zhong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Image Fusion Using Deep Image Priors. (arXiv:2110.09490v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09490","description":"<p>A significant number of researchers have recently applied deep learning\nmethods to image fusion. However, most of these works either require a large\namount of training data or depend on pre-trained models or frameworks. This\ninevitably encounters a shortage of training data or a mismatch between the\nframework and the actual problem. Recently, the publication of Deep Image Prior\n(DIP) method made it possible to do image restoration totally\ntraining-data-free. However, the original design of DIP is hard to be\ngeneralized to multi-image processing problems. This paper introduces a novel\nloss calculation structure, in the framework of DIP, while formulating image\nfusion as an inverse problem. This enables the extension of DIP to general\nmultisensor/multifocus image fusion problems. Secondly, we propose a\nmulti-channel approach to improve the effect of DIP. Finally, an evaluation is\nconducted using several commonly used image fusion assessment metrics. The\nresults are compared with state-of-the-art traditional and deep learning image\nfusion methods. Our method outperforms previous techniques for a range of\nmetrics. In particular, it is shown to provide the best objective results for\nmost metrics when applied to medical images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xudong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_P/0/1/0/all/0/1\">Paul Hill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Achim_A/0/1/0/all/0/1\">Alin Achim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Distillation: Aggregating Knowledge from Multiple Paths for Efficient Distillation. (arXiv:2110.09674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09674","description":"<p>Knowledge Distillation is becoming one of the primary trends among neural\nnetwork compression algorithms to improve the generalization performance of a\nsmaller student model with guidance from a larger teacher model. This momentous\nrise in applications of knowledge distillation is accompanied by the\nintroduction of numerous algorithms for distilling the knowledge such as soft\ntargets and hint layers. Despite this advancement in different techniques for\ndistilling the knowledge, the aggregation of different paths for distillation\nhas not been studied comprehensively. This is of particular significance, not\nonly because different paths have different importance, but also due to the\nfact that some paths might have negative effects on the generalization\nperformance of the student model. Hence, we need to adaptively adjust the\nimportance of each path to maximize the impact of distillation on the student\nmodel. In this paper, we explore different approaches for aggregating these\ndifferent paths and introduce our proposed adaptive approach based on multitask\nlearning methods. We empirically demonstrate the effectiveness of the proposed\napproach over other baselines on the applications of knowledge distillation in\nclassification, semantic segmentation, and object detection tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chennupati_S/0/1/0/all/0/1\">Sumanth Chennupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamani_M/0/1/0/all/0/1\">Mohammad Mahdi Kamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhongwei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Not to Reconstruct Anomalies. (arXiv:2110.09742v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09742","description":"<p>Video anomaly detection is often seen as one-class classification (OCC)\nproblem due to the limited availability of anomaly examples. Typically, to\ntackle this problem, an autoencoder (AE) is trained to reconstruct the input\nwith training set consisting only of normal data. At test time, the AE is then\nexpected to well reconstruct the normal data while poorly reconstructing the\nanomalous data. However, several studies have shown that, even with only normal\ndata training, AEs can often start reconstructing anomalies as well which\ndepletes the anomaly detection performance. To mitigate this problem, we\npropose a novel methodology to train AEs with the objective of reconstructing\nonly normal data, regardless of the input (i.e., normal or abnormal). Since no\nreal anomalies are available in the OCC settings, the training is assisted by\npseudo anomalies that are generated by manipulating normal data to simulate the\nout-of-normal-data distribution. We additionally propose two ways to generate\npseudo anomalies: patch and skip frame based. Extensive experiments on three\nchallenging video anomaly datasets demonstrate the effectiveness of our method\nin improving conventional AEs, achieving state-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1\">Marcella Astrid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1\">Muhammad Zaigham Zaheer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jae-Yeong Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung-Ik Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory-Augmented Deep Unfolding Network for Compressive Sensing. (arXiv:2110.09766v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09766","description":"<p>Mapping a truncated optimization method into a deep neural network, deep\nunfolding network (DUN) has attracted growing attention in compressive sensing\n(CS) due to its good interpretability and high performance. Each stage in DUNs\ncorresponds to one iteration in optimization. By understanding DUNs from the\nperspective of the human brain's memory processing, we find there exists two\nissues in existing DUNs. One is the information between every two adjacent\nstages, which can be regarded as short-term memory, is usually lost seriously.\nThe other is no explicit mechanism to ensure that the previous stages affect\nthe current stage, which means memory is easily forgotten. To solve these\nissues, in this paper, a novel DUN with persistent memory for CS is proposed,\ndubbed Memory-Augmented Deep Unfolding Network (MADUN). We design a\nmemory-augmented proximal mapping module (MAPMM) by combining two types of\nmemory augmentation mechanisms, namely High-throughput Short-term Memory (HSM)\nand Cross-stage Long-term Memory (CLM). HSM is exploited to allow DUNs to\ntransmit multi-channel short-term memory, which greatly reduces information\nloss between adjacent stages. CLM is utilized to develop the dependency of deep\ninformation across cascading stages, which greatly enhances network\nrepresentation capability. Extensive CS experiments on natural and MR images\nshow that with the strong ability to maintain and balance information our MADUN\noutperforms existing state-of-the-art methods by a large margin. The source\ncode is available at https://github.com/jianzhangcs/MADUN/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiechong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Bin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Control of Artistic Styles in Image Generation. (arXiv:2110.10278v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10278","description":"<p>Recent advances in generative models and adversarial training have enabled\nartificially generating artworks in various artistic styles. It is highly\ndesirable to gain more control over the generated style in practice. However,\nartistic styles are unlike object categories -- there are a continuous spectrum\nof styles distinguished by subtle differences. Few works have been explored to\ncapture the continuous spectrum of styles and apply it to a style generation\ntask. In this paper, we propose to achieve this by embedding original artwork\nexamples into a continuous style space. The style vectors are fed to the\ngenerator and discriminator to achieve fine-grained control. Our method can be\nused with common generative adversarial networks (such as StyleGAN).\nExperiments show that our method not only precisely controls the fine-grained\nartistic style but also improves image quality over vanilla StyleGAN as\nmeasured by FID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1\">Xin Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiayi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Z/0/1/0/all/0/1\">Zhenyu Liao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASSANet: An Anisotropic Separable Set Abstraction for Efficient Point Cloud Representation Learning. (arXiv:2110.10538v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.10538","description":"<p>Access to 3D point cloud representations has been widely facilitated by LiDAR\nsensors embedded in various mobile devices. This has led to an emerging need\nfor fast and accurate point cloud processing techniques. In this paper, we\nrevisit and dive deeper into PointNet++, one of the most influential yet\nunder-explored networks, and develop faster and more accurate variants of the\nmodel. We first present a novel Separable Set Abstraction (SA) module that\ndisentangles the vanilla SA module used in PointNet++ into two separate\nlearning stages: (1) learning channel correlation and (2) learning spatial\ncorrelation. The Separable SA module is significantly faster than the vanilla\nversion, yet it achieves comparable performance. We then introduce a new\nAnisotropic Reduction function into our Separable SA module and propose an\nAnisotropic Separable SA (ASSA) module that substantially increases the\nnetwork's accuracy. We later replace the vanilla SA modules in PointNet++ with\nthe proposed ASSA module, and denote the modified network as ASSANet. Extensive\nexperiments on point cloud classification, semantic segmentation, and part\nsegmentation show that ASSANet outperforms PointNet++ and other methods,\nachieving much higher accuracy and faster speeds. In particular, ASSANet\noutperforms PointNet++ by $7.4$ mIoU on S3DIS Area 5, while maintaining $1.6\n\\times $ faster inference speed on a single NVIDIA 2080Ti GPU. Our scaled\nASSANet variant achieves $66.8$ mIoU and outperforms KPConv, while being more\nthan $54 \\times$ faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_G/0/1/0/all/0/1\">Guocheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1\">Hasan Abed Al Kader Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guohao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1\">Ali Thabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Location Constraint Prototype Loss for Open Set Recognition. (arXiv:2110.11013v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11013","description":"<p>One of the challenges in pattern recognition is open set recognition.\nCompared with closed set recognition, open set recognition needs to reduce not\nonly the empirical risk, but also the open space risk, and the reduction of\nthese two risks corresponds to classifying the known classes and identifying\nthe unknown classes respectively. How to reduce the open space risk is the key\nof open set recognition. This paper explores the origin of the open space risk\nby analyzing the distribution of known and unknown classes features. On this\nbasis, the spatial location constraint prototype loss function is proposed to\nreduce the two risks simultaneously. Extensive experiments on multiple\nbenchmark datasets and many visualization results indicate that our methods is\nsuperior to most existing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xia_Z/0/1/0/all/0/1\">Ziheng Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1\">Ganggang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Penghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongwei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Graph Convolutional Networks for Human Action Synthesis. (arXiv:2110.11191v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11191","description":"<p>Synthesising the spatial and temporal dynamics of the human body skeleton\nremains a challenging task, not only in terms of the quality of the generated\nshapes, but also of their diversity, particularly to synthesise realistic body\nmovements of a specific action (action conditioning). In this paper, we propose\nKinetic-GAN, a novel architecture that leverages the benefits of Generative\nAdversarial Networks and Graph Convolutional Networks to synthesise the\nkinetics of the human body. The proposed adversarial architecture can condition\nup to 120 different actions over local and global body movements while\nimproving sample quality and diversity through latent space disentanglement and\nstochastic variations. Our experiments were carried out in three well-known\ndatasets, where Kinetic-GAN notably surpasses the state-of-the-art methods in\nterms of distribution quality metrics while having the ability to synthesise\nmore than one order of magnitude regarding the number of different actions. Our\ncode and models are publicly available at\nhttps://github.com/DegardinBruno/Kinetic-GAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Degardin_B/0/1/0/all/0/1\">Bruno Degardin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neves_J/0/1/0/all/0/1\">Jo&#xe3;o Neves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lopes_V/0/1/0/all/0/1\">Vasco Lopes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brito_J/0/1/0/all/0/1\">Jo&#xe3;o Brito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoubi_E/0/1/0/all/0/1\">Ehsan Yaghoubi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proenca_H/0/1/0/all/0/1\">Hugo Proen&#xe7;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast Graph Sampling for Short Video Summarization using Gershgorin Disc Alignment. (arXiv:2110.11420v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11420","description":"<p>We study the problem of efficiently summarizing a short video into several\nkeyframes, leveraging recent progress in fast graph sampling. Specifically, we\nfirst construct a similarity path graph (SPG) $\\mathcal{G}$, represented by\ngraph Laplacian matrix $\\mathbf{L}$, where the similarities between adjacent\nframes are encoded as positive edge weights. We show that maximizing the\nsmallest eigenvalue $\\lambda_{\\min}(\\mathbf{B})$ of a coefficient matrix\n$\\mathbf{B} = \\text{diag}(\\mathbf{a}) + \\mu \\mathbf{L}$, where $\\mathbf{a}$ is\nthe binary keyframe selection vector, is equivalent to minimizing a worst-case\nsignal reconstruction error. We prove that, after partitioning $\\mathcal{G}$\ninto $Q$ sub-graphs $\\{\\mathcal{G}^q\\}^Q_{q=1}$, the smallest Gershgorin circle\ntheorem (GCT) lower bound of $Q$ corresponding coefficient matrices -- $\\min_q\n\\lambda^-_{\\min}(\\mathbf{B}^q)$ -- is a lower bound for\n$\\lambda_{\\min}(\\mathbf{B})$. This inspires a fast graph sampling algorithm to\niteratively partition $\\mathcal{G}$ into $Q$ sub-graphs using $Q$ samples\n(keyframes), while maximizing $\\lambda^-_{\\min}(\\mathbf{B}^q)$ for each\nsub-graph $\\mathcal{G}^q$. Experimental results show that our algorithm\nachieves comparable video summarization performance as state-of-the-art\nmethods, at a substantially reduced complexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahami_S/0/1/0/all/0/1\">Sadid Sahami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1\">Gene Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AEI: Actors-Environment Interaction with Adaptive Attention for Temporal Action Proposals Generation. (arXiv:2110.11474v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11474","description":"<p>Humans typically perceive the establishment of an action in a video through\nthe interaction between an actor and the surrounding environment. An action\nonly starts when the main actor in the video begins to interact with the\nenvironment, while it ends when the main actor stops the interaction. Despite\nthe great progress in temporal action proposal generation, most existing works\nignore the aforementioned fact and leave their model learning to propose\nactions as a black-box. In this paper, we make an attempt to simulate that\nability of a human by proposing Actor Environment Interaction (AEI) network to\nimprove the video representation for temporal action proposals generation. AEI\ncontains two modules, i.e., perception-based visual representation (PVR) and\nboundary-matching module (BMM). PVR represents each video snippet by taking\nhuman-human relations and humans-environment relations into consideration using\nthe proposed adaptive attention mechanism. Then, the video representation is\ntaken by BMM to generate action proposals. AEI is comprehensively evaluated in\nActivityNet-1.3 and THUMOS-14 datasets, on temporal action proposal and\ndetection tasks, with two boundary-matching architectures (i.e., CNN-based and\nGCN-based) and two classifiers (i.e., Unet and P-GCN). Our AEI robustly\noutperforms the state-of-the-art methods with remarkable performance and\ngeneralization for both temporal action proposal generation and temporal action\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_K/0/1/0/all/0/1\">Khoa Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hyekang Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamazaki_K/0/1/0/all/0/1\">Kashu Yamazaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Sang Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1\">Ngan Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Semi-Supervised Learning for 3D Objects. (arXiv:2110.11601v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11601","description":"<p>In recent years, semi-supervised learning has been widely explored and shows\nexcellent data efficiency for 2D data. There is an emerging need to improve\ndata efficiency for 3D tasks due to the scarcity of labeled 3D data. This paper\nexplores how the coherence of different modelities of 3D data (e.g. point\ncloud, image, and mesh) can be used to improve data efficiency for both 3D\nclassification and retrieval tasks. We propose a novel multimodal\nsemi-supervised learning framework by introducing instance-level consistency\nconstraint and a novel multimodal contrastive prototype (M2CP) loss. The\ninstance-level consistency enforces the network to generate consistent\nrepresentations for multimodal data of the same object regardless of its\nmodality. The M2CP maintains a multimodal prototype for each class and learns\nfeatures with small intra-class variations by minimizing the feature distance\nof each object to its prototype while maximizing the distance to the others.\nOur proposed framework significantly outperforms all the state-of-the-art\ncounterparts for both classification and retrieval tasks by a large margin on\nthe modelNet10 and ModelNet40 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1\">Longlong Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yang Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">YingLi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciCap: Generating Captions for Scientific Figures. (arXiv:2110.11624v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.11624","description":"<p>Researchers use figures to communicate rich, complex information in\nscientific papers. The captions of these figures are critical to conveying\neffective messages. However, low-quality figure captions commonly occur in\nscientific articles and may decrease understanding. In this paper, we propose\nan end-to-end neural framework to automatically generate informative,\nhigh-quality captions for scientific figures. To this end, we introduce SCICAP,\na large-scale figure-caption dataset based on computer science arXiv papers\npublished between 2010 and 2020. After pre-processing - including figure-type\nclassification, sub-figure identification, text normalization, and caption text\nselection - SCICAP contained more than two million figures extracted from over\n290,000 papers. We then established baseline models that caption graph plots,\nthe dominant (19.2%) figure type. The experimental results showed both\nopportunities and steep challenges of generating captions for scientific\nfigures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_T/0/1/0/all/0/1\">Ting-Yao Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_C/0/1/0/all/0/1\">C. Lee Giles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Ting-Hao &#x27;Kenneth&#x27; Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional Variational Autoencoder for Learned Image Reconstruction. (arXiv:2110.11681v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11681","description":"<p>Learned image reconstruction techniques using deep neural networks have\nrecently gained popularity, and have delivered promising empirical results.\nHowever, most approaches focus on one single recovery for each observation, and\nthus neglect the uncertainty information. In this work, we develop a novel\ncomputational framework that approximates the posterior distribution of the\nunknown image at each query observation. The proposed framework is very\nflexible: It handles implicit noise models and priors, it incorporates the data\nformation process (i.e., the forward operator), and the learned reconstructive\nproperties are transferable between different datasets. Once the network is\ntrained using the conditional variational autoencoder loss, it provides a\ncomputationally efficient sampler for the approximate posterior distribution\nvia feed-forward propagation, and the summarizing statistics of the generated\nsamples are used for both point-estimation and uncertainty quantification. We\nillustrate the proposed framework with extensive numerical experiments on\npositron emission tomography (with both moderate and low count levels) showing\nthat the framework generates high-quality samples when compared with\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbano_R/0/1/0/all/0/1\">Riccardo Barbano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Bangti Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Using Clothes Style Transfer for Scenario-aware Person Video Generation. (arXiv:2110.11894v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.11894","description":"<p>Clothes style transfer for person video generation is a challenging task, due\nto drastic variations of intra-person appearance and video scenarios. To tackle\nthis problem, most recent AdaIN-based architectures are proposed to extract\nclothes and scenario features for generation. However, these approaches suffer\nfrom being short of fine-grained details and are prone to distort the origin\nperson. To further improve the generation performance, we propose a novel\nframework with disentangled multi-branch encoders and a shared decoder.\nMoreover, to pursue the strong video spatio-temporal consistency, an\ninner-frame discriminator is delicately designed with input being cross-frame\ndifference. Besides, the proposed framework possesses the property of scenario\nadaptation. Extensive experiments on the TEDXPeople benchmark demonstrate the\nsuperiority of our method over state-of-the-art approaches in terms of image\nquality and video coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jingning Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1\">Benlai Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingjie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1\">Siyuan Bian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Wenyi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xiang Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zejun Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-10-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}