{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-03-21T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Privacy-Preserving Speech Representation Learning using Vector Quantization. (arXiv:2203.09518v1 [eess.AS])","link":"http://arxiv.org/abs/2203.09518","description":"<p>With the popularity of virtual assistants (e.g., Siri, Alexa), the use of\nspeech recognition is now becoming more and more widespread.However, speech\nsignals contain a lot of sensitive information, such as the speaker's identity,\nwhich raises privacy concerns.The presented experiments show that the\nrepresentations extracted by the deep layers of speech recognition networks\ncontain speaker information.This paper aims to produce an anonymous\nrepresentation while preserving speech recognition performance.To this end, we\npropose to use vector quantization to constrain the representation space and\ninduce the network to suppress the speaker identity.The choice of the\nquantization dictionary size allows to configure the trade-off between utility\n(speech recognition) and privacy (speaker identity concealment).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Champion_P/0/1/0/all/0/1\">Pierre Champion</a> (MULTISPEECH), <a href=\"http://arxiv.org/find/eess/1/au:+Jouvet_D/0/1/0/all/0/1\">Denis Jouvet</a> (MULTISPEECH), <a href=\"http://arxiv.org/find/eess/1/au:+Larcher_A/0/1/0/all/0/1\">Anthony Larcher</a> (LIUM)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Federated Learning on Knowledge Graphs via Privacy-preserving Relation Embedding Aggregation. (arXiv:2203.09553v1 [cs.AI])","link":"http://arxiv.org/abs/2203.09553","description":"<p>Federated Learning (FL) on knowledge graphs (KGs) has yet to be as well\nstudied as other domains, such as computer vision and natural language\nprocessing. A recent study FedE first proposes an FL framework that shares\nentity embeddings of KGs across all clients. However, compared with model\nsharing in vanilla FL, entity embedding sharing from FedE would incur severe\nprivacy leakage. Specifically, the known entity embedding can be used to infer\nwhether a specific relation between two entities exists in a private client. In\nthis paper, we first develop a novel attack that aims to recover the original\ndata based on embedding information, which is further used to evaluate the\nvulnerabilities of FedE. Furthermore, we propose a Federated learning paradigm\nwith privacy-preserving Relation embedding aggregation (FedR) to tackle the\nprivacy issue in FedE. Compared to entity embedding sharing, relation embedding\nsharing policy can significantly reduce the communication cost due to its\nsmaller size of queries. We conduct extensive experiments to evaluate FedR with\nfive different embedding learning models and three benchmark KG datasets.\nCompared to FedE, FedR achieves similar utility and significant (nearly 2X)\nimprovements in both privacy and efficiency on link prediction task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lifu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Carl Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhanced Temporal Knowledge Embeddings with Contextualized Language Representations. (arXiv:2203.09590v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09590","description":"<p>With the emerging research effort to integrate structured and unstructured\nknowledge, many approaches incorporate factual knowledge into pre-trained\nlanguage models (PLMs) and apply the knowledge-enhanced PLMs on downstream NLP\ntasks. However, (1) they only consider \\textit{static} factual knowledge, but\nknowledge graphs (KGs) also contain \\textit{temporal facts} or \\textit{events}\nindicating evolutionary relationships among entities at different timestamps.\n(2) PLMs cannot be directly applied to many KG tasks, such as temporal KG\ncompletion.\n</p>\n<p>In this paper, we focus on \\textbf{e}nhancing temporal knowledge embeddings\nwith \\textbf{co}ntextualized \\textbf{la}nguage representations (ECOLA). We\nalign structured knowledge contained in temporal knowledge graphs with their\ntextual descriptions extracted from news articles and propose a novel\nknowledge-text prediction task to inject the abundant information from\ndescriptions into temporal knowledge embeddings. ECOLA jointly optimizes the\nknowledge-text prediction objective and the temporal knowledge embeddings,\nwhich can simultaneously take full advantage of textual and knowledge\ninformation. For training ECOLA, we introduce three temporal KG datasets with\naligned textual descriptions. Experimental results on the temporal knowledge\ngraph completion task show that ECOLA outperforms state-of-the-art temporal KG\nmodels by a large margin. The proposed datasets can serve as new temporal KG\nbenchmarks and facilitate future research on structured and unstructured\nknowledge integration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhen Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1\">Ruotong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Beiyan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zifeng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koppl_H/0/1/0/all/0/1\">Heinz K&#xf6;ppl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1\">Volker Tresp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Responsible Natural Language Annotation for the Varieties of Arabic. (arXiv:2203.09597v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09597","description":"<p>When building NLP models, there is a tendency to aim for broader coverage,\noften overlooking cultural and (socio)linguistic nuance. In this position\npaper, we make the case for care and attention to such nuances, particularly in\ndataset annotation, as well as the inclusion of cultural and linguistic\nexpertise in the process. We present a playbook for responsible dataset\ncreation for polyglossic, multidialectal languages. This work is informed by a\nstudy on Arabic annotation of social media content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">A. Stevie Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1\">Mona T. Diab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DP-KB: Data Programming with Knowledge Bases Improves Transformer Fine Tuning for Answer Sentence Selection. (arXiv:2203.09598v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09598","description":"<p>While transformers demonstrate impressive performance on many knowledge\nintensive (KI) tasks, their ability to serve as implicit knowledge bases (KBs)\nremains limited, as shown on several slot-filling, question-answering (QA),\nfact verification, and entity-linking tasks. In this paper, we implement an\nefficient, data-programming technique that enriches training data with\nKB-derived context and improves transformer utilization of encoded knowledge\nwhen fine-tuning for a particular QA task, namely answer sentence selection\n(AS2). Our method outperforms state of the art transformer approach on WikiQA\nand TrecQA, two widely studied AS2 benchmarks, increasing by 2.0% p@1, 1.3%\nMAP, 1.1% MRR, and 4.4% p@1, 0.9% MAP, 2.4% MRR, respectively. To demonstrate\nour improvements in an industry setting, we additionally evaluate our approach\non a proprietary dataset of Alexa QA pairs, and show increase of 2.3% F1 and\n2.0% MAP. We additionally find that these improvements remain even when KB\ncontext is omitted at inference time, allowing for the use of our models within\nexisting transformer workflows without additional latency or deployment costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jedema_N/0/1/0/all/0/1\">Nic Jedema</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Thuy Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1\">Alessandro Moschitti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robotic Speech Synthesis: Perspectives on Interactions, Scenarios, and Ethics. (arXiv:2203.09599v1 [cs.RO])","link":"http://arxiv.org/abs/2203.09599","description":"<p>In recent years, many works have investigated the feasibility of\nconversational robots for performing specific tasks, such as healthcare and\ninterview. Along with this development comes a practical issue: how should we\nsynthesize robotic voices to meet the needs of different situations? In this\npaper, we discuss this issue from three perspectives: 1) the difficulties of\nsynthesizing non-verbal and interaction-oriented speech signals, particularly\nbackchannels; 2) the scenario classification for robotic voice synthesis; 3)\nthe ethical issues regarding the design of robot voice for its emotion and\nidentity. We present the findings of relevant literature and our prior work,\ntrying to bring the attention of human-robot interaction researchers to design\nbetter conversational robots in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Importance of Data Size in Probing Fine-tuned Models. (arXiv:2203.09627v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09627","description":"<p>Several studies have investigated the reasons behind the effectiveness of\nfine-tuning, usually through the lens of probing. However, these studies often\nneglect the role of the size of the dataset on which the model is fine-tuned.\nIn this paper, we highlight the importance of this factor and its undeniable\nrole in probing performance. We show that the extent of encoded linguistic\nknowledge depends on the number of fine-tuning samples. The analysis also\nreveals that larger training data mainly affects higher layers, and that the\nextent of this change is a factor of the number of iterations updating the\nmodel during fine-tuning rather than the diversity of the training samples.\nFinally, we show through a set of experiments that fine-tuning data size\naffects the recoverability of the changes made to the model's linguistic\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrafarin_H/0/1/0/all/0/1\">Houman Mehrafarin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information. (arXiv:2203.09629v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09629","description":"<p>Transformer-based language models usually treat texts as linear sequences.\nHowever, most texts also have an inherent hierarchical structure, i.e., parts\nof a text can be identified using their position in this hierarchy. In\naddition, section titles usually indicate the common topic of their respective\nsentences. We propose a novel approach to formulate, extract, encode and inject\nhierarchical structure information explicitly into an extractive summarization\nmodel based on a pre-trained, encoder-only Transformer language model\n(HiStruct+ model), which improves SOTA ROUGEs for extractive summarization on\nPubMed and arXiv substantially. Using various experimental settings on three\ndatasets (i.e., CNN/DailyMail, PubMed and arXiv), our HiStruct+ model\noutperforms a strong baseline collectively, which differs from our model only\nin that the hierarchical structure information is not injected. It is also\nobserved that the more conspicuous hierarchical structure the dataset has, the\nlarger improvements our method gains. The ablation study demonstrates that the\nhierarchical position information is the main contributor to our model's SOTA\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_Q/0/1/0/all/0/1\">Qian Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorff_M/0/1/0/all/0/1\">Malte Ostendorff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rehm_G/0/1/0/all/0/1\">Georg Rehm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dim Wihl Gat Tun: The Case for Linguistic Expertise in NLP for Underdocumented Languages. (arXiv:2203.09632v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09632","description":"<p>Recent progress in NLP is driven by pretrained models leveraging massive\ndatasets and has predominantly benefited the world's political and economic\nsuperpowers. Technologically underserved languages are left behind because they\nlack such resources. Hundreds of underserved languages, nevertheless, have\navailable data sources in the form of interlinear glossed text (IGT) from\nlanguage documentation efforts. IGT remains underutilized in NLP work, perhaps\nbecause its annotations are only semi-structured and often language-specific.\nWith this paper, we make the case that IGT data can be leveraged successfully\nprovided that target language expertise is available. We specifically advocate\nfor collaboration with documentary linguists. Our paper provides a roadmap for\nsuccessful projects utilizing IGT data: (1) It is essential to define which NLP\ntasks can be accomplished with the given IGT data and how these will benefit\nthe speech community. (2) Great care and target language expertise is required\nwhen converting the data into structured formats commonly employed in NLP. (3)\nTask-specific and user-specific evaluation can help to ascertain that the tools\nwhich are created benefit the target language speech community. We illustrate\neach step through a case study on developing a morphological reinflection\nsystem for the Tsimchianic language Gitksan.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Forbes_C/0/1/0/all/0/1\">Clarissa Forbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samir_F/0/1/0/all/0/1\">Farhan Samir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_B/0/1/0/all/0/1\">Bruce Harold Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Changbing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coates_E/0/1/0/all/0/1\">Edith Coates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nicolai_G/0/1/0/all/0/1\">Garrett Nicolai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silfverberg_M/0/1/0/all/0/1\">Miikka Silfverberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Reinforcement Agent for Efficient Instant Search. (arXiv:2203.09644v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09644","description":"<p>Instant Search is a paradigm where a search system retrieves answers on the\nfly while typing. The na\\\"ive implementation of an Instant Search system would\nhit the search back-end for results each time a user types a key, imposing a\nvery high load on the underlying search system. In this paper, we propose to\naddress the load issue by identifying tokens that are semantically more salient\ntowards retrieving relevant documents and utilize this knowledge to trigger an\ninstant search selectively. We train a reinforcement agent that interacts\ndirectly with the search engine and learns to predict the word's importance.\nOur proposed method treats the underlying search system as a black box and is\nmore universally applicable to a diverse set of architectures. Furthermore, a\nnovel evaluation framework is presented to study the trade-off between the\nnumber of triggered searches and the system's performance. We utilize the\nframework to evaluate and compare the proposed reinforcement method with other\nintuitive baselines. Experimental results demonstrate the efficacy of the\nproposed method towards achieving a superior trade-off.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_R/0/1/0/all/0/1\">Ravneet Singh Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_S/0/1/0/all/0/1\">Sreejith Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Ayush Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1\">Nehil Jain</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate speech, Censorship, and Freedom of Speech: The Changing Policies of Reddit. (arXiv:2203.09673v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09673","description":"<p>This paper examines the shift in focus on content policies and user attitudes\non the social media platform Reddit. We do this by focusing on comments from\ngeneral Reddit users from five posts made by admins (moderators) on updates to\nReddit Content Policy. All five concern the nature of what kind of content is\nallowed to be posted on Reddit, and which measures will be taken against\ncontent that violates these policies. We use topic modeling to probe how the\ngeneral discourse for Redditors has changed around limitations on content, and\nlater, limitations on hate speech, or speech that incites violence against a\nparticular group. We show that there is a clear shift in both the contents and\nthe user attitudes that can be linked to contemporary societal upheaval as well\nas newly passed laws and regulations, and contribute to the wider discussion on\nhate speech moderation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wickham_E/0/1/0/all/0/1\">Elissa Nakajima Wickham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohman_E/0/1/0/all/0/1\">Emily &#xd6;hman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Intensification for Sign Language Generation: A Computational Approach. (arXiv:2203.09679v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09679","description":"<p>End-to-end sign language generation models do not accurately represent the\nprosody in sign language. A lack of temporal and spatial variations leads to\npoor-quality generated presentations that confuse human interpreters. In this\npaper, we aim to improve the prosody in generated sign languages by modeling\nintensification in a data-driven manner. We present different strategies\ngrounded in linguistics of sign language that inform how intensity modifiers\ncan be represented in gloss annotations. To employ our strategies, we first\nannotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,\nwith different levels of intensification. We then use a supervised intensity\ntagger to extend the annotated dataset and obtain labels for the remaining\nportion of it. This enhanced dataset is then used to train state-of-the-art\ntransformer models for sign language generation. We find that our efforts in\nintensification modeling yield better results when evaluated with automatic\nmetrics. Human evaluation also indicates a higher preference of the videos\ngenerated using our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Sabit Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quandt_L/0/1/0/all/0/1\">Lorna Quandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A$^3$T: Alignment-Aware Acoustic and Text Pretraining for Speech Synthesis and Editing. (arXiv:2203.09690v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09690","description":"<p>Recently, speech representation learning has improved many speech-related\ntasks such as speech recognition, speech classification, and speech-to-text\ntranslation. However, all the above tasks are in the direction of speech\nunderstanding, but for the inverse direction, speech synthesis, the potential\nof representation learning is yet to be realized, due to the challenging nature\nof generating high-quality speech. To address this problem, we propose our\nframework, Alignment-Aware Acoustic-Text Pretraining (A$^3$T), which\nreconstructs masked acoustic signals with text input and acoustic-text\nalignment during training. In this way, the pretrained model can generate high\nquality of reconstructed spectrogram, which can be applied to the speech\nediting and unseen speaker TTS directly. Experiments show A$^3$T outperforms\nSOTA models on speech editing, and improves multi-speaker speech synthesis\nwithout the external speaker verification model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1\">He Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xintong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improve few-shot voice cloning using multi-modal learning. (arXiv:2203.09708v1 [cs.SD])","link":"http://arxiv.org/abs/2203.09708","description":"<p>Recently, few-shot voice cloning has achieved a significant improvement.\nHowever, most models for few-shot voice cloning are single-modal, and\nmulti-modal few-shot voice cloning has been understudied. In this paper, we\npropose to use multi-modal learning to improve the few-shot voice cloning\nperformance. Inspired by the recent works on unsupervised speech\nrepresentation, the proposed multi-modal system is built by extending Tacotron2\nwith an unsupervised speech representation module. We evaluate our proposed\nsystem in two few-shot voice cloning scenarios, namely few-shot\ntext-to-speech(TTS) and voice conversion(VC). Experimental results demonstrate\nthat the proposed multi-modal learning can significantly improve the few-shot\nvoice cloning performance over their counterpart single-modal systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haitong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEAM: Dialogue Coherence Evaluation using AMR-based Semantic Manipulations. (arXiv:2203.09711v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09711","description":"<p>Automatic evaluation metrics are essential for the rapid development of\nopen-domain dialogue systems as they facilitate hyper-parameter tuning and\ncomparison between models. Although recently proposed trainable\nconversation-level metrics have shown encouraging results, the quality of the\nmetrics is strongly dependent on the quality of training data. Prior works\nmainly resort to heuristic text-level manipulations (e.g. utterances shuffling)\nto bootstrap incoherent conversations (negative examples) from coherent\ndialogues (positive examples). Such approaches are insufficient to\nappropriately reflect the incoherence that occurs in interactions between\nadvanced dialogue models and humans. To tackle this problem, we propose DEAM, a\nDialogue coherence Evaluation metric that relies on Abstract Meaning\nRepresentation (AMR) to apply semantic-level Manipulations for incoherent\n(negative) data generation. AMRs naturally facilitate the injection of various\ntypes of incoherence sources, such as coreference inconsistency, irrelevancy,\ncontradictions, and decrease engagement, at the semantic level, thus resulting\nin more natural incoherent samples. Our experiments show that DEAM achieves\nhigher correlations with human judgments compared to baseline methods on\nseveral dialog datasets by significant margins. We also show that DEAM can\ndistinguish between coherent and incoherent dialogues generated by baseline\nmanipulations, whereas those baseline models cannot detect incoherent examples\ngenerated by DEAM. Our results demonstrate the potential of AMR-based semantic\nmanipulations for natural negative example generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghazarian_S/0/1/0/all/0/1\">Sarik Ghazarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_N/0/1/0/all/0/1\">Nuan Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRBoost: Prompt-Based Rule Discovery and Boosting for Interactive Weakly-Supervised Learning. (arXiv:2203.09735v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09735","description":"<p>Weakly-supervised learning (WSL) has shown promising results in addressing\nlabel scarcity on many NLP tasks, but manually designing a comprehensive,\nhigh-quality labeling rule set is tedious and difficult. We study interactive\nweakly-supervised learning -- the problem of iteratively and automatically\ndiscovering novel labeling rules from data to improve the WSL model. Our\nproposed model, named PRBoost, achieves this goal via iterative prompt-based\nrule discovery and model boosting. It uses boosting to identify large-error\ninstances and then discovers candidate rules from them by prompting pre-trained\nLMs with rule templates. The candidate rules are judged by human experts, and\nthe accepted rules are used to generate complementary weak labels and\nstrengthen the current model. Experiments on four tasks show PRBoost\noutperforms state-of-the-art WSL baselines up to 7.1% and bridges the gaps with\nfully supervised models. Our Implementation is available at\n\\url{https://github.com/rz-zhang/PRBoost}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongzhi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shetty_P/0/1/0/all/0/1\">Pranav Shetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GRS: Combining Generation and Revision in Unsupervised Sentence Simplification. (arXiv:2203.09742v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09742","description":"<p>We propose GRS: an unsupervised approach to sentence simplification that\ncombines text generation and text revision. We start with an iterative\nframework in which an input sentence is revised using explicit edit operations,\nand add paraphrasing as a new edit operation. This allows us to combine the\nadvantages of generative and revision-based approaches: paraphrasing captures\ncomplex edit operations, and the use of explicit edit operations in an\niterative manner provides controllability and interpretability. We demonstrate\nthese advantages of GRS compared to existing methods on the Newsela and ASSET\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dehghan_M/0/1/0/all/0/1\">Mohammad Dehghan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Dhruv Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golab_L/0/1/0/all/0/1\">Lukasz Golab</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prototypical Verbalizer for Prompt-based Few-shot Tuning. (arXiv:2203.09770v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09770","description":"<p>Prompt-based tuning for pre-trained language models (PLMs) has shown its\neffectiveness in few-shot learning. Typically, prompt-based tuning wraps the\ninput text into a cloze question. To make predictions, the model maps the\noutput words to labels via a verbalizer, which is either manually designed or\nautomatically built. However, manual verbalizers heavily depend on\ndomain-specific prior knowledge and human efforts, while finding appropriate\nlabel words automatically still remains challenging.In this work, we propose\nthe prototypical verbalizer (ProtoVerb) which is built directly from training\ndata. Specifically, ProtoVerb learns prototype vectors as verbalizers by\ncontrastive learning. In this way, the prototypes summarize training instances\nand are able to enclose rich class-level semantics. We conduct experiments on\nboth topic classification and entity typing tasks, and the results demonstrate\nthat ProtoVerb significantly outperforms current automatic verbalizers,\nespecially when training data is extremely scarce. More surprisingly, ProtoVerb\nconsistently boosts prompt-based tuning even on untuned PLMs, indicating an\nelegant non-tuning way to utilize PLMs. Our codes are avaliable at\nhttps://github.com/thunlp/OpenPrompt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1\">Ganqu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Longtao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are You Robert or RoBERTa? Deceiving Online Authorship Attribution Models Using Neural Text Generators. (arXiv:2203.09813v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09813","description":"<p>Recently, there has been a rise in the development of powerful pre-trained\nnatural language models, including GPT-2, Grover, and XLM. These models have\nshown state-of-the-art capabilities towards a variety of different NLP tasks,\nincluding question answering, content summarisation, and text generation.\nAlongside this, there have been many studies focused on online authorship\nattribution (AA). That is, the use of models to identify the authors of online\ntexts. Given the power of natural language models in generating convincing\ntexts, this paper examines the degree to which these language models can\ngenerate texts capable of deceiving online AA models. Experimenting with both\nblog and Twitter data, we utilise GPT-2 language models to generate texts using\nthe existing posts of online users. We then examine whether these AI-based text\ngenerators are capable of mimicking authorial style to such a degree that they\ncan deceive typical AA models. From this, we find that current AI-based text\ngenerators are able to successfully mimic authorship, showing capabilities\ntowards this on both datasets. Our findings, in turn, highlight the current\ncapacity of powerful natural language models to generate original online posts\ncapable of mimicking authorial style sufficiently to deceive popular AA\nmethods; a key finding given the proposed role of AA in real world applications\nsuch as spam-detection and forensic investigation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_K/0/1/0/all/0/1\">Keenan Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurse_J/0/1/0/all/0/1\">Jason R. C. Nurse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shujun Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaVocoder: Adaptive Vocoder for Custom Voice. (arXiv:2203.09825v1 [cs.SD])","link":"http://arxiv.org/abs/2203.09825","description":"<p>Custom voice is to construct a personal speech synthesis system by adapting\nthe source speech synthesis model to the target model through the target few\nrecordings. The solution to constructing a custom voice is to combine an\nadaptive acoustic model with a robust vocoder. However, training a robust\nvocoder usually requires a multi-speaker dataset, which should include various\nage groups and various timbres, so that the trained vocoder can be used for\nunseen speakers. Collecting such a multi-speaker dataset is difficult, and the\ndataset distribution always has a mismatch with the distribution of the target\nspeaker dataset. This paper proposes an adaptive vocoder for custom voice from\nanother novel perspective to solve the above problems. The adaptive vocoder\nmainly uses a cross-domain consistency loss to solve the overfitting problem\nencountered by the GAN-based neural vocoder in the transfer learning of\nfew-shot scenes. We construct two adaptive vocoders, AdaMelGAN and AdaHiFi-GAN.\nFirst, We pre-train the source vocoder model on AISHELL3 and CSMSC datasets,\nrespectively. Then, fine-tune it on the internal dataset VXI-children with few\nadaptation data. The empirical results show that a high-quality custom voice\nsystem can be built by combining a adaptive acoustic model with a adaptive\nvocoder.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yongbing Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1\">Mingming Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_C/0/1/0/all/0/1\">Cheng Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Minghang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Under the Morphosyntactic Lens: A Multifaceted Evaluation of Gender Bias in Speech Translation. (arXiv:2203.09866v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09866","description":"<p>Gender bias is largely recognized as a problematic phenomenon affecting\nlanguage technologies, with recent studies underscoring that it might surface\ndifferently across languages. However, most of current evaluation practices\nadopt a word-level focus on a narrow set of occupational nouns under synthetic\nconditions. Such protocols overlook key features of grammatical gender\nlanguages, which are characterized by morphosyntactic chains of gender\nagreement, marked on a variety of lexical items and parts-of-speech (POS). To\novercome this limitation, we enrich the natural, gender-sensitive MuST-SHE\ncorpus (Bentivogli et al., 2020) with two new linguistic annotation layers (POS\nand agreement chains), and explore to what extent different lexical categories\nand agreement phenomena are impacted by gender skews. Focusing on speech\ntranslation, we conduct a multifaceted evaluation on three language directions\n(English-French/Italian/Spanish), with models trained on varying amounts of\ndata and different word segmentation techniques. By shedding light on model\nbehaviours, gender bias, and its detection at several levels of granularity,\nour findings emphasize the value of dedicated analyses beyond aggregated\noverall results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Savoldi_B/0/1/0/all/0/1\">Beatrice Savoldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1\">Luisa Bentivogli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SCoT: Sense Clustering over Time: a tool for the analysis of lexical change. (arXiv:2203.09892v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09892","description":"<p>We present Sense Clustering over Time (SCoT), a novel network-based tool for\nanalysing lexical change. SCoT represents the meanings of a word as clusters of\nsimilar words. It visualises their formation, change, and demise. There are two\nmain approaches to the exploration of dynamic networks: the discrete one\ncompares a series of clustered graphs from separate points in time. The\ncontinuous one analyses the changes of one dynamic network over a time-span.\nSCoT offers a new hybrid solution. First, it aggregates time-stamped documents\ninto intervals and calculates one sense graph per discrete interval. Then, it\nmerges the static graphs to a new type of dynamic semantic neighbourhood graph\nover time. The resulting sense clusters offer uniquely detailed insights into\nlexical change over continuous intervals with model transparency and\nprovenance. SCoT has been successfully used in a European study on the changing\nmeaning of `crisis'.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haase_C/0/1/0/all/0/1\">Christian Haase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saba Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yimam_S/0/1/0/all/0/1\">Seid Muhie Yimam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedrich_A/0/1/0/all/0/1\">Alexander Friedrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Multilingual Language Models Capture Differing Moral Norms?. (arXiv:2203.09904v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09904","description":"<p>Massively multilingual sentence representations are trained on large corpora\nof uncurated data, with a very imbalanced proportion of languages included in\nthe training. This may cause the models to grasp cultural values including\nmoral judgments from the high-resource languages and impose them on the\nlow-resource languages. The lack of data in certain languages can also lead to\ndeveloping random and thus potentially harmful beliefs. Both these issues can\nnegatively influence zero-shot cross-lingual model transfer and potentially\nlead to harmful outcomes. Therefore, we aim to (1) detect and quantify these\nissues by comparing different models in different languages, (2) develop\nmethods for improving undesirable properties of the models. Our initial\nexperiments using the multilingual model XLM-R show that indeed multilingual\nLMs capture moral norms, even with potentially higher human-agreement than\nmonolingual ones. However, it is not yet clear to what extent these moral norms\ndiffer between languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hammerl_K/0/1/0/all/0/1\">Katharina H&#xe4;mmerl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1\">Bj&#xf6;rn Deiseroth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1\">Patrick Schramowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Libovicky_J/0/1/0/all/0/1\">Jind&#x159;ich Libovick&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fraser_A/0/1/0/all/0/1\">Alexander Fraser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake News Detection Using Majority Voting Technique. (arXiv:2203.09936v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09936","description":"<p>Due to the evolution of the Web and social network platforms it becomes very\neasy to disseminate the information. Peoples are creating and sharing more\ninformation than ever before, which may be misleading, misinformation or fake\ninformation. Fake news detection is a crucial and challenging task due to the\nunstructured nature of the available information. In the recent years,\nresearchers have provided significant solutions to tackle with the problem of\nfake news detection, but due to its nature there are still many open issues. In\nthis paper, we have proposed majority voting approach to detect fake news\narticles. We have used different textual properties of fake and real news. We\nhave used publicly available fake news dataset, comprising of 20,800 news\narticles among which 10,387 are real and 10,413 are fake news labeled as binary\n0 and 1. For the evaluation of our approach, we have used commonly used machine\nlearning classifiers like, Decision Tree, Logistic Regression, XGBoost, Random\nForest, Extra Trees, AdaBoost, SVM, SGD and Naive Bayes. Using the\naforementioned classifiers, we built a multi-model fake news detection system\nusing Majority Voting technique to achieve the more accurate results. The\nexperimental results show that, our proposed approach achieved accuracy of\n96.38%, precision of 96%, recall of 96% and F1-measure of 96%. The evaluation\nconfirms that, Majority Voting technique achieved more acceptable results as\ncompare to individual learning technique.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Patil_D/0/1/0/all/0/1\">Dharmaraj R. Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Training a Tokenizer for Free with Private Federated Learning. (arXiv:2203.09943v1 [cs.CR])","link":"http://arxiv.org/abs/2203.09943","description":"<p>Federated learning with differential privacy, i.e. private federated learning\n(PFL), makes it possible to train models on private data distributed across\nusers' devices without harming privacy. PFL is efficient for models, such as\nneural networks, that have a fixed number of parameters, and thus a\nfixed-dimensional gradient vector. Such models include neural-net language\nmodels, but not tokenizers, the topic of this work. Training a tokenizer\nrequires frequencies of words from an unlimited vocabulary, and existing\nmethods for finding an unlimited vocabulary need a separate privacy budget.\n</p>\n<p>A workaround is to train the tokenizer on publicly available data. However,\nin this paper we first show that a tokenizer trained on mismatched data results\nin worse model performance compared to a privacy-violating \"oracle\" tokenizer\nthat accesses user data, with perplexity increasing by 20%. We also show that\nsub-word tokenizers are better suited to the federated context than word-level\nones, since they can encode new words, though with more tokens per word.\n</p>\n<p>Second, we propose a novel method to obtain a tokenizer without using any\nadditional privacy budget. During private federated learning of the language\nmodel, we sample from the model, train a new tokenizer on the sampled\nsequences, and update the model embeddings. We then continue private federated\nlearning, and obtain performance within 1% of the \"oracle\" tokenizer. Since\nthis process trains the tokenizer only indirectly on private data, we can use\nthe \"postprocessing guarantee\" of differential privacy and thus use no\nadditional privacy budget.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bagdasaryan_E/0/1/0/all/0/1\">Eugene Bagdasaryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Congzheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalen_R/0/1/0/all/0/1\">Rogier van Dalen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seigel_M/0/1/0/all/0/1\">Matt Seigel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cahill_A/0/1/0/all/0/1\">&#xc1;ine Cahill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prompt-based Generative Approach towards Multi-Hierarchical Medical Dialogue State Tracking. (arXiv:2203.09946v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09946","description":"<p>The medical dialogue system is a promising application that can provide great\nconvenience for patients. The dialogue state tracking (DST) module in the\nmedical dialogue system which interprets utterances into the machine-readable\nstructure for downstream tasks is particularly challenging. Firstly, the states\nneed to be able to represent compound entities such as symptoms with their body\npart or diseases with degrees of severity to provide enough information for\ndecision support. Secondly, these named entities in the utterance might be\ndiscontinuous and scattered across sentences and speakers. These also make it\ndifficult to annotate a large corpus which is essential for most methods.\nTherefore, we first define a multi-hierarchical state structure. We annotate\nand publish a medical dialogue dataset in Chinese. To the best of our\nknowledge, there are no publicly available ones before. Then we propose a\nPrompt-based Generative Approach which can generate slot values with\nmulti-hierarchies incrementally using a top-down approach. A dialogue style\nprompt is also supplemented to utilize the large unlabeled dialogue corpus to\nalleviate the data scarcity problem. The experiments show that our approach\noutperforms other DST methods and is rather effective in the scenario with\nlittle data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruan_T/0/1/0/all/0/1\">Tong Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanhuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Lithuanian grammatical error correction. (arXiv:2203.09963v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09963","description":"<p>Everyone wants to write beautiful and correct text, yet the lack of language\nskills, experience, or hasty typing can result in errors. By employing the\nrecent advances in transformer architectures, we construct a grammatical error\ncorrection model for Lithuanian, the language rich in archaic features. We\ncompare subword and byte-level approaches and share our best trained model,\nachieving F$_{0.5}$=0.92, and accompanying code, in an online open-source\nrepository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BIOS: An Algorithmically Generated Biomedical Knowledge Graph. (arXiv:2203.09975v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09975","description":"<p>Biomedical knowledge graphs (BioMedKGs) are essential infrastructures for\nbiomedical and healthcare big data and artificial intelligence (AI),\nfacilitating natural language processing, model development, and data exchange.\nFor many decades, these knowledge graphs have been built via expert curation,\nwhich can no longer catch up with the speed of today's AI development, and a\ntransition to algorithmically generated BioMedKGs is necessary. In this work,\nwe introduce the Biomedical Informatics Ontology System (BIOS), the first large\nscale publicly available BioMedKG that is fully generated by machine learning\nalgorithms. BIOS currently contains 4.1 million concepts, 7.4 million terms in\ntwo languages, and 7.3 million relation triplets. We introduce the methodology\nfor developing BIOS, which covers curation of raw biomedical terms,\ncomputationally identifying synonymous terms and aggregating them to create\nconcept nodes, semantic type classification of the concepts, relation\nidentification, and biomedical machine translation. We provide statistics about\nthe current content of BIOS and perform preliminary assessment for term\nquality, synonym grouping, and relation extraction. Results suggest that\nmachine learning-based BioMedKG development is a totally viable solution for\nreplacing traditional expert curation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zheng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1\">Shengxuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_H/0/1/0/all/0/1\">Huaiyuan Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Sihang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jingyi Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hongyi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhengyun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yucong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Keming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yutao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1\">Heung-Yeung Shum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CrossAligner & Co: Zero-Shot Transfer Methods for Task-Oriented Cross-lingual Natural Language Understanding. (arXiv:2203.09982v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09982","description":"<p>Task-oriented personal assistants enable people to interact with a host of\ndevices and services using natural language. One of the challenges of making\nneural dialogue systems available to more users is the lack of training data\nfor all but a few languages. Zero-shot methods try to solve this issue by\nacquiring task knowledge in a high-resource language such as English with the\naim of transferring it to the low-resource language(s). To this end, we\nintroduce CrossAligner, the principal method of a variety of effective\napproaches for zero-shot cross-lingual transfer based on learning alignment\nfrom unlabelled parallel data. We present a quantitative analysis of individual\nmethods as well as their weighted combinations, several of which exceed\nstate-of-the-art (SOTA) scores as evaluated across nine languages, fifteen test\nsets and three benchmark multilingual datasets. A detailed qualitative error\nanalysis of the best methods shows that our fine-tuned language models can\nzero-shot transfer the task knowledge better than anticipated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gritta_M/0/1/0/all/0/1\">Milan Gritta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1\">Ruoyu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iacobacci_I/0/1/0/all/0/1\">Ignacio Iacobacci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Text Multi-Modal Pre-training for Medical Representation Learning. (arXiv:2203.09994v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09994","description":"<p>As the volume of Electronic Health Records (EHR) sharply grows, there has\nbeen emerging interest in learning the representation of EHR for healthcare\napplications. Representation learning of EHR requires appropriate modeling of\nthe two dominant modalities in EHR: structured data and unstructured text. In\nthis paper, we present MedGTX, a pre-trained model for multi-modal\nrepresentation learning of the structured and textual EHR data. MedGTX uses a\nnovel graph encoder to exploit the graphical nature of structured EHR data, and\na text encoder to handle unstructured text, and a cross-modal encoder to learn\na joint representation space. We pre-train our model through four proxy tasks\non MIMIC-III, an open-source EHR data, and evaluate our model on two clinical\nbenchmarks and three novel downstream tasks which tackle real-world problems in\nEHR data. The results consistently show the effectiveness of pre-training the\nmodel for joint representation of both structured and unstructured information\nfrom EHR. Given the promising performance of MedGTX, we believe this work opens\na new door to jointly understanding the two fundamental modalities of EHR data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjin Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Seongsu Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tackeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FORCE: A Framework of Rule-Based Conversational Recommender System. (arXiv:2203.10001v1 [cs.IR])","link":"http://arxiv.org/abs/2203.10001","description":"<p>The conversational recommender systems (CRSs) have received extensive\nattention in recent years. However, most of the existing works focus on various\ndeep learning models, which are largely limited by the requirement of\nlarge-scale human-annotated datasets. Such methods are not able to deal with\nthe cold-start scenarios in industrial products. To alleviate the problem, we\npropose FORCE, a Framework Of Rule-based Conversational Recommender system that\nhelps developers to quickly build CRS bots by simple configuration. We conduct\nexperiments on two datasets in different languages and domains to verify its\neffectiveness and usability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quan_J/0/1/0/all/0/1\">Jun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ze Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Qiang Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jingqi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jingyi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuchen Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiming Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongzhi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yingying He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CaMEL: Case Marker Extraction without Labels. (arXiv:2203.10010v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10010","description":"<p>We introduce CaMEL (Case Marker Extraction without Labels), a novel and\nchallenging task in computational morphology that is especially relevant for\nlow-resource languages. We propose a first model for CaMEL that uses a\nmassively multilingual corpus to extract case markers in 83 languages based\nonly on a noun phrase chunker and an alignment system. To evaluate CaMEL, we\nautomatically construct a silver standard from UniMorph. The case markers\nextracted by our model can be used to detect and visualise similarities and\ndifferences between the case systems of different languages as well as to\nannotate fine-grained deep cases in languages in which they are not overtly\nmarked.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weissweiler_L/0/1/0/all/0/1\">Leonie Weissweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1\">Valentin Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Report from the NSF Future Directions Workshop on Automatic Evaluation of Dialog: Research Directions and Challenges. (arXiv:2203.10012v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10012","description":"<p>This is a report on the NSF Future Directions Workshop on Automatic\nEvaluation of Dialog. The workshop explored the current state of the art along\nwith its limitations and suggested promising directions for future work in this\nimportant and very rapidly changing area of research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1\">Jinho Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deriu_J/0/1/0/all/0/1\">Jan Deriu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Gasic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgila_K/0/1/0/all/0/1\">Kallirroi Georgila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rieser_V/0/1/0/all/0/1\">Verena Rieser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1\">Samira Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Traum_D/0/1/0/all/0/1\">David Traum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yizhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges and Strategies in Cross-Cultural NLP. (arXiv:2203.10020v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10020","description":"<p>Various efforts in the Natural Language Processing (NLP) community have been\nmade to accommodate linguistic diversity and serve speakers of many different\nlanguages. However, it is important to acknowledge that speakers and the\ncontent they produce and require, vary not just by language, but also by\nculture. Although language and culture are tightly linked, there are important\ndifferences. Analogous to cross-lingual and multilingual NLP, cross-cultural\nand multicultural NLP considers these differences in order to better serve\nusers of NLP systems. We propose a principled framework to frame these efforts,\nand survey existing and potential strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lent_H/0/1/0/all/0/1\">Heather Lent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lhoneux_M/0/1/0/all/0/1\">Miryam de Lhoneux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandl_S/0/1/0/all/0/1\">Stephanie Brandl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piqueras_L/0/1/0/all/0/1\">Laura Cabello Piqueras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1\">Ruixiang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fierro_C/0/1/0/all/0/1\">Constanza Fierro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rust_P/0/1/0/all/0/1\">Phillip Rust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offensive Language Detection in Under-resourced Algerian Dialectal Arabic Language. (arXiv:2203.10024v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10024","description":"<p>This paper addresses the problem of detecting the offensive and abusive\ncontent in Facebook comments, where we focus on the Algerian dialectal Arabic\nwhich is one of under-resourced languages. The latter has a variety of dialects\nmixed with different languages (i.e. Berber, French and English). In addition,\nwe deal with texts written in both Arabic and Roman scripts (i.e. Arabizi). Due\nto the scarcity of works on the same language, we have built a new corpus\nregrouping more than 8.7k texts manually annotated as normal, abusive and\noffensive. We have conducted a series of experiments using the state-of-the-art\nclassifiers of text categorisation, namely: BiLSTM, CNN, FastText, SVM and NB.\nThe results showed acceptable performances, but the problem requires further\ninvestigation on linguistic features to increase the identification accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Boucherit_O/0/1/0/all/0/1\">Oussama Boucherit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abainia_K/0/1/0/all/0/1\">Kheireddine Abainia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RELIC: Retrieving Evidence for Literary Claims. (arXiv:2203.10053v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10053","description":"<p>Humanities scholars commonly provide evidence for claims that they make about\na work of literature (e.g., a novel) in the form of quotations from the work.\nWe collect a large-scale dataset (RELiC) of 78K literary quotations and\nsurrounding critical analysis and use it to formulate the novel task of\nliterary evidence retrieval, in which models are given an excerpt of literary\nanalysis surrounding a masked quotation and asked to retrieve the quoted\npassage from the set of all passages in the work. Solving this retrieval task\nrequires a deep understanding of complex literary and linguistic phenomena,\nwhich proves challenging to methods that overwhelmingly rely on lexical and\nsemantic similarity matching. We implement a RoBERTa-based dense passage\nretriever for this task that outperforms existing pretrained information\nretrieval baselines; however, experiments and analysis by human domain experts\nindicate that there is substantial room for improvement over our dense\nretriever.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thai_K/0/1/0/all/0/1\">Katherine Thai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yapei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kalpesh Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simulating Bandit Learning from User Feedback for Extractive Question Answering. (arXiv:2203.10079v1 [cs.CL])","link":"http://arxiv.org/abs/2203.10079","description":"<p>We study learning from user feedback for extractive question answering by\nsimulating feedback using supervised data. We cast the problem as contextual\nbandit learning, and analyze the characteristics of several learning scenarios\nwith focus on reducing data annotation. We show that systems initially trained\non a small number of examples can dramatically improve given feedback from\nusers on model-predicted answers, and that one can use existing datasets to\ndeploy systems in new domains without any annotation, but instead improving the\nsystem on-the-fly via user feedback.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1\">Ge Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04631","description":"<p>Machine translation between many languages at once is highly challenging,\nsince training with ground truth requires supervision between all language\npairs, which is difficult to obtain. Our key insight is that, while languages\nmay vary drastically, the underlying visual appearance of the world remains\nconsistent. We introduce a method that uses visual observations to bridge the\ngap between languages, rather than relying on parallel corpora or topological\nproperties of the representations. We train a model that aligns segments of\ntext from different languages if and only if the images associated with them\nare similar and each image in turn is well-aligned with its textual\ndescription. We train our model from scratch on a new dataset of text in over\nfifty languages with accompanying images. Experiments show that our method\noutperforms previous work on unsupervised word and sentence translation using\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">D&#xed;dac Sur&#xed;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shellcode_IA32: A Dataset for Automatic Shellcode Generation. (arXiv:2104.13100v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2104.13100","description":"<p>We take the first step to address the task of automatically generating\nshellcodes, i.e., small pieces of code used as a payload in the exploitation of\na software vulnerability, starting from natural language comments. We assemble\nand release a novel dataset (Shellcode_IA32), consisting of challenging but\ncommon assembly instructions with their natural language descriptions. We\nexperiment with standard methods in neural machine translation (NMT) to\nestablish baseline performance levels on this task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1\">Pietro Liguori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Hossami_E/0/1/0/all/0/1\">Erfan Al-Hossami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1\">Domenico Cotroneo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1\">Roberto Natella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cukic_B/0/1/0/all/0/1\">Bojan Cukic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1\">Samira Shaikh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing Multilingual Fairness in Pre-trained Multimodal Representations. (arXiv:2106.06683v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06683","description":"<p>Recently pre-trained multimodal models, such as CLIP, have shown exceptional\ncapabilities towards connecting images and natural language. The textual\nrepresentations in English can be desirably transferred to multilingualism and\nsupport downstream multimodal tasks for different languages. Nevertheless, the\nprinciple of multilingual fairness is rarely scrutinized: do multilingual\nmultimodal models treat languages equally? Are their performances biased\ntowards particular languages? To answer these questions, we view language as\nthe fairness recipient and introduce two new fairness notions, multilingual\nindividual fairness and multilingual group fairness, for pre-trained multimodal\nmodels. Multilingual individual fairness requires that text snippets expressing\nsimilar semantics in different languages connect similarly to images, while\nmultilingual group fairness requires equalized predictive performance across\nlanguages. We characterize the extent to which pre-trained multilingual\nvision-and-language representations are individually fair across languages.\nHowever, extensive experiments demonstrate that multilingual representations do\nnot satisfy group fairness: (1) there is a severe multilingual accuracy\ndisparity issue; (2) the errors exhibit biases across languages conditioning\nthe group of people in the images, including race, gender and age.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tailor: Generating and Perturbing Text with Semantic Controls. (arXiv:2107.07150v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.07150","description":"<p>Controlled text perturbation is useful for evaluating and improving model\ngeneralizability. However, current techniques rely on training a model for\nevery target perturbation, which is expensive and hard to generalize. We\npresent Tailor, a semantically-controlled text generation system. Tailor builds\non a pretrained seq2seq model and produces textual outputs conditioned on\ncontrol codes derived from semantic representations. We craft a set of\noperations to modify the control codes, which in turn steer generation towards\ntargeted attributes. These operations can be further composed into higher-level\nones, allowing for flexible perturbation strategies. We demonstrate the\neffectiveness of these perturbations in multiple applications. First, we use\nTailor to automatically create high-quality contrast sets for four distinct\nnatural language processing (NLP) tasks. These contrast sets contain fewer\nspurious artifacts and are complementary to manually annotated ones in their\nlexical diversity. Second, we show that Tailor perturbations can improve model\ngeneralization through data augmentation. Perturbing just 2% of training data\nleads to a 5.8-point gain on an NLI challenge set measuring reliance on\nsyntactic heuristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1\">Alexis Ross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1\">Matthew E. Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1\">Matt Gardner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification. (arXiv:2108.02035v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02035","description":"<p>Tuning pre-trained language models (PLMs) with task-specific prompts has been\na promising approach for text classification. Particularly, previous studies\nsuggest that prompt-tuning has remarkable superiority in the low-data scenario\nover the generic fine-tuning methods with extra classifiers. The core idea of\nprompt-tuning is to insert text pieces, i.e., template, to the input and\ntransform a classification problem into a masked language modeling problem,\nwhere a crucial step is to construct a projection, i.e., verbalizer, between a\nlabel space and a label word space. A verbalizer is usually handcrafted or\nsearched by gradient descent, which may lack coverage and bring considerable\nbias and high variances to the results. In this work, we focus on incorporating\nexternal knowledge into the verbalizer, forming a knowledgeable prompt-tuning\n(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the\nlabel word space of the verbalizer using external knowledge bases (KBs) and\nrefine the expanded label word space with the PLM itself before predicting with\nthe expanded label word space. Extensive experiments on zero and few-shot text\nclassification tasks demonstrate the effectiveness of knowledgeable\nprompt-tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shengding Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MELM: Data Augmentation with Masked Entity Language Modeling for Low-Resource NER. (arXiv:2108.13655v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13655","description":"<p>Data augmentation is an effective solution to data scarcity in low-resource\nscenarios. However, when applied to token-level tasks such as NER, data\naugmentation methods often suffer from token-label misalignment, which leads to\nunsatsifactory performance. In this work, we propose Masked Entity Language\nModeling (MELM) as a novel data augmentation framework for low-resource NER. To\nalleviate the token-label misalignment issue, we explicitly inject NER labels\ninto sentence context, and thus the fine-tuned MELM is able to predict masked\nentity tokens by explicitly conditioning on their labels. Thereby, MELM\ngenerates high-quality augmented data with novel entities, which provides rich\nentity regularity knowledge and boosts NER performance. When training data from\nmultiple languages are available, we also integrate MELM with code-mixing for\nfurther improvement. We demonstrate the effectiveness of MELM on monolingual,\ncross-lingual and multilingual NER across various low-resource levels.\nExperimental results show that our MELM presents substantial improvement over\nthe baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers in the loop: Polarity in neural models of language. (arXiv:2109.03926v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03926","description":"<p>Representation of linguistic phenomena in computational language models is\ntypically assessed against the predictions of existing linguistic theories of\nthese phenomena. Using the notion of polarity as a case study, we show that\nthis is not always the most adequate set-up. We probe polarity via so-called\n'negative polarity items' (in particular, English 'any') in two pre-trained\nTransformer-based models (BERT and GPT-2). We show that - at least for polarity\n- metrics derived from language models are more consistent with data from\npsycholinguistic experiments than linguistic theory predictions. Establishing\nthis allows us to more adequately evaluate the performance of language models\nand also to use language models to discover new insights into natural language\ngrammar beyond existing linguistic theories. This work contributes to\nestablishing closer ties between psycholinguistic experiments and experiments\nwith language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bylinina_L/0/1/0/all/0/1\">Lisa Bylinina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles. (arXiv:2109.11087v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11087","description":"<p>A riddle is a question or statement with double or veiled meanings, followed\nby an unexpected answer. Solving riddle is a challenging task for both machine\nand human, testing the capability of understanding figurative, creative natural\nlanguage and reasoning with commonsense knowledge. We introduce BiRdQA, a\nbilingual multiple-choice question answering dataset with 6614 English riddles\nand 8751 Chinese riddles. For each riddle-answer pair, we provide four\ndistractors with additional information from Wikipedia. The distractors are\nautomatically generated at scale with minimal bias. Existing monolingual and\nmultilingual QA models fail to perform well on our dataset, indicating that\nthere is a long way to go before machine can beat human on solving tricky\nriddles. The dataset has been released to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts. (arXiv:2110.01691v3 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2110.01691","description":"<p>Although large language models (LLMs) have demonstrated impressive potential\non simple tasks, their breadth of scope, lack of transparency, and insufficient\ncontrollability can make them less effective when assisting humans on more\ncomplex tasks. In response, we introduce the concept of Chaining LLM steps\ntogether, where the output of one step becomes the input for the next, thus\naggregating the gains per step. We first define a set of LLM primitive\noperations useful for Chain construction, then present an interactive system\nwhere users can modify these Chains, along with their intermediate results, in\na modular way. In a 20-person user study, we found that Chaining not only\nimproved the quality of task outcomes, but also significantly enhanced system\ntransparency, controllability, and sense of collaboration. Additionally, we saw\nthat users developed new ways of interacting with LLMs through Chains: they\nleveraged sub-tasks to calibrate model expectations, compared and contrasted\nalternative strategies by observing parallel downstream effects, and debugged\nunexpected model outputs by \"unit-testing\" sub-components of a Chain. In two\ncase studies, we further explore how LLM Chains may be used in future\napplications\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Terry_M/0/1/0/all/0/1\">Michael Terry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Carrie J. Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PAMA-TTS: Progression-Aware Monotonic Attention for Stable Seq2Seq TTS With Accurate Phoneme Duration Control. (arXiv:2110.04486v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2110.04486","description":"<p>Sequence expansion between encoder and decoder is a critical challenge in\nsequence-to-sequence TTS. Attention-based methods achieve great naturalness but\nsuffer from unstable issues like missing and repeating phonemes, not to mention\naccurate duration control. Duration-informed methods, on the contrary, seem to\neasily adjust phoneme duration but show obvious degradation in speech\nnaturalness. This paper proposes PAMA-TTS to address the problem. It takes the\nadvantage of both flexible attention and explicit duration models. Based on the\nmonotonic attention mechanism, PAMA-TTS also leverages token duration and\nrelative position of a frame, especially countdown information, i.e. in how\nmany future frames the present phoneme will end. They help the attention to\nmove forward along the token sequence in a soft but reliable control.\nExperimental results prove that PAMA-TTS achieves the highest naturalness,\nwhile has on-par or even better duration controllability than the\nduration-informed model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yunchao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luan_J/0/1/0/all/0/1\">Jian Luan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujun Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpreting the Robustness of Neural NLP Models to Textual Perturbations. (arXiv:2110.07159v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.07159","description":"<p>Modern Natural Language Processing (NLP) models are known to be sensitive to\ninput perturbations and their performance can decrease when applied to\nreal-world, noisy data. However, it is still unclear why models are less robust\nto some perturbations than others. In this work, we test the hypothesis that\nthe extent to which a model is affected by an unseen textual perturbation\n(robustness) can be explained by the learnability of the perturbation (defined\nas how well the model learns to identify the perturbation with a small amount\nof evidence). We further give a causal justification for the learnability\nmetric. We conduct extensive experiments with four prominent NLP models --\nTextRNN, BERT, RoBERTa and XLNet -- over eight types of textual perturbations\non three datasets. We show that a model which is better at identifying a\nperturbation (higher learnability) becomes worse at ignoring such a\nperturbation at test time (lower robustness), providing empirical support for\nour hypothesis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liangming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Samson Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1\">Min-Yen Kan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fusing ASR Outputs in Joint Training for Speech Emotion Recognition. (arXiv:2110.15684v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2110.15684","description":"<p>Alongside acoustic information, linguistic features based on speech\ntranscripts have been proven useful in Speech Emotion Recognition (SER).\nHowever, due to the scarcity of emotion labelled data and the difficulty of\nrecognizing emotional speech, it is hard to obtain reliable linguistic features\nand models in this research area. In this paper, we propose to fuse Automatic\nSpeech Recognition (ASR) outputs into the pipeline for joint training SER. The\nrelationship between ASR and SER is understudied, and it is unclear what and\nhow ASR features benefit SER. By examining various ASR outputs and fusion\nmethods, our experiments show that in joint ASR-SER training, incorporating\nboth ASR hidden and text output using a hierarchical co-attention fusion\napproach improves the SER performance the most. On the IEMOCAP corpus, our\napproach achieves 63.4% weighted accuracy, which is close to the baseline\nresults achieved by combining ground-truth transcripts. In addition, we also\npresent novel word error rate analysis on IEMOCAP and layer-difference analysis\nof the Wav2vec 2.0 model to better understand the relationship between ASR and\nSER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuanchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bell_P/0/1/0/all/0/1\">Peter Bell</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lai_C/0/1/0/all/0/1\">Catherine Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02387","description":"<p>Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Correcting diacritics and typos with a ByT5 transformer model. (arXiv:2201.13242v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.13242","description":"<p>Due to the fast pace of life and online communications and the prevalence of\nEnglish and the QWERTY keyboard, people tend to forgo using diacritics, make\ntypographical errors (typos) when typing in other languages. Restoring\ndiacritics and correcting spelling is important for proper language use and the\ndisambiguation of texts for both humans and downstream algorithms. However,\nboth of these problems are typically addressed separately: the state-of-the-art\ndiacritics restoration methods do not tolerate other typos, but classical\nspellcheckers also cannot deal adequately with all the diacritics missing. In\nthis work, we tackle both problems at once by employing the newly-developed\nuniversal ByT5 byte-level seq2seq transformer model that requires no\nlanguage-specific model structures. For a comparison, we perform diacritics\nrestoration on benchmark datasets of 12 languages, with the addition of\nLithuanian. The experimental investigation proves that our approach is able to\nachieve results (&gt; 98%) comparable to the previous state-of-the-art, despite\nbeing trained less and on fewer data. Our approach is also able to restore\ndiacritics in words not seen during training with &gt; 76% accuracy. Our\nsimultaneous diacritics restoration and typos correction approach reaches &gt; 94%\nalpha-word accuracy on the 13 languages. It has no direct competitors and\nstrongly outperforms classical spell-checking or dictionary-based approaches.\nWe also demonstrate all the accuracies to further improve with more training.\nTaken together, this shows the great real-world application potential of our\nsuggested methods to more data, languages, and error classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stankevicius_L/0/1/0/all/0/1\">Lukas Stankevi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukosevicius_M/0/1/0/all/0/1\">Mantas Luko&#x161;evi&#x10d;ius</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapociute_Dzikiene_J/0/1/0/all/0/1\">Jurgita Kapo&#x10d;i&#x16b;t&#x117;-Dzikien&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briediene_M/0/1/0/all/0/1\">Monika Briedien&#x117;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krilavicius_T/0/1/0/all/0/1\">Tomas Krilavi&#x10d;ius</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Streaming Multi-Talker ASR with Token-Level Serialized Output Training. (arXiv:2202.00842v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2202.00842","description":"<p>This paper proposes a token-level serialized output training (t-SOT), a novel\nframework for streaming multi-talker automatic speech recognition (ASR). Unlike\nexisting streaming multi-talker ASR models using multiple output layers, the\nt-SOT model has only a single output layer that generates recognition tokens\n(e.g., words, subwords) of multiple speakers in chronological order based on\ntheir emission times. A special token that indicates the change of \"virtual\"\noutput channels is introduced to keep track of the overlapping utterances.\nCompared to the prior streaming multi-talker ASR models, the t-SOT model has\nthe advantages of less inference cost and a simpler model architecture.\nMoreover, in our experiments with LibriSpeechMix and LibriCSS datasets, the\nt-SOT-based transformer transducer model achieves the state-of-the-art word\nerror rates by a significant margin to the prior results. For non-overlapping\nspeech, the t-SOT model is on par with a single-talker ASR model in terms of\nboth accuracy and computational cost, opening the door for deploying one model\nfor both single- and multi-talker scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1\">Naoyuki Kanda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_X/0/1/0/all/0/1\">Xiong Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1\">Zhong Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1\">Yashesh Gaur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhuo Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinyu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshioka_T/0/1/0/all/0/1\">Takuya Yoshioka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing BERT's priors with serial reproduction chains. (arXiv:2202.12226v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.12226","description":"<p>Sampling is a promising bottom-up method for exposing what generative models\nhave learned about language, but it remains unclear how to generate\nrepresentative samples from popular masked language models (MLMs) like BERT.\nThe MLM objective yields a dependency network with no guarantee of consistent\nconditional distributions, posing a problem for naive approaches. Drawing from\ntheories of iterated learning in cognitive science, we explore the use of\nserial reproduction chains to sample from BERT's priors. In particular, we\nobserve that a unique and consistent estimator of the ground-truth joint\ndistribution is given by a Generative Stochastic Network (GSN) sampler, which\nrandomly selects which token to mask and reconstruct on each step. We show that\nthe lexical and syntactic statistics of sentences from GSN chains closely match\nthe ground-truth corpus distribution and perform better than other methods in a\nlarge corpus of naturalness judgments. Our findings establish a firmer\ntheoretical foundation for bottom-up probing and highlight richer deviations\nfrom human priors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yamakoshi_T/0/1/0/all/0/1\">Takateru Yamakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming Catastrophic Forgetting beyond Continual Learning: Balanced Training for Neural Machine Translation. (arXiv:2203.03910v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03910","description":"<p>Neural networks tend to gradually forget the previously learned knowledge\nwhen learning multiple tasks sequentially from dynamic data distributions. This\nproblem is called \\textit{catastrophic forgetting}, which is a fundamental\nchallenge in the continual learning of neural networks. In this work, we\nobserve that catastrophic forgetting not only occurs in continual learning but\nalso affects the traditional static training. Neural networks, especially\nneural machine translation models, suffer from catastrophic forgetting even if\nthey learn from a static training set. To be specific, the final model pays\nimbalanced attention to training samples, where recently exposed samples\nattract more attention than earlier samples. The underlying cause is that\ntraining samples do not get balanced training in each model update, so we name\nthis problem \\textit{imbalanced training}. To alleviate this problem, we\npropose Complementary Online Knowledge Distillation (COKD), which uses\ndynamically updated teacher models trained on specific data orders to\niteratively provide complementary knowledge to the student model. Experimental\nresults on multiple machine translation tasks show that our method successfully\nalleviates the problem of imbalanced training and achieves substantial\nimprovements over strong baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1\">Chenze Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Afrocentric NLP for African Languages: Where We Are and Where We Can Go. (arXiv:2203.08351v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.08351","description":"<p>Aligning with ACL 2022 special Theme on \"Language Diversity: from Low\nResource to Endangered Languages\", we discuss the major linguistic and\nsociopolitical challenges facing development of NLP technologies for African\nlanguages. Situating African languages in a typological framework, we discuss\nhow the particulars of these languages can be harnessed. To facilitate future\nresearch, we also highlight current efforts, communities, venues, datasets, and\ntools. Our main objective is to motivate and advocate for an Afrocentric\napproach to technology development. With this in mind, we recommend\n\\textit{what} technologies to build and \\textit{how} to build, evaluate, and\ndeploy them based on the needs of local African communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adebara_I/0/1/0/all/0/1\">Ife Adebara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1\">Muhammad Abdul-Mageed</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09161","description":"<p>Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state of the art task\nspecific models. Conventional approaches to improve model performance via\ncreating large datasets with lots of task instances or architectural/training\nchanges in model may not be feasible for non-expert users. However, they can\nwrite alternate instructions to represent an instruction task. Is\nInstruction-augumentation helpful? We augment a subset of tasks in the expanded\nversion of NATURAL INSTRUCTIONS with additional instructions and find that\nthese significantly improve model performance (up to 35%), especially in the\nlow-data regime. Our results indicate that an additional instruction can be\nequivalent to ~200 data samples on average across tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Human Gait Analysis using Gait Energy Image. (arXiv:2203.09549v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09549","description":"<p>Gait recognition is one of the most recent emerging techniques of human\nbiometric which can be used for security based purposes having unobtrusive\nlearning method. In comparison with other bio-metrics gait analysis has some\nspecial security features. Most of the biometric technique uses sequential\ntemplate based component analysis for recognition. Comparing with those\nmethods, we proposed a developed technique for gait identification using the\nfeature Gait Energy Image (GEI). GEI representation of gait contains all\ninformation of each image in one gait cycle and requires less storage and low\nprocessing speed. As only one image is enough to store the necessary\ninformation in GEI feature recognition process is very easier than any other\nfeature for gait recognition. Gait recognition has some limitations in\nrecognition process like viewing angle variation, walking speed, clothes,\ncarrying load etc. Our proposed method in the paper compares the recognition\nperformance with template based feature extraction which needs to process each\nframe in the cycle. We use GEI which gives relatively all information about all\nthe frames in the cycle and results in better performance than other feature of\ngait analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bakchy_S/0/1/0/all/0/1\">Sagor Chandro Bakchy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md. Rabiul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmud_M/0/1/0/all/0/1\">M. Rasel Mahmud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imran_F/0/1/0/all/0/1\">Faisal Imran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-similarity based Hyperrelation Network for few-shot segmentation. (arXiv:2203.09550v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09550","description":"<p>Few-shot semantic segmentation aims at recognizing the object regions of\nunseen categories with only a few annotated examples as supervision. The key to\nfew-shot segmentation is to establish a robust semantic relationship between\nthe support and query images and to prevent overfitting. In this paper, we\npropose an effective Multi-similarity Hyperrelation Network (MSHNet) to tackle\nthe few-shot semantic segmentation problem. In MSHNet, we propose a new\nGenerative Prototype Similarity (GPS), which together with cosine similarity\ncan establish a strong semantic relation between the support and query images.\nThe locally generated prototype similarity based on global feature is logically\ncomplementary to the global cosine similarity based on local feature, and the\nrelationship between the query image and the supported image can be expressed\nmore comprehensively by using the two similarities simultaneously. In addition,\nwe propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge\nmulti-layer, multi-shot and multi-similarity hyperrelational features. MSHNet\nis built on the basis of similarity rather than specific category features,\nwhich can achieve more general unity and effectively reduce overfitting. On two\nbenchmark semantic segmentation datasets Pascal-5i and COCO-20i, MSHNet\nachieves new state-of-the-art performances on 1-shot and 5-shot semantic\nsegmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xiangwen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaobing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1\">Miao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zhe Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xianghong Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoGS: Controllable Generation and Search from Sketch and Style. (arXiv:2203.09554v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09554","description":"<p>We present CoGS, a novel method for the style-conditioned, sketch-driven\nsynthesis of images. CoGS enables exploration of diverse appearance\npossibilities for a given sketched object, enabling decoupled control over the\nstructure and the appearance of the output. Coarse-grained control over object\nstructure and appearance are enabled via an input sketch and an exemplar\n\"style\" conditioning image to a transformer-based sketch and style encoder to\ngenerate a discrete codebook representation. We map the codebook representation\ninto a metric space, enabling fine-grained control over selection and\ninterpolation between multiple synthesis options for a given image before\ngenerating the image via a vector quantized GAN (VQGAN) decoder. Our framework\nthereby unifies search and synthesis tasks, in that a sketch and style pair may\nbe used to run an initial synthesis which may be refined via combination with\nsimilar results in a search corpus to produce an image more closely matching\nthe user's intent. We show that our model, trained on the 125 object classes of\nour newly created Pseudosketches dataset, is capable of producing a diverse\ngamut of semantic content and appearance styles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ham_C/0/1/0/all/0/1\">Cusuh Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tarres_G/0/1/0/all/0/1\">Gemma Canet Tarres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tu Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1\">James Hays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1\">John Collomosse</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Defect Detection and Evaluation for Marine Vessels using Multi-Stage Deep Learning. (arXiv:2203.09580v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09580","description":"<p>Detecting and evaluating surface coating defects is important for marine\nvessel maintenance. Currently, the assessment is carried out manually by\nqualified inspectors using international standards and their own experience.\nAutomating the processes is highly challenging because of the high level of\nvariation in vessel type, paint surface, coatings, lighting condition, weather\ncondition, paint colors, areas of the vessel, and time in service. We present a\nnovel deep learning-based pipeline to detect and evaluate the percentage of\ncorrosion, fouling, and delamination on the vessel surface from normal\nphotographs. We propose a multi-stage image processing framework, including\nship section segmentation, defect segmentation, and defect classification, to\nautomatically recognize different types of defects and measure the coverage\npercentage on the ship surface. Experimental results demonstrate that our\nproposed pipeline can objectively perform a similar assessment as a qualified\ninspector.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Li Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metwaly_K/0/1/0/all/0/1\">Kareem Metwaly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">James Z. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monga_V/0/1/0/all/0/1\">Vishal Monga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SepTr: Separable Transformer for Audio Spectrogram Processing. (arXiv:2203.09581v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09581","description":"<p>Following the successful application of vision transformers in multiple\ncomputer vision tasks, these models have drawn the attention of the signal\nprocessing community. This is because signals are often represented as\nspectrograms (e.g. through Discrete Fourier Transform) which can be directly\nprovided as input to vision transformers. However, naively applying\ntransformers to spectrograms is suboptimal. Since the axes represent distinct\ndimensions, i.e. frequency and time, we argue that a better approach is to\nseparate the attention dedicated to each axis. To this end, we propose the\nSeparable Transformer (SepTr), an architecture that employs two transformer\nblocks in a sequential manner, the first attending to tokens within the same\nfrequency bin, and the second attending to tokens within the same time\ninterval. We conduct experiments on three benchmark data sets, showing that our\nseparable architecture outperforms conventional vision transformers and other\nstate-of-the-art methods. Unlike standard transformers, SepTr linearly scales\nthe number of trainable parameters with the input size, thus having a lower\nmemory footprint. Our code is available as open source at\nhttps://github.com/ristea/septr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ristea_N/0/1/0/all/0/1\">Nicolae-Catalin Ristea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video-based Formative and Summative Assessment of Surgical Tasks using Deep Learning. (arXiv:2203.09589v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09589","description":"<p>To ensure satisfactory clinical outcomes, surgical skill assessment must be\nobjective, time-efficient, and preferentially automated - none of which is\ncurrently achievable. Video-based assessment (VBA) is being deployed in\nintraoperative and simulation settings to evaluate technical skill execution.\nHowever, VBA remains manually- and time-intensive and prone to subjective\ninterpretation and poor inter-rater reliability. Herein, we propose a deep\nlearning (DL) model that can automatically and objectively provide a\nhigh-stakes summative assessment of surgical skill execution based on video\nfeeds and low-stakes formative assessment to guide surgical skill acquisition.\nFormative assessment is generated using heatmaps of visual features that\ncorrelate with surgical performance. Hence, the DL model paves the way to the\nquantitative and reproducible evaluation of surgical tasks from videos with the\npotential for broad dissemination in surgical training, certification, and\ncredentialing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yanik_E/0/1/0/all/0/1\">Erim Yanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_U/0/1/0/all/0/1\">Uwe Kruger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Intes_X/0/1/0/all/0/1\">Xavier Intes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahul_R/0/1/0/all/0/1\">Rahul Rahul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1\">Suvranu De</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delta Distillation for Efficient Video Processing. (arXiv:2203.09594v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09594","description":"<p>This paper aims to accelerate video stream processing, such as object\ndetection and semantic segmentation, by leveraging the temporal redundancies\nthat exist between video frames. Instead of propagating and warping features\nusing motion alignment, such as optical flow, we propose a novel knowledge\ndistillation schema coined as Delta Distillation. In our proposal, the student\nlearns the variations in the teacher's intermediate features over time. We\ndemonstrate that these temporal variations can be effectively distilled due to\nthe temporal redundancies within video frames. During inference, both teacher\nand student cooperate for providing predictions: the former by providing\ninitial representations extracted only on the key-frame, and the latter by\niteratively estimating and applying deltas for the successive frames. Moreover,\nwe consider various design choices to learn optimal student architectures\nincluding an end-to-end learnable architecture search. By extensive experiments\non a wide range of architectures, including the most efficient ones, we\ndemonstrate that delta distillation sets a new state of the art in terms of\naccuracy vs. efficiency trade-off for semantic segmentation and object\ndetection in videos. Finally, we show that, as a by-product, delta distillation\nimproves the temporal consistency of the teacher model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Habibian_A/0/1/0/all/0/1\">Amirhossein Habibian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yahia_H/0/1/0/all/0/1\">Haitam Ben Yahia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abati_D/0/1/0/all/0/1\">Davide Abati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1\">Efstratios Gavves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Line and Paragraph Detection by Graph Convolutional Networks. (arXiv:2203.09638v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09638","description":"<p>We formulate the task of detecting lines and paragraphs in a document into a\nunified two-level clustering problem. Given a set of text detection boxes that\nroughly correspond to words, a text line is a cluster of boxes and a paragraph\nis a cluster of lines. These clusters form a two-level tree that represents a\nmajor part of the layout of a document. We use a graph convolutional network to\npredict the relations between text detection boxes and then build both levels\nof clusters from these predictions. Experimentally, we demonstrate that the\nunified approach can be highly efficient while still achieving state-of-the-art\nquality for detecting paragraphs in public benchmarks and real-world images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renshen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raptis_M/0/1/0/all/0/1\">Michalis Raptis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1\">Yasuhisa Fujii</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cascade Transformers for End-to-End Person Search. (arXiv:2203.09642v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09642","description":"<p>The goal of person search is to localize a target person from a gallery set\nof scene images, which is extremely challenging due to large scale variations,\npose/viewpoint changes, and occlusions. In this paper, we propose the Cascade\nOccluded Attention Transformer (COAT) for end-to-end person search. Our\nthree-stage cascade design focuses on detecting people in the first stage,\nwhile later stages simultaneously and progressively refine the representation\nfor person detection and re-identification. At each stage the occluded\nattention transformer applies tighter intersection over union thresholds,\nforcing the network to learn coarse-to-fine pose/scale invariant features.\nMeanwhile, we calculate each detection's occluded attention to differentiate a\nperson's tokens from other people or the background. In this way, we simulate\nthe effect of other objects occluding a person of interest at the token-level.\nThrough comprehensive experiments, we demonstrate the benefits of our method by\nachieving state-of-the-art performance on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1\">Rui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1\">Dawei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LaLonde_R/0/1/0/all/0/1\">Rodney LaLonde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davila_D/0/1/0/all/0/1\">Daniel Davila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Funk_C/0/1/0/all/0/1\">Christopher Funk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoogs_A/0/1/0/all/0/1\">Anthony Hoogs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clipp_B/0/1/0/all/0/1\">Brian Clipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MatchFormer: Interleaving Attention in Transformers for Feature Matching. (arXiv:2203.09645v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09645","description":"<p>Local feature matching is a computationally intensive task at the subpixel\nlevel. While detector-based methods coupled with feature descriptors struggle\nin low-texture scenes, CNN-based methods with a sequential extract-to-match\npipeline, fail to make use of the matching capacity of the encoder and tend to\noverburden the decoder for matching. In contrast, we propose a novel\nhierarchical extract-and-match transformer, termed as MatchFormer. Inside each\nstage of the hierarchical encoder, we interleave self-attention for feature\nextraction and cross-attention for feature matching, enabling a human-intuitive\nextract-and-match scheme. Such a match-aware encoder releases the overloaded\ndecoder and makes the model highly efficient. Further, combining self- and\ncross-attention on multi-scale features in a hierarchical architecture improves\nmatching robustness, particularly in low-texture indoor scenes or with less\noutdoor training data. Thanks to such a strategy, MatchFormer is a multi-win\nsolution in efficiency, robustness, and precision. Compared to the previous\nbest method in indoor pose estimation, our lite MatchFormer has only 45%\nGFLOPs, yet achieves a +1.3% precision gain and a 41% running speed boost. The\nlarge MatchFormer reaches state-of-the-art on four different benchmarks,\nincluding indoor pose estimation (ScanNet), outdoor pose estimation\n(MegaDepth), homography estimation and image matching (HPatch), and visual\nlocalization (InLoc). Code will be made publicly available at\nhttps://github.com/jamycheung/MatchFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regional Semantic Contrast and Aggregation for Weakly Supervised Semantic Segmentation. (arXiv:2203.09653v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09653","description":"<p>Learning semantic segmentation from weakly-labeled (e.g., image tags only)\ndata is challenging since it is hard to infer dense object regions from sparse\nsemantic tags. Despite being broadly studied, most current efforts directly\nlearn from limited semantic annotations carried by individual image or image\npairs, and struggle to obtain integral localization maps. Our work alleviates\nthis from a novel perspective, by exploring rich semantic contexts\nsynergistically among abundant weakly-labeled training data for network\nlearning and inference. In particular, we propose regional semantic contrast\nand aggregation (RCA) . RCA is equipped with a regional memory bank to store\nmassive, diverse object patterns appearing in training data, which acts as\nstrong support for exploration of dataset-level semantic structure.\nParticularly, we propose i) semantic contrast to drive network learning by\ncontrasting massive categorical object regions, leading to a more holistic\nobject pattern understanding, and ii) semantic aggregation to gather diverse\nrelational contexts in the memory to enrich semantic representations. In this\nmanner, RCA earns a strong capability of fine-grained semantic understanding,\nand eventually establishes new state-of-the-art results on two popular\nbenchmarks, i.e., PASCAL VOC 2012 and COCO 2014.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwu Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A workflow for segmenting soil and plant X-ray CT images with deep learning in Googles Colaboratory. (arXiv:2203.09674v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09674","description":"<p>X-ray micro-computed tomography (X-ray microCT) has enabled the\ncharacterization of the properties and processes that take place in plants and\nsoils at the micron scale. Despite the widespread use of this advanced\ntechnique, major limitations in both hardware and software limit the speed and\naccuracy of image processing and data analysis. Recent advances in machine\nlearning, specifically the application of convolutional neural networks to\nimage analysis, have enabled rapid and accurate segmentation of image data.\nYet, challenges remain in applying convolutional neural networks to the\nanalysis of environmentally and agriculturally relevant images. Specifically,\nthere is a disconnect between the computer scientists and engineers, who build\nthese AI/ML tools, and the potential end users in agricultural research, who\nmay be unsure of how to apply these tools in their work. Additionally, the\ncomputing resources required for training and applying deep learning models are\nunique, more common to computer gaming systems or graphics design work, than to\ntraditional computational systems. To navigate these challenges, we developed a\nmodular workflow for applying convolutional neural networks to X-ray microCT\nimages, using low-cost resources in Googles Colaboratory web application. Here\nwe present the results of the workflow, illustrating how parameters can be\noptimized to achieve best results using example scans from walnut leaves,\nalmond flower buds, and a soil aggregate. We expect that this framework will\naccelerate the adoption and use of emerging deep learning techniques within the\nplant and soil sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rippner_D/0/1/0/all/0/1\">Devin A. Rippner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raja_P/0/1/0/all/0/1\">Pranav Raja</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Earles_J/0/1/0/all/0/1\">J. Mason Earles</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Buchko_A/0/1/0/all/0/1\">Alexander Buchko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momayyezi_M/0/1/0/all/0/1\">Mina Momayyezi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duong_F/0/1/0/all/0/1\">Fiona Duong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parkinson_D/0/1/0/all/0/1\">Dilworth Parkinson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Forrestel_E/0/1/0/all/0/1\">Elizabeth Forrestel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shackel_K/0/1/0/all/0/1\">Ken Shackel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McElrone_A/0/1/0/all/0/1\">Andrew J. McElrone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Intensification for Sign Language Generation: A Computational Approach. (arXiv:2203.09679v1 [cs.CL])","link":"http://arxiv.org/abs/2203.09679","description":"<p>End-to-end sign language generation models do not accurately represent the\nprosody in sign language. A lack of temporal and spatial variations leads to\npoor-quality generated presentations that confuse human interpreters. In this\npaper, we aim to improve the prosody in generated sign languages by modeling\nintensification in a data-driven manner. We present different strategies\ngrounded in linguistics of sign language that inform how intensity modifiers\ncan be represented in gloss annotations. To employ our strategies, we first\nannotate a subset of the benchmark PHOENIX-14T, a German Sign Language dataset,\nwith different levels of intensification. We then use a supervised intensity\ntagger to extend the annotated dataset and obtain labels for the remaining\nportion of it. This enhanced dataset is then used to train state-of-the-art\ntransformer models for sign language generation. We find that our efforts in\nintensification modeling yield better results when evaluated with automatic\nmetrics. Human evaluation also indicates a higher preference of the videos\ngenerated using our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1\">Sabit Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quandt_L/0/1/0/all/0/1\">Lorna Quandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Facial Geometric Detail Recovery via Implicit Representation. (arXiv:2203.09692v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09692","description":"<p>Learning a dense 3D model with fine-scale details from a single facial image\nis highly challenging and ill-posed. To address this problem, many approaches\nfit smooth geometries through facial prior while learning details as additional\ndisplacement maps or personalized basis. However, these techniques typically\nrequire vast datasets of paired multi-view data or 3D scans, whereas such\ndatasets are scarce and expensive. To alleviate heavy data dependency, we\npresent a robust texture-guided geometric detail recovery approach using only a\nsingle in-the-wild facial image. More specifically, our method combines\nhigh-quality texture completion with the powerful expressiveness of implicit\nsurfaces. Initially, we inpaint occluded facial parts, generate complete\ntextures, and build an accurate multi-view dataset of the same subject. In\norder to estimate the detailed geometry, we define an implicit signed distance\nfunction and employ a physically-based implicit renderer to reconstruct fine\ngeometric details from the generated multi-view images. Our method not only\nrecovers accurate facial details but also decomposes normals, albedos, and\nshading parts in a self-supervised way. Finally, we register the implicit shape\ndetails to a 3D Morphable Model template, which can be used in traditional\nmodeling and rendering pipelines. Extensive experiments demonstrate that the\nproposed approach can reconstruct impressive facial details from a single\nimage, especially when compared with state-of-the-art methods trained on large\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xingyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lattas_A/0/1/0/all/0/1\">Alexandros Lattas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gecer_B/0/1/0/all/0/1\">Baris Gecer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiankang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chao Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group Contextualization for Video Recognition. (arXiv:2203.09694v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09694","description":"<p>Learning discriminative representation from the complex spatio-temporal\ndynamic space is essential for video recognition. On top of those stylized\nspatio-temporal computational units, further refining the learnt feature with\naxial contexts is demonstrated to be promising in achieving this goal. However,\nprevious works generally focus on utilizing a single kind of contexts to\ncalibrate entire feature channels and could hardly apply to deal with diverse\nvideo activities. The problem can be tackled by using pair-wise spatio-temporal\nattentions to recompute feature response with cross-axis contexts at the\nexpense of heavy computations. In this paper, we propose an efficient feature\nrefinement method that decomposes the feature channels into several groups and\nseparately refines them with different axial contexts in parallel. We refer\nthis lightweight feature calibration as group contextualization (GC).\nSpecifically, we design a family of efficient element-wise calibrators, i.e.,\nECal-G/S/T/L, where their axial contexts are information dynamics aggregated\nfrom other axes either globally or locally, to contextualize feature channel\ngroups. The GC module can be densely plugged into each residual layer of the\noff-the-shelf video networks. With little computational overhead, consistent\nimprovement is observed when plugging in GC on different networks. By utilizing\ncalibrators to embed feature with four different kinds of contexts in parallel,\nthe learnt representation is expected to be more resilient to diverse types of\nactivities. On videos with rich temporal variations, empirically GC can boost\nthe performance of 2D-CNN (e.g., TSN and TSM) to a level comparable to the\nstate-of-the-art video networks. Code is available at\nhttps://github.com/haoyanbin918/Group-Contextualization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yanbin Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1\">Chong-Wah Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiangnan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VISTA: Boosting 3D Object Detection via Dual Cross-VIew SpaTial Attention. (arXiv:2203.09704v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09704","description":"<p>Detecting objects from LiDAR point clouds is of tremendous significance in\nautonomous driving. In spite of good progress, accurate and reliable 3D\ndetection is yet to be achieved due to the sparsity and irregularity of LiDAR\npoint clouds. Among existing strategies, multi-view methods have shown great\npromise by leveraging the more comprehensive information from both bird's eye\nview (BEV) and range view (RV). These multi-view methods either refine the\nproposals predicted from single view via fused features, or fuse the features\nwithout considering the global spatial context; their performance is limited\nconsequently. In this paper, we propose to adaptively fuse multi-view features\nin a global spatial context via Dual Cross-VIew SpaTial Attention (VISTA). The\nproposed VISTA is a novel plug-and-play fusion module, wherein the multi-layer\nperceptron widely adopted in standard attention modules is replaced with a\nconvolutional one. Thanks to the learned attention mechanism, VISTA can produce\nfused features of high quality for prediction of proposals. We decouple the\nclassification and regression tasks in VISTA, and an additional constraint of\nattention variance is applied that enables the attention module to focus on\nspecific targets instead of generic points. We conduct thorough experiments on\nthe benchmarks of nuScenes and Waymo; results confirm the efficacy of our\ndesigns. At the time of submission, our method achieves 63.0% in overall mAP\nand 69.8% in NDS on the nuScenes benchmark, outperforming all published methods\nby up to 24% in safety-crucial categories such as cyclist. The source code in\nPyTorch is available at https://github.com/Gorilla-Lab-SCUT/VISTA\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shengheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhihao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deterministic Bridge Regression for Compressive Classification. (arXiv:2203.09721v1 [cs.LG])","link":"http://arxiv.org/abs/2203.09721","description":"<p>Pattern classification with compact representation is an important component\nin machine intelligence. In this work, an analytic bridge solution is proposed\nfor compressive classification. The proposal has been based upon solving a\npenalized error formulation utilizing an approximated $\\ell_p$-norm. The\nsolution comes in a primal form for over-determined systems and in a dual form\nfor under-determined systems. While the primal form is suitable for problems of\nlow dimension with large data samples, the dual form is suitable for problems\nof high dimension but with a small number of data samples. The solution has\nalso been extended for problems with multiple classification outputs. Numerical\nstudies based on simulated and real-world data validated the effectiveness of\nthe proposed solution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toh_K/0/1/0/all/0/1\">Kar-Ann Toh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molteni_G/0/1/0/all/0/1\">Giuseppe Molteni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhiping Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking the optimization process for self-supervised model-driven MRI reconstruction. (arXiv:2203.09724v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09724","description":"<p>Recovering high-quality images from undersampled measurements is critical for\naccelerated MRI reconstruction. Recently, various supervised deep\nlearning-based MRI reconstruction methods have been developed. Despite the\nachieved promising performances, these methods require fully sampled reference\ndata, the acquisition of which is resource-intensive and time-consuming.\nSelf-supervised learning has emerged as a promising solution to alleviate the\nreliance on fully sampled datasets. However, existing self-supervised methods\nsuffer from reconstruction errors due to the insufficient constraint enforced\non the non-sampled data points and the error accumulation happened alongside\nthe iterative image reconstruction process for model-driven deep learning\nreconstrutions. To address these challenges, we propose K2Calibrate, a K-space\nadaptation strategy for self-supervised model-driven MR reconstruction\noptimization. By iteratively calibrating the learned measurements, K2Calibrate\ncan reduce the network's reconstruction deterioration caused by statistically\ndependent noise. Extensive experiments have been conducted on the open-source\ndataset FastMRI, and K2Calibrate achieves better results than five\nstate-of-the-art methods. The proposed K2Calibrate is plug-and-play and can be\neasily integrated with different model-driven deep learning reconstruction\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_W/0/1/0/all/0/1\">Weijian Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Cheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_W/0/1/0/all/0/1\">Wenxin Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongjin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qiegen Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hairong Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REALY: Rethinking the Evaluation of 3D Face Reconstruction. (arXiv:2203.09729v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09729","description":"<p>The evaluation of 3D face reconstruction results typically relies on a rigid\nshape alignment between the estimated 3D model and the ground-truth scan. We\nobserve that aligning two shapes with different reference points can largely\naffect the evaluation results. This poses difficulties for precisely diagnosing\nand improving a 3D face reconstruction method. In this paper, we propose a\nnovel evaluation approach with a new benchmark REALY, consists of 100 globally\naligned face scans with accurate facial keypoints, high-quality region masks,\nand topology-consistent meshes. Our approach performs region-wise shape\nalignment and leads to more accurate, bidirectional correspondences during\ncomputing the shape errors. The fine-grained, region-wise evaluation results\nprovide us detailed understandings about the performance of state-of-the-art 3D\nface reconstruction methods. For example, our experiments on single-image based\nreconstruction methods reveal that DECA performs the best on nose regions,\nwhile GANFit performs better on cheek regions. Besides, a new and high-quality\n3DMM basis, HIFI3D++, is further derived using the same procedure as we\nconstruct REALY to align and retopologize several 3D face datasets. We will\nrelease REALY, HIFI3D++, and our new evaluation pipeline at\nhttps://realy3dface.com.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Z/0/1/0/all/0/1\">Zenghao Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jing Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzhuo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual Weighting Label Assignment Scheme for Object Detection. (arXiv:2203.09730v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09730","description":"<p>Label assignment (LA), which aims to assign each training sample a positive\n(pos) and a negative (neg) loss weight, plays an important role in object\ndetection. Existing LA methods mostly focus on the design of pos weighting\nfunction, while the neg weight is directly derived from the pos weight. Such a\nmechanism limits the learning capacity of detectors. In this paper, we explore\na new weighting paradigm, termed dual weighting (DW), to specify pos and neg\nweights separately. We first identify the key influential factors of pos/neg\nweights by analyzing the evaluation metrics in object detection, and then\ndesign the pos and neg weighting functions based on them. Specifically, the pos\nweight of a sample is determined by the consistency degree between its\nclassification and localization scores, while the neg weight is decomposed into\ntwo terms: the probability that it is a neg sample and its importance\nconditioned on being a neg sample. Such a weighting strategy offers greater\nflexibility to distinguish between important and less important samples,\nresulting in a more effective object detector. Equipped with the proposed DW\nmethod, a single FCOS-ResNet-50 detector can reach 41.5% mAP on COCO under 1x\nschedule, outperforming other existing LA methods. It consistently improves the\nbaselines on COCO by a large margin under various backbones without bells and\nwhistles. Code is available at https://github.com/strongwolf/DW.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distortion-Tolerant Monocular Depth Estimation On Omnidirectional Images Using Dual-cubemap. (arXiv:2203.09733v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09733","description":"<p>Estimating the depth of omnidirectional images is more challenging than that\nof normal field-of-view (NFoV) images because the varying distortion can\nsignificantly twist an object's shape. The existing methods suffer from\ntroublesome distortion while estimating the depth of omnidirectional images,\nleading to inferior performance. To reduce the negative impact of the\ndistortion influence, we propose a distortion-tolerant omnidirectional depth\nestimation algorithm using a dual-cubemap. It comprises two modules:\nDual-Cubemap Depth Estimation (DCDE) module and Boundary Revision (BR) module.\nIn DCDE module, we present a rotation-based dual-cubemap model to estimate the\naccurate NFoV depth, reducing the distortion at the cost of boundary\ndiscontinuity on omnidirectional depths. Then a boundary revision module is\ndesigned to smooth the discontinuous boundaries, which contributes to the\nprecise and visually continuous omnidirectional depths. Extensive experiments\ndemonstrate the superiority of our method over other state-of-the-art\nsolutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhijie Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chunyu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Lang Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1\">Kang Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+zhao_Y/0/1/0/all/0/1\">Yao zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Series Photo Selection via Multi-view Graph Learning. (arXiv:2203.09736v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09736","description":"<p>Series photo selection (SPS) is an important branch of the image aesthetics\nquality assessment, which focuses on finding the best one from a series of\nnearly identical photos. While a great progress has been observed, most of the\nexisting SPS approaches concentrate solely on extracting features from the\noriginal image, neglecting that multiple views, e.g, saturation level, color\nhistogram and depth of field of the image, will be of benefit to successfully\nreflecting the subtle aesthetic changes. Taken multi-view into consideration,\nwe leverage a graph neural network to construct the relationships between\nmulti-view features. Besides, multiple views are aggregated with an\nadaptive-weight self-attention module to verify the significance of each view.\nFinally, a siamese network is proposed to select the best one from a series of\nnearly identical photos. Experimental results demonstrate that our model\naccomplish the highest success rates compared with competitive methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yongshun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xiushan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Learning with Mutual Distillation for Monocular Depth Estimation. (arXiv:2203.09737v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09737","description":"<p>We propose a semi-supervised learning framework for monocular depth\nestimation. Compared to existing semi-supervised learning methods, which\ninherit limitations of both sparse supervised and unsupervised loss functions,\nwe achieve the complementary advantages of both loss functions, by building two\nseparate network branches for each loss and distilling each other through the\nmutual distillation loss function. We also present to apply different data\naugmentation to each branch, which improves the robustness. We conduct\nexperiments to demonstrate the effectiveness of our framework over the latest\nmethods and provide extensive ablation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jongbeom Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongnyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seungryong Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Deep Networks Transfer Invariances Across Classes?. (arXiv:2203.09739v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09739","description":"<p>To generalize well, classifiers must learn to be invariant to nuisance\ntransformations that do not alter an input's class. Many problems have\n\"class-agnostic\" nuisance transformations that apply similarly to all classes,\nsuch as lighting and background changes for image classification. Neural\nnetworks can learn these invariances given sufficient data, but many real-world\ndatasets are heavily class imbalanced and contain only a few examples for most\nof the classes. We therefore pose the question: how well do neural networks\ntransfer class-agnostic invariances learned from the large classes to the small\nones? Through careful experimentation, we observe that invariance to\nclass-agnostic transformations is still heavily dependent on class size, with\nthe networks being much less invariant on smaller classes. This result holds\neven when using data balancing techniques, and suggests poor invariance\ntransfer across classes. Our results provide one explanation for why\nclassifiers generalize poorly on unbalanced and long-tailed distributions.\nBased on this analysis, we show how a generative approach for learning the\nnuisance transformations can help transfer invariances across classes and\nimprove performance on a set of imbalanced image classification benchmarks.\nSource code for our experiments is available at\nhttps://github.com/AllanYangZhou/generative-invariance-transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1\">Allan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tajwar_F/0/1/0/all/0/1\">Fahim Tajwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1\">Alexander Robey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Knowles_T/0/1/0/all/0/1\">Tom Knowles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1\">George J. Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1\">Hamed Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chelsea Finn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Balanced Pixel-Level Self-Labeling for Domain Adaptive Semantic Segmentation. (arXiv:2203.09744v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09744","description":"<p>Domain adaptive semantic segmentation aims to learn a model with the\nsupervision of source domain data, and produce satisfactory dense predictions\non unlabeled target domain. One popular solution to this challenging task is\nself-training, which selects high-scoring predictions on target samples as\npseudo labels for training. However, the produced pseudo labels often contain\nmuch noise because the model is biased to source domain as well as majority\ncategories. To address the above issues, we propose to directly explore the\nintrinsic pixel distributions of target domain data, instead of heavily relying\non the source domain. Specifically, we simultaneously cluster pixels and\nrectify pseudo labels with the obtained cluster assignments. This process is\ndone in an online fashion so that pseudo labels could co-evolve with the\nsegmentation model without extra training rounds. To overcome the class\nimbalance problem on long-tailed categories, we employ a distribution alignment\ntechnique to enforce the marginal class distribution of cluster assignments to\nbe close to that of pseudo labels. The proposed method, namely Class-balanced\nPixel-level Self-Labeling (CPSL), improves the segmentation performance on\ntarget domain over state-of-the-arts by a large margin, especially on\nlong-tailed categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Chenhang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yabin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robot peels banana with goal-conditioned dual-action deep imitation learning. (arXiv:2203.09749v1 [cs.RO])","link":"http://arxiv.org/abs/2203.09749","description":"<p>A long-horizon dexterous robot manipulation task of deformable objects, such\nas banana peeling, is problematic because of difficulties in object modeling\nand a lack of knowledge about stable and dexterous manipulation skills. This\npaper presents a goal-conditioned dual-action deep imitation learning (DIL)\nwhich can learn dexterous manipulation skills using human demonstration data.\nPrevious DIL methods map the current sensory input and reactive action, which\neasily fails because of compounding errors in imitation learning caused by\nrecurrent computation of actions. The proposed method predicts reactive action\nwhen the precise manipulation of the target object is required (local action)\nand generates the entire trajectory when the precise manipulation is not\nrequired. This dual-action formulation effectively prevents compounding error\nwith the trajectory-based global action while respond to unexpected changes in\nthe target object with the reactive local action. Furthermore, in this\nformulation, both global/local actions are conditioned by a goal state which is\ndefined as the last step of each subtask, for robust policy prediction. The\nproposed method was tested in the real dual-arm robot and successfully\naccomplished the banana peeling task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Heecheol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohmura_Y/0/1/0/all/0/1\">Yoshiyuki Ohmura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_Y/0/1/0/all/0/1\">Yasuo Kuniyoshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoAdversary: A Pixel Pruning Method for Sparse Adversarial Attack. (arXiv:2203.09756v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09756","description":"<p>Deep neural networks (DNNs) have been proven to be vulnerable to adversarial\nexamples. A special branch of adversarial examples, namely sparse adversarial\nexamples, can fool the target DNNs by perturbing only a few pixels. However,\nmany existing sparse adversarial attacks use heuristic methods to select the\npixels to be perturbed, and regard the pixel selection and the adversarial\nattack as two separate steps. From the perspective of neural network pruning,\nwe propose a novel end-to-end sparse adversarial attack method, namely\nAutoAdversary, which can find the most important pixels automatically by\nintegrating the pixel selection into the adversarial attack. Specifically, our\nmethod utilizes a trainable neural network to generate a binary mask for the\npixel selection. After jointly optimizing the adversarial perturbation and the\nneural network, only the pixels corresponding to the value 1 in the mask are\nperturbed. Experiments demonstrate the superiority of our proposed method over\nseveral state-of-the-art methods. Furthermore, since AutoAdversary does not\nrequire a heuristic pixel selection process, it does not slow down excessively\nas other methods when the image size increases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaotao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1\">Furao Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond a Video Frame Interpolator: A Space Decoupled Learning Approach to Continuous Image Transition. (arXiv:2203.09771v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09771","description":"<p>Video frame interpolation (VFI) aims to improve the temporal resolution of a\nvideo sequence. Most of the existing deep learning based VFI methods adopt\noff-the-shelf optical flow algorithms to estimate the bidirectional flows and\ninterpolate the missing frames accordingly. Though having achieved a great\nsuccess, these methods require much human experience to tune the bidirectional\nflows and often generate unpleasant results when the estimated flows are not\naccurate. In this work, we rethink the VFI problem and formulate it as a\ncontinuous image transition (CIT) task, whose key issue is to transition an\nimage from one space to another space continuously. More specifically, we learn\nto implicitly decouple the images into a translatable flow space and a\nnon-translatable feature space. The former depicts the translatable states\nbetween the given images, while the later aims to reconstruct the intermediate\nfeatures that cannot be directly translated. In this way, we can easily perform\nimage interpolation in the flow space and intermediate image synthesis in the\nfeature space, obtaining a CIT model. The proposed space decoupled learning\n(SDL) approach is simple to implement, while it provides an effective framework\nto a variety of CIT problems beyond VFI, such as style transfer and image\nmorphing. Our extensive experiments on a variety of CIT tasks demonstrate the\nsuperiority of SDL to existing methods. The source code and models can be found\nat \\url{https://github.com/yangxy/SDL}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Peiran Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xuansong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xiansheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Completing Partial Point Clouds with Outliers by Collaborative Completion and Segmentation. (arXiv:2203.09772v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09772","description":"<p>Most existing point cloud completion methods are only applicable to partial\npoint clouds without any noises and outliers, which does not always hold in\npractice. We propose in this paper an end-to-end network, named CS-Net, to\ncomplete the point clouds contaminated by noises or containing outliers. In our\nCS-Net, the completion and segmentation modules work collaboratively to promote\neach other, benefited from our specifically designed cascaded structure. With\nthe help of segmentation, more clean point cloud is fed into the completion\nmodule. We design a novel completion decoder which harnesses the labels\nobtained by segmentation together with FPS to purify the point cloud and\nleverages KNN-grouping for better generation. The completion and segmentation\nmodules work alternately share the useful information from each other to\ngradually improve the quality of prediction. To train our network, we build a\ndataset to simulate the real case where incomplete point clouds contain\noutliers. Our comprehensive experiments and comparisons against\nstate-of-the-art completion methods demonstrate our superiority. We also\ncompare with the scheme of segmentation followed by completion and their\nend-to-end fusion, which also proves our efficacy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Changfeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yanwen Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local-Global Context Aware Transformer for Language-Guided Video Segmentation. (arXiv:2203.09773v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09773","description":"<p>We explore the task of language-guided video segmentation (LVS). Previous\nalgorithms mostly adopt 3D CNNs to learn video representation, struggling to\ncapture long-term context and easily suffering from visual-linguistic\nmisalignment. In light of this, we present Locater (local-global context aware\nTransformer), which augments the Transformer architecture with a finite memory\nso as to query the entire video with the language expression in an efficient\nmanner. The memory is designed to involve two components -- one for\npersistently preserving global video content, and one for dynamically gathering\nlocal temporal context and segmentation history. Based on the memorized\nlocal-global context and the particular content of each frame, Locater\nholistically and flexibly comprehends the expression as an adaptive query\nvector for each frame. The vector is used to query the corresponding frame for\nmask generation. The memory also allows Locater to process videos with linear\ntime complexity and constant size memory, while Transformer-style\nself-attention computation scales quadratically with sequence length. To\nthoroughly examine the visual grounding capability of LVS models, we contribute\na new LVS dataset, A2D-S+, which is built upon A2D-S dataset but poses\nincreased challenges in disambiguating among similar objects. Experiments on\nthree LVS datasets and our A2D-S+ show that Locater outperforms previous\nstate-of-the-arts. Further, our Locater based solution achieved the 1st place\nin the Referring Video Object Segmentation Track of the 3rd Large-scale Video\nObject Segmentation Challenge. Our code and dataset are available at:\nhttps://github.com/leonnnop/Locater\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1\">Jiaxu Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ContrastMask: Contrastive Learning to Segment Every Thing. (arXiv:2203.09775v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09775","description":"<p>Partially-supervised instance segmentation is a task which requests\nsegmenting objects from novel unseen categories via learning on limited seen\ncategories with annotated masks thus eliminating demands of heavy annotation\nburden. The key to addressing this task is to build an effective class-agnostic\nmask segmentation model. Unlike previous methods that learn such models only on\nseen categories, in this paper, we propose a new method, named ContrastMask,\nwhich learns a mask segmentation model on both seen and unseen categories under\na unified pixel-level contrastive learning framework. In this framework,\nannotated masks of seen categories and pseudo masks of unseen categories serve\nas a prior for contrastive learning, where features from the mask regions\n(foreground) are pulled together, and are contrasted against those from the\nbackground, and vice versa. Through this framework, feature discrimination\nbetween foreground and background is largely improved, facilitating learning of\nthe class-agnostic mask segmentation model. Exhaustive experiments on the COCO\ndataset demonstrate the superiority of our method, which outperforms previous\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuehui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1\">Kai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruixin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1\">Wei Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferable Class-Modelling for Decentralized Source Attribution of GAN-Generated Images. (arXiv:2203.09777v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09777","description":"<p>GAN-generated deepfakes as a genre of digital images are gaining ground as\nboth catalysts of artistic expression and malicious forms of deception,\ntherefore demanding systems to enforce and accredit their ethical use. Existing\ntechniques for the source attribution of synthetic images identify subtle\nintrinsic fingerprints using multiclass classification neural nets limited in\nfunctionality and scalability. Hence, we redefine the deepfake detection and\nsource attribution problems as a series of related binary classification tasks.\nWe leverage transfer learning to rapidly adapt forgery detection networks for\nmultiple independent attribution problems, by proposing a semi-decentralized\nmodular design to solve them simultaneously and efficiently. Class activation\nmapping is also demonstrated as an effective means of feature localization for\nmodel interpretation. Our models are determined via experimentation to be\ncompetitive with current benchmarks, and capable of decent performance on human\nportraits in ideal conditions. Decentralized fingerprint-based attribution is\nfound to retain validity in the presence of novel sources, but is more\nsusceptible to type II errors that intensify with image perturbations and\nattributive uncertainty. We describe both our conceptual framework and model\nprototypes for further enhancement when investigating the technical limits of\nreactive deepfake attribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khoo_B/0/1/0/all/0/1\">Brandon B. G. Khoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_C/0/1/0/all/0/1\">Chern Hong Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_R/0/1/0/all/0/1\">Raphael C.-W. Phan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuse Dense: Towards High Quality 3D Detection with Depth Completion. (arXiv:2203.09780v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09780","description":"<p>Current LiDAR-only 3D detection methods inevitably suffer from the sparsity\nof point clouds. Many multi-modal methods are proposed to alleviate this issue,\nwhile different representations of images and point clouds make it difficult to\nfuse them, resulting in suboptimal performance. In this paper, we present a\nnovel multi-modal framework SFD (Sparse Fuse Dense), which utilizes pseudo\npoint clouds generated from depth completion to tackle the issues mentioned\nabove. Different from prior works, we propose a new RoI fusion strategy 3D-GAF\n(3D Grid-wise Attentive Fusion) to make fuller use of information from\ndifferent types of point clouds. Specifically, 3D-GAF fuses 3D RoI features\nfrom the couple of point clouds in a grid-wise attentive way, which is more\nfine-grained and more precise. In addition, we propose a SynAugment\n(Synchronized Augmentation) to enable our multi-modal framework to utilize all\ndata augmentation approaches tailored to LiDAR-only methods. Lastly, we\ncustomize an effective and efficient feature extractor CPConv (Color Point\nConvolution) for pseudo point clouds. It can explore 2D image features and 3D\ngeometric features of pseudo point clouds simultaneously. Our method holds the\nhighest entry on the KITTI car 3D object detection leaderboard, demonstrating\nthe effectiveness of our SFD. Code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaopei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Liang Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Honghui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Liang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chenxi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1\">Chengqi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haifeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Robust 2D Convolution for Reliable Visual Recognition. (arXiv:2203.09790v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09790","description":"<p>2D convolution (Conv2d), which is responsible for extracting features from\nthe input image, is one of the key modules of a convolutional neural network\n(CNN). However, Conv2d is vulnerable to image corruptions and adversarial\nsamples. It is an important yet rarely investigated problem that whether we can\ndesign a more robust alternative of Conv2d for more reliable feature\nextraction. In this paper, inspired by the recently developed learnable sparse\ntransform that learns to convert the CNN features into a compact and sparse\nlatent space, we design a novel building block, denoted by RConv-MK, to\nstrengthen the robustness of extracted convolutional features. Our method\nleverages a set of learnable kernels of different sizes to extract features at\ndifferent frequencies and employs a normalized soft thresholding operator to\nadaptively remove noises and trivial features at different corruption levels.\nExtensive experiments on clean images, corrupted images as well as adversarial\nsamples validate the effectiveness of the proposed robust module for reliable\nvisual recognition. The source codes are enclosed in the submission.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lida Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shuai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiangchu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Three things everyone should know about Vision Transformers. (arXiv:2203.09795v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09795","description":"<p>After their initial success in natural language processing, transformer\narchitectures have rapidly gained traction in computer vision, providing\nstate-of-the-art results for tasks such as image classification, detection,\nsegmentation, and video analysis. We offer three insights based on simple and\neasy to implement variants of vision transformers. (1) The residual layers of\nvision transformers, which are usually processed sequentially, can to some\nextent be processed efficiently in parallel without noticeably affecting the\naccuracy. (2) Fine-tuning the weights of the attention layers is sufficient to\nadapt vision transformers to a higher resolution and to other classification\ntasks. This saves compute, reduces the peak memory consumption at fine-tuning\ntime, and allows sharing the majority of weights across tasks. (3) Adding\nMLP-based patch pre-processing layers improves Bert-like self-supervised\ntraining based on patch masking. We evaluate the impact of these design choices\nusing the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test\nset. Transfer performance is measured across six smaller datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Nouby_A/0/1/0/all/0/1\">Alaaeldin El-Nouby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1\">Jakob Verbeek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Consistency from High-quality Pseudo-labels for Weakly Supervised Object Localization. (arXiv:2203.09803v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09803","description":"<p>Pseudo-supervised learning methods have been shown to be effective for weakly\nsupervised object localization tasks. However, the effectiveness depends on the\npowerful regularization ability of deep neural networks. Based on the\nassumption that the localization network should have similar location\npredictions on different versions of the same image, we propose a two-stage\napproach to learn more consistent localization. In the first stage, we propose\na mask-based pseudo label generator algorithm, and use the pseudo-supervised\nlearning method to initialize an object localization network. In the second\nstage, we propose a simple and effective method for evaluating the confidence\nof pseudo-labels based on classification discrimination, and by learning\nconsistency from high-quality pseudo-labels, we further refine the localization\nnetwork to get better localization performance. Experimental results show that\nour proposed approach achieves excellent performance in three benchmark\ndatasets including CUB-200-2011, ImageNet-1k and Tiny-ImageNet, which\ndemonstrates its effectiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kangbo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jie Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stacked Hybrid-Attention and Group Collaborative Learning for Unbiased Scene Graph Generation. (arXiv:2203.09811v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09811","description":"<p>Scene Graph Generation, which generally follows a regular encoder-decoder\npipeline, aims to first encode the visual contents within the given image and\nthen parse them into a compact summary graph. Existing SGG approaches generally\nnot only neglect the insufficient modality fusion between vision and language,\nbut also fail to provide informative predicates due to the biased relationship\npredictions, leading SGG far from practical. Towards this end, in this paper,\nwe first present a novel Stacked Hybrid-Attention network, which facilitates\nthe intra-modal refinement as well as the inter-modal interaction, to serve as\nthe encoder. We then devise an innovative Group Collaborative Learning strategy\nto optimize the decoder. Particularly, based upon the observation that the\nrecognition capability of one classifier is limited towards an extremely\nunbalanced dataset, we first deploy a group of classifiers that are expert in\ndistinguishing different subsets of classes, and then cooperatively optimize\nthem from two aspects to promote the unbiased SGG. Experiments conducted on VG\nand GQA datasets demonstrate that, we not only establish a new state-of-the-art\nin the unbiased metric, but also nearly double the performance compared with\ntwo baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xingning Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_T/0/1/0/all/0/1\">Tian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xuemeng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1\">Liqiang Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grasp Pre-shape Selection by Synthetic Training: Eye-in-hand Shared Control on the Hannes Prosthesis. (arXiv:2203.09812v1 [cs.RO])","link":"http://arxiv.org/abs/2203.09812","description":"<p>We consider the task of object grasping with a prosthetic hand capable of\nmultiple grasp types. In this setting, communicating the intended grasp type\noften requires a high user cognitive load which can be reduced adopting shared\nautonomy frameworks. Among these, so-called eye-in-hand systems automatically\ncontrol the hand aperture and pre-shaping before the grasp, based on visual\ninput coming from a camera on the wrist. In this work, we present an\neye-in-hand learning-based approach for hand pre-shape classification from RGB\nsequences. In order to reduce the need for tedious data collection sessions for\ntraining the system, we devise a pipeline for rendering synthetic visual\nsequences of hand trajectories for the purpose. We tackle the peculiarity of\nthe eye-in-hand setting by means of a model for the human arm trajectories,\nwith domain randomization over relevant visual elements. We develop a\nsensorized setup to acquire real human grasping sequences for benchmarking and\nshow that, compared on practical use cases, models trained with our synthetic\ndataset achieve better generalization performance than models trained on real\ndata. We finally integrate our model on the Hannes prosthetic hand and show its\npractical effectiveness. Our code, real and synthetic datasets will be released\nupon acceptance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasile_F/0/1/0/all/0/1\">Federico Vasile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maiettini_E/0/1/0/all/0/1\">Elisa Maiettini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasquale_G/0/1/0/all/0/1\">Giulia Pasquale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florio_A/0/1/0/all/0/1\">Astrid Florio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boccardo_N/0/1/0/all/0/1\">Nicol&#xf2; Boccardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1\">Lorenzo Natale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?. (arXiv:2203.09824v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09824","description":"<p>This work digs into a root question in human perception: can face geometry be\ngleaned from one's voices? Previous works that study this question only adopt\ndevelopments in image synthesis and convert voices into face images to show\ncorrelations, but working on the image domain unavoidably involves predicting\nattributes that voices cannot hint, including facial textures, hairstyles, and\nbackgrounds. We instead investigate the ability to reconstruct 3D faces to\nconcentrate on only geometry, which is much more physiologically grounded. We\npropose our analysis framework, Cross-Modal Perceptionist, under both\nsupervised and unsupervised learning. First, we construct a dataset,\nVoxceleb-3D, which extends Voxceleb and includes paired voices and face meshes,\nmaking supervised learning possible. Second, we use a knowledge distillation\nmechanism to study whether face geometry can still be gleaned from voices\nwithout paired voices and 3D face data under limited availability of 3D face\nscans. We break down the core question into four parts and perform visual and\nnumerical analyses as responses to the core question. Our findings echo those\nin physiology and neuroscience about the correlation between voices and facial\nstructures. The work provides future human-centric cross-modal learning with\nexplainable foundations. See our project page:\nhttps://choyingw.github.io/works/Voice2Mesh/index.html\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Cho-Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1\">Chin-Cheng Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_U/0/1/0/all/0/1\">Ulrich Neumann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Laneformer: Object-aware Row-Column Transformers for Lane Detection. (arXiv:2203.09830v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09830","description":"<p>We present Laneformer, a conceptually simple yet powerful transformer-based\narchitecture tailored for lane detection that is a long-standing research topic\nfor visual perception in autonomous driving. The dominant paradigms rely on\npurely CNN-based architectures which often fail in incorporating relations of\nlong-range lane points and global contexts induced by surrounding objects\n(e.g., pedestrians, vehicles). Inspired by recent advances of the transformer\nencoder-decoder architecture in various vision tasks, we move forwards to\ndesign a new end-to-end Laneformer architecture that revolutionizes the\nconventional transformers into better capturing the shape and semantic\ncharacteristics of lanes, with minimal overhead in latency. First, coupling\nwith deformable pixel-wise self-attention in the encoder, Laneformer presents\ntwo new row and column self-attention operations to efficiently mine point\ncontext along with the lane shapes. Second, motivated by the appearing objects\nwould affect the decision of predicting lane segments, Laneformer further\nincludes the detected object instances as extra inputs of multi-head attention\nblocks in the encoder and decoder to facilitate the lane point detection by\nsensing semantic contexts. Specifically, the bounding box locations of objects\nare added into Key module to provide interaction with each pixel and query\nwhile the ROI-aligned features are inserted into Value module. Extensive\nexperiments demonstrate our Laneformer achieves state-of-the-art performances\non CULane benchmark, in terms of 77.1% F1 score. We hope our simple and\neffective Laneformer will serve as a strong baseline for future research in\nself-attention models for lane detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jianhua Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xinyue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DTA: Physical Camouflage Attacks using Differentiable Transformation Network. (arXiv:2203.09831v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09831","description":"<p>To perform adversarial attacks in the physical world, many studies have\nproposed adversarial camouflage, a method to hide a target object by applying\ncamouflage patterns on 3D object surfaces. For obtaining optimal physical\nadversarial camouflage, previous studies have utilized the so-called neural\nrenderer, as it supports differentiability. However, existing neural renderers\ncannot fully represent various real-world transformations due to a lack of\ncontrol of scene parameters compared to the legacy photo-realistic renderers.\nIn this paper, we propose the Differentiable Transformation Attack (DTA), a\nframework for generating a robust physical adversarial pattern on a target\nobject to camouflage it against object detection models with a wide range of\ntransformations. It utilizes our novel Differentiable Transformation Network\n(DTN), which learns the expected transformation of a rendered object when the\ntexture is changed while preserving the original properties of the target\nobject. Using our attack framework, an adversary can gain both the advantages\nof the legacy photo-realistic renderers including various physical-world\ntransformations and the benefit of white-box access by offering\ndifferentiability. Our experiments show that our camouflaged 3D vehicles can\nsuccessfully evade state-of-the-art object detection models in the\nphoto-realistic environment (i.e., CARLA on Unreal Engine). Furthermore, our\ndemonstration on a scaled Tesla Model 3 proves the applicability and\ntransferability of our method to the real world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suryanto_N/0/1/0/all/0/1\">Naufal Suryanto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yongsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyoeun Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larasati_H/0/1/0/all/0/1\">Harashta Tatimma Larasati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_Y/0/1/0/all/0/1\">Youngyeo Yun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thi-Thu-Huong Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hunmin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Se-Yoon Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Howon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perspective Flow Aggregation for Data-Limited 6D Object Pose Estimation. (arXiv:2203.09836v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09836","description":"<p>Most recent 6D object pose estimation methods, including unsupervised ones,\nrequire many real training images. Unfortunately, for some applications, such\nas those in space or deep under water, acquiring real images, even unannotated,\nis virtually impossible. In this paper, we propose a method that can be trained\nsolely on synthetic images, or optionally using a few additional real ones.\nGiven a rough pose estimate obtained from a first network, it uses a second\nnetwork to predict a dense 2D correspondence field between the image rendered\nusing the rough pose and the real image and infers the required pose\ncorrection. This approach is much less sensitive to the domain shift between\nsynthetic and real images than state-of-the-art methods. It performs on par\nwith methods that require annotated real images for training when not using\nany, and outperforms them considerably when using as few as twenty real images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yinlin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Location-Free Camouflage Generation Network. (arXiv:2203.09845v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09845","description":"<p>Camouflage is a common visual phenomenon, which refers to hiding the\nforeground objects into the background images, making them briefly invisible to\nthe human eye. Previous work has typically been implemented by an iterative\noptimization process. However, these methods struggle in 1) efficiently\ngenerating camouflage images using foreground and background with arbitrary\nstructure; 2) camouflaging foreground objects to regions with multiple\nappearances (e.g. the junction of the vegetation and the mountains), which\nlimit their practical application. To address these problems, this paper\nproposes a novel Location-free Camouflage Generation Network (LCG-Net) that\nfuse high-level features of foreground and background image, and generate\nresult by one inference. Specifically, a Position-aligned Structure Fusion\n(PSF) module is devised to guide structure feature fusion based on the\npoint-to-point structure similarity of foreground and background, and introduce\nlocal appearance features point-by-point. To retain the necessary identifiable\nfeatures, a new immerse loss is adopted under our pipeline, while a background\npatch appearance loss is utilized to ensure that the hidden objects look\ncontinuous and natural at regions with multiple appearances. Experiments show\nthat our method has results as satisfactory as state-of-the-art in the\nsingle-appearance regions and are less likely to be completely invisible, but\nfar exceed the quality of the state-of-the-art in the multi-appearance regions.\nMoreover, our method is hundreds of times faster than previous methods.\nBenefitting from the unique advantages of our method, we provide some\ndownstream applications for camouflage generation, which show its potential.\nThe related code and dataset will be released at\nhttps://github.com/Tale17/LCG-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1\">Zheng-jun Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modal Masked Pre-Training for Monocular Panoramic Depth Completion. (arXiv:2203.09855v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09855","description":"<p>In this paper, we formulate a potentially valuable panoramic depth completion\n(PDC) task as panoramic 3D cameras often produce 360{\\deg} depth with missing\ndata in complex scenes. Its goal is to recover dense panoramic depths from raw\nsparse ones and panoramic RGB images. To deal with the PDC task, we train a\ndeep network that takes both depth and image as inputs for the dense panoramic\ndepth recovery. However, it needs to face a challenging optimization problem of\nthe network parameters due to its non-convex objective function. To address\nthis problem, we propose a simple yet effective approach termed M{^3}PT:\nmulti-modal masked pre-training. Specifically, during pre-training, we\nsimultaneously cover up patches of the panoramic RGB image and sparse depth by\nshared random mask, then reconstruct the sparse depth in the masked regions. To\nour best knowledge, it is the first time that we show the effectiveness of\nmasked pre-training in a multi-modal vision task, instead of the single-modal\ntask resolved by masked autoencoders (MAE). Different from MAE where\nfine-tuning completely discards the decoder part of pre-training, there is no\narchitectural difference between the pre-training and fine-tuning stages in our\nM$^{3}$PT as they only differ in the prediction density, which potentially\nmakes the transfer learning more convenient and effective. Extensive\nexperiments verify the effectiveness of M{^3}PT on three panoramic datasets.\nNotably, we improve the state-of-the-art baselines by averagely 26.2% in RMSE,\n51.7% in MRE, 49.7% in MAE, and 37.5% in RMSElog on three benchmark datasets.\nCodes and pre-trained models are available at\nhttps://github.com/anonymoustbd/MMMPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiqiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jian Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification. (arXiv:2203.09860v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09860","description":"<p>Deep learning models were frequently reported to learn from shortcuts like\ndataset biases. As deep learning is playing an increasingly important role in\nthe modern healthcare system, it is of great need to combat shortcut learning\nin medical data as well as develop unbiased and trustworthy models. In this\npaper, we study the problem of developing debiased chest X-ray diagnosis models\nfrom the biased training data without knowing exactly the bias labels. We start\nwith the observations that the imbalance of bias distribution is one of the key\nreasons causing shortcut learning, and the dataset biases are preferred by the\nmodel if they were easier to be learned than the intended features. Based on\nthese observations, we propose a novel algorithm, pseudo bias-balanced\nlearning, which first captures and predicts per-sample bias labels via\ngeneralized cross entropy loss and then trains a debiased model using pseudo\nbias labels and bias-balanced softmax function. To our best knowledge, we are\npioneered in tackling dataset biases in medical images without explicit\nlabeling on the bias attributes. We constructed several chest X-ray datasets\nwith various dataset bias situations and demonstrated with extensive\nexperiments that our proposed method achieved consistent improvements over\nother state-of-the-art approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Luo_L/0/1/0/all/0/1\">Luyang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_D/0/1/0/all/0/1\">Dunyuan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wong_T/0/1/0/all/0/1\">Tien-Tsin Wong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heng_P/0/1/0/all/0/1\">Pheng-Ann Heng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance. (arXiv:2203.09887v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09887","description":"<p>Transformers have gained much attention by outperforming convolutional neural\nnetworks in many 2D vision tasks. However, they are known to have\ngeneralization problems and rely on massive-scale pre-training and\nsophisticated training techniques. When applying to 3D tasks, the irregular\ndata structure and limited data scale add to the difficulty of transformer's\napplication. We propose CodedVTR (Codebook-based Voxel TRansformer), which\nimproves data efficiency and generalization ability for 3D sparse voxel\ntransformers. On the one hand, we propose the codebook-based attention that\nprojects an attention space into its subspace represented by the combination of\n\"prototypes\" in a learnable codebook. It regularizes attention learning and\nimproves generalization. On the other hand, we propose geometry-aware\nself-attention that utilizes geometric information (geometric pattern, density)\nto guide attention learning. CodedVTR could be embedded into existing sparse\nconvolution-based methods, and bring consistent performance improvements for\nindoor and outdoor 3D semantic segmentation tasks\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tianchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Niansong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1\">Xuefei Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">He Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1\">Li Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Affordance Grounding from Exocentric Images. (arXiv:2203.09905v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09905","description":"<p>Affordance grounding, a task to ground (i.e., localize) action possibility\nregion in objects, which faces the challenge of establishing an explicit link\nwith object parts due to the diversity of interactive affordance. Human has the\nability that transform the various exocentric interactions to invariant\negocentric affordance so as to counter the impact of interactive diversity. To\nempower an agent with such ability, this paper proposes a task of affordance\ngrounding from exocentric view, i.e., given exocentric human-object interaction\nand egocentric object images, learning the affordance knowledge of the object\nand transferring it to the egocentric image using only the affordance label as\nsupervision. To this end, we devise a cross-view knowledge transfer framework\nthat extracts affordance-specific features from exocentric interactions and\nenhances the perception of affordance regions by preserving affordance\ncorrelation. Specifically, an Affordance Invariance Mining module is devised to\nextract specific clues by minimizing the intra-class differences originated\nfrom interaction habits in exocentric images. Besides, an Affordance\nCo-relation Preserving strategy is presented to perceive and localize\naffordance by aligning the co-relation matrix of predicted results between the\ntwo views. Particularly, an affordance grounding dataset named AGD20K is\nconstructed by collecting and labeling over 20K images from 36 affordance\ncategories. Experimental results demonstrate that our method outperforms the\nrepresentative models in terms of objective metrics and visual quality. Code:\ngithub.com/lhc1224/Cross-View-AG.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Hongchen Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1\">Wei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fourier Document Restoration for Robust Document Dewarping and Recognition. (arXiv:2203.09910v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09910","description":"<p>State-of-the-art document dewarping techniques learn to predict 3-dimensional\ninformation of documents which are prone to errors while dealing with documents\nwith irregular distortions or large variations in depth. This paper presents\nFDRNet, a Fourier Document Restoration Network that can restore documents with\ndifferent distortions and improve document recognition in a reliable and\nsimpler manner. FDRNet focuses on high-frequency components in the Fourier\nspace that capture most structural information but are largely free of\ndegradation in appearance. It dewarps documents by a flexible Thin-Plate Spline\ntransformation which can handle various deformations effectively without\nrequiring deformation annotations in training. These features allow FDRNet to\nlearn from a small amount of simply labeled training images, and the learned\nmodel can dewarp documents with complex geometric distortion and recognize the\nrestored texts accurately. To facilitate document restoration research, we\ncreate a benchmark dataset consisting of over one thousand camera documents\nwith different types of geometric and photometric distortion. Extensive\nexperiments show that FDRNet outperforms the state-of-the-art by large margins\non both dewarping and text recognition tasks. In addition, FDRNet requires a\nsmall amount of simply labeled training data and is easy to deploy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1\">Chuhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zichen Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Song Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convolutional Simultaneous Sparse Approximation with Applications to RGB-NIR Image Fusion. (arXiv:2203.09913v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09913","description":"<p>Simultaneous sparse approximation (SSA) seeks to represent a set of dependent\nsignals using sparse vectors with identical supports. The SSA model has been\nused in various signal and image processing applications involving multiple\ncorrelated input signals. In this paper, we propose algorithms for\nconvolutional SSA (CSSA) based on the alternating direction method of\nmultipliers. Specifically, we address the CSSA problem with different sparsity\nstructures and the convolutional feature learning problem in multimodal\ndata/signals based on the SSA model. We evaluate the proposed algorithms by\napplying them to multimodal and multifocus image fusion problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Veshki_F/0/1/0/all/0/1\">Farshad G. Veshki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorobyov_S/0/1/0/all/0/1\">Sergiy A. Vorobyov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revealing Reliable Signatures by Learning Top-Rank Pairs. (arXiv:2203.09927v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09927","description":"<p>Signature verification, as a crucial practical documentation analysis task,\nhas been continuously studied by researchers in machine learning and pattern\nrecognition fields. In specific scenarios like confirming financial documents\nand legal instruments, ensuring the absolute reliability of signatures is of\ntop priority. In this work, we proposed a new method to learn \"top-rank pairs\"\nfor writer-independent offline signature verification tasks. By this scheme, it\nis possible to maximize the number of absolutely reliable signatures. More\nprecisely, our method to learn top-rank pairs aims at pushing positive samples\nbeyond negative samples, after pairing each of them with a genuine reference\nsignature. In the experiment, BHSig-B and BHSig-H datasets are used for\nevaluation, on which the proposed model achieves overwhelming better pos@top\n(the ratio of absolute top positive samples to all of the positive samples)\nwhile showing encouraging performance on both Area Under the Curve (AUC) and\naccuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiaotong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suehiro_D/0/1/0/all/0/1\">Daiki Suehiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deepfake Style Transfer Mixture: a First Forensic Ballistics Study on Synthetic Images. (arXiv:2203.09928v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09928","description":"<p>Most recent style-transfer techniques based on generative architectures are\nable to obtain synthetic multimedia contents, or commonly called deepfakes,\nwith almost no artifacts. Researchers already demonstrated that synthetic\nimages contain patterns that can determine not only if it is a deepfake but\nalso the generative architecture employed to create the image data itself.\nThese traces can be exploited to study problems that have never been addressed\nin the context of deepfakes. To this aim, in this paper a first approach to\ninvestigate the image ballistics on deepfake images subject to style-transfer\nmanipulations is proposed. Specifically, this paper describes a study on\ndetecting how many times a digital image has been processed by a generative\narchitecture for style transfer. Moreover, in order to address and study\naccurately forensic ballistics on deepfake images, some mathematical properties\nof style-transfer operations were investigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guarnera_L/0/1/0/all/0/1\">Luca Guarnera</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Giudice_O/0/1/0/all/0/1\">Oliver Giudice</a> (1 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1\">Sebastiano Battiato</a> (1 and 2) ((1) University of Catania, (2) iCTLab s.r.l. - Spin-off of University of Catania, (3) Applied Research Team, IT dept., Banca d&#x27;Italia, Italy)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3DAC: Learning Attribute Compression for Point Clouds. (arXiv:2203.09931v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09931","description":"<p>We study the problem of attribute compression for large-scale unstructured 3D\npoint clouds. Through an in-depth exploration of the relationships between\ndifferent encoding steps and different attribute channels, we introduce a deep\ncompression network, termed 3DAC, to explicitly compress the attributes of 3D\npoint clouds and reduce storage usage in this paper. Specifically, the point\ncloud attributes such as color and reflectance are firstly converted to\ntransform coefficients. We then propose a deep entropy model to model the\nprobabilities of these coefficients by considering information hidden in\nattribute transforms and previous encoded attributes. Finally, the estimated\nprobabilities are used to further compress these transform coefficients to a\nfinal attributes bitstream. Extensive experiments conducted on both indoor and\noutdoor large-scale open point cloud datasets, including ScanNet and\nSemanticKITTI, demonstrated the superior compression rates and reconstruction\nquality of the proposed 3DAC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1\">Guangchi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1\">Qingyong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiling Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yulan Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the sensitivity of pose estimation neural networks: rotation parameterizations, Lipschitz constants, and provable bounds. (arXiv:2203.09937v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09937","description":"<p>In this paper, we approach the task of determining sensitivity bounds for\npose estimation neural networks. This task is particularly challenging as it\nrequires characterizing the sensitivity of 3D rotations. We develop a\nsensitivity measure that describes the maximum rotational change in a network's\noutput with respect to a Euclidean change in its input. We show that this\nmeasure is a type of Lipschitz constant, and that it is bounded by the product\nof a network's Euclidean Lipschitz constant and an intrinsic property of a\nrotation parameterization which we call the \"distance ratio constant\". We\nderive the distance ratio constant for several rotation parameterizations, and\nthen discuss why the structure of most of these parameterizations makes it\ndifficult to construct a pose estimation network with provable sensitivity\nbounds. However, we show that sensitivity bounds can be computed for networks\nwhich parameterize rotation using unconstrained exponential coordinates. We\nthen construct and train such a network and compute sensitivity bounds for it.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avant_T/0/1/0/all/0/1\">Trevor Avant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgansen_K/0/1/0/all/0/1\">Kristi A. Morgansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Enhanced Belief Propagation for Data Assocation in Multiobject Tracking. (arXiv:2203.09948v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09948","description":"<p>Situation-aware technologies enabled by multiobject tracking (MOT) methods\nwill create new services and applications in fields such as autonomous\nnavigation and applied ocean sciences. Belief propagation (BP) is a\nstate-of-the-art method for Bayesian MOT but fully relies on a statistical\nmodel and preprocessed sensor measurements. In this paper, we establish a\nhybrid method for model-based and data-driven MOT. The proposed neural enhanced\nbelief propagation (NEBP) approach complements BP by information learned from\nraw sensor data with the goal to improve data association and to reject false\nalarm measurements. We evaluate the performance of our NEBP approach for MOT on\nthe nuScenes autonomous driving dataset and demonstrate that it can outperform\nstate-of-the-art reference methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1\">Mingchao Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_F/0/1/0/all/0/1\">Florian Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancement of Novel View Synthesis Using Omnidirectional Image Completion. (arXiv:2203.09957v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09957","description":"<p>We present a method for synthesizing novel views from a single 360-degree\nimage based on the neural radiance field (NeRF) . Prior studies rely on the\nneighborhood interpolation capability of multi-layer perceptrons to complete\nmissing regions caused by occlusion and zooming, and this leads to artifacts.\nIn the proposed method, the input image is reprojected to 360-degree images at\nother camera positions, the missing regions of the reprojected images are\ncompleted by a self-supervised trained generative model, and the completed\nimages are utilized to train the NeRF. Because multiple completed images\ncontain inconsistencies in 3D, we introduce a method to train NeRF while\ndynamically selecting a sparse set of completed images, to reduce the\ndiscrimination error of the synthesized views with real images. Experiments\nindicate that the proposed method can synthesize plausible novel views while\npreserving the features of the scene for both artificial and real-world data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1\">Takayuki Hara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynthStrip: Skull-Stripping for Any Brain Image. (arXiv:2203.09974v1 [eess.IV])","link":"http://arxiv.org/abs/2203.09974","description":"<p>The removal of non-brain signal from magnetic resonance imaging (MRI) data,\nknown as skull-stripping, is an integral component of many neuroimage analysis\nstreams. Despite their abundance, popular classical skull-stripping methods are\nusually tailored to images with specific acquisition properties, namely\nnear-isotropic resolution and T1-weighted (T1w) MRI contrast, which are\nprevalent in research settings. As a result, existing tools tend to adapt\npoorly to other image types, such as stacks of thick slices acquired with fast\nspin-echo (FSE) MRI that are common in the clinic. While learning-based\napproaches for brain extraction have gained traction in recent years, these\nmethods face a similar burden, as they are only effective for image types seen\nduring the training procedure. To achieve robust skull-stripping across a\nlandscape of protocols, we introduce SynthStrip, a rapid, learning-based\nbrain-extraction tool. By leveraging anatomical segmentations to generate an\nentirely synthetic training dataset with anatomies, intensity distributions,\nand artifacts that far exceed the realistic range of medical images, SynthStrip\nlearns to successfully generalize to a variety of real acquired brain images,\nremoving the need for training data with target contrasts. We demonstrate the\nefficacy of SynthStrip for a diverse set of image acquisitions and resolutions\nacross subject populations, ranging from newborn to adult. We show substantial\nimprovements in accuracy over popular skull-stripping baselines - all with a\nsingle trained model. Our method and labeled evaluation data are available at\nhttps://w3id.org/synthstrip.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hoopes_A/0/1/0/all/0/1\">Andrew Hoopes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mora_J/0/1/0/all/0/1\">Jocelyn S. Mora</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoffmann_M/0/1/0/all/0/1\">Malte Hoffmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GiNGR: Generalized Iterative Non-Rigid Point Cloud and Surface Registration Using Gaussian Process Regression. (arXiv:2203.09986v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09986","description":"<p>In this paper, we unify popular non-rigid registration methods for point sets\nand surfaces under our general framework, GiNGR. GiNGR builds upon Gaussian\nProcess Morphable Models (GPMM) and hence separates modeling the deformation\nprior from model adaptation for registration. In addition, it provides\nexplainable hyperparameters, multi-resolution registration, trivial inclusion\nof expert annotation, and the ability to use and combine analytical and\nstatistical deformation priors. But more importantly, the reformulation allows\nfor a direct comparison of registration methods. Instead of using a general\nsolver in the optimization step, we show how Gaussian process regression (GPR)\niteratively can warp a reference onto a target, leading to smooth deformations\nfollowing the prior for any dense, sparse, or partial estimated correspondences\nin a principled way. We show how the popular CPD and ICP algorithms can be\ndirectly explained with GiNGR. Furthermore, we show how existing algorithms in\nthe GiNGR framework can perform probabilistic registration to obtain a\ndistribution of different registrations instead of a single best registration.\nThis can be used to analyze the uncertainty e.g. when registering partial\nobservations. GiNGR is publicly available and fully modular to allow for\ndomain-specific prior construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madsen_D/0/1/0/all/0/1\">Dennis Madsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aellen_J/0/1/0/all/0/1\">Jonathan Aellen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morel_Forster_A/0/1/0/all/0/1\">Andreas Morel-Forster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetter_T/0/1/0/all/0/1\">Thomas Vetter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luthi_M/0/1/0/all/0/1\">Marcel L&#xfc;thi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion and Volume Maximization-Based Clustering of Highly Mixed Hyperspectral Images. (arXiv:2203.09992v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09992","description":"<p>Hyperspectral images of a scene or object are a rich data source, often\nencoding a hundred or more spectral bands of reflectance at each pixel. Despite\nbeing very high-dimensional, these images typically encode latent\nlow-dimensional structure that can be exploited for material discrimination.\nHowever, due to an inherent trade-off between spectral and spatial resolution,\nmany hyperspectral images are generated at a coarse spatial scale, and single\npixels may correspond to spatial regions containing multiple materials. This\narticle introduces the \\emph{Diffusion and Volume maximization-based Image\nClustering} (\\emph{D-VIC}) algorithm for unsupervised material discrimination.\nD-VIC locates cluster modes -- high-density, high-purity pixels in the\nhyperspectral image that are far in diffusion distance (a data-dependent\ndistance metric) from other high-density, high-purity pixels -- and assigns\nthese pixels unique labels, as these points are meant to exemplify underlying\nmaterial structure. Non-modal pixels are labeled according to their diffusion\ndistance nearest neighbor of higher density and purity that is already labeled.\nBy directly incorporating pixel purity into its modal and non-modal labeling,\nD-VIC upweights pixels that correspond to a spatial region containing just a\nsingle material, yielding more interpretable clusterings. D-VIC is shown to\noutperform baseline and comparable state-of-the-art methods in extensive\nnumerical experiments on a range of hyperspectral images, implying that it is\nwell-equipped for material discrimination and clustering of these data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polk_S/0/1/0/all/0/1\">Sam L. Polk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kangning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1\">Robert J. Plemmons</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murphy_J/0/1/0/all/0/1\">James M. Murphy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Elastica Models for Color Image Regularization. (arXiv:2203.09995v1 [cs.CV])","link":"http://arxiv.org/abs/2203.09995","description":"<p>One classical approach to regularize color is to tream them as two\ndimensional surfaces embedded in a five dimensional spatial-chromatic space. In\nthis case, a natural regularization term arises as the image surface area.\nChoosing the chromatic coordinates as dominating over the spatial ones, the\nimage spatial coordinates could be thought of as a paramterization of the image\nsurface manifold in a three dimensional color space. Minimizing the area of the\nimage manifold leads to the Beltrami flow or mean curvature flow of the image\nsurface in the 3D color space, while minimizing the elastica of the image\nsurface yields an additional interesting regularization. Recently, the authors\nproposed a color elastica model, which minimizes both the surface area and\nelastica of the image manifold. In this paper, we propose to modify the color\nelastica and introduce two new models for color image regularization. The\nrevised measures are motivated by the relations between the color elastica\nmodel, Euler's elastica model and the total variation model for gray level\nimages. Compared to our previous color elastica model, the new models are\ndirect extensions of Euler's elastica model to color images. The proposed\nmodels are nonlinear and challenging to minimize. To overcome this difficulty,\ntwo operator-splitting methods are suggested. Specifically, nonlinearities are\ndecoupled by introducing new vector- and matrix-valued variables. Then, the\nminimization problems are converted to solving initial value problems which are\ntime-discretized by operator splitting. Each subproblem, after splitting\neither, has a closed-form solution or can be solved efficiently. The\neffectiveness and advantages of the proposed models are demonstrated by\ncomprehensive experiments. The benefits of incorporating the elastica of the\nimage surface as regularization terms compared to common alternatives are\nempirically validated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1\">Xue-Cheng Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimmel_R/0/1/0/all/0/1\">Ron Kimmel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glowinski_R/0/1/0/all/0/1\">Roland Glowinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Application of Top-hat Transformation for Enhanced Blood Vessel Extraction. (arXiv:2203.10005v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10005","description":"<p>In the medical domain, different computer-aided diagnosis systems have been\nproposed to extract blood vessels from retinal fundus images for the clinical\ntreatment of vascular diseases. Accurate extraction of blood vessels from the\nfundus images using a computer-generated method can help the clinician to\nproduce timely and accurate reports for the patient suffering from these\ndiseases. In this article, we integrate top-hat based preprocessing approach\nwith fine-tuned B-COSFIRE filter to achieve more accurate segregation of blood\nvessel pixels from the background. The use of top-hat transformation in the\npreprocessing stage enhances the efficacy of the algorithm to extract blood\nvessels in presence of structures like fovea, exudates, haemorrhages, etc.\nFurthermore, to reduce the false positives, small clusters of blood vessel\npixels are removed in the postprocessing stage. Further, we find that the\nproposed algorithm is more efficient as compared to various modern algorithms\nreported in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Das_T/0/1/0/all/0/1\">Tithi Parna Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praharaj_S/0/1/0/all/0/1\">Sheetal Praharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swain_S/0/1/0/all/0/1\">Sarita Swain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumanshu Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1\">Kundan Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultra-low Latency Spiking Neural Networks with Spatio-Temporal Compression and Synaptic Convolutional Block. (arXiv:2203.10006v1 [cs.NE])","link":"http://arxiv.org/abs/2203.10006","description":"<p>Spiking neural networks (SNNs), as one of the brain-inspired models, has\nspatio-temporal information processing capability, low power feature, and high\nbiological plausibility. The effective spatio-temporal feature makes it\nsuitable for event streams classification. However, neuromorphic datasets, such\nas N-MNIST, CIFAR10-DVS, DVS128-gesture, need to aggregate individual events\ninto frames with a new higher temporal resolution for event stream\nclassification, which causes high training and inference latency. In this work,\nwe proposed a spatio-temporal compression method to aggregate individual events\ninto a few time steps of synaptic current to reduce the training and inference\nlatency. To keep the accuracy of SNNs under high compression ratios, we also\nproposed a synaptic convolutional block to balance the dramatic change between\nadjacent time steps. And multi-threshold Leaky Integrate-and-Fire (LIF) with\nlearnable membrane time constant is introduced to increase its information\nprocessing capability. We evaluate the proposed method for event streams\nclassification tasks on neuromorphic N-MNIST, CIFAR10-DVS, DVS128 gesture\ndatasets. The experiment results show that our proposed method outperforms the\nstate-of-the-art accuracy on nearly all datasets, using fewer time steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changqing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yintang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing EEG Data with Machine and Deep Learning: A Benchmark. (arXiv:2203.10009v1 [cs.LG])","link":"http://arxiv.org/abs/2203.10009","description":"<p>Nowadays, machine and deep learning techniques are widely used in different\nareas, ranging from economics to biology. In general, these techniques can be\nused in two ways: trying to adapt well-known models and architectures to the\navailable data, or designing custom architectures. In both cases, to speed up\nthe research process, it is useful to know which type of models work best for a\nspecific problem and/or data type. By focusing on EEG signal analysis, and for\nthe first time in literature, in this paper a benchmark of machine and deep\nlearning for EEG signal classification is proposed. For our experiments we used\nthe four most widespread models, i.e., multilayer perceptron, convolutional\nneural network, long short-term memory, and gated recurrent unit, highlighting\nwhich one can be a good starting point for developing EEG classification\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avola_D/0/1/0/all/0/1\">Danilo Avola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cascio_M/0/1/0/all/0/1\">Marco Cascio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cinque_L/0/1/0/all/0/1\">Luigi Cinque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fagioli_A/0/1/0/all/0/1\">Alessio Fagioli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marini_M/0/1/0/all/0/1\">Marco Raoul Marini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pannone_D/0/1/0/all/0/1\">Daniele Pannone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parametric Scaling of Preprocessing assisted U-net Architecture for Improvised Retinal Vessel Segmentation. (arXiv:2203.10014v1 [eess.IV])","link":"http://arxiv.org/abs/2203.10014","description":"<p>Extracting blood vessels from retinal fundus images plays a decisive role in\ndiagnosing the progression in pertinent diseases. In medical image analysis,\nvessel extraction is a semantic binary segmentation problem, where blood\nvasculature needs to be extracted from the background. Here, we present an\nimage enhancement technique based on the morphological preprocessing coupled\nwith a scaled U-net architecture. Despite a relatively less number of trainable\nnetwork parameters, the scaled version of U-net architecture provides better\nperformance compare to other methods in the domain. We validated the proposed\nmethod on retinal fundus images from the DRIVE database. A significant\nimprovement as compared to the other algorithms in the domain, in terms of the\narea under ROC curve (&gt;0.9762) and classification accuracy (&gt;95.47%) are\nevident from the results. Furthermore, the proposed method is resistant to the\ncentral vessel reflex while sensitive to detect blood vessels in the presence\nof background items viz. exudates, optic disc, and fovea.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kumar_K/0/1/0/all/0/1\">Kundan Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1\">Sumanshu Agarwal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESS: Learning Event-based Semantic Segmentation from Still Images. (arXiv:2203.10016v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10016","description":"<p>Retrieving accurate semantic information in challenging high dynamic range\n(HDR) and high-speed conditions remains an open challenge for image-based\nalgorithms due to severe image degradations. Event cameras promise to address\nthese challenges since they feature a much higher dynamic range and are\nresilient to motion blur. Nonetheless, semantic segmentation with event cameras\nis still in its infancy which is chiefly due to the novelty of the sensor, and\nthe lack of high-quality, labeled datasets. In this work, we introduce ESS,\nwhich tackles this problem by directly transferring the semantic segmentation\ntask from existing labeled image datasets to unlabeled events via unsupervised\ndomain adaptation (UDA). Compared to existing UDA methods, our approach aligns\nrecurrent, motion-invariant event embeddings with image embeddings. For this\nreason, our method neither requires video data nor per-pixel alignment between\nimages and events and, crucially, does not need to hallucinate motion from\nstill images. Additionally, to spur further research in event-based semantic\nsegmentation, we introduce DSEC-Semantic, the first large-scale event-based\ndataset with fine-grained labels. We show that using image labels alone, ESS\noutperforms existing UDA approaches, and when combined with event labels, it\neven outperforms state-of-the-art supervised approaches on both DDD17 and\nDSEC-Semantic. Finally, ESS is general-purpose, which unlocks the vast amount\nof existing labeled image datasets and paves the way for new and exciting\nresearch directions in new fields previously inaccessible for event cameras.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhaoning Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messikommer_N/0/1/0/all/0/1\">Nico Messikommer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehrig_D/0/1/0/all/0/1\">Daniel Gehrig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scaramuzza_D/0/1/0/all/0/1\">Davide Scaramuzza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unbiased Subclass Regularization for Semi-Supervised Semantic Segmentation. (arXiv:2203.10026v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10026","description":"<p>Semi-supervised semantic segmentation learns from small amounts of labelled\nimages and large amounts of unlabelled images, which has witnessed impressive\nprogress with the recent advance of deep neural networks. However, it often\nsuffers from severe class-bias problem while exploring the unlabelled images,\nlargely due to the clear pixel-wise class imbalance in the labelled images.\nThis paper presents an unbiased subclass regularization network (USRN) that\nalleviates the class imbalance issue by learning class-unbiased segmentation\nfrom balanced subclass distributions. We build the balanced subclass\ndistributions by clustering pixels of each original class into multiple\nsubclasses of similar sizes, which provide class-balanced pseudo supervision to\nregularize the class-biased segmentation. In addition, we design an\nentropy-based gate mechanism to coordinate learning between the original\nclasses and the clustered subclasses which facilitates subclass regularization\neffectively by suppressing unconfident subclass predictions. Extensive\nexperiments over multiple public benchmarks show that USRN achieves superior\nperformance as compared with the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonnegative-Constrained Joint Collaborative Representation with Union Dictionary for Hyperspectral Anomaly Detection. (arXiv:2203.10030v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10030","description":"<p>Recently, many collaborative representation-based (CR) algorithms have been\nproposed for hyperspectral anomaly detection. CR-based detectors approximate\nthe image by a linear combination of background dictionaries and the\ncoefficient matrix, and derive the detection map by utilizing recovery\nresiduals. However, these CR-based detectors are often established on the\npremise of precise background features and strong image representation, which\nare very difficult to obtain. In addition, pursuing the coefficient matrix\nreinforced by the general $l_2$-min is very time consuming. To address these\nissues, a nonnegative-constrained joint collaborative representation model is\nproposed in this paper for the hyperspectral anomaly detection task. To extract\nreliable samples, a union dictionary consisting of background and anomaly\nsub-dictionaries is designed, where the background sub-dictionary is obtained\nat the superpixel level and the anomaly sub-dictionary is extracted by the\npre-detection process. And the coefficient matrix is jointly optimized by the\nFrobenius norm regularization with a nonnegative constraint and a sum-to-one\nconstraint. After the optimization process, the abnormal information is finally\nderived by calculating the residuals that exclude the assumed background\ninformation. To conduct comparable experiments, the proposed\nnonnegative-constrained joint collaborative representation (NJCR) model and its\nkernel version (KNJCR) are tested in four HSI data sets and achieve superior\nresults compared with other state-of-the-art detectors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shizhen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SHREC 2021: Classification in cryo-electron tomograms. (arXiv:2203.10035v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10035","description":"<p>Cryo-electron tomography (cryo-ET) is an imaging technique that allows\nthree-dimensional visualization of macro-molecular assemblies under near-native\nconditions. Cryo-ET comes with a number of challenges, mainly low\nsignal-to-noise and inability to obtain images from all angles. Computational\nmethods are key to analyze cryo-electron tomograms.\n</p>\n<p>To promote innovation in computational methods, we generate a novel simulated\ndataset to benchmark different methods of localization and classification of\nbiological macromolecules in tomograms. Our publicly available dataset contains\nten tomographic reconstructions of simulated cell-like volumes. Each volume\ncontains twelve different types of complexes, varying in size, function and\nstructure.\n</p>\n<p>In this paper, we have evaluated seven different methods of finding and\nclassifying proteins. Seven research groups present results obtained with\nlearning-based methods and trained on the simulated dataset, as well as a\nbaseline template matching (TM), a traditional method widely used in cryo-ET\nresearch. We show that learning-based approaches can achieve notably better\nlocalization and classification performance than TM. We also experimentally\nconfirm that there is a negative relationship between particle size and\nperformance for all methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gubins_I/0/1/0/all/0/1\">Ilja Gubins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaillet_M/0/1/0/all/0/1\">Marten L. Chaillet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schot_G/0/1/0/all/0/1\">Gijs van der Schot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trueba_M/0/1/0/all/0/1\">M. Cristina Trueba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veltkamp_R/0/1/0/all/0/1\">Remco C. Veltkamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Forster_F/0/1/0/all/0/1\">Friedrich F&#xf6;rster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kihara_D/0/1/0/all/0/1\">Daisuke Kihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moebel_E/0/1/0/all/0/1\">Emmanuel Moebel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nguyen P. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_T/0/1/0/all/0/1\">Tommi White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bunyak_F/0/1/0/all/0/1\">Filiz Bunyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papoulias_G/0/1/0/all/0/1\">Giorgos Papoulias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerolymatos_S/0/1/0/all/0/1\">Stavros Gerolymatos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zacharaki_E/0/1/0/all/0/1\">Evangelia I. Zacharaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moustakas_K/0/1/0/all/0/1\">Konstantinos Moustakas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sinuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Cheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xuefeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fa Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-input segmentation of damaged brain in acute ischemic stroke patients using slow fusion with skip connection. (arXiv:2203.10039v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10039","description":"<p>Time is a fundamental factor during stroke treatments. A fast, automatic\napproach that segments the ischemic regions helps treatment decisions. In\nclinical use today, a set of color-coded parametric maps generated from\ncomputed tomography perfusion (CTP) images are investigated manually to decide\na treatment plan. We propose an automatic method based on a neural network\nusing a set of parametric maps to segment the two ischemic regions (core and\npenumbra) in patients affected by acute ischemic stroke. Our model is based on\na convolution-deconvolution bottleneck structure with multi-input and slow\nfusion. A loss function based on the focal Tversky index addresses the data\nimbalance issue. The proposed architecture demonstrates effective performance\nand results comparable to the ground truth annotated by neuroradiologists. A\nDice coefficient of 0.81 for penumbra and 0.52 for core over the large vessel\nocclusion test set is achieved. The full implementation is available at:\nhttps://git.io/JtFGb.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomasetti_L/0/1/0/all/0/1\">Luca Tomasetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_M/0/1/0/all/0/1\">Mahdieh Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engan_K/0/1/0/all/0/1\">Kjersti Engan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollesli_L/0/1/0/all/0/1\">Liv Jorunn H&#xf8;llesli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurz_K/0/1/0/all/0/1\">Kathinka D&#xe6;hli Kurz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imaging-based histological features are predictive of MET alterations in Non-Small Cell Lung Cancer. (arXiv:2203.10062v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10062","description":"<p>MET is a proto-oncogene whose somatic activation in non-small cell lung\ncancer leads to increased cell growth and tumor progression. The two major\nclasses of MET alterations are gene amplification and exon 14 deletion, both of\nwhich are therapeutic targets and detectable using existing molecular assays.\nHowever, existing tests are limited by their consumption of valuable tissue,\ncost and complexity that prevent widespread use. MET alterations could have an\neffect on cell morphology, and quantifying these associations could open new\navenues for research and development of morphology-based screening tools. Using\nH&amp;E-stained whole slide images (WSIs), we investigated the association of\ndistinct cell-morphological features with MET amplifications and MET exon 14\ndeletions. We found that cell shape, color, grayscale intensity and\ntexture-based features from both tumor infiltrating lymphocytes and tumor cells\ndistinguished MET wild-type from MET amplified or MET exon 14 deletion cases.\nThe association of individual cell features with MET alterations suggested a\npredictive model could distinguish MET wild-type from MET amplification or MET\nexon 14 deletion. We therefore developed an L1-penalized logistic regression\nmodel, achieving a mean Area Under the Receiver Operating Characteristic Curve\n(ROC-AUC) of 0.77 +/- 0.05sd in cross-validation and 0.77 on an independent\nholdout test set. A sparse set of 43 features differentiated these classes,\nwhich included features similar to what was found in the univariate analysis as\nwell as the percent of tumor cells in the tissue. Our study demonstrates that\nMET alterations result in a detectable morphological signal in tumor cells and\nlymphocytes. These results suggest that development of low-cost predictive\nmodels based on H&amp;E-stained WSIs may improve screening for MET altered tumors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1\">Rohan P. Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osinski_B/0/1/0/all/0/1\">Bo Osinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beig_N/0/1/0/all/0/1\">Niha Beig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1\">Lingdao Sha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingale_K/0/1/0/all/0/1\">Kshitij Ingale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lunar Rover Localization Using Craters as Landmarks. (arXiv:2203.10073v1 [cs.RO])","link":"http://arxiv.org/abs/2203.10073","description":"<p>Onboard localization capabilities for planetary rovers to date have used\nrelative navigation, by integrating combinations of wheel odometry, visual\nodometry, and inertial measurements during each drive to track position\nrelative to the start of each drive. At the end of each drive, a\nground-in-the-loop (GITL) interaction is used to get a position update from\nhuman operators in a more global reference frame, by matching images or local\nmaps from onboard the rover to orbital reconnaissance images or maps of a large\nregion around the rover's current position. Autonomous rover drives are limited\nin distance so that accumulated relative navigation error does not risk the\npossibility of the rover driving into hazards known from orbital images.\nHowever, several rover mission concepts have recently been studied that require\nmuch longer drives between GITL cycles, particularly for the Moon. These\nconcepts require greater autonomy to minimize GITL cycles to enable such large\nrange; onboard global localization is a key element of such autonomy. Multiple\ntechniques have been studied in the past for onboard rover global localization,\nbut a satisfactory solution has not yet emerged. For the Moon, the ubiquitous\ncraters offer a new possibility, which involves mapping craters from orbit,\nthen recognizing crater landmarks with cameras and-or a lidar onboard the\nrover. This approach is applicable everywhere on the Moon, does not require\nhigh resolution stereo imaging from orbit as some other approaches do, and has\npotential to enable position knowledge with order of 5 to 10 m accuracy at all\ntimes. This paper describes our technical approach to crater-based lunar rover\nlocalization and presents initial results on crater detection using 3D point\ncloud data from onboard lidar or stereo cameras, as well as using shading cues\nin monocular onboard imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matthies_L/0/1/0/all/0/1\">Larry Matthies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daftry_S/0/1/0/all/0/1\">Shreyansh Daftry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tepsuporn_S/0/1/0/all/0/1\">Scott Tepsuporn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yang Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atha_D/0/1/0/all/0/1\">Deegan Atha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swan_R/0/1/0/all/0/1\">R. Michael Swan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravichandar_S/0/1/0/all/0/1\">Sanjna Ravichandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ono_M/0/1/0/all/0/1\">Masahiro Ono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Inversion for Nonlinear Imaging Models using Deep Generative Priors. (arXiv:2203.10078v1 [cs.CV])","link":"http://arxiv.org/abs/2203.10078","description":"<p>Most modern imaging systems involve a computational reconstruction pipeline\nto infer the image of interest from acquired measurements. The Bayesian\nreconstruction framework relies on the characterization of the posterior\ndistribution, which depends on a model of the imaging system and prior\nknowledge on the image, for solving such inverse problems. Here, the choice of\nthe prior distribution is critical for obtaining high-quality estimates. In\nthis work, we use deep generative models to represent the prior distribution.\nWe develop a posterior sampling scheme for the class of nonlinear inverse\nproblems where the forward model has a neural-network-like structure. This\nclass includes most existing imaging modalities. We introduce the notion of\naugmented generative models in order to suitably handle quantitative image\nrecovery. We illustrate the advantages of our framework by applying it to two\nnonlinear imaging modalities-phase retrieval and optical diffraction\ntomography.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bohra_P/0/1/0/all/0/1\">Pakshal Bohra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1\">Thanh-an Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jonathan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unser_M/0/1/0/all/0/1\">Michael Unser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Fingerprinting for Generative Models: Rooting Deepfake Attribution in Training Data. (arXiv:2007.08457v7 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2007.08457","description":"<p>Photorealistic image generation has reached a new level of quality due to the\nbreakthroughs of generative adversarial networks (GANs). Yet, the dark side of\nsuch deepfakes, the malicious use of generated media, raises concerns about\nvisual misinformation. While existing research work on deepfake detection\ndemonstrates high accuracy, it is subject to advances in generation techniques\nand adversarial iterations on detection countermeasure techniques. Thus, we\nseek a proactive and sustainable solution on deepfake detection, that is\nagnostic to the evolution of generative models, by introducing artificial\nfingerprints into the models.\n</p>\n<p>Our approach is simple and effective. We first embed artificial fingerprints\ninto training data, then validate a surprising discovery on the transferability\nof such fingerprints from training data to generative models, which in turn\nappears in the generated deepfakes. Experiments show that our fingerprinting\nsolution (1) holds for a variety of cutting-edge generative models, (2) leads\nto a negligible side effect on generation quality, (3) stays robust against\nimage-level and model-level perturbations, (4) stays hard to be detected by\nadversaries, and (5) converts deepfake detection and attribution into trivial\ntasks and outperforms the recent state-of-the-art baselines. Our solution\ncloses the responsibility loop between publishing pre-trained generative model\ninventions and their possible misuses, which makes it independent of the\ncurrent arms race. Code and models are available at\nhttps://github.com/ningyu1991/ArtificialGANFingerprints .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skripniuk_V/0/1/0/all/0/1\">Vladislav Skripniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelnabi_S/0/1/0/all/0/1\">Sahar Abdelnabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MODNet: Real-Time Trimap-Free Portrait Matting via Objective Decomposition. (arXiv:2011.11961v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.11961","description":"<p>Existing portrait matting methods either require auxiliary inputs that are\ncostly to obtain or involve multiple stages that are computationally expensive,\nmaking them less suitable for real-time applications. In this work, we present\na light-weight matting objective decomposition network (MODNet) for portrait\nmatting in real-time with a single input image. The key idea behind our\nefficient design is by optimizing a series of sub-objectives simultaneously via\nexplicit constraints. In addition, MODNet includes two novel techniques for\nimproving model efficiency and robustness. First, an Efficient Atrous Spatial\nPyramid Pooling (e-ASPP) module is introduced to fuse multi-scale features for\nsemantic estimation. Second, a self-supervised sub-objectives consistency (SOC)\nstrategy is proposed to adapt MODNet to real-world data to address the domain\nshift problem common to trimap-free methods. MODNet is easy to be trained in an\nend-to-end manner. It is much faster than contemporaneous methods and runs at\n67 frames per second on a 1080Ti GPU. Experiments show that MODNet outperforms\nprior trimap-free methods by a large margin on both Adobe Matting Dataset and a\ncarefully designed photographic portrait matting (PPM-100) benchmark proposed\nby us. Further, MODNet achieves remarkable results on daily photos and videos.\nOur code and models are available at https://github.com/ZHKKKe/MODNet, and the\nPPM-100 benchmark is released at https://github.com/ZHKKKe/PPM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ke_Z/0/1/0/all/0/1\">Zhanghan Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiayu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaican Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiong Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_R/0/1/0/all/0/1\">Rynson W.H. Lau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Globetrotter: Connecting Languages by Connecting Images. (arXiv:2012.04631v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04631","description":"<p>Machine translation between many languages at once is highly challenging,\nsince training with ground truth requires supervision between all language\npairs, which is difficult to obtain. Our key insight is that, while languages\nmay vary drastically, the underlying visual appearance of the world remains\nconsistent. We introduce a method that uses visual observations to bridge the\ngap between languages, rather than relying on parallel corpora or topological\nproperties of the representations. We train a model that aligns segments of\ntext from different languages if and only if the images associated with them\nare similar and each image in turn is well-aligned with its textual\ndescription. We train our model from scratch on a new dataset of text in over\nfifty languages with accompanying images. Experiments show that our method\noutperforms previous work on unsupervised word and sentence translation using\nretrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suris_D/0/1/0/all/0/1\">D&#xed;dac Sur&#xed;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Epstein_D/0/1/0/all/0/1\">Dave Epstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrick_C/0/1/0/all/0/1\">Carl Vondrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responsible Disclosure of Generative Models Using Scalable Fingerprinting. (arXiv:2012.08726v5 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2012.08726","description":"<p>Over the past years, deep generative models have achieved a new level of\nperformance. Generated data has become difficult, if not impossible, to be\ndistinguished from real data. While there are plenty of use cases that benefit\nfrom this technology, there are also strong concerns on how this new technology\ncan be misused to generate deep fakes and enable misinformation at scale.\nUnfortunately, current deep fake detection methods are not sustainable, as the\ngap between real and fake continues to close. In contrast, our work enables a\nresponsible disclosure of such state-of-the-art generative models, that allows\nmodel inventors to fingerprint their models, so that the generated samples\ncontaining a fingerprint can be accurately detected and attributed to a source.\nOur technique achieves this by an efficient and scalable ad-hoc generation of a\nlarge population of models with distinct fingerprints. Our recommended\noperation point uses a 128-bit fingerprint which in principle results in more\nthan $10^{38}$ identifiable models. Experiments show that our method fulfills\nkey properties of a fingerprinting mechanism and achieves effectiveness in deep\nfake detection and attribution. Code and models are available at\nhttps://github.com/ningyu1991/ScalableGANFingerprints .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skripniuk_V/0/1/0/all/0/1\">Vladislav Skripniuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dingfan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple Mutual Information based Registration Method for Thermal-Optical Image Pairs applied on a Novel Dataset. (arXiv:2101.06910v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2101.06910","description":"<p>While thermal optical registered datasets are becoming widely available, most\nof these works are based on image pairs which are pre-registered. However,\nthermal imagers where these images are registered by default are quite\nexpensive. We present in this work, a thermal image registration technique\nwhich is computationally lightweight, and can be employed regardless of the\nresolution of the images captured. We use 2 different thermal imagers to create\na completely new database and introduce it as a part of this work as well. The\nimages captured are based on 5 different classes and encompass subjects like\nthe Prayagraj Kumbh Mela, one of the largest public fairs in the world,\ncaptured over a period of 2 years.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Goswami_S/0/1/0/all/0/1\">Suranjan Goswami</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Singh_S/0/1/0/all/0/1\">Satish Kumar Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adversarial Neural Networks for Domain Generalization: When It Works and How to Improve. (arXiv:2102.03924v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.03924","description":"<p>Theoretically, domain adaptation is a well-researched problem. Further, this\ntheory has been well-used in practice. In particular, we note the bound on\ntarget error given by Ben-David et al. (2010) and the well-known\ndomain-aligning algorithm based on this work using Domain Adversarial Neural\nNetworks (DANN) presented by Ganin and Lempitsky (2015). Recently, multiple\nvariants of DANN have been proposed for the related problem of domain\ngeneralization, but without much discussion of the original motivating bound.\nIn this paper, we investigate the validity of DANN in domain generalization\nfrom this perspective. We investigate conditions under which application of\nDANN makes sense and further consider DANN as a dynamic process during\ntraining. Our investigation suggests that the application of DANN to domain\ngeneralization may not be as straightforward as it seems. To address this, we\ndesign an algorithmic extension to DANN in the domain generalization case. Our\nexperimentation validates both theory and algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sicilia_A/0/1/0/all/0/1\">Anthony Sicilia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xingchen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Seong Jae Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ImageNet as a Representative Basis for Deriving Generally Effective CNN Architectures. (arXiv:2103.09108v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.09108","description":"<p>We investigate and improve the representativeness of ImageNet as a basis for\nderiving generally effective convolutional neural network (CNN) architectures\nthat perform well on a diverse set of datasets and application domains. To this\nend, we conduct an extensive empirical study for which we train 500 CNN\narchitectures, sampled from the broad AnyNetX design space, on ImageNet as well\nas 8 other image classification datasets. We observe that the performances of\nthe architectures are highly dataset-dependent. Some datasets even exhibit a\nnegative error correlation with ImageNet across all architectures. We show how\nto significantly increase these correlations by utilizing ImageNet subsets\nrestricted to fewer classes. We also identify the cumulative width across\nlayers as well as the total depth of the network as the most sensitive design\nparameter with respect to changing datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tuggener_L/0/1/0/all/0/1\">Lukas Tuggener</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidhuber_J/0/1/0/all/0/1\">J&#xfc;rgen Schmidhuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stadelmann_T/0/1/0/all/0/1\">Thilo Stadelmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collapsible Linear Blocks for Super-Efficient Super Resolution. (arXiv:2103.09404v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2103.09404","description":"<p>With the advent of smart devices that support 4K and 8K resolution, Single\nImage Super Resolution (SISR) has become an important computer vision problem.\nHowever, most super resolution deep networks are computationally very\nexpensive. In this paper, we propose Super-Efficient Super Resolution (SESR)\nnetworks that establish a new state-of-the-art for efficient super resolution.\nOur approach is based on linear overparameterization of CNNs and creates an\nefficient model architecture for SISR. With theoretical analysis, we uncover\nthe limitations of existing overparameterization methods and show how the\nproposed method alleviates them. Detailed experiments across six benchmark\ndatasets demonstrate that SESR achieves similar or better image quality than\nstate-of-the-art models while requiring 2x to 330x fewer Multiply-Accumulate\n(MAC) operations. As a result, SESR can be used on constrained hardware to\nperform x2 (1080p to 4K) and x4 (1080p to 8K) SISR. Towards this, we estimate\nhardware performance numbers for a commercial Arm mobile-Neural Processing Unit\n(NPU) for 1080p to 4K (x2) and 1080p to 8K (x4) SISR. Our results highlight the\nchallenges faced by super resolution on AI accelerators and demonstrate that\nSESR is significantly faster (e.g., 6x-8x higher FPS) than existing models on\nmobile-NPU. Finally, SESR outperforms prior models by 1.5x-2x in latency on Arm\nCPU and GPU when deployed on a real mobile device. The code for this work is\navailable at https://github.com/ARM-software/sesr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bhardwaj_K/0/1/0/all/0/1\">Kartikeya Bhardwaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milosavljevic_M/0/1/0/all/0/1\">Milos Milosavljevic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeil_L/0/1/0/all/0/1\">Liam O&#x27;Neil</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gope_D/0/1/0/all/0/1\">Dibakar Gope</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Matas_R/0/1/0/all/0/1\">Ramon Matas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chalfin_A/0/1/0/all/0/1\">Alex Chalfin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suda_N/0/1/0/all/0/1\">Naveen Suda</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_L/0/1/0/all/0/1\">Lingchuan Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loh_D/0/1/0/all/0/1\">Danny Loh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation. (arXiv:2103.10702v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.10702","description":"<p>Text-based video segmentation is a challenging task that segments out the\nnatural language referred objects in videos. It essentially requires semantic\ncomprehension and fine-grained video understanding. Existing methods introduce\nlanguage representation into segmentation models in a bottom-up manner, which\nmerely conducts vision-language interaction within local receptive fields of\nConvNets. We argue that such interaction is not fulfilled since the model can\nbarely construct region-level relationships given partial observations, which\nis contrary to the description logic of natural language/referring expressions.\nIn fact, people usually describe a target object using relations with other\nobjects, which may not be easily understood without seeing the whole video. To\naddress the issue, we introduce a novel top-down approach by imitating how we\nhuman segment an object with the language guidance. We first figure out all\ncandidate objects in videos and then choose the refereed one by parsing\nrelations among those high-level objects. Three kinds of object-level relations\nare investigated for precise relationship understanding, i.e., positional\nrelation, text-guided semantic relation, and temporal relation. Extensive\nexperiments on A2D Sentences and J-HMDB Sentences show our method outperforms\nstate-of-the-art methods by a large margin. Qualitative results also show our\nresults are more explainable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yawei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Contrastive Loss and Attention for GANs. (arXiv:2103.16748v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.16748","description":"<p>Generative Adversarial Networks (GANs) produce impressive results on\nunconditional image generation when powered with large-scale image datasets.\nYet generated images are still easy to spot especially on datasets with high\nvariance (e.g. bedroom, church). In this paper, we propose various improvements\nto further push the boundaries in image generation. Specifically, we propose a\nnovel dual contrastive loss and show that, with this loss, discriminator learns\nmore generalized and distinguishable representations to incentivize generation.\nIn addition, we revisit attention and extensively experiment with different\nattention blocks in the generator. We find attention to be still an important\nmodule for successful image generation even though it was not used in the\nrecent state-of-the-art models. Lastly, we study different attention\narchitectures in the discriminator, and propose a reference attention\nmechanism. By combining the strengths of these remedies, we improve the\ncompelling state-of-the-art Fr\\'{e}chet Inception Distance (FID) by at least\n17.5% on several benchmark datasets. We obtain even more significant\nimprovements on compositional synthetic scenes (up to 47.5% in FID). Code and\nmodels are available at https://github.com/ningyu1991/AttentionDualContrastGAN .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Ning Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guilin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dundar_A/0/1/0/all/0/1\">Aysegul Dundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_A/0/1/0/all/0/1\">Andrew Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1\">Mario Fritz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Adversarial Registration for Improved Conditional Deformable Templates. (arXiv:2105.04349v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04349","description":"<p>Deformable templates are essential to large-scale medical image registration,\nsegmentation, and population analysis. Current conventional and deep\nnetwork-based methods for template construction use only regularized\nregistration objectives and often yield templates with blurry and/or\nanatomically implausible appearance, confounding downstream biomedical\ninterpretation. We reformulate deformable registration and conditional template\nestimation as an adversarial game wherein we encourage realism in the moved\ntemplates with a generative adversarial registration framework conditioned on\nflexible image covariates. The resulting templates exhibit significant gain in\nspecificity to attributes such as age and disease, better fit underlying\ngroup-wise spatiotemporal trends, and achieve improved sharpness and\ncentrality. These improvements enable more accurate population modeling with\ndiverse covariates for standardized downstream analyses and easier anatomical\ndelineation for structures of interest.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_N/0/1/0/all/0/1\">Neel Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1\">Mengwei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerig_G/0/1/0/all/0/1\">Guido Gerig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.01085","description":"<p>A dataset is a shred of crucial evidence to describe a task. However, each\ndata point in the dataset does not have the same potential, as some of the data\npoints can be more representative or informative than others. This unequal\nimportance among the data points may have a large impact in rehearsal-based\ncontinual learning, where we store a subset of the training examples (coreset)\nto be replayed later to alleviate catastrophic forgetting. In continual\nlearning, the quality of the samples stored in the coreset directly affects the\nmodel's effectiveness and efficiency. The coreset selection problem becomes\neven more important under realistic settings, such as imbalanced continual\nlearning or noisy data scenarios. To tackle this problem, we propose Online\nCoreset Selection (OCS), a simple yet effective method that selects the most\nrepresentative and informative coreset at each iteration and trains them in an\nonline manner. Our proposed method maximizes the model's adaptation to a\ncurrent dataset while selecting high-affinity samples to past tasks, which\ndirectly inhibits catastrophic forgetting. We validate the effectiveness of our\ncoreset selection mechanism over various standard, imbalanced, and noisy\ndatasets against strong continual learning baselines, demonstrating that it\nimproves task adaptation and prevents catastrophic forgetting in a\nsample-efficient manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jaehong Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1\">Divyam Madaan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Affiliate: Mutual Centralized Learning for Few-shot Classification. (arXiv:2106.05517v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.05517","description":"<p>Few-shot learning (FSL) aims to learn a classifier that can be easily adapted\nto accommodate new tasks not seen during training, given only a few examples.\nTo handle the limited-data problem in few-shot regimes, recent methods tend to\ncollectively use a set of local features to densely represent an image instead\nof using a mixed global feature. They generally explore a unidirectional\nquery-to-support paradigm in FSL, e.g., find the nearest/optimal support\nfeature for each query feature and aggregate these local matches for a joint\nclassification. In this paper, we propose a new method Mutual Centralized\nLearning (MCL) to fully affiliate the two disjoint sets of dense features in a\nbidirectional paradigm. We associate each local feature with a particle that\ncan bidirectionally random walk in a discrete feature space by the\naffiliations. To estimate the class probability, we propose the features'\naccessibility that measures the expected number of visits to the support\nfeatures of that class in a Markov process. We relate our method to learning a\ncentrality on an affiliation network and demonstrate its capability to be\nplugged in existing methods by highlighting centralized local features.\nExperiments show that our method achieves the state-of-the-art on both\nminiImageNet and tieredImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weifeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_C/0/1/0/all/0/1\">Chao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1\">Tu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-Based Quantification of Epistemic Uncertainty for Deep Object Detectors. (arXiv:2107.04517v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.04517","description":"<p>The vast majority of uncertainty quantification methods for deep object\ndetectors such as variational inference are based on the network output. Here,\nwe study gradient-based epistemic uncertainty metrics for deep object detectors\nto obtain reliable confidence estimates. We show that they contain predictive\ninformation and that they capture information orthogonal to that of common,\noutput-based uncertainty estimation methods like Monte-Carlo dropout and deep\nensembles. To this end, we use meta classification and meta regression to\nproduce confidence estimates using gradient metrics and other baselines for\nuncertainty quantification which are in principle applicable to any object\ndetection architecture. Specifically, we employ false positive detection and\nprediction of localization quality to investigate uncertainty content of our\nmetrics and compute the calibration errors of meta classifiers. Moreover, we\nuse them as a post-processing filter mechanism to the object detection pipeline\nand compare object detection performance. Our results show that gradient-based\nuncertainty is itself on par with output-based methods across different\ndetectors and datasets. More significantly, combined meta classifiers based on\ngradient and output-based metrics outperform the standalone models. Based on\nthis result, we conclude that gradient uncertainty adds orthogonal information\nto output-based methods. This suggests that variational inference may be\nsupplemented by gradient-based uncertainty to obtain improved confidence\nmeasures, contributing to down-stream applications of deep object detectors and\nimproving their probabilistic reliability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riedlinger_T/0/1/0/all/0/1\">Tobias Riedlinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottmann_M/0/1/0/all/0/1\">Matthias Rottmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schubert_M/0/1/0/all/0/1\">Marius Schubert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gottschalk_H/0/1/0/all/0/1\">Hanno Gottschalk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HDMapNet: An Online HD Map Construction and Evaluation Framework. (arXiv:2107.06307v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.06307","description":"<p>Constructing HD semantic maps is a central component of autonomous driving.\nHowever, traditional pipelines require a vast amount of human efforts and\nresources in annotating and maintaining the semantics in the map, which limits\nits scalability. In this paper, we introduce the problem of HD semantic map\nlearning, which dynamically constructs the local semantics based on onboard\nsensor observations. Meanwhile, we introduce a semantic map learning method,\ndubbed HDMapNet. HDMapNet encodes image features from surrounding cameras\nand/or point clouds from LiDAR, and predicts vectorized map elements in the\nbird's-eye view. We benchmark HDMapNet on nuScenes dataset and show that in all\nsettings, it performs better than baseline methods. Of note, our camera-LiDAR\nfusion-based HDMapNet outperforms existing methods by more than 50% in all\nmetrics. In addition, we develop semantic-level and instance-level metrics to\nevaluate the map learning performance. Finally, we showcase our method is\ncapable of predicting a locally consistent map. By introducing the method and\nmetrics, we invite the community to study this novel map learning problem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CycleMLP: A MLP-like Architecture for Dense Prediction. (arXiv:2107.10224v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10224","description":"<p>This paper presents a simple MLP-like architecture, CycleMLP, which is a\nversatile backbone for visual recognition and dense predictions. As compared to\nmodern MLP architectures, e.g., MLP-Mixer, ResMLP, and gMLP, whose\narchitectures are correlated to image size and thus are infeasible in object\ndetection and segmentation, CycleMLP has two advantages compared to modern\napproaches. (1) It can cope with various image sizes. (2) It achieves linear\ncomputational complexity to image size by using local windows. In contrast,\nprevious MLPs have $O(N^2)$ computations due to fully spatial connections. We\nbuild a family of models which surpass existing MLPs and even state-of-the-art\nTransformer-based models, e.g., Swin Transformer, while using fewer parameters\nand FLOPs. We expand the MLP-like models' applicability, making them a\nversatile backbone for dense prediction tasks. CycleMLP achieves competitive\nresults on object detection, instance segmentation, and semantic segmentation.\nIn particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mIoU on ADE20K\ndataset with fewer FLOPs. Moreover, CycleMLP also shows excellent zero-shot\nrobustness on ImageNet-C dataset. Code is available at\nhttps://github.com/ShoufaChen/CycleMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shoufa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chongjian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1\">Ding Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cervical Optical Coherence Tomography Image Classification Based on Contrastive Self-Supervised Texture Learning. (arXiv:2108.05081v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2108.05081","description":"<p>Background: Cervical cancer seriously affects the health of the female\nreproductive system. Optical coherence tomography (OCT) emerged as a\nnon-invasive, high-resolution imaging technology for cervical disease\ndetection. However, OCT image annotation is knowledge-intensive and\ntime-consuming, which impedes the training process of deep-learning-based\nclassification models. Purpose: This study aims to develop a computer-aided\ndiagnosis (CADx) approach to classifying in-vivo cervical OCT images based on\nself-supervised learning. Methods: In addition to high-level semantic features\nextracted by a convolutional neural network (CNN), the proposed CADx approach\nleverages unlabeled cervical OCT images' texture features learned by\ncontrastive texture learning. We conducted ten-fold cross-validation on the OCT\nimage dataset from a multi-center clinical study on 733 patients from China.\nResults: In a binary classification task for detecting high-risk diseases,\nincluding high-grade squamous intraepithelial lesion and cervical cancer, our\nmethod achieved an area-under-the-curve value of 0.9798 plus or minus 0.0157\nwith a sensitivity of 91.17 plus or minus 4.99% and a specificity of 93.96 plus\nor minus 4.72% for OCT image patches; also, it outperformed two out of four\nmedical experts on the test set. Furthermore, our method achieved a 91.53%\nsensitivity and 97.37% specificity on an external validation dataset containing\n287 3D OCT volumes from 118 Chinese patients in a new hospital using a\ncross-shaped threshold voting strategy. Conclusions: The proposed\ncontrastive-learning-based CADx method outperformed the end-to-end CNN models\nand provided better interpretability based on texture features, which holds\ngreat potential to be used in the clinical protocol of \"see-and-treat.\"\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1\">Kaiyi Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Qingbin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1\">Yutao Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PGTRNet: Two-phase Weakly Supervised Object Detection with Pseudo Ground Truth Refinement. (arXiv:2108.11439v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11439","description":"<p>Current state-of-the-art weakly supervised object detection (WSOD) studies\nmainly follow a two-stage training strategy which integrates a fully supervised\ndetector (FSD) with a pure WSOD model. There are two main problems hindering\nthe performance of the two-phase WSOD approaches, i.e., insufficient learning\nproblem and strict reliance between the FSD and the pseudo ground truth (PGT)\ngenerated by the WSOD model. This paper proposes pseudo ground truth refinement\nnetwork (PGTRNet), a simple yet effective method without introducing any extra\nlearnable parameters, to cope with these problems. PGTRNet utilizes multiple\nbounding boxes to establish the PGT, mitigating the insufficient learning\nproblem. Besides, we propose a novel online PGT refinement approach to steadily\nimprove the quality of PGT by fully taking advantage of the power of FSD during\nthe second-phase training, decoupling the first and second-phase models.\nElaborate experiments are conducted on the PASCAL VOC 2007 benchmark to verify\nthe effectiveness of our methods. Experimental results demonstrate that PGTRNet\nboosts the backbone model by 2.1% mAP and achieves the state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hefeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaohan Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers. (arXiv:2109.03814v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03814","description":"<p>Panoptic segmentation involves a combination of joint semantic segmentation\nand instance segmentation, where image contents are divided into two types:\nthings and stuff. We present Panoptic SegFormer, a general framework for\npanoptic segmentation with transformers. It contains three innovative\ncomponents: an efficient deeply-supervised mask decoder, a query decoupling\nstrategy, and an improved post-processing method. We also use Deformable DETR\nto efficiently process multi-scale features, which is a fast and efficient\nversion of DETR. Specifically, we supervise the attention modules in the mask\ndecoder in a layer-wise manner. This deep supervision strategy lets the\nattention modules quickly focus on meaningful semantic regions. It improves\nperformance and reduces the number of required training epochs by half compared\nto Deformable DETR. Our query decoupling strategy decouples the\nresponsibilities of the query set and avoids mutual interference between things\nand stuff. In addition, our post-processing strategy improves performance\nwithout additional costs by jointly considering classification and segmentation\nqualities to resolve conflicting mask overlaps. Our approach increases the\naccuracy 6.2\\% PQ over the baseline DETR model. Panoptic SegFormer achieves\nstate-of-the-art results on COCO test-dev with 56.2\\% PQ. It also shows\nstronger zero-shot robustness over existing methods. The code is released at\n\\url{https://github.com/zhiqi-li/Panoptic-SegFormer}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1\">Enze Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1\">Jose M. Alvarez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tong Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. (arXiv:2109.04275v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.04275","description":"<p>Despite the potential of multi-modal pre-training to learn highly\ndiscriminative feature representations from complementary data modalities,\ncurrent progress is being slowed by the lack of large-scale modality-diverse\ndatasets. By leveraging the natural suitability of E-commerce, where different\nmodalities capture complementary semantic information, we contribute a\nlarge-scale multi-modal pre-training dataset M5Product. The dataset comprises 5\nmodalities (image, text, table, video, and audio), covers over 6,000 categories\nand 5,000 attributes, and is 500 larger than the largest publicly available\ndataset with a similar number of modalities. Furthermore, M5Product contains\nincomplete modality pairs and noise while also having a long-tailed\ndistribution, resembling most real-world problems. We further propose\nSelf-harmonized ContrAstive LEarning (SCALE), a novel pretraining framework\nthat integrates the different modalities into a unified model through an\nadaptive feature fusion mechanism, where the importance of each modality is\nlearned directly from the modality embeddings and impacts the inter-modality\ncontrastive learning and masked tasks within a multi-modal transformer model.\nWe evaluate the current multi-modal pre-training state-of-the-art approaches\nand benchmark their ability to learn from unlabeled data when faced with the\nlarge number of modalities in the M5Product dataset. We conduct extensive\nexperiments on four downstream tasks and demonstrate the superiority of our\nSCALE model, providing insights into the importance of dataset scale and\ndiversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xunlin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yangxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1\">Michael C. Kampffmeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Minlong Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaowei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METEOR:A Dense, Heterogeneous, and Unstructured Traffic Dataset With Rare Behaviors. (arXiv:2109.07648v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07648","description":"<p>We present a new traffic dataset, METEOR, which captures traffic patterns and\nmulti-agent driving behaviors in unstructured scenarios. METEOR consists of\nmore than 1000 one-minute videos, over 2 million annotated frames with bounding\nboxes and GPS trajectories for 16 unique agent categories, and more than 13\nmillion bounding boxes for traffic agents. METEOR is a dataset for rare and\ninteresting, multi-agent driving behaviors that are grouped into traffic\nviolations, atypical interactions, and diverse scenarios. Every video in METEOR\nis tagged using a diverse range of factors corresponding to weather, time of\nthe day, road conditions, and traffic density. We use METEOR to benchmark\nperception methods for object detection and multi-agent behavior prediction.\nOur key finding is that state-of-the-art models for object detection and\nbehavior prediction, which otherwise succeed on existing datasets such as\nWaymo, fail on the METEOR dataset. METEOR marks the first step towards the\ndevelopment of more sophisticated perception models for dense, heterogeneous,\nand unstructured scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1\">Rohan Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xijun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_M/0/1/0/all/0/1\">Mridul Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kala_R/0/1/0/all/0/1\">Rahul Kala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palugulla_R/0/1/0/all/0/1\">Rishitha Palugulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_C/0/1/0/all/0/1\">Chandrababu Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Alok Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Dinesh Manocha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image-Based CLIP-Guided Essence Transfer. (arXiv:2110.12427v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12427","description":"<p>We make the distinction between (i) style transfer, in which a source image\nis manipulated to match the textures and colors of a target image, and (ii)\nessence transfer, in which one edits the source image to include high-level\nsemantic attributes from the target. Crucially, the semantic attributes that\nconstitute the essence of an image may differ from image to image. Our blending\noperator combines the powerful StyleGAN generator and the semantic encoder of\nCLIP in a novel way that is simultaneously additive in both latent spaces,\nresulting in a mechanism that guarantees both identity preservation and\nhigh-level feature transfer without relying on a facial recognition network. We\npresent two variants of our method. The first is based on optimization, while\nthe second fine-tunes an existing inversion encoder to perform essence\nextraction. Through extensive experiments, we demonstrate the superiority of\nour methods for essence transfer over existing methods for style transfer,\ndomain adaptation, and text-based semantic editing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chefer_H/0/1/0/all/0/1\">Hila Chefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benaim_S/0/1/0/all/0/1\">Sagie Benaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paiss_R/0/1/0/all/0/1\">Roni Paiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers. (arXiv:2111.02387v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02387","description":"<p>Vision-and-language (VL) pre-training has proven to be highly effective on\nvarious VL downstream tasks. While recent work has shown that fully\ntransformer-based VL models can be more efficient than previous\nregion-feature-based methods, their performance on downstream tasks often\ndegrades significantly. In this paper, we present METER, a Multimodal\nEnd-to-end TransformER framework, through which we investigate how to design\nand pre-train a fully transformer-based VL model in an end-to-end manner.\nSpecifically, we dissect the model designs along multiple dimensions: vision\nencoders (e.g., CLIP-ViT, Swin transformer), text encoders (e.g., RoBERTa,\nDeBERTa), multimodal fusion module (e.g., merged attention vs. co-attention),\narchitectural design (e.g., encoder-only vs. encoder-decoder), and pre-training\nobjectives (e.g., masked image modeling). We conduct comprehensive experiments\nand provide insights on how to train a performant VL transformer. METER\nachieves an accuracy of 77.64% on the VQAv2 test-std set using only 4M images\nfor pre-training, surpassing the state-of-the-art region-feature-based model by\n1.04%, and outperforming the previous best fully transformer-based model by\n1.6%. Notably, when further scaled up, our best VQA model achieves an accuracy\nof 80.54%. Code and pre-trained models are released at\nhttps://github.com/zdou0830/METER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lijuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TimeMatch: Unsupervised Cross-Region Adaptation by Temporal Shift Estimation. (arXiv:2111.02682v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.02682","description":"<p>The recent developments of deep learning models that capture the complex\ntemporal patterns of crop phenology have greatly advanced crop classification\nof Satellite Image Time Series (SITS). However, when applied to target regions\nspatially different from the training region, these models perform poorly\nwithout any target labels due to the temporal shift of crop phenology between\nregions. To address this unsupervised cross-region adaptation setting, existing\nmethods learn domain-invariant features without any target supervision, but not\nthe temporal shift itself. As a consequence, these techniques provide only\nlimited benefits for SITS. In this paper, we propose TimeMatch, a new\nunsupervised domain adaptation method for SITS that directly accounts for the\ntemporal shift. TimeMatch consists of two components: 1) temporal shift\nestimation, which estimates the temporal shift of the unlabeled target region\nwith a source-trained model, and 2) TimeMatch learning, which combines temporal\nshift estimation with semi-supervised learning to adapt a classifier to an\nunlabeled target region. We also introduce an open-access dataset for\ncross-region adaptation with SITS from four different regions in Europe. On\nthis dataset, we demonstrate that TimeMatch outperforms all competing methods\nby 11% in F1-score across five different adaptation scenarios, setting a new\nstate-of-the-art for cross-region adaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nyborg_J/0/1/0/all/0/1\">Joachim Nyborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelletier_C/0/1/0/all/0/1\">Charlotte Pelletier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1\">S&#xe9;bastien Lef&#xe8;vre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assent_I/0/1/0/all/0/1\">Ira Assent</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Depth from Focus with Differential Focus Volume. (arXiv:2112.01712v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01712","description":"<p>Depth-from-focus (DFF) is a technique that infers depth using the focus\nchange of a camera. In this work, we propose a convolutional neural network\n(CNN) to find the best-focused pixels in a focal stack and infer depth from the\nfocus estimation. The key innovation of the network is the novel deep\ndifferential focus volume (DFV). By computing the first-order derivative with\nthe stacked features over different focal distances, DFV is able to capture\nboth the focus and context information for focus analysis. Besides, we also\nintroduce a probability regression mechanism for focus estimation to handle\nsparsely sampled focal stacks and provide uncertainty estimation to the final\nprediction. Comprehensive experiments demonstrate that the proposed model\nachieves state-of-the-art performance on multiple datasets with good\ngeneralizability and fast speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fengting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaolei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zihan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Conditional Point Diffusion-Refinement Paradigm for 3D Point Cloud Completion. (arXiv:2112.03530v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03530","description":"<p>3D point cloud is an important 3D representation for capturing real world 3D\nobjects. However, real-scanned 3D point clouds are often incomplete, and it is\nimportant to recover complete point clouds for downstream applications. Most\nexisting point cloud completion methods use Chamfer Distance (CD) loss for\ntraining. The CD loss estimates correspondences between two point clouds by\nsearching nearest neighbors, which does not capture the overall point density\ndistribution on the generated shape, and therefore likely leads to non-uniform\npoint cloud generation. To tackle this problem, we propose a novel Point\nDiffusion-Refinement (PDR) paradigm for point cloud completion. PDR consists of\na Conditional Generation Network (CGNet) and a ReFinement Network (RFNet). The\nCGNet uses a conditional generative model called the denoising diffusion\nprobabilistic model (DDPM) to generate a coarse completion conditioned on the\npartial observation. DDPM establishes a one-to-one pointwise mapping between\nthe generated point cloud and the uniform ground truth, and then optimizes the\nmean squared error loss to realize uniform generation. The RFNet refines the\ncoarse output of the CGNet and further improves quality of the completed point\ncloud. Furthermore, we develop a novel dual-path architecture for both\nnetworks. The architecture can (1) effectively and efficiently extract\nmulti-level features from partially observed point clouds to guide completion,\nand (2) accurately manipulate spatial locations of 3D points to obtain smooth\nsurfaces and sharp details. Extensive experimental results on various benchmark\ndatasets show that our PDR paradigm outperforms previous state-of-the-art\nmethods for point cloud completion. Remarkably, with the help of the RFNet, we\ncan accelerate the iterative generation process of the DDPM by up to 50 times\nwithout much performance drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhaoyang Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1\">Zhifeng Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xudong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Liang Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dahua Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"All You Need is RAW: Defending Against Adversarial Attacks with Camera Image Pipelines. (arXiv:2112.09219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09219","description":"<p>Existing neural networks for computer vision tasks are vulnerable to\nadversarial attacks: adding imperceptible perturbations to the input images can\nfool these methods to make a false prediction on an image that was correctly\npredicted without the perturbation. Various defense methods have proposed\nimage-to-image mapping methods, either including these perturbations in the\ntraining process or removing them in a preprocessing denoising step. In doing\nso, existing methods often ignore that the natural RGB images in today's\ndatasets are not captured but, in fact, recovered from RAW color filter array\ncaptures that are subject to various degradations in the capture. In this work,\nwe exploit this RAW data distribution as an empirical prior for adversarial\ndefense. Specifically, we proposed a model-agnostic adversarial defensive\nmethod, which maps the input RGB images to Bayer RAW space and back to output\nRGB using a learned camera image signal processing (ISP) pipeline to eliminate\npotential adversarial patterns. The proposed method acts as an off-the-shelf\npreprocessing module and, unlike model-specific adversarial training methods,\ndoes not require adversarial images to train. As a result, the method\ngeneralizes to unseen tasks without additional retraining. Experiments on\nlarge-scale datasets (e.g., ImageNet, COCO) for different vision tasks (e.g.,\nclassification, semantic segmentation, object detection) validate that the\nmethod significantly outperforms existing methods across task domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1\">Bo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heide_F/0/1/0/all/0/1\">Felix Heide</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images. (arXiv:2112.11325v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11325","description":"<p>We propose iSegFormer, a memory-efficient transformer that combines a Swin\ntransformer with a lightweight multilayer perceptron (MLP) decoder. With the\nefficient Swin transformer blocks for hierarchical self-attention and the\nsimple MLP decoder for aggregating both local and global attention, iSegFormer\nlearns powerful representations while achieving high computational\nefficiencies. Specifically, we apply iSegFormer to interactive 3D medical image\nsegmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenlin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yining Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1\">Marc Niethammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAMCNet for Spatial-configuration-based Classification: A Summary of Results. (arXiv:2112.12219v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12219","description":"<p>The goal of spatial-configuration-based classification is to build a\nclassifier to distinguish two classes (e.g., responder, non-responder) based on\nthe spatial arrangements (e.g., spatial interactions between different point\ncategories) given multi-category point data from two classes. This problem is\nimportant for generating hypotheses in medical pathology towards discovering\nnew immunotherapies for cancer treatment as well as for other applications in\nbiomedical research and microbial ecology. This problem is challenging due to\nan exponential number of category subsets which may vary in the strength of\nspatial interactions. Most prior efforts on using human selected spatial\nassociation measures may not be sufficient for capturing the relevant (e.g.,\nsurrounded by) spatial interactions which may be of biological significance. In\naddition, the related deep neural networks are limited to category pairs and do\nnot explore larger subsets of point categories. To overcome these limitations,\nwe propose a Spatial-interaction Aware Multi-Category deep neural Network\n(SAMCNet) architecture and contribute novel local reference frame\ncharacterization and point pair prioritization layers for\nspatial-configuration-based classification. Extensive experimental results on\nmultiple cancer datasets show that the proposed architecture provides higher\nprediction accuracy over baseline methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farhadloo_M/0/1/0/all/0/1\">Majid Farhadloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molnar_C/0/1/0/all/0/1\">Carl Molnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gaoxiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shashi Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maus_R/0/1/0/all/0/1\">Rachel L. Maus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovic_S/0/1/0/all/0/1\">Svetomir N. Markovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_R/0/1/0/all/0/1\">Raymond Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontovich_A/0/1/0/all/0/1\">Alexey Leontovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Disturbance-Free Visual Mobile Manipulation. (arXiv:2112.12612v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.12612","description":"<p>Deep reinforcement learning has shown promising results on an abundance of\nrobotic tasks in simulation, including visual navigation and manipulation.\nPrior work generally aims to build embodied agents that solve their assigned\ntasks as quickly as possible, while largely ignoring the problems caused by\ncollision with objects during interaction. This lack of prioritization is\nunderstandable: there is no inherent cost in breaking virtual objects. As a\nresult, \"well-trained\" agents frequently collide with objects before achieving\ntheir primary goals, a behavior that would be catastrophic in the real world.\nIn this paper, we study the problem of training agents to complete the task of\nvisual mobile manipulation in the ManipulaTHOR environment while avoiding\nunnecessary collision (disturbance) with objects. We formulate disturbance\navoidance as a penalty term in the reward function, but find that directly\ntraining with such penalized rewards often results in agents being unable to\nescape poor local optima. Instead, we propose a two-stage training curriculum\nwhere an agent is first allowed to freely explore and build basic competencies\nwithout penalization, after which a disturbance penalty is introduced to refine\nthe agent's behavior. Results on testing scenes show that our curriculum not\nonly avoids these poor local optima, but also leads to 10% absolute gains in\nsuccess rate without disturbance, compared to our state-of-the-art baselines.\nMoreover, we propose a novel disturbance-prediction auxiliary task accelerating\nlearning and further improving success rates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_T/0/1/0/all/0/1\">Tianwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1\">Luca Weihs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvador_J/0/1/0/all/0/1\">Jordi Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CheXstray: Real-time Multi-Modal Data Concordance for Drift Detection in Medical Imaging AI. (arXiv:2202.02833v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.02833","description":"<p>Clinical Artificial lntelligence (AI) applications are rapidly expanding\nworldwide, and have the potential to impact to all areas of medical practice.\nMedical imaging applications constitute a vast majority of approved clinical AI\napplications. Though healthcare systems are eager to adopt AI solutions a\nfundamental question remains: \\textit{what happens after the AI model goes into\nproduction?} We use the CheXpert and PadChest public datasets to build and test\na medical imaging AI drift monitoring workflow to track data and model drift\nwithout contemporaneous ground truth. We simulate drift in multiple experiments\nto compare model performance with our novel multi-modal drift metric, which\nuses DICOM metadata, image appearance representation from a variational\nautoencoder (VAE), and model output probabilities as input. Through\nexperimentation, we demonstrate a strong proxy for ground truth performance\nusing unsupervised distributional shifts in relevant metadata, predicted\nprobabilities, and VAE latent representation. Our key contributions include (1)\nproof-of-concept for medical imaging drift detection that includes the use of\nVAE and domain specific statistical methods, (2) a multi-modal methodology to\nmeasure and unify drift metrics, (3) new insights into the challenges and\nsolutions to observe deployed medical imaging AI, and (4) creation of\nopen-source tools that enable others to easily run their own workflows and\nscenarios. This work has important implications. It addresses the concerning\ntranslation gap found in continuous medical imaging AI model monitoring common\nin dynamic healthcare environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Soin_A/0/1/0/all/0/1\">Arjun Soin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merkow_J/0/1/0/all/0/1\">Jameson Merkow</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1\">Jin Long</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cohen_J/0/1/0/all/0/1\">Joseph Paul Cohen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Saligrama_S/0/1/0/all/0/1\">Smitha Saligrama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaiser_S/0/1/0/all/0/1\">Stephen Kaiser</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Borg_S/0/1/0/all/0/1\">Steven Borg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarapov_I/0/1/0/all/0/1\">Ivan Tarapov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P Lungren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A multiscale spatiotemporal approach for smallholder irrigation detection. (arXiv:2202.04239v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.04239","description":"<p>In presenting an irrigation detection methodology that leverages multiscale\nsatellite imagery of vegetation abundance, this paper introduces a process to\nsupplement limited ground-collected labels and ensure classifier applicability\nin an area of interest. Spatiotemporal analysis of MODIS 250m Enhanced\nVegetation Index (EVI) timeseries characterizes native vegetation phenologies\nat regional scale to provide the basis for a continuous phenology map that\nguides supplementary label collection over irrigated and non-irrigated\nagriculture. Subsequently, validated dry season greening and senescence cycles\nobserved in 10m Sentinel-2 imagery are used to train a suite of classifiers for\nautomated detection of potential smallholder irrigation. Strategies to improve\nmodel robustness are demonstrated, including a method of data augmentation that\nrandomly shifts training samples; and an assessment of classifier types that\nproduce the best performance in withheld target regions. The methodology is\napplied to detect smallholder irrigation in two states in the Ethiopian\nhighlands, Tigray and Amhara. Results show that a transformer-based neural\nnetwork architecture allows for the most robust prediction performance in\nwithheld regions, followed closely by a CatBoost random forest model. Over\nwithheld ground-collection survey labels, the transformer-based model achieves\n96.7% accuracy over non-irrigated samples and 95.9% accuracy over irrigated\nsamples. Over a larger set of samples independently collected via the\nintroduced method of label supplementation, non-irrigated and irrigated labels\nare predicted with 98.3% and 95.5% accuracy, respectively. The detection model\nis then deployed over Tigray and Amhara, revealing crop rotation patterns and\nyear-over-year irrigated area change. Predictions suggest that irrigated area\nin these two states has decreased by approximately 40% from 2020 to 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Conlon_T/0/1/0/all/0/1\">Terence Conlon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_C/0/1/0/all/0/1\">Christopher Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_V/0/1/0/all/0/1\">Vijay Modi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Dirichlet uncertainty for unsupervised out-of-distribution detection of eye fundus photographs in glaucoma screening. (arXiv:2202.12634v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.12634","description":"<p>The development of automatic tools for early glaucoma diagnosis with color\nfundus photographs can significantly reduce the impact of this disease.\nHowever, current state-of-the-art solutions are not robust to real-world\nscenarios, providing over-confident predictions for out-of-distribution cases.\nWith this in mind, we propose a model based on the Dirichlet distribution that\nallows to obtain class-wise probabilities together with an uncertainty\nestimation without exposure to out-of-distribution cases. We demonstrate our\napproach on the AIROGS challenge. At the start of the final test phase (8 Feb.\n2022), our method had the highest average score among all submissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_T/0/1/0/all/0/1\">Teresa Ara&#xfa;jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aresta_G/0/1/0/all/0/1\">Guilherme Aresta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1\">Hrvoje Bogunovic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning. (arXiv:2203.00843v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.00843","description":"<p>3D dense captioning aims to describe individual objects by natural language\nin 3D scenes, where 3D scenes are usually represented as RGB-D scans or point\nclouds. However, only exploiting single modal information, e.g., point cloud,\nprevious approaches fail to produce faithful descriptions. Though aggregating\n2D features into point clouds may be beneficial, it introduces an extra\ncomputational burden, especially in inference phases. In this study, we\ninvestigate a cross-modal knowledge transfer using Transformer for 3D dense\ncaptioning, X-Trans2Cap, to effectively boost the performance of single-modal\n3D caption through knowledge distillation using a teacher-student framework. In\npractice, during the training phase, the teacher network exploits auxiliary 2D\nmodality and guides the student network that only takes point clouds as input\nthrough the feature consistency constraints. Owing to the well-designed\ncross-modal feature fusion module and the feature alignment in the training\nphase, X-Trans2Cap acquires rich appearance information embedded in 2D images\nwith ease. Thus, a more faithful caption can be generated only using point\nclouds during the inference. Qualitative and quantitative results confirm that\nX-Trans2Cap outperforms previous state-of-the-art by a large margin, i.e.,\nabout +21 and about +16 absolute CIDEr score on ScanRefer and Nr3D datasets,\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhihao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xu Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yinghong Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yao Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation. (arXiv:2203.01452v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01452","description":"<p>Panoramic images with their 360-degree directional view encompass exhaustive\ninformation about the surrounding space, providing a rich foundation for scene\nunderstanding. To unfold this potential in the form of robust panoramic\nsegmentation models, large quantities of expensive, pixel-wise annotations are\ncrucial for success. Such annotations are available, but predominantly for\nnarrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal\nresources for training panoramic models. Distortions and the distinct\nimage-feature distribution in 360-degree panoramas impede the transfer from the\nannotation-rich pinhole domain and therefore come with a big dent in\nperformance. To get around this domain difference and bring together semantic\nannotations from pinhole- and 360-degree surround-visuals, we propose to learn\nobject deformations and panoramic image distortions in the Deformable Patch\nEmbedding (DPE) and Deformable MLP (DMLP) components which blend into our\nTransformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we\ntie together shared semantics in pinhole- and panoramic feature embeddings by\ngenerating multi-scale prototype features and aligning them in our Mutual\nPrototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor\nStanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance\nto fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled\npanoramas. On the outdoor DensePASS dataset, we break state-of-the-art by\n14.39% mIoU and set the new bar at 56.38%. Code will be made publicly available\nat https://github.com/jamycheung/Trans4PASS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Chaoxiang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiss_S/0/1/0/all/0/1\">Simon Rei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Universal Backward-Compatible Representation Learning. (arXiv:2203.01583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.01583","description":"<p>Conventional model upgrades for visual search systems require offline refresh\nof gallery features by feeding gallery images into new models (dubbed as\n\"backfill\"), which is time-consuming and expensive, especially in large-scale\napplications. The task of backward-compatible representation learning is\ntherefore introduced to support backfill-free model upgrades, where the new\nquery features are interoperable with the old gallery features. Despite the\nsuccess, previous works only investigated a close-set training scenario (i.e.,\nthe new training set shares the same classes as the old one), and are limited\nby more realistic and challenging open-set scenarios. To this end, we first\nintroduce a new problem of universal backward-compatible representation\nlearning, covering all possible data split in model upgrades. We further\npropose a simple yet effective method, dubbed as Universal Backward-Compatible\nTraining (UniBCT) with a novel structural prototype refinement algorithm, to\nlearn compatible representations in all kinds of model upgrading benchmarks in\na unified manner. Comprehensive experiments on the large-scale face recognition\ndatasets MS1Mv3 and IJB-C fully demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binjie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yixiao Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yantao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shupeng Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fanzi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xuyuan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1\">Ying Shan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Texture for Fooling Person Detectors in the Physical World. (arXiv:2203.03373v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.03373","description":"<p>Nowadays, cameras equipped with AI systems can capture and analyze images to\ndetect people automatically. However, the AI system can make mistakes when\nreceiving deliberately designed patterns in the real world, i.e., physical\nadversarial examples. Prior works have shown that it is possible to print\nadversarial patches on clothes to evade DNN-based person detectors. However,\nthese adversarial examples could have catastrophic drops in the attack success\nrate when the viewing angle (i.e., the camera's angle towards the object)\nchanges. To perform a multi-angle attack, we propose Adversarial Texture\n(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people\nwearing such clothes can hide from person detectors from different viewing\nangles. We propose a generative method, named Toroidal-Cropping-based\nExpandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive\nstructures. We printed several pieces of cloth with AdvTexure and then made\nT-shirts, skirts, and dresses in the physical world. Experiments showed that\nthese clothes could fool person detectors in the physical world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanhao Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Siyuan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaopei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaolin Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Region-Aware Face Swapping. (arXiv:2203.04564v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.04564","description":"<p>This paper presents a novel Region-Aware Face Swapping (RAFSwap) network to\nachieve identity-consistent harmonious high-resolution face generation in a\nlocal-global manner: \\textbf{1)} Local Facial Region-Aware (FRA) branch\naugments local identity-relevant features by introducing the Transformer to\neffectively model misaligned cross-scale semantic interaction. \\textbf{2)}\nGlobal Source Feature-Adaptive (SFA) branch further complements global\nidentity-relevant cues for generating identity-consistent swapped faces.\nBesides, we propose a \\textit{Face Mask Predictor} (FMP) module incorporated\nwith StyleGAN2 to predict identity-relevant soft facial masks in an\nunsupervised manner that is more practical for generating harmonious\nhigh-resolution faces. Abundant experiments qualitatively and quantitatively\ndemonstrate the superiority of our method for generating more\nidentity-consistent high-resolution swapped faces over SOTA methods, \\eg,\nobtaining 96.70 ID retrieval that outperforms SOTA MegaFS by 5.87$\\uparrow$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangning Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_M/0/1/0/all/0/1\">Miao Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zili Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEAT: A Large-Scale Semantic and Emotional Multi-Modal Dataset for Conversational Gestures Synthesis. (arXiv:2203.05297v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05297","description":"<p>Achieving realistic, vivid, and human-like synthesized conversational\ngestures conditioned on multi-modal data is still an unsolved problem, due to\nthe lack of available datasets, models and standard evaluation metrics. To\naddress this, we build Body-Expression-Audio-Text dataset, BEAT, which has i)\n76 hours, high-quality, multi-modal data captured from 30 speakers talking with\neight different emotions and in four different languages, ii) 32 millions\nframe-level emotion and semantic relevance annotations.Our statistical analysis\non BEAT demonstrates the correlation of conversational gestures with facial\nexpressions, emotions, and semantics, in addition to the known correlation with\naudio, text, and speaker identity. Qualitative and quantitative experiments\ndemonstrate metrics' validness, ground truth data quality, and baseline's\nstate-of-the-art performance. To the best of our knowledge, BEAT is the largest\nmotion capture dataset for investigating the human gestures, which may\ncontribute to a number of different research fields including controllable\ngesture synthesis, cross-modality analysis, emotional gesture recognition. The\ndata, code and model will be released for research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haiyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zihao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamoto_N/0/1/0/all/0/1\">Naoya Iwamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yichen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">You Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bozkurt_E/0/1/0/all/0/1\">Elif Bozkurt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Generalization via Shuffled Style Assembly for Face Anti-Spoofing. (arXiv:2203.05340v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05340","description":"<p>With diverse presentation attacks emerging continually, generalizable face\nanti-spoofing (FAS) has drawn growing attention. Most existing methods\nimplement domain generalization (DG) on the complete representations. However,\ndifferent image statistics may have unique properties for the FAS tasks. In\nthis work, we separate the complete representation into content and style ones.\nA novel Shuffled Style Assembly Network (SSAN) is proposed to extract and\nreassemble different content and style features for a stylized feature space.\nThen, to obtain a generalized representation, a contrastive learning strategy\nis developed to emphasize liveness-related style information while suppress the\ndomain-specific one. Finally, the representations of the correct assemblies are\nused to distinguish between living and spoofing during the inferring. On the\nother hand, despite the decent performance, there still exists a gap between\nacademia and industry, due to the difference in data quantity and distribution.\nThus, a new large-scale benchmark for FAS is built up to further evaluate the\nperformance of algorithms in reality. Both qualitative and quantitative results\non existing and proposed benchmarks demonstrate the effectiveness of our\nmethods. The codes will be available at https://github.com/wangzhuo2019/SSAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1\">Weihong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tingting Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongyuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeILF: Neural Incident Light Field for Physically-based Material Estimation. (arXiv:2203.07182v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07182","description":"<p>We present a differentiable rendering framework for material and lighting\nestimation from multi-view images and a reconstructed geometry. In the\nframework, we represent scene lightings as the Neural Incident Light Field\n(NeILF) and material properties as the surface BRDF modelled by multi-layer\nperceptrons. Compared with recent approaches that approximate scene lightings\nas the 2D environment map, NeILF is a fully 5D light field that is capable of\nmodelling illuminations of any static scenes. In addition, occlusions and\nindirect lights can be handled naturally by the NeILF representation without\nrequiring multiple bounces of ray tracing, making it possible to estimate\nmaterial properties even for scenes with complex lightings and geometries. We\nalso propose a smoothness regularization and a Lambertian assumption to reduce\nthe material-lighting ambiguity during the optimization. Our method strictly\nfollows the physically-based rendering equation, and jointly optimizes material\nand lighting through the differentiable rendering process. We have intensively\nevaluated the proposed method on our in-house synthetic dataset, the DTU MVS\ndataset, and real-world BlendedMVS scenes. Our method is able to outperform\nprevious methods by a significant margin in terms of novel view rendering\nquality, setting a new state-of-the-art for image-based material and lighting\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jingyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yihang Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_T/0/1/0/all/0/1\">Tian Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKinnon_D/0/1/0/all/0/1\">David McKinnon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsin_Y/0/1/0/all/0/1\">Yanghai Tsin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_L/0/1/0/all/0/1\">Long Quan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation. (arXiv:2203.07697v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.07697","description":"<p>In this paper, we present a novel Distribution-Aware Single-stage (DAS) model\nfor tackling the challenging multi-person 3D pose estimation problem. Different\nfrom existing top-down and bottom-up methods, the proposed DAS model\nsimultaneously localizes person positions and their corresponding body joints\nin the 3D camera space in a one-pass manner. This leads to a simplified\npipeline with enhanced efficiency. In addition, DAS learns the true\ndistribution of body joints for the regression of their positions, rather than\nmaking a simple Laplacian or Gaussian assumption as previous works. This\nprovides valuable priors for model prediction and thus boosts the\nregression-based scheme to achieve competitive performance with volumetric-base\nones. Moreover, DAS exploits a recursive update strategy for progressively\napproaching to regression target, alleviating the optimization difficulty and\nfurther lifting the regression performance. DAS is implemented with a fully\nConvolutional Neural Network and end-to-end learnable. Comprehensive\nexperiments on benchmarks CMU Panoptic and MuPoTS-3D demonstrate the superior\nefficiency of the proposed DAS model, specifically 1.5x speedup over previous\nbest model, and its stat-of-the-art accuracy for multi-person 3D pose\nestimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zitian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1\">Xuecheng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaochao Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunpeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Hyperbolic Embeddings in 2D Object Detection. (arXiv:2203.08049v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08049","description":"<p>Object detection, for the most part, has been formulated in the euclidean\nspace, where euclidean or spherical geodesic distances measure the similarity\nof an image region to an object class prototype. In this work, we study whether\na hyperbolic geometry better matches the underlying structure of the object\nclassification space. We incorporate a hyperbolic classifier in two-stage,\nkeypoint-based, and transformer-based object detection architectures and\nevaluate them on large-scale, long-tailed, and zero-shot object detection\nbenchmarks. In our extensive experimental evaluations, we observe categorical\nclass hierarchies emerging in the structure of the classification space,\nresulting in lower classification errors and boosting the overall object\ndetection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Christopher Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1\">Alexander Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain Adaptive Hand Keypoint and Pixel Localization in the Wild. (arXiv:2203.08344v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.08344","description":"<p>We aim to improve the performance of regressing hand keypoints and segmenting\npixel-level hand masks under new imaging conditions (e.g., outdoors) when we\nonly have labeled images taken under very different conditions (e.g., indoors).\nIn the real world, it is important that the model trained for both tasks works\nunder various imaging conditions. However, their variation covered by existing\nlabeled hand datasets is limited. Thus, it is necessary to adapt the model\ntrained on the labeled images (source) to unlabeled images (target) with unseen\nimaging conditions. While self-training domain adaptation methods (i.e.,\nlearning from the unlabeled target images in a self-supervised manner) have\nbeen developed for both tasks, their training may degrade performance when the\npredictions on the target images are noisy. To avoid this, it is crucial to\nassign a low importance (confidence) weight to the noisy predictions during\nself-training. In this paper, we propose to utilize the divergence of two\npredictions to estimate the confidence of the target image for both tasks.\nThese predictions are given from two separate networks, and their divergence\nhelps identify the noisy predictions. To integrate our proposed confidence\nestimation into self-training, we propose a teacher-student framework where the\ntwo networks (teachers) provide supervision to a network (student) for\nself-training, and the teachers are learned from the student by knowledge\ndistillation. Our experiments show its superiority over state-of-the-art\nmethods in adaptation settings with different lighting, grasping objects,\nbackgrounds, and camera viewpoints. Our method improves by 4% the multi-task\nscore on HO3D compared to the latest adversarial adaptation method. We also\nvalidate our method on Ego4D, egocentric videos with rapid changes in imaging\nconditions outdoors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ohkawa_T/0/1/0/all/0/1\">Takehiko Ohkawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu-Jhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1\">Qichen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1\">Ryosuke Furuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1\">Kris M. Kitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Many Data Samples is an Additional Instruction Worth?. (arXiv:2203.09161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.09161","description":"<p>Recently introduced instruction-paradigm empowers non-expert users to\nleverage NLP resources by defining a new task in natural language.\nInstruction-tuned models have significantly outperformed multitask learning\nmodels (without instruction); however they are far from state of the art task\nspecific models. Conventional approaches to improve model performance via\ncreating large datasets with lots of task instances or architectural/training\nchanges in model may not be feasible for non-expert users. However, they can\nwrite alternate instructions to represent an instruction task. Is\nInstruction-augumentation helpful? We augment a subset of tasks in the expanded\nversion of NATURAL INSTRUCTIONS with additional instructions and find that\nthese significantly improve model performance (up to 35%), especially in the\nlow-data regime. Our results indicate that an additional instruction can be\nequivalent to ~200 data samples on average across tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Puri_R/0/1/0/all/0/1\">Ravsehaj Singh Puri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Mihir Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Compression-Based Feature Learning for Video Restoration. (arXiv:2203.09208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09208","description":"<p>How to efficiently utilize the temporal features is crucial, yet challenging,\nfor video restoration. The temporal features usually contain various noisy and\nuncorrelated information, and they may interfere with the restoration of the\ncurrent frame. This paper proposes learning noise-robust feature\nrepresentations to help video restoration. We are inspired by that the neural\ncodec is a natural denoiser. In neural codec, the noisy and uncorrelated\ncontents which are hard to predict but cost lots of bits are more inclined to\nbe discarded for bitrate saving. Therefore, we design a neural compression\nmodule to filter the noise and keep the most useful information in features for\nvideo restoration. To achieve robustness to noise, our compression module\nadopts a spatial channel-wise quantization mechanism to adaptively determine\nthe quantization step size for each position in the latent. Experiments show\nthat our method can significantly boost the performance on video denoising,\nwhere we obtain 0.13 dB improvement over BasicVSR++ with only 0.23x FLOPs.\nMeanwhile, our method also obtains SOTA results on video deraining and\ndehazing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Cong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yan Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interacting Attention Graph for Single Image Two-Hand Reconstruction. (arXiv:2203.09364v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09364","description":"<p>Graph convolutional network (GCN) has achieved great success in single hand\nreconstruction task, while interacting two-hand reconstruction by GCN remains\nunexplored. In this paper, we present Interacting Attention Graph Hand\n(IntagHand), the first graph convolution based network that reconstructs two\ninteracting hands from a single RGB image. To solve occlusion and interaction\nchallenges of two-hand reconstruction, we introduce two novel attention based\nmodules in each upsampling step of the original GCN. The first module is the\npyramid image feature attention (PIFA) module, which utilizes multiresolution\nfeatures to implicitly obtain vertex-to-image alignment. The second module is\nthe cross hand attention (CHA) module that encodes the coherence of interacting\nhands by building dense cross-attention between two hand vertices. As a result,\nour model outperforms all existing two-hand reconstruction methods by a large\nmargin on InterHand2.6M benchmark. Moreover, ablation studies verify the\neffectiveness of both PIFA and CHA modules for improving the reconstruction\naccuracy. Results on in-the-wild images and live video streams further\ndemonstrate the generalization ability of our network. Our code is available at\nhttps://github.com/Dw1010/IntagHand.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengcheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Liang An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongwen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lianpeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yebin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Text Attention Network for Spatial Deformation Robust Scene Text Image Super-resolution. (arXiv:2203.09388v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09388","description":"<p>Scene text image super-resolution aims to increase the resolution and\nreadability of the text in low-resolution images. Though significant\nimprovement has been achieved by deep convolutional neural networks (CNNs), it\nremains difficult to reconstruct high-resolution images for spatially deformed\ntexts, especially rotated and curve-shaped ones. This is because the current\nCNN-based methods adopt locality-based operations, which are not effective to\ndeal with the variation caused by deformations. In this paper, we propose a CNN\nbased Text ATTention network (TATT) to address this problem. The semantics of\nthe text are firstly extracted by a text recognition module as text prior\ninformation. Then we design a novel transformer-based module, which leverages\nglobal attention mechanism, to exert the semantic guidance of text prior to the\ntext reconstruction process. In addition, we propose a text structure\nconsistency loss to refine the visual appearance by imposing structural\nconsistency on the reconstructions of regular and deformed texts. Experiments\non the benchmark TextZoom dataset show that the proposed TATT not only achieves\nstate-of-the-art performance in terms of PSNR/SSIM metrics, but also\nsignificantly improves the recognition accuracy in the downstream text\nrecognition task, particularly for text instances with multi-orientation and\ncurved shapes. Code is available at https://github.com/mjq11302010044/TATT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhetong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vox2Cortex: Fast Explicit Reconstruction of Cortical Surfaces from 3D MRI Scans with Geometric Deep Neural Networks. (arXiv:2203.09446v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09446","description":"<p>The reconstruction of cortical surfaces from brain magnetic resonance imaging\n(MRI) scans is essential for quantitative analyses of cortical thickness and\nsulcal morphology. Although traditional and deep learning-based algorithmic\npipelines exist for this purpose, they have two major drawbacks: lengthy\nruntimes of multiple hours (traditional) or intricate post-processing, such as\nmesh extraction and topology correction (deep learning-based). In this work, we\naddress both of these issues and propose Vox2Cortex, a deep learning-based\nalgorithm that directly yields topologically correct, three-dimensional meshes\nof the boundaries of the cortex. Vox2Cortex leverages convolutional and graph\nconvolutional neural networks to deform an initial template to the densely\nfolded geometry of the cortex represented by an input MRI scan. We show in\nextensive experiments on three brain MRI datasets that our meshes are as\naccurate as the ones reconstructed by state-of-the-art methods in the field,\nwithout the need for time- and resource-intensive post-processing. To\naccurately reconstruct the tightly folded cortex, we work with meshes\ncontaining about 168,000 vertices at test time, scaling deep explicit\nreconstruction methods to a new level.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bongratz_F/0/1/0/all/0/1\">Fabian Bongratz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1\">Anne-Marie Rickmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1\">Sebastian P&#xf6;lsterl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1\">Christian Wachinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transframer: Arbitrary Frame Prediction with Generative Models. (arXiv:2203.09494v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09494","description":"<p>We present a general-purpose framework for image modelling and vision tasks\nbased on probabilistic frame prediction. Our approach unifies a broad range of\ntasks, from image segmentation, to novel view synthesis and video\ninterpolation. We pair this framework with an architecture we term Transframer,\nwhich uses U-Net and Transformer components to condition on annotated context\nframes, and outputs sequences of sparse, compressed image features. Transframer\nis the state-of-the-art on a variety of video generation benchmarks, is\ncompetitive with the strongest models on few-shot view synthesis, and can\ngenerate coherent 30 second videos from a single image without any explicit\ngeometric information. A single generalist Transframer simultaneously produces\npromising results on 8 tasks, including semantic segmentation, image\nclassification and optical flow prediction with no task-specific architectural\ncomponents, demonstrating that multi-task computer vision can be tackled using\nprobabilistic image models. Our approach can in principle be applied to a wide\nrange of applications that require learning the conditional structure of\nannotated image-formatted data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nash_C/0/1/0/all/0/1\">Charlie Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1\">Jo&#xe3;o Carreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1\">Jacob Walker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barr_I/0/1/0/all/0/1\">Iain Barr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1\">Andrew Jaegle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinowski_M/0/1/0/all/0/1\">Mateusz Malinowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1\">Peter Battaglia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-03-20T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/"}}]}]}