{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-04-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"A Distant Supervision Corpus for Extracting Biomedical Relationships Between Chemicals, Diseases and Genes. (arXiv:2204.06584v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06584","description":"<p>We introduce ChemDisGene, a new dataset for training and evaluating\nmulti-class multi-label document-level biomedical relation extraction models.\nOur dataset contains 80k biomedical research abstracts labeled with mentions of\nchemicals, diseases, and genes, portions of which human experts labeled with 18\ntypes of biomedical relationships between these entities (intended for\nevaluation), and the remainder of which (intended for training) has been\ndistantly labeled via the CTD database with approximately 78\\% accuracy. In\ncomparison to similar preexisting datasets, ours is both substantially larger\nand cleaner; it also includes annotations linking mentions to their entities.\nWe also provide three baseline deep neural network relation extraction models\ntrained and evaluated on our new dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_S/0/1/0/all/0/1\">Sunil Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torkar_M/0/1/0/all/0/1\">Michaela Torkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EHRKit: A Python Natural Language Processing Toolkit for Electronic Health Record Texts. (arXiv:2204.06604v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06604","description":"<p>The Electronic Health Record (EHR) is an essential part of the modern medical\nsystem and impacts healthcare delivery, operations, and research. Unstructured\ntext is attracting much attention despite structured information in the EHRs\nand has become an exciting research field. The success of the recent neural\nNatural Language Processing (NLP) method has led to a new direction for\nprocessing unstructured clinical notes. In this work, we create a python\nlibrary for clinical texts, EHRKit. This library contains two main parts:\nMIMIC-III-specific functions and tasks specific functions. The first part\nintroduces a list of interfaces for accessing MIMIC-III NOTEEVENTS data,\nincluding basic search, information retrieval, and information extraction. The\nsecond part integrates many third-party libraries for up to 12 off-shelf NLP\ntasks such as named entity recognition, summarization, machine translation,\netc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1\">Irene Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1\">Keen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xiangru Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yujie Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lucas Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1\">Chia-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosand_B/0/1/0/all/0/1\">Benjamin Rosand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity. (arXiv:2204.06618v1 [cs.CC])","link":"http://arxiv.org/abs/2204.06618","description":"<p>This paper analyzes three formal models of Transformer encoders that differ\nin the form of their self-attention mechanism: unique hard attention (UHAT);\ngeneralized unique hard attention (GUHAT), which generalizes UHAT; and\naveraging hard attention (AHAT). We show that UHAT and GUHAT Transformers,\nviewed as string acceptors, can only recognize formal languages in the\ncomplexity class AC$^0$, the class of languages recognizable by families of\nBoolean circuits of constant depth and polynomial size. This upper bound\nsubsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages\nor the PARITY language, since those languages are outside AC$^0$ (Furst et al.,\n1984). In contrast, the non-AC$^0$ languages MAJORITY and DYCK-1 are\nrecognizable by AHAT networks, implying that AHAT can recognize languages that\nUHAT and GUHAT cannot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1\">Yiding Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angluin_D/0/1/0/all/0/1\">Dana Angluin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_R/0/1/0/all/0/1\">Robert Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAMERO: Consistency Regularized Ensemble of Perturbed Language Models with Weight Sharing. (arXiv:2204.06625v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06625","description":"<p>Model ensemble is a popular approach to produce a low-variance and\nwell-generalized model. However, it induces large memory and inference costs,\nwhich are often not affordable for real-world deployment. Existing work has\nresorted to sharing weights among models. However, when increasing the\nproportion of the shared weights, the resulting models tend to be similar, and\nthe benefits of using model ensemble diminish. To retain ensemble benefits\nwhile maintaining a low memory cost, we propose a consistency-regularized\nensemble learning approach based on perturbed models, named CAMERO.\nSpecifically, we share the weights of bottom layers across all models and apply\ndifferent perturbations to the hidden representations for different models,\nwhich can effectively promote the model diversity. Meanwhile, we apply a\nprediction consistency regularizer across the perturbed models to control the\nvariance due to the model diversity. Our experiments using large language\nmodels demonstrate that CAMERO significantly improves the generalization\nperformance of the ensemble model. Specifically, CAMERO outperforms the\nstandard ensemble of 8 BERT-base models on the GLUE benchmark by 0.7 with a\nsignificantly smaller model size (114.2M vs. 880.6M).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pengcheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yelong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tuo Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"METRO: Efficient Denoising Pretraining of Large Scale Autoencoding Language Models with Model Generated Signals. (arXiv:2204.06644v1 [cs.LG])","link":"http://arxiv.org/abs/2204.06644","description":"<p>We present an efficient method of pretraining large-scale autoencoding\nlanguage models using training signals generated by an auxiliary model.\nOriginated in ELECTRA, this training strategy has demonstrated\nsample-efficiency to pretrain models at the scale of hundreds of millions of\nparameters. In this work, we conduct a comprehensive empirical study, and\npropose a recipe, namely \"Model generated dEnoising TRaining Objective\"\n(METRO), which incorporates some of the best modeling techniques developed\nrecently to speed up, stabilize, and enhance pretrained language models without\ncompromising model effectiveness. The resultant models, METRO-LM, consisting of\nup to 5.4 billion parameters, achieve new state-of-the-art on the GLUE,\nSuperGLUE, and SQuAD benchmarks. More importantly, METRO-LM are efficient in\nthat they often outperform previous large models with significantly smaller\nmodel sizes and lower pretraining cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ke_G/0/1/0/all/0/1\">Guolin Ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Di He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1\">Saurabh Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GAP: A Graph-aware Language Model Framework for Knowledge Graph-to-Text Generation. (arXiv:2204.06674v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06674","description":"<p>Recent improvements in KG-to-text generation are due to additional auxiliary\npre-trained tasks designed to give the fine-tune task a boost in performance.\nThese tasks require extensive computational resources while only suggesting\nmarginal improvements. Here, we demonstrate that by fusing graph-aware elements\ninto existing pre-trained language models, we are able to outperform\nstate-of-the-art models and close the gap imposed by additional pre-train\ntasks. We do so by proposing a mask structure to capture neighborhood\ninformation and a novel type encoder that adds a bias to the graph-attention\nweights depending on the connection type. Experiments on two KG-to-text\nbenchmark datasets show these models to be superior in quality while involving\nfewer parameters and no additional pre-trained tasks. By formulating the\nproblem as a framework, we can interchange the various proposed components and\nbegin interpreting KG-to-text generative models based on the topological and\ntype information found in a graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_A/0/1/0/all/0/1\">Anthony Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alvandipour_M/0/1/0/all/0/1\">Mehrdad Alvandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Schema Graph Fusion Network for Multi-Domain Dialogue State Tracking. (arXiv:2204.06677v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06677","description":"<p>Dialogue State Tracking (DST) aims to keep track of users' intentions during\nthe course of a conversation. In DST, modelling the relations among domains and\nslots is still an under-studied problem. Existing approaches that have\nconsidered such relations generally fall short in: (1) fusing prior slot-domain\nmembership relations and dialogue-aware dynamic slot relations explicitly, and\n(2) generalizing to unseen domains. To address these issues, we propose a novel\n\\textbf{D}ynamic \\textbf{S}chema \\textbf{G}raph \\textbf{F}usion\n\\textbf{Net}work (\\textbf{DSGFNet}), which generates a dynamic schema graph to\nexplicitly fuse the prior slot-domain membership relations and dialogue-aware\ndynamic slot relations. It also uses the schemata to facilitate knowledge\ntransfer to new domains. DSGFNet consists of a dialogue utterance encoder, a\nschema graph encoder, a dialogue-aware schema graph evolving network, and a\nschema graph enhanced dialogue state decoder. Empirical results on benchmark\ndatasets (i.e., SGD, MultiWOZ2.1, and MultiWOZ2.2), show that DSGFNet\noutperforms existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yue Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1\">Aldo Lipani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1\">Fanghua Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Transformer-based Models for Long Document Classification. (arXiv:2204.06683v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06683","description":"<p>The recent literature in text classification is biased towards short text\nsequences (e.g., sentences or paragraphs). In real-world applications,\nmulti-page multi-paragraph documents are common and they cannot be efficiently\nencoded by vanilla Transformer-based models. We compare different\nTransformer-based Long Document Classification (TrLDC) approaches that aim to\nmitigate the computational overhead of vanilla transformers to encode much\nlonger text, namely sparse attention and hierarchical encoding methods. We\nexamine several aspects of sparse attention (e.g., size of local attention\nwindow, use of global attention) and hierarchical (e.g., document splitting\nstrategy) transformers on four document classification datasets covering\ndifferent domains. We observe a clear benefit from being able to process longer\ntext, and, based on our results, we derive practical advice of applying\nTransformer-based models on long document classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darkner_S/0/1/0/all/0/1\">Sune Darkner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GPT-NeoX-20B: An Open-Source Autoregressive Language Model. (arXiv:2204.06745v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06745","description":"<p>We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language\nmodel trained on the Pile, whose weights will be made freely and openly\navailable to the public through a permissive license. It is, to the best of our\nknowledge, the largest dense autoregressive model that has publicly available\nweights at the time of submission. In this work, we describe \\model{}'s\narchitecture and training and evaluate its performance on a range of\nlanguage-understanding, mathematics, and knowledge-based tasks. We find that\nGPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in\nperformance when evaluated five-shot than similarly sized GPT-3 and FairSeq\nmodels. We open-source the training and evaluation code, as well as the model\nweights, at https://github.com/EleutherAI/gpt-neox.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sid Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_S/0/1/0/all/0/1\">Stella Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hallahan_E/0/1/0/all/0/1\">Eric Hallahan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anthony_Q/0/1/0/all/0/1\">Quentin Anthony</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golding_L/0/1/0/all/0/1\">Laurence Golding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leahy_C/0/1/0/all/0/1\">Connor Leahy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDonell_K/0/1/0/all/0/1\">Kyle McDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pieler_M/0/1/0/all/0/1\">Michael Pieler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prashanth_U/0/1/0/all/0/1\">USVSN Sai Prashanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_S/0/1/0/all/0/1\">Shivanshu Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reynolds_L/0/1/0/all/0/1\">Laria Reynolds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tow_J/0/1/0/all/0/1\">Jonathan Tow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Ben Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1\">Samuel Weinbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Top-K Decoding for Non-Autoregressive Semantic Parsing via Intent Conditioning. (arXiv:2204.06748v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06748","description":"<p>Semantic parsing (SP) is a core component of modern virtual assistants like\nGoogle Assistant and Amazon Alexa. While sequence-to-sequence-based\nauto-regressive (AR) approaches are common for conversational semantic parsing,\nrecent studies employ non-autoregressive (NAR) decoders and reduce inference\nlatency while maintaining competitive parsing quality. However, a major\ndrawback of NAR decoders is the difficulty of generating top-k (i.e., k-best)\noutputs with approaches such as beam search. To address this challenge, we\npropose a novel NAR semantic parser that introduces intent conditioning on the\ndecoder. Inspired by the traditional intent and slot tagging parsers, we\ndecouple the top-level intent prediction from the rest of a parse. As the\ntop-level intent largely governs the syntax and semantics of a parse, the\nintent conditioning allows the model to better control beam search and improves\nthe quality and diversity of top-k outputs. We introduce a hybrid\nteacher-forcing approach to avoid training and inference mismatch. We evaluate\nthe proposed NAR on conversational SP datasets, TOP &amp; TOPv2. Like the existing\nNAR models, we maintain the O(1) decoding time complexity while generating more\ndiverse outputs and improving the top-3 exact match (EM) by 2.4 points. In\ncomparison with AR models, our model speeds up beam search inference by 6.7\ntimes on CPU with competitive top-k EM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_G/0/1/0/all/0/1\">Geunseob Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidey_C/0/1/0/all/0/1\">Chris Hidey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Shachi Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aditya Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_P/0/1/0/all/0/1\">Pararth Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rushin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-label topic classification for COVID-19 literature with Bioformer. (arXiv:2204.06758v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06758","description":"<p>We describe Bioformer team's participation in the multi-label topic\nclassification task for COVID-19 literature (track 5 of BioCreative VII). Topic\nclassification is performed using different BERT models (BioBERT, PubMedBERT,\nand Bioformer). We formulate the topic classification task as a sentence pair\nclassification problem, where the title is the first sentence, and the abstract\nis the second sentence. Our results show that Bioformer outperforms BioBERT and\nPubMedBERT in this task. Compared to the baseline results, our best model\nincreased micro, macro, and instance-based F1 score by 8.8%, 15.5%, 7.4%,\nrespectively. Bioformer achieved the highest micro F1 and macro F1 scores in\nthis challenge. In post-challenge experiments, we found that pretraining of\nBioformer on COVID-19 articles further improves the performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1\">Li Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Generalize to More: Continuous Semantic Augmentation for Neural Machine Translation. (arXiv:2204.06812v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06812","description":"<p>The principal task in supervised neural machine translation (NMT) is to learn\nto generate target sentences conditioned on the source inputs from a set of\nparallel sentence pairs, and thus produce a model capable of generalizing to\nunseen instances. However, it is commonly observed that the generalization\nperformance of the model is highly influenced by the amount of parallel data\nused in training. Although data augmentation is widely used to enrich the\ntraining data, conventional methods with discrete manipulations fail to\ngenerate diverse and faithful training samples. In this paper, we present a\nnovel data augmentation paradigm termed Continuous Semantic Augmentation\n(CsaNMT), which augments each training instance with an adjacency semantic\nregion that could cover adequate variants of literal expression under the same\nmeaning. We conduct extensive experiments on both rich-resource and\nlow-resource settings involving various language pairs, including WMT14\nEnglish-{German,French}, NIST Chinese-English and multiple low-resource IWSLT\ntranslation tasks. The provided empirical evidences show that CsaNMT sets a new\nlevel of performance among existing augmentation techniques, improving on the\nstate-of-the-art by a large margin. The core codes are contained in Appendix E.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiangpeng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Heng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yue Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1\">Rongxiang Weng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Gender Debiasing Affects Internal Model Representations, and Why It Matters. (arXiv:2204.06827v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06827","description":"<p>Common studies of gender bias in NLP focus either on extrinsic bias measured\nby model performance on a downstream task or on intrinsic bias found in models'\ninternal representations. However, the relationship between extrinsic and\nintrinsic bias is relatively unknown. In this work, we illuminate this\nrelationship by measuring both quantities together: we debias a model during\ndownstream fine-tuning, which reduces extrinsic bias, and measure the effect on\nintrinsic bias, which is operationalized as bias extractability with\ninformation-theoretic probing. Through experiments on two tasks and multiple\nbias metrics, we show that our intrinsic bias metric is a better indicator of\ndebiasing than (a contextual adaptation of) the standard WEAT metric, and can\nalso expose cases of superficial debiasing. Our framework provides a\ncomprehensive perspective on bias in NLP models, which can be applied to deploy\nNLP systems in a more informed manner. Our code will be made publicly\navailable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orgad_H/0/1/0/all/0/1\">Hadas Orgad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldfarb_Tarrant_S/0/1/0/all/0/1\">Seraphina Goldfarb-Tarrant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shedding New Light on the Language of the Dark Web. (arXiv:2204.06885v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06885","description":"<p>The hidden nature and the limited accessibility of the Dark Web, combined\nwith the lack of public datasets in this domain, make it difficult to study its\ninherent characteristics such as linguistic properties. Previous works on text\nclassification of Dark Web domain have suggested that the use of deep neural\nmodels may be ineffective, potentially due to the linguistic differences\nbetween the Dark and Surface Webs. However, not much work has been done to\nuncover the linguistic characteristics of the Dark Web. This paper introduces\nCoDA, a publicly available Dark Web dataset consisting of 10000 web documents\ntailored towards text-based Dark Web analysis. By leveraging CoDA, we conduct a\nthorough linguistic analysis of the Dark Web and examine the textual\ndifferences between the Dark Web and the Surface Web. We also assess the\nperformance of various methods of Dark Web page classification. Finally, we\ncompare CoDA with an existing public Dark Web dataset and evaluate their\nsuitability for various use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Youngjin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1\">Eugene Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yongjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seungwon Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1\">Jin-Woo Chung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does BERT really agree ? Fine-grained Analysis of Lexical Dependence on a Syntactic Task. (arXiv:2204.06889v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06889","description":"<p>Although transformer-based Neural Language Models demonstrate impressive\nperformance on a variety of tasks, their generalization abilities are not well\nunderstood. They have been shown to perform strongly on subject-verb number\nagreement in a wide array of settings, suggesting that they learned to track\nsyntactic dependencies during their training even without explicit supervision.\nIn this paper, we examine the extent to which BERT is able to perform\nlexically-independent subject-verb number agreement (NA) on targeted syntactic\ntemplates. To do so, we disrupt the lexical patterns found in naturally\noccurring stimuli for each targeted structure in a novel fine-grained analysis\nof BERT's behavior. Our results on nonce sentences suggest that the model\ngeneralizes well for simple templates, but fails to perform\nlexically-independent syntactic generalization when as little as one attractor\nis present.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lasri_K/0/1/0/all/0/1\">Karim Lasri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lenci_A/0/1/0/all/0/1\">Alessandro Lenci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poibeau_T/0/1/0/all/0/1\">Thierry Poibeau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenges for Open-domain Targeted Sentiment Analysis. (arXiv:2204.06893v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06893","description":"<p>Since previous studies on open-domain targeted sentiment analysis are limited\nin dataset domain variety and sentence level, we propose a novel dataset\nconsisting of 6,013 human-labeled data to extend the data domains in topics of\ninterest and document level. Furthermore, we offer a nested target annotation\nschema to extract the complete sentiment information in documents, boosting the\npracticality and effectiveness of open-domain targeted sentiment analysis.\nMoreover, we leverage the pre-trained model BART in a sequence-to-sequence\ngeneration method for the task. Benchmark results show that there exists large\nroom for improvement of open-domain targeted sentiment analysis. Meanwhile,\nexperiments have shown that challenges remain in the effective use of\nopen-domain data, long documents, the complexity of target structure, and\ndomain variances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Hongjie Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Linyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yanxia Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_R/0/1/0/all/0/1\">Rui Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Unified Multi-task Learning Framework for Multi-goal Conversational Recommender Systems. (arXiv:2204.06923v1 [cs.IR])","link":"http://arxiv.org/abs/2204.06923","description":"<p>Recent years witnessed several advances in developing multi-goal\nconversational recommender systems (MG-CRS) that can proactively attract users'\ninterests and naturally lead user-engaged dialogues with multiple\nconversational goals and diverse topics. Four tasks are often involved in\nMG-CRS, including Goal Planning, Topic Prediction, Item Recommendation, and\nResponse Generation. Most existing studies address only some of these tasks. To\nhandle the whole problem of MG-CRS, modularized frameworks are adopted where\neach task is tackled independently without considering their interdependencies.\nIn this work, we propose a novel Unified MultI-goal conversational recommeNDer\nsystem, namely UniMIND. In specific, we unify these four tasks with different\nformulations into the same sequence-to-sequence (Seq2Seq) paradigm.\nPrompt-based learning strategies are investigated to endow the unified model\nwith the capability of multi-task learning. Finally, the overall learning and\ninference procedure consists of three stages, including multi-task learning,\nprompt-based tuning, and inference. Experimental results on two MG-CRS\nbenchmarks (DuRecDial and TG-ReDial) show that UniMIND achieves\nstate-of-the-art performance on all tasks with a unified model. Extensive\nanalyses and discussions are provided for shedding some new perspectives for\nMG-CRS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weiwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Source HamNoSys Parser for Multilingual Sign Language Encoding. (arXiv:2204.06924v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06924","description":"<p>This paper presents our recent developments in the field of automatic\nprocessing of sign language corpora using the Hamburg Sign Language Annotation\nSystem (HamNoSys). We designed an automated tool to convert HamNoSys\nannotations into numerical labels for defined initial features of body and hand\npositions. Our proposed numerical multilabels greatly simplify the structure of\nHamNoSys annotation without significant loss of gloss meaning. These numerical\nmultilabels can potentially be used to feed the machine learning models, which\nwould accelerate the development of vision-based sign language recognition. In\naddition, this tool can assist experts in the annotation process to help\nidentify semantic errors. The code and sample annotations are publicly\navailable at https://github.com/hearai/parse-hamnosys.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1\">Sylwia Majchrowska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plantykow_M/0/1/0/all/0/1\">Marta Plantykow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olech_M/0/1/0/all/0/1\">Milena Olech</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Aspect Detection from Online Unsolicited Customer Reviews. (arXiv:2204.06964v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06964","description":"<p>Within the context of review analytics, aspects are the features of products\nand services at which customers target their opinions and sentiments. Aspect\ndetection helps product owners and service providers to identify shortcomings\nand prioritize customers' needs, and hence, maintain revenues and mitigate\ncustomer churn. Existing methods focus on detecting the surface form of an\naspect by training supervised learning methods that fall short when aspects are\nlatent in reviews. In this paper, we propose an unsupervised method to extract\nlatent occurrences of aspects. Specifically, we assume that a customer\nundergoes a two-stage hypothetical generative process when writing a review:\n(1) deciding on an aspect amongst the set of aspects available for the product\nor service, and (2) writing the opinion words that are more interrelated to the\nchosen aspect from the set of all words available in a language. We employ\nlatent Dirichlet allocation to learn the latent aspects distributions for\ngenerating the reviews. Experimental results on benchmark datasets show that\nour proposed method is able to improve the state of the art when the aspects\nare latent with no surface form in reviews.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Forouhesh_M/0/1/0/all/0/1\">Mohammad Forouhesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansouri_A/0/1/0/all/0/1\">Arash Mansouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fani_H/0/1/0/all/0/1\">Hossein Fani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Visual Dialogue Models Do Scorekeeping? Exploring How Dialogue Representations Incrementally Encode Shared Knowledge. (arXiv:2204.06970v1 [cs.CL])","link":"http://arxiv.org/abs/2204.06970","description":"<p>Cognitively plausible visual dialogue models should keep a mental scoreboard\nof shared established facts in the dialogue context. We propose a theory-based\nevaluation method for investigating to what degree models pretrained on the\nVisDial dataset incrementally build representations that appropriately do\nscorekeeping. Our conclusion is that the ability to make the distinction\nbetween shared and privately known statements along the dialogue is moderately\npresent in the analysed models, but not always incrementally consistent, which\nmay partially be due to the limited need for grounding interactions in the\noriginal task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Madureira_B/0/1/0/all/0/1\">Brielen Madureira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlangen_D/0/1/0/all/0/1\">David Schlangen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based Textual Knowledge Source. (arXiv:2204.07002v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07002","description":"<p>Question answering (QA) is a natural language understanding task within the\nfields of information retrieval and information extraction that has attracted\nmuch attention from the computational linguistics and artificial intelligence\nresearch community in recent years because of the strong development of machine\nreading comprehension-based models. A reader-based QA system is a high-level\nsearch engine that can find correct answers to queries or questions in\nopen-domain or domain-specific texts using machine reading comprehension (MRC)\ntechniques. The majority of advancements in data resources and machine-learning\napproaches in the MRC and QA systems, on the other hand, especially in two\nresource-rich languages such as English and Chinese. A low-resource language\nlike Vietnamese has witnessed a scarcity of research on QA systems. This paper\npresents XLMRQA, the first Vietnamese QA system using a supervised\ntransformer-based reader on the Wikipedia-based textual knowledge source (using\nthe UIT-ViQuAD corpus), outperforming the two robust QA systems using deep\nneural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively.\nFrom the results obtained on the three systems, we analyze the influence of\nquestion types on the performance of the QA systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Do_P/0/1/0/all/0/1\">Phong Nguyen-Thuan Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Nhat Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1\">Tin Van Huynh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anti-Asian Hate Speech Detection via Data Augmented Semantic Relation Inference. (arXiv:2204.07010v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07010","description":"<p>With the spreading of hate speech on social media in recent years, automatic\ndetection of hate speech is becoming a crucial task and has attracted attention\nfrom various communities. This task aims to recognize online posts (e.g.,\ntweets) that contain hateful information. The peculiarities of languages in\nsocial media, such as short and poorly written content, lead to the difficulty\nof learning semantics and capturing discriminative features of hate speech.\nPrevious studies have utilized additional useful resources, such as sentiment\nhashtags, to improve the performance of hate speech detection. Hashtags are\nadded as input features serving either as sentiment-lexicons or extra context\ninformation. However, our close investigation shows that directly leveraging\nthese features without considering their context may introduce noise to\nclassifiers. In this paper, we propose a novel approach to leverage sentiment\nhashtags to enhance hate speech detection in a natural language inference\nframework. We design a novel framework SRIC that simultaneously performs two\ntasks: (1) semantic relation inference between online posts and sentiment\nhashtags, and (2) sentiment classification on these posts. The semantic\nrelation inference aims to encourage the model to encode sentiment-indicative\ninformation into representations of online posts. We conduct extensive\nexperiments on two real-world datasets and demonstrate the effectiveness of our\nproposed framework compared with state-of-the-art representation learning\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaxuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Y/0/1/0/all/0/1\">Yue Ning</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rows from Many Sources: Enriching row completions from Wikidata with a pre-trained Language Model. (arXiv:2204.07014v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07014","description":"<p>Row completion is the task of augmenting a given table of text and numbers\nwith additional, relevant rows. The task divides into two steps: subject\nsuggestion, the task of populating the main column; and gap filling, the task\nof populating the remaining columns. We present state-of-the-art results for\nsubject suggestion and gap filling measured on a standard benchmark\n(WikiTables). Our idea is to solve this task by harmoniously combining\nknowledge base table interpretation and free text generation. We interpret the\ntable using the knowledge base to suggest new rows and generate metadata like\nheaders through property linking. To improve candidate diversity, we synthesize\nadditional rows using free text generation via GPT-3, and crucially, we exploit\nthe metadata we interpret to produce better prompts for text generation.\nFinally, we verify that the additional synthesized content can be linked to the\nknowledge base or a trusted web source such as Wikipedia.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Negreanu_C/0/1/0/all/0/1\">Carina Negreanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karaoglu_A/0/1/0/all/0/1\">Alperen Karaoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_J/0/1/0/all/0/1\">Jack Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fabian_D/0/1/0/all/0/1\">Daniel Fabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_A/0/1/0/all/0/1\">Andrew Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chin-Yew Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Multi-task Learning with Task Dependency for Appeal Judgment Prediction. (arXiv:2204.07046v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07046","description":"<p>Legal Judgment Prediction (LJP) aims to automatically predict judgment\nresults, such as charges, relevant law articles, and the term of penalty. It\nplays a vital role in legal assistant systems and has become a popular research\ntopic in recent years. This paper concerns a worthwhile but not well-studied\nLJP task, Appeal judgment Prediction (AJP), which predicts the judgment of an\nappellate court on an appeal case based on the textual description of case\nfacts and grounds of appeal. There are two significant challenges in practice\nto solve the AJP task. One is how to model the appeal judgment procedure\nappropriately. The other is how to improve the interpretability of the\nprediction results. We propose a Sequential Multi-task Learning Framework with\nTask Dependency for Appeal Judgement Prediction (SMAJudge) to address these\nchallenges. SMAJudge utilizes two sequential components to model the complete\nproceeding from the lower court to the appellate court and employs an attention\nmechanism to make the prediction more explainable, which handles the challenges\nof AJP effectively. Experimental results obtained with a dataset consisting of\nmore than 30K appeal judgment documents have revealed the effectiveness and\nsuperiority of SMAJudge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Lianxin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaohui Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chaoran Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yilong Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"State of the Art in Artificial Intelligence applied to the Legal Domain. (arXiv:2204.07047v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07047","description":"<p>While Artificial Intelligence applied to the legal domain is a topic with\norigins in the last century, recent advances in Artificial Intelligence are\nposed to revolutionize it. This work presents an overview and contextualizes\nthe main advances on the field of Natural Language Processing and how these\nadvances have been used to further the state of the art in legal text analysis.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dias_J/0/1/0/all/0/1\">Jo&#xe3;o Dias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_P/0/1/0/all/0/1\">Pedro A. Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordeiro_N/0/1/0/all/0/1\">Nuno Cordeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antunes_A/0/1/0/all/0/1\">Ana Antunes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_B/0/1/0/all/0/1\">Bruno Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baptista_J/0/1/0/all/0/1\">Jorge Baptista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goncalves_C/0/1/0/all/0/1\">Carlos Gon&#xe7;alves</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Evaluation Of Transformer Models For De-Identification Of Clinical Text Data. (arXiv:2204.07056v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07056","description":"<p>Objective: To comparatively evaluate several transformer model architectures\nat identifying protected health information (PHI) in the i2b2/UTHealth 2014\nclinical text de-identification challenge corpus.\n</p>\n<p>Methods: The i2b2/UTHealth 2014 corpus contains N=1304 clinical notes\nobtained from N=296 patients. Using a transfer learning framework, we fine-tune\nseveral transformer model architectures on the corpus, including: BERT-base,\nBERT-large, ROBERTA-base, ROBERTA-large, ALBERT-base and ALBERT-xxlarge. During\nfine-tuning we vary the following model hyper-parameters: batch size, number\ntraining epochs, learning rate and weight decay. We fine tune models on a\ntraining data set, we evaluate and select optimally performing models on an\nindependent validation dataset, and lastly assess generalization performance on\na held-out test dataset. We assess model performance in terms of accuracy,\nprecision (positive predictive value), recall (sensitivity) and F1 score\n(harmonic mean of precision and recall). We are interested in overall model\nperformance (PHI identified vs. PHI not identified), as well as PHI-specific\nmodel performance.\n</p>\n<p>Results: We observe that the ROBERTA-large models perform best at identifying\nPHI in the i2b2/UTHealth 2014 corpus, achieving &gt;99% overall accuracy and 96.7%\nrecall/precision on the heldout test corpus. Performance was good across many\nPHI classes; however, accuracy/precision/recall decreased for identification of\nthe following entity classes: professions, organizations, ages, and certain\nlocations.\n</p>\n<p>Conclusions: Transformers are a promising model class/architecture for\nclinical text de-identification. With minimal hyper-parameter tuning\ntransformers afford researchers/clinicians the opportunity to obtain (near)\nstate-of-the-art performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meaney_C/0/1/0/all/0/1\">Christopher Meaney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakimpour_W/0/1/0/all/0/1\">Wali Hakimpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalia_S/0/1/0/all/0/1\">Sumeet Kalia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moineddin_R/0/1/0/all/0/1\">Rahim Moineddin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hate Speech Classification Using SVM and Naive BAYES. (arXiv:2204.07057v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07057","description":"<p>The spread of hatred that was formerly limited to verbal communications has\nrapidly moved over the Internet. Social media and community forums that allow\npeople to discuss and express their opinions are becoming platforms for the\nspreading of hate messages. Many countries have developed laws to avoid online\nhate speech. They hold the companies that run the social media responsible for\ntheir failure to eliminate hate speech. But as online content continues to\ngrow, so does the spread of hate speech However, manual analysis of hate speech\non online platforms is infeasible due to the huge amount of data as it is\nexpensive and time consuming. Thus, it is important to automatically process\nthe online user contents to detect and remove hate speech from online media.\nMany recent approaches suffer from interpretability problem which means that it\ncan be difficult to understand why the systems make the decisions they do.\nThrough this work, some solutions for the problem of automatic detection of\nhate messages were proposed using Support Vector Machine (SVM) and Na\\\"ive\nBayes algorithms. This achieved near state-of-the-art performance while being\nsimpler and producing more easily interpretable decisions than other methods.\nEmpirical evaluation of this technique has resulted in a classification\naccuracy of approximately 99% and 50% for SVM and NB respectively over the test\nset.\n</p>\n<p>Keywords: classification; hate speech; feature extraction, algorithm,\nsupervised learning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asogwa_D/0/1/0/all/0/1\">D.C Asogwa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chukwuneke_C/0/1/0/all/0/1\">C.I Chukwuneke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngene_C/0/1/0/all/0/1\">C.C Ngene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anigbogu_G/0/1/0/all/0/1\">G.N Anigbogu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Strategy Adaptation to New Action Sets Using Multi-dimensional Modelling. (arXiv:2204.07082v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07082","description":"<p>A major bottleneck for building statistical spoken dialogue systems for new\ndomains and applications is the need for large amounts of training data. To\naddress this problem, we adopt the multi-dimensional approach to dialogue\nmanagement and evaluate its potential for transfer learning. Specifically, we\nexploit pre-trained task-independent policies to speed up training for an\nextended task-specific action set, in which the single summary action for\nrequesting a slot is replaced by multiple slot-specific request actions. Policy\noptimisation and evaluation experiments using an agenda-based user simulator\nshow that with limited training data, much better performance levels can be\nachieved when using the proposed multi-dimensional adaptation method. We\nconfirm this improvement in a crowd-sourced human user evaluation of our spoken\ndialogue system, comparing partially trained policies. The multi-dimensional\nsystem (with adaptation on limited training data in the target scenario)\noutperforms the one-dimensional baseline (without adaptation on the same amount\nof training data) by 7% perceived success rate.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keizer_S/0/1/0/all/0/1\">Simon Keizer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braunschweiler_N/0/1/0/all/0/1\">Norbert Braunschweiler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanchev_S/0/1/0/all/0/1\">Svetlana Stoyanchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Dual Encoder Architectures for Question Answering. (arXiv:2204.07120v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07120","description":"<p>Dual encoders have been used for question-answering (QA) and information\nretrieval (IR) tasks with good results. There are two major types of dual\nencoders, Siamese Dual Encoders (SDE), with parameters shared across two\nencoders, and Asymmetric Dual Encoder (ADE), with two distinctly parameterized\nencoders. In this work, we explore the dual encoder architectures for QA\nretrieval tasks. By evaluating on MS MARCO and the MultiReQA benchmark, we show\nthat SDE performs significantly better than ADE. We further propose three\ndifferent improved versions of ADEs. Based on the evaluation of QA retrieval\ntasks and direct analysis of the embeddings, we demonstrate that sharing\nparameters in projection layers would enable ADEs to perform competitively with\nSDEs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhe Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1\">Dan Bikel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfonseca_E/0/1/0/all/0/1\">Enrique Alfonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1\">Chen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitouni_I/0/1/0/all/0/1\">Imed Zitouni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Semantic Aware Pre-training for Few-shot Text Classification. (arXiv:2204.07128v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07128","description":"<p>In text classification tasks, useful information is encoded in the label\nnames. Label semantic aware systems have leveraged this information for\nimproved text classification performance during fine-tuning and prediction.\nHowever, use of label-semantics during pre-training has not been extensively\nexplored. We therefore propose Label Semantic Aware Pre-training (LSAP) to\nimprove the generalization and data efficiency of text classification systems.\nLSAP incorporates label semantics into pre-trained generative models (T5 in our\ncase) by performing secondary pre-training on labeled sentences from a variety\nof domains. As domain-general pre-training requires large amounts of data, we\ndevelop a filtering and labeling pipeline to automatically create\nsentence-label pairs from unlabeled text. We perform experiments on intent\n(ATIS, Snips, TOPv2) and topic classification (AG News, Yahoo! Answers). LSAP\nobtains significant accuracy improvements over state-of-the-art models for\nfew-shot text classification while maintaining performance comparable to state\nof the art in high-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_A/0/1/0/all/0/1\">Aaron Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krone_J/0/1/0/all/0/1\">Jason Krone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Salvatore Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_S/0/1/0/all/0/1\">Saab Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable and Robust Self-Learning for Skill Routing in Large-Scale Conversational AI Systems. (arXiv:2204.07135v1 [cs.LG])","link":"http://arxiv.org/abs/2204.07135","description":"<p>Skill routing is an important component in large-scale conversational\nsystems. In contrast to traditional rule-based skill routing, state-of-the-art\nsystems use a model-based approach to enable natural conversations. To provide\nsupervision signal required to train such models, ideas such as human\nannotation, replication of a rule-based system, relabeling based on user\nparaphrases, and bandit-based learning were suggested. However, these\napproaches: (a) do not scale in terms of the number of skills and skill\non-boarding, (b) require a very costly expert annotation/rule-design, (c)\nintroduce risks in the user experience with each model update. In this paper,\nwe present a scalable self-learning approach to explore routing alternatives\nwithout causing abrupt policy changes that break the user experience, learn\nfrom the user interaction, and incrementally improve the routing via frequent\nmodel refreshes. To enable such robust frequent model updates, we suggest a\nsimple and effective approach that ensures controlled policy updates for\nindividual domains, followed by an off-policy evaluation for making deployment\ndecisions without any need for lengthy A/B experimentation. We conduct various\noffline and online A/B experiments on a commercial large-scale conversational\nsystem to demonstrate the effectiveness of the proposed method in real-world\nproduction settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kachuee_M/0/1/0/all/0/1\">Mohammad Kachuee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1\">Jinseok Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_S/0/1/0/all/0/1\">Sarthak Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Won_J/0/1/0/all/0/1\">Jin-Myung Won</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLUES: A Benchmark for Learning Classifiers using Natural Language Explanations. (arXiv:2204.07142v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07142","description":"<p>Supervised learning has traditionally focused on inductive learning by\nobserving labeled examples of a task. In contrast, humans have the ability to\nlearn new concepts from language. Here, we explore training zero-shot\nclassifiers for structured data purely from language. For this, we introduce\nCLUES, a benchmark for Classifier Learning Using natural language ExplanationS,\nconsisting of a range of classification tasks over structured data along with\nnatural language supervision in the form of explanations. CLUES consists of 36\nreal-world and 144 synthetic classification tasks. It contains crowdsourced\nexplanations describing real-world tasks from multiple teachers and\nprogrammatically generated explanations for the synthetic tasks. To model the\ninfluence of explanations in classifying an example, we develop ExEnt, an\nentailment-based model that learns classifiers using explanations. ExEnt\ngeneralizes up to 18% better (relative) on novel tasks than a baseline that\ndoes not use explanations. We delineate key challenges for automated learning\nfrom explanations, addressing which can lead to progress on CLUES in the\nfuture. Code and datasets are available at: https://clues-benchmark.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1\">Rakesh R Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FREDA: Flexible Relation Extraction Data Annotation. (arXiv:2204.07150v1 [cs.CL])","link":"http://arxiv.org/abs/2204.07150","description":"<p>To effectively train accurate Relation Extraction models, sufficient and\nproperly labeled data is required. Adequately labeled data is difficult to\nobtain and annotating such data is a tricky undertaking. Previous works have\nshown that either accuracy has to be sacrificed or the task is extremely\ntime-consuming, if done accurately. We are proposing an approach in order to\nproduce high-quality datasets for the task of Relation Extraction quickly.\nNeural models, trained to do Relation Extraction on the created datasets,\nachieve very good results and generalize well to other datasets. In our study,\nwe were able to annotate 10,022 sentences for 19 relations in a reasonable\namount of time, and trained a commonly used baseline model for each relation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Strobl_M/0/1/0/all/0/1\">Michael Strobl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trabelsi_A/0/1/0/all/0/1\">Amine Trabelsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying Alignment in a Collaborative Learning Activity via Automatic Methods: The Link Between What We Say and Do. (arXiv:2104.04429v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04429","description":"<p>A dialogue is successful when there is alignment between the speakers at\ndifferent linguistic levels. In this work, we consider the dialogue occurring\nbetween interlocutors engaged in a collaborative learning task, where they are\nnot only evaluated on how well they performed, but also on how much they\nlearnt. The main contribution of this work is to propose new automatic measures\nto study alignment; focusing on verbal (lexical) alignment, and behavioral\nalignment (when an instruction given by one was followed with concrete actions\nby another). A second contribution of our work is to study how spontaneous\nspeech phenomena are used in the process of alignment. Lastly, we make public\nthe dataset to study alignment in educational dialogues. Our results show that\nall teams verbally and behaviourally align to some degree regardless of their\nperformance and learning, and our measures capture that teams that did not\nsucceed in the task were simply slower to collaborate. Thus we find that teams\nthat performed better, were faster to align. Furthermore, our methodology\ncaptures a productive period that includes the time where the interlocutors\ncame up with their best solutions. We also find that well-performing teams\nverbalise the marker \"oh\" more when they are behaviourally aligned, compared to\nother times in the dialogue; showing that this marker is an important cue in\nalignment. To the best of our knowledge, we are the first to study the role of\n\"oh\" as an information management marker in a behavioral context (i.e. in\nconnection to actions taken in a physical environment), compared to only a\nverbal one. Our measures contribute to the research in the field of educational\ndialogue and the intersection between dialogue and collaborative learning\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Norman_U/0/1/0/all/0/1\">Utku Norman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinkar_T/0/1/0/all/0/1\">Tanvi Dinkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruno_B/0/1/0/all/0/1\">Barbara Bruno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chlo&#xe9; Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Group-Sparse Matrix Factorization for Transfer Learning of Word Embeddings. (arXiv:2104.08928v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2104.08928","description":"<p>Unstructured text provides decision-makers with a rich data source in many\ndomains, ranging from product reviews in retailing to nursing notes in\nhealthcare. To leverage this information, words are typically translated into\nword embeddings -- vectors that encode the semantic relationships between words\n-- through unsupervised learning algorithms such as matrix factorization.\nHowever, learning word embeddings from new domains with limited training data\ncan be challenging, because the meaning/usage may be different in the new\ndomain, e.g., the word \"positive\" typically has positive sentiment, but often\nhas negative sentiment in medical notes since it may imply that a patient is\ntested positive for a disease. Intuitively, we expect that only a small number\nof domain-specific words may have new meanings/usages. We propose an intuitive\ntwo-stage estimator that exploits this structure via a group-sparse penalty to\nefficiently transfer learn domain-specific word embeddings by combining\nlarge-scale text corpora (such as Wikipedia) with limited domain-specific text\ndata. We bound the generalization error of our estimator, proving that it can\nachieve the same accuracy (compared to not transfer learning) with\nsubstantially less domain-specific data when only a small number of embeddings\nare altered between domains. Our results provide the first bounds on\ngroup-sparse matrix factorization, which may be of independent interest. We\nempirically evaluate the effectiveness of our approach compared to\nstate-of-the-art fine-tuning heuristics from natural language processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Xu_K/0/1/0/all/0/1\">Kan Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhao_X/0/1/0/all/0/1\">Xuanyi Zhao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bastani_H/0/1/0/all/0/1\">Hamsa Bastani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Systematic Style Differences between Unsupervised and Supervised MT and an Application for High-Resource Machine Translation. (arXiv:2106.15818v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.15818","description":"<p>Modern unsupervised machine translation (MT) systems reach reasonable\ntranslation quality under clean and controlled data conditions. As the\nperformance gap between supervised and unsupervised MT narrows, it is\ninteresting to ask whether the different training methods result in\nsystematically different output beyond what is visible via quality metrics like\nadequacy or BLEU. We compare translations from supervised and unsupervised MT\nsystems of similar quality, finding that unsupervised output is more fluent and\nmore structurally different in comparison to human translation than is\nsupervised MT. We then demonstrate a way to combine the benefits of both\nmethods into a single system which results in improved adequacy and fluency as\nrated by human evaluators. Our results open the door to interesting discussions\nabout how supervised and unsupervised MT might be different yet\nmutually-beneficial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your fairness may vary: Pretrained language model fairness in toxic text classification. (arXiv:2108.01250v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01250","description":"<p>The popularity of pretrained language models in natural language processing\nsystems calls for a careful evaluation of such models in down-stream tasks,\nwhich have a higher potential for societal impact. The evaluation of such\nsystems usually focuses on accuracy measures. Our findings in this paper call\nfor attention to be paid to fairness measures as well. Through the analysis of\nmore than a dozen pretrained language models of varying sizes on two toxic text\nclassification tasks (English), we demonstrate that focusing on accuracy\nmeasures alone can lead to models with wide variation in fairness\ncharacteristics. Specifically, we observe that fairness can vary even more than\naccuracy with increasing training data size and different random\ninitializations. At the same time, we find that little of the fairness\nvariation is explained by model size, despite claims in the literature. To\nimprove model fairness without retraining, we show that two post-processing\nmethods developed for structured, tabular data can be successfully applied to a\nrange of pretrained language models. Warning: This paper contains samples of\noffensive text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1\">Ioana Baldini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1\">Dennis Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1\">Karthikeyan Natesan Ramamurthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1\">Mikhail Yurochkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Moninder Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter User Representation Using Weakly Supervised Graph Embedding. (arXiv:2108.08988v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08988","description":"<p>Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summ^N: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents. (arXiv:2110.10150v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.10150","description":"<p>Text summarization helps readers capture salient information from documents,\nnews, interviews, and meetings. However, most state-of-the-art pretrained\nlanguage models (LM) are unable to efficiently process long text for many\nsummarization tasks. In this paper, we propose Summ$^N$, a simple, flexible,\nand effective multi-stage framework for input texts that are longer than the\nmaximum context length of typical pretrained LMs. Summ$^N$ first splits the\ndata samples and generates a coarse summary in multiple stages and then\nproduces the final fine-grained summary based on it. Our framework can process\ninput text of arbitrary length by adjusting the number of stages while keeping\nthe LM input size fixed. Moreover, it can deal with both single-source\ndocuments and dialogues, and it can be used on top of different backbone\nabstractive summarization models. To the best of our knowledge, Summ$^N$ is the\nfirst multi-stage split-then-summarize framework for long input summarization.\nOur experiments demonstrate that Summ$^N$ outperforms previous state-of-the-art\nmethods by improving ROUGE scores on three long meeting summarization datasets\nAMI, ICSI, and QMSum, two long TV series datasets from SummScreen, and a long\ndocument summarization dataset GovReport. Our data and code are available at\nhttps://github.com/psunlpgroup/Summ-N.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Ziming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chen Henry Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed H. Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EventNarrative: A large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation. (arXiv:2111.00276v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.00276","description":"<p>We introduce EventNarrative, a knowledge graph-to-text dataset from publicly\navailable open-world knowledge graphs. Given the recent advances in\nevent-driven Information Extraction (IE), and that prior research on\ngraph-to-text only focused on entity-driven KGs, this paper focuses on\nevent-centric data. However, our data generation system can still be adapted to\nother other types of KG data. Existing large-scale datasets in the\ngraph-to-text area are non-parallel, meaning there is a large disconnect\nbetween the KGs and text. The datasets that have a paired KG and text, are\nsmall scale and manually generated or generated without a rich ontology, making\nthe corresponding graphs sparse. Furthermore, these datasets contain many\nunlinked entities between their KG and text pairs. EventNarrative consists of\napproximately 230,000 graphs and their corresponding natural language text, 6\ntimes larger than the current largest parallel dataset. It makes use of a rich\nontology, all of the KGs entities are linked to the text, and our manual\nannotations confirm a high data quality. Our aim is two-fold: help break new\nground in event-centric research where data is lacking, and to give researchers\na well-defined, large-scale dataset in order to better evaluate existing and\nfuture knowledge graph-to-text models. We also evaluate two types of baseline\non EventNarrative: a graph-to-text specific model and two state-of-the-art\nlanguage models, which previous work has shown to be adaptable to the knowledge\ngraph-to-text domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colas_A/0/1/0/all/0/1\">Anthony Colas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghian_A/0/1/0/all/0/1\">Ali Sadeghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Daisy Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VIRT: Improving Representation-based Models for Text Matching through Virtual Interaction. (arXiv:2112.04195v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.04195","description":"<p>With the booming of pre-trained transformers, representation-based models\nbased on Siamese transformer encoders have become mainstream techniques for\nefficient text matching. However, these models suffer from severe performance\ndegradation due to the lack of interaction between the text pair, compared with\ninteraction-based models. Prior arts attempt to address this through performing\nextra interaction for Siamese encoded representations, while the interaction\nduring encoding is still ignored. To remedy this, we propose a \\textit{Virtual}\nInteRacTion mechanism (VIRT) to transfer interactive knowledge from\ninteraction-based models into Siamese encoders through attention map\ndistillation. As a train-time-only component, VIRT could completely maintain\nthe high efficiency of the Siamese structure and brings no extra computation\ncost during inference. To fully utilize the learned interactive knowledge, we\nfurther design a VIRT-adapted interaction strategy. Experimental results on\nmultiple text matching datasets demonstrate that our method outperforms\nstate-of-the-art representation-based models. What's more, VIRT can be easily\nintegrated into existing representation-based methods to achieve further\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongyin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1\">Enhong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Materialized Knowledge Bases from Commonsense Transformers. (arXiv:2112.14815v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.14815","description":"<p>Starting from the COMET methodology by Bosselut et al. (2019), generating\ncommonsense knowledge directly from pre-trained language models has recently\nreceived significant attention. Surprisingly, up to now no materialized\nresource of commonsense knowledge generated this way is publicly available.\nThis paper fills this gap, and uses the materialized resources to perform a\ndetailed analysis of the potential of this approach in terms of precision and\nrecall. Furthermore, we identify common problem cases, and outline use cases\nenabled by materialized resources. We posit that the availability of these\nresources is important for the advancement of the field, as it enables an\noff-the-shelf-use of the resulting knowledge, as well as further analyses on\nits strengths and weaknesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tuan-Phong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1\">Simon Razniewski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question rewriting? Assessing its importance for conversational question answering. (arXiv:2201.09146v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.09146","description":"<p>In conversational question answering, systems must correctly interpret the\ninterconnected interactions and generate knowledgeable answers, which may\nrequire the retrieval of relevant information from a background repository.\nRecent approaches to this problem leverage neural language models, although\ndifferent alternatives can be considered in terms of modules for (a)\nrepresenting user questions in context, (b) retrieving the relevant background\ninformation, and (c) generating the answer. This work presents a conversational\nquestion answering system designed specifically for the Search-Oriented\nConversational AI (SCAI) shared task, and reports on a detailed analysis of its\nquestion rewriting module. In particular, we considered different variations of\nthe question rewriting module to evaluate the influence on the subsequent\ncomponents, and performed a careful analysis of the results obtained with the\nbest system configuration. Our system achieved the best performance in the\nshared task and our analysis emphasizes the importance of the conversation\ncontext representation for the overall system performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raposo_G/0/1/0/all/0/1\">Gon&#xe7;alo Raposo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_R/0/1/0/all/0/1\">Rui Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_B/0/1/0/all/0/1\">Bruno Martins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coheur_L/0/1/0/all/0/1\">Lu&#xed;sa Coheur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Frustratingly Simple Approach for End-to-End Image Captioning. (arXiv:2201.12723v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12723","description":"<p>Image Captioning is a fundamental task to join vision and language,\nconcerning about cross-modal understanding and text generation. Recent years\nwitness the emerging attention on image captioning. Most of existing works\nfollow a traditional two-stage training paradigm. Before training the\ncaptioning models, an extra object detector is utilized to recognize the\nobjects in the image at first. However, they require sizeable datasets with\nfine-grained object annotation for training the object detector, which is a\ndaunting task. In addition, the errors of the object detectors are easy to\npropagate to the following captioning models, degenerating models' performance.\nTo alleviate such defects, we propose a frustratingly simple but highly\neffective end-to-end image captioning framework, Visual Conditioned GPT\n(VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language\ndecoder (GPT2). Different from the vanilla connection method that directly\ninserts the cross-attention modules into GPT2, we come up with a self-ensemble\ncross-modal fusion mechanism that comprehensively considers both the single-\nand cross-modal knowledge. As a result, we do not need extra object detectors\nfor model training. Experimental results conducted on three popular image\ncaptioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our\nVC-GPT achieves either the best or the second-best performance across all\nevaluation metrics over extensive baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transformers and the representation of biomedical background knowledge. (arXiv:2202.02432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2202.02432","description":"<p>BioBERT and BioMegatron are Transformers models adapted for the biomedical\ndomain based on publicly available biomedical corpora. As such, they have the\npotential to encode large-scale biological knowledge. We investigate the\nencoding and representation of biological knowledge in these models, and its\npotential utility to support inference in cancer precision medicine - namely,\nthe interpretation of the clinical significance of genomic alterations. We\ncompare the performance of different transformer baselines; we use probing to\ndetermine the consistency of encodings for distinct entities; and we use\nclustering methods to compare and contrast the internal properties of the\nembeddings for genes, variants, drugs and diseases. We show that these models\ndo indeed encode biological knowledge, although some of this is lost in\nfine-tuning for specific tasks. Finally, we analyse how the models behave with\nregard to biases and imbalances in the dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wysocki_O/0/1/0/all/0/1\">Oskar Wysocki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zili Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ORegan_P/0/1/0/all/0/1\">Paul O&#x27;Regan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wysocka_M/0/1/0/all/0/1\">Magdalena Wysocka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landers_D/0/1/0/all/0/1\">D&#xf3;nal Landers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andr&#xe9; Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkillNet: A Sparsely Activated Model for General-Purpose Natural Language Understanding. (arXiv:2203.03312v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.03312","description":"<p>Prevailing deep models are single-purpose and overspecialize at individual\ntasks. However, when being extended to new tasks, they typically forget\npreviously learned skills and learn from scratch. We address this issue by\nintroducing SkillNet, a general-purpose model that stitches together existing\nskills to learn new tasks more effectively. The key feature of our approach is\nthat it is sparsely activated guided by predefined skills. Different from\ntraditional dense models that always activate all the model parameters,\nSkillNet only activates parts of the model parameters whose skills are relevant\nto the target task. When learning for a new task, our approach precisely\nactivates required skills and also provides an option to add new skills. We\nevaluate on natural language understandings tasks and have the following\nfindings. First, with only one model checkpoint, SkillNet performs better than\ntask-specific fine-tuning and two multi-task learning baselines (i.e., dense\nmodel and Mixture-of-Experts model) on six tasks. Second, sparsely activated\npre-training further improves the overall performance. Third, SkillNet\nsignificantly outperforms baseline systems when being extended to new tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Cong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuangzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chart-to-Text: A Large-Scale Benchmark for Chart Summarization. (arXiv:2203.06486v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2203.06486","description":"<p>Charts are commonly used for exploring data and communicating insights.\nGenerating natural language summaries from charts can be very helpful for\npeople in inferring key insights that would otherwise require a lot of\ncognitive and perceptual efforts. We present Chart-to-text, a large-scale\nbenchmark with two datasets and a total of 44,096 charts covering a wide range\nof topics and chart types. We explain the dataset construction process and\nanalyze the datasets. We also introduce a number of state-of-the-art neural\nmodels as baselines that utilize image captioning and data-to-text generation\ntechniques to tackle two problem variations: one assumes the underlying data\ntable of the chart is available while the other needs to extract data from\nchart images. Our analysis with automatic and human evaluation shows that while\nour best models usually generate fluent summaries and yield reasonable BLEU\nscores, they also suffer from hallucinations and factual errors as well as\ndifficulties in correctly explaining complex patterns and trends in charts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kantharaj_S/0/1/0/all/0/1\">Shankar Kantharaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leong_R/0/1/0/all/0/1\">Rixie Tiffany Ko Leong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masry_A/0/1/0/all/0/1\">Ahmed Masry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakkar_M/0/1/0/all/0/1\">Megh Thakkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeLoRes: Decorrelating Latent Spaces for Low-Resource Audio Representation Learning. (arXiv:2203.13628v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2203.13628","description":"<p>Inspired by the recent progress in self-supervised learning for computer\nvision, in this paper, through the DeLoRes learning framework, we introduce two\nnew general-purpose audio representation learning approaches, the DeLoRes-S and\nDeLoRes-M. Our main objective is to make our network learn representations in a\nresource-constrained setting (both data and compute), that can generalize well\nacross a diverse set of downstream tasks. Inspired from the Barlow Twins\nobjective function, we propose to learn embeddings that are invariant to\ndistortions of an input audio sample, while making sure that they contain\nnon-redundant information about the sample. To achieve this, we measure the\ncross-correlation matrix between the outputs of two identical networks fed with\ndistorted versions of an audio segment sampled from an audio file and make it\nas close to the identity matrix as possible. We call this the DeLoRes learning\nframework, which we employ in different fashions with the DeLoRes-S and\nDeLoRes-M. We use a combination of a small subset of the large-scale AudioSet\ndataset and FSD50K for self-supervised learning and are able to learn with less\nthan half the parameters compared to state-of-the-art algorithms. For\nevaluation, we transfer these learned representations to 11 downstream\nclassification tasks, including speech, music, and animal sounds, and achieve\nstate-of-the-art results on 7 out of 11 tasks on linear evaluation with\nDeLoRes-M and show competitive results with DeLoRes-S, even when pre-trained\nusing only a fraction of the total data when compared to prior art. Our\ntransfer learning evaluation setup also shows extremely competitive results for\nboth DeLoRes-S and DeLoRes-M, with DeLoRes-M achieving state-of-the-art in 4\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sreyan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seth_A/0/1/0/all/0/1\">Ashish Seth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_a/0/1/0/all/0/1\">and Deepak Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Umesh_S/0/1/0/all/0/1\">S Umesh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WAVPROMPT: Towards Few-Shot Spoken Language Understanding with Frozen Language Models. (arXiv:2203.15863v2 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2203.15863","description":"<p>Large-scale auto-regressive language models pretrained on massive text have\ndemonstrated their impressive ability to perform new natural language tasks\nwith only a few text examples, without the need for fine-tuning. Recent studies\nfurther show that such a few-shot learning ability can be extended to the\ntext-image setting by training an encoder to encode the images into embeddings\nfunctioning like the text embeddings of the language model. Interested in\nexploring the possibility of transferring the few-shot learning ability to the\naudio-text setting, we propose a novel speech understanding framework,\nWavPrompt, where we finetune a wav2vec model to generate a sequence of audio\nembeddings understood by the language model. We show that WavPrompt is a\nfew-shot learner that can perform speech understanding tasks better than a\nnaive text baseline. We conduct detailed ablation studies on different\ncomponents and hyperparameters to empirically identify the best model\nconfiguration. In addition, we conduct a non-speech understanding experiment to\nshow WavPrompt can extract more information than just the transcriptions. Code\nis available at https://github.com/Hertin/WavPrompt\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_H/0/1/0/all/0/1\">Heting Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_J/0/1/0/all/0/1\">Junrui Ni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qian_K/0/1/0/all/0/1\">Kaizhi Qian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_S/0/1/0/all/0/1\">Shiyu Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3M: Multi-loss, Multi-path and Multi-level Neural Networks for speech recognition. (arXiv:2204.03178v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2204.03178","description":"<p>Recently, Conformer based CTC/AED model has become a mainstream architecture\nfor ASR. In this paper, based on our prior work, we identify and integrate\nseveral approaches to achieve further improvements for ASR tasks, which we\ndenote as multi-loss, multi-path and multi-level, summarized as \"3M\" model.\nSpecifically, multi-loss refers to the joint CTC/AED loss and multi-path\ndenotes the Mixture-of-Experts(MoE) architecture which can effectively increase\nthe model capacity without remarkably increasing computation cost. Multi-level\nmeans that we introduce auxiliary loss at multiple level of a deep model to\nhelp training. We evaluate our proposed method on the public WenetSpeech\ndataset and experimental results show that the proposed method provides\n12.2%-17.6% relative CER improvement over the baseline model trained by Wenet\ntoolkit. On our large scale dataset of 150k hours corpus, the 3M model has also\nshown obvious superiority over the baseline Conformer model. Code is publicly\navailable at https://github.com/tencent-ailab/3m-asr.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_Z/0/1/0/all/0/1\">Zhao You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shulin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Embeddings Are Capable of Capturing Rhythmic Similarity of Words. (arXiv:2204.04833v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.04833","description":"<p>Word embedding systems such as Word2Vec and GloVe are well-known in deep\nlearning approaches to NLP. This is largely due to their ability to capture\nsemantic relationships between words. In this work we investigated their\nusefulness in capturing rhythmic similarity of words instead. The results show\nthat vectors these embeddings assign to rhyming words are more similar to each\nother, compared to the other words. It is also revealed that GloVe performs\nrelatively better than Word2Vec in this regard. We also proposed a first of its\nkind metric for quantifying rhythmic similarity of a pair of words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rezaei_H/0/1/0/all/0/1\">Hosein Rezaei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GERE: Generative Evidence Retrieval for Fact Verification. (arXiv:2204.05511v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05511","description":"<p>Fact verification (FV) is a challenging task which aims to verify a claim\nusing multiple evidential sentences from trustworthy corpora, e.g., Wikipedia.\nMost existing approaches follow a three-step pipeline framework, including\ndocument retrieval, sentence retrieval and claim verification. High-quality\nevidences provided by the first two steps are the foundation of the effective\nreasoning in the last step. Despite being important, high-quality evidences are\nrarely studied by existing works for FV, which often adopt the off-the-shelf\nmodels to retrieve relevant documents and sentences in an\n\"index-retrieve-then-rank\" fashion. This classical approach has clear drawbacks\nas follows: i) a large document index as well as a complicated search process\nis required, leading to considerable memory and computational overhead; ii)\nindependent scoring paradigms fail to capture the interactions among documents\nand sentences in ranking; iii) a fixed number of sentences are selected to form\nthe final evidence set. In this work, we propose GERE, the first system that\nretrieves evidences in a generative fashion, i.e., generating the document\ntitles as well as evidence sentence identifiers. This enables us to mitigate\nthe aforementioned technical issues since: i) the memory and computational cost\nis greatly reduced because the document index is eliminated and the heavy\nranking process is replaced by a light generative process; ii) the dependency\nbetween documents and that between sentences could be captured via sequential\ngeneration process; iii) the generative formulation allows us to dynamically\nselect a precise set of relevant evidences for each claim. The experimental\nresults on the FEVER dataset show that GERE achieves significant improvements\nover the state-of-the-art baselines, with both time-efficiency and\nmemory-efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiangui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yixing Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection. (arXiv:2204.05515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.05515","description":"<p>Compared with unimodal data, multimodal data can provide more features to\nhelp the model analyze the sentiment of data. Previous research works rarely\nconsider token-level feature fusion, and few works explore learning the common\nfeatures related to sentiment in multimodal data to help the model fuse\nmultimodal features. In this paper, we propose a Contrastive Learning and\nMulti-Layer Fusion (CLMLF) method for multimodal sentiment detection.\nSpecifically, we first encode text and image to obtain hidden representations,\nand then use a multi-layer fusion module to align and fuse the token-level\nfeatures of text and image. In addition to the sentiment analysis task, we also\ndesigned two contrastive learning tasks, label based contrastive learning and\ndata based contrastive learning tasks, which will help the model learn common\nfeatures related to sentiment in multimodal data. Extensive experiments\nconducted on three publicly available multimodal datasets demonstrate the\neffectiveness of our approach for multimodal sentiment detection compared with\nexisting methods. The codes are available for use at\nhttps://github.com/Link-Li/CLMLF\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Conghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Multi-Label Prompting: Simple and Interpretable Few-Shot Classification. (arXiv:2204.06305v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2204.06305","description":"<p>Prompt-based learning (i.e., prompting) is an emerging paradigm for\nexploiting knowledge learned by a pretrained language model. In this paper, we\npropose Automatic Multi-Label Prompting (AMuLaP), a simple yet effective method\nto automatically select label mappings for few-shot text classification with\nprompting. Our method exploits one-to-many label mappings and a\nstatistics-based algorithm to select label mappings given a prompt template.\nOur experiments demonstrate that AMuLaP achieves competitive performance on the\nGLUE benchmark without human effort or external resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Han Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Canwen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Estimating Structural Disparities for Face Models. (arXiv:2204.06562v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06562","description":"<p>In machine learning, disparity metrics are often defined by measuring the\ndifference in the performance or outcome of a model, across different\nsub-populations (groups) of datapoints. Thus, the inputs to disparity\nquantification consist of a model's predictions $\\hat{y}$, the ground-truth\nlabels for the predictions $y$, and group labels $g$ for the data points.\nPerformance of the model for each group is calculated by comparing $\\hat{y}$\nand $y$ for the datapoints within a specific group, and as a result, disparity\nof performance across the different groups can be calculated. In many real\nworld scenarios however, group labels ($g$) may not be available at scale\nduring training and validation time, or collecting them might not be feasible\nor desirable as they could often be sensitive information. As a result,\nevaluating disparity metrics across categorical groups would not be feasible.\nOn the other hand, in many scenarios noisy groupings may be obtainable using\nsome form of a proxy, which would allow measuring disparity metrics across\nsub-populations. Here we explore performing such analysis on computer vision\nmodels trained on human faces, and on tasks such as face attribute prediction\nand affect estimation. Our experiments indicate that embeddings resulting from\nan off-the-shelf face recognition model, could meaningfully serve as a proxy\nfor such estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ardeshir_S/0/1/0/all/0/1\">Shervin Ardeshir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segalin_C/0/1/0/all/0/1\">Cristina Segalin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kallus_N/0/1/0/all/0/1\">Nathan Kallus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Character-focused Video Thumbnail Retrieval. (arXiv:2204.06563v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06563","description":"<p>We explore retrieving character-focused video frames as candidates for being\nvideo thumbnails. To evaluate each frame of the video based on the character(s)\npresent in it, characters (faces) are evaluated in two aspects:\nFacial-expression: We train a CNN model to measure whether a face has an\nacceptable facial expression for being in a video thumbnail. This model is\ntrained to distinguish faces extracted from artworks/thumbnails, from faces\nextracted from random frames of videos. Prominence and interactions:\nCharacter(s) in the thumbnail should be important character(s) in the video, to\nprevent the algorithm from suggesting non-representative frames as candidates.\nWe use face clustering to identify the characters in the video, and form a\ngraph in which the prominence (frequency of appearance) of the character(s),\nand their interactions (co-occurrence) are captured. We use this graph to infer\nthe relevance of the characters present in each candidate frame. Once every\nface is scored based on the two criteria above, we infer frame level scores by\ncombining the scores for all the faces within a frame.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ardeshir_S/0/1/0/all/0/1\">Shervin Ardeshir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_N/0/1/0/all/0/1\">Nagendra Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taghavi_H/0/1/0/all/0/1\">Hossein Taghavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OccAM's Laser: Occlusion-based Attribution Maps for 3D Object Detectors on LiDAR Data. (arXiv:2204.06577v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06577","description":"<p>While 3D object detection in LiDAR point clouds is well-established in\nacademia and industry, the explainability of these models is a largely\nunexplored field. In this paper, we propose a method to generate attribution\nmaps for the detected objects in order to better understand the behavior of\nsuch models. These maps indicate the importance of each 3D point in predicting\nthe specific objects. Our method works with black-box models: We do not require\nany prior knowledge of the architecture nor access to the model's internals,\nlike parameters, activations or gradients. Our efficient perturbation-based\napproach empirically estimates the importance of each point by testing the\nmodel with randomly generated subsets of the input point cloud. Our\nsub-sampling strategy takes into account the special characteristics of LiDAR\ndata, such as the depth-dependent point density. We show a detailed evaluation\nof the attribution maps and demonstrate that they are interpretable and highly\ninformative. Furthermore, we compare the attribution maps of recent 3D object\ndetection architectures to provide insights into their decision-making\nprocesses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schinagl_D/0/1/0/all/0/1\">David Schinagl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krispel_G/0/1/0/all/0/1\">Georg Krispel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Possegger_H/0/1/0/all/0/1\">Horst Possegger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_P/0/1/0/all/0/1\">Peter M. Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bischof_H/0/1/0/all/0/1\">Horst Bischof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Illumination-Invariant Active Camera Relocalization for Fine-Grained Change Detection in the Wild. (arXiv:2204.06580v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06580","description":"<p>Active camera relocalization (ACR) is a new problem in computer vision that\nsignificantly reduces the false alarm caused by image distortions due to camera\npose misalignment in fine-grained change detection (FGCD). Despite the fruitful\nachievements that ACR can support, it still remains a challenging problem\ncaused by the unstable results of relative pose estimation, especially for\noutdoor scenes, where the lighting condition is out of control, i.e., the twice\nobservations may have highly varied illuminations. This paper studies an\nillumination-invariant active camera relocalization method, it improves both in\nrelative pose estimation and scale estimation. We use plane segments as an\nintermediate representation to facilitate feature matching, thus further\nboosting pose estimation robustness and reliability under lighting variances.\nMoreover, we construct a linear system to obtain the absolute scale in each ACR\niteration by minimizing the image warping error, thus, significantly reduce the\ntime consume of ACR process, it is nearly $1.6$ times faster than the\nstate-of-the-art ACR strategy. Our work greatly expands the feasibility of\nreal-world fine-grained change monitoring tasks for cultural heritages.\nExtensive experiments tests and real-world applications verify the\neffectiveness and robustness of the proposed pose estimation method using for\nACR tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1\">Nan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1\">Wei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Relation Learning for Regression and Its Application to Brain Age Estimation. (arXiv:2204.06598v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06598","description":"<p>Most deep learning models for temporal regression directly output the\nestimation based on single input images, ignoring the relationships between\ndifferent images. In this paper, we propose deep relation learning for\nregression, aiming to learn different relations between a pair of input images.\nFour non-linear relations are considered: \"cumulative relation\", \"relative\nrelation\", \"maximal relation\" and \"minimal relation\". These four relations are\nlearned simultaneously from one deep neural network which has two parts:\nfeature extraction and relation regression. We use an efficient convolutional\nneural network to extract deep features from the pair of input images and apply\na Transformer for relation learning. The proposed method is evaluated on a\nmerged dataset with 6,049 subjects with ages of 0-97 years using 5-fold\ncross-validation for the task of brain age estimation. The experimental results\nhave shown that the proposed method achieved a mean absolute error (MAE) of\n2.38 years, which is lower than the MAEs of 8 other state-of-the-art algorithms\nwith statistical significance (p$&lt;$0.05) in paired T-test (two-side).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Sheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yanfang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yangming Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Metrical Reconstruction of Human Faces. (arXiv:2204.06607v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06607","description":"<p>Face reconstruction and tracking is a building block of numerous applications\nin AR/VR, human-machine interaction, as well as medical applications. Most of\nthese applications rely on a metrically correct prediction of the shape,\nespecially, when the reconstructed subject is put into a metrical context\n(i.e., when there is a reference object of known size). A metrical\nreconstruction is also needed for any application that measures distances and\ndimensions of the subject (e.g., to virtually fit a glasses frame).\nState-of-the-art methods for face reconstruction from a single image are\ntrained on large 2D image datasets in a self-supervised fashion. However, due\nto the nature of a perspective projection they are not able to reconstruct the\nactual face dimensions, and even predicting the average human face outperforms\nsome of these methods in a metrical sense. To learn the actual shape of a face,\nwe argue for a supervised training scheme. Since there exists no large-scale 3D\ndataset for this task, we annotated and unified small- and medium-scale\ndatabases. The resulting unified dataset is still a medium-scale dataset with\nmore than 2k identities and training purely on it would lead to overfitting. To\nthis end, we take advantage of a face recognition network pretrained on a\nlarge-scale 2D image dataset, which provides distinct features for different\nfaces and is robust to expression, illumination, and camera changes. Using\nthese features, we train our face shape estimator in a supervised fashion,\ninheriting the robustness and generalization of the face recognition network.\nOur method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art\nreconstruction methods by a large margin, both on current non-metric benchmarks\nas well as on our metric benchmarks (15% and 24% lower average error on NoW,\nrespectively).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zielonka_W/0/1/0/all/0/1\">Wojciech Zielonka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolkart_T/0/1/0/all/0/1\">Timo Bolkart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1\">Justus Thies</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Memory Management for Video Object Segmentation. (arXiv:2204.06626v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06626","description":"<p>Matching-based networks have achieved state-of-the-art performance for video\nobject segmentation (VOS) tasks by storing every-k frames in an external memory\nbank for future inference. Storing the intermediate frames' predictions\nprovides the network with richer cues for segmenting an object in the current\nframe. However, the size of the memory bank gradually increases with the length\nof the video, which slows down inference speed and makes it impractical to\nhandle arbitrary length videos.\n</p>\n<p>This paper proposes an adaptive memory bank strategy for matching-based\nnetworks for semi-supervised video object segmentation (VOS) that can handle\nvideos of arbitrary length by discarding obsolete features. Features are\nindexed based on their importance in the segmentation of the objects in\nprevious frames. Based on the index, we discard unimportant features to\naccommodate new features. We present our experiments on DAVIS 2016, DAVIS 2017,\nand Youtube-VOS that demonstrate that our method outperforms state-of-the-art\nthat employ first-and-latest strategy with fixed-sized memory banks and\nachieves comparable performance to the every-k strategy with increasing-sized\nmemory banks. Furthermore, experiments show that our method increases inference\nspeed by up to 80% over the every-k and 35% over first-and-latest strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pourganjalikhan_A/0/1/0/all/0/1\">Ali Pourganjalikhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poullis_C/0/1/0/all/0/1\">Charalambos Poullis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Approach for Optimum-Path Forest Classification Using Fuzzy Logic. (arXiv:2204.06635v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06635","description":"<p>In the past decades, fuzzy logic has played an essential role in many\nresearch areas. Alongside, graph-based pattern recognition has shown to be of\ngreat importance due to its flexibility in partitioning the feature space using\nthe background from graph theory. Some years ago, a new framework for both\nsupervised, semi-supervised, and unsupervised learning named Optimum-Path\nForest (OPF) was proposed with competitive results in several applications,\nbesides comprising a low computational burden. In this paper, we propose the\nFuzzy Optimum-Path Forest, an improved version of the standard OPF classifier\nthat learns the samples' membership in an unsupervised fashion, which are\nfurther incorporated during supervised training. Such information is used to\nidentify the most relevant training samples, thus improving the classification\nstep. Experiments conducted over twelve public datasets highlight the\nrobustness of the proposed approach, which behaves similarly to standard OPF in\nworst-case scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souza_R/0/1/0/all/0/1\">Renato W. R. de Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1\">Jo&#xe3;o V. C. de Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passos_L/0/1/0/all/0/1\">Leandro A. Passos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Weiping Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1\">Jo&#xe3;o P. Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albuquerque_V/0/1/0/all/0/1\">Victor Hugo C. de Albuquerque</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wassmap: Wasserstein Isometric Mapping for Image Manifold Learning. (arXiv:2204.06645v1 [cs.LG])","link":"http://arxiv.org/abs/2204.06645","description":"<p>In this paper, we propose Wasserstein Isometric Mapping (Wassmap), a\nparameter-free nonlinear dimensionality reduction technique that provides\nsolutions to some drawbacks in existing global nonlinear dimensionality\nreduction algorithms in imaging applications. Wassmap represents images via\nprobability measures in Wasserstein space, then uses pairwise quadratic\nWasserstein distances between the associated measures to produce a\nlow-dimensional, approximately isometric embedding. We show that the algorithm\nis able to exactly recover parameters of some image manifolds including those\ngenerated by translations or dilations of a fixed generating measure.\nAdditionally, we show that a discrete version of the algorithm retrieves\nparameters from manifolds generated from discrete measures by providing a\ntheoretical bridge to transfer recovery results from functional data to\ndiscrete data. Testing of the proposed algorithms on various image data\nmanifolds show that Wassmap yields good embeddings compared with other global\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henscheid_N/0/1/0/all/0/1\">Nick Henscheid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Shujie Kang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A deep learning algorithm for reducing false positives in screening mammography. (arXiv:2204.06671v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06671","description":"<p>Screening mammography improves breast cancer outcomes by enabling early\ndetection and treatment. However, false positive callbacks for additional\nimaging from screening exams cause unnecessary procedures, patient anxiety, and\nfinancial burden. This work demonstrates an AI algorithm that reduces false\npositives by identifying mammograms not suspicious for breast cancer. We\ntrained the algorithm to determine the absence of cancer using 123,248 2D\ndigital mammograms (6,161 cancers) and performed a retrospective study on\n14,831 screening exams (1,026 cancers) from 15 US and 3 UK sites. Retrospective\nevaluation of the algorithm on the largest of the US sites (11,592 mammograms,\n101 cancers) a) left the cancer detection rate unaffected (p=0.02,\nnon-inferiority margin 0.25 cancers per 1000 exams), b) reduced callbacks for\ndiagnostic exams by 31.1% compared to standard clinical readings, c) reduced\nbenign needle biopsies by 7.4%, and d) reduced screening exams requiring\nradiologist interpretation by 41.6% in the simulated clinical workflow. This\nwork lays the foundation for semi-autonomous breast cancer screening systems\nthat could benefit patients and healthcare systems by reducing false positives,\nunnecessary procedures, patient anxiety, and expenses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pedemonte_S/0/1/0/all/0/1\">Stefano Pedemonte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsue_T/0/1/0/all/0/1\">Trevor Tsue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mombourquette_B/0/1/0/all/0/1\">Brent Mombourquette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_Y/0/1/0/all/0/1\">Yen Nhi Truong Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matthews_T/0/1/0/all/0/1\">Thomas Matthews</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoil_R/0/1/0/all/0/1\">Rodrigo Morales Hoil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1\">Meet Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghare_N/0/1/0/all/0/1\">Nikita Ghare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zingman_Daniels_N/0/1/0/all/0/1\">Naomi Zingman-Daniels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holley_S/0/1/0/all/0/1\">Susan Holley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appleton_C/0/1/0/all/0/1\">Catherine M. Appleton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jason Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahl_R/0/1/0/all/0/1\">Richard L. Wahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Understanding of Sketches. (arXiv:2204.06675v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06675","description":"<p>Sketching is used as a ubiquitous tool of expression by novices and experts\nalike. In this thesis I explore two methods that help a system provide a\ngeometric machine-understanding of sketches, and in-turn help a user accomplish\na downstream task.\n</p>\n<p>The first work deals with interpretation of a 2D-line drawing as a graph\nstructure, and also illustrates its effectiveness through its physical\nreconstruction by a robot. We setup a two-step pipeline to solve the problem.\nFormerly, we estimate the vertices of the graph with sub-pixel level accuracy.\nWe achieve this using a combination of deep convolutional neural networks\nlearned under a supervised setting for pixel-level estimation followed by the\nconnected component analysis for clustering. Later we follow it up with a\nfeedback-loop-based edge estimation method. To complement the\ngraph-interpretation, we further perform data-interchange to a robot legible\nASCII format, and thus teach a robot to replicate a line drawing.\n</p>\n<p>In the second work, we test the 3D-geometric understanding of a sketch-based\nsystem without explicit access to the information about 3D-geometry. The\nobjective is to complete a contour-like sketch of a 3D-object, with\nillumination and texture information. We propose a data-driven approach to\nlearn a conditional distribution modelled as deep convolutional neural networks\nto be trained under an adversarial setting; and we validate it against a\nhuman-in-the-loop. The method itself is further supported by synthetic data\ngeneration using constructive solid geometry following a standard graphics\npipeline. In order to validate the efficacy of our method, we design a\nuser-interface plugged into a popular sketch-based workflow, and setup a simple\ntask-based exercise, for an artist. Thereafter, we also discover that\nform-exploration is an additional utility of our application.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Venkataramaiyer_R/0/1/0/all/0/1\">Raghav Brahmadesam Venkataramaiyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MINSU (Mobile Inventory And Scanning Unit):Computer Vision and AI. (arXiv:2204.06681v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06681","description":"<p>The MINSU(Mobile Inventory and Scanning Unit) algorithm uses the\ncomputational vision analysis method to record the residual quantity/fullness\nof the cabinet. To do so, it goes through a five-step method: object detection,\nforeground subtraction, K-means clustering, percentage estimation, and\ncounting. The input image goes through the object detection method to analyze\nthe specific position of the cabinets in terms of coordinates. After doing so,\nit goes through the foreground subtraction method to make the image more\nfocus-able to the cabinet itself by removing the background (some manual work\nmay have to be done such as selecting the parts that were not grab cut by the\nalgorithm). In the K-means clustering method, the multi-colored image turns\ninto a 3 colored monotonous image for quicker and more accurate analysis. At\nlast, the image goes through percentage estimation and counting. In these two\nmethods, the proportion that the material inside the cabinet is found in\npercentage which then is used to approximate the number of materials inside.\nHad this project been successful, the residual quantity management could solve\nthe problem addressed earlier in the introduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_J/0/1/0/all/0/1\">Jihoon Ryoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1\">Byungkon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongyeob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seunghyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngho Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HASA: Hybrid Architecture Search with Aggregation Strategy for Echinococcosis Classification and Ovary Segmentation in Ultrasound Images. (arXiv:2204.06697v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06697","description":"<p>Different from handcrafted features, deep neural networks can automatically\nlearn task-specific features from data. Due to this data-driven nature, they\nhave achieved remarkable success in various areas. However, manual design and\nselection of suitable network architectures are time-consuming and require\nsubstantial effort of human experts. To address this problem, researchers have\nproposed neural architecture search (NAS) algorithms which can automatically\ngenerate network architectures but suffer from heavy computational cost and\ninstability if searching from scratch. In this paper, we propose a hybrid NAS\nframework for ultrasound (US) image classification and segmentation. The hybrid\nframework consists of a pre-trained backbone and several searched cells (i.e.,\nnetwork building blocks), which takes advantage of the strengths of both NAS\nand the expert knowledge from existing convolutional neural networks.\nSpecifically, two effective and lightweight operations, a mixed depth-wise\nconvolution operator and a squeeze-and-excitation block, are introduced into\nthe candidate operations to enhance the variety and capacity of the searched\ncells. These two operations not only decrease model parameters but also boost\nnetwork performance. Moreover, we propose a re-aggregation strategy for the\nsearched cells, aiming to further improve the performance for different vision\ntasks. We tested our method on two large US image datasets, including a 9-class\nechinococcosis dataset containing 9566 images for classification and an ovary\ndataset containing 3204 images for segmentation. Ablation experiments and\ncomparison with other handcrafted or automatically searched architectures\ndemonstrate that our method can generate more powerful and lightweight models\nfor the above US image classification and segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1\">Jikuan Qian</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1\">Mingyuan Luo</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehui Lin</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1\">Wenhui Hong</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Ruobing Huang</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Haining Fan</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a> (1,2 and 3), <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a> (1,2 and 3) ((1) aNational-Regional Key Technology Engineering Laboratory for Medical Ultrasound, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China, (2) Medical Ultrasound Image Computing (MUSIC) Laboratory, Shenzhen University, Shenzhen, China, (3) Marshall Laboratory of Biomedical Engineering, Shenzhen University, Shenzhen, China, (4) Qinghai University Affiliated Hospital, Xining, Qinghai, China)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Convolutional Neural Networks in Frequency Domain. (arXiv:2204.06718v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06718","description":"<p>Convolutional neural network (CNN) achieves impressive success in the field\nof computer vision during the past few decades. As the core of CNNs, image\nconvolution operation helps CNNs to achieve good performance on image-related\ntasks. However, image convolution is hard to be implemented and parallelized.\nIn this paper, we propose a novel neural network model, namely CEMNet, that can\nbe trained in frequency domain. The most important motivation of this research\nis that we can use the very simple element-wise multiplication operation to\nreplace the image convolution in frequency domain based on Cross-Correlation\nTheorem. We further introduce Weight Fixation Mechanism to alleviate\nover-fitting, and analyze the working behavior of Batch Normalization, Leaky\nReLU and Dropout in frequency domain to design their counterparts for CEMNet.\nAlso, to deal with complex inputs brought by DFT, we design two branch network\nstructure for CEMNet. Experimental results imply that CEMNet works well in\nfrequency domain, and achieve good performance on MNIST and CIFAR-10 databases.\nTo our knowledge, CEMNet is the first model trained in Fourier Domain that\nachieves more than 70\\% validation accuracy on CIFAR-10 database.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hengyue Pan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Information fusion approach for biomass estimation in a plateau mountainous forest using a synergistic system comprising UAS-based digital camera and LiDAR. (arXiv:2204.06746v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06746","description":"<p>Forest land plays a vital role in global climate, ecosystems, farming and\nhuman living environments. Therefore, forest biomass estimation methods are\nnecessary to monitor changes in the forest structure and function, which are\nkey data in natural resources research. Although accurate forest biomass\nmeasurements are important in forest inventory and assessments, high-density\nmeasurements that involve airborne light detection and ranging (LiDAR) at a low\nflight height in large mountainous areas are highly expensive. The objective of\nthis study was to quantify the aboveground biomass (AGB) of a plateau\nmountainous forest reserve using a system that synergistically combines an\nunmanned aircraft system (UAS)-based digital aerial camera and LiDAR to\nleverage their complementary advantages. In this study, we utilized digital\naerial photogrammetry (DAP), which has the unique advantages of speed, high\nspatial resolution, and low cost, to compensate for the deficiency of forestry\ninventory using UAS-based LiDAR that requires terrain-following flight for\nhigh-resolution data acquisition. Combined with the sparse LiDAR points\nacquired by using a high-altitude and high-speed UAS for terrain extraction,\ndense normalized DAP point clouds can be obtained to produce an accurate and\nhigh-resolution canopy height model (CHM). Based on the CHM and spectral\nattributes obtained from multispectral images, we estimated and mapped the AGB\nof the region of interest with considerable cost efficiency. Our study supports\nthe development of predictive models for large-scale wall-to-wall AGB mapping\nby leveraging the complementarity between DAP and LiDAR measurements. This work\nalso reveals the potential of utilizing a UAS-based digital camera and LiDAR\nsynergistically in a plateau mountainous forest area.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huang_R/0/1/0/all/0/1\">Rong Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_W/0/1/0/all/0/1\">Wei Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_L/0/1/0/all/0/1\">Lin Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_X/0/1/0/all/0/1\">Xin Shen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation with Implicit Pseudo Supervision for Semantic Segmentation. (arXiv:2204.06747v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06747","description":"<p>Pseudo-labelling is a popular technique in unsuper-vised domain adaptation\nfor semantic segmentation. However, pseudo labels are noisy and inevitably have\nconfirmation bias due to the discrepancy between source and target domains and\ntraining process. In this paper, we train the model by the pseudo labels which\nare implicitly produced by itself to learn new complementary knowledge about\ntarget domain. Specifically, we propose a tri-learning architecture, where\nevery two branches produce the pseudo labels to train the third one. And we\nalign the pseudo labels based on the similarity of the probability\ndistributions for each two branches. To further implicitly utilize the pseudo\nlabels, we maximize the distances of features for different classes and\nminimize the distances for the same classes by triplet loss. Extensive\nexperiments on GTA5 to Cityscapes and SYNTHIA to Cityscapes tasks show that the\nproposed method has considerable improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wanyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zengmao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bian_W/0/1/0/all/0/1\">Wei Bian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RecurSeed and CertainMix for Weakly Supervised Semantic Segmentation. (arXiv:2204.06754v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06754","description":"<p>Although weakly supervised semantic segmentation using only image-level\nlabels (WSSS-IL) is potentially useful, its low performance and implementation\ncomplexity still limit its application. The main causes are (a) non-detection\nand (b) false-detection phenomena: (a) The class activation maps refined from\nexisting WSSS-IL methods still only represent partial regions for large-scale\nobjects, and (b) for small-scale objects, over-activation causes them to\ndeviate from the object edges. We propose RecurSeed which alternately reduces\nnon- and false-detections through recursive iterations, thereby implicitly\nfinding an optimal junction that minimizes both errors. To maximize the\neffectiveness of RecurSeed, we also propose a novel data augmentation (DA)\napproach called CertainMix, which virtually creates object masks and further\nexpresses their edges in combining the segmentation results, thereby obtaining\na new DA method effectively reflecting object existence reliability through the\nspatial information. We achieved new state-of-the-art performances on both the\nPASCAL VOC 2012 and MS COCO 2014 benchmarks (VOC val 72.4%, COCO val 45.0%).\nThe code is available at https://github.com/OFRIN/RecurSeed_and_CertainMix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Sang Hyun Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_I/0/1/0/all/0/1\">In Jae Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyung-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-performance Evolutionary Algorithms for Online Neuron Control. (arXiv:2204.06765v1 [cs.NE])","link":"http://arxiv.org/abs/2204.06765","description":"<p>Recently, optimization has become an emerging tool for neuroscientists to\nstudy neural code. In the visual system, neurons respond to images with graded\nand noisy responses. Image patterns eliciting highest responses are diagnostic\nof the coding content of the neuron. To find these patterns, we have used\nblack-box optimizers to search a 4096d image space, leading to the evolution of\nimages that maximize neuronal responses. Although genetic algorithm (GA) has\nbeen commonly used, there haven't been any systematic investigations to reveal\nthe best performing optimizer or the underlying principles necessary to improve\nthem.\n</p>\n<p>Here, we conducted a large scale in silico benchmark of optimizers for\nactivation maximization and found that Covariance Matrix Adaptation (CMA)\nexcelled in its achieved activation. We compared CMA against GA and found that\nCMA surpassed the maximal activation of GA by 66% in silico and 44% in vivo. We\nanalyzed the structure of Evolution trajectories and found that the key to\nsuccess was not covariance matrix adaptation, but local search towards\ninformative dimensions and an effective step size decay. Guided by these\nprinciples and the geometry of the image manifold, we developed SphereCMA\noptimizer which competed well against CMA, proving the validity of the\nidentified principles. Code available at\nhttps://github.com/Animadversio/ActMax-Optimizer-Dev\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Binxu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_C/0/1/0/all/0/1\">Carlos R. Ponce</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViTOL: Vision Transformer for Weakly Supervised Object Localization. (arXiv:2204.06772v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06772","description":"<p>Weakly supervised object localization (WSOL) aims at predicting object\nlocations in an image using only image-level category labels. Common challenges\nthat image classification models encounter when localizing objects are, (a)\nthey tend to look at the most discriminative features in an image that confines\nthe localization map to a very small region, (b) the localization maps are\nclass agnostic, and the models highlight objects of multiple classes in the\nsame image and, (c) the localization performance is affected by background\nnoise. To alleviate the above challenges we introduce the following simple\nchanges through our proposed method ViTOL. We leverage the vision-based\ntransformer for self-attention and introduce a patch-based attention dropout\nlayer (p-ADL) to increase the coverage of the localization map and a gradient\nattention rollout mechanism to generate class-dependent attention maps. We\nconduct extensive quantitative, qualitative and ablation experiments on the\nImageNet-1K and CUB datasets. We achieve state-of-the-art MaxBoxAcc-V2\nlocalization scores of 70.47% and 73.17% on the two datasets respectively. Code\nis available on https://github.com/Saurav-31/ViTOL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Saurav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_S/0/1/0/all/0/1\">Sourav Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1\">Abhay Rawat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tallamraju_R/0/1/0/all/0/1\">Rahul Tallamraju</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual-Inertial Odometry with Online Calibration of Velocity-Control Based Kinematic Motion Models. (arXiv:2204.06776v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06776","description":"<p>Visual-inertial odometry (VIO) is an important technology for autonomous\nrobots with power and payload constraints. In this paper, we propose a novel\napproach for VIO with stereo cameras which integrates and calibrates the\nvelocity-control based kinematic motion model of wheeled mobile robots online.\nIncluding such a motion model can help to improve the accuracy of VIO. Compared\nto several previous approaches proposed to integrate wheel odometer\nmeasurements for this purpose, our method does not require wheel encoders and\ncan be applied when the robot motion can be modeled with velocity-control based\nkinematic motion model. We use radial basis function (RBF) kernels to\ncompensate for the time delay and deviations between control commands and\nactual robot motion. The motion model is calibrated online by the VIO system\nand can be used as a forward model for motion control and planning. We evaluate\nour approach with data obtained in variously sized indoor environments,\ndemonstrate improvements over a pure VIO method, and evaluate the prediction\naccuracy of the online calibrated model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haolong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stueckler_J/0/1/0/all/0/1\">Joerg Stueckler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Shuffle-Mixer: An Efficient Context-Aware Vision Learner of Transformer-MLP Paradigm for Dense Prediction in Medical Volume. (arXiv:2204.06779v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06779","description":"<p>Dense prediction in medical volume provides enriched guidance for clinical\nanalysis. CNN backbones have met bottleneck due to lack of long-range\ndependencies and global context modeling power. Recent works proposed to\ncombine vision transformer with CNN, due to its strong global capture ability\nand learning capability. However, most works are limited to simply applying\npure transformer with several fatal flaws (i.e., lack of inductive bias, heavy\ncomputation and little consideration for 3D data). Therefore, designing an\nelegant and efficient vision transformer learner for dense prediction in\nmedical volume is promising and challenging. In this paper, we propose a novel\n3D Shuffle-Mixer network of a new Local Vision Transformer-MLP paradigm for\nmedical dense prediction. In our network, a local vision transformer block is\nutilized to shuffle and learn spatial context from full-view slices of\nrearranged volume, a residual axial-MLP is designed to mix and capture\nremaining volume context in a slice-aware manner, and a MLP view aggregator is\nemployed to project the learned full-view rich context to the volume feature in\na view-aware manner. Moreover, an Adaptive Scaled Enhanced Shortcut is proposed\nfor local vision transformer to enhance feature along spatial and channel\ndimensions adaptively, and a CrossMerge is proposed to skip-connects the\nmulti-scale feature appropriately in the pyramid architecture. Extensive\nexperiments demonstrate the proposed model outperforms other state-of-the-art\nmedical dense prediction methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1\">Jianye Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Cheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianbo Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Ming Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Renzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jianhua Yao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explainable Analysis of Deep Learning Methods for SAR Image Classification. (arXiv:2204.06783v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06783","description":"<p>Deep learning methods exhibit outstanding performance in synthetic aperture\nradar (SAR) image interpretation tasks. However, these are black box models\nthat limit the comprehension of their predictions. Therefore, to meet this\nchallenge, we have utilized explainable artificial intelligence (XAI) methods\nfor the SAR image classification task. Specifically, we trained\nstate-of-the-art convolutional neural networks for each polarization format on\nOpenSARUrban dataset and then investigate eight explanation methods to analyze\nthe predictions of the CNN classifiers of SAR images. These XAI methods are\nalso evaluated qualitatively and quantitatively which shows that Occlusion\nachieves the most reliable interpretation performance in terms of\nMax-Sensitivity but with a low-resolution explanation heatmap. The explanation\nresults provide some insights into the internal mechanism of black-box\ndecisions for SAR image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shenghan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Ziteng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1\">Weiwei Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zenghui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1\">Wenxian Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pyramidal Attention for Saliency Detection. (arXiv:2204.06788v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06788","description":"<p>Salient object detection (SOD) extracts meaningful contents from an input\nimage. RGB-based SOD methods lack the complementary depth clues; hence,\nproviding limited performance for complex scenarios. Similarly, RGB-D models\nprocess RGB and depth inputs, but the depth data availability during testing\nmay hinder the model's practical applicability. This paper exploits only RGB\nimages, estimates depth from RGB, and leverages the intermediate depth\nfeatures. We employ a pyramidal attention structure to extract multi-level\nconvolutional-transformer features to process initial stage representations and\nfurther enhance the subsequent ones. At each stage, the backbone transformer\nmodel produces global receptive fields and computing in parallel to attain\nfine-grained global predictions refined by our residual convolutional attention\ndecoder for optimal saliency prediction. We report significantly improved\nperformance against 21 and 40 state-of-the-art SOD methods on eight RGB and\nRGB-D datasets, respectively. Consequently, we present a new SOD perspective of\ngenerating RGB-D SOD without acquiring depth data during training and testing\nand assist RGB methods with depth clues for improved performance. The code and\ntrained models are available at\nhttps://github.com/tanveer-hussain/EfficientSOD2\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hussain_T/0/1/0/all/0/1\">Tanveer Hussain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_A/0/1/0/all/0/1\">Abbas Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baik_S/0/1/0/all/0/1\">Sung Wook Baik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO-Pose: Enhancing YOLO for Multi Person Pose Estimation Using Object Keypoint Similarity Loss. (arXiv:2204.06806v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06806","description":"<p>We introduce YOLO-pose, a novel heatmap-free approach for joint detection,\nand 2D multi-person pose estimation in an image based on the popular YOLO\nobject detection framework. Existing heatmap based two-stage approaches are\nsub-optimal as they are not end-to-end trainable and training relies on a\nsurrogate L1 loss that is not equivalent to maximizing the evaluation metric,\ni.e. Object Keypoint Similarity (OKS). Our framework allows us to train the\nmodel end-to-end and optimize the OKS metric itself. The proposed model learns\nto jointly detect bounding boxes for multiple persons and their corresponding\n2D poses in a single forward pass and thus bringing in the best of both\ntop-down and bottom-up approaches. Proposed approach doesn't require the\npostprocessing of bottom-up approaches to group detected keypoints into a\nskeleton as each bounding box has an associated pose, resulting in an inherent\ngrouping of the keypoints. Unlike top-down approaches, multiple forward passes\nare done away with since all persons are localized along with their pose in a\nsingle inference. YOLO-pose achieves new state-of-the-art results on COCO\nvalidation (90.2% AP50) and test-dev set (90.3% AP50), surpassing all existing\nbottom-up approaches in a single forward pass without flip test, multi-scale\ntesting, or any other test time augmentation. All experiments and results\nreported in this paper are without any test time augmentation, unlike\ntraditional approaches that use flip-test and multi-scale testing to boost\nperformance. Our training codes will be made publicly available at\nhttps://github.com/TexasInstruments/edgeai-yolov5 and\nhttps://github.com/TexasInstruments/edgeai-yolox\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maji_D/0/1/0/all/0/1\">Debapriya Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagori_S/0/1/0/all/0/1\">Soyeb Nagori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1\">Manu Mathew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_D/0/1/0/all/0/1\">Deepak Poddar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable Vertebral Fracture Quantification via Anchor-Free Landmarks Localization. (arXiv:2204.06818v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06818","description":"<p>Vertebral body compression fractures are early signs of osteoporosis. Though\nthese fractures are visible on Computed Tomography (CT) images, they are\nfrequently missed by radiologists in clinical settings. Prior research on\nautomatic methods of vertebral fracture classification proves its reliable\nquality; however, existing methods provide hard-to-interpret outputs and\nsometimes fail to process cases with severe abnormalities such as highly\npathological vertebrae or scoliosis. We propose a new two-step algorithm to\nlocalize the vertebral column in 3D CT images and then detect individual\nvertebrae and quantify fractures in 2D simultaneously. We train neural networks\nfor both steps using a simple 6-keypoints based annotation scheme, which\ncorresponds precisely to the current clinical recommendation. Our algorithm has\nno exclusion criteria, processes 3D CT in 2 seconds on a single GPU, and\nprovides an interpretable and verifiable output. The method approaches\nexpert-level performance and demonstrates state-of-the-art results in vertebrae\n3D localization (the average error is 1 mm), vertebrae 2D detection (precision\nand recall are 0.99), and fracture identification (ROC AUC at the patient level\nis up to 0.96). Our anchor-free vertebra detection network shows excellent\ngeneralizability on a new domain by achieving ROC AUC 0.95, sensitivity 0.85,\nspecificity 0.9 on a challenging VerSe dataset with many unseen vertebra types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zakharov_A/0/1/0/all/0/1\">Alexey Zakharov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pisov_M/0/1/0/all/0/1\">Maxim Pisov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bukharaev_A/0/1/0/all/0/1\">Alim Bukharaev</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petraikin_A/0/1/0/all/0/1\">Alexey Petraikin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morozov_S/0/1/0/all/0/1\">Sergey Morozov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gombolevskiy_V/0/1/0/all/0/1\">Victor Gombolevskiy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1\">Mikhail Belyaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Vehicle Detection in Satellite Video. (arXiv:2204.06828v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06828","description":"<p>This work presents a deep learning approach for vehicle detection in\nsatellite video. Vehicle detection is perhaps impossible in single EO satellite\nimages due to the tininess of vehicles (4-10 pixel) and their similarity to the\nbackground. Instead, we consider satellite video which overcomes the lack of\nspatial information by temporal consistency of vehicle movement. A new\nspatiotemporal model of a compact $3 \\times 3$ convolutional, neural network is\nproposed which neglects pooling layers and uses leaky ReLUs. Then we use a\nreformulation of the output heatmap including Non-Maximum-Suppression (NMS) for\nthe final segmentation. Empirical results on two new annotated satellite videos\nreconfirm the applicability of this approach for vehicle detection. They more\nimportantly indicate that pre-training on WAMI data and then fine-tuning on few\nannotated video frames for a new video is sufficient. In our experiment only\nfive annotated images yield a $F_1$ score of 0.81 on a new video showing more\ncomplex traffic patterns than the Las Vegas video. Our best result on Las Vegas\nis a $F_1$ score of 0.87 which makes the proposed approach a leading method for\nthis benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pflugfelder_R/0/1/0/all/0/1\">Roman Pflugfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weissenfeld_A/0/1/0/all/0/1\">Axel Weissenfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1\">Julian Wagner</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Indirect Illumination for Inverse Rendering. (arXiv:2204.06837v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06837","description":"<p>Recent advances in implicit neural representations and differentiable\nrendering make it possible to simultaneously recover the geometry and materials\nof an object from multi-view RGB images captured under unknown static\nillumination. Despite the promising results achieved, indirect illumination is\nrarely modeled in previous methods, as it requires expensive recursive path\ntracing which makes the inverse rendering computationally intractable. In this\npaper, we propose a novel approach to efficiently recovering spatially-varying\nindirect illumination. The key insight is that indirect illumination can be\nconveniently derived from the neural radiance field learned from input images\ninstead of being estimated jointly with direct illumination and materials. By\nproperly modeling the indirect illumination and visibility of direct\nillumination, interreflection- and shadow-free albedo can be recovered. The\nexperiments on both synthetic and real data demonstrate the superior\nperformance of our approach compared to previous work and its capability to\nsynthesize realistic renderings under novel viewpoints and illumination. Our\ncode and data are available at https://zju3dv.github.io/invrender/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanqing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiaming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xingyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Huan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Rongfei Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiaowei Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OmniPD: One-Step Person Detection in Top-View Omnidirectional Indoor Scenes. (arXiv:2204.06846v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06846","description":"<p>We propose a one-step person detector for topview omnidirectional indoor\nscenes based on convolutional neural networks (CNNs). While state of the art\nperson detectors reach competitive results on perspective images, missing CNN\narchitectures as well as training data that follows the distortion of\nomnidirectional images makes current approaches not applicable to our data. The\nmethod predicts bounding boxes of multiple persons directly in omnidirectional\nimages without perspective transformation, which reduces overhead of pre- and\npost-processing and enables real-time performance. The basic idea is to utilize\ntransfer learning to fine-tune CNNs trained on perspective images with data\naugmentation techniques for detection in omnidirectional images. We fine-tune\ntwo variants of Single Shot MultiBox detectors (SSDs). The first one uses\nMobilenet v1 FPN as feature extractor (moSSD). The second one uses ResNet50 v1\nFPN (resSSD). Both models are pre-trained on Microsoft Common Objects in\nContext (COCO) dataset. We fine-tune both models on PASCAL VOC07 and VOC12\ndatasets, specifically on class person. Random 90-degree rotation and random\nvertical flipping are used for data augmentation in addition to the methods\nproposed by original SSD. We reach an average precision (AP) of 67.3 % with\nmoSSD and 74.9 % with resSSD onthe evaluation dataset. To enhance the\nfine-tuning process, we add a subset of HDA Person dataset and a subset of\nPIROPOdatabase and reduce the number of perspective images to PASCAL VOC07. The\nAP rises to 83.2 % for moSSD and 86.3 % for resSSD, respectively. The average\ninference speed is 28 ms per image for moSSD and 38 ms per image for resSSD\nusing Nvidia Quadro P6000. Our method is applicable to other CNN-based object\ndetectors and can potentially generalize for detecting other objects in\nomnidirectional images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jingrui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_R/0/1/0/all/0/1\">Roman Seidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirtz_G/0/1/0/all/0/1\">Gangolf Hirtz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensuring accurate stain reproduction in deep generative networks for virtual immunohistochemistry. (arXiv:2204.06849v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06849","description":"<p>Immunohistochemistry is a valuable diagnostic tool for cancer pathology.\nHowever, it requires specialist labs and equipment, is time-intensive, and is\ndifficult to reproduce. Consequently, a long term aim is to provide a digital\nmethod of recreating physical immunohistochemical stains. Generative\nAdversarial Networks have become exceedingly advanced at mapping one image type\nto another and have shown promise at inferring immunostains from haematoxylin\nand eosin. However, they have a substantial weakness when used with pathology\nimages as they can fabricate structures that are not present in the original\ndata. CycleGANs can mitigate invented tissue structures in pathology image\nmapping but have a related disposition to generate areas of inaccurate\nstaining. In this paper, we describe a modification to the loss function of a\nCycleGAN to improve its mapping ability for pathology images by enforcing\nrealistic stain replication while retaining tissue structure. Our approach\nimproves upon others by considering structure and staining during model\ntraining. We evaluated our network using the Fr\\'echet Inception distance,\ncoupled with a new technique that we propose to appraise the accuracy of\nvirtual immunohistochemistry. This assesses the overlap between each stain\ncomponent in the inferred and ground truth images through colour deconvolution,\nthresholding and the Sorensen-Dice coefficient. Our modified loss function\nresulted in a Dice coefficient for the virtual stain of 0.78 compared with the\nreal AE1/AE3 slide. This was superior to the unaltered CycleGAN's score of\n0.74. Additionally, our loss function improved the Fr\\'echet Inception distance\nfor the reconstruction to 74.54 from 76.47. We, therefore, describe an advance\nin virtual restaining that can extend to other immunostains and tumour types\nand deliver reproducible, fast and readily accessible immunohistochemistry\nworldwide.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Walsh_C/0/1/0/all/0/1\">Christopher D. Walsh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Edwards_J/0/1/0/all/0/1\">Joanne Edwards</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Insall_R/0/1/0/all/0/1\">Robert H. Insall</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Training to Improve Player and Ball Detection in Soccer. (arXiv:2204.06859v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06859","description":"<p>Accurate player and ball detection has become increasingly important in\nrecent years for sport analytics. As most state-of-the-art methods rely on\ntraining deep learning networks in a supervised fashion, they require huge\namounts of annotated data, which are rarely available. In this paper, we\npresent a novel generic semi-supervised method to train a network based on a\nlabeled image dataset by leveraging a large unlabeled dataset of soccer\nbroadcast videos. More precisely, we design a teacher-student approach in which\nthe teacher produces surrogate annotations on the unlabeled data to be used\nlater for training a student which has the same architecture as the teacher.\nFurthermore, we introduce three training loss parametrizations that allow the\nstudent to doubt the predictions of the teacher during training depending on\nthe proposal confidence score. We show that including unlabeled data in the\ntraining process allows to substantially improve the performances of the\ndetection network trained only on the labeled data. Finally, we provide a\nthorough performance study including different proportions of labeled and\nunlabeled data, and establish the first benchmark on the new SoccerNet-v3\ndetection task, with an mAP of 52.3%. Our code is available at\nhttps://github.com/rvandeghen/SST .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vandeghen_R/0/1/0/all/0/1\">Renaud Vandeghen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cioppa_A/0/1/0/all/0/1\">Anthony Cioppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Identity-Preserved Motion Retargeting in Video Synthesis by Feature Disentanglement. (arXiv:2204.06862v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06862","description":"<p>Most motion retargeting methods in human action video synthesis decompose the\ninput video to motion (dynamic information) and shape (static information).\nHowever, we observe if the dynamic information is directly transferred to\nanother subject, it will result in unnatural synthesised motion. This\nphenomenon is mainly caused by neglecting subject-dependent information in\nmotion. To solve the problem, we propose a novel motion retargeting method\nwhich can combine both subject-independent (common motion content) information\nfrom a source video and subject-dependent (individualized identity motion)\ninformation from a target video. So it can synthesize videos with a much\nnatural appearance along with identity-preserved motion. In the proposed method\ntwo encoders are employed to extract identity and motion content\nrepresentations respectively. We employ the adaptive instance normalization\n(AdaIN) layer in the generator and the instance normalization (IN) layer in the\nmotion content encoder to synthesize the new motion. Besides, we also collected\na dataset, named $Chuang101$, with 101 subjects in total. Each subject performs\nidentical dancing movement, and so it is convenient for feature disentanglement\namong motion and identity of each subject. Furthermore, an efficient\nquantitative metric for identify information is designed by gait recognition.\nThe experiments show the proposed method can synthesize videos more naturally\nwhen the subject's identity is preserved.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jingzhe Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shiqi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clothes-Changing Person Re-identification with RGB Modality Only. (arXiv:2204.06890v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06890","description":"<p>The key to address clothes-changing person re-identification (re-id) is to\nextract clothes-irrelevant features, e.g., face, hairstyle, body shape, and\ngait. Most current works mainly focus on modeling body shape from\nmulti-modality information (e.g., silhouettes and sketches), but do not make\nfull use of the clothes-irrelevant information in the original RGB images. In\nthis paper, we propose a Clothes-based Adversarial Loss (CAL) to mine\nclothes-irrelevant features from the original RGB images by penalizing the\npredictive power of re-id model w.r.t. clothes. Extensive experiments\ndemonstrate that using RGB images only, CAL outperforms all state-of-the-art\nmethods on widely-used clothes-changing person re-id benchmarks. Besides,\ncompared with images, videos contain richer appearance and additional temporal\ninformation, which can be used to model proper spatiotemporal patterns to\nassist clothes-changing re-id. Since there is no publicly available\nclothes-changing video re-id dataset, we contribute a new dataset named CCVID\nand show that there exists much room for improvement in modeling spatiotemporal\ninformation. The code and new dataset are available at:\nhttps://github.com/guxinqian/Simple-CCReID.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xinqian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Hong Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Bingpeng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shutao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xilin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Sample Extension for Unsupervised Person Re-Identification. (arXiv:2204.06892v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06892","description":"<p>Most existing unsupervised person re-identification (Re-ID) methods use\nclustering to generate pseudo labels for model training. Unfortunately,\nclustering sometimes mixes different true identities together or splits the\nsame identity into two or more sub clusters. Training on these noisy clusters\nsubstantially hampers the Re-ID accuracy. Due to the limited samples in each\nidentity, we suppose there may lack some underlying information to well reveal\nthe accurate clusters. To discover these information, we propose an Implicit\nSample Extension (\\OurWholeMethod) method to generate what we call support\nsamples around the cluster boundaries. Specifically, we generate support\nsamples from actual samples and their neighbouring clusters in the embedding\nspace through a progressive linear interpolation (PLI) strategy. PLI controls\nthe generation with two critical factors, i.e., 1) the direction from the\nactual sample towards its K-nearest clusters and 2) the degree for mixing up\nthe context information from the K-nearest clusters. Meanwhile, given the\nsupport samples, ISE further uses a label-preserving loss to pull them towards\ntheir corresponding actual samples, so as to compact each cluster.\nConsequently, ISE reduces the \"sub and mixed\" clustering errors, thus improving\nthe Re-ID performance. Extensive experiments demonstrate that the proposed\nmethod is effective and achieves state-of-the-art performance for unsupervised\nperson Re-ID. Code is available at:\n\\url{https://github.com/PaddlePaddle/PaddleClas}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Javen Qinfeng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhaoxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatial Likelihood Voting with Self-Knowledge Distillation for Weakly Supervised Object Detection. (arXiv:2204.06899v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06899","description":"<p>Weakly supervised object detection (WSOD), which is an effective way to train\nan object detection model using only image-level annotations, has attracted\nconsiderable attention from researchers. However, most of the existing methods,\nwhich are based on multiple instance learning (MIL), tend to localize instances\nto the discriminative parts of salient objects instead of the entire content of\nall objects. In this paper, we propose a WSOD framework called the Spatial\nLikelihood Voting with Self-knowledge Distillation Network (SLV-SD Net). In\nthis framework, we introduce a spatial likelihood voting (SLV) module to\nconverge region proposal localization without bounding box annotations.\nSpecifically, in every iteration during training, all the region proposals in a\ngiven image act as voters voting for the likelihood of each category in the\nspatial dimensions. After dilating the alignment on the area with large\nlikelihood values, the voting results are regularized as bounding boxes, which\nare then used for the final classification and localization. Based on SLV, we\nfurther propose a self-knowledge distillation (SD) module to refine the feature\nrepresentations of the given image. The likelihood maps generated by the SLV\nmodule are used to supervise the feature learning of the backbone network,\nencouraging the network to attend to wider and more diverse areas of the image.\nExtensive experiments on the PASCAL VOC 2007/2012 and MS-COCO datasets\ndemonstrate the excellent performance of SLV-SD Net. In addition, SLV-SD Net\nproduces new state-of-the-art results on these benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Ze Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1\">Zhihang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1\">Mingyuan Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1\">Rongxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xiang Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaowu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-sheng Hua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SoccerNet-Tracking: Multiple Object Tracking Dataset and Benchmark in Soccer Videos. (arXiv:2204.06918v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06918","description":"<p>Tracking objects in soccer videos is extremely important to gather both\nplayer and team statistics, whether it is to estimate the total distance run,\nthe ball possession or the team formation. Video processing can help automating\nthe extraction of those information, without the need of any invasive sensor,\nhence applicable to any team on any stadium. Yet, the availability of datasets\nto train learnable models and benchmarks to evaluate methods on a common\ntestbed is very limited. In this work, we propose a novel dataset for multiple\nobject tracking composed of 200 sequences of 30s each, representative of\nchallenging soccer scenarios, and a complete 45-minutes half-time for long-term\ntracking. The dataset is fully annotated with bounding boxes and tracklet IDs,\nenabling the training of MOT baselines in the soccer domain and a full\nbenchmarking of those methods on our segregated challenge sets. Our analysis\nshows that multiple player, referee and ball tracking in soccer videos is far\nfrom being solved, with several improvement required in case of fast motion or\nin scenarios of severe occlusion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cioppa_A/0/1/0/all/0/1\">Anthony Cioppa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1\">Silvio Giancola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deliege_A/0/1/0/all/0/1\">Adrien Deliege</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droogenbroeck_M/0/1/0/all/0/1\">Marc Van Droogenbroeck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sketch guided and progressive growing GAN for realistic and editable ultrasound image synthesis. (arXiv:2204.06929v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06929","description":"<p>Ultrasound (US) imaging is widely used for anatomical structure inspection in\nclinical diagnosis. The training of new sonographers and deep learning based\nalgorithms for US image analysis usually requires a large amount of data.\nHowever, obtaining and labeling large-scale US imaging data are not easy tasks,\nespecially for diseases with low incidence. Realistic US image synthesis can\nalleviate this problem to a great extent. In this paper, we propose a\ngenerative adversarial network (GAN) based image synthesis framework. Our main\ncontributions include: 1) we present the first work that can synthesize\nrealistic B-mode US images with high-resolution and customized texture editing\nfeatures; 2) to enhance structural details of generated images, we propose to\nintroduce auxiliary sketch guidance into a conditional GAN. We superpose the\nedge sketch onto the object mask and use the composite mask as the network\ninput; 3) to generate high-resolution US images, we adopt a progressive\ntraining strategy to gradually generate high-resolution images from\nlow-resolution images. In addition, a feature loss is proposed to minimize the\ndifference of high-level features between the generated and real images, which\nfurther improves the quality of generated images; 4) the proposed US image\nsynthesis method is quite universal and can also be generalized to the US\nimages of other anatomical structures besides the three ones tested in our\nstudy (lung, hip joint, and ovary); 5) extensive experiments on three large US\nimage datasets are conducted to validate our method. Ablation studies,\ncustomized texture editing, user studies, and segmentation tests demonstrate\npromising results of our method in synthesizing realistic US images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liang_J/0/1/0/all/0/1\">Jiamin Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yuhao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haoming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_S/0/1/0/all/0/1\">Shuangchi He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1\">Xindi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zejian Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xue_W/0/1/0/all/0/1\">Wufeng Xue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_J/0/1/0/all/0/1\">Jun Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometric Deep Learning to Identify the Critical 3D Structural Features of the Optic Nerve Head for Glaucoma Diagnosis. (arXiv:2204.06931v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06931","description":"<p>Purpose: The optic nerve head (ONH) undergoes complex and deep 3D\nmorphological changes during the development and progression of glaucoma.\nOptical coherence tomography (OCT) is the current gold standard to visualize\nand quantify these changes, however the resulting 3D deep-tissue information\nhas not yet been fully exploited for the diagnosis and prognosis of glaucoma.\nTo this end, we aimed: (1) To compare the performance of two relatively recent\ngeometric deep learning techniques in diagnosing glaucoma from a single OCT\nscan of the ONH; and (2) To identify the 3D structural features of the ONH that\nare critical for the diagnosis of glaucoma.\n</p>\n<p>Methods: In this study, we included a total of 2,247 non-glaucoma and 2,259\nglaucoma scans from 1,725 subjects. All subjects had their ONHs imaged in 3D\nwith Spectralis OCT. All OCT scans were automatically segmented using deep\nlearning to identify major neural and connective tissues. Each ONH was then\nrepresented as a 3D point cloud. We used PointNet and dynamic graph\nconvolutional neural network (DGCNN) to diagnose glaucoma from such 3D ONH\npoint clouds and to identify the critical 3D structural features of the ONH for\nglaucoma diagnosis.\n</p>\n<p>Results: Both the DGCNN (AUC: 0.97$\\pm$0.01) and PointNet (AUC:\n0.95$\\pm$0.02) were able to accurately detect glaucoma from 3D ONH point\nclouds. The critical points formed an hourglass pattern with most of them\nlocated in the inferior and superior quadrant of the ONH.\n</p>\n<p>Discussion: The diagnostic accuracy of both geometric deep learning\napproaches was excellent. Moreover, we were able to identify the critical 3D\nstructural features of the ONH for glaucoma diagnosis that tremendously\nimproved the transparency and interpretability of our method. Consequently, our\napproach may have strong potential to be used in clinical applications for the\ndiagnosis and prognosis of a wide range of ophthalmic disorders.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Braeu_F/0/1/0/all/0/1\">Fabian A. Braeu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thiery_A/0/1/0/all/0/1\">Alexandre H. Thi&#xe9;ry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tun_T/0/1/0/all/0/1\">Tin A. Tun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kadziauskiene_A/0/1/0/all/0/1\">Aiste Kadziauskiene</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barbastathis_G/0/1/0/all/0/1\">George Barbastathis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aung_T/0/1/0/all/0/1\">Tin Aung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Girard_M/0/1/0/all/0/1\">Micha&#xeb;l J.A. Girard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEHAVE: Dataset and Method for Tracking Human Object Interactions. (arXiv:2204.06950v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06950","description":"<p>Modelling interactions between humans and objects in natural environments is\ncentral to many applications including gaming, virtual and mixed reality, as\nwell as human behavior analysis and human-robot collaboration. This challenging\noperation scenario requires generalization to vast number of objects, scenes,\nand human actions. Unfortunately, there exist no such dataset. Moreover, this\ndata needs to be acquired in diverse natural environments, which rules out 4D\nscanners and marker based capture systems. We present BEHAVE dataset, the first\nfull body human- object interaction dataset with multi-view RGBD frames and\ncorresponding 3D SMPL and object fits along with the annotated contacts between\nthem. We record around 15k frames at 5 locations with 8 subjects performing a\nwide range of interactions with 20 common objects. We use this data to learn a\nmodel that can jointly track humans and objects in natural environments with an\neasy-to-use portable multi-camera setup. Our key insight is to predict\ncorrespondences from the human and the object to a statistical body model to\nobtain human-object contacts during interactions. Our approach can record and\ntrack not just the humans and objects but also their interactions, modeled as\nsurface contacts, in 3D. Our code and data can be found at:\n<a href=\"http://virtualhumans.mpi-inf.mpg.de/behave\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1\">Bharat Lal Bhatnagar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xianghui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_I/0/1/0/all/0/1\">Ilya A. Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1\">Gerard Pons-Moll</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Deep Learning Meets Chan-Vese Model. (arXiv:2204.06951v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06951","description":"<p>The Chan-Vese (CV) model is a classic region-based method in image\nsegmentation. However, its piecewise constant assumption does not always hold\nfor practical applications. Many improvements have been proposed but the issue\nis still far from well solved. In this work, we propose an unsupervised image\nsegmentation approach that integrates the CV model with deep neural networks,\nwhich significantly improves the original CV model's segmentation accuracy. Our\nbasic idea is to apply a deep neural network that maps the image into a latent\nspace to alleviate the violation of the piecewise constant assumption in image\nspace. We formulate this idea under the classic Bayesian framework by\napproximating the likelihood with an evidence lower bound (ELBO) term while\nkeeping the prior term in the CV model. Thus, our model only needs the input\nimage itself and does not require pre-training from external datasets.\nMoreover, we extend the idea to multi-phase case and dataset based unsupervised\nimage segmentation. Extensive experiments validate the effectiveness of our\nmodel and show that the proposed method is noticeably better than other\nunsupervised segmentation approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1\">Dihan Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_C/0/1/0/all/0/1\">Chenglong Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zuoqiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kaisheng Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LEFM-Nets: Learnable Explicit Feature Map Deep Networks for Segmentation of Histopathological Images of Frozen Sections. (arXiv:2204.06955v1 [eess.IV])","link":"http://arxiv.org/abs/2204.06955","description":"<p>Accurate segmentation of medical images is essential for diagnosis and\ntreatment of diseases. These problems are solved by highly complex models, such\nas deep networks (DN), requiring a large amount of labeled data for training.\nThereby, many DNs possess task- or imaging modality specific architectures with\na decision-making process that is often hard to explain and interpret. Here, we\npropose a framework that embeds existing DNs into a low-dimensional subspace\ninduced by the learnable explicit feature map (LEFM) layer. Compared to the\nexisting DN, the framework adds one hyperparameter and only modestly increase\nthe number of learnable parameters. The method is aimed at, but not limited to,\nsegmentation of low-dimensional medical images, such as color histopathological\nimages of stained frozen sections. Since features in the LEFM layer are\npolynomial functions of the original features, proposed LEFM-Nets contribute to\nthe interpretability of network decisions. In this work, we combined LEFM with\nthe known networks: DeepLabv3+, UNet, UNet++ and MA-net. New LEFM-Nets are\napplied to the segmentation of adenocarcinoma of a colon in a liver from images\nof hematoxylin and eosin (H&amp;E) stained frozen sections. LEFM-Nets are also\ntested on nuclei segmentation from images of H&amp;E stained frozen sections of ten\nhuman organs. On the first problem, LEFM-Nets achieved statistically\nsignificant performance improvement in terms of micro balanced accuracy and\n$F_1$ score than original networks. LEFM-Nets achieved only better performance\nin comparison with the original networks on the second problem. The source code\nis available at https://github.com/dsitnik/lefm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Sitnik_D/0/1/0/all/0/1\">Dario Sitnik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kopriva_I/0/1/0/all/0/1\">Ivica Kopriva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The multi-modal universe of fast-fashion: the Visuelle 2.0 benchmark. (arXiv:2204.06972v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06972","description":"<p>We present Visuelle 2.0, the first dataset useful for facing diverse\nprediction problems that a fast-fashion company has to manage routinely.\nFurthermore, we demonstrate how the use of computer vision is substantial in\nthis scenario. Visuelle 2.0 contains data for 6 seasons / 5355 clothing\nproducts of Nuna Lie, a famous Italian company with hundreds of shops located\nin different areas within the country. In particular, we focus on a specific\nprediction problem, namely short-observation new product sale forecasting\n(SO-fore). SO-fore assumes that the season has started and a set of new\nproducts is on the shelves of the different stores. The goal is to forecast the\nsales for a particular horizon, given a short, available past (few weeks),\nsince no earlier statistics are available. To be successful, SO-fore approaches\nshould capture this short past and exploit other modalities or exogenous data.\nTo these aims, Visuelle 2.0 is equipped with disaggregated data at the\nitem-shop level and multi-modal information for each clothing item, allowing\ncomputer vision approaches to come into play. The main message that we deliver\nis that the use of image data with deep networks boosts performances obtained\nwhen using the time series in long-term forecasting scenarios, ameliorating the\nWAPE by 8.2% and the MAE by 7.7%. The dataset is available at:\nhttps://humaticslab.github.io/forecasting/visuelle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skenderi_G/0/1/0/all/0/1\">Geri Skenderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joppi_C/0/1/0/all/0/1\">Christian Joppi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denitto_M/0/1/0/all/0/1\">Matteo Denitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scarpa_B/0/1/0/all/0/1\">Berniero Scarpa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cristani_M/0/1/0/all/0/1\">Marco Cristani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HyDe: The First Open-Source, Python-Based, GPU-Accelerated Hyperspectral Denoising Package. (arXiv:2204.06979v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06979","description":"<p>As with any physical instrument, hyperspectral cameras induce different kinds\nof noise in the acquired data. Therefore, Hyperspectral denoising is a crucial\nstep for analyzing hyperspectral images (HSIs). Conventional computational\nmethods rarely use GPUs to improve efficiency and are not fully open-source.\nAlternatively, deep learning-based methods are often open-source and use GPUs,\nbut their training and utilization for real-world applications remain\nnon-trivial for many researchers. Consequently, we propose HyDe: the first\nopen-source, GPU-accelerated Python-based, hyperspectral image denoising\ntoolbox, which aims to provide a large set of methods with an easy-to-use\nenvironment. HyDe includes a variety of methods ranging from low-rank\nwavelet-based methods to deep neural network (DNN) models. HyDe's interface\ndramatically improves the interoperability of these methods and the performance\nof the underlying functions. In fact, these methods maintain similar HSI\ndenoising performance to their original implementations while consuming nearly\nten times less energy. Furthermore, we present a method for training DNNs for\ndenoising HSIs which are not spatially related to the training dataset, i.e.,\ntraining on ground-level HSIs for denoising HSIs with other perspectives\nincluding airborne, drone-borne, and space-borne. To utilize the trained DNNs,\nwe show a sliding window method to effectively denoise HSIs which would\notherwise require more than 40 GB. The package can be found at:\n\\url{https://github.com/Helmholtz-AI-Energy/HyDe}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Coquelin_D/0/1/0/all/0/1\">Daniel Coquelin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1\">Behnood Rasti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gotz_M/0/1/0/all/0/1\">Markus G&#xf6;tz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1\">Pedram Ghamisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gloaguen_R/0/1/0/all/0/1\">Richard Gloaguen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Streit_A/0/1/0/all/0/1\">Achim Streit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Image Relational Knowledge Distillation for Semantic Segmentation. (arXiv:2204.06986v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06986","description":"<p>Current Knowledge Distillation (KD) methods for semantic segmentation often\nguide the student to mimic the teacher's structured information generated from\nindividual data samples. However, they ignore the global semantic relations\namong pixels across various images that are valuable for KD. This paper\nproposes a novel Cross-Image Relational KD (CIRKD), which focuses on\ntransferring structured pixel-to-pixel and pixel-to-region relations among the\nwhole images. The motivation is that a good teacher network could construct a\nwell-structured feature space in terms of global pixel dependencies. CIRKD\nmakes the student mimic better structured semantic relations from the teacher,\nthus improving the segmentation performance. Experimental results over\nCityscapes, CamVid and Pascal VOC datasets demonstrate the effectiveness of our\nproposed approach against state-of-the-art distillation methods. The code is\navailable at https://github.com/winycg/CIRKD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chuanguang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Helong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1\">Zhulin An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xue Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yongjun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Atmospheric Turbulence Removal with Complex-Valued Convolutional Neural Network. (arXiv:2204.06989v1 [cs.CV])","link":"http://arxiv.org/abs/2204.06989","description":"<p>Atmospheric turbulence distorts visual imagery and is always problematic for\ninformation interpretation by both human and machine. Most well-developed\napproaches to remove atmospheric turbulence distortion are model-based.\nHowever, these methods require high computation and large memory preventing\ntheir feasibility of real-time operation. Deep learning-based approaches have\nhence gained more attention but currently work efficiently only on static\nscenes. This paper presents a novel learning-based framework offering short\ntemporal spanning to support dynamic scenes. We exploit complex-valued\nconvolutions as phase information, altered by atmospheric turbulence, is\ncaptured better than using ordinary real-valued convolutions. Two concatenated\nmodules are proposed. The first module aims to remove geometric distortions\nand, if enough memory, the second module is applied to refine micro details of\nthe videos. Experimental results show that our proposed framework efficiently\nmitigate the atmospheric turbulence distortion and significantly outperforms\nthe existing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Application of Geometric Deep Learning for the Diagnosis of Glaucoma. (arXiv:2204.07004v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07004","description":"<p>Purpose: (1) To assess the performance of geometric deep learning (PointNet)\nin diagnosing glaucoma from a single optical coherence tomography (OCT) 3D scan\nof the optic nerve head (ONH); (2) To compare its performance to that obtained\nwith a standard 3D convolutional neural network (CNN), and with a gold-standard\nglaucoma parameter, i.e. retinal nerve fiber layer (RNFL) thickness.\n</p>\n<p>Methods: 3D raster scans of the ONH were acquired with Spectralis OCT for 477\nglaucoma and 2,296 non-glaucoma subjects at the Singapore National Eye Centre.\nAll volumes were automatically segmented using deep learning to identify 7\nmajor neural and connective tissues including the RNFL, the prelamina, and the\nlamina cribrosa (LC). Each ONH was then represented as a 3D point cloud with\n1,000 points chosen randomly from all tissue boundaries. To simplify the\nproblem, all ONH point clouds were aligned with respect to the plane and center\nof Bruch's membrane opening. Geometric deep learning (PointNet) was then used\nto provide a glaucoma diagnosis from a single OCT point cloud. The performance\nof our approach was compared to that obtained with a 3D CNN, and with RNFL\nthickness.\n</p>\n<p>Results: PointNet was able to provide a robust glaucoma diagnosis solely from\nthe ONH represented as a 3D point cloud (AUC=95%). The performance of PointNet\nwas superior to that obtained with a standard 3D CNN (AUC=87%) and with that\nobtained from RNFL thickness alone (AUC=80%).\n</p>\n<p>Discussion: We provide a proof-of-principle for the application of geometric\ndeep learning in the field of glaucoma. Our technique requires significantly\nless information as input to perform better than a 3D CNN, and with an AUC\nsuperior to that obtained from RNFL thickness alone. Geometric deep learning\nmay have wide applicability in the field of Ophthalmology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Thiery_A/0/1/0/all/0/1\">Alexandre H. Thiery</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Braeu_F/0/1/0/all/0/1\">Fabian Braeu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tun_T/0/1/0/all/0/1\">Tin A. Tun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aung_T/0/1/0/all/0/1\">Tin Aung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Girard_M/0/1/0/all/0/1\">Michael J.A. Girard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretability of Machine Learning Methods Applied to Neuroimaging. (arXiv:2204.07005v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07005","description":"<p>Deep learning methods have become very popular for the processing of natural\nimages, and were then successfully adapted to the neuroimaging field. As these\nmethods are non-transparent, interpretability methods are needed to validate\nthem and ensure their reliability. Indeed, it has been shown that deep learning\nmodels may obtain high performance even when using irrelevant features, by\nexploiting biases in the training set. Such undesirable situations can\npotentially be detected by using interpretability methods. Recently, many\nmethods have been proposed to interpret neural networks. However, this domain\nis not mature yet. Machine learning users face two major issues when aiming to\ninterpret their models: which method to choose, and how to assess its\nreliability? Here, we aim at providing answers to these questions by presenting\nthe most common interpretability methods and metrics developed to assess their\nreliability, as well as their applications and benchmarks in the neuroimaging\ncontext. Note that this is not an exhaustive survey: we aimed to focus on the\nstudies which we found to be the most representative and relevant.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thibeau_Sutre_E/0/1/0/all/0/1\">Elina Thibeau-Sutre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collin_S/0/1/0/all/0/1\">Sasha Collin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgos_N/0/1/0/all/0/1\">Ninon Burgos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colliot_O/0/1/0/all/0/1\">Olivier Colliot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Environmental Sound Representation to Robustness of 2D CNN Models Against Adversarial Attacks. (arXiv:2204.07018v1 [cs.SD])","link":"http://arxiv.org/abs/2204.07018","description":"<p>This paper investigates the impact of different standard environmental sound\nrepresentations (spectrograms) on the recognition performance and adversarial\nattack robustness of a victim residual convolutional neural network, namely\nResNet-18. Our main motivation for focusing on such a front-end classifier\nrather than other complex architectures is balancing recognition accuracy and\nthe total number of training parameters. Herein, we measure the impact of\ndifferent settings required for generating more informative Mel-frequency\ncepstral coefficient (MFCC), short-time Fourier transform (STFT), and discrete\nwavelet transform (DWT) representations on our front-end model. This\nmeasurement involves comparing the classification performance over the\nadversarial robustness. We demonstrate an inverse relationship between\nrecognition accuracy and model robustness against six benchmarking attack\nalgorithms on the balance of average budgets allocated by the adversary and the\nattack cost. Moreover, our experimental results have shown that while the\nResNet-18 model trained on DWT spectrograms achieves a high recognition\naccuracy, attacking this model is relatively more costly for the adversary than\nother 2D representations. We also report some results on different\nconvolutional neural network architectures such as ResNet-34, ResNet-56,\nAlexNet, and GoogLeNet, SB-CNN, and LSTM-based.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Esmaeilpour_M/0/1/0/all/0/1\">Mohammad Esmaeilpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardinal_P/0/1/0/all/0/1\">Patrick Cardinal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koerich_A/0/1/0/all/0/1\">Alessandro Lameiras Koerich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Q-TART: Quickly Training for Adversarial Robustness and in-Transferability. (arXiv:2204.07024v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07024","description":"<p>Raw deep neural network (DNN) performance is not enough; in real-world\nsettings, computational load, training efficiency and adversarial security are\njust as or even more important. We propose to simultaneously tackle\nPerformance, Efficiency, and Robustness, using our proposed algorithm Q-TART,\nQuickly Train for Adversarial Robustness and in-Transferability. Q-TART follows\nthe intuition that samples highly susceptible to noise strongly affect the\ndecision boundaries learned by DNNs, which in turn degrades their performance\nand adversarial susceptibility. By identifying and removing such samples, we\ndemonstrate improved performance and adversarial robustness while using only a\nsubset of the training data. Through our experiments we highlight Q-TART's high\nperformance across multiple Dataset-DNN combinations, including ImageNet, and\nprovide insights into the complementary behavior of Q-TART alongside existing\nadversarial training approaches to increase robustness by over 1.3% while using\nup to 17.9% less training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_M/0/1/0/all/0/1\">Madan Ravi Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sekeh_S/0/1/0/all/0/1\">Salimeh Yasaei Sekeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corso_J/0/1/0/all/0/1\">Jason J. Corso</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Autonomous Satellite Detection and Tracking using Optical Flow. (arXiv:2204.07025v1 [astro-ph.IM])","link":"http://arxiv.org/abs/2204.07025","description":"<p>In this paper, an autonomous method of satellite detection and tracking in\nimages is implemented using optical flow. Optical flow is used to estimate the\nimage velocities of detected objects in a series of space images. Given that\nmost objects in an image will be stars, the overall image velocity from star\nmotion is used to estimate the image's frame-to-frame motion. Objects seen to\nbe moving with velocity profiles distinct from the overall image velocity are\nthen classified as potential resident space objects. The detection algorithm is\nexercised using both simulated star images and ground-based imagery of\nsatellites. Finally, this algorithm will be tested and compared using a\ncommercial and an open-source software approach to provide the reader with two\ndifferent options based on their need.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/astro-ph/1/au:+Zuehlke_D/0/1/0/all/0/1\">David Zuehlke</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Posada_D/0/1/0/all/0/1\">Daniel Posada</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Tiwari_M/0/1/0/all/0/1\">Madhur Tiwari</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Henderson_T/0/1/0/all/0/1\">Troy Henderson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activation Regression for Continuous Domain Generalization with Applications to Crop Classification. (arXiv:2204.07030v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07030","description":"<p>Geographic variance in satellite imagery impacts the ability of machine\nlearning models to generalise to new regions. In this paper, we model\ngeographic generalisation in medium resolution Landsat-8 satellite imagery as a\ncontinuous domain adaptation problem, demonstrating how models generalise\nbetter with appropriate domain knowledge. We develop a dataset spatially\ndistributed across the entire continental United States, providing macroscopic\ninsight into the effects of geography on crop classification in multi-spectral\nand temporally distributed satellite imagery. Our method demonstrates improved\ngeneralisability from 1) passing geographically correlated climate variables\nalong with the satellite data to a Transformer model and 2) regressing on the\nmodel features to reconstruct these domain variables. Combined, we provide a\nnovel perspective on geographic generalisation in satellite imagery and a\nsimple-yet-effective approach to leverage domain knowledge. Code is available\nat: \\url{https://github.com/samar-khanna/cropmap}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1\">Samar Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Bram Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bala_K/0/1/0/all/0/1\">Kavita Bala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim-to-Real 6D Object Pose Estimation via Iterative Self-training for Robotic Bin-picking. (arXiv:2204.07049v1 [cs.RO])","link":"http://arxiv.org/abs/2204.07049","description":"<p>In this paper, we propose an iterative self-training framework for\nsim-to-real 6D object pose estimation to facilitate cost-effective robotic\ngrasping. Given a bin-picking scenario, we establish a photo-realistic\nsimulator to synthesize abundant virtual data, and use this to train an initial\npose estimation network. This network then takes the role of a teacher model,\nwhich generates pose predictions for unlabeled real data. With these\npredictions, we further design a comprehensive adaptive selection scheme to\ndistinguish reliable results, and leverage them as pseudo labels to update a\nstudent model for pose estimation on real data. To continuously improve the\nquality of pseudo labels, we iterate the above steps by taking the trained\nstudent model as a new teacher and re-label real data using the refined teacher\nmodel. We evaluate our method on a public benchmark and our newly-released\ndataset, achieving an ADD(-S) improvement of 11.49% and 22.62% respectively.\nOur method is also able to improve robotic bin-picking success by 19.54%,\ndemonstrating the potential of iterative sim-to-real solutions for robotic\napplications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1\">Kai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1\">Stephen James</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yichuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CroCo: Cross-Modal Contrastive learning for localization of Earth Observation data. (arXiv:2204.07052v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07052","description":"<p>It is of interest to localize a ground-based LiDAR point cloud on remote\nsensing imagery. In this work, we tackle a subtask of this problem, i.e. to map\na digital elevation model (DEM) rasterized from aerial LiDAR point cloud on the\naerial imagery. We proposed a contrastive learning-based method that trains on\nDEM and high-resolution optical imagery and experiment the framework on\ndifferent data sampling strategies and hyperparameters. In the best scenario,\nthe Top-1 score of 0.71 and Top-5 score of 0.81 are obtained. The proposed\nmethod is promising for feature learning from RGB and DEM for localization and\nis potentially applicable to other data sources too. Source code will be\nreleased at https://github.com/wtseng530/AVLocalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Hsin Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Ho&#xe0;ng-&#xc2;n L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boulch_A/0/1/0/all/0/1\">Alexandre Boulch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1\">S&#xe9;bastien Lef&#xe8;vre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiede_D/0/1/0/all/0/1\">Dirk Tiede</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Egocentric Human-Object Interaction Detection Exploiting Synthetic Data. (arXiv:2204.07061v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07061","description":"<p>We consider the problem of detecting Egocentric HumanObject Interactions\n(EHOIs) in industrial contexts. Since collecting and labeling large amounts of\nreal images is challenging, we propose a pipeline and a tool to generate\nphoto-realistic synthetic First Person Vision (FPV) images automatically\nlabeled for EHOI detection in a specific industrial scenario. To tackle the\nproblem of EHOI detection, we propose a method that detects the hands, the\nobjects in the scene, and determines which objects are currently involved in an\ninteraction. We compare the performance of our method with a set of\nstate-of-the-art baselines. Results show that using a synthetic dataset\nimproves the performance of an EHOI detection system, especially when few real\ndata are available. To encourage research on this topic, we publicly release\nthe proposed dataset at the following url:\nhttps://iplab.dmi.unict.it/EHOI_SYNTH/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leonardi_R/0/1/0/all/0/1\">Rosario Leonardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragusa_F/0/1/0/all/0/1\">Francesco Ragusa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic Segmentation using Synthetic and Real Data. (arXiv:2204.07069v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07069","description":"<p>Being able to understand the relations between the user and the surrounding\nenvironment is instrumental to assist users in a worksite. For instance,\nunderstanding which objects a user is interacting with from images and video\ncollected through a wearable device can be useful to inform the worker on the\nusage of specific objects in order to improve productivity and prevent\naccidents. Despite modern vision systems can rely on advanced algorithms for\nobject detection, semantic and panoptic segmentation, these methods still\nrequire large quantities of domain-specific labeled data, which can be\ndifficult to obtain in industrial scenarios. Motivated by this observation, we\npropose a pipeline which allows to generate synthetic images from 3D models of\nreal environments and real objects. The generated images are automatically\nlabeled and hence effortless to obtain. Exploiting the proposed pipeline, we\ngenerate a dataset comprising synthetic images automatically labeled for\npanoptic segmentation. This set is complemented by a small number of manually\nlabeled real images for fine-tuning. Experiments show that the use of synthetic\nimages allows to drastically reduce the number of real images needed to obtain\nreasonable panoptic segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quattrocchi_C/0/1/0/all/0/1\">Camillo Quattrocchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauro_D/0/1/0/all/0/1\">Daniele Di Mauro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SemiMultiPose: A Semi-supervised Multi-animal Pose Estimation Framework. (arXiv:2204.07072v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07072","description":"<p>Multi-animal pose estimation is essential for studying animals' social\nbehaviors in neuroscience and neuroethology. Advanced approaches have been\nproposed to support multi-animal estimation and achieve state-of-the-art\nperformance. However, these models rarely exploit unlabeled data during\ntraining even though real world applications have exponentially more unlabeled\nframes than labeled frames. Manually adding dense annotations for a large\nnumber of images or videos is costly and labor-intensive, especially for\nmultiple instances. Given these deficiencies, we propose a novel\nsemi-supervised architecture for multi-animal pose estimation, leveraging the\nabundant structures pervasive in unlabeled frames in behavior videos to enhance\ntraining, which is critical for sparsely-labeled problems. The resulting\nalgorithm will provide superior multi-animal pose estimation results on three\nanimal experiments compared to the state-of-the-art baseline and exhibits more\npredictive power in sparsely-labeled data regimes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blau_A/0/1/0/all/0/1\">Ari Blau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gebhardt_C/0/1/0/all/0/1\">Christoph Gebhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bendesky_A/0/1/0/all/0/1\">Andres Bendesky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paninski_L/0/1/0/all/0/1\">Liam Paninski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Anqi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end Learning for Joint Depth and Image Reconstruction from Diffracted Rotation. (arXiv:2204.07076v1 [eess.IV])","link":"http://arxiv.org/abs/2204.07076","description":"<p>Monocular depth estimation is still an open challenge due to the ill-posed\nnature of the problem at hand. Deep learning based techniques have been\nextensively studied and proved capable of producing acceptable depth estimation\naccuracy even if the lack of meaningful and robust depth cues within single RGB\ninput images severally limits their performance. Coded aperture-based methods\nusing phase and amplitude masks encode strong depth cues within 2D images by\nmeans of depth-dependent Point Spread Functions (PSFs) at the price of a\nreduced image quality. In this paper, we propose a novel end-to-end learning\napproach for depth from diffracted rotation. A phase mask that produces a\nRotating Point Spread Function (RPSF) as a function of defocus is jointly\noptimized with the weights of a depth estimation neural network. To this aim,\nwe introduce a differentiable physical model of the aperture mask and exploit\nan accurate simulation of the camera imaging pipeline. Our approach requires a\nsignificantly less complex model and less training data, yet it is superior to\nexisting methods in the task of monocular depth estimation on indoor\nbenchmarks. In addition, we address the problem of image degradation by\nincorporating a non-blind and non-uniform image deblurring module to recover\nthe sharp all-in-focus image from its RPSF-blurred counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mel_M/0/1/0/all/0/1\">Mazen Mel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siddiqui_M/0/1/0/all/0/1\">Muhammad Siddiqui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zanuttigh_P/0/1/0/all/0/1\">Pietro Zanuttigh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weakly Supervised Attended Object Detection Using Gaze Data as Annotations. (arXiv:2204.07090v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07090","description":"<p>We consider the problem of detecting and recognizing the objects observed by\nvisitors (i.e., attended objects) in cultural sites from egocentric vision. A\nstandard approach to the problem involves detecting all objects and selecting\nthe one which best overlaps with the gaze of the visitor, measured through a\ngaze tracker. Since labeling large amounts of data to train a standard object\ndetector is expensive in terms of costs and time, we propose a weakly\nsupervised version of the task which leans only on gaze data and a frame-level\nlabel indicating the class of the attended object. To study the problem, we\npresent a new dataset composed of egocentric videos and gaze coordinates of\nsubjects visiting a museum. We hence compare three different baselines for\nweakly supervised attended object detection on the collected data. Results show\nthat the considered approaches achieve satisfactory performance in a weakly\nsupervised manner, which allows for significant time savings with respect to a\nfully supervised detector based on Faster R-CNN. To encourage research on the\ntopic, we publicly release the code and the dataset at the following url:\nhttps://iplab.dmi.unict.it/WS_OBJ_DET/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazzamuto_M/0/1/0/all/0/1\">Michele Mazzamuto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragusa_F/0/1/0/all/0/1\">Francesco Ragusa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1\">Antonino Furnari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Signorello_G/0/1/0/all/0/1\">Giovanni Signorello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1\">Giovanni Maria Farinella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Degraded Acacia tree species using deep neural networks on uav drone imagery. (arXiv:2204.07096v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07096","description":"<p>Deep-learning-based image classification and object detection has been\napplied successfully to tree monitoring. However, studies of tree crowns and\nfallen trees, especially on flood inundated areas, remain largely unexplored.\nDetection of degraded tree trunks on natural environments such as water,\nmudflats, and natural vegetated areas is challenging due to the mixed colour\nimage backgrounds. In this paper, Unmanned Aerial Vehicles (UAVs), or drones,\nwith embedded RGB cameras were used to capture the fallen Acacia Xanthophloea\ntrees from six designated plots around Lake Nakuru, Kenya. Motivated by the\nneed to detect fallen trees around the lake, two well-established deep neural\nnetworks, i.e. Faster Region-based Convolution Neural Network (Faster R-CNN)\nand Retina-Net were used for fallen tree detection. A total of 7,590\nannotations of three classes on 256 x 256 image patches were used for this\nstudy. Experimental results show the relevance of deep learning in this\ncontext, with Retina-Net model achieving 38.9% precision and 57.9% recall.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Osio_A/0/1/0/all/0/1\">Anne Achieng Osio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1\">Ho&#xe0;ng-&#xc2;n L&#xea;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayugi_S/0/1/0/all/0/1\">Samson Ayugi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onyango_F/0/1/0/all/0/1\">Fred Onyango</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Odwe_P/0/1/0/all/0/1\">Peter Odwe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1\">S&#xe9;bastien Lef&#xe8;vre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Residual Swin Transformer Channel Attention Network for Image Demosaicing. (arXiv:2204.07098v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07098","description":"<p>Image demosaicing is problem of interpolating full- resolution color images\nfrom raw sensor (color filter array) data. During last decade, deep neural\nnetworks have been widely used in image restoration, and in particular, in\ndemosaicing, attaining significant performance improvement. In recent years,\nvision transformers have been designed and successfully used in various\ncomputer vision applications. One of the recent methods of image restoration\nbased on a Swin Transformer (ST), SwinIR, demonstrates state-of-the-art\nperformance with a smaller number of parameters than neural network-based\nmethods. Inspired by the success of SwinIR, we propose in this paper a novel\nSwin Transformer-based network for image demosaicing, called RSTCANet. To\nextract image features, RSTCANet stacks several residual Swin Transformer\nChannel Attention blocks (RSTCAB), introducing the channel attention for each\ntwo successive ST blocks. Extensive experiments demonstrate that RSTCANet out-\nperforms state-of-the-art image demosaicing methods, and has a smaller number\nof parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1\">Wenzhu Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egiazarian_K/0/1/0/all/0/1\">Karen Egiazarian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look Back and Forth: Video Super-Resolution with Explicit Temporal Difference Modeling. (arXiv:2204.07114v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07114","description":"<p>Temporal modeling is crucial for video super-resolution. Most of the video\nsuper-resolution methods adopt the optical flow or deformable convolution for\nexplicitly motion compensation. However, such temporal modeling techniques\nincrease the model complexity and might fail in case of occlusion or complex\nmotion, resulting in serious distortion and artifacts. In this paper, we\npropose to explore the role of explicit temporal difference modeling in both LR\nand HR space. Instead of directly feeding consecutive frames into a VSR model,\nwe propose to compute the temporal difference between frames and divide those\npixels into two subsets according to the level of difference. They are\nseparately processed with two branches of different receptive fields in order\nto better extract complementary information. To further enhance the\nsuper-resolution result, not only spatial residual features are extracted, but\nthe difference between consecutive frames in high-frequency domain is also\ncomputed. It allows the model to exploit intermediate SR results in both future\nand past to refine the current SR output. The difference at different time\nsteps could be cached such that information from further distance in time could\nbe propagated to the current frame for refinement. Experiments on several video\nsuper-resolution benchmark datasets demonstrate the effectiveness of the\nproposed method and its favorable performance against state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Isobe_T/0/1/0/all/0/1\">Takashi Isobe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xu Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1\">Xin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changlin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruihuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yongjie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1\">Jing Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Huchuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Yu-Wing Tai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeiT III: Revenge of the ViT. (arXiv:2204.07118v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07118","description":"<p>A Vision Transformer (ViT) is a simple neural architecture amenable to serve\nseveral computer vision tasks. It has limited built-in architectural priors, in\ncontrast to more recent architectures that incorporate priors either about the\ninput data or of specific tasks. Recent works show that ViTs benefit from\nself-supervised pre-training, in particular BerT-like pre-training like BeiT.\nIn this paper, we revisit the supervised training of ViTs. Our procedure builds\nupon and simplifies a recipe introduced for training ResNet-50. It includes a\nnew simple data-augmentation procedure with only 3 augmentations, closer to the\npractice in self-supervised learning. Our evaluations on Image classification\n(ImageNet-1k with and without pre-training on ImageNet-21k), transfer learning\nand semantic segmentation show that our procedure outperforms by a large margin\nprevious fully supervised training recipes for ViT. It also reveals that the\nperformance of our ViT trained with supervision is comparable to that of more\nrecent architectures. Our results could serve as better baselines for recent\nself-supervised approaches demonstrated on ViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Touvron_H/0/1/0/all/0/1\">Hugo Touvron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1\">Herv&#xe9; J&#xe9;gou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GIFS: Neural Implicit Function for General Shape Representation. (arXiv:2204.07126v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07126","description":"<p>Recent development of neural implicit function has shown tremendous success\non high-quality 3D shape reconstruction. However, most works divide the space\ninto inside and outside of the shape, which limits their representing power to\nsingle-layer and watertight shapes. This limitation leads to tedious data\nprocessing (converting non-watertight raw data to watertight) as well as the\nincapability of representing general object shapes in the real world. In this\nwork, we propose a novel method to represent general shapes including\nnon-watertight shapes and shapes with multi-layer surfaces. We introduce\nGeneral Implicit Function for 3D Shape (GIFS), which models the relationships\nbetween every two points instead of the relationships between points and\nsurfaces. Instead of dividing 3D space into predefined inside-outside regions,\nGIFS encodes whether two points are separated by any surface. Experiments on\nShapeNet show that GIFS outperforms previous state-of-the-art methods in terms\nof reconstruction quality, rendering efficiency, and visual fidelity. Project\npage is available at https://jianglongye.com/gifs .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jianglong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuntao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naiyan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Siamese Networks for Label-Efficient Learning. (arXiv:2204.07141v1 [cs.LG])","link":"http://arxiv.org/abs/2204.07141","description":"<p>We propose Masked Siamese Networks (MSN), a self-supervised learning\nframework for learning image representations. Our approach matches the\nrepresentation of an image view containing randomly masked patches to the\nrepresentation of the original unmasked image. This self-supervised\npre-training strategy is particularly scalable when applied to Vision\nTransformers since only the unmasked patches are processed by the network. As a\nresult, MSNs improve the scalability of joint-embedding architectures, while\nproducing representations of a high semantic level that perform competitively\non low-shot image classification. For instance, on ImageNet-1K, with only 5,000\nannotated images, our base MSN model achieves 72.4% top-1 accuracy, and with 1%\nof ImageNet-1K labels, we achieve 75.7% top-1 accuracy, setting a new\nstate-of-the-art for self-supervised learning on this benchmark. Our code is\npublicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1\">Mahmoud Assran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1\">Mathilde Caron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1\">Ishan Misra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1\">Piotr Bojanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordes_F/0/1/0/all/0/1\">Florian Bordes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1\">Pascal Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1\">Armand Joulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1\">Michael Rabbat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1\">Nicolas Ballas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood Attention Transformer. (arXiv:2204.07143v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07143","description":"<p>We present Neighborhood Attention Transformer (NAT), an efficient, accurate\nand scalable hierarchical transformer that works well on both image\nclassification and downstream vision tasks. It is built upon Neighborhood\nAttention (NA), a simple and flexible attention mechanism that localizes the\nreceptive field for each query to its nearest neighboring pixels. NA is a\nlocalization of self-attention, and approaches it as the receptive field size\nincreases. It is also equivalent in FLOPs and memory usage to Swin\nTransformer's shifted window attention given the same receptive field size,\nwhile being less constrained. Furthermore, NA includes local inductive biases,\nwhich eliminate the need for extra operations such as pixel shifts.\nExperimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1\naccuracy on ImageNet with only 4.3 GFLOPs and 28M parameters, 51.4% mAP on\nMS-COCO and 48.4% mIoU on ADE20k. We will open-source our checkpoints, training\nscript, configurations, and our CUDA kernel at:\nhttps://github.com/SHI-Labs/Neighborhood-Attention-Transformer .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deformable Sprites for Unsupervised Video Decomposition. (arXiv:2204.07151v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07151","description":"<p>We describe a method to extract persistent elements of a dynamic scene from\nan input video. We represent each scene element as a \\emph{Deformable Sprite}\nconsisting of three components: 1) a 2D texture image for the entire video, 2)\nper-frame masks for the element, and 3) non-rigid deformations that map the\ntexture image into each video frame. The resulting decomposition allows for\napplications such as consistent video editing. Deformable Sprites are a type of\nvideo auto-encoder model that is optimized on individual videos, and does not\nrequire training on a large dataset, nor does it rely on pre-trained models.\nMoreover, our method does not require object masks or other user input, and\ndiscovers moving objects of a wider variety than previous work. We evaluate our\napproach on standard video datasets and show qualitative results on a diverse\narray of Internet videos. Code and video results can be found at\nhttps://deformable-sprites.github.io\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_V/0/1/0/all/0/1\">Vickie Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1\">Richard Tucker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1\">Angjoo Kanazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snavely_N/0/1/0/all/0/1\">Noah Snavely</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in your hands? 3D Reconstruction of Generic Objects in Hands. (arXiv:2204.07153v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07153","description":"<p>Our work aims to reconstruct hand-held objects given a single RGB image. In\ncontrast to prior works that typically assume known 3D templates and reduce the\nproblem to 3D pose estimation, our work reconstructs generic hand-held object\nwithout knowing their 3D templates. Our key insight is that hand articulation\nis highly predictive of the object shape, and we propose an approach that\nconditionally reconstructs the object based on the articulation and the visual\ninput. Given an image depicting a hand-held object, we first use off-the-shelf\nsystems to estimate the underlying hand pose and then infer the object shape in\na normalized hand-centric coordinate frame. We parameterized the object by\nsigned distance which are inferred by an implicit network which leverages the\ninformation from both visual feature and articulation-aware coordinates to\nprocess a query point. We perform experiments across three datasets and show\nthat our method consistently outperforms baselines and is able to reconstruct a\ndiverse set of objects. We analyze the benefits and robustness of explicit\narticulation conditioning and also show that this allows the hand pose\nestimation to further improve in test-time optimization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yufei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Abhinav Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MiniViT: Compressing Vision Transformers with Weight Multiplexing. (arXiv:2204.07154v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07154","description":"<p>Vision Transformer (ViT) models have recently drawn much attention in\ncomputer vision due to their high model capability. However, ViT models suffer\nfrom huge number of parameters, restricting their applicability on devices with\nlimited memory. To alleviate this problem, we propose MiniViT, a new\ncompression framework, which achieves parameter reduction in vision\ntransformers while retaining the same performance. The central idea of MiniViT\nis to multiplex the weights of consecutive transformer blocks. More\nspecifically, we make the weights shared across layers, while imposing a\ntransformation on the weights to increase diversity. Weight distillation over\nself-attention is also applied to transfer knowledge from large-scale ViT\nmodels to weight-multiplexed compact models. Comprehensive experiments\ndemonstrate the efficacy of MiniViT, showing that it can reduce the size of the\npre-trained Swin-B transformer by 48\\%, while achieving an increase of 1.0\\% in\nTop-1 accuracy on ImageNet. Moreover, using a single-layer of parameters,\nMiniViT is able to compress DeiT-B by 9.7 times from 86M to 9M parameters,\nwithout seriously compromising the performance. Finally, we verify the\ntransferability of MiniViT by reporting its performance on downstream\nbenchmarks. Code and models are available at here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinnian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengchen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Any-resolution Training for High-resolution Image Synthesis. (arXiv:2204.07156v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07156","description":"<p>Generative models operate at fixed resolution, even though natural images\ncome in a variety of sizes. As high-resolution details are downsampled away,\nand low-resolution images are discarded altogether, precious supervision is\nlost. We argue that every pixel matters and create datasets with variable-size\nimages, collected at their native resolutions. Taking advantage of this data is\nchallenging; high-resolution processing is costly, and current architectures\ncan only process fixed-resolution data. We introduce continuous-scale training,\na process that samples patches at random scales to train a new generator with\nvariable output resolutions. First, conditioning the generator on a target\nscale allows us to generate higher resolutions images than previously possible,\nwithout adding layers to the model. Second, by conditioning on continuous\ncoordinates, we can sample patches that still obey a consistent global layout,\nwhich also allows for scalable training at higher resolutions. Controlled FFHQ\nexperiments show our method takes advantage of the multi-resolution training\ndata better than discrete multi-scale approaches, achieving better FID scores\nand cleaner high-frequency details. We also train on other natural image\ndomains including churches, mountains, and birds, and demonstrate arbitrary\nscale synthesis with both coherent global layouts and realistic local details,\ngoing beyond 2K resolution in our experiments. Our project page is available\nat: https://chail.github.io/anyres-gan/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1\">Lucy Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gharbi_M/0/1/0/all/0/1\">Michael Gharbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Richard Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Forecasting of Panoptic Segmentations with Difference Attention. (arXiv:2204.07157v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07157","description":"<p>Forecasting of a representation is important for safe and effective autonomy.\nFor this, panoptic segmentations have been studied as a compelling\nrepresentation in recent work. However, recent state-of-the-art on panoptic\nsegmentation forecasting suffers from two issues: first, individual object\ninstances are treated independently of each other; second, individual object\ninstance forecasts are merged in a heuristic manner. To address both issues, we\nstudy a new panoptic segmentation forecasting model that jointly forecasts all\nobject instances in a scene using a transformer model based on 'difference\nattention.' It further refines the predictions by taking depth estimates into\naccount. We evaluate the proposed model on the Cityscapes and AIODrive\ndatasets. We find difference attention to be particularly suitable for\nforecasting because the difference of quantities like locations enables a model\nto explicitly reason about velocities and acceleration. Because of this, we\nattain state-of-the-art on panoptic segmentation forecasting metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Graber_C/0/1/0/all/0/1\">Colin Graber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jazra_C/0/1/0/all/0/1\">Cyril Jazra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Wenjie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Liangyan Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1\">Alexander Schwing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Level Set Theory for Neural Implicit Evolution under Explicit Flows. (arXiv:2204.07159v1 [cs.CV])","link":"http://arxiv.org/abs/2204.07159","description":"<p>Coordinate-based neural networks parameterizing implicit surfaces have\nemerged as efficient representations of geometry. They effectively act as\nparametric level sets with the zero-level set defining the surface of interest.\nWe present a framework that allows applying deformation operations defined for\ntriangle meshes onto such implicit surfaces. Several of these operations can be\nviewed as energy-minimization problems that induce an instantaneous flow field\non the explicit surface. Our method uses the flow field to deform parametric\nimplicit surfaces by extending the classical theory of level sets. We also\nderive a consolidated view for existing methods on differentiable surface\nextraction and rendering, by formalizing connections to the level-set theory.\nWe show that these methods drift from the theory and that our approach exhibits\nimprovements for applications like surface smoothing, mean-curvature flow,\ninverse rendering and user-defined editing on implicit geometry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehta_I/0/1/0/all/0/1\">Ishit Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1\">Manmohan Chandraker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthi_R/0/1/0/all/0/1\">Ravi Ramamoorthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accurate Lung Nodules Segmentation with Detailed Representation Transfer and Soft Mask Supervision. (arXiv:2007.14556v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2007.14556","description":"<p>Accurate lung lesion segmentation from Computed Tomography (CT) images is\ncrucial to the analysis and diagnosis of lung diseases such as COVID-19 and\nlung cancer. However, the smallness and variety of lung nodules and the lack of\nhigh-quality labeling make the accurate lung nodule segmentation difficult. To\naddress these issues, we first introduce a novel segmentation mask named Soft\nMask which has richer and more accurate edge details description and better\nvisualization and develop a universal automatic Soft Mask annotation pipeline\nto deal with different datasets correspondingly. Then, a novel Network with\ndetailed representation transfer and Soft Mask supervision (DSNet) is proposed\nto process the input low-resolution images of lung nodules into high-quality\nsegmentation results. Our DSNet contains a special Detail Representation\nTransfer Module (DRTM) for reconstructing the detailed representation to\nalleviate the small size of lung nodules images, and an adversarial training\nframework with Soft Mask for further improving the accuracy of segmentation.\nExtensive experiments validate that our DSNet outperforms other\nstate-of-the-art methods for accurate lung nodule segmentation and has strong\ngeneralization ability in other accurate medical segmentation tasks with\ncompetitive results. Besides, we provide a new challenging lung nodules\nsegmentation dataset for further studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1\">Changwei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1\">Rongtao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shibiao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_W/0/1/0/all/0/1\">Weiliang Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1\">Jun Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activation Map Adaptation for Effective Knowledge Distillation. (arXiv:2010.13500v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.13500","description":"<p>Model compression becomes a recent trend due to the requirement of deploying\nneural networks on embedded and mobile devices. Hence, both accuracy and\nefficiency are of critical importance. To explore a balance between them, a\nknowledge distillation strategy is proposed for general visual representation\nlearning. It utilizes our well-designed activation map adaptive module to\nreplace some blocks of the teacher network, exploring the most appropriate\nsupervisory features adaptively during the training process. Using the\nteacher's hidden layer output to prompt the student network to train so as to\ntransfer effective semantic information.To verify the effectiveness of our\nstrategy, this paper applied our method to cifar-10 dataset. Results\ndemonstrate that the method can boost the accuracy of the student network by\n0.6% with 6.5% loss reduction, and significantly improve its training speed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1\">Hong Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Minghao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1\">Chupeng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zongmin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xinhui Xue</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVAM: Saliency-guided Visual Attention Modeling by Autonomous Underwater Robots. (arXiv:2011.06252v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.06252","description":"<p>This paper presents a holistic approach to saliency-guided visual attention\nmodeling (SVAM) for use by autonomous underwater robots. Our proposed model,\nnamed SVAM-Net, integrates deep visual features at various scales and semantics\nfor effective salient object detection (SOD) in natural underwater images. The\nSVAM-Net architecture is configured in a unique way to jointly accommodate\nbottom-up and top-down learning within two separate branches of the network\nwhile sharing the same encoding layers. We design dedicated spatial attention\nmodules (SAMs) along these learning pathways to exploit the coarse-level and\nfine-level semantic features for SOD at four stages of abstractions. The\nbottom-up branch performs a rough yet reasonably accurate saliency estimation\nat a fast rate, whereas the deeper top-down branch incorporates a residual\nrefinement module (RRM) that provides fine-grained localization of the salient\nobjects. Extensive performance evaluation of SVAM-Net on benchmark datasets\nclearly demonstrates its effectiveness for underwater SOD. We also validate its\ngeneralization performance by several ocean trials' data that include test\nimages of diverse underwater scenes and waterbodies, and also images with\nunseen natural objects. Moreover, we analyze its computational feasibility for\nrobotic deployments and demonstrate its utility in several important use cases\nof visual attention modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Jahidul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruobing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattar_J/0/1/0/all/0/1\">Junaed Sattar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Introducing a new high-resolution handwritten digits data set with writer characteristics. (arXiv:2011.07946v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07946","description":"<p>The contributions in this article are two-fold. First, we introduce a new\nhand-written digit data set that we collected. It contains high-resolution\nimages of hand-written The contributions in this article are two-fold. First,\nwe introduce a new handwritten digit data set that we collected. It contains\nhigh-resolution images of handwritten digits together with various writer\ncharacteristics which are not available in the well-known MNIST database. The\nmultiple writer characteristics gathered are a novelty of our data set and\ncreate new research opportunities. The data set is publicly available online.\nSecond, we analyse this new data set. We begin with simple supervised tasks. We\nassess the predictability of the writer characteristics gathered, the effect of\nusing some of those characteristics as predictors in classification task and\nthe effect of higher resolution images on classification accuracy. We also\nexplore semi-supervised applications; we can leverage the high quantity of\nhandwritten digits data sets already existing online to improve the accuracy of\nvarious classifications task with noticeable success. Finally, we also\ndemonstrate the generative perspective offered by this new data set; we are\nable to generate images that mimics the writing style of specific writers. The\ndata set has unique and distinct features and our analysis establishes\nbenchmarks and showcases some of the new opportunities made possible with this\nnew data set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beaulac_C/0/1/0/all/0/1\">C&#xe9;dric Beaulac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenthal_J/0/1/0/all/0/1\">Jeffrey S. Rosenthal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Temporal Learning on Monocular Videos for 3D Human Pose Estimation. (arXiv:2012.01511v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.01511","description":"<p>In this paper we propose an unsupervised learning method to extract temporal\ninformation on monocular videos, where we detect and encode subject of interest\nin each frame and leverage contrastive self-supervised (CSS) learning to\nextract rich latent vectors. Instead of simply treating the latent features of\nnearby frames as positive pairs and those of temporally-distant ones as\nnegative pairs as in other CSS approaches, we explicitly disentangle each\nlatent vector into a time-variant component and a time-invariant one. We then\nshow that applying CSS only to the time-variant features and encouraging a\ngradual transition on them between nearby and away frames while also\nreconstructing the input, extract rich temporal features into the time-variant\ncomponent, well-suited for human pose estimation. Our approach reduces error by\nabout 50\\% compared to the standard CSS strategies, outperforms other\nunsupervised single-view methods and matches the performance of multi-view\ntechniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1\">Sina Honari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constantin_V/0/1/0/all/0/1\">Victor Constantin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1\">Mathieu Salzmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Modal Contrastive Learning for Text-to-Image Generation. (arXiv:2101.04702v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.04702","description":"<p>The output of text-to-image synthesis systems should be coherent, clear,\nphoto-realistic scenes with high semantic fidelity to their conditioned text\ndescriptions. Our Cross-Modal Contrastive Generative Adversarial Network\n(XMC-GAN) addresses this challenge by maximizing the mutual information between\nimage and text. It does this via multiple contrastive losses which capture\ninter-modality and intra-modality correspondences. XMC-GAN uses an attentional\nself-modulation generator, which enforces strong text-image correspondence, and\na contrastive discriminator, which acts as a critic as well as a feature\nencoder for contrastive learning. The quality of XMC-GAN's output is a major\nstep up from previous models, as we show on three challenging datasets. On\nMS-COCO, not only does XMC-GAN improve state-of-the-art FID from 24.70 to 9.33,\nbut--more importantly--people prefer XMC-GAN by 77.3 for image quality and 74.1\nfor image-text alignment, compared to three other recent models. XMC-GAN also\ngeneralizes to the challenging Localized Narratives dataset (which has longer,\nmore detailed descriptions), improving state-of-the-art FID from 48.70 to\n14.12. Lastly, we train and evaluate XMC-GAN on the challenging Open Images\ndata, establishing a strong benchmark FID score of 26.91.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1\">Jing Yu Koh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Honglak Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Green View Index and Green View Index best path using Google Street View and deep learning. (arXiv:2104.12627v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.12627","description":"<p>Streetscape is an important part of the urban landscape, analysing and\nstudying them can increase the understanding of the cities' infrastructure,\nwhich can lead to better planning and design of the urban living environment.\nIn this paper, we used Google Street View to obtain street view images of Osaka\nCity. The semantic segmentation model HRNet-OCR \\cite{HRNet-OCR} is used to\nsegment the Osaka City street view images and analyse the Green View Index\n(GVI) of Osaka City. Based on the GVI value, because of the limitations of\nArcGIS software, we take advantage of adjacency matrix and Floyd algorithm is\nused to calculate Green View Index best path. Our analysis not only allows for\nthe calculation of specific routes for the optimal GVI paths, but also allows\nfor the visualization and integration of neighbourhood landscape. By\nsummarising all the data, a more specific and objective analysis of the\nlandscape in the study area can be carried out and based on this, the available\nnatural resources can be maximised for a better life.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1\">Anqi Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Adversarial Transferability with Gradient Refining. (arXiv:2105.04834v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04834","description":"<p>Deep neural networks are vulnerable to adversarial examples, which are\ncrafted by adding human-imperceptible perturbations to original images. Most\nexisting adversarial attack methods achieve nearly 100% attack success rates\nunder the white-box setting, but only achieve relatively low attack success\nrates under the black-box setting. To improve the transferability of\nadversarial examples for the black-box setting, several methods have been\nproposed, e.g., input diversity, translation-invariant attack, and\nmomentum-based attack. In this paper, we propose a method named Gradient\nRefining, which can further improve the adversarial transferability by\ncorrecting useless gradients introduced by input diversity through multiple\ntransformations. Our method is generally applicable to many gradient-based\nattack methods combined with input diversity. Extensive experiments are\nconducted on the ImageNet dataset and our method can achieve an average\ntransfer success rate of 82.07% for three different models under single-model\nsetting, which outperforms the other state-of-the-art methods by a large margin\nof 6.0% averagely. And we have applied the proposed method to the competition\nCVPR 2021 Unrestricted Adversarial Attacks on ImageNet organized by Alibaba and\nwon the second place in attack success rates among 1558 teams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoqiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Huanqian Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Ying Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xingxing Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Mobile GUI: from Pixel-Words to Screen-Sentences. (arXiv:2105.11941v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11941","description":"<p>The ubiquity of mobile phones makes mobile GUI understanding an important\ntask. Most previous works in this domain require human-created metadata of\nscreens (e.g. View Hierarchy) during inference, which unfortunately is often\nnot available or reliable enough for GUI understanding. Inspired by the\nimpressive success of Transformers in NLP tasks, targeting for purely\nvision-based GUI understanding, we extend the concepts of Words/Sentence to\nPixel-Words/Screen-Sentence, and propose a mobile GUI understanding\narchitecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the\nindividual Words, we define the Pixel-Words as atomic visual components (text\nand graphic components), which are visually consistent and semantically clear\nacross screenshots of a large variety of design styles. The Pixel-Words\nextracted from a screenshot are aggregated into Screen-Sentence with a Screen\nTransformer proposed to model their relations. Since the Pixel-Words are\ndefined as atomic visual components, the ambiguity between their visual\nappearance and semantics is dramatically reduced. We are able to make use of\nmetadata available in training data to auto-generate high-quality annotations\nfor Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words\nannotations is built based on the public RICO dataset, which will be released\nto help to address the lack of high-quality training data in this area. We\ntrain a detector to extract Pixel-Words from screenshots on this dataset and\nachieve metadata-free GUI understanding during inference. We conduct\nexperiments and show that Pixel-Words can be well extracted on RICO-PW and well\ngeneralized to a new dataset, P2S-UI, collected by ourselves. The effectiveness\nof PW2SS is further verified in the GUI understanding tasks including relation\nprediction, clickability prediction, screen retrieval, and app type\nclassification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingwen Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuwang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sam Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliard_G/0/1/0/all/0/1\">Grayson Hilliard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Online Learning System for Wireless Charging Alignment using Surround-view Fisheye Cameras. (arXiv:2105.12763v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12763","description":"<p>Electric Vehicles are increasingly common, with inductive chargepads being\nconsidered a convenient and efficient means of charging electric vehicles.\nHowever, drivers are typically poor at aligning the vehicle to the necessary\naccuracy for efficient inductive charging, making the automated alignment of\nthe two charging plates desirable. In parallel to the electrification of the\nvehicular fleet, automated parking systems that make use of surround-view\ncamera systems are becoming increasingly popular. In this work, we propose a\nsystem based on the surround-view camera architecture to detect, localize, and\nautomatically align the vehicle with the inductive chargepad. The visual design\nof the chargepads is not standardized and not necessarily known beforehand.\nTherefore, a system that relies on offline training will fail in some\nsituations. Thus, we propose a self-supervised online learning method that\nleverages the driver's actions when manually aligning the vehicle with the\nchargepad and combine it with weak supervision from semantic segmentation and\ndepth to learn a classifier to auto-annotate the chargepad in the video for\nfurther training. In this way, when faced with a previously unseen chargepad,\nthe driver needs only manually align the vehicle a single time. As the\nchargepad is flat on the ground, it is not easy to detect it from a distance.\nThus, we propose using a Visual SLAM pipeline to learn landmarks relative to\nthe chargepad to enable alignment from a greater range. We demonstrate the\nworking system on an automated vehicle as illustrated in the video\nhttps://youtu.be/_cLCmkW4UYo. To encourage further research, we will share a\nchargepad dataset used in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dahal_A/0/1/0/all/0/1\">Ashok Dahal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Varun Ravi Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yogamani_S/0/1/0/all/0/1\">Senthil Yogamani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eising_C/0/1/0/all/0/1\">Ciaran Eising</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning. (arXiv:2106.06047v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.06047","description":"<p>Federated learning is an emerging research paradigm enabling collaborative\ntraining of machine learning models among different organizations while keeping\ndata private at each institution. Despite recent progress, there remain\nfundamental challenges such as the lack of convergence and the potential for\ncatastrophic forgetting across real-world heterogeneous devices. In this paper,\nwe demonstrate that self-attention-based architectures (e.g., Transformers) are\nmore robust to distribution shifts and hence improve federated learning over\nheterogeneous data. Concretely, we conduct the first rigorous empirical\ninvestigation of different neural architectures across a range of federated\nalgorithms, real-world benchmarks, and heterogeneous data splits. Our\nexperiments show that simply replacing convolutional networks with Transformers\ncan greatly reduce catastrophic forgetting of previous devices, accelerate\nconvergence, and reach a better global model, especially when dealing with\nheterogeneous data. We release our code and pretrained models at\nhttps://github.com/Liangqiong/ViT-FL-main to encourage future exploration in\nrobust architectures as an alternative to current research efforts on the\noptimization front.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuyin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Paul Pu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingda Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Feifei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Handling Data Heterogeneity with Generative Replay in Collaborative Learning for Medical Imaging. (arXiv:2106.13208v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13208","description":"<p>Collaborative learning, which enables collaborative and decentralized\ntraining of deep neural networks at multiple institutions in a\nprivacy-preserving manner, is rapidly emerging as a valuable technique in\nhealthcare applications. However, its distributed nature often leads to\nsignificant heterogeneity in data distributions across institutions. In this\npaper, we present a novel generative replay strategy to address the challenge\nof data heterogeneity in collaborative learning methods. Different from\ntraditional methods that directly aggregating the model parameters, we leverage\ngenerative adversarial learning to aggregate the knowledge from all the local\ninstitutions. Specifically, instead of directly training a model for task\nperformance, we develop a novel dual model architecture: a primary model learns\nthe desired task, and an auxiliary \"generative replay model\" allows aggregating\nknowledge from the heterogenous clients. The auxiliary model is then\nbroadcasted to the central sever, to regulate the training of primary model\nwith an unbiased target distribution. Experimental results demonstrate the\ncapability of the proposed method in handling heterogeneous data across\ninstitutions. On highly heterogeneous data partitions, our model achieves\n~4.88% improvement in the prediction accuracy on a diabetic retinopathy\nclassification dataset, and ~49.8% reduction of mean absolution value on a Bone\nAge prediction dataset, respectively, compared to the state-of-the art\ncollaborative learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Liangqiong Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandar_N/0/1/0/all/0/1\">Niranjan Balachandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1\">Daniel Rubin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCR-Net: A deep learning based script independent handwritten character recognition network. (arXiv:2108.06663v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.06663","description":"<p>Despite being studied extensively for a few decades, handwritten character\nrecognition (HCR) is considered a challenging learning problem in pattern\nrecognition and there is very limited research on script independent models.\nThis is mainly because of diversity of scripts, focus of the conventional\nresearch on handcrafted feature extraction techniques, and unavailability of\npublic datasets and codes to reproduce the results. On the other hand, deep\nlearning has witnessed huge success in different areas of pattern recognition,\nincluding HCR, and provides end-to-end learning but it has been studied for\nspecific scripts only. In this paper, we have proposed a novel deep learning\narchitecture which exploits transfer learning and image-augmentation for\nend-to-end learning for script independent handwritten character recognition,\ncalled HCR-Net. HCR-Net is based on a novel transfer learning approach for HCR,\nwhere some of lower layers of a pre-trained network are utilized. Due to\ntransfer learning and image-augmentation, HCR-Net provides faster training,\nbetter performance and better generalizations, and can achieve up to 99\\%\nresults of its final accuracy in just first epoch. The experimental results on\npublicly available datasets of Bangla, Punjabi, Hindi, English, Swedish, Urdu,\nFarsi, Tibetan, Kannada, Malayalam, Telugu, Marathi, Nepali and Arabic\nlanguages prove the efficacy of HCR-Net and establishes several new benchmarks.\nFor reproducibility of the results and for the advancements of the HCR\nresearch, complete code is publicly released at\nhttps://github.com/jmdvinodjmd/HCR-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_V/0/1/0/all/0/1\">Vinod Kumar Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sukhdeep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anuj Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Burst Image Restoration and Enhancement. (arXiv:2110.03680v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03680","description":"<p>Modern handheld devices can acquire burst image sequence in a quick\nsuccession. However, the individual acquired frames suffer from multiple\ndegradations and are misaligned due to camera shake and object motions. The\ngoal of Burst Image Restoration is to effectively combine complimentary cues\nacross multiple burst frames to generate high-quality outputs. Towards this\ngoal, we develop a novel approach by solely focusing on the effective\ninformation exchange between burst frames, such that the degradations get\nfiltered out while the actual scene details are preserved and enhanced. Our\ncentral idea is to create a set of pseudo-burst features that combine\ncomplementary information from all the input burst frames to seamlessly\nexchange information. However, the pseudo-burst cannot be successfully created\nunless the individual burst frames are properly aligned to discount inter-frame\nmovements. Therefore, our approach initially extracts pre-processed features\nfrom each burst frame and matches them using an edge-boosting burst alignment\nmodule. The pseudo-burst features are then created and enriched using\nmulti-scale contextual information. Our final step is to adaptively aggregate\ninformation from the pseudo-burst features to progressively increase resolution\nin multiple stages while merging the pseudo-burst features. In comparison to\nexisting works that usually follow a late fusion scheme with single-stage\nupsampling, our approach performs favorably, delivering state-of-the-art\nperformance on burst superresolution, burst low-light image enhancement, and\nburst denoising tasks. The source code and pre-trained models are available at\n\\url{https://github.com/akshaydudhane16/BIPNet}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dudhane_A/0/1/0/all/0/1\">Akshay Dudhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamir_S/0/1/0/all/0/1\">Syed Waqas Zamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1\">Salman Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahad Shahbaz Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constrained Deep One-Class Feature Learning For Classifying Imbalanced Medical Images. (arXiv:2111.10610v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.10610","description":"<p>Medical image data are usually imbalanced across different classes. One-class\nclassification has attracted increasing attention to address the data imbalance\nproblem by distinguishing the samples of the minority class from the majority\nclass. Previous methods generally aim to either learn a new feature space to\nmap training samples together or to fit training samples by autoencoder-like\nmodels. These methods mainly focus on capturing either compact or descriptive\nfeatures, where the information of the samples of a given one class is not\nsufficiently utilized. In this paper, we propose a novel deep learning-based\nmethod to learn compact features by adding constraints on the bottleneck\nfeatures, and to preserve descriptive features by training an autoencoder at\nthe same time. Through jointly optimizing the constraining loss and the\nautoencoder's reconstruction loss, our method can learn more relevant features\nassociated with the given class, making the majority and minority samples more\ndistinguishable. Experimental results on three clinical datasets (including the\nMRI breast images, FFDM breast images and chest X-ray images) obtains\nstate-of-art performance compared to previous methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_L/0/1/0/all/0/1\">Long Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arefan_D/0/1/0/all/0/1\">Dooman Arefan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panigrahy_A/0/1/0/all/0/1\">Ashok Panigrahy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Shandong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Medical Knowledge-Guided Deep Learning for Imbalanced Medical Image Classification. (arXiv:2111.10620v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.10620","description":"<p>Deep learning models have gained remarkable performance on a variety of image\nclassification tasks. However, many models suffer from limited performance in\nclinical or medical settings when data are imbalanced. To address this\nchallenge, we propose a medical-knowledge-guided one-class classification\napproach that leverages domain-specific knowledge of classification tasks to\nboost the model's performance. The rationale behind our approach is that some\nexisting prior medical knowledge can be incorporated into data-driven deep\nlearning to facilitate model learning. We design a deep learning-based\none-class classification pipeline for imbalanced image classification, and\ndemonstrate in three use cases how we take advantage of medical knowledge of\neach specific classification task by generating additional middle classes to\nachieve higher classification performances. We evaluate our approach on three\ndifferent clinical image classification tasks (a total of 8459 images) and show\nsuperior model performance when compared to six state-of-the-art methods. All\ncodes of this work will be publicly available upon acceptance of the paper.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Gao_L/0/1/0/all/0/1\">Long Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arefan_D/0/1/0/all/0/1\">Dooman Arefan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Panigrahy_A/0/1/0/all/0/1\">Ashok Panigrahy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zuley_M/0/1/0/all/0/1\">Margarita L. Zuley</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1\">Shandong Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Local Learning Matters: Rethinking Data Heterogeneity in Federated Learning. (arXiv:2111.14213v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.14213","description":"<p>Federated learning (FL) is a promising strategy for performing\nprivacy-preserving, distributed learning with a network of clients (i.e., edge\ndevices). However, the data distribution among clients is often non-IID in\nnature, making efficient optimization difficult. To alleviate this issue, many\nFL algorithms focus on mitigating the effects of data heterogeneity across\nclients by introducing a variety of proximal terms, some incurring considerable\ncompute and/or memory overheads, to restrain local updates with respect to the\nglobal model. Instead, we consider rethinking solutions to data heterogeneity\nin FL with a focus on local learning generality rather than proximal\nrestriction. To this end, we first present a systematic study informed by\nsecond-order indicators to better understand algorithm effectiveness in FL.\nInterestingly, we find that standard regularization methods are surprisingly\nstrong performers in mitigating data heterogeneity effects. Based on our\nfindings, we further propose a simple and effective method, FedAlign, to\novercome data heterogeneity and the pitfalls of previous methods. FedAlign\nachieves competitive accuracy with state-of-the-art FL methods across a variety\nof settings while minimizing computation and memory overhead. Code is available\nat https://github.com/mmendiet/FedAlign\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mendieta_M/0/1/0/all/0/1\">Matias Mendieta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Taojiannan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Minwoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhengming Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FIBA: Frequency-Injection based Backdoor Attack in Medical Image Analysis. (arXiv:2112.01148v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01148","description":"<p>In recent years, the security of AI systems has drawn increasing research\nattention, especially in the medical imaging realm. To develop a secure medical\nimage analysis (MIA) system, it is a must to study possible backdoor attacks\n(BAs), which can embed hidden malicious behaviors into the system. However,\ndesigning a unified BA method that can be applied to various MIA systems is\nchallenging due to the diversity of imaging modalities (e.g., X-Ray, CT, and\nMRI) and analysis tasks (e.g., classification, detection, and segmentation).\nMost existing BA methods are designed to attack natural image classification\nmodels, which apply spatial triggers to training images and inevitably corrupt\nthe semantics of poisoned pixels, leading to the failures of attacking dense\nprediction models. To address this issue, we propose a novel\nFrequency-Injection based Backdoor Attack method (FIBA) that is capable of\ndelivering attacks in various MIA tasks. Specifically, FIBA leverages a trigger\nfunction in the frequency domain that can inject the low-frequency information\nof a trigger image into the poisoned image by linearly combining the spectral\namplitude of both images. Since it preserves the semantics of the poisoned\nimage pixels, FIBA can perform attacks on both classification and dense\nprediction models. Experiments on three benchmarks in MIA (i.e., ISIC-2019 for\nskin lesion classification, KiTS-19 for kidney tumor segmentation, and EAD-2019\nfor endoscopic artifact detection), validate the effectiveness of FIBA and its\nsuperiority over state-of-the-art methods in attacking MIA models as well as\nbypassing backdoor defense. Source code will be available at\nhttps://github.com/HazardFY/FIBA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yu Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1\">Benteng Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shanshan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distill and De-bias: Mitigating Bias in Face Recognition using Knowledge Distillation. (arXiv:2112.09786v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09786","description":"<p>Face recognition networks generally demonstrate bias with respect to\nsensitive attributes like gender, skintone etc. For gender and skintone, we\nobserve that the regions of the face that a network attends to vary by the\ncategory of an attribute. This might contribute to bias. Building on this\nintuition, we propose a novel distillation-based approach called Distill and\nDe-bias (D&amp;D) to enforce a network to attend to similar face regions,\nirrespective of the attribute category. In D&amp;D, we train a teacher network on\nimages from one category of an attribute; e.g. light skintone. Then distilling\ninformation from the teacher, we train a student network on images of the\nremaining category; e.g., dark skintone. A feature-level distillation loss\nconstrains the student network to generate teacher-like representations. This\nallows the student network to attend to similar face regions for all attribute\ncategories and enables it to reduce bias. We also propose a second distillation\nstep on top of D&amp;D, called D&amp;D++. For the D&amp;D++ network, we distill the\n`un-biasedness' of the D&amp;D network into a new student network, the D&amp;D++\nnetwork. We train the new network on all attribute categories; e.g., both light\nand dark skintones. This helps us train a network that is less biased for an\nattribute, while obtaining higher face verification performance than D&amp;D. We\nshow that D&amp;D++ outperforms existing baselines in reducing gender and skintone\nbias on the IJB-C dataset, while obtaining higher face verification performance\nthan existing adversarial de-biasing methods. We evaluate the effectiveness of\nour proposed methods on two state-of-the-art face recognition networks:\nCrystalface and ArcFace.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dhar_P/0/1/0/all/0/1\">Prithviraj Dhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Joshua Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1\">Aniket Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Castillo_C/0/1/0/all/0/1\">Carlos D. Castillo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_P/0/1/0/all/0/1\">P. Jonathon Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2. (arXiv:2112.14683v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.14683","description":"<p>Videos show continuous events, yet most $-$ if not all $-$ video synthesis\nframeworks treat them discretely in time. In this work, we think of videos of\nwhat they should be $-$ time-continuous signals, and extend the paradigm of\nneural representations to build a continuous-time video generator. For this, we\nfirst design continuous motion representations through the lens of positional\nembeddings. Then, we explore the question of training on very sparse videos and\ndemonstrate that a good generator can be learned by using as few as 2 frames\nper clip. After that, we rethink the traditional image + video discriminators\npair and design a holistic discriminator that aggregates temporal information\nby simply concatenating frames' features. This decreases the training cost and\nprovides richer learning signal to the generator, making it possible to train\ndirectly on 1024$^2$ videos for the first time. We build our model on top of\nStyleGAN2 and it is just ${\\approx}5\\%$ more expensive to train at the same\nresolution while achieving almost the same image quality. Moreover, our latent\nspace features similar properties, enabling spatial manipulations that our\nmethod can propagate in time. We can generate arbitrarily long videos at\narbitrary high frame rate, while prior work struggles to generate even 64\nframes at a fixed rate. Our model is tested on four modern 256$^2$ and one\n1024$^2$-resolution video synthesis benchmarks. In terms of sheer metrics, it\nperforms on average ${\\approx}30\\%$ better than the closest runner-up. Project\nwebsite: https://universome.github.io.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulyakov_S/0/1/0/all/0/1\">Sergey Tulyakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BottleFit: Learning Compressed Representations in Deep Neural Networks for Effective and Efficient Split Computing. (arXiv:2201.02693v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.02693","description":"<p>Although mission-critical applications require the use of deep neural\nnetworks (DNNs), their continuous execution at mobile devices results in a\nsignificant increase in energy consumption. While edge offloading can decrease\nenergy consumption, erratic patterns in channel quality, network and edge\nserver load can lead to severe disruption of the system's key operations. An\nalternative approach, called split computing, generates compressed\nrepresentations within the model (called \"bottlenecks\"), to reduce bandwidth\nusage and energy consumption. Prior work has proposed approaches that introduce\nadditional layers, to the detriment of energy consumption and latency. For this\nreason, we propose a new framework called BottleFit, which, in addition to\ntargeted DNN architecture modifications, includes a novel training strategy to\nachieve high accuracy even with strong compression rates. We apply BottleFit on\ncutting-edge DNN models in image classification, and show that BottleFit\nachieves 77.1% data compression with up to 0.6% accuracy loss on ImageNet\ndataset, while state of the art such as SPINN loses up to 6% in accuracy. We\nexperimentally measure the power consumption and latency of an image\nclassification application running on an NVIDIA Jetson Nano board (GPU-based)\nand a Raspberry PI board (GPU-less). We show that BottleFit decreases power\nconsumption and latency respectively by up to 49% and 89% with respect to\n(w.r.t.) local computing and by 37% and 55% w.r.t. edge offloading. We also\ncompare BottleFit with state-of-the-art autoencoders-based approaches, and show\nthat (i) BottleFit reduces power consumption and execution time respectively by\nup to 54% and 44% on the Jetson and 40% and 62% on Raspberry PI; (ii) the size\nof the head model executed on the mobile device is 83 times smaller. We publish\nthe code repository for reproducibility of the results in this study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1\">Yoshitomo Matsubara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callegaro_D/0/1/0/all/0/1\">Davide Callegaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1\">Marco Levorato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Restuccia_F/0/1/0/all/0/1\">Francesco Restuccia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConDor: Self-Supervised Canonicalization of 3D Pose for Partial Shapes. (arXiv:2201.07788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.07788","description":"<p>Progress in 3D object understanding has relied on manually canonicalized\nshape datasets that contain instances with consistent position and orientation\n(3D pose). This has made it hard to generalize these methods to in-the-wild\nshapes, eg., from internet model collections or depth sensors. ConDor is a\nself-supervised method that learns to Canonicalize the 3D orientation and\nposition for full and partial 3D point clouds. We build on top of Tensor Field\nNetworks (TFNs), a class of permutation- and rotation-equivariant, and\ntranslation-invariant 3D networks. During inference, our method takes an unseen\nfull or partial 3D point cloud at an arbitrary pose and outputs an equivariant\ncanonical pose. During training, this network uses self-supervision losses to\nlearn the canonical pose from an un-canonicalized collection of full and\npartial 3D point clouds. ConDor can also learn to consistently co-segment\nobject parts without any supervision. Extensive quantitative results on four\nnew metrics show that our approach outperforms existing methods while enabling\nnew applications such as operation on depth images and annotation transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sajnani_R/0/1/0/all/0/1\">Rahul Sajnani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poulenard_A/0/1/0/all/0/1\">Adrien Poulenard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1\">Jivitesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dua_R/0/1/0/all/0/1\">Radhika Dua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas J. Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_S/0/1/0/all/0/1\">Srinath Sridhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Contrastive Learning with Cluster Ensemble for Unsupervised Person Re-identification. (arXiv:2201.11995v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11995","description":"<p>Unsupervised person re-identification (ReID) aims to match a query image of a\npedestrian to the images in gallery set without supervision labels. The most\npopular approaches to tackle unsupervised person ReID are usually performing a\nclustering algorithm to yield pseudo labels at first and then exploit the\npseudo labels to train a deep neural network. However, the pseudo labels are\nnoisy and sensitive to the hyper-parameter(s) in clustering algorithm. In this\npaper, we propose a Hybrid Contrastive Learning (HCL) approach for unsupervised\nperson ReID, which is based on a hybrid between instance-level and\ncluster-level contrastive loss functions. Moreover, we present a\nMulti-Granularity Clustering Ensemble based Hybrid Contrastive Learning\n(MGCE-HCL) approach, which adopts a multi-granularity clustering ensemble\nstrategy to mine priority information among the pseudo positive sample pairs\nand defines a priority-weighted hybrid contrastive loss for better tolerating\nthe noises in the pseudo positive samples. We conduct extensive experiments on\ntwo benchmark datasets Market-1501 and DukeMTMC-reID. Experimental results\nvalidate the effectiveness of our proposals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">He Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingkun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Guang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Frustratingly Simple Approach for End-to-End Image Captioning. (arXiv:2201.12723v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12723","description":"<p>Image Captioning is a fundamental task to join vision and language,\nconcerning about cross-modal understanding and text generation. Recent years\nwitness the emerging attention on image captioning. Most of existing works\nfollow a traditional two-stage training paradigm. Before training the\ncaptioning models, an extra object detector is utilized to recognize the\nobjects in the image at first. However, they require sizeable datasets with\nfine-grained object annotation for training the object detector, which is a\ndaunting task. In addition, the errors of the object detectors are easy to\npropagate to the following captioning models, degenerating models' performance.\nTo alleviate such defects, we propose a frustratingly simple but highly\neffective end-to-end image captioning framework, Visual Conditioned GPT\n(VC-GPT), by connecting the pre-trained visual encoder (CLIP-ViT) and language\ndecoder (GPT2). Different from the vanilla connection method that directly\ninserts the cross-attention modules into GPT2, we come up with a self-ensemble\ncross-modal fusion mechanism that comprehensively considers both the single-\nand cross-modal knowledge. As a result, we do not need extra object detectors\nfor model training. Experimental results conducted on three popular image\ncaptioning benchmarks (MSCOCO, Flickr30k and NoCaps) demonstrate that our\nVC-GPT achieves either the best or the second-best performance across all\nevaluation metrics over extensive baseline systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1\">Ziyang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1\">Yadong Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rongsheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jing Ma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Neural Network based Framework for Effective Laparoscopic Video Quality Assessment. (arXiv:2202.04517v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2202.04517","description":"<p>Video quality assessment is a challenging problem having a critical\nsignificance in the context of medical imaging. For instance, in laparoscopic\nsurgery, the acquired video data suffers from different kinds of distortion\nthat not only hinder surgery performance but also affect the execution of\nsubsequent tasks in surgical navigation and robotic surgeries. For this reason,\nwe propose in this paper neural network-based approaches for distortion\nclassification as well as quality prediction. More precisely, a Residual\nNetwork (ResNet) based approach is firstly developed for simultaneous ranking\nand classification task. Then, this architecture is extended to make it\nappropriate for the quality prediction task by using an additional Fully\nConnected Neural Network (FCNN). To train the overall architecture (ResNet and\nFCNN models), transfer learning and end-to-end learning approaches are\ninvestigated. Experimental results, carried out on a new laparoscopic video\nquality database, have shown the efficiency of the proposed methods compared to\nrecent conventional and deep learning based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_Z/0/1/0/all/0/1\">Zohaib Amjad Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beghdadi_A/0/1/0/all/0/1\">Azeddine Beghdadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kaaniche_M/0/1/0/all/0/1\">Mounir Kaaniche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheikh_F/0/1/0/all/0/1\">Faouzi Alaya Cheikh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gharbi_O/0/1/0/all/0/1\">Osama Gharbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations. (arXiv:2202.07800v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2202.07800","description":"<p>Vision Transformers (ViTs) take all the image patches as tokens and construct\nmulti-head self-attention (MHSA) among them. Complete leverage of these image\ntokens brings redundant computations since not all the tokens are attentive in\nMHSA. Examples include that tokens containing semantically meaningless or\ndistractive image backgrounds do not positively contribute to the ViT\npredictions. In this work, we propose to reorganize image tokens during the\nfeed-forward process of ViT models, which is integrated into ViT during\ntraining. For each forward inference, we identify the attentive image tokens\nbetween MHSA and FFN (i.e., feed-forward network) modules, which is guided by\nthe corresponding class token attention. Then, we reorganize image tokens by\npreserving attentive image tokens and fusing inattentive ones to expedite\nsubsequent MHSA and FFN computations. To this end, our method EViT improves\nViTs from two perspectives. First, under the same amount of input image tokens,\nour method reduces MHSA and FFN computation for efficient inference. For\ninstance, the inference speed of DeiT-S is increased by 50% while its\nrecognition accuracy is decreased by only 0.3% for ImageNet classification.\nSecond, by maintaining the same computational cost, our method empowers ViTs to\ntake more image tokens as input for recognition accuracy improvement, where the\nimage tokens are from higher resolution images. An example is that we improve\nthe recognition accuracy of DeiT-S by 1% for ImageNet classification at the\nsame computational cost of a vanilla DeiT-S. Meanwhile, our method does not\nintroduce more parameters to ViTs. Experiments on the standard benchmarks show\nthe effectiveness of our method. The code is available at\nhttps://github.com/youweiliang/evit\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Youwei Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1\">Chongjian Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1\">Zhan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1\">Pengtao Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Self-Supervised Learning of Global and Object-Centric Representations. (arXiv:2203.05997v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.05997","description":"<p>Self-supervision allows learning meaningful representations of natural\nimages, which usually contain one central object. How well does it transfer to\nmulti-entity scenes? We discuss key aspects of learning structured\nobject-centric representations with self-supervision and validate our insights\nthrough several experiments on the CLEVR dataset. Regarding the architecture,\nwe confirm the importance of competition for attention-based object discovery,\nwhere each image patch is exclusively attended by one object. For training, we\nshow that contrastive losses equipped with matching can be applied directly in\na latent space, avoiding pixel-based reconstruction. However, such an\noptimization objective is sensitive to false negatives (recurring objects) and\nfalse positives (matching errors). Careful consideration is thus required\naround data augmentation and negative sample selection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baldassarre_F/0/1/0/all/0/1\">Federico Baldassarre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1\">Hossein Azizpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoSDF: Shape Priors for 3D Completion, Reconstruction and Generation. (arXiv:2203.09516v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.09516","description":"<p>Powerful priors allow us to perform inference with insufficient information.\nIn this paper, we propose an autoregressive prior for 3D shapes to solve\nmultimodal 3D tasks such as shape completion, reconstruction, and generation.\nWe model the distribution over 3D shapes as a non-sequential autoregressive\ndistribution over a discretized, low-dimensional, symbolic grid-like latent\nrepresentation of 3D shapes. This enables us to represent distributions over 3D\nshapes conditioned on information from an arbitrary set of spatially anchored\nquery locations and thus perform shape completion in such arbitrary settings\n(e.g., generating a complete chair given only a view of the back leg). We also\nshow that the learned autoregressive prior can be leveraged for conditional\ntasks such as single-view reconstruction and language-based generation. This is\nachieved by learning task-specific naive conditionals which can be approximated\nby light-weight models trained on minimal paired data. We validate the\neffectiveness of the proposed method using both quantitative and qualitative\nevaluation and show that the proposed method outperforms the specialized\nstate-of-the-art methods trained for individual tasks. The project page with\ncode and video visualizations can be found at\nhttps://yccyenchicheng.github.io/AutoSDF/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1\">Paritosh Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yen-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1\">Maneesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tulsiani_S/0/1/0/all/0/1\">Shubham Tulsiani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unifying Motion Deblurring and Frame Interpolation with Events. (arXiv:2203.12178v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.12178","description":"<p>Slow shutter speed and long exposure time of frame-based cameras often cause\nvisual blur and loss of inter-frame information, degenerating the overall\nquality of captured videos. To this end, we present a unified framework of\nevent-based motion deblurring and frame interpolation for blurry video\nenhancement, where the extremely low latency of events is leveraged to\nalleviate motion blur and facilitate intermediate frame prediction.\nSpecifically, the mapping relation between blurry frames and sharp latent\nimages is first predicted by a learnable double integral network, and a fusion\nnetwork is then proposed to refine the coarse results via utilizing the\ninformation from consecutive blurry inputs and the concurrent events. By\nexploring the mutual constraints among blurry frames, latent images, and event\nstreams, we further propose a self-supervised learning framework to enable\nnetwork training with real-world blurry videos and events. Extensive\nexperiments demonstrate that our method compares favorably against the\nstate-of-the-art approaches and achieves remarkable performance on both\nsynthetic and real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GEN-VLKT: Simplify Association and Enhance Interaction Understanding for HOI Detection. (arXiv:2203.13954v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.13954","description":"<p>The task of Human-Object Interaction~(HOI) detection could be divided into\ntwo core problems, i.e., human-object association and interaction\nunderstanding. In this paper, we reveal and address the disadvantages of the\nconventional query-driven HOI detectors from the two aspects. For the\nassociation, previous two-branch methods suffer from complex and costly\npost-matching, while single-branch methods ignore the features distinction in\ndifferent tasks. We propose Guided-Embedding Network~(GEN) to attain a\ntwo-branch pipeline without post-matching. In GEN, we design an instance\ndecoder to detect humans and objects with two independent query sets and a\nposition Guided Embedding~(p-GE) to mark the human and object in the same\nposition as a pair. Besides, we design an interaction decoder to classify\ninteractions, where the interaction queries are made of instance Guided\nEmbeddings (i-GE) generated from the outputs of each instance decoder layer.\nFor the interaction understanding, previous methods suffer from long-tailed\ndistribution and zero-shot discovery. This paper proposes a Visual-Linguistic\nKnowledge Transfer (VLKT) training strategy to enhance interaction\nunderstanding by transferring knowledge from a visual-linguistic pre-trained\nmodel CLIP. In specific, we extract text embeddings for all labels with CLIP to\ninitialize the classifier and adopt a mimic loss to minimize the visual feature\ndistance between GEN and CLIP. As a result, GEN-VLKT outperforms the state of\nthe art by large margins on multiple datasets, e.g., +5.05 mAP on HICO-Det. The\nsource codes are available at https://github.com/YueLiao/gen-vlkt.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1\">Yue Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1\">Aixi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Miao Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Si Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Hyperspectral Images Using SVM with Shape-adaptive Reconstruction and Smoothed Total Variation. (arXiv:2203.15619v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2203.15619","description":"<p>In this work, a novel algorithm called SVM with Shape-adaptive Reconstruction\nand Smoothed Total Variation (SaR-SVM-STV) is introduced to classify\nhyperspectral images, which makes full use of spatial and spectral information.\nThe Shape-adaptive Reconstruction (SaR) is introduced to preprocess each pixel\nbased on the Pearson Correlation between pixels in its shape-adaptive (SA)\nregion. Support Vector Machines (SVMs) are trained to estimate the pixel-wise\nprobability maps of each class. Then the Smoothed Total Variation (STV) model\nis applied to denoise and generate the final classification map. Experiments\nshow that SaR-SVM-STV outperforms the SVM-STV method with a few training\nlabels, demonstrating the significance of reconstructing hyperspectral images\nbefore classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruoning Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1\">Kangning Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_R/0/1/0/all/0/1\">Raymond H. Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plemmons_R/0/1/0/all/0/1\">Robert J. Plemmons</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Perceptual Quality Assessment of UGC Gaming Videos. (arXiv:2204.00128v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.00128","description":"<p>In recent years, with the vigorous development of the video game industry,\nthe proportion of gaming videos on major video websites like YouTube has\ndramatically increased. However, relatively little research has been done on\nthe automatic quality prediction of gaming videos, especially on those that\nfall in the category of \"User-Generated-Content\" (UGC). Since current leading\ngeneral-purpose Video Quality Assessment (VQA) models do not perform well on\nthis type of gaming videos, we have created a new VQA model specifically\ndesigned to succeed on UGC gaming videos, which we call the Gaming Video\nQuality Predictor (GAME-VQP). GAME-VQP successfully predicts the unique\nstatistical characteristics of gaming videos by drawing upon features designed\nunder modified natural scene statistics models, combined with gaming specific\nfeatures learned by a Convolution Neural Network. We study the performance of\nGAME-VQP on a very recent large UGC gaming video database called\nLIVE-YT-Gaming, and find that it both outperforms other mainstream general VQA\nmodels as well as VQA models specifically designed for gaming videos. The new\nmodel will be made public after paper being accepted.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1\">Xiangxu Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_Z/0/1/0/all/0/1\">Zhengzhong Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Birkbeck_N/0/1/0/all/0/1\">Neil Birkbeck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adsumilli_B/0/1/0/all/0/1\">Balu Adsumilli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bovik_A/0/1/0/all/0/1\">Alan C. Bovik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word separation in continuous sign language using isolated signs and post-processing. (arXiv:2204.00923v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.00923","description":"<p>Continuous Sign Language Recognition (CSLR) is a long challenging task in\nComputer Vision due to the difficulties in detecting the explicit boundaries\nbetween the words in a sign sentence. To deal with this challenge, we propose a\ntwo-stage model. In the first stage, the predictor model, which includes a\ncombination of CNN, SVD, and LSTM, is trained with the isolated signs. In the\nsecond stage, we apply a post-processing algorithm to the Softmax outputs\nobtained from the first part of the model in order to separate the isolated\nsigns in the continuous signs. Due to the lack of a large dataset, including\nboth the sign sequences and the corresponding isolated signs, two public\ndatasets in Isolated Sign Language Recognition (ISLR), RKS-PERSIANSIGN and\nASLVID, are used for evaluation. Results of the continuous sign videos confirm\nthe efficiency of the proposed model to deal with isolated sign boundaries\ndetection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiani_K/0/1/0/all/0/1\">Kourosh Kiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1\">Sergio Escalera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Dynamic Correlations in Spatiotemporal Graphs for Motion Prediction. (arXiv:2204.01297v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.01297","description":"<p>Human motion prediction is a challenge task due to the dynamic spatiotemporal\ngraph correlations in different motion sequences. How to efficiently represent\nspatiotemporal graph correlations and model dynamic correlation variances\nbetween different motion sequences is a challenge for spatiotemporal graph\nrepresentation in motion prediction. In this work, we present Dynamic\nSpatioTemporal Graph Convolution (DSTD-GC). The proposed DSTD-GC decomposes\ndynamic spatiotemporal graph modeling into a combination of Dynamic Spatial\nGraph Convolution (DS-GC) and Dynamic Temporal Graph Convolution (DT-GC). As\nhuman motions are subject to common constraints like body connections and\npresent dynamic motion patterns from different samples, we present Constrained\nDynamic Correlation Modeling strategy to represent the spatial/temporal graph\nas a shared spatial/temporal correlation and a function to extract\ntemporal-specific /spatial-specific adjustments for each sample. The modeling\nstrategy represents the spatiotemporal graph with 28.6\\% parameters of the\nstate-of-the-art static decomposition representation while also explicitly\nmodels sample-specific spatiotemporal correlation variances. Moreover, we also\nmathematically reformulating spatiotemporal graph convolutions and their\ndecomposed variants into a unified form and find that DSTD-GC relaxes strict\nconstraints of other graph convolutions, leading to a stronger representation\ncapability. Combining DSTD-GC with prior knowledge, we propose a powerful\nspatiotemporal graph convolution network called DSTD-GCN which outperforms\nstate-of-the-art methods on the Human3.6M and CMU Mocap datasets in prediction\naccuracy with fewest parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jiajun Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fuxing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaoli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jianqin Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-end multi-particle reconstruction in high occupancy imaging calorimeters with graph neural networks. (arXiv:2204.01681v2 [physics.ins-det] UPDATED)","link":"http://arxiv.org/abs/2204.01681","description":"<p>We present an end-to-end reconstruction algorithm to build particle\ncandidates from detector hits in next-generation granular calorimeters similar\nto that foreseen for the high-luminosity upgrade of the CMS detector. The\nalgorithm exploits a distance-weighted graph neural network, trained with\nobject condensation, a graph segmentation technique. Through a single-shot\napproach, the reconstruction task is paired with energy regression. We describe\nthe reconstruction performance in terms of efficiency as well as in terms of\nenergy resolution. In addition, we show the jet reconstruction performance of\nour method and discuss its inference computational cost. To our knowledge, this\nwork is the first-ever example of single-shot calorimetric reconstruction of\n${\\cal O}(1000)$ particles in high-luminosity conditions with 200 pileup.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Qasim_S/0/1/0/all/0/1\">Shah Rukh Qasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chernyavskaya_N/0/1/0/all/0/1\">Nadezda Chernyavskaya</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kieseler_J/0/1/0/all/0/1\">Jan Kieseler</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Long_K/0/1/0/all/0/1\">Kenneth Long</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Viazlo_O/0/1/0/all/0/1\">Oleksandr Viazlo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nawaz_R/0/1/0/all/0/1\">Raheel Nawaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO. (arXiv:2204.03359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03359","description":"<p>Image-Text matching (ITM) is a common task for evaluating the quality of\nVision and Language (VL) models. However, existing ITM benchmarks have a\nsignificant limitation. They have many missing correspondences, originating\nfrom the data construction process itself. For example, a caption is only\nmatched with one image although the caption can be matched with other similar\nimages, and vice versa. To correct the massive false negatives, we construct\nthe Extended COCO Validation (ECCV) Caption dataset by supplying the missing\nassociations with machine and human annotators. We employ five state-of-the-art\nITM models with diverse properties for our annotation process. Our dataset\nprovides x3.6 positive image-to-caption associations and x8.5 caption-to-image\nassociations compared to the original MS-COCO. We also propose to use an\ninformative ranking-based metric, rather than the popular Recall@K(R@K). We\nre-evaluate the existing 25 VL models on existing and proposed benchmarks. Our\nfindings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K R@K,\nCxC R@1 are highly correlated with each other, while the rankings change when\nwe shift to the ECCV mAP. Lastly, we delve into the effect of the bias\nintroduced by the choice of machine annotator. Source code and dataset are\navailable at https://github.com/naver-ai/eccv-caption\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1\">Sanghyuk Chun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1\">Wonjae Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Song Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Seong Joon Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HunYuan_tvr for Text-Video Retrivial. (arXiv:2204.03382v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.03382","description":"<p>Text-Video Retrieval plays an important role in multi-modal understanding and\nhas attracted increasing attention in recent years. Most existing methods focus\non constructing contrastive pairs between whole videos and complete caption\nsentences, while ignoring fine-grained cross-modal relationships, e.g., short\nclips and phrases or single frame and word. In this paper, we propose a novel\nmethod, named HunYuan\\_tvr, to explore hierarchical cross-modal interactions by\nsimultaneously exploring video-sentence, clip-phrase, and frame-word\nrelationships. Considering intrinsic semantic relations between frames,\nHunYuan\\_tvr first performs self-attention to explore frame-wise correlations\nand adaptively clusters correlated frames into clip-level representations.\nThen, the clip-wise correlation is explored to aggregate clip representations\ninto a compact one to describe the video globally. In this way, we can\nconstruct hierarchical video representations for frame-clip-video\ngranularities, and also explore word-wise correlations to form\nword-phrase-sentence embeddings for the text modality. Finally, hierarchical\ncontrastive learning is designed to explore cross-modal\nrelationships,~\\emph{i.e.,} frame-word, clip-phrase, and video-sentence, which\nenables HunYuan\\_tvr to achieve a comprehensive multi-modal understanding.\nFurther boosted by adaptive label denosing and marginal sample enhancement,\nHunYuan\\_tvr obtains new state-of-the-art results on various benchmarks, e.g.,\nRank@1 of 55.0%, 57.8%, 29.7%, 52.1%, and 57.3% on MSR-VTT, MSVD, LSMDC,\nDiDemo, and ActivityNet respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1\">Shaobo Min</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_W/0/1/0/all/0/1\">Weijie Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1\">Rong-Cheng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_D/0/1/0/all/0/1\">Dihong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengfei Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenzhe Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Sixiao Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongfa Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Segmenting across places: The need for fair transfer learning with satellite imagery. (arXiv:2204.04358v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04358","description":"<p>The increasing availability of high-resolution satellite imagery has enabled\nthe use of machine learning to support land-cover measurement and inform\npolicy-making. However, labelling satellite images is expensive and is\navailable for only some locations. This prompts the use of transfer learning to\nadapt models from data-rich locations to others. Given the potential for\nhigh-impact applications of satellite imagery across geographies, a systematic\nassessment of transfer learning implications is warranted. In this work, we\nconsider the task of land-cover segmentation and study the fairness\nimplications of transferring models across locations. We leverage a large\nsatellite image segmentation benchmark with 5987 images from 18 districts (9\nurban and 9 rural). Via fairness metrics we quantify disparities in model\nperformance along two axes -- across urban-rural locations and across\nland-cover classes. Findings show that state-of-the-art models have better\noverall accuracy in rural areas compared to urban areas, through unsupervised\ndomain adaptation methods transfer learning better to urban versus rural areas\nand enlarge fairness gaps. In analysis of reasons for these findings, we show\nthat raw satellite images are overall more dissimilar between source and target\ndistricts for rural than for urban locations. This work highlights the need to\nconduct fairness analysis for satellite imagery segmentation models and\nmotivates the development of methods for fair transfer learning in order not to\nintroduce disparities between places, particularly urban and rural locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Miao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1\">Harvineet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chok_L/0/1/0/all/0/1\">Lazarus Chok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chunara_R/0/1/0/all/0/1\">Rumi Chunara</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Graph Variational Autoencoders for Indoor Furniture layout Generation. (arXiv:2204.04867v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2204.04867","description":"<p>We present a structured graph variational autoencoder for generating the\nlayout of indoor 3D scenes. Given the room type (e.g., living room or library)\nand the room layout (e.g., room elements such as floor and walls), our\narchitecture generates a collection of objects (e.g., furniture items such as\nsofa, table and chairs) that is consistent with the room type and layout. This\nis a challenging problem because the generated scene should satisfy multiple\nconstrains, e.g., each object must lie inside the room and two objects cannot\noccupy the same volume. To address these challenges, we propose a deep\ngenerative model that encodes these relationships as soft constraints on an\nattributed graph (e.g., the nodes capture attributes of room and furniture\nelements, such as class, pose and size, and the edges capture geometric\nrelationships such as relative orientation). The architecture consists of a\ngraph encoder that maps the input graph to a structured latent space, and a\ngraph decoder that generates a furniture graph, given a latent code and the\nroom graph. The latent space is modeled with auto-regressive priors, which\nfacilitates the generation of highly structured scenes. We also propose an\nefficient training procedure that combines matching and constrained learning.\nExperiments on the 3D-FRONT dataset show that our method produces scenes that\nare diverse and are adapted to the room layout.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chattopadhyay_A/0/1/0/all/0/1\">Aditya Chattopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Paul Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_R/0/1/0/all/0/1\">Rene Vidal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning State-of-the-Art with Uncertainties. (arXiv:2204.05173v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2204.05173","description":"<p>With the availability of data, hardware, software ecosystem and relevant\nskill sets, the machine learning community is undergoing a rapid development\nwith new architectures and approaches appearing at high frequency every year.\nIn this article, we conduct an exemplary image classification study in order to\ndemonstrate how confidence intervals around accuracy measurements can greatly\nenhance the communication of research results as well as impact the reviewing\nprocess. In addition, we explore the hallmarks and limitations of this\napproximation. We discuss the relevance of this approach reflecting on a\nspotlight publication of ICLR22. A reproducible workflow is made available as\nan open-source adjoint to this publication. Based on our discussion, we make\nsuggestions for improving the authoring and reviewing process of machine\nlearning articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Steinbach_P/0/1/0/all/0/1\">Peter Steinbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gernhardt_F/0/1/0/all/0/1\">Felicita Gernhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tanveer_M/0/1/0/all/0/1\">Mahnoor Tanveer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmerler_S/0/1/0/all/0/1\">Steve Schmerler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Starke_S/0/1/0/all/0/1\">Sebastian Starke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViViD++: Vision for Visibility Dataset. (arXiv:2204.06183v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2204.06183","description":"<p>In this paper, we present a dataset capturing diverse visual data formats\nthat target varying luminance conditions. While RGB cameras provide nourishing\nand intuitive information, changes in lighting conditions potentially result in\ncatastrophic failure for robotic applications based on vision sensors.\nApproaches overcoming illumination problems have included developing more\nrobust algorithms or other types of visual sensors, such as thermal and event\ncameras. Despite the alternative sensors' potential, there still are few\ndatasets with alternative vision sensors. Thus, we provided a dataset recorded\nfrom alternative vision sensors, by handheld or mounted on a car, repeatedly in\nthe same space but in different conditions. We aim to acquire visible\ninformation from co-aligned alternative vision sensors. Our sensor system\ncollects data more independently from visible light intensity by measuring the\namount of infrared dissipation, depth by structured reflection, and\ninstantaneous temporal changes in luminance. We provide these measurements\nalong with inertial sensors and ground-truth for developing robust visual SLAM\nunder poor illumination. The full dataset is available at:\nhttps://visibilitydataset.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Alex Junho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1\">Younggun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1\">Young-sik Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1\">Ayoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WSSS4LUAD: Grand Challenge on Weakly-supervised Tissue Semantic Segmentation for Lung Adenocarcinoma. (arXiv:2204.06455v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2204.06455","description":"<p>Lung cancer is the leading cause of cancer death worldwide, and\nadenocarcinoma (LUAD) is the most common subtype. Exploiting the potential\nvalue of the histopathology images can promote precision medicine in oncology.\nTissue segmentation is the basic upstream task of histopathology image\nanalysis. Existing deep learning models have achieved superior segmentation\nperformance but require sufficient pixel-level annotations, which is\ntime-consuming and expensive. To enrich the label resources of LUAD and to\nalleviate the annotation efforts, we organize this challenge WSSS4LUAD to call\nfor the outstanding weakly-supervised semantic segmentation (WSSS) techniques\nfor histopathology images of LUAD. Participants have to design the algorithm to\nsegment tumor epithelial, tumor-associated stroma and normal tissue with only\npatch-level labels. This challenge includes 10,091 patch-level annotations (the\ntraining set) and over 130 million labeled pixels (the validation and test\nsets), from 87 WSIs (67 from GDPH, 20 from TCGA). All the labels were generated\nby a pathologist-in-the-loop pipeline with the help of AI models and checked by\nthe label review board. Among 532 registrations, 28 teams submitted the results\nin the test phase with over 1,000 submissions. Finally, the first place team\nachieved mIoU of 0.8413 (tumor: 0.8389, stroma: 0.7931, normal: 0.8919).\nAccording to the technical reports of the top-tier teams, CAM is still the most\npopular approach in WSSS. Cutmix data augmentation has been widely adopted to\ngenerate more reliable samples. With the success of this challenge, we believe\nthat WSSS approaches with patch-level annotations can be a complement to the\ntraditional pixel annotations while reducing the annotation efforts. The entire\ndataset has been released to encourage more researches on computational\npathology in LUAD and more novel WSSS techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1\">Chu Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_X/0/1/0/all/0/1\">Xipeng Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_L/0/1/0/all/0/1\">Lixu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_H/0/1/0/all/0/1\">Huan Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Bingbing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_S/0/1/0/all/0/1\">Su Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_S/0/1/0/all/0/1\">Shanshan Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhenwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mai_J/0/1/0/all/0/1\">Jinhai Mai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_J/0/1/0/all/0/1\">Jiatai Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchao Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zeyan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhizhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yumeng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Huihui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1\">Chao Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_C/0/1/0/all/0/1\">Chunhui Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mao_L/0/1/0/all/0/1\">Lijian Mao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duan_L/0/1/0/all/0/1\">Luwen Duan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1\">Jingsong Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1\">Dong Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Z/0/1/0/all/0/1\">Zijie Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yongbing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1\">Yiwen Zou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_Y/0/1/0/all/0/1\">Yiduo Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiaomeng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Haiming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_Y/0/1/0/all/0/1\">Yanfen Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_G/0/1/0/all/0/1\">Guoqiang Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jun Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Huihua Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1\">Chunming Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenbing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_C/0/1/0/all/0/1\">Changhong Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingling Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zaiyi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open-Set Recognition: a Good Closed-Set Classifier is All You Need?. (arXiv:2110.06207v2 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2110.06207","description":"<p>The ability to identify whether or not a test sample belongs to one of the\nsemantic classes in a classifier's training set is critical to practical\ndeployment of the model. This task is termed open-set recognition (OSR) and has\nreceived significant attention in recent years. In this paper, we first\ndemonstrate that the ability of a classifier to make the 'none-of-above'\ndecision is highly correlated with its accuracy on the closed-set classes. We\nfind that this relationship holds across loss objectives and architectures, and\nfurther demonstrate the trend both on the standard OSR benchmarks as well as on\na large-scale ImageNet evaluation. Second, we use this correlation to boost the\nperformance of a maximum logit score OSR 'baseline' by improving its closed-set\naccuracy, and with this strong baseline achieve state-of-the-art on a number of\nOSR benchmarks. Similarly, we boost the performance of the existing\nstate-of-the-art method by improving its closed-set accuracy, but the resulting\ndiscrepancy with the strong baseline is marginal. Our third contribution is to\npresent the 'Semantic Shift Benchmark' (SSB), which better respects the task of\ndetecting semantic novelty, in contrast to other forms of distribution shift\nalso considered in related sub-fields, such as out-of-distribution detection.\nOn this new evaluation, we again demonstrate that there is negligible\ndifference between the strong baseline and the existing state-of-the-art.\nProject Page: https://www.robots.ox.ac.uk/~vgg/research/osr/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaze_S/0/1/0/all/0/1\">Sagar Vaze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DL4SciVis: A State-of-the-Art Survey on Deep Learning for Scientific Visualization. (arXiv:2204.06504v1 [cs.GR] CROSS LISTED)","link":"http://arxiv.org/abs/2204.06504","description":"<p>Since 2016, we have witnessed the tremendous growth of artificial\nintelligence+visualization (AI+VIS) research. However, existing survey papers\non AI+VIS focus on visual analytics and information visualization, not\nscientific visualization (SciVis). In this paper, we survey related deep\nlearning (DL) works in SciVis, specifically in the direction of DL4SciVis:\ndesigning DL solutions for solving SciVis problems. To stay focused, we\nprimarily consider works that handle scalar and vector field data but exclude\nmesh data. We classify and discuss these works along six dimensions: domain\nsetting, research task, learning type, network architecture, loss function, and\nevaluation metric. The paper concludes with a discussion of the remaining gaps\nto fill along the discussed dimensions and the grand challenges we need to\ntackle as a community. This state-of-the-art survey guides SciVis researchers\nin gaining an overview of this emerging topic and points out future directions\nto grow this research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jun Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-04-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}