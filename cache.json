{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-12-24T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Evolution and trade-off dynamics of functional load. (arXiv:2112.12224v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12224","description":"<p>Function Load (FL) quantifies the contributions by phonological contrasts to\ndistinctions made across the lexicon. Previous research has linked particularly\nlow values of FL to sound change. Here we broaden the scope of enquiry into FL,\nto its evolution at all values. We apply phylogenetic methods to examine the\ndiachronic evolution of FL across 90 languages of the Pama-Nyungan (PN) family\nof Australia. We find a high degree of phylogenetic signal in FL. Though\nphylogenetic signal has been reported for phonological structures, such as\nphonotactics, its detection in measures of phonological function is novel. We\nalso find a significant, negative correlation between the FL of vowel length\nand of the following consonant, that is, a deep-time historical trade-off\ndynamic, which we relate to known allophony in modern PN languages and\ncompensatory sound changes in their past. The finding reveals a historical\ndynamic, similar to transphonologization, which we characterize as a flow of\ncontrastiveness between subsystems of the phonology. Recurring across a\nlanguage family which spans a whole continent and many millennia of time depth,\nour finding provides one of the most compelling examples yet of Sapir's 'drift'\nhypothesis, of non-accidentally parallel development in historically related\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Round_E/0/1/0/all/0/1\">Erich Round</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dockum_R/0/1/0/all/0/1\">Rikker Dockum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryder_R/0/1/0/all/0/1\">Robin J. Ryder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Morphological classifiers. (arXiv:2112.12262v1 [cs.LG])","link":"http://arxiv.org/abs/2112.12262","description":"<p>This work proposes a new type of classifier called Morphological Classifier\n(MC). MCs aggregate concepts from mathematical morphology and supervised\nlearning. The outcomes of this aggregation are classifiers that may preserve\nshape characteristics of classes, subject to the choice of a stopping criterion\nand structuring element. MCs are fundamentally based on set theory, and their\nclassification model can be a mathematical set itself. Two types of\nmorphological classifiers are proposed in the current work, namely,\nMorphological k-NN (MkNN) and Morphological Dilation Classifier (MDC), which\ndemonstrate the feasibility of the approach. This work provides evidence\nregarding the advantages of MCs, e.g., very fast classification times as well\nas competitive accuracy rates. The performance of MkNN and MDC was tested using\np -dimensional datasets. MCs tied or outperformed 14 well established\nclassifiers in 5 out of 8 datasets. In all occasions, the obtained accuracies\nwere higher than the average accuracy obtained with all classifiers. Moreover,\nthe proposed implementations utilize the power of the Graphics Processing Units\n(GPUs) to speed up processing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_E/0/1/0/all/0/1\">&#xc9;. O. Rodrigues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Conci_A/0/1/0/all/0/1\">A. Conci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liatsis_P/0/1/0/all/0/1\">P. Liatsis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Effect of Dialogue History in Multilingual Task Oriented Dialogue Systems. (arXiv:2112.12318v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12318","description":"<p>While the English virtual assistants have achieved exciting performance with\nan enormous amount of training resources, the needs of non-English-speakers\nhave not been satisfied well. Up to Dec 2021, Alexa, one of the most popular\nsmart speakers around the world, is able to support 9 different languages [1],\nwhile there are thousands of languages in the world, 91 of which are spoken by\nmore than 10 million people according to statistics published in 2019 [2].\nHowever, training a virtual assistant in other languages than English is often\nmore difficult, especially for those low-resource languages. The lack of\nhigh-quality training data restricts the performance of models, resulting in\npoor user satisfaction. Therefore, we devise an efficient and effective\ntraining solution for multilingual task-orientated dialogue systems, using the\nsame dataset generation pipeline and end-to-end dialogue system architecture as\nBiToD[5], which adopted some key design choices for a minimalistic natural\nlanguage design where formal dialogue states are used in place of natural\nlanguage inputs. This reduces the room for error brought by weaker natural\nlanguage models, and ensures the model can correctly extract the essential slot\nvalues needed to perform dialogue state tracking (DST). Our goal is to reduce\nthe amount of natural language encoded at each turn, and the key parameter we\ninvestigate is the number of turns (H) to feed as history to model. We first\nexplore the turning point where increasing H begins to yield limiting returns\non the overall performance. Then we examine whether the examples a model with\nsmall H gets wrong can be categorized in a way for the model to do few-shot\nfinetuning on. Lastly, will explore the limitations of this approach, and\nwhether there is a certain type of examples that this approach will not be able\nto resolve.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Michael Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kaili Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradshahi_M/0/1/0/all/0/1\">Mehrad Moradshahi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Making sense of electrical vehicle discussions using sentiment analysis on closely related news and user comments. (arXiv:2112.12327v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12327","description":"<p>We used a token-wise and document-wise sentiment analysis using both\nunsupervised and supervised models applied to both news and user reviews\ndataset. And our token-wise sentiment analysis found a statistically\nsignificant difference in sentiment between the two groups (both of which were\nvery large N), our document-wise supervised sentiment analysis found no\nsignificant difference in sentiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Everts_J/0/1/0/all/0/1\">Josh Everts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xuan Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Multi-Lingual Pre-trained Language Models Reveal Consistent Token Attributions in Different Languages?. (arXiv:2112.12356v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12356","description":"<p>During the past several years, a surge of multi-lingual Pre-trained Language\nModels (PLMs) has been proposed to achieve state-of-the-art performance in many\ncross-lingual downstream tasks. However, the understanding of why multi-lingual\nPLMs perform well is still an open domain. For example, it is unclear whether\nmulti-Lingual PLMs reveal consistent token attributions in different languages.\nTo address this, in this paper, we propose a Cross-lingual Consistency of Token\nAttributions (CCTA) evaluation framework. Extensive experiments in three\ndownstream tasks demonstrate that multi-lingual PLMs assign significantly\ndifferent attributions to multi-lingual synonyms. Moreover, we have the\nfollowing observations: 1) the Spanish achieves the most consistent token\nattributions in different languages when it is used for training PLMs; 2) the\nconsistency of token attributions strongly correlates with performance in\ndownstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junxiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zong_B/0/1/0/all/0/1\">Bo Zong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Wei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jingchao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haifeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Liang Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"S+PAGE: A Speaker and Position-Aware Graph Neural Network Model for Emotion Recognition in Conversation. (arXiv:2112.12389v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12389","description":"<p>Emotion recognition in conversation (ERC) has attracted much attention in\nrecent years for its necessity in widespread applications. Existing ERC methods\nmostly model the self and inter-speaker context separately, posing a major\nissue for lacking enough interaction between them. In this paper, we propose a\nnovel Speaker and Position-Aware Graph neural network model for ERC (S+PAGE),\nwhich contains three stages to combine the benefits of both Transformer and\nrelational graph convolution network (R-GCN) for better contextual modeling.\nFirstly, a two-stream conversational Transformer is presented to extract the\ncoarse self and inter-speaker contextual features for each utterance. Then, a\nspeaker and position-aware conversation graph is constructed, and we propose an\nenhanced R-GCN model, called PAG, to refine the coarse features guided by a\nrelative positional encoding. Finally, both of the features from the former two\nstages are input into a conditional random field layer to model the emotion\ntransfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chen Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Juyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yang Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse-softmax: A Simpler and Faster Alternative Softmax Transformation. (arXiv:2112.12433v1 [cs.LG])","link":"http://arxiv.org/abs/2112.12433","description":"<p>The softmax function is widely used in artificial neural networks for the\nmulticlass classification problems, where the softmax transformation enforces\nthe output to be positive and sum to one, and the corresponding loss function\nallows to use maximum likelihood principle to optimize the model. However,\nsoftmax leaves a large margin for loss function to conduct optimizing operation\nwhen it comes to high-dimensional classification, which results in\nlow-performance to some extent. In this paper, we provide an empirical study on\na simple and concise softmax variant, namely sparse-softmax, to alleviate the\nproblem that occurred in traditional softmax in terms of high-dimensional\nclassification problems. We evaluate our approach in several interdisciplinary\ntasks, the experimental results show that sparse-softmax is simpler, faster,\nand produces better results than the baseline models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shaoshi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhenyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">BoCheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_P/0/1/0/all/0/1\">Pengbin Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jianlin Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1\">Shengfeng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiarun Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TOD-DA: Towards Boosting the Robustness of Task-oriented Dialogue Modeling on Spoken Conversations. (arXiv:2112.12441v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12441","description":"<p>Task-oriented dialogue systems have been plagued by the difficulties of\nobtaining large-scale and high-quality annotated conversations. Furthermore,\nmost of the publicly available datasets only include written conversations,\nwhich are insufficient to reflect actual human behaviors in practical spoken\ndialogue systems. In this paper, we propose Task-oriented Dialogue Data\nAugmentation (TOD-DA), a novel model-agnostic data augmentation paradigm to\nboost the robustness of task-oriented dialogue modeling on spoken\nconversations. The TOD-DA consists of two modules: 1) Dialogue Enrichment to\nexpand training data on task-oriented conversations for easing data sparsity\nand 2) Spoken Conversation Simulator to imitate oral style expressions and\nspeech recognition errors in diverse granularities for bridging the gap between\nwritten and spoken conversations. With such designs, our approach ranked first\nin both tasks of DSTC10 Track2, a benchmark for task-oriented dialogue modeling\non spoken conversations, demonstrating the superiority and effectiveness of our\nproposed TOD-DA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xin Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinxian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1\">Dongfeng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yingzhan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1\">Siqi Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Huang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liankai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ju_Q/0/1/0/all/0/1\">Qiang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jian Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More Than Words: Towards Better Quality Interpretations of Text Classifiers. (arXiv:2112.12444v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12444","description":"<p>The large size and complex decision mechanisms of state-of-the-art text\nclassifiers make it difficult for humans to understand their predictions,\nleading to a potential lack of trust by the users. These issues have led to the\nadoption of methods like SHAP and Integrated Gradients to explain\nclassification decisions by assigning importance scores to input tokens.\nHowever, prior work, using different randomization tests, has shown that\ninterpretations generated by these methods may not be robust. For instance,\nmodels making the same predictions on the test set may still lead to different\nfeature importance rankings. In order to address the lack of robustness of\ntoken-based interpretability, we explore explanations at higher semantic levels\nlike sentences. We use computational metrics and human subject studies to\ncompare the quality of sentence-based interpretations against token-based ones.\nOur experiments show that higher-level feature attributions offer several\nadvantages: 1) they are more robust as measured by the randomization tests, 2)\nthey lead to lower variability when using approximation-based methods like\nSHAP, and 3) they are more intelligible to humans in situations where the\nlinguistic coherence resides at a higher granularity level. Based on these\nfindings, we show that token-based interpretability, while being a convenient\nfirst choice given the input interfaces of the ML models, is not the most\neffective one in all situations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zafar_M/0/1/0/all/0/1\">Muhammad Bilal Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_P/0/1/0/all/0/1\">Philipp Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donini_M/0/1/0/all/0/1\">Michele Donini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1\">C&#xe9;dric Archambeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biessmann_F/0/1/0/all/0/1\">Felix Biessmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1\">Sanjiv Ranjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kenthapadi_K/0/1/0/all/0/1\">Krishnaram Kenthapadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TFW2V: An Enhanced Document Similarity Method for the Morphologically Rich Finnish Language. (arXiv:2112.12489v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12489","description":"<p>Measuring the semantic similarity of different texts has many important\napplications in Digital Humanities research such as information retrieval,\ndocument clustering and text summarization. The performance of different\nmethods depends on the length of the text, the domain and the language. This\nstudy focuses on experimenting with some of the current approaches to Finnish,\nwhich is a morphologically rich language. At the same time, we propose a simple\nmethod, TFW2V, which shows high efficiency in handling both long text documents\nand limited amounts of data. Furthermore, we design an objective evaluation\nmethod which can be used as a framework for benchmarking text similarity\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Duong_Q/0/1/0/all/0/1\">Quan Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1\">Mika H&#xe4;m&#xe4;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1\">Khalid Alnajjar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation based Consistency Contrastive Pre-training for Automatic Speech Recognition. (arXiv:2112.12522v1 [cs.SD])","link":"http://arxiv.org/abs/2112.12522","description":"<p>Self-supervised acoustic pre-training has achieved amazing results on the\nautomatic speech recognition (ASR) task. Most of the successful acoustic\npre-training methods use contrastive learning to learn the acoustic\nrepresentations by distinguish the representations from different time steps,\nignoring the speaker and environment robustness. As a result, the pre-trained\nmodel could show poor performance when meeting out-of-domain data during\nfine-tuning. In this letter, we design a novel consistency contrastive learning\n(CCL) method by utilizing data augmentation for acoustic pre-training.\nDifferent kinds of augmentation are applied on the original audios and then the\naugmented audios are fed into an encoder. The encoder should not only contrast\nthe representations within one audio but also maximize the measurement of the\nrepresentations across different augmented audios. By this way, the pre-trained\nmodel can learn a text-related representation method which is more robust with\nthe change of the speaker or the environment.Experiments show that by applying\nthe CCL method on the Wav2Vec2.0, better results can be realized both on the\nin-domain data and the out-of-domain data. Especially for noisy out-of-domain\ndata, more than 15% relative improvement can be obtained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Changfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gaofeng Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yifan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingwei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengyuan Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are E2E ASR models ready for an industrial usage?. (arXiv:2112.12572v1 [eess.AS])","link":"http://arxiv.org/abs/2112.12572","description":"<p>The Automated Speech Recognition (ASR) community experiences a major turning\npoint with the rise of the fully-neural (End-to-End, E2E) approaches. At the\nsame time, the conventional hybrid model remains the standard choice for the\npractical usage of ASR. According to previous studies, the adoption of E2E ASR\nin real-world applications was hindered by two main limitations: their ability\nto generalize on unseen domains and their high operational cost. In this paper,\nwe investigate both above-mentioned drawbacks by performing a comprehensive\nmulti-domain benchmark of several contemporary E2E models and a hybrid\nbaseline. Our experiments demonstrate that E2E models are viable alternatives\nfor the hybrid approach, and even outperform the baseline both in accuracy and\nin operational efficiency. As a result, our study shows that the generalization\nand complexity issues are no longer the major obstacle for industrial\nintegration, and draws the community's attention to other potential limitations\nof the E2E approaches in some specific use-cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antipov_G/0/1/0/all/0/1\">Grigory Antipov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Romanian BERTs Using Multiple Teachers. (arXiv:2112.12650v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12650","description":"<p>As transfer learning from large-scale pre-trained language models has become\nprevalent in Natural Language Processing, running these models in\ncomputationally constrained environments remains a challenging problem yet to\naddress. Several solutions including knowledge distillation, network\nquantization or network pruning have been proposed; however, these approaches\nfocus mostly on the English language, thus widening the gap when considering\nlow-resource languages. In this work, we introduce three light and fast\nversions of distilled BERT models for the Romanian language:\nDistil-BERT-base-ro, Distil-RoBERT-base and DistilMulti-BERT-base-ro. The first\ntwo models resulted from individually distilling the knowledge of the two base\nversions of Romanian BERTs available in literature, while the last one was\nobtained by distilling their ensemble. To our knowledge, this is the first\nattempt to create publicly available Romanian distilled BERT models, which were\nthoroughly evaluated on five tasks: part-of-speech tagging, named entity\nrecognition, sentiment analysis, semantic textual similarity and dialect\nidentification. The experimental results on these benchmarks proved that our\nthree distilled models maintain most performance in terms of accuracy with\ntheir teachers, while being twice as fast on a GPU and ~35\\% smaller. In\naddition, we further test the similarity between our students and their\nteachers prediction by measuring their label and probability loyalty, together\nwith regression loyalty - a new metric introduced in this work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1\">Andrei-Marius Avram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catrina_D/0/1/0/all/0/1\">Darius Catrina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1\">Dumitru-Clementin Cercel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dascalu_M/0/1/0/all/0/1\">Mihai Dasc&#x103;lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebedea_T/0/1/0/all/0/1\">Traian Rebedea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1\">Vasile P&#x103;i&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1\">Dan Tufi&#x15f;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards more patient friendly clinical notes through language models and ontologies. (arXiv:2112.12672v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12672","description":"<p>Clinical notes are an efficient way to record patient information but are\nnotoriously hard to decipher for non-experts. Automatically simplifying medical\ntext can empower patients with valuable information about their health, while\nsaving clinicians time. We present a novel approach to automated simplification\nof medical text based on word frequencies and language modelling, grounded on\nmedical ontologies enriched with layman terms. We release a new dataset of\npairs of publicly available medical sentences and a version of them simplified\nby clinicians. Also, we define a novel text simplification metric and\nevaluation framework, which we use to conduct a large-scale human evaluation of\nour method against the state of the art. Our method based on a language model\ntrained on medical forum data generates simpler sentences while preserving both\ngrammar and the original meaning, surpassing the current state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moramarco_F/0/1/0/all/0/1\">Francesco Moramarco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juric_D/0/1/0/all/0/1\">Damir Juric</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savkov_A/0/1/0/all/0/1\">Aleksandar Savkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flann_J/0/1/0/all/0/1\">Jack Flann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehl_M/0/1/0/all/0/1\">Maria Lehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boda_K/0/1/0/all/0/1\">Kristian Boda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grafen_T/0/1/0/all/0/1\">Tessa Grafen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhelezniak_V/0/1/0/all/0/1\">Vitalii Zhelezniak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gohil_S/0/1/0/all/0/1\">Sunir Gohil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korfiatis_A/0/1/0/all/0/1\">Alex Papadopoulos Korfiatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammerla_N/0/1/0/all/0/1\">Nils Hammerla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE 3.0 Titan: Exploring Larger-scale Knowledge Enhanced Pre-training for Language Understanding and Generation. (arXiv:2112.12731v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12731","description":"<p>Pre-trained language models have achieved state-of-the-art results in various\nNatural Language Processing (NLP) tasks. GPT-3 has shown that scaling up\npre-trained language models can further exploit their enormous potential. A\nunified framework named ERNIE 3.0 was recently proposed for pre-training\nlarge-scale knowledge enhanced models and trained a model with 10 billion\nparameters. ERNIE 3.0 outperformed the state-of-the-art models on various NLP\ntasks. In order to explore the performance of scaling up ERNIE 3.0, we train a\nhundred-billion-parameter model called ERNIE 3.0 Titan with up to 260 billion\nparameters on the PaddlePaddle platform. Furthermore, we design a\nself-supervised adversarial loss and a controllable language modeling loss to\nmake ERNIE 3.0 Titan generate credible and controllable texts. To reduce the\ncomputation overhead and carbon emission, we propose an online distillation\nframework for ERNIE 3.0 Titan, where the teacher model will teach students and\ntrain itself simultaneously. ERNIE 3.0 Titan is the largest Chinese dense\npre-trained model so far. Empirical results show that the ERNIE 3.0 Titan\noutperforms the state-of-the-art models on 68 NLP datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhihua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Siyu Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_W/0/1/0/all/0/1\">Weibao Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shikun Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Junyuan Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiaxiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yuxiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yangfan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qiuliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Li Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1\">Peng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dianhai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yanjun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Ge Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sub-Character Tokenization for Chinese Pretrained Language Models. (arXiv:2106.00400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.00400","description":"<p>Tokenization is fundamental to pretrained language models (PLMs). Existing\ntokenization methods for Chinese PLMs typically treat each character as an\nindivisible token. However, they ignore the unique feature of the Chinese\nwriting system where additional linguistic information exists below the\ncharacter level, i.e., at the sub-character level. To utilize such information,\nwe propose sub-character (SubChar for short) tokenization. Specifically, we\nfirst encode the input text by converting each Chinese character into a short\nsequence based on its glyph or pronunciation, and then construct the vocabulary\nbased on the encoded text with sub-word tokenization. Experimental results show\nthat SubChar tokenizers have two main advantages over existing tokenizers: 1)\nThey can tokenize inputs into much shorter sequences, thus improving the\ncomputational efficiency. 2) Pronunciation-based SubChar tokenizers can encode\nChinese homophones into the same transliteration sequences and produce the same\ntokenization output, hence being robust to all homophone typos. At the same\ntime, models trained with SubChar tokenizers perform competitively on\ndownstream tasks. We release our code at\nhttps://github.com/thunlp/SubCharTokenization to facilitate future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingfa Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Primer on Pretrained Multilingual Language Models. (arXiv:2107.00676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.00676","description":"<p>Multilingual Language Models (\\MLLMs) such as mBERT, XLM, XLM-R,\n\\textit{etc.} have emerged as a viable option for bringing the power of\npretraining to a large number of languages. Given their success in zero-shot\ntransfer learning, there has emerged a large body of work in (i) building\nbigger \\MLLMs~covering a large number of languages (ii) creating exhaustive\nbenchmarks covering a wider variety of tasks and languages for evaluating\n\\MLLMs~ (iii) analysing the performance of \\MLLMs~on monolingual, zero-shot\ncross-lingual and bilingual tasks (iv) understanding the universal language\npatterns (if any) learnt by \\MLLMs~ and (v) augmenting the (often) limited\ncapacity of \\MLLMs~ to improve their performance on seen or even unseen\nlanguages. In this survey, we review the existing literature covering the above\nbroad areas of research pertaining to \\MLLMs. Based on our survey, we recommend\nsome promising directions of future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-trained Language Models as Prior Knowledge for Playing Text-based Games. (arXiv:2107.08408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.08408","description":"<p>Recently, text world games have been proposed to enable artificial agents to\nunderstand and reason about real-world scenarios. These text-based games are\nchallenging for artificial agents, as it requires an understanding of and\ninteraction using natural language in a partially observable environment.\nAgents observe the environment via textual descriptions designed to be\nchallenging enough for even human players. Past approaches have not paid enough\nattention to the language understanding capability of the proposed agents.\nTypically, these approaches train from scratch, an agent that learns both\ntextual representations and the gameplay online during training using a\ntemporal loss function. Given the sample-inefficiency of RL approaches, it is\ninefficient to learn rich enough textual representations to be able to\nunderstand and reason using the textual observation in such a complicated game\nenvironment setting. In this paper, we improve the semantic understanding of\nthe agent by proposing a simple RL with LM framework where we use\ntransformer-based language models with Deep RL models. We perform a detailed\nstudy of our framework to demonstrate how our model outperforms all existing\nagents on the popular game, Zork1, to achieve a score of 44.7, which is 1.6\nhigher than the state-of-the-art model. Overall, our proposed approach\noutperforms 4 games out of the 14 text-based games, while performing comparable\nto the state-of-the-art models on the remaining games.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singh_I/0/1/0/all/0/1\">Ishika Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1\">Gargi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1\">Ashutosh Modi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Fine-Grained Reasoning for Fake News Detection. (arXiv:2110.15064v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.15064","description":"<p>The detection of fake news often requires sophisticated reasoning skills,\nsuch as logically combining information by considering word-level subtle clues.\nIn this paper, we move towards fine-grained reasoning for fake news detection\nby better reflecting the logical processes of human thinking and enabling the\nmodeling of subtle clues. In particular, we propose a fine-grained reasoning\nframework by following the human information-processing model, introduce a\nmutual-reinforcement-based method for incorporating human knowledge about which\nevidence is more important, and design a prior-aware bi-channel kernel graph\nnetwork to model subtle differences between pieces of evidence. Extensive\nexperiments show that our model outperforms the state-of-the-art methods and\ndemonstrate the explainability of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiqiao Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hao Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTMap: A BERT-based Ontology Alignment System. (arXiv:2112.02682v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2112.02682","description":"<p>Ontology alignment (a.k.a ontology matching (OM)) plays a critical role in\nknowledge integration. Owing to the success of machine learning in many\ndomains, it has been applied in OM. However, the existing methods, which often\nadopt ad-hoc feature engineering or non-contextual word embeddings, have not\nyet outperformed rule-based systems especially in an unsupervised setting. In\nthis paper, we propose a novel OM system named BERTMap which can support both\nunsupervised and semi-supervised settings. It first predicts mappings using a\nclassifier based on fine-tuning the contextual embedding model BERT on text\nsemantics corpora extracted from ontologies, and then refines the mappings\nthrough extension and repair by utilizing the ontology structure and logic. Our\nevaluation with three alignment tasks on biomedical ontologies demonstrates\nthat BERTMap can often perform better than the leading OM systems LogMap and\nAML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoyan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonyrajah_D/0/1/0/all/0/1\">Denvar Antonyrajah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horrocks_I/0/1/0/all/0/1\">Ian Horrocks</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"English-to-Chinese Transliteration with Phonetic Back-transliteration. (arXiv:2112.10321v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2112.10321","description":"<p>Transliteration is a task of translating named entities from a language to\nanother, based on phonetic similarity. The task has embraced deep learning\napproaches in recent years, yet, most ignore the phonetic features of the\ninvolved languages. In this work, we incorporate phonetic information into\nneural networks in two ways: we synthesize extra data using forward and\nback-translation but in a phonetic manner; and we pre-train models on a\nphonetic task before learning transliteration. Our experiments include three\nlanguage pairs and six directions, namely English to and from Chinese, Hebrew\nand Thai. Results indicate that our proposed approach brings benefits to the\nmodel and achieves better or similar performance when compared to state of the\nart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuofei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Songpeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Recur, Attend or Convolve? Frame Dependency Modeling Matters for Cross-Domain Robustness in Action Recognition. (arXiv:2112.12175v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12175","description":"<p>Most action recognition models today are highly parameterized, and evaluated\non datasets with predominantly spatially distinct classes. Previous results for\nsingle images have shown that 2D Convolutional Neural Networks (CNNs) tend to\nbe biased toward texture rather than shape for various computer vision tasks\n(Geirhos et al., 2019), reducing generalization. Taken together, this raises\nsuspicion that large video models learn spurious correlations rather than to\ntrack relevant shapes over time and infer generalizable semantics from their\nmovement. A natural way to avoid parameter explosion when learning visual\npatterns over time is to make use of recurrence across the time-axis. In this\narticle, we empirically study the cross-domain robustness for recurrent,\nattention-based and convolutional video models, respectively, to investigate\nwhether this robustness is influenced by the frame dependency modeling. Our\nnovel Temporal Shape dataset is proposed as a light-weight dataset to assess\nthe ability to generalize across temporal shapes which are not revealed from\nsingle frames. We find that when controlling for performance and layer\nstructure, recurrent models show better out-of-domain generalization ability on\nthe Temporal Shape dataset than convolution- and attention-based models.\nMoreover, our experiments indicate that convolution- and attention-based models\nexhibit more texture bias on Diving48 than recurrent models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Broome_S/0/1/0/all/0/1\">Sofia Broom&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pokropek_E/0/1/0/all/0/1\">Ernest Pokropek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Boyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Personality Recognition using Cross-Attention Transformer and Behaviour Encoding. (arXiv:2112.12180v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12180","description":"<p>Personality computing and affective computing have gained recent interest in\nmany research areas. The datasets for the task generally have multiple\nmodalities like video, audio, language and bio-signals. In this paper, we\npropose a flexible model for the task which exploits all available data. The\ntask involves complex relations and to avoid using a large model for video\nprocessing specifically, we propose the use of behaviour encoding which boosts\nperformance with minimal change to the model. Cross-attention using\ntransformers has become popular in recent times and is utilised for fusion of\ndifferent modalities. Since long term relations may exist, breaking the input\ninto chunks is not desirable, thus the proposed model processes the entire\ninput together. Our experiments show the importance of each of the above\ncontributions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_T/0/1/0/all/0/1\">Tanay Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1\">Dhruv Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balazia_M/0/1/0/all/0/1\">Michal Balazia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1\">Neelabh Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremond_F/0/1/0/all/0/1\">Francois Bremond</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-grained Multi-Modal Self-Supervised Learning. (arXiv:2112.12182v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12182","description":"<p>Multi-Modal Self-Supervised Learning from videos has been shown to improve\nmodel's performance on various downstream tasks. However, such Self-Supervised\npre-training requires large batch sizes and a large amount of computation\nresources due to the noise present in the uncurated data. This is partly due to\nthe fact that the prevalent training scheme is trained on coarse-grained\nsetting, in which vectors representing the whole video clips or natural\nlanguage sentences are used for computing similarity. Such scheme makes\ntraining noisy as part of the video clips can be totally not correlated with\nthe other-modality input such as text description. In this paper, we propose a\nfine-grained multi-modal self-supervised training scheme that computes the\nsimilarity between embeddings at finer-scale (such as individual feature map\nembeddings and embeddings of phrases), and uses attention mechanisms to reduce\nnoisy pairs' weighting in the loss function. We show that with the proposed\npre-training scheme, we can train smaller models, with smaller batch-size and\nmuch less computational resources to achieve downstream tasks performances\ncomparable to State-Of-The-Art, for tasks including action recognition and\ntext-image retrievals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Duo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karout_S/0/1/0/all/0/1\">Salah Karout</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved 2D Keypoint Detection in Out-of-Balance and Fall Situations -- combining input rotations and a kinematic model. (arXiv:2112.12193v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12193","description":"<p>Injury analysis may be one of the most beneficial applications of deep\nlearning based human pose estimation. To facilitate further research on this\ntopic, we provide an injury specific 2D dataset for alpine skiing, covering in\ntotal 533 images. We further propose a post processing routine, that combines\nrotational information with a simple kinematic model. We could improve\ndetection results in fall situations by up to 21% regarding the PCK@0.2 metric.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zwolfer_M/0/1/0/all/0/1\">Michael Zw&#xf6;lfer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinrich_D/0/1/0/all/0/1\">Dieter Heinrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schindelwig_K/0/1/0/all/0/1\">Kurt Schindelwig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wandt_B/0/1/0/all/0/1\">Bastian Wandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rhodin_H/0/1/0/all/0/1\">Helge Rhodin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spoerri_J/0/1/0/all/0/1\">Joerg Spoerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachbauer_W/0/1/0/all/0/1\">Werner Nachbauer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximum Entropy on Erroneous Predictions (MEEP): Improving model calibration for medical image segmentation. (arXiv:2112.12218v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12218","description":"<p>Modern deep neural networks have achieved remarkable progress in medical\nimage segmentation tasks. However, it has recently been observed that they tend\nto produce overconfident estimates, even in situations of high uncertainty,\nleading to poorly calibrated and unreliable models. In this work we introduce\nMaximum Entropy on Erroneous Predictions (MEEP), a training strategy for\nsegmentation networks which selectively penalizes overconfident predictions,\nfocusing only on misclassified pixels. In particular, we design a\nregularization term that encourages high entropy posteriors for wrong\npredictions, increasing the network uncertainty in complex scenarios. Our\nmethod is agnostic to the neural architecture, does not increase model\ncomplexity and can be coupled with multiple segmentation loss functions. We\nbenchmark the proposed strategy in two challenging medical image segmentation\ntasks: white matter hyperintensity lesions in magnetic resonance images (MRI)\nof the brain, and atrial segmentation in cardiac MRI. The experimental results\ndemonstrate that coupling MEEP with standard segmentation losses leads to\nimprovements not only in terms of model calibration, but also in segmentation\nquality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Larrazabal_A/0/1/0/all/0/1\">Agostina Larrazabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_C/0/1/0/all/0/1\">Cesar Martinez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1\">Jose Dolz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MC-DGCNN: A Novel DNN Architecture for Multi-Category Point Set Classification. (arXiv:2112.12219v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12219","description":"<p>Point set classification aims to build a representation learning model that\ndistinguishes between spatial and categorical configurations of point set data.\nThis problem is societally important since in many applications domains such as\nimmunology, and microbial ecology. This problem is challenging since the\ninteractions between different categories of points are not always equal; as a\nresult, the representation learning model must selectively learn the most\nrelevant multi-categorical relationships. The related works are limited (1) in\nlearning the importance of different multi-categorical relationships,\nespecially for high-order interactions, and (2) do not fully exploit the\nspatial distribution of points beyond simply measuring relative distance or\napplying a feed-forward neural network to coordinates. To overcome these\nlimitations, we leverage the dynamic graph convolutional neural network (DGCNN)\narchitecture to design a novel multi-category DGCNN (MC-DGCNN), contributing\nlocation representation and point pair attention layers for multi-categorical\npoint set classification. MC-DGCNN has the ability to identify the categorical\nimportance of each point pair and extends this to N-way spatial relationships,\nwhile still preserving all the properties and benefits of DGCNN (e.g.,\ndifferentiability). Experimental results show that the proposed architecture is\ncomputationally efficient and significantly outperforms current deep learning\narchitectures on real-world datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farhadloo_M/0/1/0/all/0/1\">Majid Farhadloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Molnar_C/0/1/0/all/0/1\">Carl Molnar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Gaoxiang Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1\">Shashi Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maus_R/0/1/0/all/0/1\">Rachel L. Maus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Markovic_S/0/1/0/all/0/1\">Svetomir N. Markovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moore_R/0/1/0/all/0/1\">Raymond Moore</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontovich_A/0/1/0/all/0/1\">Alexey Leontovich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Synthetic Data in Object Detection on Unmanned Aerial Vehicles. (arXiv:2112.12252v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12252","description":"<p>Acquiring data to train deep learning-based object detectors on Unmanned\nAerial Vehicles (UAVs) is expensive, time-consuming and may even be prohibited\nby law in specific environments. On the other hand, synthetic data is fast and\ncheap to access. In this work, we explore the potential use of synthetic data\nin object detection from UAVs across various application environments. For\nthat, we extend the open-source framework DeepGTAV to work for UAV scenarios.\nWe capture various large-scale high-resolution synthetic data sets in several\ndomains to demonstrate their use in real-world object detection from UAVs by\nanalyzing multiple training strategies across several models. Furthermore, we\nanalyze several different data generation and sampling parameters to provide\nactionable engineering advice for further scientific research. The DeepGTAV\nframework is available at https://git.io/Jyf5j.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kiefer_B/0/1/0/all/0/1\">Benjamin Kiefer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ott_D/0/1/0/all/0/1\">David Ott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Human Activity Recognition on wrist-worn accelerometers using self-supervised neural networks. (arXiv:2112.12272v1 [cs.LG])","link":"http://arxiv.org/abs/2112.12272","description":"<p>Measures of Activity of Daily Living (ADL) are an important indicator of\noverall health but difficult to measure in-clinic. Automated and accurate human\nactivity recognition (HAR) using wrist-worn accelerometers enables practical\nand cost efficient remote monitoring of ADL. Key obstacles in developing high\nquality HAR is the lack of large labeled datasets and the performance loss when\napplying models trained on small curated datasets to the continuous stream of\nheterogeneous data in real-life. In this work we design a self-supervised\nlearning paradigm to create a robust representation of accelerometer data that\ncan generalize across devices and subjects. We demonstrate that this\nrepresentation can separate activities of daily living and achieve strong HAR\naccuracy (on multiple benchmark datasets) using very few labels. We also\npropose a segmentation algorithm which can identify segments of salient\nactivity and boost HAR accuracy on continuous real-life data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_N/0/1/0/all/0/1\">Niranjan Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myers_L/0/1/0/all/0/1\">Lance Myers</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust and Precise Facial Landmark Detection by Self-Calibrated Pose Attention Network. (arXiv:2112.12328v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12328","description":"<p>Current fully-supervised facial landmark detection methods have progressed\nrapidly and achieved remarkable performance. However, they still suffer when\ncoping with faces under large poses and heavy occlusions for inaccurate facial\nshape constraints and insufficient labeled training samples. In this paper, we\npropose a semi-supervised framework, i.e., a Self-Calibrated Pose Attention\nNetwork (SCPAN) to achieve more robust and precise facial landmark detection in\nchallenging scenarios. To be specific, a Boundary-Aware Landmark Intensity\n(BALI) field is proposed to model more effective facial shape constraints by\nfusing boundary and landmark intensity field information. Moreover, a\nSelf-Calibrated Pose Attention (SCPA) model is designed to provide a\nself-learned objective function that enforces intermediate supervision without\nlabel information by introducing a self-calibrated mechanism and a pose\nattention mask. We show that by integrating the BALI fields and SCPA model into\na novel self-calibrated pose attention network, more facial prior knowledge can\nbe learned and the detection accuracy and robustness of our method for faces\nwith large poses and heavy occlusions have been improved. The experimental\nresults obtained for challenging benchmark datasets demonstrate that our\napproach outperforms state-of-the-art methods in the literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1\">Jun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_H/0/1/0/all/0/1\">Hui Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1\">Zhihui Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1\">Witold Pedrycz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hang Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"More is Better: A Novel Multi-view Framework for Domain Generalization. (arXiv:2112.12329v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12329","description":"<p>Aiming to generalize the model trained in source domains to unseen target\ndomains, domain generalization (DG) has attracted lots of attention recently.\nThe key issue of DG is how to prevent overfitting to the observed source\ndomains because target domain is unavailable during training. We investigate\nthat overfitting not only causes the inferior generalization ability to unseen\ntarget domains but also leads unstable prediction in the test stage. In this\npaper, we observe that both sampling multiple tasks in training stage and\ngenerating augmented images in test stage largely benefit generalization\nperformance. Thus, by treating tasks and images as different views, we propose\na novel multi-view DG framework. Specifically, in training stage, to enhance\ngeneralization ability, we develop a multi-view regularized meta-learning\nalgorithm that employs multiple tasks to produce a suitable optimization\ndirection during updating model. In test stage, to alleviate unstable\nprediction, we utilize multiple augmented images to yield multi-view\nprediction, which significantly promotes model reliability via fusing the\nresults of different views of a test image. Extensive experiments on three\nbenchmark datasets validate our method outperforms several state-of-the-art\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Transformation Invariant Geometric Deep Learning: Are Initial Representations All You Need?. (arXiv:2112.12345v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12345","description":"<p>Geometric deep learning, i.e., designing neural networks to handle the\nubiquitous geometric data such as point clouds and graphs, have achieved great\nsuccesses in the last decade. One critical inductive bias is that the model can\nmaintain invariance towards various transformations such as translation,\nrotation, and scaling. The existing graph neural network (GNN) approaches can\nonly maintain permutation-invariance, failing to guarantee invariance with\nrespect to other transformations. Besides GNNs, other works design\nsophisticated transformation-invariant layers, which are computationally\nexpensive and difficult to be extended. To solve this problem, we revisit why\nthe existing neural networks cannot maintain transformation invariance when\nhandling geometric data. Our findings show that transformation-invariant and\ndistance-preserving initial representations are sufficient to achieve\ntransformation invariance rather than needing sophisticated neural layer\ndesigns. Motivated by these findings, we propose Transformation Invariant\nNeural Networks (TinvNN), a straightforward and general framework for geometric\ndata. Specifically, we realize transformation-invariant and distance-preserving\ninitial point representations by modifying multi-dimensional scaling before\nfeeding the representations into neural networks. We prove that TinvNN can\nstrictly guarantee transformation invariance, being general and flexible enough\nto be combined with the existing neural networks. Extensive experimental\nresults on point cloud analysis and combinatorial optimization demonstrate the\neffectiveness and general applicability of our proposed method. Based on the\nexperimental results, we advocate that TinvNN should be considered a new\nstarting point and an essential baseline for further studies of\ntransformation-invariant geometric deep learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Ziwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zeyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wenwu Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hierarchical Attention for Weakly-supervised Chest X-Ray Abnormality Localization and Diagnosis. (arXiv:2112.12349v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12349","description":"<p>We consider the problem of abnormality localization for clinical\napplications. While deep learning has driven much recent progress in medical\nimaging, many clinical challenges are not fully addressed, limiting its broader\nusage. While recent methods report high diagnostic accuracies, physicians have\nconcerns trusting these algorithm results for diagnostic decision-making\npurposes because of a general lack of algorithm decision reasoning and\ninterpretability. One potential way to address this problem is to further train\nthese models to localize abnormalities in addition to just classifying them.\nHowever, doing this accurately will require a large amount of disease\nlocalization annotations by clinical experts, a task that is prohibitively\nexpensive to accomplish for most applications. In this work, we take a step\ntowards addressing these issues by means of a new attention-driven weakly\nsupervised algorithm comprising a hierarchical attention mining framework that\nunifies activation- and gradient-based visual attention in a holistic manner.\nOur key algorithmic innovations include the design of explicit ordinal\nattention constraints, enabling principled model training in a\nweakly-supervised fashion, while also facilitating the generation of\nvisual-attention-driven model explanations by means of localization cues. On\ntwo large-scale chest X-ray datasets (NIH ChestX-ray14 and CheXpert), we\ndemonstrate significant localization performance improvements over the current\nstate of the art while also achieving competitive classification performance.\nOur code is available on https://github.com/oyxhust/HAM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xi Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_S/0/1/0/all/0/1\">Srikrishna Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Terrence Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huo_J/0/1/0/all/0/1\">Jiayu Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiang Sean Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">Jie-Zhi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Random Point Initialization Approach to Image Segmentation with Variational Level-sets. (arXiv:2112.12355v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12355","description":"<p>Image segmentation is an essential component in many image processing and\ncomputer vision tasks. The primary goal of image segmentation is to simplify an\nimage for easier analysis, and there are two broad approaches for achieving\nthis: edge based methods, which extract the boundaries of specific known\nobjects, and region based methods, which partition the image into regions that\nare statistically homogeneous. One of the more prominent edge finding methods,\nknown as the level set method, evolves a zero-level contour in the image plane\nwith gradient descent until the contour has converged to the object boundaries.\nWhile the classical level set method and its variants have proved successful in\nsegmenting real images, they are susceptible to becoming stuck in noisy regions\nof the image plane without a priori knowledge of the image and they are unable\nto provide details beyond object outer boundary locations. We propose a\nmodification to the variational level set image segmentation method that can\nquickly detect object boundaries by making use of random point initialization.\nWe demonstrate the efficacy of our approach by comparing the performance of our\nmethod on real images to that of the prominent Canny Method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1\">J.N. Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corcoran_J/0/1/0/all/0/1\">J.N. Corcoran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Path Structural Contrastive Embeddings for Learning Novel Objects. (arXiv:2112.12359v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12359","description":"<p>Learning novel classes from a very few labeled samples has attracted\nincreasing attention in machine learning areas. Recent research on either\nmeta-learning based or transfer-learning based paradigm demonstrates that\ngaining information on a good feature space can be an effective solution to\nachieve favorable performance on few-shot tasks. In this paper, we propose a\nsimple but effective paradigm that decouples the tasks of learning feature\nrepresentations and classifiers and only learns the feature embedding\narchitecture from base classes via the typical transfer-learning training\nstrategy. To maintain both the generalization ability across base and novel\nclasses and discrimination ability within each class, we propose a dual path\nfeature learning scheme that effectively combines structural similarity with\ncontrastive feature construction. In this way, both inner-class alignment and\ninter-class uniformity can be well balanced, and result in improved\nperformance. Experiments on three popular benchmarks show that when\nincorporated with a simple prototype based classifier, our method can still\nachieve promising results for both standard and generalized few-shot problems\nin either an inductive or transductive inference setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Elvis Han Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Weng Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Practical Data-Free Approach to One-shot Federated Learning with Heterogeneity. (arXiv:2112.12371v1 [cs.LG])","link":"http://arxiv.org/abs/2112.12371","description":"<p>One-shot Federated Learning (FL) has recently emerged as a promising\napproach, which allows the central server to learn a model in a single\ncommunication round. Despite the low communication cost, existing one-shot FL\nmethods are mostly impractical or face inherent limitations, e.g., a public\ndataset is required, clients' models are homogeneous, need to upload additional\ndata/model information. To overcome these issues, we propose a more practical\ndata-free approach named FedSyn for one-shot FL framework with heterogeneity.\nOur FedSyn trains the global model by a data generation stage and a model\ndistillation stage. To the best of our knowledge, FedSyn is the first method\nthat can be practically applied to various real-world applications due to the\nfollowing advantages: (1) FedSyn requires no additional information (except the\nmodel parameters) to be transferred between clients and the server; (2) FedSyn\ndoes not require any auxiliary dataset for training; (3) FedSyn is the first to\nconsider both model and statistical heterogeneities in FL, i.e., the clients'\ndata are non-iid and different clients may have different model architectures.\nExperiments on a variety of real-world datasets demonstrate the superiority of\nour FedSyn. For example, FedSyn outperforms the best baseline method Fed-ADI by\n5.08% on CIFAR10 dataset when data are non-iid.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jianghe Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DILF-EN framework for Class-Incremental Learning. (arXiv:2112.12385v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12385","description":"<p>Deep learning models suffer from catastrophic forgetting of the classes in\nthe older phases as they get trained on the classes introduced in the new phase\nin the class-incremental learning setting. In this work, we show that the\neffect of catastrophic forgetting on the model prediction varies with the\nchange in orientation of the same image, which is a novel finding. Based on\nthis, we propose a novel data-ensemble approach that combines the predictions\nfor the different orientations of the image to help the model retain further\ninformation regarding the previously seen classes and thereby reduce the effect\nof forgetting on the model predictions. However, we cannot directly use the\ndata-ensemble approach if the model is trained using traditional techniques.\nTherefore, we also propose a novel dual-incremental learning framework that\ninvolves jointly training the network with two incremental learning objectives,\ni.e., the class-incremental learning objective and our proposed\ndata-incremental learning objective. In the dual-incremental learning\nframework, each image belongs to two classes, i.e., the image class (for\nclass-incremental learning) and the orientation class (for data-incremental\nlearning). In class-incremental learning, each new phase introduces a new set\nof classes, and the model cannot access the complete training data from the\nolder phases. In our proposed data-incremental learning, the orientation\nclasses remain the same across all the phases, and the data introduced by the\nnew phase in class-incremental learning acts as new training data for these\norientation classes. We empirically demonstrate that the dual-incremental\nlearning framework is vital to the data-ensemble approach. We apply our\nproposed approach to state-of-the-art class-incremental learning methods and\nempirically show that our framework significantly improves the performance of\nthese methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Mohammed Asad Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_I/0/1/0/all/0/1\">Indu Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_P/0/1/0/all/0/1\">Pratik Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pravendra Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KFWC: A Knowledge-Driven Deep Learning Model for Fine-grained Classification of Wet-AMD. (arXiv:2112.12386v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12386","description":"<p>Automated diagnosis using deep neural networks can help ophthalmologists\ndetect the blinding eye disease wet Age-related Macular Degeneration (AMD).\nWet-AMD has two similar subtypes, Neovascular AMD and Polypoidal Choroidal\nVessels (PCV). However, due to the difficulty in data collection and the\nsimilarity between images, most studies have only achieved the coarse-grained\nclassification of wet-AMD rather than a finer-grained one of wet-AMD subtypes.\nTo solve this issue, in this paper we propose a Knowledge-driven Fine-grained\nWet-AMD Classification Model (KFWC), to classify fine-grained diseases with\ninsufficient data. With the introduction of a priori knowledge of 10 lesion\nsigns of input images into the KFWC, we aim to accelerate the KFWC by means of\nmulti-label classification pre-training, to locate the decisive image features\nin the fine-grained disease classification task and therefore achieve better\nclassification. Simultaneously, the KFWC can also provide good interpretability\nand effectively alleviate the pressure of data collection and annotation in the\nfield of fine-grained disease classification for wet-AMD. The experiments\ndemonstrate the effectiveness of the KFWC which reaches 99.71% in AU-ROC\nscores, and its considerable improvements over the data-driven w/o Knowledge\nand ophthalmologists, with the rates of 6.69% over the strongest baseline and\n4.14% over ophthalmologists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+E_H/0/1/0/all/0/1\">Haihong E</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1\">Jiawen He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_T/0/1/0/all/0/1\">Tianyi Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1\">Lifei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_L/0/1/0/all/0/1\">Lifei Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1\">Ruru Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_M/0/1/0/all/0/1\">Meina Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DD-NeRF: Double-Diffusion Neural Radiance Field as a Generalizable Implicit Body Representation. (arXiv:2112.12390v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12390","description":"<p>We present DD-NeRF, a novel generalizable implicit field for representing\nhuman body geometry and appearance from arbitrary input views. The core\ncontribution is a double diffusion mechanism, which leverages the sparse\nconvolutional neural network to build two volumes that represent a human body\nat different levels: a coarse body volume takes advantage of unclothed\ndeformable mesh to provide the large-scale geometric guidance, and a detail\nfeature volume learns the intricate geometry from local image features. We also\nemploy a transformer network to aggregate image features and raw pixels across\nviews, for computing the final high-fidelity radiance field. Experiments on\nvarious datasets show that the proposed approach outperforms previous works in\nboth geometry reconstruction and novel view synthesis quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_G/0/1/0/all/0/1\">Guangming Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hongzhi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1\">Yi Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iteratively Selecting an Easy Reference Frame Makes Unsupervised Video Object Segmentation Easier. (arXiv:2112.12402v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12402","description":"<p>Unsupervised video object segmentation (UVOS) is a per-pixel binary labeling\nproblem which aims at separating the foreground object from the background in\nthe video without using the ground truth (GT) mask of the foreground object.\nMost of the previous UVOS models use the first frame or the entire video as a\nreference frame to specify the mask of the foreground object. Our question is\nwhy the first frame should be selected as a reference frame or why the entire\nvideo should be used to specify the mask. We believe that we can select a\nbetter reference frame to achieve the better UVOS performance than using only\nthe first frame or the entire video as a reference frame. In our paper, we\npropose Easy Frame Selector (EFS). The EFS enables us to select an 'easy'\nreference frame that makes the subsequent VOS become easy, thereby improving\nthe VOS performance. Furthermore, we propose a new framework named as Iterative\nMask Prediction (IMP). In the framework, we repeat applying EFS to the given\nvideo and selecting an 'easier' reference frame from the video than the\nprevious iteration, increasing the VOS performance incrementally. The IMP\nconsists of EFS, Bi-directional Mask Prediction (BMP), and Temporal Information\nUpdating (TIU). From the proposed framework, we achieve state-of-the-art\nperformance in three UVOS benchmark sets: DAVIS16, FBMS, and SegTrack-V2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngjo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seong_H/0/1/0/all/0/1\">Hongje Seong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_E/0/1/0/all/0/1\">Euntai Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InstaIndoor and Multi-modal Deep Learning for Indoor Scene Recognition. (arXiv:2112.12409v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12409","description":"<p>Indoor scene recognition is a growing field with great potential for\nbehaviour understanding, robot localization, and elderly monitoring, among\nothers. In this study, we approach the task of scene recognition from a novel\nstandpoint, using multi-modal learning and video data gathered from social\nmedia. The accessibility and variety of social media videos can provide\nrealistic data for modern scene recognition techniques and applications. We\npropose a model based on fusion of transcribed speech to text and visual\nfeatures, which is used for classification on a novel dataset of social media\nvideos of indoor scenes named InstaIndoor. Our model achieves up to 70%\naccuracy and 0.7 F1-Score. Furthermore, we highlight the potential of our\napproach by benchmarking on a YouTube-8M subset of indoor scenes as well, where\nit achieves 74% accuracy and 0.74 F1-Score. We hope the contributions of this\nwork pave the way to novel research in the challenging field of indoor scene\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glavan_A/0/1/0/all/0/1\">Andreea Glavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1\">Estefania Talavera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Modeling Against Adversarial Attacks. (arXiv:2112.12431v1 [cs.LG])","link":"http://arxiv.org/abs/2112.12431","description":"<p>Adversarial training, the process of training a deep learning model with\nadversarial data, is one of the most successful adversarial defense methods for\ndeep learning models. We have found that the robustness to white-box attack of\nan adversarially trained model can be further improved if we fine tune this\nmodel in inference stage to adapt to the adversarial input, with the extra\ninformation in it. We introduce an algorithm that \"post trains\" the model at\ninference stage between the original output class and a \"neighbor\" class, with\nexisting training data. The accuracy of pre-trained Fast-FGSM CIFAR10\nclassifier base model against white-box projected gradient attack (PGD) can be\nsignificantly improved from 46.8% to 64.5% with our algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiwen Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1\">Teck Khim Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Your Face Mirrors Your Deepest Beliefs-Predicting Personality and Morals through Facial Emotion Recognition. (arXiv:2112.12455v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12455","description":"<p>Can we really \"read the mind in the eyes\"? Moreover, can AI assist us in this\ntask? This paper answers these two questions by introducing a machine learning\nsystem that predicts personality characteristics of individuals on the basis of\ntheir face. It does so by tracking the emotional response of the individual's\nface through facial emotion recognition (FER) while watching a series of 15\nshort videos of different genres. To calibrate the system, we invited 85 people\nto watch the videos, while their emotional responses were analyzed through\ntheir facial expression. At the same time, these individuals also took four\nwell-validated surveys of personality characteristics and moral values: the\nrevised NEO FFI personality inventory, the Haidt moral foundations test, the\nSchwartz personal value system, and the domain-specific risk-taking scale\n(DOSPERT). We found that personality characteristics and moral values of an\nindividual can be predicted through their emotional response to the videos as\nshown in their face, with an accuracy of up to 86% using gradient-boosted\ntrees. We also found that different personality characteristics are better\npredicted by different videos, in other words, there is no single video that\nwill provide accurate predictions for all personality characteristics, but it\nis the response to the mix of different videos that allows for accurate\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gloor_P/0/1/0/all/0/1\">P. A. Gloor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1\">A. Fronzetti Colladon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altuntas_E/0/1/0/all/0/1\">E. Altuntas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetinkaya_C/0/1/0/all/0/1\">C. Cetinkaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaiser_M/0/1/0/all/0/1\">M. F. Kaiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ripperger_L/0/1/0/all/0/1\">L. Ripperger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schaefer_T/0/1/0/all/0/1\">T. Schaefer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pose Adaptive Dual Mixup for Few-Shot Single-View 3D Reconstruction. (arXiv:2112.12484v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12484","description":"<p>We present a pose adaptive few-shot learning procedure and a two-stage data\ninterpolation regularization, termed Pose Adaptive Dual Mixup (PADMix), for\nsingle-image 3D reconstruction. While augmentations via interpolating\nfeature-label pairs are effective in classification tasks, they fall short in\nshape predictions potentially due to inconsistencies between interpolated\nproducts of two images and volumes when rendering viewpoints are unknown.\nPADMix targets this issue with two sets of mixup procedures performed\nsequentially. We first perform an input mixup which, combined with a pose\nadaptive learning procedure, is helpful in learning 2D feature extraction and\npose adaptive latent encoding. The stagewise training allows us to build upon\nthe pose invariant representations to perform a follow-up latent mixup under\none-to-one correspondences between features and ground-truth volumes. PADMix\nsignificantly outperforms previous literature on few-shot settings over the\nShapeNet dataset and sets new benchmarks on the more challenging real-world\nPix3D dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1\">Ta-Ying Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hsuan-Ru Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1\">Niki Trigoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tyng-Luh Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaTr: Layout-Aware Transformer for Scene-Text VQA. (arXiv:2112.12494v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12494","description":"<p>We propose a novel multimodal architecture for Scene Text Visual Question\nAnswering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA\nrequires models to reason over different modalities. Thus, we first investigate\nthe impact of each modality, and reveal the importance of the language module,\nespecially when enriched with layout information. Accounting for this, we\npropose a single objective pre-training scheme that requires only text and\nspatial cues. We show that applying this pre-training scheme on scanned\ndocuments has certain advantages over using natural images, despite the domain\ngap. Scanned documents are easy to procure, text-dense and have a variety of\nlayouts, helping the model learn various spatial cues (e.g. left-of, below\netc.) by tying together language and layout information. Compared to existing\napproaches, our method performs vocabulary-free decoding and, as shown,\ngeneralizes well beyond the training vocabulary. We further demonstrate that\nLaTr improves robustness towards OCR errors, a common reason for failure cases\nin STVQA. In addition, by leveraging a vision transformer, we eliminate the\nneed for an external object detector. LaTr outperforms state-of-the-art STVQA\nmethods on multiple datasets. In particular, +7.6% on TextVQA, +10.8% on ST-VQA\nand +4.0% on OCR-VQA (all absolute accuracy numbers).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_R/0/1/0/all/0/1\">Ron Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FedFR: Joint Optimization Federated Framework for Generic and Personalized Face Recognition. (arXiv:2112.12496v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12496","description":"<p>Current state-of-the-art deep learning based face recognition (FR) models\nrequire a large number of face identities for central training. However, due to\nthe growing privacy awareness, it is prohibited to access the face images on\nuser devices to continually improve face recognition models. Federated Learning\n(FL) is a technique to address the privacy issue, which can collaboratively\noptimize the model without sharing the data between clients. In this work, we\npropose a FL based framework called FedFR to improve the generic face\nrepresentation in a privacy-aware manner. Besides, the framework jointly\noptimizes personalized models for the corresponding clients via the proposed\nDecoupled Feature Customization module. The client-specific personalized model\ncan serve the need of optimized face recognition experience for registered\nidentities at the local device. To the best of our knowledge, we are the first\nto explore the personalized face recognition in FL setup. The proposed\nframework is validated to be superior to previous approaches on several generic\nand personalized face recognition benchmarks with diverse FL scenarios. The\nsource codes and our proposed personalized FR benchmark under FL setup are\navailable at https://github.com/jackie840129/FedFR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chih-Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chien-Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chien_S/0/1/0/all/0/1\">Shao-Yi Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1\">Shang-Hong Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive Multi-View Deep Subspace Clustering Net. (arXiv:2112.12506v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12506","description":"<p>In this paper, we propose a novel Attentive Multi-View Deep Subspace Nets\n(AMVDSN), which deeply explores underlying consistent and view-specific\ninformation from multiple views and fuse them by considering each view's\ndynamic contribution obtained by attention mechanism. Unlike most multi-view\nsubspace learning methods that they directly reconstruct data points on raw\ndata or only consider consistency or complementarity when learning\nrepresentation in deep or shallow space, our proposed method seeks to find a\njoint latent representation that explicitly considers both consensus and\nview-specific information among multiple views, and then performs subspace\nclustering on learned joint latent representation.Besides, different views\ncontribute differently to representation learning, we therefore introduce\nattention mechanism to derive dynamic weight for each view, which performs much\nbetter than previous fusion methods in the field of multi-view subspace\nclustering. The proposed algorithm is intuitive and can be easily optimized\njust by using Stochastic Gradient Descent (SGD) because of the neural network\nframework, which also provides strong non-linear characterization capability\ncompared with traditional subspace clustering approaches. The experimental\nresults on seven real-world data sets have demonstrated the effectiveness of\nour proposed algorithm against some state-of-the-art subspace learning\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1\">Run-kun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jian-wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xin Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neuroevolution deep learning architecture search for estimation of river surface elevation from photogrammetric Digital Surface Models. (arXiv:2112.12510v1 [cs.NE])","link":"http://arxiv.org/abs/2112.12510","description":"<p>Development of the new methods of surface water observation is crucial in the\nperspective of increasingly frequent extreme hydrological events related to\nglobal warming and increasing demand for water. Orthophotos and digital surface\nmodels (DSMs) obtained using UAV photogrammetry can be used to determine the\nWater Surface Elevation (WSE) of a river. However, this task is difficult due\nto disturbances of the water surface on DSMs caused by limitations of\nphotogrammetric algorithms. In this study, machine learning was used to extract\na WSE value from disturbed photogrammetric data. A brand new dataset has been\nprepared specifically for this purpose by hydrology and photogrammetry experts.\nThe new method is an important step toward automating water surface level\nmeasurements with high spatial and temporal resolution. Such data can be used\nto validate and calibrate of hydrological, hydraulic and hydrodynamic models\nmaking hydrological forecasts more accurate, in particular predicting extreme\nand dangerous events such as floods or droughts. For our knowledge this is the\nfirst approach in which dataset was created for this purpose and deep learning\nmodels were used for this task. Additionally, neuroevolution algorithm was set\nto explore different architectures to find local optimal models and\nnon-gradient search was performed to fine-tune the model parameters. The\nachieved results have better accuracy compared to manual methods of determining\nWSE from photogrammetric DSMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Szostak_R/0/1/0/all/0/1\">Rados&#x142;aw Szostak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietron_M/0/1/0/all/0/1\">Marcin Pietro&#x144;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zimnoch_M/0/1/0/all/0/1\">Miros&#x142;aw Zimnoch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachniew_P/0/1/0/all/0/1\">Przemys&#x142;aw Wachniew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cwiakala_P/0/1/0/all/0/1\">Pawe&#x142; &#x106;wi&#x105;ka&#x142;a</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puniach_E/0/1/0/all/0/1\">Edyta Puniach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PyCIL: A Python Toolbox for Class-Incremental Learning. (arXiv:2112.12533v1 [cs.LG])","link":"http://arxiv.org/abs/2112.12533","description":"<p>Traditional machine learning systems are deployed under the closed-world\nsetting, which requires the entire training data before the offline training\nprocess. However, real-world applications often face the incoming new classes,\nand a model should incorporate them continually. The learning paradigm is\ncalled Class-Incremental Learning (CIL). We propose a Python toolbox that\nimplements several key algorithms for class-incremental learning to ease the\nburden of researchers in the machine learning community. The toolbox contains\nimplementations of a number of founding works of CIL such as EWC and iCaRL, but\nalso provides current state-of-the-art algorithms that can be used for\nconducting novel fundamental research. This toolbox, named PyCIL for Python\nClass-Incremental Learning, is available at https://github.com/G-U-N/PyCIL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Da-Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fu-Yun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FourierMask: Instance Segmentation using Fourier Mapping in Implicit Neural Networks. (arXiv:2112.12535v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12535","description":"<p>We present FourierMask, which employs Fourier series combined with implicit\nneural representations to generate instance segmentation masks. We apply a\nFourier mapping (FM) to the coordinate locations and utilize the mapped\nfeatures as inputs to an implicit representation (coordinate-based multi-layer\nperceptron (MLP)). FourierMask learns to predict the coefficients of the FM for\na particular instance, and therefore adapts the FM to a specific object. This\nallows FourierMask to be generalized to predict instance segmentation masks\nfrom natural images. Since implicit functions are continuous in the domain of\ninput coordinates, we illustrate that by sub-sampling the input pixel\ncoordinates, we can generate higher resolution masks during inference.\nFurthermore, we train a renderer MLP (FourierRend) on the uncertain predictions\nof FourierMask and illustrate that it significantly improves the quality of the\nmasks. FourierMask shows competitive results on the MS COCO dataset compared to\nthe baseline Mask R-CNN at the same output resolution and surpasses it on\nhigher resolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Riaz_H/0/1/0/all/0/1\">Hamd ul Moqeet Riaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benbarka_N/0/1/0/all/0/1\">Nuri Benbarka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoeffer_T/0/1/0/all/0/1\">Timon Hoeffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1\">Andreas Zell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the relationship between calibrated predictors and unbiased volume estimation. (arXiv:2112.12560v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12560","description":"<p>Machine learning driven medical image segmentation has become standard in\nmedical image analysis. However, deep learning models are prone to\noverconfident predictions. This has led to a renewed focus on calibrated\npredictions in the medical imaging and broader machine learning communities.\nCalibrated predictions are estimates of the probability of a label that\ncorrespond to the true expected value of the label conditioned on the\nconfidence. Such calibrated predictions have utility in a range of medical\nimaging applications, including surgical planning under uncertainty and active\nlearning systems. At the same time it is often an accurate volume measurement\nthat is of real importance for many medical applications. This work\ninvestigates the relationship between model calibration and volume estimation.\nWe demonstrate both mathematically and empirically that if the predictor is\ncalibrated per image, we can obtain the correct volume by taking an expectation\nof the probability scores per pixel/voxel of the image. Furthermore, we show\nthat convex combinations of calibrated classifiers preserve volume estimation,\nbut do not preserve calibration. Therefore, we conclude that having a\ncalibrated predictor is a sufficient, but not necessary condition for obtaining\nan unbiased estimate of the volume. We validate our theoretical findings\nempirically on a collection of 18 different (calibrated) training strategies on\nthe tasks of glioma volume estimation on BraTS 2018, and ischemic stroke lesion\nvolume estimation on ISLES 2018 datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Popordanoska_T/0/1/0/all/0/1\">Teodora Popordanoska</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bertels_J/0/1/0/all/0/1\">Jeroen Bertels</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vandermeulen_D/0/1/0/all/0/1\">Dirk Vandermeulen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_F/0/1/0/all/0/1\">Frederik Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blaschko_M/0/1/0/all/0/1\">Matthew B. Blaschko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Generative Zero-Shot Learning by Synthesizing Diverse Features with Attribute Augmentation. (arXiv:2112.12573v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12573","description":"<p>The recent advance in deep generative models outlines a promising perspective\nin the realm of Zero-Shot Learning (ZSL). Most generative ZSL methods use\ncategory semantic attributes plus a Gaussian noise to generate visual features.\nAfter generating unseen samples, this family of approaches effectively\ntransforms the ZSL problem into a supervised classification scheme. However,\nthe existing models use a single semantic attribute, which contains the\ncomplete attribute information of the category. The generated data also carry\nthe complete attribute information, but in reality, visual samples usually have\nlimited attributes. Therefore, the generated data from attribute could have\nincomplete semantics. Based on this fact, we propose a novel framework to boost\nZSL by synthesizing diverse features. This method uses augmented semantic\nattributes to train the generative model, so as to simulate the real\ndistribution of visual features. We evaluate the proposed model on four\nbenchmark datasets, observing significant performance improvement against the\nstate-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaojie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yuming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shidong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haofeng Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NVS-MonoDepth: Improving Monocular Depth Prediction with Novel View Synthesis. (arXiv:2112.12577v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12577","description":"<p>Building upon the recent progress in novel view synthesis, we propose its\napplication to improve monocular depth estimation. In particular, we propose a\nnovel training method split in three main steps. First, the prediction results\nof a monocular depth network are warped to an additional view point. Second, we\napply an additional image synthesis network, which corrects and improves the\nquality of the warped RGB image. The output of this network is required to look\nas similar as possible to the ground-truth view by minimizing the pixel-wise\nRGB reconstruction error. Third, we reapply the same monocular depth estimation\nonto the synthesized second view point and ensure that the depth predictions\nare consistent with the associated ground truth depth. Experimental results\nprove that our method achieves state-of-the-art or comparable performance on\nthe KITTI and NYU-Depth-v2 datasets with a lightweight and simple vanilla U-Net\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bauer_Z/0/1/0/all/0/1\">Zuria Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuoyue Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orts_Escolano_S/0/1/0/all/0/1\">Sergio Orts-Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cazorla_M/0/1/0/all/0/1\">Miguel Cazorla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-efficient learning for 3D mirror symmetry detection. (arXiv:2112.12579v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12579","description":"<p>We introduce a geometry-inspired deep learning method for detecting 3D mirror\nplane from single-view images. We reduce the demand for massive training data\nby explicitly adding 3D mirror geometry into learning as an inductive prior. We\nextract semantic features, calculate intra-pixel correlations, and build a 3D\ncorrelation volume for each plane. The correlation volume indicates the extent\nto which the input resembles its mirrors at various depth, allowing us to\nidentify the likelihood of the given plane being a mirror plane. Subsequently,\nwe treat the correlation volumes as feature descriptors for sampled planes and\nmap them to a unit hemisphere where the normal of sampled planes lies. Lastly,\nwe design multi-stage spherical convolutions to identify the optimal mirror\nplane in a coarse-to-fine manner. Experiments on both synthetic and real-world\ndatasets show the benefit of 3D mirror geometry in improving data efficiency\nand inference speed (up to 25 FPS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yancong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1\">Silvia-Laura Pintea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan van Gemert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"INTRPRT: A Systematic Review of and Guidelines for Designing and Validating Transparent AI in Medical Image Analysis. (arXiv:2112.12596v1 [cs.HC])","link":"http://arxiv.org/abs/2112.12596","description":"<p>Transparency in Machine Learning (ML), attempts to reveal the working\nmechanisms of complex models. Transparent ML promises to advance human factors\nengineering goals of human-centered AI in the target users. From a\nhuman-centered design perspective, transparency is not a property of the ML\nmodel but an affordance, i.e. a relationship between algorithm and user; as a\nresult, iterative prototyping and evaluation with users is critical to\nattaining adequate solutions that afford transparency. However, following\nhuman-centered design principles in healthcare and medical image analysis is\nchallenging due to the limited availability of and access to end users. To\ninvestigate the state of transparent ML in medical image analysis, we conducted\na systematic review of the literature. Our review reveals multiple severe\nshortcomings in the design and validation of transparent ML for medical image\nanalysis applications. We find that most studies to date approach transparency\nas a property of the model itself, similar to task performance, without\nconsidering end users during neither development nor evaluation. Additionally,\nthe lack of user research, and the sporadic validation of transparency claims\nput contemporary research on transparent ML for medical image analysis at risk\nof being incomprehensible to users, and thus, clinically irrelevant. To\nalleviate these shortcomings in forthcoming research while acknowledging the\nchallenges of human-centered design in healthcare, we introduce the INTRPRT\nguideline, a systematic design directive for transparent ML systems in medical\nimage analysis. The INTRPRT guideline suggests formative user research as the\nfirst step of transparent model design to understand user needs and domain\nrequirements. Following this process produces evidence to support design\nchoices, and ultimately, increases the likelihood that the algorithms afford\ntransparency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haomin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_C/0/1/0/all/0/1\">Catalina Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chien-Ming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Universal GAN Image Detection. (arXiv:2112.12606v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12606","description":"<p>The ever higher quality and wide diffusion of fake images have spawn a quest\nfor reliable forensic tools. Many GAN image detectors have been proposed,\nrecently. In real world scenarios, however, most of them show limited\nrobustness and generalization ability. Moreover, they often rely on side\ninformation not available at test time, that is, they are not universal. We\ninvestigate these problems and propose a new GAN image detector based on a\nlimited sub-sampling architecture and a suitable contrastive learning paradigm.\nExperiments carried out in challenging conditions prove the proposed method to\nbe a first step towards universal GAN image detection, ensuring also good\nrobustness to common image impairments, and good generalization to unseen\narchitectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cozzolino_D/0/1/0/all/0/1\">Davide Cozzolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gragnaniello_D/0/1/0/all/0/1\">Diego Gragnaniello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poggi_G/0/1/0/all/0/1\">Giovanni Poggi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verdoliva_L/0/1/0/all/0/1\">Luisa Verdoliva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predi\\c{c}\\~ao da Idade Cerebral a partir de Imagens de Resson\\^ancia Magn\\'etica utilizando Redes Neurais Convolucionais. (arXiv:2112.12609v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12609","description":"<p>In this work, deep learning techniques for brain age prediction from magnetic\nresonance images are investigated, aiming to assist in the identification of\nbiomarkers of the natural aging process. The identification of biomarkers is\nuseful for detecting an early-stage neurodegenerative process, as well as for\npredicting age-related or non-age-related cognitive decline. Two techniques are\nimplemented and compared in this work: a 3D Convolutional Neural Network\napplied to the volumetric image and a 2D Convolutional Neural Network applied\nto slices from the axial plane, with subsequent fusion of individual\npredictions. The best result was obtained by the 2D model, which achieved a\nmean absolute error of 3.83 years.\n</p>\n<p>--\n</p>\n<p>Neste trabalho s\\~ao investigadas t\\'ecnicas de aprendizado profundo para a\npredi\\c{c}\\~ao da idade cerebral a partir de imagens de resson\\^ancia\nmagn\\'etica, visando auxiliar na identifica\\c{c}\\~ao de biomarcadores do\nprocesso natural de envelhecimento. A identifica\\c{c}\\~ao de biomarcadores \\'e\n\\'util para a detec\\c{c}\\~ao de um processo neurodegenerativo em est\\'agio\ninicial, al\\'em de possibilitar prever um decl\\'inio cognitivo relacionado ou\nn\\~ao \\`a idade. Duas t\\'ecnicas s\\~ao implementadas e comparadas neste\ntrabalho: uma Rede Neural Convolucional 3D aplicada na imagem volum\\'etrica e\numa Rede Neural Convolucional 2D aplicada a fatias do plano axial, com\nposterior fus\\~ao das predi\\c{c}\\~oes individuais. O melhor resultado foi\nobtido pelo modelo 2D, que alcan\\c{c}ou um erro m\\'edio absoluto de 3.83 anos.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Oliveira_V/0/1/0/all/0/1\">Victor H. R. Oliveira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Antunes_A/0/1/0/all/0/1\">Augusto Antunes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soares_A/0/1/0/all/0/1\">Alexandre S. Soares</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reys_A/0/1/0/all/0/1\">Arthur D. Reys</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Junior_R/0/1/0/all/0/1\">Robson Z. J&#xfa;nior</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pedro_S/0/1/0/all/0/1\">Saulo D. S. Pedro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Silva_D/0/1/0/all/0/1\">Danilo Silva</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving. (arXiv:2112.12610v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12610","description":"<p>The accelerating development of autonomous driving technology has placed\ngreater demands on obtaining large amounts of high-quality data.\nRepresentative, labeled, real world data serves as the fuel for training deep\nlearning networks, critical for improving self-driving perception algorithms.\nIn this paper, we introduce PandaSet, the first dataset produced by a complete,\nhigh-precision autonomous vehicle sensor kit with a no-cost commercial license.\nThe dataset was collected using one 360{\\deg} mechanical spinning LiDAR, one\nforward-facing, long-range LiDAR, and 6 cameras. The dataset contains more than\n100 scenes, each of which is 8 seconds long, and provides 28 types of labels\nfor object classification and 37 types of labels for semantic segmentation. We\nprovide baselines for LiDAR-only 3D object detection, LiDAR-camera fusion 3D\nobject detection and LiDAR point cloud segmentation. For more details about\nPandaSet and the development kit, see https://scale.com/open-datasets/pandaset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1\">Pengchuan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zhenlei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_S/0/1/0/all/0/1\">Steven Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zishuo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_X/0/1/0/all/0/1\">Xiaolin Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Judy Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zesong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Kai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kun Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diange Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Disturbance-Free Visual Mobile Manipulation. (arXiv:2112.12612v1 [cs.RO])","link":"http://arxiv.org/abs/2112.12612","description":"<p>Embodied AI has shown promising results on an abundance of robotic tasks in\nsimulation, including visual navigation and manipulation. The prior work\ngenerally pursues high success rates with shortest paths while largely ignoring\nthe problems caused by collision during interaction. This lack of\nprioritization is understandable: in simulated environments there is no\ninherent cost to breaking virtual objects. As a result, well-trained agents\nfrequently have catastrophic collision with objects despite final success. In\nthe robotics community, where the cost of collision is large, collision\navoidance is a long-standing and crucial topic to ensure that robots can be\nsafely deployed in the real world. In this work, we take the first step towards\ncollision/disturbance-free embodied AI agents for visual mobile manipulation,\nfacilitating safe deployment in real robots. We develop a new\ndisturbance-avoidance methodology at the heart of which is the auxiliary task\nof disturbance prediction. When combined with a disturbance penalty, our\nauxiliary task greatly enhances sample efficiency and final performance by\nknowledge distillation of disturbance into the agent. Our experiments on\nManipulaTHOR show that, on testing scenes with novel objects, our method\nimproves the success rate from 61.7% to 85.6% and the success rate without\ndisturbance from 29.8% to 50.2% over the original baseline. Extensive ablation\nstudies show the value of our pipelined approach. Project site is at\nhttps://sites.google.com/view/disturb-free\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_T/0/1/0/all/0/1\">Tianwei Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehsani_K/0/1/0/all/0/1\">Kiana Ehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1\">Luca Weihs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvador_J/0/1/0/all/0/1\">Jordi Salvador</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Manifold Learning Benefits GANs. (arXiv:2112.12618v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12618","description":"<p>In this paper, we improve Generative Adversarial Networks by incorporating a\nmanifold learning step into the discriminator. We consider locality-constrained\nlinear and subspace-based manifolds, and locality-constrained non-linear\nmanifolds. In our design, the manifold learning and coding steps are\nintertwined with layers of the discriminator, with the goal of attracting\nintermediate feature representations onto manifolds. We adaptively balance the\ndiscrepancy between feature representations and their manifold view, which\nrepresents a trade-off between denoising on the manifold and refining the\nmanifold. We conclude that locality-constrained non-linear manifolds have the\nupper hand over linear manifolds due to their non-uniform density and\nsmoothness. We show substantial improvements over different recent\nstate-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1\">Yao Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nock_R/0/1/0/all/0/1\">Richard Nock</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison and Analysis of Image-to-Image Generative Adversarial Networks: A Survey. (arXiv:2112.12625v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12625","description":"<p>Generative Adversarial Networks (GANs) have recently introduced effective\nmethods of performing Image-to-Image translations. These models can be applied\nand generalized to a variety of domains in Image-to-Image translation without\nchanging any parameters. In this paper, we survey and analyze eight\nImage-to-Image Generative Adversarial Networks: Pix2Px, CycleGAN, CoGAN,\nStarGAN, MUNIT, StarGAN2, DA-GAN, and Self Attention GAN. Each of these models\npresented state-of-the-art results and introduced new techniques to build\nImage-to-Image GANs. In addition to a survey of the models, we also survey the\n18 datasets they were trained on and the 9 metrics they were evaluated on.\nFinally, we present results of a controlled experiment for 6 of these models on\na common set of metrics and datasets. The results were mixed and showed that on\ncertain datasets, tasks, and metrics some models outperformed others. The last\nsection of this paper discusses those results and establishes areas of future\nresearch. As researchers continue to innovate new Image-to-Image GANs, it is\nimportant that they gain a good understanding of the existing methods,\ndatasets, and metrics. This paper provides a comprehensive overview and\ndiscussion to help build this foundation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxena_S/0/1/0/all/0/1\">Sagar Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teli_M/0/1/0/all/0/1\">Mohammad Nayeem Teli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InDuDoNet+: A Model-Driven Interpretable Dual Domain Network for Metal Artifact Reduction in CT Images. (arXiv:2112.12660v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12660","description":"<p>During the computed tomography (CT) imaging process, metallic implants within\npatients always cause harmful artifacts, which adversely degrade the visual\nquality of reconstructed CT images and negatively affect the subsequent\nclinical diagnosis. For the metal artifact reduction (MAR) task, current deep\nlearning based methods have achieved promising performance. However, most of\nthem share two main common limitations: 1) the CT physical imaging geometry\nconstraint is not comprehensively incorporated into deep network structures; 2)\nthe entire framework has weak interpretability for the specific MAR task;\nhence, the role of every network module is difficult to be evaluated. To\nalleviate these issues, in the paper, we construct a novel interpretable dual\ndomain network, termed InDuDoNet+, into which CT imaging process is finely\nembedded. Concretely, we derive a joint spatial and Radon domain reconstruction\nmodel and propose an optimization algorithm with only simple operators for\nsolving it. By unfolding the iterative steps involved in the proposed algorithm\ninto the corresponding network modules, we easily build the InDuDoNet+ with\nclear interpretability. Furthermore, we analyze the CT values among different\ntissues, and merge the prior observations into a prior network for our\nInDuDoNet+, which significantly improve its generalization performance.\nComprehensive experiments on synthesized data and clinical data substantiate\nthe superiority of the proposed methods as well as the superior generalization\nperformance beyond the current state-of-the-art (SOTA) MAR methods. Code is\navailable at \\url{https://github.com/hongwang01/InDuDoNet_plus}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hong Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haimiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Omni-Seg: A Single Dynamic Network for Multi-label Renal Pathology Image Segmentation using Partially Labeled Data. (arXiv:2112.12665v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12665","description":"<p>Computer-assisted quantitative analysis on Giga-pixel pathology images has\nprovided a new avenue in precision medicine. The innovations have been largely\nfocused on cancer pathology (i.e., tumor segmentation and characterization). In\nnon-cancer pathology, the learning algorithms can be asked to examine more\ncomprehensive tissue types simultaneously, as a multi-label setting. The prior\narts typically needed to train multiple segmentation networks in order to match\nthe domain-specific knowledge for heterogeneous tissue types (e.g., glomerular\ntuft, glomerular unit, proximal tubular, distal tubular, peritubular\ncapillaries, and arteries). In this paper, we propose a dynamic single\nsegmentation network (Omni-Seg) that learns to segment multiple tissue types\nusing partially labeled images (i.e., only one tissue type is labeled for each\ntraining image) for renal pathology. By learning from ~150,000 patch-wise\npathological images from six tissue types, the proposed Omni-Seg network\nachieved superior segmentation accuracy and less resource consumption when\ncompared to the previous the multiple-network and multi-head design. In the\ntesting stage, the proposed method obtains \"completely labeled\" tissue\nsegmentation results using only \"partially labeled\" training images. The source\ncode is available at https://github.com/ddrrnn123/Omni-Seg.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Deng_R/0/1/0/all/0/1\">Ruining Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cui_C/0/1/0/all/0/1\">Can Cui</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Skeleton-based Few-shot Action Recognition with JEANIE is not so Na\\\"ive. (arXiv:2112.12668v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12668","description":"<p>In this paper, we propose a Few-shot Learning pipeline for 3D skeleton-based\naction recognition by Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE).\nTo factor out misalignment between query and support sequences of 3D body\njoints, we propose an advanced variant of Dynamic Time Warping which jointly\nmodels each smooth path between the query and support frames to achieve\nsimultaneously the best alignment in the temporal and simulated camera\nviewpoint spaces for end-to-end learning under the limited few-shot training\ndata. Sequences are encoded with a temporal block encoder based on Simple\nSpectral Graph Convolution, a lightweight linear Graph Neural Network backbone\n(we also include a setting with a transformer). Finally, we propose a\nsimilarity-based loss which encourages the alignment of sequences of the same\nclass while preventing the alignment of unrelated sequences. We demonstrate\nstate-of-the-art results on NTU-60, NTU-120, Kinetics-skeleton and UWA3D\nMultiview Activity II.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1\">Piotr Koniusz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TagLab: A human-centric AI system for interactive semantic segmentation. (arXiv:2112.12702v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12702","description":"<p>Fully automatic semantic segmentation of highly specific semantic classes and\ncomplex shapes may not meet the accuracy standards demanded by scientists. In\nsuch cases, human-centered AI solutions, able to assist operators while\npreserving human control over complex tasks, are a good trade-off to speed up\nimage labeling while maintaining high accuracy levels. TagLab is an open-source\nAI-assisted software for annotating large orthoimages which takes advantage of\ndifferent degrees of automation; it speeds up image annotation from scratch\nthrough assisted tools, creates custom fully automatic semantic segmentation\nmodels, and, finally, allows the quick edits of automatic predictions. Since\nthe orthoimages analysis applies to several scientific disciplines, TagLab has\nbeen designed with a flexible labeling pipeline. We report our results in two\ndifferent scenarios, marine ecology, and architectural heritage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pavoni_G/0/1/0/all/0/1\">Gaia Pavoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Corsini_M/0/1/0/all/0/1\">Massimiliano Corsini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponchio_F/0/1/0/all/0/1\">Federico Ponchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muntoni_A/0/1/0/all/0/1\">Alessandro Muntoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cignoni_P/0/1/0/all/0/1\">Paolo Cignoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Digital Editions as Distant Supervision for Layout Analysis of Printed Books. (arXiv:2112.12703v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12703","description":"<p>Archivists, textual scholars, and historians often produce digital editions\nof historical documents. Using markup schemes such as those of the Text\nEncoding Initiative and EpiDoc, these digital editions often record documents'\nsemantic regions (such as notes and figures) and physical features (such as\npage and line breaks) as well as transcribing their textual content. We\ndescribe methods for exploiting this semantic markup as distant supervision for\ntraining and evaluating layout analysis models. In experiments with several\nmodel architectures on the half-million pages of the Deutsches Textarchiv\n(DTA), we find a high correlation of these region-level evaluation methods with\npixel-level and word-level metrics. We discuss the possibilities for improving\naccuracy with self-training and the ability of models trained on the DTA to\ngeneralize to other historical printed books.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Toselli_A/0/1/0/all/0/1\">Alejandro H. Toselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Si Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1\">David A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based Reconstruction for Fast MRI -- A Systematic Review and Meta-analysis. (arXiv:2112.12744v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12744","description":"<p>Compressed sensing (CS) has been playing a key role in accelerating the\nmagnetic resonance imaging (MRI) acquisition process. With the resurgence of\nartificial intelligence, deep neural networks and CS algorithms are being\nintegrated to redefine the state of the art of fast MRI. The past several years\nhave witnessed substantial growth in the complexity, diversity, and performance\nof deep learning-based CS techniques that are dedicated to fast MRI. In this\nmeta-analysis, we systematically review the deep learning-based CS techniques\nfor fast MRI, describe key model designs, highlight breakthroughs, and discuss\npromising directions. We have also introduced a comprehensive analysis\nframework and a classification system to assess the pivotal role of deep\nlearning in CS-based acceleration for MRI.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yutong Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schonlieb_C/0/1/0/all/0/1\">Carola-Bibiane Sch&#xf6;nlieb</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lio_P/0/1/0/all/0/1\">Pietro Li&#xf2;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leiner_T/0/1/0/all/0/1\">Tim Leiner</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dragotti_P/0/1/0/all/0/1\">Pier Luigi Dragotti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_G/0/1/0/all/0/1\">Ge Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Impact of Attention and Self-Attention Mechanisms on the Classification of Skin Lesions. (arXiv:2112.12748v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12748","description":"<p>Attention mechanisms have raised significant interest in the research\ncommunity, since they promise significant improvements in the performance of\nneural network architectures. However, in any specific problem, we still lack a\nprincipled way to choose specific mechanisms and hyper-parameters that lead to\nguaranteed improvements. More recently, self-attention has been proposed and\nwidely used in transformer-like architectures, leading to significant\nbreakthroughs in some applications. In this work we focus on two forms of\nattention mechanisms: attention modules and self-attention. Attention modules\nare used to reweight the features of each layer input tensor. Different modules\nhave different ways to perform this reweighting in fully connected or\nconvolutional layers. The attention models studied are completely modular and\nin this work they will be used with the popular ResNet architecture.\nSelf-Attention, originally proposed in the area of Natural Language Processing\nmakes it possible to relate all the items in an input sequence. Self-Attention\nis becoming increasingly popular in Computer Vision, where it is sometimes\ncombined with convolutional layers, although some recent architectures do away\nentirely with convolutions. In this work, we study and perform an objective\ncomparison of a number of different attention mechanisms in a specific computer\nvision task, the classification of samples in the widely used Skin Cancer MNIST\ndataset. The results show that attention modules do sometimes improve the\nperformance of convolutional neural network architectures, but also that this\nimprovement, although noticeable and statistically significant, is not\nconsistent in different settings. The results obtained with self-attention\nmechanisms, on the other hand, show consistent and significant improvements,\nleading to the best results even in architectures with a reduced number of\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pedro_R/0/1/0/all/0/1\">Rafael Pedro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1\">Arlindo L. Oliveira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SLIP: Self-supervision meets Language-Image Pre-training. (arXiv:2112.12750v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12750","description":"<p>Recent work has shown that self-supervised pre-training leads to improvements\nover supervised learning on challenging visual recognition tasks. CLIP, an\nexciting new approach to learning with language supervision, demonstrates\npromising performance on a wide variety of benchmarks. In this work, we explore\nwhether self-supervised learning can aid in the use of language supervision for\nvisual representation learning. We introduce SLIP, a multi-task learning\nframework for combining self-supervised learning and CLIP pre-training. After\npre-training with Vision Transformers, we thoroughly evaluate representation\nquality and compare performance to both CLIP and self-supervised learning under\nthree distinct settings: zero-shot transfer, linear classification, and\nend-to-end finetuning. Across ImageNet and a battery of additional datasets, we\nfind that SLIP improves accuracy by a large margin. We validate our results\nfurther with experiments on different model sizes, training schedules, and\npre-training datasets. Our findings show that SLIP enjoys the best of both\nworlds: better performance than self-supervision (+8.1% linear accuracy) and\nlanguage supervision (+5.2% zero-shot accuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mu_N/0/1/0/all/0/1\">Norman Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirillov_A/0/1/0/all/0/1\">Alexander Kirillov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wagner_D/0/1/0/all/0/1\">David Wagner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BANMo: Building Animatable 3D Neural Models from Many Casual Videos. (arXiv:2112.12761v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12761","description":"<p>Prior work for articulated 3D shape reconstruction often relies on\nspecialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D\ndeformable models (e.g., SMAL or SMPL). Such methods are not able to scale to\ndiverse sets of objects in the wild. We present BANMo, a method that requires\nneither a specialized sensor nor a pre-defined template shape. BANMo builds\nhigh-fidelity, articulated 3D models (including shape and animatable skinning\nweights) from many monocular casual videos in a differentiable rendering\nframework. While the use of many videos provides more coverage of camera views\nand object articulations, they introduce significant challenges in establishing\ncorrespondence across scenes with different backgrounds, illumination\nconditions, etc. Our key insight is to merge three schools of thought; (1)\nclassic deformable shape models that make use of articulated bones and blend\nskinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to\ngradient-based optimization, and (3) canonical embeddings that generate\ncorrespondences between pixels and an articulated model. We introduce neural\nblend skinning models that allow for differentiable and invertible articulated\ndeformations. When combined with canonical embeddings, such models allow us to\nestablish dense correspondences across videos that can be self-supervised with\ncycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity\n3D reconstructions than prior works for humans and animals, with the ability to\nrender realistic images from novel viewpoints and poses. Project webpage:\nbanmo-www.github.io .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gengshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanbyul Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross Modal Retrieval with Querybank Normalisation. (arXiv:2112.12777v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12777","description":"<p>Profiting from large-scale training datasets, advances in neural architecture\ndesign and efficient inference, joint embeddings have become the dominant\napproach for tackling cross-modal retrieval. In this work we first show that,\ndespite their effectiveness, state-of-the-art joint embeddings suffer\nsignificantly from the longstanding hubness problem in which a small number of\ngallery embeddings form the nearest neighbours of many queries. Drawing\ninspiration from the NLP literature, we formulate a simple but effective\nframework called Querybank Normalisation (QB-Norm) that re-normalises query\nsimilarities to account for hubs in the embedding space. QB-Norm improves\nretrieval performance without requiring retraining. Differently from prior\nwork, we show that QB-Norm works effectively without concurrent access to any\ntest set queries. Within the QB-Norm framework, we also propose a novel\nsimilarity normalisation method, the Dynamic Inverted Softmax, that is\nsignificantly more robust than existing approaches. We showcase QB-Norm across\na range of cross modal retrieval models and benchmarks where it consistently\nenhances strong baselines beyond the state of the art. Code is available at\nhttps://vladbogo.github.io/QB-Norm/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bogolin_S/0/1/0/all/0/1\">Simion-Vlad Bogolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croitoru_I/0/1/0/all/0/1\">Ioana Croitoru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1\">Samuel Albanie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeMask: Semantically Masked Transformers for Semantic Segmentation. (arXiv:2112.12782v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12782","description":"<p>Finetuning a pretrained backbone in the encoder part of an image transformer\nnetwork has been the traditional approach for the semantic segmentation task.\nHowever, such an approach leaves out the semantic context that an image\nprovides during the encoding stage. This paper argues that incorporating\nsemantic information of the image into pretrained hierarchical\ntransformer-based backbones while finetuning improves the performance\nconsiderably. To achieve this, we propose SeMask, a simple and effective\nframework that incorporates semantic information into the encoder with the help\nof a semantic attention operation. In addition, we use a lightweight semantic\ndecoder during training to provide supervision to the intermediate semantic\nprior maps at every stage. Our experiments demonstrate that incorporating\nsemantic priors enhances the performance of the established hierarchical\nencoders with a slight increase in the number of FLOPs. We provide empirical\nproof by integrating SeMask into each variant of the Swin-Transformer as our\nencoder paired with different decoders. Our framework achieves a new\nstate-of-the-art of 58.22% mIoU on the ADE20K dataset and improvements of over\n3% in the mIoU metric on the Cityscapes dataset. The code and checkpoints are\npublicly available at\nhttps://github.com/Picsart-AI-Research/SeMask-Segmentation .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_J/0/1/0/all/0/1\">Jitesh Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anukriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orlov_N/0/1/0/all/0/1\">Nikita Orlov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zilong Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NinjaDesc: Content-Concealing Visual Descriptors via Adversarial Learning. (arXiv:2112.12785v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12785","description":"<p>In the light of recent analyses on privacy-concerning scene revelation from\nvisual descriptors, we develop descriptors that conceal the input image\ncontent. In particular, we propose an adversarial learning framework for\ntraining visual descriptors that prevent image reconstruction, while\nmaintaining the matching accuracy. We let a feature encoding network and image\nreconstruction network compete with each other, such that the feature encoder\ntries to impede the image reconstruction with its generated descriptors, while\nthe reconstructor tries to recover the input image from the descriptors. The\nexperimental results demonstrate that the visual descriptors obtained with our\nmethod significantly deteriorate the image reconstruction quality with minimal\nimpact on correspondence matching and camera localization performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1\">Tony Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hyo Jin Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_V/0/1/0/all/0/1\">Vincent Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Detone_D/0/1/0/all/0/1\">Daniel Detone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tsun-Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianwei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilg_E/0/1/0/all/0/1\">Eddy Ilg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balntas_V/0/1/0/all/0/1\">Vassileios Balntas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sweeney_C/0/1/0/all/0/1\">Chris Sweeney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELSA: Enhanced Local Self-Attention for Vision Transformer. (arXiv:2112.12786v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12786","description":"<p>Self-attention is powerful in modeling long-range dependencies, but it is\nweak in local finer-level feature learning. The performance of local\nself-attention (LSA) is just on par with convolution and inferior to dynamic\nfilters, which puzzles researchers on whether to use LSA or its counterparts,\nwhich one is better, and what makes LSA mediocre. To clarify these, we\ncomprehensively investigate LSA and its counterparts from two sides:\n\\emph{channel setting} and \\emph{spatial processing}. We find that the devil\nlies in the generation and application of spatial attention, where relative\nposition embeddings and the neighboring filter application are key factors.\nBased on these findings, we propose the enhanced local self-attention (ELSA)\nwith Hadamard attention and the ghost head. Hadamard attention introduces the\nHadamard product to efficiently generate attention in the neighboring case,\nwhile maintaining the high-order mapping. The ghost head combines attention\nmaps with static matrices to increase channel capacity. Experiments demonstrate\nthe effectiveness of ELSA. Without architecture / hyperparameter modification,\ndrop-in replacing LSA with ELSA boosts Swin Transformer \\cite{swin} by up to\n+1.4 on top-1 accuracy. ELSA also consistently benefits VOLO \\cite{volo} from\nD1 to D5, where ELSA-VOLO-D5 achieves 87.2 on the ImageNet-1K without extra\ntraining images. In addition, we evaluate ELSA in downstream tasks. ELSA\nsignificantly improves the baseline by up to +1.9 box Ap / +1.3 mask Ap on the\nCOCO, and by up to +1.9 mIoU on the ADE20K. Code is available at\n\\url{https://github.com/damo-cv/ELSA}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingkai Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularity Normalization: Neuroscience-Inspired Unsupervised Attention across Neural Network Layers. (arXiv:1902.10658v13 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/1902.10658","description":"<p>Inspired by the adaptation phenomenon of neuronal firing, we propose the\nregularity normalization (RN) as an unsupervised attention mechanism (UAM)\nwhich computes the statistical regularity in the implicit space of neural\nnetworks under the Minimum Description Length (MDL) principle. Treating the\nneural network optimization process as a partially observable model selection\nproblem, the regularity normalization constrains the implicit space by a\nnormalization factor, the universal code length. We compute this universal code\nincrementally across neural network layers and demonstrate the flexibility to\ninclude data priors such as top-down attention and other oracle information.\nEmpirically, our approach outperforms existing normalization methods in\ntackling limited, imbalanced and non-stationary input distribution in image\nclassification, classic control, procedurally-generated reinforcement learning,\ngenerative modeling, handwriting generation and question answering tasks with\nvarious neural network architectures. Lastly, the unsupervised attention\nmechanisms is a useful probing tool for neural networks by tracking the\ndependency and critical learning stages across layers and recurrent time steps\nof deep networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SVGA-Net: Sparse Voxel-Graph Attention Network for 3D Object Detection from Point Clouds. (arXiv:2006.04043v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.04043","description":"<p>Accurate 3D object detection from point clouds has become a crucial component\nin autonomous driving. However, the volumetric representations and the\nprojection methods in previous works fail to establish the relationships\nbetween the local point sets. In this paper, we propose Sparse Voxel-Graph\nAttention Network (SVGA-Net), a novel end-to-end trainable network which mainly\ncontains voxel-graph module and sparse-to-dense regression module to achieve\ncomparable 3D detection tasks from raw LIDAR data. Specifically, SVGA-Net\nconstructs the local complete graph within each divided 3D spherical voxel and\nglobal KNN graph through all voxels. The local and global graphs serve as the\nattention mechanism to enhance the extracted features. In addition, the novel\nsparse-to-dense regression module enhances the 3D box estimation accuracy\nthrough feature maps aggregation at different levels. Experiments on KITTI\ndetection benchmark demonstrate the efficiency of extending the graph\nrepresentation to 3D object detection and the proposed SVGA-Net can achieve\ndecent detection accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qingdong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhengning Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ego2Hands: A Dataset for Egocentric Two-hand Segmentation and Detection. (arXiv:2011.07252v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2011.07252","description":"<p>Hand segmentation and detection in truly unconstrained RGB-based settings is\nimportant for many applications. However, existing datasets are far from\nsufficient in terms of size and variety due to the infeasibility of manual\nannotation of large amounts of segmentation and detection data. As a result,\ncurrent methods are limited by many underlying assumptions such as constrained\nenvironment, consistent skin color and lighting. In this work, we present\nEgo2Hands, a large-scale RGB-based egocentric hand segmentation/detection\ndataset that is semi-automatically annotated and a color-invariant\ncompositing-based data generation technique capable of creating training data\nwith large quantity and variety. For quantitative analysis, we manually\nannotated an evaluation set that significantly exceeds existing benchmarks in\nquantity, diversity and annotation accuracy. We provide cross-dataset\nevaluation as well as thorough analysis on the performance of state-of-the-art\nmodels on Ego2Hands to show that our dataset and data generation technique can\nproduce models that generalize to unseen environments without domain\nadaptation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fanqing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1\">Tony Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint. (arXiv:2102.08360v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.08360","description":"<p>Deep neural networks have increasingly been used as an auxiliary tool in\nhealthcare applications, due to their ability to improve performance of several\ndiagnosis tasks. However, these methods are not widely adopted in clinical\nsettings due to the practical limitations in the reliability, generalizability,\nand interpretability of deep learning based systems. As a result, methods have\nbeen developed that impose additional constraints during network training to\ngain more control as well as improve interpretabilty, facilitating their\nacceptance in healthcare community. In this work, we investigate the benefit of\nusing Orthogonal Spheres (OS) constraint for classification of COVID-19 cases\nfrom chest X-ray images. The OS constraint can be written as a simple\northonormality term which is used in conjunction with the standard\ncross-entropy loss during classification network training. Previous studies\nhave demonstrated significant benefits in applying such constraints to deep\nlearning models. Our findings corroborate these observations, indicating that\nthe orthonormality loss function effectively produces improved semantic\nlocalization via GradCAM visualizations, enhanced classification performance,\nand reduced model calibration error. Our approach achieves an improvement in\naccuracy of 1.6% and 4.8% for two- and three-class classification,\nrespectively; similar results are found for models with data augmentation\napplied. In addition to these findings, our work also presents a new\napplication of the OS regularizer in healthcare, increasing the post-hoc\ninterpretability and performance of deep learning models for COVID-19\nclassification to facilitate adoption of these methods in clinical settings. We\nalso identify the limitations of our strategy that can be explored for further\nresearch in future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Ella Y. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1\">Anirudh Som</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1\">Ankita Shukla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1\">Hongjun Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1\">Pavan Turaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PureGaze: Purifying Gaze Feature for Generalizable Gaze Estimation. (arXiv:2103.13173v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.13173","description":"<p>Gaze estimation methods learn eye gaze from facial features. However, among\nrich information in the facial image, real gaze-relevant features only\ncorrespond to subtle changes in eye region, while other gaze-irrelevant\nfeatures like illumination, personal appearance and even facial expression may\naffect the learning in an unexpected way. This is a major reason why existing\nmethods show significant performance degradation in cross-domain/dataset\nevaluation. In this paper, we tackle the cross-domain problem in gaze\nestimation. Different from common domain adaption methods, we propose a domain\ngeneralization method to improve the cross-domain performance without touching\ntarget samples. The domain generalization is realized by gaze feature\npurification. We eliminate gaze-irrelevant factors such as illumination and\nidentity to improve the cross-domain performance. We design a plug-and-play\nself-adversarial framework for the gaze feature purification. The framework\nenhances not only our baseline but also existing gaze estimation methods\ndirectly and significantly. To the best of our knowledge, we are the first to\npropose domain generalization methods in gaze estimation. Our method achieves\nnot only state-of-the-art performance among typical gaze estimation methods but\nalso competitive results among domain adaption methods. The code is released in\nhttps://github.com/yihuacheng/PureGaze.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yihua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1\">Yiwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Less is More: Pay Less Attention in Vision Transformers. (arXiv:2105.14217v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.14217","description":"<p>Transformers have become one of the dominant architectures in deep learning,\nparticularly as a powerful alternative to convolutional neural networks (CNNs)\nin computer vision. However, Transformer training and inference in previous\nworks can be prohibitively expensive due to the quadratic complexity of\nself-attention over a long sequence of representations, especially for\nhigh-resolution dense prediction tasks. To this end, we present a novel Less\nattention vIsion Transformer (LIT), building upon the fact that the early\nself-attention layers in Transformers still focus on local patterns and bring\nminor benefits in recent hierarchical vision Transformers. Specifically, we\npropose a hierarchical Transformer where we use pure multi-layer perceptrons\n(MLPs) to encode rich local patterns in the early stages while applying\nself-attention modules to capture longer dependencies in deeper layers.\nMoreover, we further propose a learned deformable token merging module to\nadaptively fuse informative patches in a non-uniform manner. The proposed LIT\nachieves promising performance on image recognition tasks, including image\nclassification, object detection and instance segmentation, serving as a strong\nbackbone for many vision tasks. Code is available at:\nhttps://github.com/zhuang-group/LIT\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1\">Bohan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Haoyu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jianfei Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action Transformer: A Self-Attention Model for Short-Time Human Action Recognition. (arXiv:2107.00606v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00606","description":"<p>Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time,\nshort-time HAR. The proposed methodology was extensively tested on MPOSE2021\nand compared to several state-of-the-art architectures, proving the\neffectiveness of the AcT model and laying the foundations for future work on\nHAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Measuring and Controlling the Spectral Bias of the Deep Image Prior. (arXiv:2107.01125v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.01125","description":"<p>The deep image prior showed that a randomly initialized network with a\nsuitable architecture can be trained to solve inverse imaging problems by\nsimply optimizing it's parameters to reconstruct a single degraded image.\nHowever, it suffers from two practical limitations. First, it remains unclear\nhow to control the prior beyond the choice of the network architecture. Second,\ntraining requires an oracle stopping criterion as during the optimization the\nperformance degrades after reaching an optimum value. To address these\nchallenges we introduce a frequency-band correspondence measure to characterize\nthe spectral bias of the deep image prior, where low-frequency image signals\nare learned faster and better than high-frequency counterparts. Based on our\nobservations, we propose techniques to prevent the eventual performance\ndegradation and accelerate convergence. We introduce a Lipschitz-controlled\nconvolution layer and a Gaussian-controlled upsampling layer as plug-in\nreplacements for layers used in the deep architectures. The experiments show\nthat with these changes the performance does not degrade during optimization,\nrelieving us from the need for an oracle stopping criterion. We further outline\na stopping criterion to avoid superfluous computation. Finally, we show that\nour approach obtains favorable results compared to current approaches across\nvarious denoising, deblocking, inpainting, super-resolution and detail\nenhancement tasks. Code is available at\n\\url{https://github.com/shizenglin/Measure-and-Control-Spectral-Bias}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zenglin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Pruning Method Based on DenseNet Network for Image Classification. (arXiv:2108.12604v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12604","description":"<p>Deep neural networks have made significant progress in the field of computer\nvision. Recent studies have shown that depth, width and shortcut connections of\nneural network architectures play a crucial role in their performance. One of\nthe most advanced neural network architectures, DenseNet, has achieved\nexcellent convergence rates through dense connections. However, it still has\nobvious shortcomings in the usage of amount of memory. In this paper, we\nintroduce a new type of pruning tool, threshold, which refers to the principle\nof the threshold voltage in MOSFET. This work employs this method to connect\nblocks of different depths in different ways to reduce the usage of memory. It\nis denoted as ThresholdNet. We evaluate ThresholdNet and other different\nnetworks on datasets of CIFAR10. Experiments show that HarDNet is twice as fast\nas DenseNet, and on this basis, ThresholdNet is 10% faster and 10% lower error\nrate than HarDNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_R/0/1/0/all/0/1\">Rui-Yang Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Ting-Yu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1\">Jen-Shiun Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Data Selection for Data-Centric Semi-Supervised Learning. (arXiv:2110.03006v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03006","description":"<p>We study unsupervised data selection for semi-supervised learning (SSL),\nwhere a large-scale unlabeled dataset is available and a small subset of data\nis budgeted for label acquisition. Existing SSL methods focus on learning a\nmodel that effectively integrates information from given small labeled data and\nlarge unlabeled data, whereas we focus on selecting the right data to annotate\nfor SSL without requiring any label or task information. Intuitively, instances\nto be labeled shall collectively have maximum diversity and coverage for\ndownstream tasks, and individually have maximum information propagation utility\nfor SSL. We formalize these concepts in a three-step data-centric SSL method\nthat improves FixMatch in stability and accuracy by 8% on CIFAR-10 (0.08%\nlabeled) and 14% on ImageNet-1K (0.2% labeled). It is also a universal\nframework that works with various SSL methods, delivering consistent\nperformance gains. Our work demonstrates that small computation spent on\ncarefully selecting data for annotation brings big annotation efficiency and\nmodel performance gain without changing the learning pipeline. Our completely\nunsupervised data selection can be easily extended to other weakly supervised\nlearning settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xudong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_L/0/1/0/all/0/1\">Long Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Stella X. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Adaptation: Historical Contrastive Learning for Unsupervised Domain Adaptation without Source Data. (arXiv:2110.03374v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.03374","description":"<p>Unsupervised domain adaptation aims to align a labeled source domain and an\nunlabeled target domain, but it requires to access the source data which often\nraises concerns in data privacy, data portability and data transmission\nefficiency. We study unsupervised model adaptation (UMA), or called\nUnsupervised Domain Adaptation without Source Data, an alternative setting that\naims to adapt source-trained models towards target distributions without\naccessing source data. To this end, we design an innovative historical\ncontrastive learning (HCL) technique that exploits historical source hypothesis\nto make up for the absence of source data in UMA. HCL addresses the UMA\nchallenge from two perspectives. First, it introduces historical contrastive\ninstance discrimination (HCID) that learns from target samples by contrasting\ntheir embeddings which are generated by the currently adapted model and the\nhistorical models. With the historical models, HCID encourages UMA to learn\ninstance-discriminative target representations while preserving the source\nhypothesis. Second, it introduces historical contrastive category\ndiscrimination (HCCD) that pseudo-labels target samples to learn\ncategory-discriminative target representations. Specifically, HCCD re-weights\npseudo labels according to their prediction consistency across the current and\nhistorical models. Extensive experiments show that HCL outperforms and\nstate-of-the-art methods consistently across a variety of visual tasks and\nsetups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1\">Dayan Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">Aoran Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capacity of Group-invariant Linear Readouts from Equivariant Representations: How Many Objects can be Linearly Classified Under All Possible Views?. (arXiv:2110.07472v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.07472","description":"<p>Equivariance has emerged as a desirable property of representations of\nobjects subject to identity-preserving transformations that constitute a group,\nsuch as translations and rotations. However, the expressivity of a\nrepresentation constrained by group equivariance is still not fully understood.\nWe address this gap by providing a generalization of Cover's Function Counting\nTheorem that quantifies the number of linearly separable and group-invariant\nbinary dichotomies that can be assigned to equivariant representations of\nobjects. We find that the fraction of separable dichotomies is determined by\nthe dimension of the space that is fixed by the group action. We show how this\nrelation extends to operations such as convolutions, element-wise\nnonlinearities, and global and local pooling. While other operations do not\nchange the fraction of separable dichotomies, local pooling decreases the\nfraction, despite being a highly nonlinear operation. Finally, we test our\ntheory on intermediate representations of randomly initialized and fully\ntrained convolutional neural networks and find perfect agreement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Farrell_M/0/1/0/all/0/1\">Matthew Farrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bordelon_B/0/1/0/all/0/1\">Blake Bordelon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_S/0/1/0/all/0/1\">Shubhendu Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1\">Cengiz Pehlevan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Asymmetric Hashing with Dual Semantic Regression and Class Structure Quantization. (arXiv:2110.12478v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.12478","description":"<p>Recently, deep hashing methods have been widely used in image retrieval task.\nMost existing deep hashing approaches adopt one-to-one quantization to reduce\ninformation loss. However, such class-unrelated quantization cannot give\ndiscriminative feedback for network training. In addition, these methods only\nutilize single label to integrate supervision information of data for hashing\nfunction learning, which may result in inferior network generalization\nperformance and relatively low-quality hash codes since the inter-class\ninformation of data is totally ignored. In this paper, we propose a dual\nsemantic asymmetric hashing (DSAH) method, which generates discriminative hash\ncodes under three-fold constraints. Firstly, DSAH utilizes class prior to\nconduct class structure quantization so as to transmit class information during\nthe quantization process. Secondly, a simple yet effective label mechanism is\ndesigned to characterize both the intra-class compactness and inter-class\nseparability of data, thereby achieving semantic-sensitive binary code\nlearning. Finally, a meaningful pairwise similarity preserving loss is devised\nto minimize the distances between class-related network outputs based on an\naffinity graph. With these three main components, high-quality hash codes can\nbe generated through network. Extensive experiments conducted on various\ndatasets demonstrate the superiority of DSAH in comparison with\nstate-of-the-art deep hashing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jianglin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hailing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1\">Mengfan Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Jiajun Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novel coronavirus pneumonia lesion segmentation in CT images. (arXiv:2110.12827v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.12827","description":"<p>Background: The 2019 novel coronavirus disease (COVID-19) has been spread\nwidely in the world, causing a huge threat to people's living environment.\nObjective: Under computed tomography (CT) imaging, the structure features of\nCOVID-19 lesions are complicated and varied greatly in different cases. To\naccurately locate COVID-19 lesions and assist doctors to make the best\ndiagnosis and treatment plan, a deep-supervised ensemble learning network is\npresented for COVID-19 lesion segmentation in CT images. Methods: Considering\nthe fact that a large number of COVID-19 CT images and the corresponding lesion\nannotations are difficult to obtained, a transfer learning strategy is employed\nto make up for the shortcoming and alleviate the overfitting problem. Based on\nthe reality that traditional single deep learning framework is difficult to\nextract COVID-19 lesion features effectively, which may cause some lesions to\nbe undetected. To overcome the problem, a deep-supervised ensemble learning\nnetwork is presented to combine with local and global features for COVID-19\nlesion segmentation. Results: The performance of the proposed method was\nvalidated in experiments with a publicly available dataset. Compared with\nmanual annotations, the proposed method acquired a high intersection over union\n(IoU) of 0.7279. Conclusion: A deep-supervised ensemble learning network was\npresented for coronavirus pneumonia lesion segmentation in CT images. The\neffectiveness of the proposed method was verified by visual inspection and\nquantitative evaluation. Experimental results shown that the proposed mehtod\nhas a perfect performance in COVID-19 lesion segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_Y/0/1/0/all/0/1\">Yuanyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zixu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tu_H/0/1/0/all/0/1\">Hongbin Tu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1\">Xiong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influential Prototypical Networks for Few Shot Learning: A Dermatological Case Study. (arXiv:2111.00698v4 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.00698","description":"<p>Prototypical network (PN) is a simple yet effective few shot learning\nstrategy. It is a metric-based meta-learning technique where classification is\nperformed by computing Euclidean distances to prototypical representations of\neach class. Conventional PN attributes equal importance to all samples and\ngenerates prototypes by simply averaging the support sample embeddings\nbelonging to each class. In this work, we propose a novel version of PN that\nattributes weights to support samples corresponding to their influence on the\nsupport sample distribution. Influence weights of samples are calculated based\non maximum mean discrepancy (MMD) between the mean embeddings of sample\ndistributions including and excluding the sample. Comprehensive evaluation of\nour proposed influential PN (IPNet) is performed by comparing its performance\nwith other baseline PNs on three different benchmark dermatological datasets.\nIPNet outperforms all baseline models with compelling results across all three\ndatasets and various N-way, K-shot classification tasks. Findings from\ncross-domain adaptation experiments further establish the robustness and\ngeneralizability of IPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_R/0/1/0/all/0/1\">Ranjana Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Overcoming the Domain Gap in Neural Action Representations. (arXiv:2112.01176v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01176","description":"<p>Relating animal behaviors to brain activity is a fundamental goal in\nneuroscience, with practical applications in building robust brain-machine\ninterfaces. However, the domain gap between individuals is a major issue that\nprevents the training of general models that work on unlabeled subjects.\n</p>\n<p>Since 3D pose data can now be reliably extracted from multi-view video\nsequences without manual intervention, we propose to use it to guide the\nencoding of neural action representations together with a set of neural and\nbehavioral augmentations exploiting the properties of microscopy imaging. To\nreduce the domain gap, during training, we swap neural and behavioral data\nacross animals that seem to be performing similar actions.\n</p>\n<p>To demonstrate this, we test our methods on three very different multimodal\ndatasets; one that features flies and their neural activity, one that contains\nhuman neural Electrocorticography (ECoG) data, and lastly the RGB video data of\nhuman activities from different viewpoints.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gunel_S/0/1/0/all/0/1\">Semih G&#xfc;nel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aymanns_F/0/1/0/all/0/1\">Florian Aymanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honari_S/0/1/0/all/0/1\">Sina Honari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramdya_P/0/1/0/all/0/1\">Pavan Ramdya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1\">Pascal Fua</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn2Reg: comprehensive multi-task medical image registration challenge, dataset and evaluation in the era of deep learning. (arXiv:2112.04489v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.04489","description":"<p>Image registration is a fundamental medical image analysis task, and a wide\nvariety of approaches have been proposed. However, only a few studies have\ncomprehensively compared medical image registration approaches on a wide range\nof clinically relevant tasks, in part because of the lack of availability of\nsuch diverse data. This limits the development of registration methods, the\nadoption of research advances into practice, and a fair benchmark across\ncompeting approaches. The Learn2Reg challenge addresses these limitations by\nproviding a multi-task medical image registration benchmark for comprehensive\ncharacterisation of deformable registration algorithms. A continuous evaluation\nwill be possible at https://learn2reg.grand-challenge.org. Learn2Reg covers a\nwide range of anatomies (brain, abdomen, and thorax), modalities (ultrasound,\nCT, MR), availability of annotations, as well as intra- and inter-patient\nregistration evaluation. We established an easily accessible framework for\ntraining and validation of 3D registration methods, which enabled the\ncompilation of results of over 65 individual method submissions from more than\n20 unique teams. We used a complementary set of metrics, including robustness,\naccuracy, plausibility, and runtime, enabling unique insight into the current\nstate-of-the-art of medical image registration. This paper describes datasets,\ntasks, evaluation methods and results of the challenge, and the results of\nfurther analysis of transferability to new datasets, the importance of label\nsupervision, and resulting bias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hering_A/0/1/0/all/0/1\">Alessa Hering</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hansen_L/0/1/0/all/0/1\">Lasse Hansen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mok_T/0/1/0/all/0/1\">Tony C. W. Mok</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_A/0/1/0/all/0/1\">Albert C. S. Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Siebert_H/0/1/0/all/0/1\">Hanna Siebert</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hager_S/0/1/0/all/0/1\">Stephanie H&#xe4;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_A/0/1/0/all/0/1\">Annkristin Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuckertz_S/0/1/0/all/0/1\">Sven Kuckertz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heldmann_S/0/1/0/all/0/1\">Stefan Heldmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vesal_S/0/1/0/all/0/1\">Sulaiman Vesal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rusu_M/0/1/0/all/0/1\">Mirabela Rusu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sonn_G/0/1/0/all/0/1\">Geoffrey Sonn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Estienne_T/0/1/0/all/0/1\">Th&#xe9;o Estienne</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_L/0/1/0/all/0/1\">Luyi Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yunzhi Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brudfors_M/0/1/0/all/0/1\">Mikael Brudfors</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balbastre_Y/0/1/0/all/0/1\">Ya&#xeb;l Balbastre</a>, <a href=\"http://arxiv.org/find/eess/1/au:+outard_S/0/1/0/all/0/1\">SamuelJ outard</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lifshitz_G/0/1/0/all/0/1\">Gal Lifshitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raviv_D/0/1/0/all/0/1\">Dan Raviv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jinxin Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jaouen_V/0/1/0/all/0/1\">Vincent Jaouen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Visvikis_D/0/1/0/all/0/1\">Dimitris Visvikis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fourcade_C/0/1/0/all/0/1\">Constance Fourcade</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rubeaux_M/0/1/0/all/0/1\">Mathieu Rubeaux</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pan_W/0/1/0/all/0/1\">Wentao Pan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1\">Zhe Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jian_B/0/1/0/all/0/1\">Bailiang Jian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benetti_F/0/1/0/all/0/1\">Francesca De Benetti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wodzinski_M/0/1/0/all/0/1\">Marek Wodzinski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gunnarsson_N/0/1/0/all/0/1\">Niklas Gunnarsson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sjolund_J/0/1/0/all/0/1\">Jens Sj&#xf6;lund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiu_H/0/1/0/all/0/1\">Huaqi Qiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zeju Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grossbrohmer_C/0/1/0/all/0/1\">Christoph Gro&#xdf;br&#xf6;hmer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hoopes_A/0/1/0/all/0/1\">Andrew Hoopes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reinertsen_I/0/1/0/all/0/1\">Ingerid Reinertsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Y/0/1/0/all/0/1\">Yiming Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Landman_B/0/1/0/all/0/1\">Bennett Landman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Murphy_K/0/1/0/all/0/1\">Keelin Murphy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lessmann_N/0/1/0/all/0/1\">Nikolas Lessmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heinrich_M/0/1/0/all/0/1\">Mattias P. Heinrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Methods for Aggregated Domain Generalization. (arXiv:2112.04766v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.04766","description":"<p>Domain generalization involves learning a classifier from a heterogeneous\ncollection of training sources such that it generalizes to data drawn from\nsimilar unknown target domains, with applications in large-scale learning and\npersonalized inference. In many settings, privacy concerns prohibit obtaining\ndomain labels for the training data samples, and instead only have an\naggregated collection of training points. Existing approaches that utilize\ndomain labels to create domain-invariant feature representations are\ninapplicable in this setting, requiring alternative approaches to learn\ngeneralizable classifiers. In this paper, we propose a domain-adaptive approach\nto this problem, which operates in two steps: (a) we cluster training data\nwithin a carefully chosen feature space to create pseudo-domains, and (b) using\nthese pseudo-domains we learn a domain-adaptive classifier that makes\npredictions using information about both the input and the pseudo-domain it\nbelongs to. Our approach achieves state-of-the-art performance on a variety of\ndomain generalization benchmarks without using domain labels whatsoever.\nFurthermore, we provide novel theoretical guarantees on domain generalization\nusing cluster information. Our approach is amenable to ensemble-based methods\nand provides substantial gains even on large-scale benchmark datasets. The code\ncan be found at: https://github.com/xavierohan/AdaClust_DomainBed\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thomas_X/0/1/0/all/0/1\">Xavier Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Dhruv Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pentland_A/0/1/0/all/0/1\">Alex Pentland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1\">Abhimanyu Dubey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ScaleNet: A Shallow Architecture for Scale Estimation. (arXiv:2112.04846v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.04846","description":"<p>In this paper, we address the problem of estimating scale factors between\nimages. We formulate the scale estimation problem as a prediction of a\nprobability distribution over scale factors. We design a new architecture,\nScaleNet, that exploits dilated convolutions as well as self and\ncross-correlation layers to predict the scale between images. We demonstrate\nthat rectifying images with estimated scales leads to significant performance\nimprovements for various tasks and methods. Specifically, we show how ScaleNet\ncan be combined with sparse local features and dense correspondence networks to\nimprove camera pose estimation, 3D reconstruction, or dense geometric matching\nin different benchmarks and datasets. We provide an extensive evaluation on\nseveral tasks and analyze the computational overhead of ScaleNet. The code,\nevaluation protocols, and trained models are publicly available at\nhttps://github.com/axelBarroso/ScaleNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barroso_Laguna_A/0/1/0/all/0/1\">Axel Barroso-Laguna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yurun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikolajczyk_K/0/1/0/all/0/1\">Krystian Mikolajczyk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Paced Deep Regression Forests with Consideration on Ranking Fairness. (arXiv:2112.06455v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06455","description":"<p>Deep discriminative models (DDMs), such as deep regression forests, deep\nneural decision forests, have been extensively studied recently to solve\nproblems like facial age estimation, head pose estimation, gaze estimation and\nso forth. Such problems are challenging in part because a large amount of\neffective training data without noise and bias is often not available. While\nsome progress has been achieved through learning more discriminative features,\nor reweighting samples, we argue what is more desirable is to learn gradually\nto discriminate like human beings. Then, we resort to self-paced learning\n(SPL). But a natural question arises: can self-paced regime lead DDMs to\nachieve more robust and less biased solutions? A serious problem with SPL,\nwhich is firstly discussed by this work, is it tends to aggravate the bias of\nsolutions, especially for obvious imbalanced data. To this end, this paper\nproposes a new self-paced paradigm for deep discriminative model, which\ndistinguishes noisy and underrepresented examples according to the output\nlikelihood and entropy associated with each example, and tackle the fundamental\nranking problem in SPL from a new perspective: fairness. This paradigm is\nfundamental, and could be easily combined with a variety of DDMs. Extensive\nexperiments on three computer vision tasks, such as facial age estimation, head\npose estimation and gaze estimation, demonstrate the efficacy of our paradigm.\nTo the best of our knowledge, our work is the first paper in the literature of\nSPL that considers ranking fairness for self-paced regime construction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1\">Lili Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1\">Mingming Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Yazhou Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yali Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zenglin Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved YOLOv5 network for real-time multi-scale traffic sign detection. (arXiv:2112.08782v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08782","description":"<p>Traffic sign detection is a challenging task for the unmanned driving system,\nespecially for the detection of multi-scale targets and the real-time problem\nof detection. In the traffic sign detection process, the scale of the targets\nchanges greatly, which will have a certain impact on the detection accuracy.\nFeature pyramid is widely used to solve this problem but it might break the\nfeature consistency across different scales of traffic signs. Moreover, in\npractical application, it is difficult for common methods to improve the\ndetection accuracy of multi-scale traffic signs while ensuring real-time\ndetection. In this paper, we propose an improved feature pyramid model, named\nAF-FPN, which utilizes the adaptive attention module (AAM) and feature\nenhancement module (FEM) to reduce the information loss in the process of\nfeature map generation and enhance the representation ability of the feature\npyramid. We replaced the original feature pyramid network in YOLOv5 with\nAF-FPN, which improves the detection performance for multi-scale targets of the\nYOLOv5 network under the premise of ensuring real-time detection. Furthermore,\na new automatic learning data augmentation method is proposed to enrich the\ndataset and improve the robustness of the model to make it more suitable for\npractical scenarios. Extensive experimental results on the Tsinghua-Tencent\n100K (TT100K) dataset demonstrate the effectiveness and superiority of the\nproposed method when compared with several state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junfan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1\">Zhekang Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Align and Prompt: Video-and-Language Pre-training with Entity Prompts. (arXiv:2112.09583v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.09583","description":"<p>Video-and-language pre-training has shown promising improvements on various\ndownstream tasks. Most previous methods capture cross-modal interactions with a\ntransformer-based multimodal encoder, not fully addressing the misalignment\nbetween unimodal video and text features. Besides, learning fine-grained\nvisual-language alignment usually requires off-the-shelf object detectors to\nprovide object information, which is bottlenecked by the detector's limited\nvocabulary and expensive computation cost.\n</p>\n<p>We propose Align and Prompt: an efficient and effective video-and-language\npre-training framework with better cross-modal alignment. First, we introduce a\nvideo-text contrastive (VTC) loss to align unimodal video-text features at the\ninstance level, which eases the modeling of cross-modal interactions. Then, we\npropose a new visually-grounded pre-training task, prompting entity modeling\n(PEM), which aims to learn fine-grained region-entity alignment. To achieve\nthis, we first introduce an entity prompter module, which is trained with VTC\nto produce the similarity between a video crop and text prompts instantiated\nwith entity names. The PEM task then asks the model to predict the entity\npseudo-labels (i.e~normalized similarity scores) for randomly-selected video\ncrops. The resulting pre-trained model achieves state-of-the-art performance on\nboth text-video retrieval and videoQA, outperforming prior work by a\nsubstantial margin. Our code and pre-trained models are available at\nhttps://github.com/salesforce/ALPRO.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niebles_J/0/1/0/all/0/1\">Juan Carlos Niebles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOIT: Segmenting Objects with Instance-Aware Transformers. (arXiv:2112.11037v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11037","description":"<p>This paper presents an end-to-end instance segmentation framework, termed\nSOIT, that Segments Objects with Instance-aware Transformers. Inspired by DETR\n\\cite{carion2020end}, our method views instance segmentation as a direct set\nprediction problem and effectively removes the need for many hand-crafted\ncomponents like RoI cropping, one-to-many label assignment, and non-maximum\nsuppression (NMS). In SOIT, multiple queries are learned to directly reason a\nset of object embeddings of semantic category, bounding-box location, and\npixel-wise mask in parallel under the global image context. The class and\nbounding-box can be easily embedded by a fixed-length vector. The pixel-wise\nmask, especially, is embedded by a group of parameters to construct a\nlightweight instance-aware transformer. Afterward, a full-resolution mask is\nproduced by the instance-aware transformer without involving any RoI-based\noperation. Overall, SOIT introduces a simple single-stage instance segmentation\nframework that is both RoI- and NMS-free. Experimental results on the MS COCO\ndataset demonstrate that SOIT outperforms state-of-the-art instance\nsegmentation approaches significantly. Moreover, the joint learning of multiple\ntasks in a unified query embedding can also substantially improve the detection\nperformance. Code is available at \\url{https://github.com/yuxiaodongHRI/SOIT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dahu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Ye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tingqun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-23T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}