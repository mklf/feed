{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-26T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision. (arXiv:2108.10904v1 [cs.CV])","link":"http://arxiv.org/abs/2108.10904","description":"<p>With recent progress in joint modeling of visual and textual representations,\nVision-Language Pretraining (VLP) has achieved impressive performance on many\nmultimodal downstream tasks. However, the requirement for expensive annotations\nincluding clean image captions and regional labels limits the scalability of\nexisting approaches, and complicates the pretraining procedure with the\nintroduction of multiple dataset-specific objectives. In this work, we relax\nthese constraints and present a minimalist pretraining framework, named Simple\nVisual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training\ncomplexity by exploiting large-scale weak supervision, and is trained\nend-to-end with a single prefix language modeling objective. Without utilizing\nextra data or task-specific customization, the resulting model significantly\noutperforms previous pretraining methods and achieves new state-of-the-art\nresults on a wide range of discriminative and generative vision-language\nbenchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE\n(+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score).\nFurthermore, we demonstrate that SimVLM acquires strong generalization and\ntransfer ability, enabling zero-shot behavior including open-ended visual\nquestion answering and cross-modality transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiahui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1\">Adams Wei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Multisource Feature Fusion for the Text Clustering. (arXiv:2108.10926v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10926","description":"<p>The text clustering technique is an unsupervised text mining method which are\nused to partition a huge amount of text documents into groups. It has been\nreported that text clustering algorithms are hard to achieve better performance\nthan supervised methods and their clustering performance is highly dependent on\nthe picked text features. Currently, there are many different types of text\nfeature generation algorithms, each of which extracts text features from some\nspecific aspects, such as VSM and distributed word embedding, thus seeking a\nnew way of obtaining features as complete as possible from the corpus is the\nkey to enhance the clustering effects. In this paper, we present a hybrid\nmultisource feature fusion (HMFF) framework comprising three components,\nfeature representation of multimodel, mutual similarity matrices and feature\nfusion, in which we construct mutual similarity matrices for each feature\nsource and fuse discriminative features from mutual similarity matrices by\nreducing dimensionality to generate HMFF features, then k-means clustering\nalgorithm could be configured to partition input samples into groups. The\nexperimental tests show our HMFF framework outperforms other recently published\nalgorithms on 7 of 11 public benchmark datasets and has the leading performance\non the rest 4 benchmark datasets as well. At last, we compare HMFF framework\nwith those competitors on a COVID-19 dataset from the wild with the unknown\ncluster count, which shows the clusters generated by HMFF framework partition\nthose similar samples much closer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1\">Shenglin Gui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The State of SLIVAR: What's next for robots, human-robot interaction, and (spoken) dialogue systems?. (arXiv:2108.10931v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10931","description":"<p>We synthesize the reported results and recommendations of recent workshops\nand seminars that convened to discuss open questions within the important\nintersection of robotics, human-robot interaction, and spoken dialogue systems\nresearch. The goal of this growing area of research interest is to enable\npeople to more effectively and naturally communicate with robots. To carry\nforward opportunities networking and discussion towards concrete, potentially\nfundable projects, we encourage interested parties to consider participating in\nfuture virtual and in-person discussions and workshops.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SN Computer Science: Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10939","description":"<p>Offensive Language detection in social media platforms has been an active\nfield of research over the past years. In non-native English spoken countries,\nsocial media users mostly use a code-mixed form of text in their\nposts/comments. This poses several challenges in the offensive content\nidentification tasks, and considering the low resources available for Tamil,\nthe task becomes much harder. The current study presents extensive experiments\nusing multiple deep learning, and transfer learning models to detect offensive\ncontent on YouTube. We propose a novel and flexible approach of selective\ntranslation and transliteration techniques to reap better results from\nfine-tuning and ensembling multilingual transformer networks like BERT, Distil-\nBERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best\nmodel for this task. The best performing models were ULMFiT and mBERTBiLSTM for\nthis Tamil code-mix dataset instead of more popular transfer learning models\nsuch as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The\nproposed model ULMFiT and mBERTBiLSTM yielded good results and are promising\nfor effective offensive speech identification in low-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasantharajan_C/0/1/0/all/0/1\">Charangan Vasantharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayasivam_U/0/1/0/all/0/1\">Uthayasanker Thayasivam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robustness Evaluation of Entity Disambiguation Using Prior Probes:the Case of Entity Overshadowing. (arXiv:2108.10949v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10949","description":"<p>Entity disambiguation (ED) is the last step of entity linking (EL), when\ncandidate entities are reranked according to the context they appear in. All\ndatasets for training and evaluating models for EL consist of convenience\nsamples, such as news articles and tweets, that propagate the prior probability\nbias of the entity distribution towards more frequently occurring entities. It\nwas previously shown that the performance of the EL systems on such datasets is\noverestimated since it is possible to obtain higher accuracy scores by merely\nlearning the prior. To provide a more adequate evaluation benchmark, we\nintroduce the ShadowLink dataset, which includes 16K short text snippets\nannotated with entity mentions. We evaluate and report the performance of\npopular EL systems on the ShadowLink benchmark. The results show a considerable\ndifference in accuracy between more and less common entities for all of the EL\nsystems under evaluation, demonstrating the effects of prior probability bias\nand entity overshadowing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Provatorova_V/0/1/0/all/0/1\">Vera Provatorova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakulenko_S/0/1/0/all/0/1\">Svitlana Vakulenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargav_S/0/1/0/all/0/1\">Samarth Bhargav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using BERT Encoding and Sentence-Level Language Model for Sentence Ordering. (arXiv:2108.10986v1 [cs.CL])","link":"http://arxiv.org/abs/2108.10986","description":"<p>Discovering the logical sequence of events is one of the cornerstones in\nNatural Language Understanding. One approach to learn the sequence of events is\nto study the order of sentences in a coherent text. Sentence ordering can be\napplied in various tasks such as retrieval-based Question Answering, document\nsummarization, storytelling, text generation, and dialogue systems.\nFurthermore, we can learn to model text coherence by learning how to order a\nset of shuffled sentences. Previous research has relied on RNN, LSTM, and\nBiLSTM architecture for learning text language models. However, these networks\nhave performed poorly due to the lack of attention mechanisms. We propose an\nalgorithm for sentence ordering in a corpus of short stories. Our proposed\nmethod uses a language model based on Universal Transformers (UT) that captures\nsentences' dependencies by employing an attention mechanism. Our method\nimproves the previous state-of-the-art in terms of Perfect Match Ratio (PMR)\nscore in the ROCStories dataset, a corpus of nearly 100K short human-made\nstories. The proposed model includes three components: Sentence Encoder,\nLanguage Model, and Sentence Arrangement with Brute Force Search. The first\ncomponent generates sentence embeddings using SBERT-WK pre-trained model\nfine-tuned on the ROCStories data. Then a Universal Transformer network\ngenerates a sentence-level language model. For decoding, the network generates\na candidate sentence as the following sentence of the current sentence. We use\ncosine similarity as a scoring function to assign scores to the candidate\nembedding and the embeddings of other sentences in the shuffled set. Then a\nBrute Force Search is employed to maximize the sum of similarities between\npairs of consecutive sentences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golestani_M/0/1/0/all/0/1\">Melika Golestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1\">Seyedeh Zahra Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borhanifard_Z/0/1/0/all/0/1\">Zeinab Borhanifard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tahmasebian_F/0/1/0/all/0/1\">Farnaz Tahmasebian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Hesham Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language Processing Accurately Categorizes Indications, Findings and Pathology Reports from Multicenter Colonoscopy. (arXiv:2108.11034v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11034","description":"<p>Colonoscopy is used for colorectal cancer (CRC) screening. Extracting details\nof the colonoscopy findings from free text in electronic health records (EHRs)\ncan be used to determine patient risk for CRC and colorectal screening\nstrategies. We developed and evaluated the accuracy of a deep learning model\nframework to extract information for the clinical decision support system to\ninterpret relevant free-text reports, including indications, pathology, and\nfindings notes. The Bio-Bi-LSTM-CRF framework was developed using Bidirectional\nLong Short-term Memory (Bi-LSTM) and Conditional Random Fields (CRF) to extract\nseveral clinical features from these free-text reports including indications\nfor the colonoscopy, findings during the colonoscopy, and pathology of resected\nmaterial. We trained the Bio-Bi-LSTM-CRF and existing Bi-LSTM-CRF models on 80%\nof 4,000 manually annotated notes from 3,867 patients. These clinical notes\nwere from a group of patients over 40 years of age enrolled in four Veterans\nAffairs Medical Centers. A total of 10% of the remaining annotated notes were\nused to train hyperparameter and the remaining 10% were used to evaluate the\naccuracy of our model Bio-Bi-LSTM-CRF and compare to Bi-LSTM-CRF.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vadyala_S/0/1/0/all/0/1\">Shashank Reddy Vadyala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sherer_E/0/1/0/all/0/1\">Eric A. Sherer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How COVID-19 has Impacted American Attitudes Toward China: A Study on Twitter. (arXiv:2108.11040v1 [cs.SI])","link":"http://arxiv.org/abs/2108.11040","description":"<p>Past research has studied social determinants of attitudes toward foreign\ncountries. Confounded by potential endogeneity biases due to unobserved factors\nor reverse causality, the causal impact of these factors on public opinion is\nusually difficult to establish. Using social media data, we leverage the\nsuddenness of the COVID-19 pandemic to examine whether a major global event has\ncausally changed American views of another country. We collate a database of\nmore than 297 million posts on the social media platform Twitter about China or\nCOVID-19 up to June 2020, and we treat tweeting about COVID-19 as a proxy for\nindividual awareness of COVID-19. Using regression discontinuity and\ndifference-in-difference estimation, we find that awareness of COVID-19 causes\na sharp rise in anti-China attitudes. Our work has implications for\nunderstanding how self-interest affects policy preference and how Americans\nview migrant communities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cook_G/0/1/0/all/0/1\">Gavin Cook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yu Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Viola: A Topic Agnostic Generate-and-Rank Dialogue System. (arXiv:2108.11063v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11063","description":"<p>We present Viola, an open-domain dialogue system for spoken conversation that\nuses a topic-agnostic dialogue manager based on a simple generate-and-rank\napproach. Leveraging recent advances of generative dialogue systems powered by\nlarge language models, Viola fetches a batch of response candidates from\nvarious neural dialogue models trained with different datasets and\nknowledge-grounding inputs. Additional responses originating from\ntemplate-based generators are also considered, depending on the user's input\nand detected entities. The hand-crafted generators build on a dynamic knowledge\ngraph injected with rich content that is crawled from the web and automatically\nprocessed on a daily basis. Viola's response ranker is a fine-tuned polyencoder\nthat chooses the best response given the dialogue history. While dedicated\nannotations for the polyencoder alone can indirectly steer it away from\nchoosing problematic responses, we add rule-based safety nets to detect neural\ndegeneration and a dedicated classifier to filter out offensive content. We\nanalyze conversations that Viola took part in for the Alexa Prize Socialbot\nGrand Challenge 4 and discuss the strengths and weaknesses of our approach.\nLastly, we suggest future work with a focus on curating conversation data\nspecifcially for socialbots that will contribute towards a more robust\ndata-driven socialbot.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1\">Hyundong Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shbita_B/0/1/0/all/0/1\">Basel Shbita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shenoy_K/0/1/0/all/0/1\">Kartik Shenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1\">Nikhil Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pindikanti_H/0/1/0/all/0/1\">Hitesh Pindikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jennifer Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YANMTT: Yet Another Neural Machine Translation Toolkit. (arXiv:2108.11126v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11126","description":"<p>In this paper we present our open-source neural machine translation (NMT)\ntoolkit called \"Yet Another Neural Machine Translation Toolkit\" abbreviated as\nYANMTT which is built on top of the Transformers library. Despite the growing\nimportance of sequence to sequence pre-training there surprisingly few, if not\nnone, well established toolkits that allow users to easily do pre-training.\nToolkits such as Fairseq which do allow pre-training, have very large codebases\nand thus they are not beginner friendly. With regards to transfer learning via\nfine-tuning most toolkits do not explicitly allow the user to have control over\nwhat parts of the pre-trained models can be transferred. YANMTT aims to address\nthese issues via the minimum amount of code to pre-train large scale NMT\nmodels, selectively transfer pre-trained parameters and fine-tune them, perform\ntranslation as well as extract representations and attentions for visualization\nand analyses. Apart from these core features our toolkit also provides other\nadvanced functionalities such as but not limited to document/multi-source NMT,\nsimultaneous NMT and model compression via distillation which we believe are\nrelevant to the purpose behind our toolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dabre_R/0/1/0/all/0/1\">Raj Dabre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Models In a Spelling Bee: Language Models Implicitly Learn the Character Composition of Tokens. (arXiv:2108.11193v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11193","description":"<p>Standard pretrained language models operate on sequences of subword tokens\nwithout direct access to the characters that compose each token's string\nrepresentation. We probe the embedding layer of pretrained language models and\nshow that models learn the internal character composition of whole word and\nsubword tokens to a surprising extent, without ever seeing the characters\ncoupled with the tokens. Our results show that the embedding layer of RoBERTa\nholds enough information to accurately spell up to a third of the vocabulary\nand reach high average character ngram overlap on all token types. We further\ntest whether enriching subword models with additional character information can\nimprove language modeling, and observe that this method has a near-identical\nlearning curve as training without spelling-based enrichment. Overall, our\nresults suggest that language modeling objectives incentivize the model to\nimplicitly learn some notion of spelling, and that explicitly teaching the\nmodel how to spell does not enhance its performance on such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Itzhak_I/0/1/0/all/0/1\">Itay Itzhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1\">Omer Levy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Promises of Transformer-Based LMs for the Representation of Normative Claims in the Legal Domain. (arXiv:2108.11215v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11215","description":"<p>In this article, we explore the potential of transformer-based language\nmodels (LMs) to correctly represent normative statements in the legal domain,\ntaking tax law as our use case. In our experiment, we use a variety of LMs as\nbases for both word- and sentence-based clusterers that are then evaluated on a\nsmall, expert-compiled test-set, consisting of real-world samples from tax law\nresearch literature that can be clearly assigned to one of four normative\ntheories. The results of the experiment show that clusterers based on\nsentence-BERT-embeddings deliver the most promising results. Based on this main\nexperiment, we make first attempts at using the best performing models in a\nbootstrapping loop to build classifiers that map normative claims on one of\nthese four normative theories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gubelmann_R/0/1/0/all/0/1\">Reto Gubelmann</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hongler_P/0/1/0/all/0/1\">Peter Hongler</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Handschuh_S/0/1/0/all/0/1\">Siegfried Handschuh</a> (1) ((1) University of St.Gallen (HSG))"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ontology-Enhanced Slot Filling. (arXiv:2108.11275v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11275","description":"<p>Slot filling is a fundamental task in dialog state tracking in task-oriented\ndialog systems. In multi-domain task-oriented dialog system, user utterances\nand system responses may mention multiple named entities and attributes values.\nA system needs to select those that are confirmed by the user and fill them\ninto destined slots. One difficulty is that since a dialogue session contains\nmultiple system-user turns, feeding in all the tokens into a deep model such as\nBERT can be challenging due to limited capacity of input word tokens and GPU\nmemory. In this paper, we investigate an ontology-enhanced approach by matching\nthe named entities occurred in all dialogue turns using ontology. The matched\nentities in the previous dialogue turns will be accumulated and encoded as\nadditional inputs to a BERT-based dialogue state tracker. In addition, our\nimprovement includes ontology constraint checking and the correction of slot\nname tokenization. Experimental results showed that our ontology-enhanced\ndialogue state tracker improves the joint goal accuracy (slot F1) from 52.63%\n(91.64%) to 53.91% (92%) on MultiWOZ 2.1 corpus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuhao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tam_Y/0/1/0/all/0/1\">Yik-Cheung Tam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProoFVer: Natural Logic Theorem Proving for Fact Verification. (arXiv:2108.11357v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11357","description":"<p>We propose ProoFVer, a proof system for fact verification using natural\nlogic. The textual entailment model in ProoFVer is a seq2seq model generating\nvalid natural-logic based logical inferences as its proofs. The generation of\nproofs makes ProoFVer an explainable system. The proof consists of iterative\nlexical mutations of spans in the claim with spans in a set of retrieved\nevidence sentences. Further, each such mutation is marked with an entailment\nrelation using natural logic operators. The veracity of a claim is determined\nsolely based on the sequence of natural logic relations present in the proof.\nBy design, this makes ProoFVer a faithful by construction system that generates\nfaithful explanations. ProoFVer outperforms existing fact-verification models,\nwith more than two percent absolute improvements in performance and robustness.\nIn addition to its explanations being faithful, ProoFVer also scores high on\nrationale extraction, with a five point absolute improvement compared to\nattention-based rationales in existing models. Finally, we find that humans\ncorrectly simulate ProoFVer's decisions more often using the proofs, than the\ndecisions of an existing model that directly use the retrieved evidence for\ndecision making.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_A/0/1/0/all/0/1\">Amrith Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1\">Sebastian Riedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Entity Linking: A Survey of Models Based on Deep Learning. (arXiv:2006.00575v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.00575","description":"<p>In this survey, we provide a comprehensive description of recent neural\nentity linking (EL) systems developed since 2015 as a result of the \"deep\nlearning revolution\" in NLP. Our goal is to systemize design features of neural\nentity linking systems and compare their performance to the prominent classic\nmethods on common benchmarks. We distill generic architectural components of a\nneural EL system, like candidate generation and entity ranking, and summarize\nprominent methods for each of them. The vast variety of modifications of this\ngeneral neural entity linking architecture are grouped by several common\nthemes: joint entity recognition and linking, models for global linking,\ndomain-independent techniques including zero-shot and distant supervision\nmethods, and cross-lingual approaches. Since many neural models take advantage\nof entity and mention/context embeddings to catch semantic meaning of them, we\nprovide an overview of popular embedding techniques. Finally, we briefly\ndiscuss applications of entity linking, focusing on the recently emerged\nuse-case of enhancing deep pre-trained masked language models based on the\ntransformer architecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sevgili_O/0/1/0/all/0/1\">Ozge Sevgili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shelmanov_A/0/1/0/all/0/1\">Artem Shelmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arkhipov_M/0/1/0/all/0/1\">Mikhail Arkhipov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1\">Alexander Panchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biemann_C/0/1/0/all/0/1\">Chris Biemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.03060","description":"<p>A key challenge in training neural networks for a given medical imaging task\nis often the difficulty of obtaining a sufficient number of manually labeled\nexamples. In contrast, textual imaging reports, which are often readily\navailable in medical records, contain rich but unstructured interpretations\nwritten by experts as part of standard clinical practice. We propose using\nthese textual reports as a form of weak supervision to improve the image\ninterpretation performance of a neural network without requiring additional\nmanually labeled examples. We use an image-text matching task to train a\nfeature extractor and then fine-tune it in a transfer learning setting for a\nsupervised task using a small labeled dataset. The end result is a neural\nnetwork that automatically interprets imagery without requiring textual reports\nduring inference. This approach can be applied to any task for which text-image\npairs are readily available. We evaluate our method on three classification\ntasks and find consistent performance improvements, reducing the need for\nlabeled data by 67%-98%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_G/0/1/0/all/0/1\">Gongbo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenwell_C/0/1/0/all/0/1\">Connor Greenwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kavuluru_R/0/1/0/all/0/1\">Ramakanth Kavuluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_N/0/1/0/all/0/1\">Nathan Jacobs</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Telling the What while Pointing to the Where: Multimodal Queries for Image Retrieval. (arXiv:2102.04980v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2102.04980","description":"<p>Most existing image retrieval systems use text queries as a way for the user\nto express what they are looking for. However, fine-grained image retrieval\noften requires the ability to also express where in the image the content they\nare looking for is. The text modality can only cumbersomely express such\nlocalization preferences, whereas pointing is a more natural fit. In this\npaper, we propose an image retrieval setup with a new form of multimodal\nqueries, where the user simultaneously uses both spoken natural language (the\nwhat) and mouse traces over an empty canvas (the where) to express the\ncharacteristics of the desired target image. We then describe simple\nmodifications to an existing image retrieval model, enabling it to operate in\nthis setup. Qualitative and quantitative experiments show that our model\neffectively takes this spatial guidance into account, and provides\nsignificantly more accurate retrieval results compared to text-only equivalent\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1\">Soravit Changpinyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pont_Tuset_J/0/1/0/all/0/1\">Jordi Pont-Tuset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1\">Vittorio Ferrari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Road to Know-Where: An Object-and-Room Informed Sequential BERT for Indoor Vision-Language Navigation. (arXiv:2104.04167v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04167","description":"<p>Vision-and-Language Navigation (VLN) requires an agent to find a path to a\nremote location on the basis of natural-language instructions and a set of\nphoto-realistic panoramas. Most existing methods take the words in the\ninstructions and the discrete views of each panorama as the minimal unit of\nencoding. However, this requires a model to match different nouns (e.g., TV,\ntable) against the same input view feature. In this work, we propose an\nobject-informed sequential BERT to encode visual perceptions and linguistic\ninstructions at the same fine-grained level, namely objects and words. Our\nsequential BERT also enables the visual-textual clues to be interpreted in\nlight of the temporal context, which is crucial to multi-round VLN tasks.\nAdditionally, we enable the model to identify the relative direction (e.g.,\nleft/right/front/back) of each navigable location and the room type (e.g.,\nbedroom, kitchen) of its current and final navigation goal, as such information\nis widely mentioned in instructions implying the desired next and final\nlocations. We thus enable the model to know-where the objects lie in the\nimages, and to know-where they stand in the scene. Extensive experiments\ndemonstrate the effectiveness compared against several state-of-the-art methods\non three indoor VLN tasks: REVERIE, NDH, and R2R. Project repository:\nhttps://github.com/YuankaiQi/ORIST\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yuankai Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zizheng Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1\">Yicong Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming-Hsuan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1\">Anton van den Hengel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qi Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07650","description":"<p>Recently, prompt-tuning has achieved promising results on some few-class\nclassification tasks. The core idea of prompt-tuning is to insert text pieces,\ni.e., template, to the input and transform a classification task into a masked\nlanguage modeling problem. However, as for relation extraction, determining the\nappropriate prompt template requires domain expertise, and single label word\nhandcrafted or auto-searched is cumbersome and time-consuming to verify their\neffectiveness in non-few-shot scenarios, which also fails to leverage the\nabundant semantic knowledge in the entities and relation labels. To this end,\nwe focus on incorporating knowledge into prompt-tuning for relation extraction\nand propose a knowledge-aware prompt-tuning with synergistic optimization\n(KNIGHT) approach. Specifically, we inject entity and relation knowledge into\nprompt construction with learnable virtual template words and answer words and\njointly optimize their representation with knowledge constraints. Extensive\nexperimental results on 5 datasets with standard and low-resource settings\ndemonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yunzhi Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Annotated Commodity News Corpus for Event Extraction. (arXiv:2105.08214v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.08214","description":"<p>Commodity News contains a wealth of information such as sum-mary of the\nrecent commodity price movement and notable events that led tothe movement.\nThrough event extraction, useful information extracted fromcommodity news is\nextremely useful in mining for causal relation betweenevents and commodity\nprice movement, which can be used for commodity priceprediction. To facilitate\nthe future research, we introduce a new dataset withthe following information\nidentified and annotated: (i) entities (both nomi-nal and named), (ii) events\n(trigger words and argument roles), (iii) eventmetadata: modality, polarity and\nintensity and (iv) event-event relations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Meisin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soon_L/0/1/0/all/0/1\">Lay-Ki Soon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siew_E/0/1/0/all/0/1\">Eu-Gene Siew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugianto_L/0/1/0/all/0/1\">Ly Fie Sugianto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Laughing Heads: Can Transformers Detect What Makes a Sentence Funny?. (arXiv:2105.09142v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.09142","description":"<p>The automatic detection of humor poses a grand challenge for natural language\nprocessing. Transformer-based systems have recently achieved remarkable results\non this task, but they usually (1)~were evaluated in setups where serious vs\nhumorous texts came from entirely different sources, and (2)~focused on\nbenchmarking performance without providing insights into how the models work.\nWe make progress in both respects by training and analyzing transformer-based\nhumor recognition models on a recently introduced dataset consisting of minimal\npairs of aligned sentences, one serious, the other humorous. We find that,\nalthough our aligned dataset is much harder than previous datasets,\ntransformer-based models recognize the humorous sentence in an aligned pair\nwith high accuracy (78%). In a careful error analysis, we characterize easy vs\nhard instances. Finally, by analyzing attention weights, we obtain important\ninsights into the mechanisms by which transformers recognize humor. Most\nremarkably, we find clear evidence that one single attention head learns to\nrecognize the words that make a test sentence humorous, even without access to\nthis information at training time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1\">Maxime Peyrard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borges_B/0/1/0/all/0/1\">Beatriz Borges</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1\">Kristina Gligori&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1\">Robert West</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Self-supervised Method for Entity Alignment. (arXiv:2106.09395v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.09395","description":"<p>Entity alignment, aiming to identify equivalent entities across different\nknowledge graphs (KGs), is a fundamental problem for constructing large-scale\nKGs. Over the course of its development, supervision has been considered\nnecessary for accurate alignments. Inspired by the recent progress of\nself-supervised learning, we explore the extent to which we can get rid of\nsupervision for entity alignment. Existing supervised methods for this task\nfocus on pulling each pair of positive (labeled) entities close to each other.\nHowever, our analysis suggests that the learning of entity alignment can\nactually benefit more from pushing sampled (unlabeled) negatives far away than\npulling positive aligned pairs close. We present SelfKG by leveraging this\ndiscovery to design a contrastive learning strategy across two KGs. Extensive\nexperiments on benchmark datasets demonstrate that SelfKG without supervision\ncan match or achieve comparable results with state-of-the-art supervised\nbaselines. The performance of SelfKG demonstrates self-supervised learning\noffers great potential for entity alignment in KGs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_H/0/1/0/all/0/1\">Haoyun Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinghao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zeyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharlamov_E/0/1/0/all/0/1\">Evgeny Kharlamov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jie Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion. (arXiv:2108.01387v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.01387","description":"<p>We present InferWiki, a Knowledge Graph Completion (KGC) dataset that\nimproves upon existing benchmarks in inferential ability, assumptions, and\npatterns. First, each testing sample is predictable with supportive data in the\ntraining set. To ensure it, we propose to utilize rule-guided train/test\ngeneration, instead of conventional random split. Second, InferWiki initiates\nthe evaluation following the open-world assumption and improves the inferential\ndifficulty of the closed-world assumption, by providing manually annotated\nnegative and unknown triples. Third, we include various inference patterns\n(e.g., reasoning path length and types) for comprehensive evaluation. In\nexperiments, we curate two settings of InferWiki varying in sizes and\nstructures, and apply the construction process on CoDEx as comparative\ndatasets. The results and empirical analyses demonstrate the necessity and\nhigh-quality of InferWiki. Nevertheless, the performance gap among various\ninferential assumptions and patterns presents the difficulty and inspires\nfuture research direction. Our datasets can be found in\nhttps://github.com/TaoMiner/inferwiki\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1\">Xin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yonggang Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanwang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09193","description":"<p>Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-25T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}