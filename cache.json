{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-12-28T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Measuring Attribution in Natural Language Generation Models. (arXiv:2112.12870v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12870","description":"<p>With recent improvements in natural language generation (NLG) models for\nvarious applications, it has become imperative to have the means to identify\nand evaluate whether NLG output is only sharing verifiable information about\nthe external world. In this work, we present a new evaluation framework\nentitled Attributable to Identified Sources (AIS) for assessing the output of\nnatural language generation models, when such output pertains to the external\nworld. We first define AIS and introduce a two-stage annotation pipeline for\nallowing annotators to appropriately evaluate model output according to AIS\nguidelines. We empirically validate this approach on three generation datasets\n(two in the conversational QA domain and one in summarization) via human\nevaluation studies that suggest that AIS could serve as a common framework for\nmeasuring whether model-generated statements are supported by underlying\nsources. We release guidelines for the human evaluation studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rashkin_H/0/1/0/all/0/1\">Hannah Rashkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaev_V/0/1/0/all/0/1\">Vitaly Nikolaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamm_M/0/1/0/all/0/1\">Matthew Lamm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1\">Michael Collins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrov_S/0/1/0/all/0/1\">Slav Petrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomar_G/0/1/0/all/0/1\">Gaurav Singh Tomar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reitter_D/0/1/0/all/0/1\">David Reitter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spoiler in a Textstack: How Much Can Transformers Help?. (arXiv:2112.12913v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12913","description":"<p>This paper presents our research regarding spoiler detection in reviews. In\nthis use case, we describe the method of fine-tuning and organizing the\navailable text-based model tasks with the latest deep learning achievements and\ntechniques to interpret the models' results.\n</p>\n<p>Until now, spoiler research has been rarely described in the literature. We\ntested the transfer learning approach and different latest transformer\narchitectures on two open datasets with annotated spoilers (ROC AUC above 81\\%\non TV Tropes Movies dataset, and Goodreads dataset above 88\\%). We also\ncollected data and assembled a new dataset with fine-grained annotations. To\nthat end, we employed interpretability techniques and measures to assess the\nmodels' reliability and explain their results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Anna Wr&#xf3;blewska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rzepinski_P/0/1/0/all/0/1\">Pawe&#x142; Rzepi&#x144;ski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sysko_Romanczuk_S/0/1/0/all/0/1\">Sylwia Sysko-Roma&#x144;czuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counterfactual Memorization in Neural Language Models. (arXiv:2112.12938v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12938","description":"<p>Modern neural language models widely used in tasks across NLP risk memorizing\nsensitive information from their training data. As models continue to scale up\nin parameters, training data, and compute, understanding memorization in\nlanguage models is both important from a learning-theoretical point of view,\nand is practically crucial in real world applications. An open question in\nprevious studies of memorization in language models is how to filter out\n\"common\" memorization. In fact, most memorization criteria strongly correlate\nwith the number of occurrences in the training set, capturing \"common\"\nmemorization such as familiar phrases, public knowledge or templated texts. In\nthis paper, we provide a principled perspective inspired by a taxonomy of human\nmemory in Psychology. From this perspective, we formulate a notion of\ncounterfactual memorization, which characterizes how a model's predictions\nchange if a particular document is omitted during training. We identify and\nstudy counterfactually-memorized training examples in standard text datasets.\nWe further estimate the influence of each training example on the validation\nset and on generated texts, and show that this can provide direct evidence of\nthe source of memorization at test time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ippolito_D/0/1/0/all/0/1\">Daphne Ippolito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Katherine Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jagielski_M/0/1/0/all/0/1\">Matthew Jagielski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tramer_F/0/1/0/all/0/1\">Florian Tram&#xe8;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Scientific Publications using Domain-Specific Word Embedding and Topic Modelling. (arXiv:2112.12940v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12940","description":"<p>The scientific world is changing at a rapid pace, with new technology being\ndeveloped and new trends being set at an increasing frequency. This paper\npresents a framework for conducting scientific analyses of academic\npublications, which is crucial to monitor research trends and identify\npotential innovations. This framework adopts and combines various techniques of\nNatural Language Processing, such as word embedding and topic modelling. Word\nembedding is used to capture semantic meanings of domain-specific words. We\npropose two novel scientific publication embedding, i.e., PUB-G and PUB-W,\nwhich are capable of learning semantic meanings of general as well as\ndomain-specific words in various research fields. Thereafter, topic modelling\nis used to identify clusters of research topics within these larger research\nfields. We curated a publication dataset consisting of two conferences and two\njournals from 1995 to 2020 from two research domains. Experimental results show\nthat our PUB-G and PUB-W embeddings are superior in comparison to other\nbaseline embeddings by a margin of ~0.18-1.03 based on topic coherence.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Singhal_T/0/1/0/all/0/1\">Trisha Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blessing_L/0/1/0/all/0/1\">Lucienne T.M. Blessing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_K/0/1/0/all/0/1\">Kwan Hui Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distinguishing Transformative from Incremental Clinical Evidence: A Classifier of Clinical Research using Textual features from Abstracts and Citing Sentences. (arXiv:2112.12996v1 [cs.CL])","link":"http://arxiv.org/abs/2112.12996","description":"<p>In clinical research and clinical decision-making, it is important to know if\na study changes or only supports the current standards of care for specific\ndisease management. We define such a change as transformative and a support as\nincremental research. It usually requires a huge amount of domain expertise and\ntime for humans to finish such tasks. Faculty Opinions provides us with a\nwell-annotated corpus on whether a research challenges or only confirms\nestablished research. In this study, a machine learning approach is proposed to\ndistinguishing transformative from incremental clinical evidence. The texts\nfrom both abstract and a 2-year window of citing sentences are collected for a\ntraining set of clinical studies recommended and labeled by Faculty Opinions\nexperts. We achieve the best performance with an average AUC of 0.755\n(0.705-0.875) using Random Forest as the classifier and citing sentences as the\nfeature. The results showed that transformative research has typical language\npatterns in citing sentences unlike abstract sentences. We provide an efficient\ntool for identifying those clinical evidence challenging or only confirming\nestablished claims for clinicians and researchers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xuanyu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1\">Jian Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Combining Improvements for Exploiting Dependency Trees in Neural Semantic Parsing. (arXiv:2112.13179v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13179","description":"<p>The dependency tree of a natural language sentence can capture the\ninteractions between semantics and words. However, it is unclear whether those\nmethods which exploit such dependency information for semantic parsing can be\ncombined to achieve further improvement and the relationship of those methods\nwhen they combine. In this paper, we examine three methods to incorporate such\ndependency information in a Transformer based semantic parser and empirically\nstudy their combinations. We first replace standard self-attention heads in the\nencoder with parent-scaled self-attention (PASCAL) heads, i.e., the ones that\ncan attend to the dependency parent of each token. Then we concatenate\nsyntax-aware word representations (SAWRs), i.e., the intermediate hidden\nrepresentations of a neural dependency parser, with ordinary word embedding to\nenhance the encoder. Later, we insert the constituent attention (CA) module to\nthe encoder, which adds an extra constraint to attention heads that can better\ncapture the inherent dependency structure of input sentences. Transductive\nensemble learning (TEL) is used for model aggregation, and an ablation study is\nconducted to show the contribution of each method. Our experiments show that CA\nis complementary to PASCAL or SAWRs, and PASCAL + CA provides state-of-the-art\nperformance among neural approaches on ATIS, GEO, and JOBS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_D/0/1/0/all/0/1\">Defeng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1\">Jianmin Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jiafei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Ran Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CABACE: Injecting Character Sequence Information and Domain Knowledge for Enhanced Acronym and Long-Form Extraction. (arXiv:2112.13237v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13237","description":"<p>Acronyms and long-forms are commonly found in research documents, more so in\ndocuments from scientific and legal domains. Many acronyms used in such\ndocuments are domain-specific and are very rarely found in normal text corpora.\nOwing to this, transformer-based NLP models often detect OOV (Out of\nVocabulary) for acronym tokens, especially for non-English languages, and their\nperformance suffers while linking acronyms to their long forms during\nextraction. Moreover, pretrained transformer models like BERT are not\nspecialized to handle scientific and legal documents. With these points being\nthe overarching motivation behind this work, we propose a novel framework\nCABACE: Character-Aware BERT for ACronym Extraction, which takes into account\ncharacter sequences in text and is adapted to scientific and legal domains by\nmasked language modelling. We further use an objective with an augmented loss\nfunction, adding the max loss and mask loss terms to the standard cross-entropy\nloss for training CABACE. We further leverage pseudo labelling and adversarial\ndata generation to improve the generalizability of the framework. Experimental\nresults prove the superiority of the proposed framework in comparison to\nvarious baselines. Additionally, we show that the proposed framework is better\nsuited than baseline models for zero-shot generalization to non-English\nlanguages, thus reinforcing the effectiveness of our approach. Our team\nBacKGProp secured the highest scores on the French dataset, second-highest on\nDanish and Vietnamese, and third-highest in the English-Legal dataset on the\nglobal leaderboard for the acronym extraction (AE) shared task at SDU AAAI-22.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kannen_N/0/1/0/all/0/1\">Nithish Kannen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_D/0/1/0/all/0/1\">Divyanshu Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Abhranil Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1\">Shubhraneel Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PerCQA: Persian Community Question Answering Dataset. (arXiv:2112.13238v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13238","description":"<p>Community Question Answering (CQA) forums provide answers for many real-life\nquestions. Thanks to the large size, these forums are very popular among\nmachine learning researchers. Automatic answer selection, answer ranking,\nquestion retrieval, expert finding, and fact-checking are example learning\ntasks performed using CQA data. In this paper, we present PerCQA, the first\nPersian dataset for CQA. This dataset contains the questions and answers\ncrawled from the most well-known Persian forum. After data acquisition, we\nprovide rigorous annotation guidelines in an iterative process, and then the\nannotation of question-answer pairs in SemEvalCQA format. PerCQA contains 989\nquestions and 21,915 annotated answers. We make PerCQA publicly available to\nencourage more research in Persian CQA. We also build strong benchmarks for the\ntask of answer selection in PerCQA by using mono- and multi-lingual pre-trained\nlanguage models\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jamali_N/0/1/0/all/0/1\">Naghme Jamali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1\">Yadollah Yaghoobzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Hesham Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Preliminary Study for Literary Rhyme Generation based on Neuronal Representation, Semantics and Shallow Parsing. (arXiv:2112.13241v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13241","description":"<p>In recent years, researchers in the area of Computational Creativity have\nstudied the human creative process proposing different approaches to reproduce\nit with a formal procedure. In this paper, we introduce a model for the\ngeneration of literary rhymes in Spanish, combining structures of language and\nneural network models %(\\textit{Word2vec}).%, into a structure for semantic\nassimilation. The results obtained with a manual evaluation of the texts\ngenerated by our algorithm are encouraging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moreno_Jimenez_L/0/1/0/all/0/1\">Luis-Gil Moreno-Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torres_Moreno_J/0/1/0/all/0/1\">Juan-Manuel Torres-Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wedemann_R/0/1/0/all/0/1\">Roseli S. Wedemann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deeper Clinical Document Understanding Using Relation Extraction. (arXiv:2112.13259v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13259","description":"<p>The surging amount of biomedical literature &amp; digital clinical records\npresents a growing need for text mining techniques that can not only identify\nbut also semantically relate entities in unstructured data. In this paper we\npropose a text mining framework comprising of Named Entity Recognition (NER)\nand Relation Extraction (RE) models, which expands on previous work in three\nmain ways. First, we introduce two new RE model architectures -- an\naccuracy-optimized one based on BioBERT and a speed-optimized one utilizing\ncrafted features over a Fully Connected Neural Network (FCNN). Second, we\nevaluate both models on public benchmark datasets and obtain new\nstate-of-the-art F1 scores on the 2012 i2b2 Clinical Temporal Relations\nchallenge (F1 of 73.6, +1.2% over the previous SOTA), the 2010 i2b2 Clinical\nRelations challenge (F1 of 69.1, +1.2%), the 2019 Phenotype-Gene Relations\ndataset (F1 of 87.9, +8.5%), the 2012 Adverse Drug Events Drug-Reaction dataset\n(F1 of 90.0, +6.3%), and the 2018 n2c2 Posology Relations dataset (F1 of 96.7,\n+0.6%). Third, we show two practical applications of this framework -- for\nbuilding a biomedical knowledge graph and for improving the accuracy of mapping\nentities to clinical codes. The system is built using the Spark NLP library\nwhich provides a production-grade, natively scalable, hardware-optimized,\ntrainable &amp; tunable NLP framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haq_H/0/1/0/all/0/1\">Hasham Ul Haq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocaman_V/0/1/0/all/0/1\">Veysel Kocaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talby_D/0/1/0/all/0/1\">David Talby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stance Quantification: Definition of the Problem. (arXiv:2112.13288v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13288","description":"<p>Stance detection is commonly defined as the automatic process of determining\nthe positions of text producers, towards a target. In this paper, we define a\nresearch problem closely related to stance detection, namely, stance\nquantification, for the first time. We define stance quantification on a pair\nincluding (1) a set of natural language text items and (2) a target. At the end\nof the stance quantification process, a triple is obtained which consists of\nthe percentages of the number of text items classified as Favor, Against,\nNeither, respectively, towards the target in the input pair. Also defined in\nthe current paper is a significant subproblem of the stance quantification\nproblem, namely, multi-target stance quantification. We believe that stance\nquantification at the aggregate level can lead to fruitful results in many\napplication settings, and furthermore, stance quantification might be the sole\nstance related analysis alternative in settings where privacy concerns prevent\nresearchers from applying generic stance detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kucuk_D/0/1/0/all/0/1\">Dilek K&#xfc;&#xe7;&#xfc;k</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Algorithm for the Shortest Superstring Problem. (arXiv:2112.13319v1 [quant-ph])","link":"http://arxiv.org/abs/2112.13319","description":"<p>In this paper, we consider the ``Shortest Superstring Problem''(SSP) or the\n``Shortest Common Superstring Problem''(SCS). The problem is as follows. For a\npositive integer $n$, a sequence of n strings $S=(s^1,\\dots,s^n)$ is given. We\nshould construct the shortest string $t$ (we call it superstring) that contains\neach string from the given sequence as a substring. The problem is connected\nwith the sequence assembly method for reconstructing a long DNA sequence from\nsmall fragments. We present a quantum algorithm with running time\n$O^*(1.728^n)$. Here $O^*$ notation does not consider polynomials of $n$ and\nthe length of $t$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/quant-ph/1/au:+Khadiev_K/0/1/0/all/0/1\">Kamil Khadiev</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Machado_C/0/1/0/all/0/1\">Carlos Manuel Bosch Machado</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Budget Sensitive Reannotation of Noisy Relation Classification Data Using Label Hierarchy. (arXiv:2112.13320v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13320","description":"<p>Large crowd-sourced datasets are often noisy and relation classification (RC)\ndatasets are no exception. Reannotating the entire dataset is one probable\nsolution however it is not always viable due to time and budget constraints.\nThis paper addresses the problem of efficient reannotation of a large noisy\ndataset for the RC. Our goal is to catch more annotation errors in the dataset\nwhile reannotating fewer instances. Existing work on RC dataset reannotation\nlacks the flexibility about how much data to reannotate. We introduce the\nconcept of a reannotation budget to overcome this limitation. The immediate\nfollow-up problem is: Given a specific reannotation budget, which subset of the\ndata should we reannotate? To address this problem, we present two strategies\nto selectively reannotate RC datasets. Our strategies utilize the taxonomic\nhierarchy of relation labels. The intuition of our work is to rely on the graph\ndistance between actual and predicted relation labels in the label hierarchy\ngraph. We evaluate our reannotation strategies on the well-known TACRED\ndataset. We design our experiments to answer three specific research questions.\nFirst, does our strategy select novel candidates for reannotation? Second, for\na given reannotation budget is our reannotation strategy more efficient at\ncatching annotation errors? Third, what is the impact of data reannotation on\nRC model performance measurement? Experimental results show that our both\nreannotation strategies are novel and efficient. Our analysis indicates that\nthe current reported performance of RC models on noisy TACRED data is inflated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parekh_A/0/1/0/all/0/1\">Akshay Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Ashish Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awekar_A/0/1/0/all/0/1\">Amit Awekar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Interdisciplinary Approach for the Automated Detection and Visualization of Media Bias in News Articles. (arXiv:2112.13352v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13352","description":"<p>Media coverage has a substantial effect on the public perception of events.\nNevertheless, media outlets are often biased. One way to bias news articles is\nby altering the word choice. The automatic identification of bias by word\nchoice is challenging, primarily due to the lack of gold-standard data sets and\nhigh context dependencies. In this research project, I aim to devise data sets\nand methods to identify media bias. To achieve this, I plan to research methods\nusing natural language processing and deep learning while employing models and\nusing analysis concepts from psychology and linguistics. The first results\nindicate the effectiveness of an interdisciplinary research approach. My vision\nis to devise a system that helps news readers become aware of media coverage\ndifferences caused by bias. So far, my best performing BERT-based model is\npre-trained on a larger corpus consisting of distant labels, indicating that\ndistant supervision has the potential to become a solution for the difficult\ntask of bias detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Spinde_T/0/1/0/all/0/1\">Timo Spinde</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delivery Issues Identification from Customer Feedback Data. (arXiv:2112.13372v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13372","description":"<p>Millions of packages are delivered successfully by online and local retail\nstores across the world every day. The proper delivery of packages is needed to\nensure high customer satisfaction and repeat purchases. These deliveries suffer\nvarious problems despite the best efforts from the stores. These issues happen\nnot only due to the large volume and high demand for low turnaround time but\nalso due to mechanical operations and natural factors. These issues range from\nreceiving wrong items in the package to delayed shipment to damaged packages\nbecause of mishandling during transportation. Finding solutions to various\ndelivery issues faced by both sending and receiving parties plays a vital role\nin increasing the efficiency of the entire process. This paper shows how to\nfind these issues using customer feedback from the text comments and uploaded\nimages. We used transfer learning for both Text and Image models to minimize\nthe demand for thousands of labeled examples. The results show that the model\ncan find different issues. Furthermore, it can also be used for tasks like\nbottleneck identification, process improvement, automating refunds, etc.\nCompared with the existing process, the ensemble of text and image models\nproposed in this paper ensures the identification of several types of delivery\nissues, which is more suitable for the real-life scenarios of delivery of items\nin retail businesses. This method can supply a new idea of issue detection for\nthe delivery of packages in similar industries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_M/0/1/0/all/0/1\">Mahima Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Shubham Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArT: All-round Thinker for Unsupervised Commonsense Question-Answering. (arXiv:2112.13428v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13428","description":"<p>Without labeled question-answer pairs for necessary training, unsupervised\ncommonsense question-answering (QA) appears to be extremely challenging due to\nits indispensable unique prerequisite on commonsense source like knowledge\nbases (KBs), which are usually highly resource consuming in construction.\nRecently pre-trained language models (PrLMs) show effectiveness as an\nalternative for commonsense clues when they play a role of knowledge generator.\nHowever, existing work simply generates hundreds of pseudo-answers, or roughly\nperforms knowledge generation according to templates once for all, which may\nresult in much noise and thus hinders the quality of generated knowledge.\nMotivated by human thinking experience, we propose an approach of All-round\nThinker (ArT) by fully taking association during knowledge generating. In\ndetail, our model first focuses on key parts in the given context, and then\ngenerates highly related knowledge on such a basis in an association way like\nhuman thinking. Besides, for casual reasoning, a reverse thinking mechanism is\nproposed to conduct bidirectional inferring between cause and effect. ArT is\ntotally unsupervised and KBs-free. We evaluate it on three commonsense QA\nbenchmarks: COPA, SocialIQA and SCT. On all scales of PrLM backbones, ArT shows\nits brilliant performance and outperforms previous advanced unsupervised\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiawei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Methods & Metrics for LFQA tasks. (arXiv:2112.13432v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13432","description":"<p>Long-form question answering (LFQA) tasks require retrieving the documents\npertinent to a query, using them to form a paragraph-length answer. Despite\nconsiderable progress in LFQA modeling, fundamental issues impede its progress:\ni) train/validation/test dataset overlap, ii) absence of automatic metrics and\niii) generated answers not being \"grounded\" in retrieved documents. This work\naddresses every one these critical bottlenecks, contributing natural language\ninference/generation (NLI/NLG) methods and metrics that make significant\nstrides to their alleviation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mahapatra_S/0/1/0/all/0/1\">Suchismit Mahapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagojevic_V/0/1/0/all/0/1\">Vladimir Blagojevic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bertorello_P/0/1/0/all/0/1\">Pablo Bertorello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Prasanna Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Event-based clinical findings extraction from radiology reports with pre-trained language model. (arXiv:2112.13512v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13512","description":"<p>Radiology reports contain a diverse and rich set of clinical abnormalities\ndocumented by radiologists during their interpretation of the images.\nComprehensive semantic representations of radiological findings would enable a\nwide range of secondary use applications to support diagnosis, triage, outcomes\nprediction, and clinical research. In this paper, we present a new corpus of\nradiology reports annotated with clinical findings. Our annotation schema\ncaptures detailed representations of pathologic findings that are observable on\nimaging (\"lesions\") and other types of clinical problems (\"medical problems\").\nThe schema used an event-based representation to capture fine-grained details,\nincluding assertion, anatomy, characteristics, size, count, etc. Our gold\nstandard corpus contained a total of 500 annotated computed tomography (CT)\nreports. We extracted triggers and argument entities using two state-of-the-art\ndeep learning architectures, including BERT. We then predicted the linkages\nbetween trigger and argument entities (referred to as argument roles) using a\nBERT-based relation extraction model. We achieved the best extraction\nperformance using a BERT model pre-trained on 3 million radiology reports from\nour institution: 90.9%-93.4% F1 for finding triggers 72.0%-85.6% F1 for\narguments roles. To assess model generalizability, we used an external\nvalidation set randomly sampled from the MIMIC Chest X-ray (MIMIC-CXR)\ndatabase. The extraction performance on this validation set was 95.6% for\nfinding triggers and 79.1%-89.7% for argument roles, demonstrating that the\nmodel generalized well to the cross-institutional data with a different imaging\nmodality. We extracted the finding events from all the radiology reports in the\nMIMIC-CXR database and provided the extractions to the research community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lau_W/0/1/0/all/0/1\">Wilson Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lybarger_K/0/1/0/all/0/1\">Kevin Lybarger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunn_M/0/1/0/all/0/1\">Martin L. Gunn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yetisgen_M/0/1/0/all/0/1\">Meliha Yetisgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Personalized Answer Generation in E-Commerce via Multi-Perspective Preference Modeling. (arXiv:2112.13556v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13556","description":"<p>Recently, Product Question Answering (PQA) on E-Commerce platforms has\nattracted increasing attention as it can act as an intelligent online shopping\nassistant and improve the customer shopping experience. Its key function,\nautomatic answer generation for product-related questions, has been studied by\naiming to generate content-preserving while question-related answers. However,\nan important characteristic of PQA, i.e., personalization, is neglected by\nexisting methods. It is insufficient to provide the same \"completely\nsummarized\" answer to all customers, since many customers are more willing to\nsee personalized answers with customized information only for themselves, by\ntaking into consideration their own preferences towards product aspects or\ninformation needs. To tackle this challenge, we propose a novel Personalized\nAnswer GEneration method (PAGE) with multi-perspective preference modeling,\nwhich explores historical user-generated contents to model user preference for\ngenerating personalized answers in PQA. Specifically, we first retrieve\nquestion-related user history as external knowledge to model knowledge-level\nuser preference. Then we leverage Gaussian Softmax distribution model to\ncapture latent aspect-level user preference. Finally, we develop a\npersona-aware pointer network to generate personalized answers in terms of both\ncontent and style by utilizing personal user preference and dynamic user\nvocabulary. Experimental results on real-world E-Commerce QA datasets\ndemonstrate that the proposed method outperforms existing methods by generating\ninformative and customized answers, and show that answer generation in\nE-Commerce can benefit from personalization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenxuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_W/0/1/0/all/0/1\">Wai Lam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chinese Learners' Phonetic Transfer of /i/ from Mandarin Chinese to General American English: Evidence from Perception and Production Experiments. (arXiv:2112.13571v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13571","description":"<p>Ever since the development of Contrastive Analysis (CA) in the 1950s, which\nfocuses on comparing and contrasting two language systems, linguists have\nstarted to systematically explore the influence of the mother tongue on\nacquiring a second language. This phenomenon is later defined as \"language\ntransfer\". The current paper concerns language transfer at the phonetic level\nand concentrates on the transfer phenomenon existing in advanced-level Chinese\nlearners' acquisition of English vowels /i/ and its lax counterpart. By\ndetermining whether advanced-level Chinese English-language learners (ELLs) can\naccurately distinguish between /i/ and its lax counterpart, and pronounce them\nin English words precisely, this paper serves as a reference for further\nstudying Chinese ELLs' language transfer. Two objectives were to be met:\nfirstly, learners' perceptual ability to distinguish between vowels /i/ and its\nlax counterpart should be examined; and secondly, the effect of the phonetic\ntransfer should be determined. A perception test and a production test were\nused to attain these two objectives. Both tests were completed by 12\nadvanced-level Chinese ELLs, six males and six females. Results indicate that\nboth male and female participants could consciously distinguish between /i/ and\nits lax counterpart. All participants have signs of experiencing negative\nphonetic transfer in their pronunciation, except that the current data do not\ndecisively reflect an impact of the phonetic transfer on female ELLs'\nacquisition of the high front lax vowel in English words.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lintao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Polite Emotional Dialogue Acts for Conversational Analysis in Dialy Dialog Data. (arXiv:2112.13572v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13572","description":"<p>Many socio-linguistic cues are used in the conversational analysis, such as\nemotion, sentiment, and dialogue acts. One of the fundamental social cues is\npoliteness, which linguistically possesses properties useful in conversational\nanalysis. This short article presents some of the brief findings of polite\nemotional dialogue acts, where we can correlate the relational bonds between\nthese socio-linguistics cues. We found that the utterances with emotion classes\nAnger and Disgust are more likely to be impolite while Happiness and Sadness to\nbe polite. Similar phenomenon occurs with dialogue acts, Inform and Commissive\ncontain many polite utterances than Question and Directive. Finally, we will\nconclude on the future work of these findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bothe_C/0/1/0/all/0/1\">Chandrakant Bothe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-modal Attention Network for Stock Movements Prediction. (arXiv:2112.13593v1 [cs.LG])","link":"http://arxiv.org/abs/2112.13593","description":"<p>Stock prices move as piece-wise trending fluctuation rather than a purely\nrandom walk. Traditionally, the prediction of future stock movements is based\non the historical trading record. Nowadays, with the development of social\nmedia, many active participants in the market choose to publicize their\nstrategies, which provides a window to glimpse over the whole market's attitude\ntowards future movements by extracting the semantics behind social media.\nHowever, social media contains conflicting information and cannot replace\nhistorical records completely. In this work, we propose a multi-modality\nattention network to reduce conflicts and integrate semantic and numeric\nfeatures to predict future stock movements comprehensively. Specifically, we\nfirst extract semantic information from social media and estimate their\ncredibility based on posters' identity and public reputation. Then we\nincorporate the semantic from online posts and numeric features from historical\nrecords to make the trading strategy. Experimental results show that our\napproach outperforms previous methods by a significant margin in both\nprediction accuracy (61.20\\%) and trading profits (9.13\\%). It demonstrates\nthat our method improves the performance of stock movements prediction and\ninforms future research on multi-modality fusion towards stock prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1\">Shwai He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shi Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeteroQA: Learning towards Question-and-Answering through Multiple Information Sources via Heterogeneous Graph Modeling. (arXiv:2112.13597v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13597","description":"<p>Community Question Answering (CQA) is a well-defined task that can be used in\nmany scenarios, such as E-Commerce and online user community for special\ninterests.\n</p>\n<p>In these communities, users can post articles, give comment, raise a question\nand answer it.\n</p>\n<p>These data form the heterogeneous information sources where each information\nsource have their own special structure and context (comments attached to an\narticle or related question with answers).\n</p>\n<p>Most of the CQA methods only incorporate articles or Wikipedia to extract\nknowledge and answer the user's question.\n</p>\n<p>However, various types of information sources in the community are not fully\nexplored by these CQA methods and these multiple information sources (MIS) can\nprovide more related knowledge to user's questions.\n</p>\n<p>Thus, we propose a question-aware heterogeneous graph transformer to\nincorporate the MIS in the user community to automatically generate the answer.\n</p>\n<p>To evaluate our proposed method, we conduct the experiments on two datasets:\n$\\text{MSM}^{\\text{plus}}$ the modified version of benchmark dataset MS-MARCO\nand the AntQA dataset which is the first large-scale CQA dataset with four\ntypes of MIS.\n</p>\n<p>Extensive experiments on two datasets show that our model outperforms all the\nbaselines in terms of all the metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Shen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuchi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuying Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark. (arXiv:2112.13610v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13610","description":"<p>Realizing general-purpose language intelligence has been a longstanding goal\nfor natural language processing, where standard evaluation benchmarks play a\nfundamental and guiding role. We argue that for general-purpose language\nintelligence evaluation, the benchmark itself needs to be comprehensive and\nsystematic. To this end, we propose CUGE, a Chinese Language Understanding and\nGeneration Evaluation benchmark with the following features: (1) Hierarchical\nbenchmark framework, where datasets are principally selected and organized with\na language capability-task-dataset hierarchy. (2) Multi-level scoring strategy,\nwhere different levels of model performance are provided based on the\nhierarchical framework. To facilitate CUGE, we provide a public leaderboard\nthat can be customized to support flexible model judging criteria. Evaluation\nresults on representative pre-trained language models indicate ample room for\nimprovement towards general-purpose language intelligence. CUGE is publicly\navailable at cuge.baai.ac.cn.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qingxiu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jian Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1\">Boxi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaojun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jinran Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1\">Zheni Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuancheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jinliang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chengqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huadong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guoyang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zile Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xianpei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1\">Zhifang Sui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Parameter Differentiation based Multilingual Neural Machine Translation. (arXiv:2112.13619v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13619","description":"<p>Multilingual neural machine translation (MNMT) aims to translate multiple\nlanguages with a single model and has been proved successful thanks to\neffective knowledge transfer among different languages with shared parameters.\nHowever, it is still an open question which parameters should be shared and\nwhich ones need to be task-specific. Currently, the common practice is to\nheuristically design or search language-specific modules, which is difficult to\nfind the optimal configuration. In this paper, we propose a novel parameter\ndifferentiation based method that allows the model to determine which\nparameters should be language-specific during training. Inspired by cellular\ndifferentiation, each shared parameter in our method can dynamically\ndifferentiate into more specialized types. We further define the\ndifferentiation criterion as inter-task gradient similarity. Therefore,\nparameters with conflicting inter-task gradients are more likely to be\nlanguage-specific. Extensive experiments on multilingual datasets have\ndemonstrated that our method significantly outperforms various strong baselines\nwith different parameter sharing configurations. Further analyses reveal that\nthe parameter sharing configuration obtained by our method correlates well with\nthe linguistic proximities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiajun Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on non-English Question Answering Dataset. (arXiv:2112.13634v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13634","description":"<p>Research in question answering datasets and models has gained a lot of\nattention in the research community. Many of them release their own question\nanswering datasets as well as the models. There is tremendous progress that we\nhave seen in this area of research. The aim of this survey is to recognize,\nsummarize and analyze the existing datasets that have been released by many\nresearchers, especially in non-English datasets as well as resources such as\nresearch code, and evaluation metrics. In this paper, we review question\nanswering datasets that are available in common languages other than English\nsuch as French, German, Japanese, Chinese, Arabic, Russian, as well as the\nmultilingual and cross-lingual question-answering datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1\">Andreas Chandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrizain_A/0/1/0/all/0/1\">Affandy Fahrizain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim/0/1/0/all/0/1\">Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laufried_S/0/1/0/all/0/1\">Simon Willyanto Laufried</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Empathetic Responses by Looking Ahead the User's Sentiment. (arXiv:1906.08487v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1906.08487","description":"<p>An important aspect of human conversation difficult for machines is\nconversing with empathy, which is to understand the user's emotion and respond\nappropriately. Recent neural conversation models that attempted to generate\nempathetic responses either focused on conditioning the output to a given\nemotion, or incorporating the current user emotional state. However, these\napproaches do not factor in how the user would feel towards the generated\nresponse. Hence, in this paper, we propose Sentiment Look-ahead, which is a\nnovel perspective for empathy that models the future user emotional state. In\nshort, Sentiment Look-ahead is a reward function under a reinforcement learning\nframework that provides a higher reward to the generative model when the\ngenerated utterance improves the user's sentiment. We implement and evaluate\nthree different possible implementations of sentiment look-ahead and\nempirically show that our proposed approach can generate significantly more\nempathetic, relevant, and fluent responses than other competitive baselines\nsuch as multitask learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jamin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Data Synthesis Method for Grammatical Error Correction. (arXiv:1909.13302v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1909.13302","description":"<p>Due to the lack of parallel data in current Grammatical Error Correction\n(GEC) task, models based on Sequence to Sequence framework cannot be adequately\ntrained to obtain higher performance. We propose two data synthesis methods\nwhich can control the error rate and the ratio of error types on synthetic\ndata. The first approach is to corrupt each word in the monolingual corpus with\na fixed probability, including replacement, insertion and deletion. Another\napproach is to train error generation models and further filtering the decoding\nresults of the models. The experiments on different synthetic data show that\nthe error rate is 40% and the ratio of error types is the same can improve the\nmodel performance better. Finally, we synthesize about 100 million data and\nachieve comparable performance as the state of the art, which uses twice as\nmuch data as we use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liner Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chencheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yongping Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erhong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Higher Criticism for Discriminating Word-Frequency Tables and Testing Authorship. (arXiv:1911.01208v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.01208","description":"<p>We adapt the Higher Criticism (HC) goodness-of-fit test to measure the\ncloseness between word-frequency tables. We apply this measure to authorship\nattribution challenges, where the goal is to identify the author of a document\nusing other documents whose authorship is known. The method is simple yet\nperforms well without handcrafting and tuning; reporting accuracy at the state\nof the art level in various current challenges. As an inherent side effect, the\nHC calculation identifies a subset of discriminating words. In practice, the\nidentified words have low variance across documents belonging to a corpus of\nhomogeneous authorship. We conclude that in comparing the similarity of a new\ndocument and a corpus of a single author, HC is mostly affected by words\ncharacteristic of the author and is relatively unaffected by topic structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kipnis_A/0/1/0/all/0/1\">Alon Kipnis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backpropagation through Signal Temporal Logic Specifications: Infusing Logical Structure into Gradient-Based Methods. (arXiv:2008.00097v3 [eess.SY] UPDATED)","link":"http://arxiv.org/abs/2008.00097","description":"<p>This paper presents a technique, named STLCG, to compute the quantitative\nsemantics of Signal Temporal Logic (STL) formulas using computation graphs.\nSTLCG provides a platform which enables the incorporation of logical\nspecifications into robotics problems that benefit from gradient-based\nsolutions. Specifically, STL is a powerful and expressive formal language that\ncan specify spatial and temporal properties of signals generated by both\ncontinuous and hybrid systems. The quantitative semantics of STL provide a\nrobustness metric, i.e., how much a signal satisfies or violates an STL\nspecification. In this work, we devise a systematic methodology for translating\nSTL robustness formulas into computation graphs. With this representation, and\nby leveraging off-the-shelf automatic differentiation tools, we are able to\nefficiently backpropagate through STL robustness formulas and hence enable a\nnatural and easy-to-use integration of STL specifications with many\ngradient-based approaches used in robotics. Through a number of examples\nstemming from various robotics applications, we demonstrate that STLCG is\nversatile, computationally efficient, and capable of incorporating human-domain\nknowledge into the problem formulation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Leung_K/0/1/0/all/0/1\">Karen Leung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arechiga_N/0/1/0/all/0/1\">Nikos Ar&#xe9;chiga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavone_M/0/1/0/all/0/1\">Marco Pavone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Optimality of Vagueness: \"Around\", \"Between\", and the Gricean Maxims. (arXiv:2008.11841v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2008.11841","description":"<p>Why is ordinary language vague? We argue that in contexts in which a\ncooperative speaker is not perfectly informed about the world, the use of vague\nexpressions can offer an optimal tradeoff between truthfulness (Gricean\nQuality) and informativeness (Gricean Quantity). Focusing on expressions of\napproximation such as \"around\", which are semantically vague, we show that they\nallow the speaker to convey indirect probabilistic information, in a way that\ncan give the listener a more accurate representation of the information\navailable to the speaker than any more precise expression would (intervals of\nthe form \"between\"). That is, vague sentences can be more informative than\ntheir precise counterparts. We give a probabilistic treatment of the\ninterpretation of \"around\", and offer a model for the interpretation and use of\n\"around\"-statements within the Rational Speech Act (RSA) framework. In our\naccount the shape of the speaker's distribution matters in ways not predicted\nby the Lexical Uncertainty model standardly used in the RSA framework for vague\npredicates. We use our approach to draw further lessons concerning the semantic\nflexibility of vague expressions and their irreducibility to more precise\nmeanings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Egre_P/0/1/0/all/0/1\">Paul Egr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_B/0/1/0/all/0/1\">Benjamin Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortier_A/0/1/0/all/0/1\">Ad&#xe8;le Mortier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verheyen_S/0/1/0/all/0/1\">Steven Verheyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.06206","description":"<p>Fine-tuning pre-trained models have achieved impressive performance on\nstandard natural language processing benchmarks. However, the resultant model\ngeneralizability remains poorly understood. We do not know, for example, how\nexcellent performance can lead to the perfection of generalization models. In\nthis study, we analyze a fine-tuned BERT model from different perspectives\nusing relation extraction. We also characterize the differences in\ngeneralization techniques according to our proposed improvements. From\nempirical experimentation, we find that BERT suffers a bottleneck in terms of\nrobustness by way of randomizations, adversarial and counterfactual tests, and\nbiases (i.e., selection and semantic). These findings highlight opportunities\nfor future improvements. Our open-sourced testbed DiagnoseRE is available in\n\\url{https://github.com/zjunlp/DiagnoseRE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reflective Decoding: Beyond Unidirectional Generation with Off-the-Shelf Language Models. (arXiv:2010.08566v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.08566","description":"<p>Publicly available, large pretrained LanguageModels (LMs) generate text with\nremarkable quality, but only sequentially from left to right. As a result, they\nare not immediately applicable to generation tasks that break the\nunidirectional assumption, such as paraphrasing or text-infilling,\nnecessitating task-specific supervision.\n</p>\n<p>In this paper, we present Reflective Decoding, a novel unsupervised algorithm\nthat allows for direct application of unidirectional LMs to non-sequential\ntasks. Our 2-step approach requires no supervision or even parallel corpora,\nonly two off-the-shelf pretrained LMs in opposite directions: forward and\nbackward. First, in the contextualization step, we use LMs to generate\nensembles of past and future contexts which collectively capture the input\n(e.g. the source sentence for paraphrasing). Second, in the reflection step, we\ncondition on these \"context ensembles\", generating outputs that are compatible\nwith them. Comprehensive empirical results demonstrate that Reflective Decoding\noutperforms strong unsupervised baselines on both paraphrasing and abductive\ntext infilling, significantly narrowing the gap between unsupervised and\nsupervised methods. Reflective Decoding surpasses multiple supervised baselines\non various metrics including human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1\">Chandra Bhagavatula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jena Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v4 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2102.01547","description":"<p>In this paper, we propose an open source, production first, and production\nready speech recognition toolkit called WeNet in which a new two-pass approach\nis implemented to unify streaming and non-streaming end-to-end (E2E) speech\nrecognition in a single model. The main motivation of WeNet is to close the gap\nbetween the research and the production of E2E speechrecognition models. WeNet\nprovides an efficient way to ship ASR applications in several real-world\nscenarios, which is the main difference and advantage to other open source E2E\nspeech recognition toolkits. In our toolkit, a new two-pass method is\nimplemented. Our method propose a dynamic chunk-based attention strategy of the\nthe transformer layers to allow arbitrary right context length modifies in\nhybrid CTC/attention architecture. The inference latency could be easily\ncontrolled by only changing the chunk size. The CTC hypotheses are then\nrescored by the attention decoder to get the final result. Our experiments on\nthe AISHELL-1 dataset using WeNet show that, our model achieves 5.03\\% relative\ncharacter error rate (CER) reduction in non-streaming ASR compared to a\nstandard non-streaming transformer. After model quantification, our model\nperform reasonable RTF and latency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1\">Zhuoyuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Binbin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhendong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Lei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1\">Xin Lei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Node Co-occurrence based Graph Neural Networks for Knowledge Graph Link Prediction. (arXiv:2104.07396v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07396","description":"<p>We introduce a novel embedding model, named NoGE, which aims to integrate\nco-occurrence among entities and relations into graph neural networks to\nimprove knowledge graph completion (i.e., link prediction). Given a knowledge\ngraph, NoGE constructs a single graph considering entities and relations as\nindividual nodes. NoGE then computes weights for edges among nodes based on the\nco-occurrence of entities and relations. Next, NoGE proposes Dual Quaternion\nGraph Neural Networks (DualQGNN) and utilizes DualQGNN to update vector\nrepresentations for entity and relation nodes. NoGE then adopts a score\nfunction to produce the triple scores. Comprehensive experimental results show\nthat NoGE obtains state-of-the-art results on three new and difficult benchmark\ndatasets CoDEx for knowledge graph completion.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dai Quoc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_V/0/1/0/all/0/1\">Vinh Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1\">Dinh Phung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Quoc Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter. (arXiv:2105.07148v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07148","description":"<p>Lexicon information and pre-trained models, such as BERT, have been combined\nto explore Chinese sequence labelling tasks due to their respective strengths.\nHowever, existing methods solely fuse lexicon features via a shallow and random\ninitialized sequence layer and do not integrate them into the bottom layers of\nBERT. In this paper, we propose Lexicon Enhanced BERT (LEBERT) for Chinese\nsequence labelling, which integrates external lexicon knowledge into BERT\nlayers directly by a Lexicon Adapter layer. Compared with the existing methods,\nour model facilitates deep lexicon knowledge fusion at the lower layers of\nBERT. Experiments on ten Chinese datasets of three tasks including Named Entity\nRecognition, Word Segmentation, and Part-of-Speech tagging, show that LEBERT\nachieves the state-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiyan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wenming Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Killing One Bird with Two Stones: Model Extraction and Attribute Inference Attacks against BERT-based APIs. (arXiv:2105.10909v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2105.10909","description":"<p>The collection and availability of big data, combined with advances in\npre-trained models (e.g., BERT, XLNET, etc), have revolutionized the predictive\nperformance of modern natural language processing tasks, ranging from text\nclassification to text generation. This allows corporations to provide machine\nlearning as a service (MLaaS) by encapsulating fine-tuned BERT-based models as\nAPIs. However, BERT-based APIs have exhibited a series of security and privacy\nvulnerabilities. For example, prior work has exploited the security issues of\nthe BERT-based APIs through the adversarial examples crafted by the extracted\nmodel. However, the privacy leakage problems of the BERT-based APIs through the\nextracted model have not been well studied. On the other hand, due to the high\ncapacity of BERT-based APIs, the fine-tuned model is easy to be overlearned,\nbut what kind of information can be leaked from the extracted model remains\nunknown. In this work, we bridge this gap by first presenting an effective\nmodel extraction attack, where the adversary can practically steal a BERT-based\nAPI (the target/victim model) by only querying a limited number of queries. We\nfurther develop an effective attribute inference attack which can infer the\nsensitive attribute of the training data used by the BERT-based APIs. Our\nextensive experiments on benchmark datasets under various realistic settings\nvalidate the potential vulnerabilities of BERT-based APIs. Moreover, we\ndemonstrate that two promising defense methods become ineffective against our\nattacks, which calls for more effective defense methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatically Detecting Cyberbullying Comments on Online Game Forums. (arXiv:2106.01598v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.01598","description":"<p>Online game forums are popular to most of game players. They use it to\ncommunicate and discuss the strategy of the game, or even to make friends.\nHowever, game forums also contain abusive and harassment speech, disturbing and\nthreatening players. Therefore, it is necessary to automatically detect and\nremove cyberbullying comments to keep the game forum clean and friendly. We use\nthe Cyberbullying dataset collected from World of Warcraft (WoW) and League of\nLegends (LoL) forums and train classification models to automatically detect\nwhether a comment of a player is abusive or not. The result obtains 82.69% of\nmacro F1-score for LoL forum and 83.86% of macro F1-score for WoW forum by the\nToxic-BERT model on the Cyberbullying dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Hanh Hong-Phuc Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Trung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mischievous Nominal Constructions in Universal Dependencies. (arXiv:2108.12928v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12928","description":"<p>While the highly multilingual Universal Dependencies (UD) project provides\nextensive guidelines for clausal structure as well as structure within\ncanonical nominal phrases, a standard treatment is lacking for many\n\"mischievous\" nominal phenomena that break the mold. As a result, numerous\ninconsistencies within and across corpora can be found, even in languages with\nextensive UD treebanking work, such as English. This paper surveys the kinds of\nmischievous nominal expressions attested in English UD corpora and proposes\nsolutions primarily with English in mind, but which may offer paths to\nsolutions for a variety of UD languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schneider_N/0/1/0/all/0/1\">Nathan Schneider</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1\">Amir Zeldes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning. (arXiv:2109.06935v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06935","description":"<p>Recent work has shown evidence that the knowledge acquired by multilingual\nBERT (mBERT) has two components: a language-specific and a language-neutral\none. This paper analyses the relationship between them, in the context of\nfine-tuning on two tasks -- POS tagging and natural language inference -- which\nrequire the model to bring to bear different degrees of language-specific\nknowledge. Visualisations reveal that mBERT loses the ability to cluster\nrepresentations by language after fine-tuning, a result that is supported by\nevidence from language identification experiments. However, further experiments\non 'unlearning' language-specific representations using gradient reversal and\niterative adversarial learning are shown not to add further improvement to the\nlanguage-independent component over and above the effect of fine-tuning. The\nresults presented here suggest that the process of fine-tuning causes a\nreorganisation of the model's limited representational capacity, enhancing\nlanguage-independent representations at the expense of language-specific ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tanti_M/0/1/0/all/0/1\">Marc Tanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plas_L/0/1/0/all/0/1\">Lonneke van der Plas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borg_C/0/1/0/all/0/1\">Claudia Borg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatt_A/0/1/0/all/0/1\">Albert Gatt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FST Morphological Analyser and Generator for Mapud\\\"ungun. (arXiv:2109.09176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.09176","description":"<p>Following the Mapuche grammar by Smeets, this article describes the main\nmorphophonological aspects of Mapud\\\"ungun, explaining what triggers them and\nthe contexts where they arise. We present a computational approach producing a\nfinite state morphological analyser (and generator) capable of classifying and\nappropriately tagging all the components (roots and suffixes) that interact in\na Mapuche word form. The bulk of the article focuses on presenting details\nabout the morphology of Mapud\\\"ungun verb and its formalisation using FOMA. A\nsystem evaluation process and its results are also present in this article.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chandia_A/0/1/0/all/0/1\">Andr&#xe9;s Chand&#xed;a</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge. (arXiv:2109.12573v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12573","description":"<p>Cross-lingual pre-training has achieved great successes using monolingual and\nbilingual plain text corpora. However, most pre-trained models neglect\nmultilingual knowledge, which is language agnostic but comprises abundant\ncross-lingual structure alignment. In this paper, we propose XLM-K, a\ncross-lingual language model incorporating multilingual knowledge in\npre-training. XLM-K augments existing multilingual pre-training with two\nknowledge tasks, namely Masked Entity Prediction Task and Object Entailment\nTask. We evaluate XLM-K on MLQA, NER and XNLI. Experimental results clearly\ndemonstrate significant improvements over existing multilingual language\nmodels. The results on MLQA and NER exhibit the superiority of XLM-K in\nknowledge related tasks. The success in XNLI shows a better cross-lingual\ntransferability obtained in XLM-K. What is more, we provide a detailed probing\nanalysis to confirm the desired knowledge captured in our pre-training regimen.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoze Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weizhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing deployment hazards (\"External Safety\"). Throughout, we clarify each\nproblem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic Model Supervised by Understanding Map. (arXiv:2110.06043v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.06043","description":"<p>Inspired by the notion of Center of Mass in physics, an extension called\nSemantic Center of Mass (SCOM) is proposed, and used to discover the abstract\n\"topic\" of a document. The notion is under a framework model called\nUnderstanding Map Supervised Topic Model (UM-S-TM). The devise aim of UM-S-TM\nis to let both the document content and a semantic network -- specifically,\nUnderstanding Map -- play a role, in interpreting the meaning of a document.\nBased on different justifications, three possible methods are devised to\ndiscover the SCOM of a document. Some experiments on artificial documents and\nUnderstanding Maps are conducted to test their outcomes. In addition, its\nability of vectorization of documents and capturing sequential information are\ntested. We also compared UM-S-TM with probabilistic topic models like Latent\nDirichlet Allocation (LDA) and probabilistic Latent Semantic Analysis (pLSA).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Gangli Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Evaluation and Moderation of Open-domain Dialogue Systems. (arXiv:2111.02110v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.02110","description":"<p>The development of Open-Domain Dialogue Systems (ODS)is a trending topic due\nto the large number of research challenges, large societal and business impact,\nand advances in the underlying technology. However, the development of these\nkinds of systems requires two important characteristics:1) automatic evaluation\nmechanisms that show high correlations with human judgements across multiple\ndialogue evaluation aspects (with explainable features for providing\nconstructive and explicit feedback on the quality of generative models'\nresponses for quick development and deployment)and 2) mechanisms that can help\nto control chatbot responses,while avoiding toxicity and employing intelligent\nways to handle toxic user comments and keeping interaction flow and engagement.\nThis track at the 10th Dialogue System Technology Challenge (DSTC10) is part of\nthe ongoing effort to promote scalable and toxic-free ODS. This paper describes\nthe datasets and baselines provided to participants, as well as submission\nevaluation results for each of the two proposed subtasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1\">Jo&#xe3;o Sedoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1\">Luis Fernando D&#x27;Haro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banchs_R/0/1/0/all/0/1\">Rafael Banchs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudnicky_A/0/1/0/all/0/1\">Alexander Rudnicky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Profitable Trade-Off Between Memory and Performance In Multi-Domain Chatbot Architectures. (arXiv:2111.03963v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.03963","description":"<p>Text classification problem is a very broad field of study in the field of\nnatural language processing. In short, the text classification problem is to\ndetermine which of the previously determined classes the given text belongs to.\nSuccessful studies have been carried out in this field in the past studies. In\nthe study, Bidirectional Encoder Representations for Transformers (BERT), which\nis a frequently preferred method for solving the classification problem in the\nfield of natural language processing, is used. By solving classification\nproblems through a single model to be used in a chatbot architecture, it is\naimed to alleviate the load on the server that will be created by more than one\nmodel used for solving more than one classification problem. At this point,\nwith the masking method applied during the estimation of a single BERT model,\nwhich was created for classification in more than one subject, the estimation\nof the model was provided on a problem-based basis. Three separate data sets\ncovering different fields from each other are divided by various methods in\norder to complicate the problem, and classification problems that are very\nclose to each other in terms of field are also included in this way. The\ndataset used in this way consists of five classification problems with 154\nclasses. A BERT model containing all classification problems and other BERT\nmodels trained specifically for the problems were compared with each other in\nterms of performance and the space they occupied on the server.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1\">D. Emre Ta&#x15f;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1\">&#x15e;&#xfc;kr&#xfc; Ozan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akca_M/0/1/0/all/0/1\">M. Fatih Akca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olmez_O/0/1/0/all/0/1\">O&#x11f;uzhan &#xd6;lmez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulum_S/0/1/0/all/0/1\">Semih G&#xfc;l&#xfc;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kutal_S/0/1/0/all/0/1\">Se&#xe7;ilay Kutal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belhan_C/0/1/0/all/0/1\">Ceren Belhan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-Supervised Learning and Explicit Policy Injection. (arXiv:2111.14592v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.14592","description":"<p>Pre-trained models have proved to be powerful in enhancing task-oriented\ndialog systems. However, current pre-training methods mainly focus on enhancing\ndialog understanding and generation tasks while neglecting the exploitation of\ndialog policy. In this paper, we propose GALAXY, a novel pre-trained dialog\nmodel that explicitly learns dialog policy from limited labeled dialogs and\nlarge-scale unlabeled dialog corpora via semi-supervised learning.\nSpecifically, we introduce a dialog act prediction task for policy optimization\nduring pre-training and employ a consistency regularization term to refine the\nlearned representation with the help of unlabeled dialogs. We also implement a\ngating mechanism to weigh suitable unlabeled dialog samples. Empirical results\nshow that GALAXY substantially improves the performance of task-oriented dialog\nsystems, and achieves new state-of-the-art results on benchmark datasets:\nIn-Car, MultiWOZ2.0 and MultiWOZ2.1, improving their end-to-end combined scores\nby 2.5, 5.3 and 5.5 points, respectively. We also show that GALAXY has a\nstronger few-shot ability than existing models under various low-resource\nsettings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wanwei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yinpei Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinhe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuchuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zheng Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dermot Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1\">Peng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Min Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongbin Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GKS: Graph-based Knowledge Selector for Task-oriented Dialog System. (arXiv:2112.03719v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.03719","description":"<p>In previous research, knowledge-selection tasks mostly rely on language\nmodel-based methods or knowledge ranking. However, while approaches that rely\non the language models take all knowledge as sequential input, knowledge does\nnot contain sequential information in most circumstances. On the other hand,\nthe knowledge-ranking methods leverage dialog history and each given knowledge\nsnippet separately, but they do not consider information between knowledge\nsnippets. In the Tenth Dialog System Technology Challenges (DSTC10), we\nparticipated in the second Knowledge-grounded Task-oriented Dialogue Modeling\non Spoken Conversations. To deal with the problems mentioned above, we modified\ntraining methods based on state-of-the-art (SOTA) models for the first and\nthird sub-tasks. As for the second sub-task of knowledge selection, we proposed\nGraph-Knowledge Selector (GKS), utilizing a graph-attention base model\nincorporated with the language model. GKS makes knowledge-selection decisions\nin the dialog by simultaneously considering each knowledge embedding generated\nfrom the language model without sequential features. Moreover, GKS leverages\nconsiderable knowledge in decision-making and takes relations across knowledge\nas part of the selection process. As a result, GKS outperforms several SOTA\nmodels proposed in the data-set on knowledge selection from the Ninth Dialog\nSystem Technology Challenges (DSTC9).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jen-Chieh Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jia-Yan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Sung-Ping Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Ya-Chieh Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Self-Attention Generative Adversarial Network for Iterative Reconstruction of CT Images. (arXiv:2112.12810v1 [eess.IV])","link":"http://arxiv.org/abs/2112.12810","description":"<p>Computed tomography (CT) uses X-ray measurements taken from sensors around\nthe body to generate tomographic images of the human body. Conventional\nreconstruction algorithms can be used if the X-ray data are adequately sampled\nand of high quality; however, concerns such as reducing dose to the patient, or\ngeometric limitations on data acquisition, may result in low quality or\nincomplete data. Images reconstructed from these data using conventional\nmethods are of poor quality, due to noise and other artifacts. The aim of this\nstudy is to train a single neural network to reconstruct high-quality CT images\nfrom noisy or incomplete CT scan data, including low-dose, sparse-view, and\nlimited-angle scenarios. To accomplish this task, we train a generative\nadversarial network (GAN) as a signal prior, to be used in conjunction with the\niterative simultaneous algebraic reconstruction technique (SART) for CT data.\nThe network includes a self-attention block to model long-range dependencies in\nthe data. We compare our Self-Attention GAN for CT image reconstruction with\nseveral state-of-the-art approaches, including denoising cycle GAN, CIRCLE GAN,\nand a total variation superiorized algorithm. Our approach is shown to have\ncomparable overall performance to CIRCLE GAN, while outperforming the other two\napproaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xing_R/0/1/0/all/0/1\">Ruiwen Xing</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Humphries_T/0/1/0/all/0/1\">Thomas Humphries</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Si_D/0/1/0/all/0/1\">Dong Si</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDN-VO: Estimating Visual Odometry with Confidence. (arXiv:2112.12812v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12812","description":"<p>Visual Odometry (VO) is used in many applications including robotics and\nautonomous systems. However, traditional approaches based on feature matching\nare computationally expensive and do not directly address failure cases,\ninstead relying on heuristic methods to detect failure. In this work, we\npropose a deep learning-based VO model to efficiently estimate 6-DoF poses, as\nwell as a confidence model for these estimates. We utilise a CNN - RNN hybrid\nmodel to learn feature representations from image sequences. We then employ a\nMixture Density Network (MDN) which estimates camera motion as a mixture of\nGaussians, based on the extracted spatio-temporal representations. Our model\nuses pose labels as a source of supervision, but derives uncertainties in an\nunsupervised manner. We evaluate the proposed model on the KITTI and nuScenes\ndatasets and report extensive quantitative and qualitative results to analyse\nthe performance of both pose and uncertainty estimation. Our experiments show\nthat the proposed model exceeds state-of-the-art performance in addition to\ndetecting failure cases using the predicted pose uncertainty.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaygusuz_N/0/1/0/all/0/1\">Nimet Kaygusuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1\">Oscar Mendez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Camera Sensor Fusion for Visual Odometry using Deep Uncertainty Estimation. (arXiv:2112.12818v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12818","description":"<p>Visual Odometry (VO) estimation is an important source of information for\nvehicle state estimation and autonomous driving. Recently, deep learning based\napproaches have begun to appear in the literature. However, in the context of\ndriving, single sensor based approaches are often prone to failure because of\ndegraded image quality due to environmental factors, camera placement, etc. To\naddress this issue, we propose a deep sensor fusion framework which estimates\nvehicle motion using both pose and uncertainty estimations from multiple\non-board cameras. We extract spatio-temporal feature representations from a set\nof consecutive images using a hybrid CNN - RNN model. We then utilise a Mixture\nDensity Network (MDN) to estimate the 6-DoF pose as a mixture of distributions\nand a fusion module to estimate the final pose using MDN outputs from\nmulti-cameras. We evaluate our approach on the publicly available, large scale\nautonomous vehicle dataset, nuScenes. The results show that the proposed fusion\napproach surpasses the state-of-the-art, and provides robust estimates and\naccurate trajectories compared to individual camera-based estimations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kaygusuz_N/0/1/0/all/0/1\">Nimet Kaygusuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1\">Oscar Mendez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1\">Richard Bowden</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dense anomaly detection by robust learning on synthetic negative data. (arXiv:2112.12833v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12833","description":"<p>Standard machine learning is unable to accommodate inputs which do not belong\nto the training distribution. The resulting models often give rise to confident\nincorrect predictions which may lead to devastating consequences. This problem\nis especially demanding in the context of dense prediction since input images\nmay be partially anomalous. Previous work has addressed dense anomaly detection\nby discriminative training on mixed-content images. We extend this approach\nwith synthetic negative patches which simultaneously achieve high inlier\nlikelihood and uniform discriminative prediction. We generate synthetic\nnegatives with normalizing flows due to their outstanding distribution coverage\nand capability to generate samples at different resolutions. We also propose to\ndetect anomalies according to a principled information-theoretic criterion\nwhich can be consistently applied through training and inference. The resulting\nmodels set the new state of the art on standard benchmarks and datasets in\nspite of minimal computational overhead and refraining from auxiliary negative\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Grcic_M/0/1/0/all/0/1\">Matej Grci&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bevandic_P/0/1/0/all/0/1\">Petra Bevandi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segvic_S/0/1/0/all/0/1\">Sini&#x161;a &#x160;egvi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faster Deep Ensemble Averaging for Quantification of DNA Damage from Comet Assay Images With Uncertainty Estimates. (arXiv:2112.12839v1 [q-bio.QM])","link":"http://arxiv.org/abs/2112.12839","description":"<p>Several neurodegenerative diseases involve the accumulation of cellular DNA\ndamage. Comet assays are a popular way of estimating the extent of DNA damage.\nCurrent literature on the use of deep learning to quantify DNA damage presents\nan empirical approach to hyper-parameter optimization and does not include\nuncertainty estimates. Deep ensemble averaging is a standard approach to\nestimating uncertainty but it requires several iterations of network training,\nwhich makes it time-consuming. Here we present an approach to quantify the\nextent of DNA damage that combines deep learning with a rigorous and\ncomprehensive method to optimize the hyper-parameters with the help of\nstatistical tests. We also use an architecture that allows for a faster\ncomputation of deep ensemble averaging and performs statistical tests\napplicable to networks using transfer learning. We applied our approach to a\ncomet assay dataset with more than 1300 images and achieved an $R^2$ of 0.84,\nwhere the output included the confidence interval for each prediction. The\nproposed architecture is an improvement over the current approaches since it\nspeeds up the uncertainty estimation by 30X while being statistically more\nrigorous.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Namuduri_S/0/1/0/all/0/1\">Srikanth Namuduri</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Mehta_P/0/1/0/all/0/1\">Prateek Mehta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Barbe_L/0/1/0/all/0/1\">Lise Barbe</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lam_S/0/1/0/all/0/1\">Stephanie Lam</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Faghihmonzavi_Z/0/1/0/all/0/1\">Zohreh Faghihmonzavi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Finkbeiner_S/0/1/0/all/0/1\">Steve Finkbeiner</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Bhansali_S/0/1/0/all/0/1\">Shekhar Bhansali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the impact of class imbalance on the performance of chest x-ray image classifiers. (arXiv:2112.12843v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12843","description":"<p>This work aims to understand the impact of class imbalance on the performance\nof chest x-ray classifiers, in light of the standard evaluation practices\nadopted by researchers in terms of discrimination and calibration performance.\nFirstly, we conducted a literature study to analyze common scientific practices\nand confirmed that: (1) even when dealing with highly imbalanced datasets, the\ncommunity tends to use metrics that are dominated by the majority class; and\n(2) it is still uncommon to include calibration studies for chest x-ray\nclassifiers, albeit its importance in the context of healthcare. Secondly, we\nperform a systematic experiment on two major chest x-ray datasets to explore\nthe behavior of several performance metrics under different class ratios and\nshow that widely adopted metrics can conceal the performance in the minority\nclass. Finally, we propose the adoption of two alternative metrics, the\nprecision-recall curve and the Balanced Brier score, which better reflect the\nperformance of the system in such scenarios. Our results indicate that current\nevaluation practices adopted by the research community for chest x-ray\nclassifiers may not reflect the performance of such systems for computer-aided\ndiagnosis in real clinical scenarios, and suggest alternatives to improve this\nsituation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mosquera_C/0/1/0/all/0/1\">Candelaria Mosquera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_L/0/1/0/all/0/1\">Luciana Ferrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milone_D/0/1/0/all/0/1\">Diego Milone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luna_D/0/1/0/all/0/1\">Daniel Luna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1\">Enzo Ferrante</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSPACE: Synthetic Parametric Humans Animated in Complex Environments. (arXiv:2112.12867v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12867","description":"<p>Advances in the state of the art for 3d human sensing are currently limited\nby the lack of visual datasets with 3d ground truth, including multiple people,\nin motion, operating in real-world environments, with complex illumination or\nocclusion, and potentially observed by a moving camera. Sophisticated scene\nunderstanding would require estimating human pose and shape as well as\ngestures, towards representations that ultimately combine useful metric and\nbehavioral signals with free-viewpoint photo-realistic visualisation\ncapabilities. To sustain progress, we build a large-scale photo-realistic\ndataset, Human-SPACE (HSPACE), of animated humans placed in complex synthetic\nindoor and outdoor environments. We combine a hundred diverse individuals of\nvarying ages, gender, proportions, and ethnicity, with hundreds of motions and\nscenes, as well as parametric variations in body shape (for a total of 1,600\ndifferent humans), in order to generate an initial dataset of over 1 million\nframes. Human animations are obtained by fitting an expressive human body\nmodel, GHUM, to single scans of people, followed by novel re-targeting and\npositioning procedures that support the realistic animation of dressed humans,\nstatistical variation of body proportions, and jointly consistent scene\nplacement of multiple moving people. Assets are generated automatically, at\nscale, and are compatible with existing real time rendering and game engines.\nThe dataset with evaluation server will be made available for research. Our\nlarge-scale analysis of the impact of synthetic data, in connection with real\ndata and weak supervision, underlines the considerable potential for continuing\nquality improvements and limiting the sim-to-real gap, in this practical\nsetting, in connection with increased model capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bazavan_E/0/1/0/all/0/1\">Eduard Gabriel Bazavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_A/0/1/0/all/0/1\">Andrei Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanfir_M/0/1/0/all/0/1\">Mihai Zanfir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1\">William T. Freeman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukthankar_R/0/1/0/all/0/1\">Rahul Sukthankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sminchisescu_C/0/1/0/all/0/1\">Cristian Sminchisescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A formal approach to good practices in Pseudo-Labeling for Unsupervised Domain Adaptive Re-Identification. (arXiv:2112.12887v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12887","description":"<p>The use of pseudo-labels prevails in order to tackle Unsupervised Domain\nAdaptive (UDA) Re-Identification (re-ID) with the best performance. Indeed,\nthis family of approaches has given rise to several UDA re-ID specific\nframeworks, which are effective. In these works, research directions to improve\nPseudo-Labeling UDA re-ID performance are varied and mostly based on intuition\nand experiments: refining pseudo-labels, reducing the impact of errors in\npseudo-labels... It can be hard to deduce from them general good practices,\nwhich can be implemented in any Pseudo-Labeling method, to consistently improve\nits performance. To address this key question, a new theoretical view on\nPseudo-Labeling UDA re-ID is proposed. The contributions are threefold: (i) A\nnovel theoretical framework for Pseudo-Labeling UDA re-ID, formalized through a\nnew general learning upper-bound on the UDA re-ID performance. (ii) General\ngood practices for Pseudo-Labeling, directly deduced from the interpretation of\nthe proposed theoretical framework, in order to improve the target re-ID\nperformance. (iii) Extensive experiments on challenging person and vehicle\ncross-dataset re-ID tasks, showing consistent performance improvements for\nvarious state-of-the-art methods and various proposed implementations of good\npractices.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dubourvieux_F/0/1/0/all/0/1\">Fabian Dubourvieux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audigier_R/0/1/0/all/0/1\">Romaric Audigier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loesch_A/0/1/0/all/0/1\">Ang&#xe9;lique Loesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainouz_S/0/1/0/all/0/1\">Samia Ainouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canu_S/0/1/0/all/0/1\">St&#xe9;phane Canu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cluster-guided Image Synthesis with Unconditional Models. (arXiv:2112.12911v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12911","description":"<p>Generative Adversarial Networks (GANs) are the driving force behind the\nstate-of-the-art in image generation. Despite their ability to synthesize\nhigh-resolution photo-realistic images, generating content with on-demand\nconditioning of different granularity remains a challenge. This challenge is\nusually tackled by annotating massive datasets with the attributes of interest,\na laborious task that is not always a viable option. Therefore, it is vital to\nintroduce control into the generation process of unsupervised generative\nmodels. In this work, we focus on controllable image generation by leveraging\nGANs that are well-trained in an unsupervised fashion. To this end, we discover\nthat the representation space of intermediate layers of the generator forms a\nnumber of clusters that separate the data according to semantically meaningful\nattributes (e.g., hair color and pose). By conditioning on the cluster\nassignments, the proposed method is able to control the semantic class of the\ngenerated image. Our approach enables sampling from each cluster by Implicit\nMaximum Likelihood Estimation (IMLE). We showcase the efficacy of our approach\non faces (CelebA-HQ and FFHQ), animals (Imagenet) and objects (LSUN) using\ndifferent pre-trained generative models. The results highlight the ability of\nour approach to condition image generation on attributes like gender, pose and\nhair style on faces, as well as a variety of features on different object\nclasses.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Georgopoulos_M/0/1/0/all/0/1\">Markos Georgopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oldfield_J/0/1/0/all/0/1\">James Oldfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chrysos_G/0/1/0/all/0/1\">Grigorios G Chrysos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagakis_Y/0/1/0/all/0/1\">Yannis Panagakis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Semantics Allow for Textual Reasoning Better in Scene Text Recognition. (arXiv:2112.12916v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12916","description":"<p>Existing Scene Text Recognition (STR) methods typically use a language model\nto optimize the joint probability of the 1D character sequence predicted by a\nvisual recognition (VR) model, which ignore the 2D spatial context of visual\nsemantics within and between character instances, making them not generalize\nwell to arbitrary shape scene text. To address this issue, we make the first\nattempt to perform textual reasoning based on visual semantics in this paper.\nTechnically, given the character segmentation maps predicted by a VR model, we\nconstruct a subgraph for each instance, where nodes represent the pixels in it\nand edges are added between nodes based on their spatial similarity. Then,\nthese subgraphs are sequentially connected by their root nodes and merged into\na complete graph. Based on this graph, we devise a graph convolutional network\nfor textual reasoning (GTR) by supervising it with a cross-entropy loss. GTR\ncan be easily plugged in representative STR models to improve their performance\nowing to better textual reasoning. Specifically, we construct our model, namely\nS-GTR, by paralleling GTR to the language model in a segmentation-based STR\nbaseline, which can effectively exploit the visual-linguistic complementarity\nvia mutual learning. S-GTR sets new state-of-the-art on six challenging STR\nbenchmarks and generalizes well to multi-linguistic datasets. Code is available\nat https://github.com/adeline-cs/GTR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yue He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1\">Fengxiang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaoyue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-initialization Optimization Network for Accurate 3D Human Pose and Shape Estimation. (arXiv:2112.12917v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12917","description":"<p>3D human pose and shape recovery from a monocular RGB image is a challenging\ntask. Existing learning based methods highly depend on weak supervision\nsignals, e.g. 2D and 3D joint location, due to the lack of in-the-wild paired\n3D supervision. However, considering the 2D-to-3D ambiguities existed in these\nweak supervision labels, the network is easy to get stuck in local optima when\ntrained with such labels. In this paper, we reduce the ambituity by optimizing\nmultiple initializations. Specifically, we propose a three-stage framework\nnamed Multi-Initialization Optimization Network (MION). In the first stage, we\nstrategically select different coarse 3D reconstruction candidates which are\ncompatible with the 2D keypoints of input sample. Each coarse reconstruction\ncan be regarded as an initialization leads to one optimization branch. In the\nsecond stage, we design a mesh refinement transformer (MRT) to respectively\nrefine each coarse reconstruction result via a self-attention mechanism.\nFinally, a Consistency Estimation Network (CEN) is proposed to find the best\nresult from mutiple candidates by evaluating if the visual evidence in RGB\nimage matches a given 3D reconstruction. Experiments demonstrate that our\nMulti-Initialization Optimization Network outperforms existing 3D mesh based\nmethods on multiple public benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiangyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Ming Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_G/0/1/0/all/0/1\">Guibo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xuetao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Voxels Are Equal: Semantic Scene Completion from the Point-Voxel Perspective. (arXiv:2112.12925v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12925","description":"<p>We revisit Semantic Scene Completion (SSC), a useful task to predict the\nsemantic and occupancy representation of 3D scenes, in this paper. A number of\nmethods for this task are always based on voxelized scene representations for\nkeeping local scene structure. However, due to the existence of visible empty\nvoxels, these methods always suffer from heavy computation redundancy when the\nnetwork goes deeper, and thus limit the completion quality. To address this\ndilemma, we propose our novel point-voxel aggregation network for this task.\nFirstly, we transfer the voxelized scenes to point clouds by removing these\nvisible empty voxels and adopt a deep point stream to capture semantic\ninformation from the scene efficiently. Meanwhile, a light-weight voxel stream\ncontaining only two 3D convolution layers preserves local structures of the\nvoxelized scenes. Furthermore, we design an anisotropic voxel aggregation\noperator to fuse the structure details from the voxel stream into the point\nstream, and a semantic-aware propagation module to enhance the up-sampling\nprocess in the point stream by semantic labels. We demonstrate that our model\nsurpasses state-of-the-arts on two benchmarks by a large margin, with only\ndepth images as the input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaxiang Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaokang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Gang Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification. (arXiv:2112.12927v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12927","description":"<p>Learning a common latent embedding by aligning the latent spaces of\ncross-modal autoencoders is an effective strategy for Generalized Zero-Shot\nClassification (GZSC). However, due to the lack of fine-grained instance-wise\nannotations, it still easily suffer from the domain shift problem for the\ndiscrepancy between the visual representation of diversified images and the\nsemantic representation of fixed attributes. In this paper, we propose an\ninnovative autoencoder network by learning Aligned Cross-Modal Representations\n(dubbed ACMR) for GZSC. Specifically, we propose a novel Vision-Semantic\nAlignment (VSA) method to strengthen the alignment of cross-modal latent\nfeatures on the latent subspaces guided by a learned classifier. In addition,\nwe propose a novel Information Enhancement Module (IEM) to reduce the\npossibility of latent variables collapse meanwhile encouraging the\ndiscriminative ability of latent variables. Extensive experiments on publicly\navailable datasets demonstrate the state-of-the-art performance of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhiyu Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaobin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zheng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jingyan Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Realtime Global Attention Network for Semantic Segmentation. (arXiv:2112.12939v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12939","description":"<p>In this paper, we proposed an end-to-end realtime global attention neural\nnetwork (RGANet) for the challenging task of semantic segmentation. Different\nfrom the encoding strategy deployed by self-attention paradigms, the proposed\nglobal attention module encodes global attention via depth-wise convolution and\naffine transformations. The integration of these global attention modules into\na hierarchy architecture maintains high inferential performance. In addition,\nan improved evaluation metric, namely MGRID, is proposed to alleviate the\nnegative effect of non-convex, widely scattered ground-truth areas. Results\nfrom extensive experiments on state-of-the-art architectures for semantic\nsegmentation manifest the leading performance of proposed approaches for\nrobotic monocular visual perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xi Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep ensembles in bioimage segmentation. (arXiv:2112.12955v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12955","description":"<p>Semantic segmentation consists in classifying each pixel of an image by\nassigning it to a specific label chosen from a set of all the available ones.\nDuring the last few years, a lot of attention shifted to this kind of task.\nMany computer vision researchers tried to apply autoencoder structures to\ndevelop models that can learn the semantics of the image as well as a low-level\nrepresentation of it. In an autoencoder architecture, given an input, an\nencoder computes a low dimensional representation of the input that is then\nused by a decoder to reconstruct the original data. In this work, we propose an\nensemble of convolutional neural networks (CNNs). In ensemble methods, many\ndifferent models are trained and then used for classification, the ensemble\naggregates the outputs of the single classifiers. The approach leverages on\ndifferences of various classifiers to improve the performance of the whole\nsystem. Diversity among the single classifiers is enforced by using different\nloss functions. In particular, we present a new loss function that results from\nthe combination of Dice and Structural Similarity Index. The proposed ensemble\nis implemented by combining different backbone networks using the DeepLabV3+\nand HarDNet environment. The proposal is evaluated through an extensive\nempirical evaluation on two real-world scenarios: polyp and skin segmentation.\nAll the code is available online at https://github.com/LorisNanni.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuza_D/0/1/0/all/0/1\">Daniela Cuza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loreggia_A/0/1/0/all/0/1\">Andrea Loreggia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SGTR: End-to-end Scene Graph Generation with Transformer. (arXiv:2112.12970v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12970","description":"<p>Scene Graph Generation (SGG) remains a challenging visual understanding task\ndue to its complex compositional property. Most previous works adopt a\nbottom-up two-stage or a point-based one-stage approach, which often suffers\nfrom overhead time complexity or sub-optimal design assumption. In this work,\nwe propose a novel SGG method to address the aforementioned issues, which\nformulates the task as a bipartite graph construction problem. To solve the\nproblem, we develop a transformer-based end-to-end framework that first\ngenerates the entity and predicate proposal set, followed by inferring directed\nedges to form the relation triplets. In particular, we develop a new\nentity-aware predicate representation based on a structural predicate generator\nto leverage the compositional property of relationships. Moreover, we design a\ngraph assembling module to infer the connectivity of the bipartite scene graph\nbased on our entity-aware structure, enabling us to generate the scene graph in\nan end-to-end manner. Extensive experimental results show that our design is\nable to achieve the state-of-the-art or comparable performance on two\nchallenging benchmarks, surpassing most of the existing approaches and enjoying\nhigher efficiency in inference. We hope our model can serve as a strong\nbaseline for the Transformer-based scene graph generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongjie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Songyang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuming He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Doppler velocity-based algorithm for Clustering and Velocity Estimation of moving objects. (arXiv:2112.12984v1 [cs.RO])","link":"http://arxiv.org/abs/2112.12984","description":"<p>We propose a Doppler velocity-based cluster and velocity estimation algorithm\nbased on the characteristics of FMCW LiDAR which achieves highly accurate,\nsingle-scan, and real-time motion state detection and velocity estimation. We\nprove the continuity of the Doppler velocity on the same object. Based on this\nprinciple, we achieve the distinction between moving objects and stationary\nbackground via region growing clustering algorithm. The obtained stationary\nbackground will be used to estimate the velocity of the FMCW LiDAR by the\nleast-squares method. Then we estimate the velocity of the moving objects using\nthe estimated LiDAR velocity and the Doppler velocity of moving objects\nobtained by clustering. To ensure real-time processing, we set the appropriate\nleast-squares parameters. Meanwhile, to verify the effectiveness of the\nalgorithm, we create the FMCW LiDAR model on the autonomous driving simulation\nplatform CARLA for spawning data. The results show that our algorithm can\nprocess at least a 4.5million points and estimate the velocity of 150 moving\nobjects per second under the arithmetic power of the Ryzen 3600x CPU, with a\nmotion state detection accuracy of over 99% and estimated velocity accuracy of\n0.1 m/s.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1\">Kai Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaozhi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSeg3D: An Interactive 3D Shape Segmentation Tool. (arXiv:2112.12988v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12988","description":"<p>A large-scale dataset is essential for learning good features in 3D shape\nunderstanding, but there are only a few datasets that can satisfy deep learning\ntraining. One of the major reasons is that current tools for annotating\nper-point semantic labels using polygons or scribbles are tedious and\ninefficient. To facilitate segmentation annotations in 3D shapes, we propose an\neffective annotation tool, named iSeg for 3D shape. It can obtain a satisfied\nsegmentation result with minimal human clicks (&lt; 10). Under our observation,\nmost objects can be considered as the composition of finite primitive shapes,\nand we train iSeg3D model on our built primitive-composed shape data to learn\nthe geometric prior knowledge in a self-supervised manner. Given human\ninteractions, the learned knowledge can be used to segment parts on arbitrary\nshapes, in which positive clicks help associate the primitives into the\nsemantic parts and negative clicks can avoid over-segmentation. Besides, We\nalso provide an online human-in-loop fine-tuning module that enables the model\nperform better segmentation with less clicks. Experiments demonstrate the\neffectiveness of iSeg3D on PartNet shape segmentation. Data and codes will be\nmade publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1\">Sucheng Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Liu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenqiang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cewu Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Aware Continual Zero-Shot Learning. (arXiv:2112.12989v1 [cs.CV])","link":"http://arxiv.org/abs/2112.12989","description":"<p>We introduce Domain Aware Continual Zero-Shot Learning (DACZSL), the task of\nvisually recognizing images of unseen categories in unseen domains\nsequentially. We created DACZSL on top of the DomainNet dataset by dividing it\ninto a sequence of tasks, where classes are incrementally provided on seen\ndomains during training and evaluation is conducted on unseen domains for both\nseen and unseen classes. We also proposed a novel Domain-Invariant CZSL Network\n(DIN), which outperforms state-of-the-art baseline models that we adapted to\nDACZSL setting. We adopt a structure-based approach to alleviate forgetting\nknowledge from previous tasks with a small per-task private network in addition\nto a global shared network. To encourage the private network to capture the\ndomain and task-specific representation, we train our model with a novel\nadversarial knowledge disentanglement setting to make our global network\ntask-invariant and domain-invariant over all the tasks. Our method also learns\na class-wise learnable prompt to obtain better class-level text representation,\nwhich is used to represent side information to enable zero-shot prediction of\nfuture unseen classes. Our code and benchmarks will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"US-GAN: On the importance of Ultimate Skip Connection for Facial Expression Synthesis. (arXiv:2112.13002v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13002","description":"<p>Recent studies have shown impressive results in multi-domain image-to-image\ntranslation for facial expression synthesis. While effective, these methods\nrequire a large number of labelled samples for plausible results. Their\nperformance significantly degrades when we train them on smaller datasets. To\naddress this limitation, in this work, we present US-GAN, a smaller and\neffective method for synthesizing plausible expressions by employing notably\nsmaller datasets. The proposed method comprises of encoding layers, single\nresidual block, decoding layers and an ultimate skip connection that links the\ninput image to an output image. It has three times lesser parameters as\ncompared to state-of-the-art facial expression synthesis methods. Experimental\nresults demonstrate the quantitative and qualitative effectiveness of our\nproposed method. In addition, we also show that an ultimate skip connection is\nsufficient for recovering rich facial and overall color details of the input\nface image that a larger state-of-the-art model fails to recover.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Akram_A/0/1/0/all/0/1\">Arbish Akram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Nazar Khan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Spectral Reconstruction from RGB Images via Implicit Neural Representation. (arXiv:2112.13003v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13003","description":"<p>Existing methods for spectral reconstruction usually learn a discrete mapping\nfrom RGB images to a number of spectral bands. However, this modeling strategy\nignores the continuous nature of spectral signature. In this paper, we propose\nNeural Spectral Reconstruction (NeSR) to lift this limitation, by introducing a\nnovel continuous spectral representation. To this end, we embrace the concept\nof implicit function and implement a parameterized embodiment with a neural\nnetwork. Specifically, we first adopt a backbone network to extract spatial\nfeatures of RGB inputs. Based on it, we devise Spectral Profile Interpolation\n(SPI) module and Neural Attention Mapping (NAM) module to enrich deep features,\nwhere the spatial-spectral correlation is involved for a better representation.\nThen, we view the number of sampled spectral bands as the coordinate of\ncontinuous implicit function, so as to learn the projection from deep features\nto spectral intensities. Extensive experiments demonstrate the distinct\nadvantage of NeSR in reconstruction accuracy over baseline methods. Moreover,\nNeSR extends the flexibility of spectral reconstruction by enabling an\narbitrary number of spectral bands as the target output.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruikang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_M/0/1/0/all/0/1\">Mingde Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1\">Zhiwei Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Pedestrian Odometry: The Brown Pedestrian Odometry Dataset (BPOD). (arXiv:2112.13018v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13018","description":"<p>We present the Brown Pedestrian Odometry Dataset (BPOD) for benchmarking\nvisual odometry algorithms in head-mounted pedestrian settings. This dataset\nwas captured using synchronized global and rolling shutter stereo cameras in 12\ndiverse indoor and outdoor locations on Brown University's campus. Compared to\nexisting datasets, BPOD contains more image blur and self-rotation, which are\ncommon in pedestrian odometry but rare elsewhere. Ground-truth trajectories are\ngenerated from stick-on markers placed along the pedestrian's path, and the\npedestrian's position is documented using a third-person video. We evaluate the\nperformance of representative direct, feature-based, and learning-based VO\nmethods on BPOD. Our results show that significant development is needed to\nsuccessfully capture pedestrian trajectories. The link to the dataset is here:\n\\url{https://doi.org/10.26300/c1n7-7p93\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Charatan_D/0/1/0/all/0/1\">David Charatan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongyi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimia_B/0/1/0/all/0/1\">Benjamin Kimia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Linguistic Commands to Navigable Regions. (arXiv:2112.13031v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13031","description":"<p>Humans have a natural ability to effortlessly comprehend linguistic commands\nsuch as \"park next to the yellow sedan\" and instinctively know which region of\nthe road the vehicle should navigate. Extending this ability to autonomous\nvehicles is the next step towards creating fully autonomous agents that respond\nand act according to human commands. To this end, we propose the novel task of\nReferring Navigable Regions (RNR), i.e., grounding regions of interest for\nnavigation based on the linguistic command. RNR is different from Referring\nImage Segmentation (RIS), which focuses on grounding an object referred to by\nthe natural language expression instead of grounding a navigable region. For\nexample, for a command \"park next to the yellow sedan,\" RIS will aim to segment\nthe referred sedan, and RNR aims to segment the suggested parking region on the\nroad. We introduce a new dataset, Talk2Car-RegSeg, which extends the existing\nTalk2car dataset with segmentation masks for the regions described by the\nlinguistic commands. A separate test split with concise manoeuvre-oriented\ncommands is provided to assess the practicality of our dataset. We benchmark\nthe proposed dataset using a novel transformer-based architecture. We present\nextensive ablations and show superior performance over baselines on multiple\nevaluation metrics. A downstream path planner generating trajectories based on\nRNR outputs confirms the efficacy of the proposed framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rufus_N/0/1/0/all/0/1\">Nivedita Rufus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_K/0/1/0/all/0/1\">Kanishk Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_U/0/1/0/all/0/1\">Unni Krishnan R Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_V/0/1/0/all/0/1\">Vineet Gandhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">K Madhava Krishna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Channel-Wise Attention-Based Network for Self-Supervised Monocular Depth Estimation. (arXiv:2112.13047v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13047","description":"<p>Self-supervised learning has shown very promising results for monocular depth\nestimation. Scene structure and local details both are significant clues for\nhigh-quality depth estimation. Recent works suffer from the lack of explicit\nmodeling of scene structure and proper handling of details information, which\nleads to a performance bottleneck and blurry artefacts in predicted results. In\nthis paper, we propose the Channel-wise Attention-based Depth Estimation\nNetwork (CADepth-Net) with two effective contributions: 1) The structure\nperception module employs the self-attention mechanism to capture long-range\ndependencies and aggregates discriminative features in channel dimensions,\nexplicitly enhances the perception of scene structure, obtains the better scene\nunderstanding and rich feature representation. 2) The detail emphasis module\nre-calibrates channel-wise feature maps and selectively emphasizes the\ninformative features, aiming to highlight crucial local details information and\nfuse different level features more efficiently, resulting in more precise and\nsharper depth prediction. Furthermore, the extensive experiments validate the\neffectiveness of our method and show that our model achieves the\nstate-of-the-art results on the KITTI benchmark and Make3D datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jiaxing Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_P/0/1/0/all/0/1\">Penghui Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">YuSheng Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Gated Memory Recurrent Network for Efficient Scalable HDR Deghosting. (arXiv:2112.13050v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13050","description":"<p>We propose a novel recurrent network-based HDR deghosting method for fusing\narbitrary length dynamic sequences. The proposed method uses convolutional and\nrecurrent architectures to generate visually pleasing, ghosting-free HDR\nimages. We introduce a new recurrent cell architecture, namely Self-Gated\nMemory (SGM) cell, that outperforms the standard LSTM cell while containing\nfewer parameters and having faster running times. In the SGM cell, the\ninformation flow through a gate is controlled by multiplying the gate's output\nby a function of itself. Additionally, we use two SGM cells in a bidirectional\nsetting to improve output quality. The proposed approach achieves\nstate-of-the-art performance compared to existing HDR deghosting methods\nquantitatively across three publicly available datasets while simultaneously\nachieving scalability to fuse variable-length input sequence without\nnecessitating re-training. Through extensive ablations, we demonstrate the\nimportance of individual components in our proposed approach. The code is\navailable at https://val.cds.iisc.ac.in/HDR/HDRRNN/index.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prabhakar_K/0/1/0/all/0/1\">K. Ram Prabhakar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1\">Susmit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1\">R. Venkatesh Babu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Wasserstein Dice Loss, Test-time Augmentation, and Transformers for the BraTS 2021 challenge. (arXiv:2112.13054v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13054","description":"<p>Brain tumor segmentation from multiple Magnetic Resonance Imaging (MRI)\nmodalities is a challenging task in medical image computation. The main\nchallenges lie in the generalizability to a variety of scanners and imaging\nprotocols. In this paper, we explore strategies to increase model robustness\nwithout increasing inference time. Towards this aim, we explore finding a\nrobust ensemble from models trained using different losses, optimizers, and\ntrain-validation data split. Importantly, we explore the inclusion of a\ntransformer in the bottleneck of the U-Net architecture. While we find\ntransformer in the bottleneck performs slightly worse than the baseline U-Net\nin average, the generalized Wasserstein Dice loss consistently produces\nsuperior results. Further, we adopt an efficient test time augmentation\nstrategy for faster and robust inference. Our final ensemble of seven 3D U-Nets\nwith test-time augmentation produces an average dice score of 89.4% and an\naverage Hausdorff 95% distance of 10.0 mm when evaluated on the BraTS 2021\ntesting dataset. Our code and trained models are publicly available at\nhttps://github.com/LucasFidon/TRABIT_BraTS2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fidon_L/0/1/0/all/0/1\">Lucas Fidon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shit_S/0/1/0/all/0/1\">Suprosanna Shit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1\">Ivan Ezhov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paetzold_J/0/1/0/all/0/1\">Johannes C. Paetzold</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1\">S&#xe9;bastien Ourselin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NIP: Neuron-level Inverse Perturbation Against Adversarial Attacks. (arXiv:2112.13060v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13060","description":"<p>Although deep learning models have achieved unprecedented success, their\nvulnerabilities towards adversarial attacks have attracted increasing\nattention, especially when deployed in security-critical domains. To address\nthe challenge, numerous defense strategies, including reactive and proactive\nones, have been proposed for robustness improvement. From the perspective of\nimage feature space, some of them cannot reach satisfying results due to the\nshift of features. Besides, features learned by models are not directly related\nto classification results. Different from them, We consider defense method\nessentially from model inside and investigated the neuron behaviors before and\nafter attacks. We observed that attacks mislead the model by dramatically\nchanging the neurons that contribute most and least to the correct label.\nMotivated by it, we introduce the concept of neuron influence and further\ndivide neurons into front, middle and tail part. Based on it, we propose\nneuron-level inverse perturbation(NIP), the first neuron-level reactive defense\nmethod against adversarial attacks. By strengthening front neurons and\nweakening those in the tail part, NIP can eliminate nearly all adversarial\nperturbations while still maintaining high benign accuracy. Besides, it can\ncope with different sizes of perturbations via adaptivity, especially larger\nones. Comprehensive experiments conducted on three datasets and six models show\nthat NIP outperforms the state-of-the-art baselines against eleven adversarial\nattacks. We further provide interpretable proofs via neuron activation and\nvisualization for better understanding.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Haibin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CatchBackdoor: Backdoor Testing by Critical Trojan Neural Path Identification via Differential Fuzzing. (arXiv:2112.13064v1 [cs.CR])","link":"http://arxiv.org/abs/2112.13064","description":"<p>The success of deep neural networks (DNNs) in real-world applications has\nbenefited from abundant pre-trained models. However, the backdoored pre-trained\nmodels can pose a significant trojan threat to the deployment of downstream\nDNNs. Existing DNN testing methods are mainly designed to find incorrect corner\ncase behaviors in adversarial settings but fail to discover the backdoors\ncrafted by strong trojan attacks. Observing the trojan network behaviors shows\nthat they are not just reflected by a single compromised neuron as proposed by\nprevious work but attributed to the critical neural paths in the activation\nintensity and frequency of multiple neurons. This work formulates the DNN\nbackdoor testing and proposes the CatchBackdoor framework. Via differential\nfuzzing of critical neurons from a small number of benign examples, we identify\nthe trojan paths and particularly the critical ones, and generate backdoor\ntesting examples by simulating the critical neurons in the identified paths.\nExtensive experiments demonstrate the superiority of CatchBackdoor, with higher\ndetection performance than existing methods. CatchBackdoor works better on\ndetecting backdoors by stealthy blending and adaptive attacks, which existing\nmethods fail to detect. Moreover, our experiments show that CatchBackdoor may\nreveal the potential backdoors of models in Model Zoo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Haibo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ruoxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jinyin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Ting Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yue Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zhaoyan Ming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Virtuoso: Video-based Intelligence for real-time tuning on SOCs. (arXiv:2112.13076v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13076","description":"<p>Efficient and adaptive computer vision systems have been proposed to make\ncomputer vision tasks, such as image classification and object detection,\noptimized for embedded or mobile devices. These solutions, quite recent in\ntheir origin, focus on optimizing the model (a deep neural network, DNN) or the\nsystem by designing an adaptive system with approximation knobs. In spite of\nseveral recent efforts, we show that existing solutions suffer from two major\ndrawbacks. First, the system does not consider energy consumption of the models\nwhile making a decision on which model to run. Second, the evaluation does not\nconsider the practical scenario of contention on the device, due to other\nco-resident workloads. In this work, we propose an efficient and adaptive video\nobject detection system, Virtuoso, which is jointly optimized for accuracy,\nenergy efficiency, and latency. Underlying Virtuoso is a multi-branch execution\nkernel that is capable of running at different operating points in the\naccuracy-energy-latency axes, and a lightweight runtime scheduler to select the\nbest fit execution branch to satisfy the user requirement. To fairly compare\nwith Virtuoso, we benchmark 15 state-of-the-art or widely used protocols,\nincluding Faster R-CNN (FRCNN), YOLO v3, SSD, EfficientDet, SELSA, MEGA, REPP,\nFastAdapt, and our in-house adaptive variants of FRCNN+, YOLO+, SSD+, and\nEfficientDet+ (our variants have enhanced efficiency for mobiles). With this\ncomprehensive benchmark, Virtuoso has shown superiority to all the above\nprotocols, leading the accuracy frontier at every efficiency level on NVIDIA\nJetson mobile GPUs. Specifically, Virtuoso has achieved an accuracy of 63.9%,\nwhich is more than 10% higher than some of the popular object detection models,\nFRCNN at 51.1%, and YOLO at 49.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jayoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">PengCheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasari_V/0/1/0/all/0/1\">Venkat Dasari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weston_N/0/1/0/all/0/1\">Noah Weston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Saurabh Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaterji_S/0/1/0/all/0/1\">Somali Chaterji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Scale Feature Fusion: Learning Better Semantic Segmentation for Road Pothole Detection. (arXiv:2112.13082v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13082","description":"<p>This paper presents a novel pothole detection approach based on single-modal\nsemantic segmentation. It first extracts visual features from input images\nusing a convolutional neural network. A channel attention module then reweighs\nthe channel features to enhance the consistency of different feature maps.\nSubsequently, we employ an atrous spatial pyramid pooling module (comprising of\natrous convolutions in series, with progressive rates of dilation) to integrate\nthe spatial context information. This helps better distinguish between potholes\nand undamaged road areas. Finally, the feature maps in the adjacent layers are\nfused using our proposed multi-scale feature fusion module. This further\nreduces the semantic gap between different feature channel layers. Extensive\nexperiments were carried out on the Pothole-600 dataset to demonstrate the\neffectiveness of our proposed method. The quantitative comparisons suggest that\nour method achieves the state-of-the-art (SoTA) performance on both RGB images\nand transformed disparity images, outperforming three SoTA single-modal\nsemantic segmentation networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jiahe Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bocus_M/0/1/0/all/0/1\">Mohammud J. Bocus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosking_B/0/1/0/all/0/1\">Brett Hosking</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rigen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vityazev_S/0/1/0/all/0/1\">Sergey Vityazev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1\">Rui Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimViT: Exploring a Simple Vision Transformer with sliding windows. (arXiv:2112.13085v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13085","description":"<p>Although vision Transformers have achieved excellent performance as backbone\nmodels in many vision tasks, most of them intend to capture global relations of\nall tokens in an image or a window, which disrupts the inherent spatial and\nlocal correlations between patches in 2D structure. In this paper, we introduce\na simple vision Transformer named SimViT, to incorporate spatial structure and\nlocal information into the vision Transformers. Specifically, we introduce\nMulti-head Central Self-Attention(MCSA) instead of conventional Multi-head\nSelf-Attention to capture highly local relations. The introduction of sliding\nwindows facilitates the capture of spatial structure. Meanwhile, SimViT\nextracts multi-scale hierarchical features from different layers for dense\nprediction tasks. Extensive experiments show the SimViT is effective and\nefficient as a general-purpose backbone model for various image processing\ntasks. Especially, our SimViT-Micro only needs 3.3M parameters to achieve 71.1%\ntop-1 accuracy on ImageNet-1k dataset, which is the smallest size vision\nTransformer model by now. Our code will be available in\nhttps://github.com/ucasligang/SimViT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Di Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xing Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Lingyu Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Changwen Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Invertible Network for Unpaired Low-light Image Enhancement. (arXiv:2112.13107v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13107","description":"<p>Existing unpaired low-light image enhancement approaches prefer to employ the\ntwo-way GAN framework, in which two CNN generators are deployed for enhancement\nand degradation separately. However, such data-driven models ignore the\ninherent characteristics of transformation between the low and normal light\nimages, leading to unstable training and artifacts. Here, we propose to\nleverage the invertible network to enhance low-light image in forward process\nand degrade the normal-light one inversely with unpaired learning. The\ngenerated and real images are then fed into discriminators for adversarial\nlearning. In addition to the adversarial loss, we design various loss functions\nto ensure the stability of training and preserve more image details.\nParticularly, a reversibility loss is introduced to alleviate the over-exposure\nproblem. Moreover, we present a progressive self-guided enhancement process for\nlow-light images and achieve favorable performance against the SOTAs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jize Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haolin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaohe Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1\">Wangmeng Zuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ultrasound Speckle Suppression and Denoising using MRI-derived Normalizing Flow Priors. (arXiv:2112.13110v1 [eess.SP])","link":"http://arxiv.org/abs/2112.13110","description":"<p>Ultrasonography offers an inexpensive, widely-accessible and compact medical\nimaging solution. However, compared to other imaging modalities such as CT and\nMRI, ultrasound images notoriously suffer from strong speckle noise, which\noriginates from the random interference of sub-wavelength scattering. This\ndeteriorates ultrasound image quality and makes interpretation challenging. We\nhere propose a new unsupervised ultrasound speckle reduction and image\ndenoising method based on maximum-a-posteriori estimation with deep generative\npriors that are learned from high-quality MRI images. To model the generative\ntissue reflectivity prior, we exploit normalizing flows, which in recent years\nhave shown to be very powerful in modeling signal priors across a variety of\napplications. To facilitate generaliation, we factorize the prior and train our\nflow model on patches from the NYU fastMRI (fully-sampled) dataset. This prior\nis then used for inference in an iterative denoising scheme. We first validate\nthe utility of our learned priors on noisy MRI data (no prior domain shift),\nand then turn to evaluating performance on both simulated and in-vivo\nultrasound images from the PICMUS and CUBDL datasets. The results show that the\nmethod outperforms other (unsupervised) ultrasound denoising methods (NLM and\nOBNLM) both quantitatively and qualitatively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Schaft_V/0/1/0/all/0/1\">Vincent van de Schaft</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sloun_R/0/1/0/all/0/1\">Ruud J.G. van Sloun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curse of Zero Task Diversity: On the Failure of Transfer Learning to Outperform MAML and their Empirical Equivalence. (arXiv:2112.13121v1 [cs.LG])","link":"http://arxiv.org/abs/2112.13121","description":"<p>It has been recently observed that a transfer learning solution might be all\nwe needed to solve many few-shot learning benchmarks. This raises important\nquestions about when and how meta-learning algorithms should be deployed. In\nthis paper, we make a first step in clarifying these questions by first\nformulating a computable metric for a few-shot learning benchmark that we\nhypothesize is predictive of whether meta-learning solutions will succeed or\nnot. We name this metric the diversity coefficient of a few-shot learning\nbenchmark. Using the diversity coefficient, we show that the MiniImagenet\nbenchmark has zero diversity - according to twenty-four different ways to\ncompute the diversity. We proceed to show that when making a fair comparison\nbetween MAML learned solutions to transfer learning, both have identical\nmeta-test accuracy. This suggests that transfer learning fails to outperform\nMAML - contrary to what previous work suggests. Together, these two facts\nprovide the first test of whether diversity correlates with meta-learning\nsuccess and therefore show that a diversity coefficient of zero correlates with\na high similarity between transfer learning and MAML learned solutions -\nespecially at meta-test time. We therefore conjecture meta-learned solutions\nhave the same meta-test performance as transfer learning when the diversity\ncoefficient is zero.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1\">Brando Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does MAML Only Work via Feature Re-use? A Data Centric Perspective. (arXiv:2112.13137v1 [cs.LG])","link":"http://arxiv.org/abs/2112.13137","description":"<p>Recent work has suggested that a good embedding is all we need to solve many\nfew-shot learning benchmarks. Furthermore, other work has strongly suggested\nthat Model Agnostic Meta-Learning (MAML) also works via this same method - by\nlearning a good embedding. These observations highlight our lack of\nunderstanding of what meta-learning algorithms are doing and when they work. In\nthis work, we provide empirical results that shed some light on how\nmeta-learned MAML representations function. In particular, we identify three\ninteresting properties: 1) In contrast to previous work, we show that it is\npossible to define a family of synthetic benchmarks that result in a low degree\nof feature re-use - suggesting that current few-shot learning benchmarks might\nnot have the properties needed for the success of meta-learning algorithms; 2)\nmeta-overfitting occurs when the number of classes (or concepts) are finite,\nand this issue disappears once the task has an unbounded number of concepts\n(e.g., online learning); 3) more adaptation at meta-test time with MAML does\nnot necessarily result in a significant representation change or even an\nimprovement in meta-test performance - even when training on our proposed\nsynthetic benchmarks. Finally, we suggest that to understand meta-learning\nalgorithms better, we must go beyond tracking only absolute performance and, in\naddition, formally quantify the degree of meta-learning and track both metrics\ntogether. Reporting results in future work this way will help us identify the\nsources of meta-overfitting more accurately and help us design more flexible\nmeta-learning algorithms that learn beyond fixed feature re-use. Finally, we\nconjecture the core challenge of re-thinking meta-learning is in the design of\nfew-shot learning data sets and benchmarks - rather than in the algorithms, as\nsuggested by previous work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1\">Brando Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Compact Building Models from Point Clouds Using Deep Implicit Fields. (arXiv:2112.13142v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13142","description":"<p>Three-dimensional (3D) building models play an increasingly pivotal role in\nmany real-world applications while obtaining a compact representation of\nbuildings remains an open problem. In this paper, we present a novel framework\nfor reconstructing compact, watertight, polygonal building models from point\nclouds. Our framework comprises three components: (a) a cell complex is\ngenerated via adaptive space partitioning that provides a polyhedral embedding\nas the candidate set; (b) an implicit field is learned by a deep neural network\nthat facilitates building occupancy estimation; (c) a Markov random field is\nformulated to extract the outer surface of a building via combinatorial\noptimization. We evaluate and compare our method with state-of-the-art methods\nin shape reconstruction, surface approximation, and geometry simplification.\nExperiments on both synthetic and real-world point clouds have demonstrated\nthat, with our neural-guided strategy, high-quality building models can be\nobtained with significant advantages in fidelity, compactness, and\ncomputational efficiency. Our method shows robustness to noise and insufficient\nmeasurements, and it can directly generalize from synthetic scans to real-world\nmeasurements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhaiyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khademi_S/0/1/0/all/0/1\">Seyran Khademi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ledoux_H/0/1/0/all/0/1\">Hugo Ledoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast and Scalable Computation of the Forward and Inverse Discrete Periodic Radon Transform. (arXiv:2112.13149v1 [cs.AR])","link":"http://arxiv.org/abs/2112.13149","description":"<p>The Discrete Periodic Radon Transform (DPRT) has been extensively used in\napplications that involve image reconstructions from projections. This\nmanuscript introduces a fast and scalable approach for computing the forward\nand inverse DPRT that is based on the use of: (i) a parallel array of\nfixed-point adder trees, (ii) circular shift registers to remove the need for\naccessing external memory components when selecting the input data for the\nadder trees, (iii) an image block-based approach to DPRT computation that can\nfit the proposed architecture to available resources, and (iv) fast\ntranspositions that are computed in one or a few clock cycles that do not\ndepend on the size of the input image. As a result, for an $N\\times N$ image\n($N$ prime), the proposed approach can compute up to $N^{2}$ additions per\nclock cycle. Compared to previous approaches, the scalable approach provides\nthe fastest known implementations for different amounts of computational\nresources. For example, for a $251\\times 251$ image, for approximately $25\\%$\nfewer flip-flops than required for a systolic implementation, we have that the\nscalable DPRT is computed 36 times faster. For the fastest case, we introduce\noptimized architectures that can compute the DPRT and its inverse in just\n$2N+\\left\\lceil \\log_{2}N\\right\\rceil+1$ and $2N+3\\left\\lceil\n\\log_{2}N\\right\\rceil+B+2$ cycles respectively, where $B$ is the number of bits\nused to represent each input pixel. On the other hand, the scalable DPRT\napproach requires more 1-bit additions than for the systolic implementation and\nprovides a trade-off between speed and additional 1-bit additions. All of the\nproposed DPRT architectures were implemented in VHDL and validated using an\nFPGA implementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carranza_C/0/1/0/all/0/1\">Cesar Carranza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llamocca_D/0/1/0/all/0/1\">Daniel Llamocca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattichis_M/0/1/0/all/0/1\">Marios Pattichis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fast 2D Convolutions and Cross-Correlations Using Scalable Architectures. (arXiv:2112.13150v1 [cs.AR])","link":"http://arxiv.org/abs/2112.13150","description":"<p>The manuscript describes fast and scalable architectures and associated\nalgorithms for computing convolutions and cross-correlations. The basic idea is\nto map 2D convolutions and cross-correlations to a collection of 1D\nconvolutions and cross-correlations in the transform domain. This is\naccomplished through the use of the Discrete Periodic Radon Transform (DPRT)\nfor general kernels and the use of SVD-LU decompositions for low-rank kernels.\nThe approach uses scalable architectures that can be fitted into modern FPGA\nand Zynq-SOC devices. Based on different types of available resources, for\n$P\\times P$ blocks, 2D convolutions and cross-correlations can be computed in\njust $O(P)$ clock cycles up to $O(P^2)$ clock cycles. Thus, there is a\ntrade-off between performance and required numbers and types of resources. We\nprovide implementations of the proposed architectures using modern programmable\ndevices (Virtex-7 and Zynq-SOC). Based on the amounts and types of required\nresources, we show that the proposed approaches significantly outperform\ncurrent methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carranza_C/0/1/0/all/0/1\">Cesar Carranza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llamocca_D/0/1/0/all/0/1\">Daniel Llamocca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pattichis_M/0/1/0/all/0/1\">Marios Pattichis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Clustering based Deduction Learning for Image Recognition and Classification. (arXiv:2112.13165v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13165","description":"<p>The paper proposes a semantic clustering based deduction learning by\nmimicking the learning and thinking process of human brains. Human beings can\nmake judgments based on experience and cognition, and as a result, no one would\nrecognize an unknown animal as a car. Inspired by this observation, we propose\nto train deep learning models using the clustering prior that can guide the\nmodels to learn with the ability of semantic deducing and summarizing from\nclassification attributes, such as a cat belonging to animals while a car\npertaining to vehicles. %Specifically, if an image is labeled as a cat, then\nthe model is trained to learn that \"this image is totally not any random class\nthat is the outlier of animal\". The proposed approach realizes the high-level\nclustering in the semantic space, enabling the model to deduce the relations\namong various classes during the learning process. In addition, the paper\nintroduces a semantic prior based random search for the opposite labels to\nensure the smooth distribution of the clustering and the robustness of the\nclassifiers. The proposed approach is supported theoretically and empirically\nthrough extensive experiments. We compare the performance across\nstate-of-the-art classifiers on popular benchmarks, and the generalization\nability is verified by adding noisy labeling to the datasets. Experimental\nresults demonstrate the superiority of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenchi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_X/0/1/0/all/0/1\">Xuemin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1\">Bo Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DSRGAN: Detail Prior-Assisted Perceptual Single Image Super-Resolution via Generative Adversarial Networks. (arXiv:2112.13191v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13191","description":"<p>The generative adversarial network (GAN) is successfully applied to study the\nperceptual single image superresolution (SISR). However, the GAN often tends to\ngenerate images with high frequency details being inconsistent with the real\nones. Inspired by conventional detail enhancement algorithms, we propose a\nnovel prior knowledge, the detail prior, to assist the GAN in alleviating this\nproblem and restoring more realistic details. The proposed method, named\nDSRGAN, includes a well designed detail extraction algorithm to capture the\nmost important high frequency information from images. Then, two discriminators\nare utilized for supervision on image-domain and detail-domain restorations,\nrespectively. The DSRGAN merges the restored detail into the final output via a\ndetail enhancement manner. The special design of DSRGAN takes advantages from\nboth the model-based conventional algorithm and the data-driven deep learning\nnetwork. Experimental results demonstrate that the DSRGAN outperforms the\nstate-of-the-art SISR methods on perceptual metrics and achieves comparable\nresults in terms of fidelity metrics simultaneously. Following the DSRGAN, it\nis feasible to incorporate other conventional image processing algorithms into\na deep learning network to form a model-based deep SISR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Ziyang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zhengguo Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1\">Xingming Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zhong Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1\">Weihai Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network-Aware 5G Edge Computing for Object Detection: Augmenting Wearables to \"See'' More, Farther and Faster. (arXiv:2112.13194v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13194","description":"<p>Advanced wearable devices are increasingly incorporating high-resolution\nmulti-camera systems. As state-of-the-art neural networks for processing the\nresulting image data are computationally demanding, there has been growing\ninterest in leveraging fifth generation (5G) wireless connectivity and mobile\nedge computing for offloading this processing to the cloud. To assess this\npossibility, this paper presents a detailed simulation and evaluation of 5G\nwireless offloading for object detection within a powerful, new smart wearable\ncalled VIS4ION, for the Blind-and-Visually Impaired (BVI). The current VIS4ION\nsystem is an instrumented book-bag with high-resolution cameras, vision\nprocessing and haptic and audio feedback. The paper considers uploading the\ncamera data to a mobile edge cloud to perform real-time object detection and\ntransmitting the detection results back to the wearable. To determine the video\nrequirements, the paper evaluates the impact of video bit rate and resolution\non object detection accuracy and range. A new street scene dataset with labeled\nobjects relevant to BVI navigation is leveraged for analysis. The vision\nevaluation is combined with a detailed full-stack wireless network simulation\nto determine the distribution of throughputs and delays with real navigation\npaths and ray-tracing from new high-resolution 3D models in an urban\nenvironment. For comparison, the wireless simulation considers both a standard\n4G-Long Term Evolution (LTE) carrier and high-rate 5G millimeter-wave (mmWave)\ncarrier. The work thus provides a thorough and realistic assessment of edge\ncomputing with mmWave connectivity in an application with both high bandwidth\nand low latency requirements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yuan_Z/0/1/0/all/0/1\">Zhongzheng Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Azzino_T/0/1/0/all/0/1\">Tommy Azzino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hao_Y/0/1/0/all/0/1\">Yu Hao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_Y/0/1/0/all/0/1\">Yixuan Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pei_H/0/1/0/all/0/1\">Haoyang Pei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Boldini_A/0/1/0/all/0/1\">Alain Boldini</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mezzavilla_M/0/1/0/all/0/1\">Marco Mezzavilla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beheshti_M/0/1/0/all/0/1\">Mahya Beheshti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Porfiri_M/0/1/0/all/0/1\">Maurizio Porfiri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hudson_T/0/1/0/all/0/1\">Todd Hudson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seiple_W/0/1/0/all/0/1\">William Seiple</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rangan_S/0/1/0/all/0/1\">Sundeep Rangan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rizzo_J/0/1/0/all/0/1\">J.R. Rizzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudocylindrical Convolutions for Learned Omnidirectional Image Compression. (arXiv:2112.13227v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13227","description":"<p>Although equirectangular projection (ERP) is a convenient form to store\nomnidirectional images (also known as 360-degree images), it is neither\nequal-area nor conformal, thus not friendly to subsequent visual communication.\nIn the context of image compression, ERP will over-sample and deform things and\nstuff near the poles, making it difficult for perceptually optimal bit\nallocation. In conventional 360-degree image compression, techniques such as\nregion-wise packing and tiled representation are introduced to alleviate the\nover-sampling problem, achieving limited success. In this paper, we make one of\nthe first attempts to learn deep neural networks for omnidirectional image\ncompression. We first describe parametric pseudocylindrical representation as a\ngeneralization of common pseudocylindrical map projections. A computationally\ntractable greedy method is presented to determine the (sub)-optimal\nconfiguration of the pseudocylindrical representation in terms of a novel proxy\nobjective for rate-distortion performance. We then propose pseudocylindrical\nconvolutions for 360-degree image compression. Under reasonable constraints on\nthe parametric representation, the pseudocylindrical convolution can be\nefficiently implemented by standard convolution with the so-called\npseudocylindrical padding. To demonstrate the feasibility of our idea, we\nimplement an end-to-end 360-degree image compression system, consisting of the\nlearned pseudocylindrical representation, an analysis transform, a non-uniform\nquantizer, a synthesis transform, and an entropy model. Experimental results on\n$19,790$ omnidirectional images show that our method achieves consistently\nbetter rate-distortion performance than the competing methods. Moreover, the\nvisual quality by our method is significantly improved for all images at all\nbitrates.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Li_M/0/1/0/all/0/1\">Mu Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evolutionary Generation of Visual Motion Illusions. (arXiv:2112.13243v1 [cs.NE])","link":"http://arxiv.org/abs/2112.13243","description":"<p>Why do we sometimes perceive static images as if they were moving? Visual\nmotion illusions enjoy a sustained popularity, yet there is no definitive\nanswer to the question of why they work. We present a generative model, the\nEvolutionary Illusion GENerator (EIGen), that creates new visual motion\nillusions. The structure of EIGen supports the hypothesis that illusory motion\nmight be the result of perceiving the brain's own predictions rather than\nperceiving raw visual input from the eyes. The scientific motivation of this\npaper is to demonstrate that the perception of illusory motion could be a side\neffect of the predictive abilities of the brain. The philosophical motivation\nof this paper is to call attention to the untapped potential of \"motivated\nfailures\", ways for artificial systems to fail as biological systems fail, as a\nworthy outlet for Artificial Intelligence and Artificial Life research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinapayen_L/0/1/0/all/0/1\">Lana Sinapayen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_E/0/1/0/all/0/1\">Eiji Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artifact Reduction in Fundus Imaging using Cycle Consistent Adversarial Neural Networks. (arXiv:2112.13264v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13264","description":"<p>Fundus images are very useful in identifying various ophthalmic disorders.\nHowever, due to the presence of artifacts, the visibility of the retina is\nseverely affected. This may result in misdiagnosis of the disorder which may\nlead to more complicated problems. Since deep learning is a powerful tool to\nextract patterns from data without much human intervention, they can be applied\nto image-to-image translation problems. An attempt has been made in this paper\nto automatically rectify such artifacts present in the images of the fundus. We\nuse a CycleGAN based model which consists of residual blocks to reduce the\nartifacts in the images. Significant improvements are seen when compared to the\nexisting techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+S_S/0/1/0/all/0/1\">Sai Koushik S S</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Srinivasa_K/0/1/0/all/0/1\">K.G. Srinivasa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Clustering Active Learning for Person Re-identification. (arXiv:2112.13308v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13308","description":"<p>Supervised person re-identification (re-id) approaches require a large amount\nof pairwise manual labeled data, which is not applicable in most real-world\nscenarios for re-id deployment. On the other hand, unsupervised re-id methods\nrely on unlabeled data to train models but performs poorly compared with\nsupervised re-id methods. In this work, we aim to combine unsupervised re-id\nlearning with a small number of human annotations to achieve a competitive\nperformance. Towards this goal, we present a Unsupervised Clustering Active\nLearning (UCAL) re-id deep learning approach. It is capable of incrementally\ndiscovering the representative centroid-pairs and requiring human annotate\nthem. These few labeled representative pairwise data can improve the\nunsupervised representation learning model with other large amounts of\nunlabeled data. More importantly, because the representative centroid-pairs are\nselected for annotation, UCAL can work with very low-cost human effort.\nExtensive experiments demonstrate the superiority of the proposed model over\nstate-of-the-art active learning methods on three re-id benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenjing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Minxian Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Cross-Scale Prediction for Efficient Neural Video Compression. (arXiv:2112.13309v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13309","description":"<p>In this paper, we present the first neural video codec that can compete with\nthe latest coding standard H.266/VVC in terms of sRGB PSNR on UVG dataset for\nthe low-latency mode. Existing neural hybrid video coding approaches rely on\noptical flow or Gaussian-scale flow for prediction, which cannot support\nfine-grained adaptation to diverse motion content. Towards more\ncontent-adaptive prediction, we propose a novel cross-scale prediction module\nthat achieves more effective motion compensation. Specifically, on the one\nhand, we produce a reference feature pyramid as prediction sources, then\ntransmit cross-scale flows that leverage the feature scale to control the\nprecision of prediction. On the other hand, we introduce the mechanism of\nweighted prediction into the scenario of prediction with a single reference\nframe, where cross-scale weight maps are transmitted to synthesize a fine\nprediction result. In addition to the cross-scale prediction module, we further\npropose a multi-stage quantization strategy, which improves the rate-distortion\nperformance with no extra computational penalty during inference. We show the\nencouraging performance of our efficient neural video codec (ENVC) on several\ncommon benchmark datasets and analyze in detail the effectiveness of every\nimportant component.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1\">Zongyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1\">Runsen Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jin_X/0/1/0/all/0/1\">Xin Jin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1\">Zhibo Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Miti-DETR: Object Detection based on Transformers with Mitigatory Self-Attention Convergence. (arXiv:2112.13310v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13310","description":"<p>Object Detection with Transformers (DETR) and related works reach or even\nsurpass the highly-optimized Faster-RCNN baseline with self-attention network\narchitectures. Inspired by the evidence that pure self-attention possesses a\nstrong inductive bias that leads to the transformer losing the expressive power\nwith respect to network depth, we propose a transformer architecture with a\nmitigatory self-attention mechanism by applying possible direct mapping\nconnections in the transformer architecture to mitigate the rank collapse so as\nto counteract feature expression loss and enhance the model performance. We\napply this proposal in object detection tasks and develop a model named\nMiti-DETR. Miti-DETR reserves the inputs of each single attention layer to the\noutputs of that layer so that the \"non-attention\" information has participated\nin any attention propagation. The formed residual self-attention network\naddresses two critical issues: (1) stop the self-attention networks from\ndegenerating to rank-1 to the maximized degree; and (2) further diversify the\npath distribution of parameter update so that easier attention learning is\nexpected. Miti-DETR significantly enhances the average detection precision and\nconvergence speed towards existing DETR-based models on the challenging COCO\nobject detection dataset. Moreover, the proposed transformer with the residual\nself-attention network can be easily generalized or plugged in other related\ntask models without specific customization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenchi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Offline Handwriting Recognition using Deep Learning Models. (arXiv:2112.13328v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13328","description":"<p>Handwritten text recognition is an open problem of great interest in the area\nof automatic document image analysis. The transcription of handwritten content\npresent in digitized documents is significant in analyzing historical archives\nor digitizing information from handwritten documents, forms, and\ncommunications. In the last years, great advances have been made in this area\ndue to applying deep learning techniques to its resolution. This Thesis\naddresses the offline continuous handwritten text recognition (HTR) problem,\nconsisting of developing algorithms and models capable of transcribing the text\npresent in an image without the need for the text to be segmented into\ncharacters. For this purpose, we have proposed a new recognition model based on\nintegrating two types of deep learning architectures: convolutional neural\nnetworks (CNN) and sequence-to-sequence (seq2seq) models, respectively. The\nconvolutional component of the model is oriented to identify relevant features\npresent in characters, and the seq2seq component builds the transcription of\nthe text by modeling the sequential nature of the text. For the design of this\nnew model, an extensive analysis of the capabilities of different convolutional\narchitectures in the simplified problem of isolated character recognition has\nbeen carried out in order to identify the most suitable ones to be integrated\ninto the continuous model. Additionally, extensive experimentation of the\nproposed model for the continuous problem has been carried out to determine its\nrobustness to changes in parameterization. The generalization capacity of the\nmodel has also been validated by evaluating it on three handwritten text\ndatabases using different languages: IAM in English, RIMES in French, and\nOsborne in Spanish, respectively. The new proposed model provides competitive\nresults with those obtained with other well-established methodologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sueiras_J/0/1/0/all/0/1\">Jorge Sueiras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It\\^{o}-Taylor Sampling Scheme for Denoising Diffusion Probabilistic Models using Ideal Derivatives. (arXiv:2112.13339v1 [stat.ML])","link":"http://arxiv.org/abs/2112.13339","description":"<p>Denoising Diffusion Probabilistic Models (DDPMs) have been attracting\nattention recently as a new challenger to popular deep neural generative models\nincluding GAN, VAE, etc. However, DDPMs have a disadvantage that they often\nrequire a huge number of refinement steps during the synthesis. To address this\nproblem, this paper proposes a new DDPM sampler based on a second-order\nnumerical scheme for stochastic differential equations (SDEs), while the\nconventional sampler is based on a first-order numerical scheme. In general, it\nis not easy to compute the derivatives that are required in higher-order\nnumerical schemes. However, in the case of DDPM, this difficulty is alleviated\nby the trick which the authors call \"ideal derivative substitution\". The newly\nderived higher-order sampler was applied to both image and speech generation\ntasks, and it is experimentally observed that the proposed sampler could\nsynthesize plausible images and audio signals in relatively smaller number of\nrefinement steps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Tachibana_H/0/1/0/all/0/1\">Hideyuki Tachibana</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Go_M/0/1/0/all/0/1\">Mocho Go</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Inahara_M/0/1/0/all/0/1\">Muneyoshi Inahara</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Katayama_Y/0/1/0/all/0/1\">Yotaro Katayama</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Watanabe_Y/0/1/0/all/0/1\">Yotaro Watanabe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AlertTrap: A study on object detection in remote insects trap monitoring system using on-the-edge deep learning platform. (arXiv:2112.13341v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13341","description":"<p>Fruit flies are one of the most harmful insect species to fruit yields. In\nAlertTrap, implementation of SSD architecture with different state-of-the-art\nbackbone feature extractors such as MobileNetV1 and MobileNetV2 appear to be\npotential solutions for the real-time detection problem. SSD-MobileNetV1 and\nSSD-MobileNetV2 perform well and result in AP@0.5 of 0.957 and 1.0\nrespectively. YOLOv4-tiny outperforms the SSD family with 1.0 in AP@0.5;\nhowever, its throughput velocity is slightly slower.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_A/0/1/0/all/0/1\">An D. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_D/0/1/0/all/0/1\">Duy A. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_D/0/1/0/all/0/1\">Dong T. Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Hien B. Vo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Delivery Issues Identification from Customer Feedback Data. (arXiv:2112.13372v1 [cs.CL])","link":"http://arxiv.org/abs/2112.13372","description":"<p>Millions of packages are delivered successfully by online and local retail\nstores across the world every day. The proper delivery of packages is needed to\nensure high customer satisfaction and repeat purchases. These deliveries suffer\nvarious problems despite the best efforts from the stores. These issues happen\nnot only due to the large volume and high demand for low turnaround time but\nalso due to mechanical operations and natural factors. These issues range from\nreceiving wrong items in the package to delayed shipment to damaged packages\nbecause of mishandling during transportation. Finding solutions to various\ndelivery issues faced by both sending and receiving parties plays a vital role\nin increasing the efficiency of the entire process. This paper shows how to\nfind these issues using customer feedback from the text comments and uploaded\nimages. We used transfer learning for both Text and Image models to minimize\nthe demand for thousands of labeled examples. The results show that the model\ncan find different issues. Furthermore, it can also be used for tasks like\nbottleneck identification, process improvement, automating refunds, etc.\nCompared with the existing process, the ensemble of text and image models\nproposed in this paper ensures the identification of several types of delivery\nissues, which is more suitable for the real-life scenarios of delivery of items\nin retail businesses. This method can supply a new idea of issue detection for\nthe delivery of packages in similar industries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chopra_A/0/1/0/all/0/1\">Ankush Chopra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_M/0/1/0/all/0/1\">Mahima Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Shubham Pandey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sinogram upsampling using Primal-Dual UNet for undersampled CT and radial MRI reconstruction. (arXiv:2112.13443v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13443","description":"<p>CT and MRI are two widely used clinical imaging modalities for non-invasive\ndiagnosis. However, both of these modalities come with certain problems. CT\nuses harmful ionising radiation, and MRI suffers from slow acquisition speed.\nBoth problems can be tackled by undersampling, such as sparse sampling.\nHowever, such undersampled data leads to lower resolution and introduces\nartefacts. Several techniques, including deep learning based methods, have been\nproposed to reconstruct such data. However, the undersampled reconstruction\nproblem for these two modalities was always considered as two different\nproblems and tackled separately by different research works. This paper\nproposes a unified solution for both sparse CT and undersampled radial MRI\nreconstruction, achieved by applying Fourier transform-based pre-processing on\nthe radial MRI and then reconstructing both modalities using sinogram\nupsampling combined with filtered back-projection. The Primal-Dual network is a\ndeep learning based method for reconstructing sparsely-sampled CT data. This\npaper introduces Primal-Dual UNet, which improves the Primal-Dual network in\nterms of accuracy and reconstruction speed. The proposed method resulted in an\naverage SSIM of 0.932 while performing sparse CT reconstruction for fan-beam\ngeometry with a sparsity level of 16, achieving a statistically significant\nimprovement over the previous model, which resulted in 0.919. Furthermore, the\nproposed model resulted in 0.903 and 0.957 average SSIM while reconstructing\nundersampled brain and abdominal MRI data with an acceleration factor of 16 -\nstatistically significant improvements over the original model, which resulted\nin 0.867 and 0.949. Finally, this paper shows that the proposed network not\nonly improves the overall image quality, but also improves the image quality\nfor the regions-of-interest; as well as generalises better in presence of a\nneedle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ernst_P/0/1/0/all/0/1\">Philipp Ernst</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1\">Soumick Chatterjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rose_G/0/1/0/all/0/1\">Georg Rose</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1\">Oliver Speck</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1\">Andreas N&#xfc;rnberger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PreDisM: Pre-Disaster Modelling With CNN Ensembles for At-Risk Communities. (arXiv:2112.13465v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13465","description":"<p>The machine learning community has recently had increased interest in the\nclimate and disaster damage domain due to a marked increased occurrences of\nnatural hazards (e.g., hurricanes, forest fires, floods, earthquakes). However,\nnot enough attention has been devoted to mitigating probable destruction from\nimpending natural hazards. We explore this crucial space by predicting\nbuilding-level damages on a before-the-fact basis that would allow state actors\nand non-governmental organizations to be best equipped with resource\ndistribution to minimize or preempt losses. We introduce PreDisM that employs\nan ensemble of ResNets and fully connected layers over decision trees to\ncapture image-level and meta-level information to accurately estimate weakness\nof man-made structures to disaster-occurrences. Our model performs well and is\nresponsive to tuning across types of disasters and highlights the space of\npreemptive hazard damage modelling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anand_V/0/1/0/all/0/1\">Vishal Anand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miura_Y/0/1/0/all/0/1\">Yuki Miura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Video Joint Modelling Based on Hierarchical Transformer for Co-summarization. (arXiv:2112.13478v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13478","description":"<p>Video summarization aims to automatically generate a summary (storyboard or\nvideo skim) of a video, which can facilitate large-scale video retrieving and\nbrowsing. Most of the existing methods perform video summarization on\nindividual videos, which neglects the correlations among similar videos. Such\ncorrelations, however, are also informative for video understanding and video\nsummarization. To address this limitation, we propose Video Joint Modelling\nbased on Hierarchical Transformer (VJMHT) for co-summarization, which takes\ninto consideration the semantic dependencies across videos. Specifically, VJMHT\nconsists of two layers of Transformer: the first layer extracts semantic\nrepresentation from individual shots of similar videos, while the second layer\nperforms shot-level video joint modelling to aggregate cross-video semantic\ninformation. By this means, complete cross-video high-level patterns are\nexplicitly modelled and learned for the summarization of individual videos.\nMoreover, Transformer-based video representation reconstruction is introduced\nto maximize the high-level similarity between the summary and the original\nvideo. Extensive experiments are conducted to verify the effectiveness of the\nproposed modules and the superiority of VJMHT in terms of F-measure and\nrank-based evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haopeng_L/0/1/0/all/0/1\">Li Haopeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiuhong_K/0/1/0/all/0/1\">Ke Qiuhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mingming_G/0/1/0/all/0/1\">Gong Mingming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rui_Z/0/1/0/all/0/1\">Zhang Rui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Compact Neural Network-based Algorithm for Robust Image Watermarking. (arXiv:2112.13491v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13491","description":"<p>Digital image watermarking seeks to protect the digital media information\nfrom unauthorized access, where the message is embedded into the digital image\nand extracted from it, even some noises or distortions are applied under\nvarious data processing including lossy image compression and interactive\ncontent editing. Traditional image watermarking solutions easily suffer from\nrobustness when specified with some prior constraints, while recent deep\nlearning-based watermarking methods could not tackle the information loss\nproblem well under various separate pipelines of feature encoder and decoder.\nIn this paper, we propose a novel digital image watermarking solution with a\ncompact neural network, named Invertible Watermarking Network (IWN). Our IWN\narchitecture is based on a single Invertible Neural Network (INN), this\nbijective propagation framework enables us to effectively solve the challenge\nof message embedding and extraction simultaneously, by taking them as a pair of\ninverse problems for each other and learning a stable invertible mapping. In\norder to enhance the robustness of our watermarking solution, we specifically\nintroduce a simple but effective bit message normalization module to condense\nthe bit message to be embedded, and a noise layer is designed to simulate\nvarious practical attacks under our IWN framework. Extensive experiments\ndemonstrate the superiority of our solution under various distortions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hong-Bo Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Rong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jia Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shao-Ping Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Transformer for Small-Size Datasets. (arXiv:2112.13492v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13492","description":"<p>Recently, the Vision Transformer (ViT), which applied the transformer\nstructure to the image classification task, has outperformed convolutional\nneural networks. However, the high performance of the ViT results from\npre-training using a large-size dataset such as JFT-300M, and its dependence on\na large dataset is interpreted as due to low locality inductive bias. This\npaper proposes Shifted Patch Tokenization (SPT) and Locality Self-Attention\n(LSA), which effectively solve the lack of locality inductive bias and enable\nit to learn from scratch even on small-size datasets. Moreover, SPT and LSA are\ngeneric and effective add-on modules that are easily applicable to various\nViTs. Experimental results show that when both SPT and LSA were applied to the\nViTs, the performance improved by an average of 2.96% in Tiny-ImageNet, which\nis a representative small-size dataset. Especially, Swin Transformer achieved\nan overwhelming performance improvement of 4.08% thanks to the proposed SPT and\nLSA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seung Hoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Byung Cheol Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Estimating Parameters of the Tree Root in Heterogeneous Soil Environments via Mask-Guided Multi-Polarimetric Integration Neural Network. (arXiv:2112.13494v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13494","description":"<p>Ground-penetrating radar (GPR) has been used as a non-destructive tool for\ntree root inspection. Estimating root-related parameters from GPR radargrams\ngreatly facilitates root health monitoring and imaging. However, the task of\nestimating root-related parameters is challenging as the root reflection is a\ncomplex function of multiple root parameters and root orientations. Existing\nmethods can only estimate a single root parameter at a time without considering\nthe influence of other parameters and root orientations, resulting in limited\nestimation accuracy under different root conditions. In addition, soil\nheterogeneity introduces clutter in GPR radargrams, making the data processing\nand interpretation even harder. To address these issues, a novel neural network\narchitecture, called mask-guided multi-polarimetric integration neural network\n(MMI-Net), is proposed to automatically and simultaneously estimate multiple\nroot-related parameters in heterogeneous soil environments. The MMI-Net\nincludes two sub-networks: a MaskNet that predicts a mask to highlight the root\nreflection area to eliminate interfering environmental clutter, and a ParaNet\nthat uses the predicted mask as guidance to integrate, extract, and emphasize\ninformative features in multi-polarimetric radargrams for accurate estimation\nof five key root-related parameters. The parameters include the root depth,\ndiameter, relative permittivity, horizontal and vertical orientation angles.\nExperimental results demonstrate that the proposed MMI-Net achieves high\nestimation accuracy in these root-related parameters. This is the first work\nthat takes the combined contributions of root parameters and spatial\norientations into account and simultaneously estimates multiple root-related\nparameters. The data and code implemented in the paper can be found at\nhttps://haihan-sun.github.io/GPR.html.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hai-Han Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yee Hui Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1\">Qiqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chongyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ow_G/0/1/0/all/0/1\">Genevieve Ow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yusof_M/0/1/0/all/0/1\">Mohamed Lokman Mohd Yusof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yucel_A/0/1/0/all/0/1\">Abdulkadir C. Yucel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MSHT: Multi-stage Hybrid Transformer for the ROSE Image Analysis of Pancreatic Cancer. (arXiv:2112.13513v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13513","description":"<p>Pancreatic cancer is one of the most malignant cancers in the world, which\ndeteriorates rapidly with very high mortality. The rapid on-site evaluation\n(ROSE) technique innovates the workflow by immediately analyzing the fast\nstained cytopathological images with on-site pathologists, which enables faster\ndiagnosis in this time-pressured process. However, the wider expansion of ROSE\ndiagnosis has been hindered by the lack of experienced pathologists. To\novercome this problem, we propose a hybrid high-performance deep learning model\nto enable the automated workflow, thus freeing the occupation of the valuable\ntime of pathologists. By firstly introducing the Transformer block into this\nfield with our particular multi-stage hybrid design, the spatial features\ngenerated by the convolutional neural network (CNN) significantly enhance the\nTransformer global modeling. Turning multi-stage spatial features as global\nattention guidance, this design combines the robustness from the inductive bias\nof CNN with the sophisticated global modeling power of Transformer. A dataset\nof 4240 ROSE images is collected to evaluate the method in this unexplored\nfield. The proposed multi-stage hybrid Transformer (MSHT) achieves 95.68% in\nclassification accuracy, which is distinctively higher than the\nstate-of-the-art models. Facing the need for interpretability, MSHT outperforms\nits counterparts with more accurate attention regions. The results demonstrate\nthat the MSHT can distinguish cancer samples accurately at an unprecedented\nimage scale, laying the foundation for deploying automatic decision systems and\nenabling the expansion of ROSE in clinical practice. The code and records are\navailable at: https://github.com/sagizty/Multi-Stage-Hybrid-Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tianyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1\">Yunlu Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_G/0/1/0/all/0/1\">Guangda Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_A/0/1/0/all/0/1\">Aiming Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lyu_S/0/1/0/all/0/1\">Shangqin Lyu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_P/0/1/0/all/0/1\">Peng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_F/0/1/0/all/0/1\">Fan Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_C/0/1/0/all/0/1\">Chenbin Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1\">Yangyang Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1\">Youdan Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_G/0/1/0/all/0/1\">Guanglei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Contrastive Learning for General Face Forgery Detection. (arXiv:2112.13522v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13522","description":"<p>With various facial manipulation techniques arising, face forgery detection\nhas drawn growing attention due to security concerns. Previous works always\nformulate face forgery detection as a classification problem based on\ncross-entropy loss, which emphasizes category-level differences rather than the\nessential discrepancies between real and fake faces, limiting model\ngeneralization in unseen domains. To address this issue, we propose a novel\nface forgery detection framework, named Dual Contrastive Learning (DCL), which\nspecially constructs positive and negative paired data and performs designed\ncontrastive learning at different granularities to learn generalized feature\nrepresentation. Concretely, combined with the hard sample selection strategy,\nInter-Instance Contrastive Learning (Inter-ICL) is first proposed to promote\ntask-related discriminative features learning by especially constructing\ninstance pairs. Moreover, to further explore the essential discrepancies,\nIntra-Instance Contrastive Learning (Intra-ICL) is introduced to focus on the\nlocal content inconsistencies prevalent in the forged faces by constructing\nlocal-region pairs inside instances. Extensive experiments and visualizations\non several datasets demonstrate the generalization of our method against the\nstate-of-the-art competitors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+L_J/0/1/0/all/0/1\">Jilin L</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Generative Vision Transformer with Energy-Based Latent Space for Saliency Prediction. (arXiv:2112.13528v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13528","description":"<p>Vision transformer networks have shown superiority in many computer vision\ntasks. In this paper, we take a step further by proposing a novel generative\nvision transformer with latent variables following an informative energy-based\nprior for salient object detection. Both the vision transformer network and the\nenergy-based prior model are jointly trained via Markov chain Monte Carlo-based\nmaximum likelihood estimation, in which the sampling from the intractable\nposterior and prior distributions of the latent variables are performed by\nLangevin dynamics. Further, with the generative vision transformer, we can\neasily obtain a pixel-wise uncertainty map from an image, which indicates the\nmodel confidence in predicting saliency from the image. Different from the\nexisting generative models which define the prior distribution of the latent\nvariables as a simple isotropic Gaussian distribution, our model uses an\nenergy-based informative prior which can be more expressive to capture the\nlatent space of the data. We apply the proposed framework to both RGB and RGB-D\nsalient object detection tasks. Extensive experimental results show that our\nframework can achieve not only accurate saliency predictions but also\nmeaningful uncertainty maps that are consistent with the human perception.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Ping Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Attack for Asynchronous Event-based Data. (arXiv:2112.13534v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13534","description":"<p>Deep neural networks (DNNs) are vulnerable to adversarial examples that are\ncarefully designed to cause the deep learning model to make mistakes.\nAdversarial examples of 2D images and 3D point clouds have been extensively\nstudied, but studies on event-based data are limited. Event-based data can be\nan alternative to a 2D image under high-speed movements, such as autonomous\ndriving. However, the given adversarial events make the current deep learning\nmodel vulnerable to safety issues. In this work, we generate adversarial\nexamples and then train the robust models for event-based data, for the first\ntime. Our algorithm shifts the time of the original events and generates\nadditional adversarial events. Additional adversarial events are generated in\ntwo stages. First, null events are added to the event-based data to generate\nadditional adversarial events. The perturbation size can be controlled with the\nnumber of null events. Second, the location and time of additional adversarial\nevents are set to mislead DNNs in a gradient-based attack. Our algorithm\nachieves an attack success rate of 97.95\\% on the N-Caltech101 dataset.\nFurthermore, the adversarial training model improves robustness on the\nadversarial event data compared to the original model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1\">Hyun Myung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Meta-Learned Feature Critics for Domain Generalized Semantic Segmentation. (arXiv:2112.13538v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13538","description":"<p>How to handle domain shifts when recognizing or segmenting visual data across\ndomains has been studied by learning and vision communities. In this paper, we\naddress domain generalized semantic segmentation, in which the segmentation\nmodel is trained on multiple source domains and is expected to generalize to\nunseen data domains. We propose a novel meta-learning scheme with feature\ndisentanglement ability, which derives domain-invariant features for semantic\nsegmentation with domain generalization guarantees. In particular, we introduce\na class-specific feature critic module in our framework, enforcing the\ndisentangled visual features with domain generalization guarantees. Finally,\nour quantitative results on benchmark datasets confirm the effectiveness and\nrobustness of our proposed model, performing favorably against state-of-the-art\ndomain adaptation and generalization methods in segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shiau_Z/0/1/0/all/0/1\">Zu-Yun Shiau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Ci-Siang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chiang Frank Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Classification in Unseen Domains by Episodic Meta-Learning Across Visual Domains. (arXiv:2112.13539v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13539","description":"<p>Few-shot classification aims to carry out classification given only few\nlabeled examples for the categories of interest. Though several approaches have\nbeen proposed, most existing few-shot learning (FSL) models assume that base\nand novel classes are drawn from the same data domain. When it comes to\nrecognizing novel-class data in an unseen domain, this becomes an even more\nchallenging task of domain generalized few-shot classification. In this paper,\nwe present a unique learning framework for domain-generalized few-shot\nclassification, where base classes are from homogeneous multiple source\ndomains, while novel classes to be recognized are from target domains which are\nnot seen during training. By advancing meta-learning strategies, our learning\nframework exploits data across multiple source domains to capture\ndomain-invariant features, with FSL ability introduced by metric-learning based\nmechanisms across support and query data. We conduct extensive experiments to\nverify the effectiveness of our proposed learning framework and show learning\nfrom small yet homogeneous source data is able to perform preferably against\nlearning from large-scale one. Moreover, we provide insights into choices of\nbackbone models for domain-generalized few-shot classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuan-Chia Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Ci-Siang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fu-En Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Chiang Frank Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Edge Restoring Filter. (arXiv:2112.13540v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13540","description":"<p>In computer vision, image processing and computer graphics, image smoothing\nfiltering is a very basic and important task and to be expected possessing good\nedge-preserving smoothing property. Here we address the problem that the\nedge-preserving ability of many popular local smoothing filters needs to be\nimproved. In this paper, we propose the image Edge Restoring Filter (ERF) to\nrestore the blur edge pixels in the output of local smoothing filters to be\nclear. The proposed filter can been implemented after many local smoothing\nfilter (such as Box filter, Gaussian filter, Bilateral Filter, Guided Filter\nand so on). The combinations of \"original local smoothing filters + ERF\" have\nbetter edge-preserving smoothing property than the original local smoothing\nfilters. Experiments on image smoothing, image denoising and image enhancement\ndemonstrate the excellent edges restoring ability of the proposed filter and\ngood edgepreserving smoothing property of the combination \"original local\nsmoothing filters + ERF\". The proposed filter would benefit a great variety of\napplications given that smoothing filtering is a high frequently used and\nfundamental operation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViR:the Vision Reservoir. (arXiv:2112.13545v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13545","description":"<p>The most recent year has witnessed the success of applying the Vision\nTransformer (ViT) for image classification. However, there are still evidences\nindicating that ViT often suffers following two aspects, i) the high\ncomputation and the memory burden from applying the multiple Transformer layers\nfor pre-training on a large-scale dataset, ii) the over-fitting when training\non small datasets from scratch. To address these problems, a novel method,\nnamely, Vision Reservoir computing (ViR), is proposed here for image\nclassification, as a parallel to ViT. By splitting each image into a sequence\nof tokens with fixed length, the ViR constructs a pure reservoir with a nearly\nfully connected topology to replace the Transformer module in ViT. Two kinds of\ndeep ViR models are subsequently proposed to enhance the network performance.\nComparative experiments between the ViR and the ViT are carried out on several\nimage classification benchmarks. Without any pre-training process, the ViR\noutperforms the ViT in terms of both model and computational complexity.\nSpecifically, the number of parameters of the ViR is about 15% even 5% of the\nViT, and the memory footprint is about 20% to 40% of the ViT. The superiority\nof the ViR performance is explained by Small-World characteristics, Lyapunov\nexponents, and memory capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiehuang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PRIME: A Few Primitives Can Boost Robustness to Common Corruptions. (arXiv:2112.13547v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13547","description":"<p>Despite their impressive performance on image classification tasks, deep\nnetworks have a hard time generalizing to many common corruptions of their\ndata. To fix this vulnerability, prior works have mostly focused on increasing\nthe complexity of their training pipelines, combining multiple methods, in the\nname of diversity. However, in this work, we take a step back and follow a\nprincipled approach to achieve robustness to common corruptions. We propose\nPRIME, a general data augmentation scheme that consists of simple families of\nmax-entropy image transformations. We show that PRIME outperforms the prior art\nfor corruption robustness, while its simplicity and plug-and-play nature\nenables it to be combined with other methods to further boost their robustness.\nFurthermore, we analyze PRIME to shed light on the importance of the mixing\nstrategy on synthesizing corrupted images, and to reveal the\nrobustness-accuracy trade-offs arising in the context of common corruptions.\nFinally, we show that the computational efficiency of our method allows it to\nbe easily used in both on-line and off-line data augmentation schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modas_A/0/1/0/all/0/1\">Apostolos Modas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1\">Rahul Rade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1\">Guillermo Ortiz-Jim&#xe9;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosavi_Dezfooli_S/0/1/0/all/0/1\">Seyed-Mohsen Moosavi-Dezfooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1\">Pascal Frossard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Responsive Listening Head Generation: A Benchmark Dataset and Baseline. (arXiv:2112.13548v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13548","description":"<p>Responsive listening during face-to-face conversations is a critical element\nof social interaction and is well established in psychological research.\nThrough non-verbal signals response to the speakers' words, intonations, or\nbehaviors in real-time, listeners show how they are engaged in dialogue. In\nthis work, we build the Responsive Listener Dataset (RLD), a conversation video\ncorpus collected from the public resources featuring 67 speakers, 76 listeners\nwith three different attitudes. We define the responsive listening head\ngeneration task as the synthesis of a non-verbal head with motions and\nexpressions reacting to the multiple inputs, including the audio and visual\nsignal of the speaker. Unlike speech-driven gesture or talking head generation,\nwe introduce more modals in this task, hoping to benefit several research\nfields, including human-to-human interaction, video-to-video translation,\ncross-modal understanding, and generation. Furthermore, we release an attitude\nconditioned listening head generation baseline. Project page:\n\\url{https://project.mhzhou.com/rld}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mohan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yalong Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiejun Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Robust and Lightweight Model through Separable Structured Transformations. (arXiv:2112.13551v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13551","description":"<p>With the proliferation of mobile devices and the Internet of Things, deep\nlearning models are increasingly deployed on devices with limited computing\nresources and memory, and are exposed to the threat of adversarial noise.\nLearning deep models with both lightweight and robustness is necessary for\nthese equipments. However, current deep learning solutions are difficult to\nlearn a model that possesses these two properties without degrading one or the\nother. As is well known, the fully-connected layers contribute most of the\nparameters of convolutional neural networks. We perform a separable structural\ntransformation of the fully-connected layer to reduce the parameters, where the\nlarge-scale weight matrix of the fully-connected layer is decoupled by the\ntensor product of several separable small-sized matrices. Note that data, such\nas images, no longer need to be flattened before being fed to the\nfully-connected layer, retaining the valuable spatial geometric information of\nthe data. Moreover, in order to further enhance both lightweight and\nrobustness, we propose a joint constraint of sparsity and differentiable\ncondition number, which is imposed on these separable matrices. We evaluate the\nproposed approach on MLP, VGG-16 and Vision Transformer. The experimental\nresults on datasets such as ImageNet, SVHN, CIFAR-100 and CIFAR10 show that we\nsuccessfully reduce the amount of network parameters by 90%, while the robust\naccuracy loss is less than 1.5%, which is better than the SOTA methods based on\nthe original fully-connected layer. Interestingly, it can achieve an\noverwhelming advantage even at a high compression rate, e.g., 200 times.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yanhui Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yangyu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of Histopathology Images of Lung Cancer Using Convolutional Neural Network (CNN). (arXiv:2112.13553v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13553","description":"<p>Cancer is the uncontrollable cell division of abnormal cells inside the human\nbody, which can spread to other body organs. It is one of the non-communicable\ndiseases (NCDs) and NCDs accounts for 71% of total deaths worldwide whereas\nlung cancer is the second most diagnosed cancer after female breast cancer.\nCancer survival rate of lung cancer is only 19%. There are various methods for\nthe diagnosis of lung cancer, such as X-ray, CT scan, PET-CT scan, bronchoscopy\nand biopsy. However, to know the subtype of lung cancer based on the tissue\ntype H and E staining is widely used, where the staining is done on the tissue\naspirated from a biopsy. Studies have reported that the type of histology is\nassociated with prognosis and treatment in lung cancer. Therefore, early and\naccurate detection of lung cancer histology is an urgent need and as its\ntreatment is dependent on the type of histology, molecular profile and stage of\nthe disease, it is most essential to analyse the histopathology images of lung\ncancer. Hence, to speed up the vital process of diagnosis of lung cancer and\nreduce the burden on pathologists, Deep learning techniques are used. These\ntechniques have shown improved efficacy in the analysis of histopathology\nslides of cancer. Several studies reported the importance of convolution neural\nnetworks (CNN) in the classification of histopathological pictures of various\ncancer types such as brain, skin, breast, lung, colorectal cancer. In this\nstudy tri-category classification of lung cancer images (normal, adenocarcinoma\nand squamous cell carcinoma) are carried out by using ResNet 50, VGG-19,\nInception_ResNet_V2 and DenseNet for the feature extraction and triplet loss to\nguide the CNN such that it increases inter-cluster distance and reduces\nintra-cluster distance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Baranwal_N/0/1/0/all/0/1\">Neha Baranwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Doravari_P/0/1/0/all/0/1\">Preethi Doravari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kachhoria_R/0/1/0/all/0/1\">Renu Kachhoria</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAM-AL: Dilated Attention Mechanism with Attention Loss for 3D Infant Brain Image Segmentation. (arXiv:2112.13559v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13559","description":"<p>While Magnetic Resonance Imaging (MRI) has played an essential role in infant\nbrain analysis, segmenting MRI into a number of tissues such as gray matter\n(GM), white matter (WM), and cerebrospinal fluid (CSF) is crucial and complex\ndue to the extremely low intensity contrast between tissues at around 6-9\nmonths of age as well as amplified noise, myelination, and incomplete volume.\nIn this paper, we tackle those limitations by developing a new deep learning\nmodel, named DAM-AL, which contains two main contributions, i.e., dilated\nattention mechanism and hard-case attention loss. Our DAM-AL network is\ndesigned with skip block layers and atrous block convolution. It contains both\nchannel-wise attention at high-level context features and spatial attention at\nlow-level spatial structural features. Our attention loss consists of two terms\ncorresponding to region information and hard samples attention. Our proposed\nDAM-AL has been evaluated on the infant brain iSeg 2017 dataset and the\nexperiments have been conducted on both validation and testing sets. We have\nbenchmarked DAM-AL on Dice coefficient and ASD metrics and compared it with\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hoang_D/0/1/0/all/0/1\">Dinh-Hieu Hoang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Diep_G/0/1/0/all/0/1\">Gia-Han Diep</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1\">Minh-Triet Tran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1\">Ngan T.H Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hard Example Guided Hashing for Image Retrieval. (arXiv:2112.13565v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13565","description":"<p>Compared with the traditional hashing methods, deep hashing methods generate\nhash codes with rich semantic information and greatly improves the performances\nin the image retrieval field. However, it is unsatisfied for current deep\nhashing methods to predict the similarity of hard examples. It exists two main\nfactors affecting the ability of learning hard examples, which are weak key\nfeatures extraction and the shortage of hard examples. In this paper, we give a\nnovel end-to-end model to extract the key feature from hard examples and obtain\nhash code with the accurate semantic information. In addition, we redesign a\nhard pair-wise loss function to assess the hard degree and update penalty\nweights of examples. It effectively alleviates the shortage problem in hard\nexamples. Experimental results on CIFAR-10 and NUS-WIDE demonstrate that our\nmodel outperformances the mainstream hashing-based image retrieval methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hai Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1\">Meiyin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junle Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Songsen Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vegetation Stratum Occupancy Prediction from Airborne LiDAR 3D Point Clouds. (arXiv:2112.13583v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13583","description":"<p>We propose a new deep learning-based method for estimating the occupancy of\nvegetation strata from 3D point clouds captured from an aerial platform. Our\nmodel predicts rasterized occupancy maps for three vegetation strata: lower,\nmedium, and higher strata. Our training scheme allows our network to only being\nsupervized with values aggregated over cylindrical plots, which are easier to\nproduce than pixel-wise or point-wise annotations. Our method outperforms\nhandcrafted and deep learning baselines in terms of precision while\nsimultaneously providing visual and interpretable predictions. We provide an\nopen-source implementation of our method along along a dataset of 199\nagricultural plots to train and evaluate occupancy regression algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalinicheva_E/0/1/0/all/0/1\">Ekaterina Kalinicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Landrieu_L/0/1/0/all/0/1\">Loic Landrieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mallet_C/0/1/0/all/0/1\">Cl&#xe9;ment Mallet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chehata_N/0/1/0/all/0/1\">Nesrine Chehata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Image Synthesis and Editing: A Survey. (arXiv:2112.13592v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13592","description":"<p>As information exists in various modalities in real world, effective\ninteraction and fusion among multimodal information plays a key role for the\ncreation and perception of multimodal data in computer vision and deep learning\nresearch. With superb power in modelling the interaction among multimodal\ninformation, multimodal image synthesis and editing have become a hot research\ntopic in recent years. Different from traditional visual guidance which\nprovides explicit clues, multimodal guidance offers intuitive and flexible\nmeans in image synthesis and editing. On the other hand, this field is also\nfacing several challenges in alignment of features with inherent modality gaps,\nsynthesis of high-resolution images, faithful evaluation metrics, etc. In this\nsurvey, we comprehensively contextualize the advance of the recent multimodal\nimage synthesis \\&amp; editing and formulate taxonomies according to data modality\nand model architectures. We start with an introduction to different types of\nguidance modalities in image synthesis and editing. We then describe multimodal\nimage synthesis and editing approaches extensively with detailed frameworks\nincluding Generative Adversarial Networks (GANs), GAN Inversion, Transformers,\nand other methods such as NeRF and Diffusion models. This is followed by a\ncomprehensive description of benchmark datasets and corresponding evaluation\nmetrics as widely adopted in multimodal image synthesis and editing, as well as\ndetailed comparisons of different synthesis methods with analysis of respective\nadvantages and limitations. Finally, we provide insights into the current\nresearch challenges and possible future research directions. A project\nassociated with this survey is available at https://github.com/fnzhan/MISE\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Rongliang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Depth estimation of endoscopy using sim-to-real transfer. (arXiv:2112.13595v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13595","description":"<p>In order to use the navigation system effectively, distance information\nsensors such as depth sensors are essential. Since depth sensors are difficult\nto use in endoscopy, many groups propose a method using convolutional neural\nnetworks. In this paper, the ground truth of the depth image and the endoscopy\nimage is generated through endoscopy simulation using the colon model segmented\nby CT colonography. Photo-realistic simulation images can be created using a\nsim-to-real approach using cycleGAN for endoscopy images. By training the\ngenerated dataset, we propose a quantitative endoscopy depth estimation\nnetwork. The proposed method represents a better-evaluated score than the\nexisting unsupervised training-based results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jeong_B/0/1/0/all/0/1\">Bong Hyuk Jeong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_H/0/1/0/all/0/1\">Hang Keun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Son_Y/0/1/0/all/0/1\">Young Don Son</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Study of Adder Neural Networks for Object Detection. (arXiv:2112.13608v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13608","description":"<p>Adder neural networks (AdderNets) have shown impressive performance on image\nclassification with only addition operations, which are more energy efficient\nthan traditional convolutional neural networks built with multiplications.\nCompared with classification, there is a strong demand on reducing the energy\nconsumption of modern object detectors via AdderNets for real-world\napplications such as autonomous driving and face detection. In this paper, we\npresent an empirical study of AdderNets for object detection. We first reveal\nthat the batch normalization statistics in the pre-trained adder backbone\nshould not be frozen, since the relatively large feature variance of AdderNets.\nMoreover, we insert more shortcut connections in the neck part and design a new\nfeature fusion architecture for avoiding the sparse features of adder layers.\nWe present extensive ablation studies to explore several design choices of\nadder detectors. Comparisons with state-of-the-arts are conducted on COCO and\nPASCAL VOC benchmarks. Specifically, the proposed Adder FCOS achieves a 37.8\\%\nAP on the COCO val set, demonstrating comparable performance to that of the\nconvolutional counterpart with an about $1.4\\times$ energy reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Minjing Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generation of Synthetic Rat Brain MRI scans with a 3D Enhanced Alpha-GAN. (arXiv:2112.13626v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13626","description":"<p>Translational brain research using Magnetic Resonance Imaging (MRI) is\nbecoming increasingly popular as animal models are an essential part of\nscientific studies and ultra-high-field scanners become more available. Some\ndrawbacks of MRI are MRI scanner availability, and the time needed to perform a\nfull scanning session (it usually takes over 30 minutes). Data protection laws\nand 3R ethical rule also make it difficult to create large data sets for\ntraining Deep Learning models. Generative Adversarial Networks (GAN) have been\nshown capable of performing data augmentation with higher quality than other\ntechniques. In this work, the alpha-GAN architecture is used to test its\nability to generate realistic 3D MRI scans of the rat brain. As far as the\nauthors are aware, this is the first time an approach based on GANs is used for\ndata augmentation in preclinical data. The generated scans are evaluated using\nvarious qualitative and quantitative metrics. A Turing test performed by 4\nexperts has shown that the generated scans can trick almost any expert. The\ngenerated scans were also used to evaluate their impact on the performance of\nan existing deep learning model developed for rat brain segmentation of white\nmatter, grey matter, and cerebrospinal fluid. The models were compared using\nthe Dice score. The best results for the segmentation of whole brain and white\nmatter were achieved when 174 real scans and 348 synthetic ones were used, with\nimprovements of 0.0172 and 0.0129. The use of 174 real scans and 87 synthetic\nones led to improvements of 0.0038 and 0.0764 of grey matter and cerebrospinal\nfluid segmentation. Thus, by using the proposed new normalisation layer and\nloss functions, it was possible to improve the realism of the generated rat MRI\nscans and it was demonstrated that using the data generated improved the\nsegmentation model more than using conventional data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ferreira_A/0/1/0/all/0/1\">Andr&#xe9; Ferreira</a> (1), <a href=\"http://arxiv.org/find/eess/1/au:+Magalhaes_R/0/1/0/all/0/1\">Ricardo Magalh&#xe3;es</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Meriaux_S/0/1/0/all/0/1\">S&#xe9;bastien M&#xe9;riaux</a> (2), <a href=\"http://arxiv.org/find/eess/1/au:+Alves_V/0/1/0/all/0/1\">Victor Alves</a> (1) ((1) Centro Algoritmi, University of Minho, Braga, Portugal, (2) Universit&#xe9; Paris-Saclay, CEA, CNRS, BAOBAB, NeuroSpin, Gif-sur-Yvette, France)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaptivePose: Human Parts as Adaptive Points. (arXiv:2112.13635v1 [cs.CV])","link":"http://arxiv.org/abs/2112.13635","description":"<p>Multi-person pose estimation methods generally follow top-down and bottom-up\nparadigms, both of which can be considered as two-stage approaches thus leading\nto the high computation cost and low efficiency. Towards a compact and\nefficient pipeline for multi-person pose estimation task, in this paper, we\npropose to represent the human parts as points and present a novel body\nrepresentation, which leverages an adaptive point set including the human\ncenter and seven human-part related points to represent the human instance in a\nmore fine-grained manner. The novel representation is more capable of capturing\nthe various pose deformation and adaptively factorizes the long-range\ncenter-to-joint displacement thus delivers a single-stage differentiable\nnetwork to more precisely regress multi-person pose, termed as AdaptivePose.\nFor inference, our proposed network eliminates the grouping as well as\nrefinements and only needs a single-step disentangling process to form\nmulti-person pose. Without any bells and whistles, we achieve the best\nspeed-accuracy trade-offs of 67.4% AP / 29.4 fps with DLA-34 and 71.3% AP / 9.1\nfps with HRNet-W48 on COCO test-dev dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yabo Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dongdong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingshu He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-normalized Classification of Parkinson's Disease DaTscan Images. (arXiv:2112.13637v1 [eess.IV])","link":"http://arxiv.org/abs/2112.13637","description":"<p>Classifying SPECT images requires a preprocessing step which normalizes the\nimages using a normalization region. The choice of the normalization region is\nnot standard, and using different normalization regions introduces\nnormalization region-dependent variability. This paper mathematically analyzes\nthe effect of the normalization region to show that normalized-classification\nis exactly equivalent to a subspace separation of the half rays of the images\nunder multiplicative equivalence. Using this geometry, a new self-normalized\nclassification strategy is proposed. This strategy eliminates the normalizing\nregion altogether. The theory is used to classify DaTscan images of 365\nParkinson's disease (PD) subjects and 208 healthy control (HC) subjects from\nthe Parkinson's Progression Marker Initiative (PPMI). The theory is also used\nto understand PD progression from baseline to year 4.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tagare_H/0/1/0/all/0/1\">Hemant D. Tagare</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding the Perceived Quality of Video Predictions. (arXiv:2005.00356v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2005.00356","description":"<p>The study of video prediction models is believed to be a fundamental approach\nto representation learning for videos. While a plethora of generative models\nfor predicting the future frame pixel values given the past few frames exist,\nthe quantitative evaluation of the predicted frames has been found to be\nextremely challenging. In this context, we study the problem of quality\nassessment of predicted videos. We create the Indian Institute of Science\nPredicted Videos Quality Assessment (IISc PVQA) Database consisting of 300\nvideos, obtained by applying different prediction models on different datasets,\nand accompanying human opinion scores. We collected subjective ratings of\nquality from 50 human participants for these videos. Our subjective study\nreveals that human observers were highly consistent in their judgments of\nquality of predicted videos. We benchmark several popularly used measures for\nevaluating video prediction and show that they do not adequately correlate with\nthese subjective scores. We introduce two new features to effectively capture\nthe quality of predicted videos, motion-compensated cosine similarities of deep\nfeatures of predicted frames with past frames, and deep features extracted from\nrescaled frame differences. We show that our feature design leads to state of\nthe art quality prediction in accordance with human judgments on our IISc PVQA\nDatabase. The database and code are publicly available on our project website:\nhttps://nagabhushansn95.github.io/publications/2020/pvqa\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Somraj_N/0/1/0/all/0/1\">Nagabhushan Somraj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kashi_M/0/1/0/all/0/1\">Manoj Surya Kashi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arun_S/0/1/0/all/0/1\">S. P. Arun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soundararajan_R/0/1/0/all/0/1\">Rajiv Soundararajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attentive WaveBlock: Complementarity-enhanced Mutual Networks for Unsupervised Domain Adaptation in Person Re-identification and Beyond. (arXiv:2006.06525v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2006.06525","description":"<p>Unsupervised domain adaptation (UDA) for person re-identification is\nchallenging because of the huge gap between the source and target domain. A\ntypical self-training method is to use pseudo-labels generated by clustering\nalgorithms to iteratively optimize the model on the target domain. However, a\ndrawback to this is that noisy pseudo-labels generally cause trouble in\nlearning. To address this problem, a mutual learning method by dual networks\nhas been developed to produce reliable soft labels. However, as the two neural\nnetworks gradually converge, their complementarity is weakened and they likely\nbecome biased towards the same kind of noise. This paper proposes a novel\nlight-weight module, the Attentive WaveBlock (AWB), which can be integrated\ninto the dual networks of mutual learning to enhance the complementarity and\nfurther depress noise in the pseudo-labels. Specifically, we first introduce a\nparameter-free module, the WaveBlock, which creates a difference between\nfeatures learned by two networks by waving blocks of feature maps differently.\nThen, an attention mechanism is leveraged to enlarge the difference created and\ndiscover more complementary features. Furthermore, two kinds of combination\nstrategies, i.e. pre-attention and post-attention, are explored. Experiments\ndemonstrate that the proposed method achieves state-of-the-art performance with\nsignificant improvements on multiple UDA person re-identification tasks. We\nalso prove the generality of the proposed method by applying it to vehicle\nre-identification and image classification tasks. Our codes and models are\navailable at https://github.com/WangWenhao0716/Attentive-WaveBlock.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1\">Fang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shengcai Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Elements of End-to-end Deep Face Recognition: A Survey of Recent Advances. (arXiv:2009.13290v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2009.13290","description":"<p>Face recognition is one of the most popular and long-standing topics in\ncomputer vision. With the recent development of deep learning techniques and\nlarge-scale datasets, deep face recognition has made remarkable progress and\nbeen widely used in many real-world applications. Given a natural image or\nvideo frame as input, an end-to-end deep face recognition system outputs the\nface feature for recognition. To achieve this, a typical end-to-end system is\nbuilt with three key elements: face detection, face alignment, and face\nrepresentation. The face detection locates faces in the image or frame. Then,\nthe face alignment is proceeded to calibrate the faces to the canonical view\nand crop them with a normalized pixel size. Finally, in the stage of face\nrepresentation, the discriminative features are extracted from the aligned face\nfor recognition. Nowadays, all of the three elements are fulfilled by the\ntechnique of deep convolutional neural network. In this survey article, we\npresent a comprehensive review about the recent advance of each element. To\nstart with, we present an overview of the end-to-end deep face recognition.\nThen, we review the advance of each element, respectively, covering many\naspects such as the to-date algorithm designs, evaluation metrics, datasets,\nperformance comparison, existing challenges, and promising directions for\nfuture research. Also, we provide a detailed discussion about the effect of\neach element on its subsequent elements and the holistic system. Through this\nsurvey, we wish to bring contributions in two aspects: first, readers can\nconveniently identify the methods which are quite strong-baseline style in the\nsubcategory for further exploration; second, one can also employ suitable\nmethods for establishing a state-of-the-art end-to-end face recognition system\nfrom scratch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1\">Hang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Hailin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Ping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1\">Tao Mei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distributionally Robust Learning for Uncertainty Calibration under Domain Shift. (arXiv:2010.05784v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.05784","description":"<p>We propose a framework for learning calibrated uncertainties under domain\nshifts. We consider the case where the source (training) distribution differs\nfrom the target (test) distribution. We detect such domain shifts through the\nuse of a binary domain classifier and integrate it with the task network and\ntrain them jointly end-to-end. The binary domain classifier yields a density\nratio that reflects the closeness of a target (test) sample to the source\n(training) distribution. We employ it to adjust the uncertainty of prediction\nin the task network. This idea of using the density ratio is based on the\ndistributionally robust learning (DRL) framework, which accounts for the domain\nshift through adversarial risk minimization. We demonstrate that our method\ngenerates calibrated uncertainties that benefit many downstream tasks, such as\nunsupervised domain adaptation (UDA) and semi-supervised learning (SSL). In\nthese tasks, methods like self-training and FixMatch use uncertainties to\nselect confident pseudo-labels for re-training. Our experiments show that the\nintroduction of DRL leads to significant improvements in cross-domain\nperformance. We also demonstrate that the estimated density ratios show\nagreement with the human selection frequencies, suggesting a positive\ncorrelation with a proxy of human perceived uncertainties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1\">Yisong Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v7 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.05260","description":"<p>In cases of serious crime, including sexual abuse, often the only available\ninformation with demonstrated potential for identification is images of the\nhands. Since this evidence is captured in uncontrolled situations, it is\ndifficult to analyse. As global approaches to feature comparison are limited in\nthis case, it is important to extend to consider local information. In this\nwork, we propose hand-based person identification by learning both global and\nlocal deep feature representation. Our proposed method, Global and Part-Aware\nNetwork (GPA-Net), creates global and local branches on the conv-layer for\nlearning robust discriminative global and part-level features. For learning the\nlocal (part-level) features, we perform uniform partitioning on the conv-layer\nin both horizontal and vertical directions. We retrieve the parts by conducting\na soft partition without explicitly partitioning the images or requiring\nexternal cues such as pose estimation. We make extensive evaluations on two\nlarge multi-ethnic and publicly available hand datasets, demonstrating that our\nproposed method significantly outperforms competing approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Adaptive Training: Bridging Supervised and Self-Supervised Learning. (arXiv:2101.08732v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2101.08732","description":"<p>We propose self-adaptive training -- a unified training algorithm that\ndynamically calibrates and enhances training processes by model predictions\nwithout incurring an extra computational cost -- to advance both supervised and\nself-supervised learning of deep neural networks. We analyze the training\ndynamics of deep networks on training data that are corrupted by, e.g., random\nnoise and adversarial examples. Our analysis shows that model predictions are\nable to magnify useful underlying information in data and this phenomenon\noccurs broadly even in the absence of any label information, highlighting that\nmodel predictions could substantially benefit the training processes:\nself-adaptive training improves the generalization of deep networks under noise\nand enhances the self-supervised representation learning. The analysis also\nsheds light on understanding deep learning, e.g., a potential explanation of\nthe recently-discovered double-descent phenomenon in empirical risk\nminimization and the collapsing issue of the state-of-the-art self-supervised\nlearning algorithms. Experiments on the CIFAR, STL, and ImageNet datasets\nverify the effectiveness of our approach in three applications: classification\nwith label noise, selective classification, and linear evaluation. To\nfacilitate future research, the code has been made publicly available at\nhttps://github.com/LayneH/self-adaptive-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Lang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongyang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Anytime 3D Object Reconstruction using Multi-modal Variational Autoencoder. (arXiv:2101.10391v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.10391","description":"<p>For effective human-robot teaming, it is important for the robots to be able\nto share their visual perception with the human operators. In a harsh remote\ncollaboration setting, data compression techniques such as autoencoder can be\nutilized to obtain and transmit the data in terms of latent variables in a\ncompact form. In addition, to ensure real-time runtime performance even under\nunstable environments, an anytime estimation approach is desired that can\nreconstruct the full contents from incomplete information. In this context, we\npropose a method for imputation of latent variables whose elements are\npartially lost. To achieve the anytime property with only a few dimensions of\nvariables, exploiting prior information of the category-level is essential. A\nprior distribution used in variational autoencoders is simply assumed to be\nisotropic Gaussian regardless of the labels of each training datapoint. This\ntype of flattened prior makes it difficult to perform imputation from the\ncategory-level distributions. We overcome this limitation by exploiting a\ncategory-specific multi-modal prior distribution in the latent space. The\nmissing elements of the partially transferred data can be sampled, by finding a\nspecific modal according to the remaining elements. Since the method is\ndesigned to use partial elements for anytime estimation, it can also be applied\nfor data over-compression. Based on the experiments on the ModelNet and\nPascal3D datasets, the proposed approach shows consistently superior\nperformance over autoencoder and variational autoencoder up to 70% data loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hyeonwoo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Semi-Supervised Method Using Large Unlabeled and Limited Labeled COVID-19 Data. (arXiv:2102.06388v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2102.06388","description":"<p>The new coronavirus has caused more than one million deaths and continues to\nspread rapidly. This virus targets the lungs, causing respiratory distress\nwhich can be mild or severe. The X-ray or computed tomography (CT) images of\nlungs can reveal whether the patient is infected with COVID-19 or not. Many\nresearchers are trying to improve COVID-19 detection using artificial\nintelligence. Our motivation is to develop an automatic method that can cope\nwith scenarios in which preparing labeled data is time consuming or expensive.\nIn this article, we propose a Semi-supervised Classification using Limited\nLabeled Data (SCLLD) relying on Sobel edge detection and Generative Adversarial\nNetworks (GANs) to automate the COVID-19 diagnosis. The GAN discriminator\noutput is a probabilistic value which is used for classification in this work.\nThe proposed system is trained using 10,000 CT scans collected from Omid\nHospital, whereas a public dataset is also used for validating our system. The\nproposed method is compared with other state-of-the-art supervised methods such\nas Gaussian processes. To the best of our knowledge, this is the first time a\nsemi-supervised method for COVID-19 detection is presented. Our system is\ncapable of learning from a mixture of limited labeled and unlabeled data where\nsupervised learners fail due to a lack of sufficient amount of labeled data.\nThus, our semi-supervised training method significantly outperforms the\nsupervised training of Convolutional Neural Network (CNN) when labeled training\ndata is scarce. The 95% confidence intervals for our method in terms of\naccuracy, sensitivity, and specificity are 99.56 +- 0.20%, 99.88 +- 0.24%, and\n99.40 +- 0.18%, respectively, whereas intervals for the CNN (trained\nsupervised) are 68.34 +- 4.11%, 91.2 +- 6.15%, and 46.40 +- 5.21%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Izadi_N/0/1/0/all/0/1\">Navid Hoseini Izadi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joloudari_J/0/1/0/all/0/1\">Javad Hassannataj Joloudari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hussain_S/0/1/0/all/0/1\">Sadiq Hussain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Arco_J/0/1/0/all/0/1\">Juan E. Arco</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sani_Z/0/1/0/all/0/1\">Zahra Alizadeh Sani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khozeimeh_F/0/1/0/all/0/1\">Fahime Khozeimeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Islam_S/0/1/0/all/0/1\">Sheikh Mohammed Shariful Islam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1\">U Rajendra Acharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reconstructing Recognizable 3D Face Shapes based on 3D Morphable Models. (arXiv:2104.03515v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.03515","description":"<p>Many recent works have reconstructed distinctive 3D face shapes by\naggregating shape parameters of the same identity and separating those of\ndifferent people based on parametric models (e.g., 3D morphable models\n(3DMMs)). However, despite the high accuracy in the face recognition task using\nthese shape parameters, the visual discrimination of face shapes reconstructed\nfrom those parameters is unsatisfactory. The following research question has\nnot been answered in previous works: Do discriminative shape parameters\nguarantee visual discrimination in represented 3D face shapes? This paper\nanalyzes the relationship between shape parameters and reconstructed shape\ngeometry and proposes a novel shape identity-aware regularization(SIR) loss for\nshape parameters, aiming at increasing discriminability in both the shape\nparameter and shape geometry domains. Moreover, to cope with the lack of\ntraining data containing both landmark and identity annotations, we propose a\nnetwork structure and an associated training strategy to leverage mixed data\ncontaining either identity or landmark labels. We compare our method with\nexisting methods in terms of the reconstruction error, visual\ndistinguishability, and face recognition accuracy of the shape parameters.\nExperimental results show that our method outperforms the state-of-the-art\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Diqiong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yiwei Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Fanglue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yai_Y/0/1/0/all/0/1\">Yukun Yai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Risheng Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_R/0/1/0/all/0/1\">Ruofeng Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1\">Min Tang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skeleton-based Hand-Gesture Recognition with Lightweight Graph Convolutional Networks. (arXiv:2104.04255v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04255","description":"<p>Graph convolutional networks (GCNs) aim at extending deep learning to\narbitrary irregular domains, namely graphs. Their success is highly dependent\non how the topology of input graphs is defined and most of the existing GCN\narchitectures rely on predefined or handcrafted graph structures. In this\npaper, we introduce a novel method that learns the topology (or connectivity)\nof input graphs as a part of GCN design. The main contribution of our method\nresides in building an orthogonal connectivity basis that optimally aggregates\nnodes, through their neighborhood, prior to achieve convolution. Our method\nalso considers a stochasticity criterion which acts as a regularizer that makes\nthe learned basis and the underlying GCNs lightweight while still being highly\neffective. Experiments conducted on the challenging task of skeleton-based\nhand-gesture recognition show the high effectiveness of the learned GCNs w.r.t.\nthe related work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from 2D: Contrastive Pixel-to-Point Knowledge Transfer for 3D Pretraining. (arXiv:2104.04687v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.04687","description":"<p>Most 3D neural networks are trained from scratch owing to the lack of\nlarge-scale labeled 3D datasets. In this paper, we present a novel 3D\npretraining method by leveraging 2D networks learned from rich 2D datasets. We\npropose the contrastive pixel-to-point knowledge transfer to effectively\nutilize the 2D information by mapping the pixel-level and point-level features\ninto the same embedding space. Due to the heterogeneous nature between 2D and\n3D networks, we introduce the back-projection function to align the features\nbetween 2D and 3D to make the transfer possible. Additionally, we devise an\nupsampling feature projection layer to increase the spatial resolution of\nhigh-level 2D feature maps, which enables learning fine-grained 3D\nrepresentations. With a pretrained 2D network, the proposed pretraining process\nrequires no additional 2D or 3D labeled data, further alleviating the expensive\n3D data annotation cost. To the best of our knowledge, we are the first to\nexploit existing 2D trained weights to pretrain 3D deep neural networks. Our\nintensive experiments show that the 3D models pretrained with 2D knowledge\nboost the performances of 3D networks across various real-world 3D downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yu-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_H/0/1/0/all/0/1\">Hung-Yueh Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hung-Ting Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chin-Tang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_C/0/1/0/all/0/1\">Ching-Yu Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Winston H. Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Chebyshev Basis in Graph Convolutional Networks for Skeleton-based Action Recognition. (arXiv:2104.05482v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05482","description":"<p>Spectral graph convolutional networks (GCNs) are particular deep models which\naim at extending neural networks to arbitrary irregular domains. The principle\nof these networks consists in projecting graph signals using the\neigen-decomposition of their Laplacians, then achieving filtering in the\nspectral domain prior to back-project the resulting filtered signals onto the\ninput graph domain. However, the success of these operations is highly\ndependent on the relevance of the used Laplacians which are mostly handcrafted\nand this makes GCNs clearly sub-optimal. In this paper, we introduce a novel\nspectral GCN that learns not only the usual convolutional parameters but also\nthe Laplacian operators. The latter are designed \"end-to-end\" as a part of a\nrecursive Chebyshev decomposition with the particularity of conveying both the\ndifferential and the non-differential properties of the learned representations\n-- with increasing order and discrimination power -- without overparametrizing\nthe trained GCNs. Extensive experiments, conducted on the challenging task of\nskeleton-based action recognition, show the generalization ability and the\noutperformance of our proposed Laplacian design w.r.t. different baselines\n(built upon handcrafted and other learned Laplacians) as well as the related\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahbi_H/0/1/0/all/0/1\">Hichem Sahbi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time series forecasting of new cases and new deaths rate for COVID-19 using deep learning methods. (arXiv:2104.15007v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2104.15007","description":"<p>The first known case of Coronavirus disease 2019 (COVID-19) was identified in\nDecember 2019. It has spread worldwide, leading to an ongoing pandemic, imposed\nrestrictions and costs to many countries. Predicting the number of new cases\nand deaths during this period can be a useful step in predicting the costs and\nfacilities required in the future. The purpose of this study is to predict new\ncases and deaths rate one, three and seven-day ahead during the next 100 days.\nThe motivation for predicting every n days (instead of just every day) is the\ninvestigation of the possibility of computational cost reduction and still\nachieving reasonable performance. Such a scenario may be encountered in\nreal-time forecasting of time series. Six different deep learning methods are\nexamined on the data adopted from the WHO website. Three methods are LSTM,\nConvolutional LSTM, and GRU. The bidirectional extension is then considered for\neach method to forecast the rate of new cases and new deaths in Australia and\nIran countries.\n</p>\n<p>This study is novel as it carries out a comprehensive evaluation of the\naforementioned three deep learning methods and their bidirectional extensions\nto perform prediction on COVID-19 new cases and new death rate time series. To\nthe best of our knowledge, this is the first time that Bi-GRU and Bi-Conv-LSTM\nmodels are used for prediction on COVID-19 new cases and new deaths time\nseries. The evaluation of the methods is presented in the form of graphs and\nFriedman statistical test. The results show that the bidirectional models have\nlower errors than other models. A several error evaluation metrics are\npresented to compare all models, and finally, the superiority of bidirectional\nmethods is determined. This research could be useful for organisations working\nagainst COVID-19 and determining their long-term plans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayoobi_N/0/1/0/all/0/1\">Nooshin Ayoobi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharifrazi_D/0/1/0/all/0/1\">Danial Sharifrazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alizadehsani_R/0/1/0/all/0/1\">Roohallah Alizadehsani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeibi_A/0/1/0/all/0/1\">Afshin Shoeibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1\">Juan M. Gorriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moosaei_H/0/1/0/all/0/1\">Hossein Moosaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khosravi_A/0/1/0/all/0/1\">Abbas Khosravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nahavandi_S/0/1/0/all/0/1\">Saeid Nahavandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chofreh_A/0/1/0/all/0/1\">Abdoulmohammad Gholamzadeh Chofreh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goni_F/0/1/0/all/0/1\">Feybi Ariani Goni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klemes_J/0/1/0/all/0/1\">Jiri Jaromir Klemes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mosavi_A/0/1/0/all/0/1\">Amir Mosavi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-contact Pain Recognition from Video Sequences with Remote Physiological Measurements Prediction. (arXiv:2105.08822v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.08822","description":"<p>Automatic pain recognition is paramount for medical diagnosis and treatment.\nThe existing works fall into three categories: assessing facial appearance\nchanges, exploiting physiological cues, or fusing them in a multi-modal manner.\nHowever, (1) appearance changes are easily affected by subjective factors which\nimpedes objective pain recognition. Besides, the appearance-based approaches\nignore long-range spatial-temporal dependencies that are important for modeling\nexpressions over time; (2) the physiological cues are obtained by attaching\nsensors on human body, which is inconvenient and uncomfortable. In this paper,\nwe present a novel multi-task learning framework which encodes both appearance\nchanges and physiological cues in a non-contact manner for pain recognition.\nThe framework is able to capture both local and long-range dependencies via the\nproposed attention mechanism for the learned appearance representations, which\nare further enriched by temporally attended physiological cues (remote\nphotoplethysmography, rPPG) that are recovered from videos in the auxiliary\ntask. This framework is dubbed rPPG-enriched Spatio-Temporal Attention Network\n(rSTAN) and allows us to establish the state-of-the-art performance of\nnon-contact pain recognition on publicly available pain databases. It\ndemonstrates that rPPG predictions can be used as an auxiliary task to\nfacilitate non-contact automatic pain recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ruijing Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1\">Ziyu Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1\">Xiaoyi Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jinye Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Visual Representation Learning by Online Constrained K-Means. (arXiv:2105.11527v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.11527","description":"<p>Cluster discrimination is an effective pretext task for unsupervised\nrepresentation learning, which often consists of two phases: clustering and\ndiscrimination. Clustering is to assign each instance a pseudo label that will\nbe used to learn representations in discrimination. The main challenge resides\nin clustering since prevalent clustering methods (e.g., k-means) have to run in\na batch mode and there can be a trivial solution consisting of a dominating\ncluster. To address these challenges, we first investigate the objective of\nclustering-based representation learning. Based on this, we propose a novel\nclustering-based pretext task with online Constrained K-means (CoKe). Compared\nwith the balanced clustering that each cluster has exactly the same size, we\nonly constrain the minimal size of each cluster to flexibly capture the\ninherent data structure. More importantly, our online assignment method has a\ntheoretical guarantee to approach the global optimum. By decoupling clustering\nand discrimination, CoKe can achieve competitive performance when optimizing\nwith only a single view from each instance. Extensive experiments on ImageNet\nverify both the efficacy and efficiency of our proposal. Code will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qian_Q/0/1/0/all/0/1\">Qi Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yuanhong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Juhua Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Mutual Learning for Semi-supervised Semantic Segmentation. (arXiv:2106.00609v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00609","description":"<p>Recent semi-supervised learning (SSL) methods are commonly based on pseudo\nlabeling. Since the SSL performance is greatly influenced by the quality of\npseudo labels, mutual learning has been proposed to effectively suppress the\nnoises in the pseudo supervision. In this work, we propose robust mutual\nlearning that improves the prior approach in two aspects. First, the vanilla\nmutual learners suffer from the coupling issue that models may converge to\nlearn homogeneous knowledge. We resolve this issue by introducing mean teachers\nto generate mutual supervisions so that there is no direct interaction between\nthe two students. We also show that strong data augmentations, model noises and\nheterogeneous network architectures are essential to alleviate the model\ncoupling. Second, we notice that mutual learning fails to leverage the\nnetwork's own ability for pseudo label refinement. Therefore, we introduce\nself-rectification that leverages the internal knowledge and explicitly\nrectifies the pseudo labels before the mutual teaching. Such self-rectification\nand mutual teaching collaboratively improve the pseudo label accuracy\nthroughout the learning. The proposed robust mutual learning demonstrates\nstate-of-the-art performance on semantic segmentation in low-data regime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Ting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fang Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias. (arXiv:2106.03348v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.03348","description":"<p>Transformers have shown great potential in various computer vision tasks\nowing to their strong capability in modeling long-range dependency using the\nself-attention mechanism. Nevertheless, vision transformers treat an image as\n1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in\nmodeling local visual structures and dealing with scale variance.\nAlternatively, they require large-scale training data and longer training\nschedules to learn the IB implicitly. In this paper, we propose a novel Vision\nTransformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE.\nTechnically, ViTAE has several spatial pyramid reduction modules to downsample\nand embed the input image into tokens with rich multi-scale context by using\nmultiple convolutions with different dilation rates. In this way, it acquires\nan intrinsic scale invariance IB and is able to learn robust feature\nrepresentation for objects at various scales. Moreover, in each transformer\nlayer, ViTAE has a convolution block in parallel to the multi-head\nself-attention module, whose features are fused and fed into the feed-forward\nnetwork. Consequently, it has the intrinsic locality IB and is able to learn\nlocal features and global dependencies collaboratively. Experiments on ImageNet\nas well as downstream tasks prove the superiority of ViTAE over the baseline\ntransformer and concurrent works. Source code and pretrained models will be\navailable at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yufei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Transformer for High-Resolution GANs. (arXiv:2106.07631v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.07631","description":"<p>Attention-based models, exemplified by the Transformer, can effectively model\nlong range dependency, but suffer from the quadratic complexity of\nself-attention operation, making them difficult to be adopted for\nhigh-resolution image generation based on Generative Adversarial Networks\n(GANs). In this paper, we introduce two key ingredients to Transformer to\naddress this challenge. First, in low-resolution stages of the generative\nprocess, standard global self-attention is replaced with the proposed\nmulti-axis blocked self-attention which allows efficient mixing of local and\nglobal attention. Second, in high-resolution stages, we drop self-attention\nwhile only keeping multi-layer perceptrons reminiscent of the implicit neural\nfunction. To further improve the performance, we introduce an additional\nself-modulation component based on cross-attention. The resulting model,\ndenoted as HiT, has a nearly linear computational complexity with respect to\nthe image size and thus directly scales to synthesizing high definition images.\nWe show in the experiments that the proposed HiT achieves state-of-the-art FID\nscores of 30.83 and 2.95 on unconditional ImageNet $128 \\times 128$ and FFHQ\n$256 \\times 256$, respectively, with a reasonable throughput. We believe the\nproposed HiT is an important milestone for generators in GANs which are\ncompletely free of convolutions. Our code is made publicly available at\nhttps://github.com/google-research/hit-gan\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action Transformer: A Self-Attention Model for Short-Time Pose-Based Human Action Recognition. (arXiv:2107.00606v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.00606","description":"<p>Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time,\nshort-time HAR. The proposed methodology was extensively tested on MPOSE2021\nand compared to several state-of-the-art architectures, proving the\neffectiveness of the AcT model and laying the foundations for future work on\nHAR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CamTuner: Reinforcement-Learning based System for Camera Parameter Tuning to enhance Analytics. (arXiv:2107.03964v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.03964","description":"<p>Video analytics systems critically rely on video cameras, which capture\nhigh-quality video frames, to achieve high analytics accuracy. Although modern\nvideo cameras often expose tens of configurable parameter settings that can be\nset by end-users, deployment of surveillance cameras today often uses a fixed\nset of parameter settings because the end-users lack the skill or understanding\nto reconfigure these parameters.\n</p>\n<p>In this paper, we first show that in a typical surveillance camera\ndeployment, environmental condition changes can significantly affect the\naccuracy of analytics units such as person detection, face detection and face\nrecognition, and how such adverse impact can be mitigated by dynamically\nadjusting camera settings. We then propose CAMTUNER, a framework that can be\neasily applied to an existing video analytics pipeline (VAP) to enable\nautomatic and dynamic adaptation of complex camera settings to changing\nenvironmental conditions, and autonomously optimize the accuracy of analytics\nunits (AUs) in the VAP. CAMTUNER is based on SARSA reinforcement learning (RL)\nand it incorporates two novel components: a light-weight analytics quality\nestimator and a virtual camera. CAMTUNER is implemented in a system with AXIS\nsurveillance cameras and several VAPs (with various AUs) that processed\nday-long customer videos captured at airport entrances. Our evaluations show\nthat CAMTUNER can adapt quickly to changing environments. We compared CAMTUNER\nwith two alternative approaches where either static camera settings were used,\nor a strawman approach where camera settings were manually changed every hour\n(based on human perception of quality). We observed that for the face detection\nand person detection AUs, CAMTUNER is able to achieve up to 13.8% and 9.2%\nhigher accuracy, respectively, compared to the best of the two approaches\n(average improvement of 8% for both AUs).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1\">Sibendu Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1\">Kunal Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coviello_G/0/1/0/all/0/1\">Giuseppe Coviello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaradas_M/0/1/0/all/0/1\">Murugan Sankaradas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Po_O/0/1/0/all/0/1\">Oliver Po</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Y. Charlie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakradhar_S/0/1/0/all/0/1\">Srimat T. Chakradhar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"eGHWT: The Extended Generalized Haar-Walsh Transform. (arXiv:2107.05121v3 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2107.05121","description":"<p>Extending computational harmonic analysis tools from the classical setting of\nregular lattices to the more general setting of graphs and networks is very\nimportant and much research has been done recently. The Generalized Haar-Walsh\nTransform (GHWT) developed by Irion and Saito (2014) is a multiscale transform\nfor signals on graphs, which is a generalization of the classical Haar and\nWalsh-Hadamard Transforms. We propose the extended Generalized Haar-Walsh\nTransform (eGHWT), which is a generalization of the adapted time-frequency\ntilings of Thiele and Villemoes (1996). The eGHWT examines not only the\nefficiency of graph-domain partitions but also that of \"sequency-domain\"\npartitions simultaneously. Consequently, the eGHWT and its associated\nbest-basis selection algorithm for graph signals significantly improve the\nperformance of the previous GHWT with the similar computational cost, $O(N \\log\nN)$, where $N$ is the number of nodes of an input graph. While the GHWT\nbest-basis algorithm seeks the most suitable orthonormal basis for a given task\namong more than $(1.5)^N$ possible orthonormal bases in $\\mathbb{R}^N$, the\neGHWT best-basis algorithm can find a better one by searching through more than\n$0.618\\cdot(1.84)^N$ possible orthonormal bases in $\\mathbb{R}^N$. This article\ndescribes the details of the eGHWT best-basis algorithm and demonstrates its\nsuperiority using several examples including genuine graph signals as well as\nconventional digital images viewed as graph signals. Furthermore, we also show\nhow the eGHWT can be extended to 2D signals and matrix-form data by viewing\nthem as a tensor product of graphs generated from their columns and rows and\ndemonstrate its effectiveness on applications such as image approximation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Saito_N/0/1/0/all/0/1\">Naoki Saito</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_Y/0/1/0/all/0/1\">Yiqun Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A case study for skin lesion classification. (arXiv:2107.12734v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.12734","description":"<p>We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Raumanns_R/0/1/0/all/0/1\">Ralf Raumanns</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schouten_G/0/1/0/all/0/1\">Gerard Schouten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joosten_M/0/1/0/all/0/1\">Max Joosten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pluim_J/0/1/0/all/0/1\">Josien P. W. Pluim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheplygina_V/0/1/0/all/0/1\">Veronika Cheplygina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering \"Semantics\" in Super-Resolution Networks. (arXiv:2108.00406v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.00406","description":"<p>Super-resolution (SR) is a fundamental and representative task of low-level\nvision area. It is generally thought that the features extracted from the SR\nnetwork have no specific semantic information, and the network simply learns\ncomplex non-linear mappings from input to output. Can we find any \"semantics\"\nin SR networks? In this paper, we give affirmative answers to this question. By\nanalyzing the feature representations with dimensionality reduction and\nvisualization, we successfully discover the deep semantic representations in SR\nnetworks, \\textit{i.e.}, deep degradation representations (DDR), which relate\nto the image degradation types and degrees. We also reveal the differences in\nrepresentation semantics between classification and SR networks. Through\nextensive experiments and analysis, we draw a series of observations and\nconclusions, which are of great significance for future work, such as\ninterpreting the intrinsic mechanisms of low-level CNN networks and developing\nnew evaluation approaches for blind SR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yihao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhipeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wenhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.02234","description":"<p>In this paper, we propose a novel hand-based person recognition method for\nthe purpose of criminal investigations since the hand image is often the only\navailable information in cases of serious crime such as sexual abuse. Our\nproposed method, Multi-Branch with Attention Network (MBA-Net), incorporates\nboth channel and spatial attention modules in branches in addition to a global\n(without attention) branch to capture global structural information for\ndiscriminative feature learning. The attention modules focus on the relevant\nfeatures of the hand image while suppressing the irrelevant backgrounds. In\norder to overcome the weakness of the attention mechanisms, equivariant to\npixel shuffling, we integrate relative positional encodings into the spatial\nattention module to capture the spatial positions of pixels. Extensive\nevaluations on two large multi-ethnic and publicly available hand datasets\ndemonstrate that our proposed method achieves state-of-the-art performance,\nsurpassing the existing hand-based identification methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1\">Nathanael L. Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1\">Bryan Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1\">Hossein Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1\">Plamen Angelov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1\">Sue Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Re-using Adversarial Mask Discriminators for Test-time Training under Distribution Shifts. (arXiv:2108.11926v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11926","description":"<p>Thanks to their ability to learn flexible data-driven losses, Generative\nAdversarial Networks (GANs) are an integral part of many semi- and\nweakly-supervised methods for medical image segmentation. GANs jointly optimise\na generator and an adversarial discriminator on a set of training data. After\ntraining is complete, the discriminator is usually discarded, and only the\ngenerator is used for inference. But should we discard discriminators? In this\nwork, we argue that training stable discriminators produces expressive loss\nfunctions that we can re-use at inference to detect and \\textit{correct}\nsegmentation mistakes. First, we identify key challenges and suggest possible\nsolutions to make discriminators re-usable at inference. Then, we show that we\ncan combine discriminators with image reconstruction costs (via decoders) to\nendow a causal perspective to test-time training and further improve the model.\nOur method is simple and improves the test-time performance of pre-trained\nGANs. Moreover, we show that it is compatible with standard post-processing\ntechniques and it has the potential to be used for Online Continual Learning.\nWith our work, we open new research avenues for re-using adversarial\ndiscriminators at inference. Our code is available at\nhttps://vios-s.github.io/adversarial-test-time-training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Valvano_G/0/1/0/all/0/1\">Gabriele Valvano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leo_A/0/1/0/all/0/1\">Andrea Leo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsaftaris_S/0/1/0/all/0/1\">Sotirios A. Tsaftaris</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking. (arXiv:2109.03805v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03805","description":"<p>Panoptic scene understanding and tracking of dynamic agents are essential for\nrobots and automated vehicles to navigate in urban environments. As LiDARs\nprovide accurate illumination-independent geometric depictions of the scene,\nperforming these tasks using LiDAR point clouds provides reliable predictions.\nHowever, existing datasets lack diversity in the type of urban scenes and have\na limited number of dynamic object instances which hinders both learning of\nthese tasks as well as credible benchmarking of the developed methods. In this\npaper, we introduce the large-scale Panoptic nuScenes benchmark dataset that\nextends our popular nuScenes dataset with point-wise groundtruth annotations\nfor semantic segmentation, panoptic segmentation, and panoptic tracking tasks.\nTo facilitate comparison, we provide several strong baselines for each of these\ntasks on our proposed dataset. Moreover, we analyze the drawbacks of the\nexisting metrics for panoptic tracking and propose the novel instance-centric\nPAT metric that addresses the concerns. We present exhaustive experiments that\ndemonstrate the utility of Panoptic nuScenes compared to existing datasets and\nmake the online evaluation server available at nuScenes.org. We believe that\nthis extension will accelerate the research of novel methods for scene\nunderstanding of dynamic urban environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fong_W/0/1/0/all/0/1\">Whye Kit Fong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1\">Rohit Mohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1\">Juana Valeria Hurtado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lubing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1\">Holger Caesar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IceNet for Interactive Contrast Enhancement. (arXiv:2109.05838v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.05838","description":"<p>A CNN-based interactive contrast enhancement algorithm, called IceNet, is\nproposed in this work, which enables a user to adjust image contrast easily\naccording to his or her preference. Specifically, a user provides a parameter\nfor controlling the global brightness and two types of scribbles to darken or\nbrighten local regions in an image. Then, given these annotations, IceNet\nestimates a gamma map for the pixel-wise gamma correction. Finally, through\ncolor restoration, an enhanced image is obtained. The user may provide\nannotations iteratively to obtain a satisfactory image. IceNet is also capable\nof producing a personalized enhanced image automatically, which can serve as a\nbasis for further adjustment if so desired. Moreover, to train IceNet\neffectively and reliably, we propose three differentiable losses. Extensive\nexperiments show that IceNet can provide users with satisfactorily enhanced\nimages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ko_K/0/1/0/all/0/1\">Keunsoo Ko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_C/0/1/0/all/0/1\">Chang-Su Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LGD: Label-guided Self-distillation for Object Detection. (arXiv:2109.11496v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.11496","description":"<p>In this paper, we propose the first self-distillation framework for general\nobject detection, termed LGD (Label-Guided self-Distillation). Previous studies\nrely on a strong pretrained teacher to provide instructive knowledge that could\nbe unavailable in real-world scenarios. Instead, we generate an instructive\nknowledge by inter-and-intra relation modeling among objects, requiring only\nstudent representations and regular labels. Concretely, our framework involves\nsparse label-appearance encoding, inter-object relation adaptation and\nintra-object knowledge mapping to obtain the instructive knowledge. They\njointly form an implicit teacher at training phase, dynamically dependent on\nlabels and evolving student representations. Modules in LGD are trained\nend-to-end with student detector and are discarded in inference.\nExperimentally, LGD obtains decent results on various detectors, datasets, and\nextensive tasks like instance segmentation. For example in MS-COCO dataset, LGD\nimproves RetinaNet with ResNet-50 under 2x single-scale training from 36.2% to\n39.0% mAP (+ 2.8%). It boosts much stronger detectors like FCOS with\nResNeXt-101 DCN v2 under 2x multi-scale training from 46.1% to 47.9% (+ 1.8%).\nCompared with a classical teacher-based method FGFI, LGD not only performs\nbetter without requiring pretrained teacher but also reduces 51% training cost\nbeyond inherent student learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Peizhen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zijian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1\">Nanning Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Joint Progressive and Coarse-to-fine Registration of Brain MRI via Deformation Field Integration and Non-Rigid Feature Fusion. (arXiv:2109.12384v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.12384","description":"<p>Registration of brain MRI images requires to solve a deformation field, which\nis extremely difficult in aligning intricate brain tissues, e.g., subcortical\nnuclei, etc. Existing efforts resort to decomposing the target deformation\nfield into intermediate sub-fields with either tiny motions, i.e., progressive\nregistration stage by stage, or lower resolutions, i.e., coarse-to-fine\nestimation of the full-size deformation field. In this paper, we argue that\nthose efforts are not mutually exclusive, and propose a unified framework for\nrobust brain MRI registration in both progressive and coarse-to-fine manners\nsimultaneously. Specifically, building on a dual-encoder U-Net, the\nfixed-moving MRI pair is encoded and decoded into multi-scale deformation\nsub-fields from coarse to fine. Each decoding block contains two proposed novel\nmodules: i) in Deformation Field Integration (DFI), a single integrated\nsub-field is calculated, warping by which is equivalent to warping\nprogressively by sub-fields from all previous decoding blocks, and ii) in\nNon-rigid Feature Fusion (NFF), features of the fixed-moving pair are aligned\nby DFI-integrated sub-field, and then fused to predict a finer sub-field.\nLeveraging both DFI and NFF, the target deformation field is factorized into\nmulti-scale sub-fields, where the coarser fields alleviate the estimate of a\nfiner one and the finer field learns to make up those misalignments insolvable\nby previous coarser ones. The extensive and comprehensive experimental results\non both private and public datasets demonstrate a superior registration\nperformance of brain MRI images over progressive registration only and\ncoarse-to-fine estimation only, with an increase by at most 8% in the average\nDice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lv_J/0/1/0/all/0/1\">Jinxin Lv</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiwei Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_H/0/1/0/all/0/1\">Hongkuan Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Haobo Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yilang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing deployment hazards (\"External Safety\"). Throughout, we clarify each\nproblem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Pose Transfer with Correspondence Learning and Mesh Refinement. (arXiv:2109.15025v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.15025","description":"<p>3D pose transfer is one of the most challenging 3D generation tasks. It aims\nto transfer the pose of a source mesh to a target mesh and keep the identity\n(e.g., body shape) of the target mesh. Some previous works require key point\nannotations to build reliable correspondence between the source and target\nmeshes, while other methods do not consider any shape correspondence between\nsources and targets, which leads to limited generation quality. In this work,\nwe propose a correspondence-refinement network to achieve the 3D pose transfer\nfor both human and animal meshes. The correspondence between source and target\nmeshes is first established by solving an optimal transport problem. Then, we\nwarp the source mesh according to the dense correspondence and obtain a coarse\nwarped mesh. The warped mesh will be better refined with our proposed Elastic\nInstance Normalization, which is a conditional normalization layer and can help\nto generate high-quality meshes. Extensive experimental results show that the\nproposed architecture can effectively transfer the poses from source to target\nmeshes and produce better results with satisfied visual performance than\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chaoyue Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jiacheng Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ruibo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fayao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1\">Guosheng Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Bias Filtering for Single Labeled Domain Generalization. (arXiv:2110.00726v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00726","description":"<p>Domain generalization (DG) utilizes multiple labeled source datasets to train\na generalizable model for unseen target domains. However, due to expensive\nannotation costs, the requirements of labeling all the source data are hard to\nbe met in real-world applications. In this paper, we investigate a Single\nLabeled Domain Generalization (SLDG) task with only one source domain being\nlabeled, which is more practical and challenging than the Conventional Domain\nGeneralization (CDG). A major obstacle in the SLDG task is the\ndiscriminability-generalization bias: discriminative information in the labeled\nsource dataset may contain domain-specific bias, constraining the\ngeneralization of the trained model. To tackle this challenging task, we\npropose a novel method called Domain-Specific Bias Filtering (DSBF), which\ninitializes a discriminative model with the labeled source data and then\nfilters out its domain-specific bias with the unlabeled source data for\ngeneralization improvement. We divide the filtering process into (1) feature\nextractor debiasing via k-means clustering-based semantic feature re-extraction\nand (2) classifier calibrating through attention-guided semantic feature\nprojection. DSBF unifies the exploration of the labeled and the unlabeled\nsource data to enhance the discriminability and generalization of the trained\nmodel, resulting in a highly generalizable model. We further provide\ntheoretical analysis to verify the proposed domain-specific bias filtering\nprocess. Extensive experiments on multiple datasets show the superior\nperformance of DSBF in tackling both the challenging SLDG task and the CDG\ntask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1\">Junkun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Defang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lanfen Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimized U-Net for Brain Tumor Segmentation. (arXiv:2110.03352v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.03352","description":"<p>We propose an optimized U-Net architecture for a brain tumor segmentation\ntask in the BraTS21 challenge. To find the optimal model architecture and the\nlearning schedule, we have run an extensive ablation study to test: deep\nsupervision loss, Focal loss, decoder attention, drop block, and residual\nconnections. Additionally, we have searched for the optimal depth of the U-Net\nencoder, number of convolutional channels and post-processing strategy. Our\nmethod won the validation phase and took third place in the test phase. We have\nopen-sourced the code to reproduce our BraTS21 submission at the NVIDIA Deep\nLearning Examples GitHub Repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Futrega_M/0/1/0/all/0/1\">Micha&#x142; Futrega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Milesi_A/0/1/0/all/0/1\">Alexandre Milesi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marcinkiewicz_M/0/1/0/all/0/1\">Michal Marcinkiewicz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ribalta_P/0/1/0/all/0/1\">Pablo Ribalta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Foreground Extraction via Deep Region Competition. (arXiv:2110.15497v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.15497","description":"<p>We present Deep Region Competition (DRC), an algorithm designed to extract\nforeground objects from images in a fully unsupervised manner. Foreground\nextraction can be viewed as a special case of generic image segmentation that\nfocuses on identifying and disentangling objects from the background. In this\nwork, we rethink the foreground extraction by reconciling energy-based prior\nwith generative image modeling in the form of Mixture of Experts (MoE), where\nwe further introduce the learned pixel re-assignment as the essential inductive\nbias to capture the regularities of background regions. With this modeling, the\nforeground-background partition can be naturally found through\nExpectation-Maximization (EM). We show that the proposed method effectively\nexploits the interaction between the mixture components during the partitioning\nprocess, which closely connects to region competition, a seminal approach for\ngeneric image segmentation. Experiments demonstrate that DRC exhibits more\ncompetitive performances on complex real-world data and challenging\nmulti-object scenes compared with prior methods. Moreover, we show empirically\nthat DRC can potentially generalize to novel foreground objects even from\ncategories unseen during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Peiyu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Sirui Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaojian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yixin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Ying Nian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Song-Chun Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Influential Prototypical Networks for Few Shot Learning: A Dermatological Case Study. (arXiv:2111.00698v5 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2111.00698","description":"<p>Prototypical network (PN) is a simple yet effective few shot learning\nstrategy. It is a metric-based meta-learning technique where classification is\nperformed by computing Euclidean distances to prototypical representations of\neach class. Conventional PN attributes equal importance to all samples and\ngenerates prototypes by simply averaging the support sample embeddings\nbelonging to each class. In this work, we propose a novel version of PN that\nattributes weights to support samples corresponding to their influence on the\nsupport sample distribution. Influence weights of samples are calculated based\non maximum mean discrepancy (MMD) between the mean embeddings of sample\ndistributions including and excluding the sample. Comprehensive evaluation of\nour proposed influential PN (IPNet) is performed by comparing its performance\nwith other baseline PNs on three different benchmark dermatological datasets.\nIPNet outperforms all baseline models with compelling results across all three\ndatasets and various N-way, K-shot classification tasks. Findings from\ncross-domain adaptation experiments further establish the robustness and\ngeneralizability of IPNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_R/0/1/0/all/0/1\">Ranjana Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bathula_D/0/1/0/all/0/1\">Deepti R. Bathula</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognizing Vector Graphics without Rasterization. (arXiv:2111.03281v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.03281","description":"<p>In this paper, we consider a different data format for images: vector\ngraphics. In contrast to raster graphics which are widely used in image\nrecognition, vector graphics can be scaled up or down into any resolution\nwithout aliasing or information loss, due to the analytic representation of the\nprimitives in the document. Furthermore, vector graphics are able to give extra\nstructural information on how low-level elements group together to form high\nlevel shapes or structures. These merits of graphic vectors have not been fully\nleveraged in existing methods. To explore this data format, we target on the\nfundamental recognition tasks: object localization and classification. We\npropose an efficient CNN-free pipeline that does not render the graphic into\npixels (i.e. rasterization), and takes textual document of the vector graphics\nas input, called YOLaT (You Only Look at Text). YOLaT builds multi-graphs to\nmodel the structural and spatial information in vector graphics, and a\ndual-stream graph neural network is proposed to detect objects from the graph.\nOur experiments show that by directly operating on vector graphics, YOLaT\nout-performs raster-graphic based object detection baselines in terms of both\naverage precision and efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caihua Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yifei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xuanyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongsheng Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethink Dilated Convolution for Real-time Semantic Segmentation. (arXiv:2111.09957v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.09957","description":"<p>Recent advances in semantic segmentation generally adapt an ImageNet\npretrained backbone with a special context module after it to quickly increase\nthe field-of-view. Although successful, the backbone, in which most of the\ncomputation lies, does not have a large enough field-of-view to make the best\ndecisions. Some recent advances tackle this problem by rapidly downsampling the\nresolution in the backbone while also having one or more parallel branches with\nhigher resolutions. We take a different approach by designing a ResNeXt\ninspired block structure that uses two parallel 3x3 convolutional layers with\ndifferent dilation rates to increase the field-of-view while also preserving\nthe local details. By repeating this block structure in the backbone, we do not\nneed to append any special context module after it. In addition, we propose a\nlightweight decoder that restores local information better than common\nalternatives. To demonstrate the effectiveness of our approach, our model\nRegSeg achieves state-of-the-art results on real-time Cityscapes and CamVid\ndatasets. Using a T4 GPU with mixed precision, RegSeg achieves 78.3 mIOU on\nCityscapes test set at 30 FPS, and 80.9 mIOU on CamVid test set at 70 FPS, both\nwithout ImageNet pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Roland Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoised Internal Models: a Brain-Inspired Autoencoder against Adversarial Attacks. (arXiv:2111.10844v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10844","description":"<p>Despite its great success, deep learning severely suffers from robustness;\nthat is, deep neural networks are very vulnerable to adversarial attacks, even\nthe simplest ones. Inspired by recent advances in brain science, we propose the\nDenoised Internal Models (DIM), a novel generative autoencoder-based model to\ntackle this challenge. Simulating the pipeline in the human brain for visual\nsignal processing, DIM adopts a two-stage approach. In the first stage, DIM\nuses a denoiser to reduce the noise and the dimensions of inputs, reflecting\nthe information pre-processing in the thalamus. Inspired from the sparse coding\nof memory-related traces in the primary visual cortex, the second stage\nproduces a set of internal models, one for each category. We evaluate DIM over\n42 adversarial attacks, showing that DIM effectively defenses against all the\nattacks and outperforms the SOTA on the overall robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yurui Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiachen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chunxu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jisong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer Vision User Entity Behavior Analytics. (arXiv:2111.13176v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13176","description":"<p>Insider threats are costly, hard to detect, and unfortunately rising in\noccurrence. Seeking to improve detection of such threats, we develop novel\ntechniques to enable us to extract powerful features, generate high quality\nimage encodings, and augment attack vectors for greater classification power.\nCombined, they form Computer Vision User and Entity Behavior Analytics, a\ndetection system designed from the ground up to improve upon advancements in\nacademia and mitigate the issues that prevent the usage of advanced models in\nindustry. The proposed system beats state-of-art methods used in academia and\nas well as in industry.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1\">Sameer Khanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Well Do Sparse Imagenet Models Transfer?. (arXiv:2111.13445v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.13445","description":"<p>Transfer learning is a classic paradigm by which models pretrained on large\n\"upstream\" datasets are adapted to yield good results on \"downstream,\"\nspecialized datasets. Generally, it is understood that more accurate models on\nthe \"upstream\" dataset will provide better transfer accuracy \"downstream\". In\nthis work, we perform an in-depth investigation of this phenomenon in the\ncontext of convolutional neural networks (CNNs) trained on the ImageNet\ndataset, which have been pruned - that is, compressed by sparsifiying their\nconnections. Specifically, we consider transfer using unstructured pruned\nmodels obtained by applying several state-of-the-art pruning methods, including\nmagnitude-based, second-order, re-growth and regularization approaches, in the\ncontext of twelve standard transfer tasks. In a nutshell, our study shows that\nsparse models can match or even outperform the transfer performance of dense\nmodels, even at high sparsities, and, while doing so, can lead to significant\ninference and even training speedups. At the same time, we observe and analyze\nsignificant differences in the behaviour of different pruning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1\">Eugenia Iofinova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1\">Alexandra Peste</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1\">Mark Kurtz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High Quality Segmentation for Ultra High-resolution Images. (arXiv:2111.14482v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.14482","description":"<p>To segment 4K or 6K ultra high-resolution images needs extra computation\nconsideration in image segmentation. Common strategies, such as down-sampling,\npatch cropping, and cascade model, cannot address well the balance issue\nbetween accuracy and computation cost. Motivated by the fact that humans\ndistinguish among objects continuously from coarse to precise levels, we\npropose the Continuous Refinement Model~(CRM) for the ultra high-resolution\nsegmentation refinement task. CRM continuously aligns the feature map with the\nrefinement target and aggregates features to reconstruct these images' details.\nBesides, our CRM shows its significant generalization ability to fill the\nresolution gap between low-resolution training images and ultra high-resolution\ntesting ones. We present quantitative performance evaluation and visualization\nto show that our proposed method is fast and effective on image segmentation\nrefinement. Code will be released at https://github.com/dvlab-research/Entity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tiancheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuechen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuen_J/0/1/0/all/0/1\">Jason Kuen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xingyu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jianlong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label-Efficient Semantic Segmentation with Diffusion Models. (arXiv:2112.03126v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03126","description":"<p>Denoising diffusion probabilistic models have recently received much research\nattention since they outperform alternative approaches, such as GANs, and\ncurrently provide state-of-the-art generative performance. The superior\nperformance of diffusion models has made them an appealing tool in several\napplications, including inpainting, super-resolution, and semantic editing. In\nthis paper, we demonstrate that diffusion models can also serve as an\ninstrument for semantic segmentation, especially in the setup when labeled data\nis scarce. In particular, for several pretrained diffusion models, we\ninvestigate the intermediate activations from the networks that perform the\nMarkov step of the reverse diffusion process. We show that these activations\neffectively capture the semantic information from an input image and appear to\nbe excellent pixel-level representations for the segmentation problem. Based on\nthese observations, we describe a simple segmentation method, which can work\neven if only a few training images are provided. Our approach significantly\noutperforms the existing alternatives on several datasets for the same amount\nof human supervision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1\">Dmitry Baranchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubachev_I/0/1/0/all/0/1\">Ivan Rubachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1\">Andrey Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khrulkov_V/0/1/0/all/0/1\">Valentin Khrulkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1\">Artem Babenko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Diffusion Models for Implicit Image Segmentation Ensembles. (arXiv:2112.03145v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03145","description":"<p>Diffusion models have shown impressive performance for generative modelling\nof images. In this paper, we present a novel semantic segmentation method based\non diffusion models. By modifying the training and sampling scheme, we show\nthat diffusion models can perform lesion segmentation of medical images. To\ngenerate an image specific segmentation, we train the model on the ground truth\nsegmentation, and use the image as a prior during training and in every step\nduring the sampling process. With the given stochastic sampling process, we can\ngenerate a distribution of segmentation masks. This property allows us to\ncompute pixel-wise uncertainty maps of the segmentation, and allows an implicit\nensemble of segmentations that increases the segmentation performance. We\nevaluate our method on the BRATS2020 dataset for brain tumor segmentation.\nCompared to state-of-the-art segmentation models, our approach yields good\nsegmentation results and, additionally, detailed uncertainty maps.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wolleb_J/0/1/0/all/0/1\">Julia Wolleb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sandkuhler_R/0/1/0/all/0/1\">Robin Sandk&#xfc;hler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Florentin Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valmaggia_P/0/1/0/all/0/1\">Philippe Valmaggia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cattin_P/0/1/0/all/0/1\">Philippe C. Cattin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Top-Down Deep Clustering with Multi-generator GANs. (arXiv:2112.03398v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.03398","description":"<p>Deep clustering (DC) leverages the representation power of deep architectures\nto learn embedding spaces that are optimal for cluster analysis. This approach\nfilters out low-level information irrelevant for clustering and has proven\nremarkably successful for high dimensional data spaces. Some DC methods employ\nGenerative Adversarial Networks (GANs), motivated by the powerful latent\nrepresentations these models are able to learn implicitly. In this work, we\npropose HC-MGAN, a new technique based on GANs with multiple generators\n(MGANs), which have not been explored for clustering. Our method is inspired by\nthe observation that each generator of a MGAN tends to generate data that\ncorrelates with a sub-region of the real data distribution. We use this\nclustered generation to train a classifier for inferring from which generator a\ngiven image came from, thus providing a semantically meaningful clustering for\nthe real distribution. Additionally, we design our method so that it is\nperformed in a top-down hierarchical clustering tree, thus proposing the first\nhierarchical DC method, to the best of our knowledge. We conduct several\nexperiments to evaluate the proposed method against recent DC methods,\nobtaining competitive results. Last, we perform an exploratory analysis of the\nhierarchical clustering tree that highlights how accurately it organizes the\ndata in a hierarchy of semantically coherent patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mello_D/0/1/0/all/0/1\">Daniel de Mello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assuncao_R/0/1/0/all/0/1\">Renato Assun&#xe7;&#xe3;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murai_F/0/1/0/all/0/1\">Fabricio Murai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Activation to Saliency: Forming High-Quality Labels for Completely Unsupervised Salient Object Detection. (arXiv:2112.03650v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.03650","description":"<p>Existing deep learning-based Unsupervised Salient Object Detection (USOD)\nmethods rely on supervised pre-trained deep models. Moreover, they generate\npseudo labels based on hand-crafted features, which lack high-level semantic\ninformation. In order to overcome these shortcomings, we propose a new\ntwo-stage Activation-to-Saliency (A2S) framework that effectively excavates\nhigh-quality saliency cues to train a robust saliency detector. It is worth\nnoting that our method does not require any manual annotation, even in the\npre-training phase. In the first stage, we transform an unsupervisedly\npre-trained network to aggregate multi-level features to a single activation\nmap, where an Adaptive Decision Boundary (ADB) is proposed to assist the\ntraining of the transformed network. Moreover, a new loss function is proposed\nto facilitate the generation of high-quality pseudo labels. In the second\nstage, a self-rectification learning paradigm strategy is developed to train a\nsaliency detector and refine the pseudo labels online. In addition, we\nconstruct a lightweight saliency detector using two Residual Attention Modules\n(RAMs) to largely reduce the risk of overfitting. Extensive experiments on\nseveral SOD benchmarks prove that our framework reports significant performance\ncompared with existing USOD methods. Moreover, training our framework on 3,000\nimages consumes about 1 hour, which is over 30$\\times$ faster than previous\nstate-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huajun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peijia Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lingxiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_J/0/1/0/all/0/1\">Jianhuang Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaohua Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MinkLoc3D-SI: 3D LiDAR place recognition with sparse convolutions, spherical coordinates, and intensity. (arXiv:2112.06539v2 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2112.06539","description":"<p>The 3D LiDAR place recognition aims to estimate a coarse localization in a\npreviously seen environment based on a single scan from a rotating 3D LiDAR\nsensor. The existing solutions to this problem include hand-crafted point cloud\ndescriptors (e.g., ScanContext, M2DP, LiDAR IRIS) and deep learning-based\nsolutions (e.g., PointNetVLAD, PCAN, LPDNet, DAGC, MinkLoc3D), which are often\nonly evaluated on accumulated 2D scans from the Oxford RobotCar dataset. We\nintroduce MinkLoc3D-SI, a sparse convolution-based solution that utilizes\nspherical coordinates of 3D points and processes the intensity of 3D LiDAR\nmeasurements, improving the performance when a single 3D LiDAR scan is used.\nOur method integrates the improvements typical for hand-crafted descriptors\n(like ScanContext) with the most efficient 3D sparse convolutions (MinkLoc3D).\nOur experiments show improved results on single scans from 3D LiDARs (USyd\nCampus dataset) and great generalization ability (KITTI dataset). Using\nintensity information on accumulated 2D scans (RobotCar Intensity dataset)\nimproves the performance, even though spherical representation doesn't produce\na noticeable improvement. As a result, MinkLoc3D-SI is suited for single scans\nobtained from a 3D LiDAR, making it applicable in autonomous vehicles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zywanowski_K/0/1/0/all/0/1\">Kamil &#x17b;ywanowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banaszczyk_A/0/1/0/all/0/1\">Adam Banaszczyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowicki_M/0/1/0/all/0/1\">Micha&#x142; R. Nowicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorowski_J/0/1/0/all/0/1\">Jacek Komorowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ensemble CNN Networks for GBM Tumors Segmentation using Multi-parametric MRI. (arXiv:2112.06554v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06554","description":"<p>Glioblastomas are the most aggressive fast-growing primary brain cancer which\noriginate in the glial cells of the brain. Accurate identification of the\nmalignant brain tumor and its sub-regions is still one of the most challenging\nproblems in medical image segmentation. The Brain Tumor Segmentation Challenge\n(BraTS) has been a popular benchmark for automatic brain glioblastomas\nsegmentation algorithms since its initiation. In this year, BraTS 2021\nchallenge provides the largest multi-parametric (mpMRI) dataset of 2,000\npre-operative patients. In this paper, we propose a new aggregation of two deep\nlearning frameworks namely, DeepSeg and nnU-Net for automatic glioblastoma\nrecognition in pre-operative mpMRI. Our ensemble method obtains Dice similarity\nscores of 92.00, 87.33, and 84.10 and Hausdorff Distances of 3.81, 8.91, and\n16.02 for the enhancing tumor, tumor core, and whole tumor regions,\nrespectively, on the BraTS 2021 validation set, ranking us among the top ten\nteams. These experimental findings provide evidence that it can be readily\napplied clinically and thereby aiding in the brain cancer prognosis, therapy\nplanning, and therapy response monitoring. A docker image for reproducing our\nsegmentation results is available online at\n(https://hub.docker.com/r/razeineldin/deepseg21).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeineldin_R/0/1/0/all/0/1\">Ramy A. Zeineldin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karar_M/0/1/0/all/0/1\">Mohamed E. Karar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathis_Ullrich_F/0/1/0/all/0/1\">Franziska Mathis-Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burgert_O/0/1/0/all/0/1\">Oliver Burgert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple and Robust Loss Design for Multi-Label Learning with Missing Labels. (arXiv:2112.07368v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.07368","description":"<p>Multi-label learning in the presence of missing labels (MLML) is a\nchallenging problem. Existing methods mainly focus on the design of network\nstructures or training schemes, which increase the complexity of\nimplementation. This work seeks to fulfill the potential of loss function in\nMLML without increasing the procedure and complexity. Toward this end, we\npropose two simple yet effective methods via robust loss design based on an\nobservation that a model can identify missing labels during training with a\nhigh precision. The first is a novel robust loss for negatives, namely the Hill\nloss, which re-weights negatives in the shape of a hill to alleviate the effect\nof false negatives. The second is a self-paced loss correction (SPLC) method,\nwhich uses a loss derived from the maximum likelihood criterion under an\napproximate distribution of missing labels. Comprehensive experiments on a vast\nrange of multi-label image classification datasets demonstrate that our methods\ncan remarkably boost the performance of MLML and achieve new state-of-the-art\nloss functions in MLML.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Youcai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yuhao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xinyu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Fei Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Rui Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaqian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yandong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Homography Decomposition Networks for Planar Object Tracking. (arXiv:2112.07909v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07909","description":"<p>Planar object tracking plays an important role in AI applications, such as\nrobotics, visual servoing, and visual SLAM. Although the previous planar\ntrackers work well in most scenarios, it is still a challenging task due to the\nrapid motion and large transformation between two consecutive frames. The\nessential reason behind this problem is that the condition number of such a\nnon-linear system changes unstably when the searching range of the homography\nparameter space becomes larger. To this end, we propose a novel Homography\nDecomposition Networks(HDN) approach that drastically reduces and stabilizes\nthe condition number by decomposing the homography transformation into two\ngroups. Specifically, a similarity transformation estimator is designed to\npredict the first group robustly by a deep convolution equivariant network. By\ntaking advantage of the scale and rotation estimation with high confidence, a\nresidual transformation is estimated by a simple regression model. Furthermore,\nthe proposed end-to-end network is trained in a semi-supervised fashion.\nExtensive experiments show that our proposed approach outperforms the\nstate-of-the-art planar tracking methods at a large margin on the challenging\nPOT, UCSB and POIC datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_X/0/1/0/all/0/1\">Xinrui Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueran Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeePaste -- Inpainting for Pasting. (arXiv:2112.10600v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10600","description":"<p>One of the challenges of supervised learning training is the need to procure\nan substantial amount of tagged data. A well-known method of solving this\nproblem is to use synthetic data in a copy-paste fashion, so that we cut\nobjects and paste them onto relevant backgrounds. Pasting the objects naively\nresults in artifacts that cause models to give poor results on real data. We\npresent a new method for cleanly pasting objects on different backgrounds so\nthat the dataset created gives competitive performance on real data. The main\nemphasis is on the treatment of the border of the pasted object using\ninpainting. We show state-of-the-art results both on instance detection and\nforeground segmentation\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Werman_L/0/1/0/all/0/1\">Levi Kassel Michael Werman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPViT: Multi-Path Vision Transformer for Dense Prediction. (arXiv:2112.11010v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11010","description":"<p>Dense computer vision tasks such as object detection and segmentation require\neffective multi-scale feature representation for detecting or classifying\nobjects or regions with varying sizes. While Convolutional Neural Networks\n(CNNs) have been the dominant architectures for such tasks, recently introduced\nVision Transformers (ViTs) aim to replace them as a backbone. Similar to CNNs,\nViTs build a simple multi-stage structure (i.e., fine-to-coarse) for\nmulti-scale representation with single-scale patches. In this work, with a\ndifferent perspective from existing Transformers, we explore multi-scale patch\nembedding and multi-path structure, constructing the Multi-Path Vision\nTransformer (MPViT). MPViT embeds features of the same size~(i.e., sequence\nlength) with patches of different scales simultaneously by using overlapping\nconvolutional patch embedding. Tokens of different scales are then\nindependently fed into the Transformer encoders via multiple paths and the\nresulting features are aggregated, enabling both fine and coarse feature\nrepresentations at the same feature level. Thanks to the diverse, multi-scale\nfeature representations, our MPViTs scaling from tiny~(5M) to base~(73M)\nconsistently achieve superior performance over state-of-the-art Vision\nTransformers on ImageNet classification, object detection, instance\nsegmentation, and semantic segmentation. These extensive results demonstrate\nthat MPViT can serve as a versatile backbone network for various vision tasks.\nCode will be made publicly available at \\url{https://git.io/MPViT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jonghee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willette_J/0/1/0/all/0/1\">Jeff Willette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection. (arXiv:2112.11088v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11088","description":"<p>Recently, fusing the LiDAR point cloud and camera image to improve the\nperformance and robustness of 3D object detection has received more and more\nattention, as these two modalities naturally possess strong complementarity. In\nthis paper, we propose EPNet++ for multi-modal 3D object detection by\nintroducing a novel Cascade Bi-directional Fusion~(CB-Fusion) module and a\nMulti-Modal Consistency~(MC) loss. More concretely, the proposed CB-Fusion\nmodule boosts the plentiful semantic information of point features with the\nimage features in a cascade bi-directional interaction fusion manner, leading\nto more comprehensive and discriminative feature representations. The MC loss\nexplicitly guarantees the consistency between predicted scores from two\nmodalities to obtain more comprehensive and reliable confidence scores. The\nexperiment results on the KITTI, JRDB and SUN-RGBD datasets demonstrate the\nsuperiority of EPNet++ over the state-of-the-art methods. Besides, we emphasize\na critical but easily overlooked problem, which is to explore the performance\nand robustness of a 3D detector in a sparser scene. Extensive experiments\npresent that EPNet++ outperforms the existing SOTA methods with remarkable\nmargins in highly sparse point cloud cases, which might be an available\ndirection to reduce the expensive cost of LiDAR sensors. Code will be released\nin the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tengteng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Registration of Forest Point Clouds by Global Matching of Relative Stem Positions. (arXiv:2112.11121v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11121","description":"<p>Registering point clouds of forest environments is an essential prerequisite\nfor LiDAR applications in precision forestry. State-of-the-art methods for\nforest point cloud registration require the extraction of individual tree\nattributes, and they have an efficiency bottleneck when dealing with point\nclouds of real-world forests with dense trees. We propose an automatic, robust,\nand efficient method for the registration of forest point clouds. Our approach\nfirst locates tree stems from raw point clouds and then matches the stems based\non their relative spatial relationship to determine the registration\ntransformation. In contrast to existing methods, our algorithm requires no\nextra individual tree attributes and has linear complexity to the number of\ntrees in the environment, allowing it to align point clouds of large forest\nenvironments. Extensive experiments have revealed that our method is superior\nto the state-of-the-art methods regarding registration accuracy and robustness,\nand it significantly outperforms existing techniques in terms of efficiency.\nBesides, we introduce a new benchmark dataset that complements the very few\nexisting open datasets for the development and evaluation of registration\nmethods for forest point clouds. The source code of our method and the dataset\nare available at https://github.com/zexinyang/AlignTree.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zexin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaojun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoter_J/0/1/0/all/0/1\">Jantien Stoter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenlun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YOLO-Z: Improving small object detection in YOLOv5 for autonomous vehicles. (arXiv:2112.11798v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11798","description":"<p>As autonomous vehicles and autonomous racing rise in popularity, so does the\nneed for faster and more accurate detectors. While our naked eyes are able to\nextract contextual information almost instantly, even from far away, image\nresolution and computational resources limitations make detecting smaller\nobjects (that is, objects that occupy a small pixel area in the input image) a\ngenuinely challenging task for machines and a wide-open research field. This\nstudy explores how the popular YOLOv5 object detector can be modified to\nimprove its performance in detecting smaller objects, with a particular\napplication in autonomous racing. To achieve this, we investigate how replacing\ncertain structural elements of the model (as well as their connections and\nother parameters) can affect performance and inference time. In doing so, we\npropose a series of models at different scales, which we name `YOLO-Z', and\nwhich display an improvement of up to 6.9% in mAP when detecting smaller\nobjects at 50% IOU, at the cost of just a 3ms increase in inference time\ncompared to the original YOLOv5. Our objective is to inform future research on\nthe potential of adjusting a popular detector such as YOLOv5 to address\nspecific tasks and provide insights on how specific changes can impact small\nobject detection. Such findings, applied to the broader context of autonomous\nvehicles, could increase the amount of contextual information available to such\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benjumea_A/0/1/0/all/0/1\">Aduen Benjumea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1\">Izzeddin Teeti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1\">Fabio Cuzzolin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1\">Andrew Bradley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Discriminative Single-Shot Segmentation Network for Visual Object Tracking. (arXiv:2112.11846v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11846","description":"<p>Template-based discriminative trackers are currently the dominant tracking\nparadigm due to their robustness, but are restricted to bounding box tracking\nand a limited range of transformation models, which reduces their localization\naccuracy. We propose a discriminative single-shot segmentation tracker -- D3S2,\nwhich narrows the gap between visual object tracking and video object\nsegmentation. A single-shot network applies two target models with\ncomplementary geometric properties, one invariant to a broad range of\ntransformations, including non-rigid deformations, the other assuming a rigid\nobject to simultaneously achieve robust online target segmentation. The overall\ntracking reliability is further increased by decoupling the object and feature\nscale estimation. Without per-dataset finetuning, and trained only for\nsegmentation as the primary output, D3S2 outperforms all published trackers on\nthe recent short-term tracking benchmark VOT2020 and performs very close to the\nstate-of-the-art trackers on the GOT-10k, TrackingNet, OTB100 and LaSoT. D3S2\noutperforms the leading segmentation tracker SiamMask on video object\nsegmentation benchmarks and performs on par with top video object segmentation\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lukezic_A/0/1/0/all/0/1\">Alan Luke&#x17e;i&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1\">Ji&#x159;&#xed; Matas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1\">Matej Kristan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual Path Structural Contrastive Embeddings for Learning Novel Objects. (arXiv:2112.12359v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12359","description":"<p>Learning novel classes from a very few labeled samples has attracted\nincreasing attention in machine learning areas. Recent research on either\nmeta-learning based or transfer-learning based paradigm demonstrates that\ngaining information on a good feature space can be an effective solution to\nachieve favorable performance on few-shot tasks. In this paper, we propose a\nsimple but effective paradigm that decouples the tasks of learning feature\nrepresentations and classifiers and only learns the feature embedding\narchitecture from base classes via the typical transfer-learning training\nstrategy. To maintain both the generalization ability across base and novel\nclasses and discrimination ability within each class, we propose a dual path\nfeature learning scheme that effectively combines structural similarity with\ncontrastive feature construction. In this way, both inner-class alignment and\ninter-class uniformity can be well balanced, and result in improved\nperformance. Experiments on three popular benchmarks show that when\nincorporated with a simple prototype based classifier, our method can still\nachieve promising results for both standard and generalized few-shot problems\nin either an inductive or transductive inference setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1\">Elvis Han Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yanan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Donghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1\">Weng Kee Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LaTr: Layout-Aware Transformer for Scene-Text VQA. (arXiv:2112.12494v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12494","description":"<p>We propose a novel multimodal architecture for Scene Text Visual Question\nAnswering (STVQA), named Layout-Aware Transformer (LaTr). The task of STVQA\nrequires models to reason over different modalities. Thus, we first investigate\nthe impact of each modality, and reveal the importance of the language module,\nespecially when enriched with layout information. Accounting for this, we\npropose a single objective pre-training scheme that requires only text and\nspatial cues. We show that applying this pre-training scheme on scanned\ndocuments has certain advantages over using natural images, despite the domain\ngap. Scanned documents are easy to procure, text-dense and have a variety of\nlayouts, helping the model learn various spatial cues (e.g. left-of, below\netc.) by tying together language and layout information. Compared to existing\napproaches, our method performs vocabulary-free decoding and, as shown,\ngeneralizes well beyond the training vocabulary. We further demonstrate that\nLaTr improves robustness towards OCR errors, a common reason for failure cases\nin STVQA. In addition, by leveraging a vision transformer, we eliminate the\nneed for an external object detector. LaTr outperforms state-of-the-art STVQA\nmethods on multiple datasets. In particular, +7.6% on TextVQA, +10.8% on ST-VQA\nand +4.0% on OCR-VQA (all absolute accuracy numbers).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Biten_A/0/1/0/all/0/1\">Ali Furkan Biten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_R/0/1/0/all/0/1\">Ron Litman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yusheng Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Appalaraju_S/0/1/0/all/0/1\">Srikar Appalaraju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manmatha_R/0/1/0/all/0/1\">R. Manmatha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BANMo: Building Animatable 3D Neural Models from Many Casual Videos. (arXiv:2112.12761v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.12761","description":"<p>Prior work for articulated 3D shape reconstruction often relies on\nspecialized sensors (e.g., synchronized multi-camera systems), or pre-built 3D\ndeformable models (e.g., SMAL or SMPL). Such methods are not able to scale to\ndiverse sets of objects in the wild. We present BANMo, a method that requires\nneither a specialized sensor nor a pre-defined template shape. BANMo builds\nhigh-fidelity, articulated 3D models (including shape and animatable skinning\nweights) from many monocular casual videos in a differentiable rendering\nframework. While the use of many videos provides more coverage of camera views\nand object articulations, they introduce significant challenges in establishing\ncorrespondence across scenes with different backgrounds, illumination\nconditions, etc. Our key insight is to merge three schools of thought; (1)\nclassic deformable shape models that make use of articulated bones and blend\nskinning, (2) volumetric neural radiance fields (NeRFs) that are amenable to\ngradient-based optimization, and (3) canonical embeddings that generate\ncorrespondences between pixels and an articulated model. We introduce neural\nblend skinning models that allow for differentiable and invertible articulated\ndeformations. When combined with canonical embeddings, such models allow us to\nestablish dense correspondences across videos that can be self-supervised with\ncycle consistency. On real and synthetic datasets, BANMo shows higher-fidelity\n3D reconstructions than prior works for humans and animals, with the ability to\nrender realistic images from novel viewpoints and poses. Project webpage:\nbanmo-www.github.io .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Gengshan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_M/0/1/0/all/0/1\">Minh Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1\">Deva Ramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_H/0/1/0/all/0/1\">Hanbyul Joo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-27T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}