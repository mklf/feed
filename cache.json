{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-02-02T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Learning Fair Representations via Rate-Distortion Maximization. (arXiv:2202.00035v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00035","description":"<p>Text representations learned by machine learning models often encode\nundesirable demographic information of the user. Predictive models based on\nthese representations can rely on such information resulting in biased\ndecisions. We present a novel debiasing technique Fairness-aware Rate\nMaximization (FaRM), that removes demographic information by making\nrepresentations of instances belonging to the same protected attribute class\nuncorrelated using the rate-distortion function. FaRM is able to debias\nrepresentations with or without a target task at hand. FaRM can also be adapted\nto simultaneously remove information about multiple protected attributes.\nEmpirical evaluations show that FaRM achieves state-of-the-art performance on\nseveral datasets, and learned representations leak significantly less protected\nattribute information against an attack by a non-linear probing network.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning affective meanings that derives the social behavior using Bidirectional Encoder Representations from Transformers. (arXiv:2202.00065v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00065","description":"<p>Predicting the outcome of a process requires modeling the system dynamic and\nobserving the states. In the context of social behaviors, sentiments\ncharacterize the states of the system. Affect Control Theory (ACT) uses\nsentiments to manifest potential interaction. ACT is a generative theory of\nculture and behavior based on a three-dimensional sentiment lexicon.\nTraditionally, the sentiments are quantified using survey data which is fed\ninto a regression model to explain social behavior. The lexicons used in the\nsurvey are limited due to prohibitive cost. This paper uses a fine-tuned\nBidirectional Encoder Representations from Transformers (BERT) model to develop\na replacement for these surveys. This model achieves state-of-the-art accuracy\nin estimating affective meanings, expanding the affective lexicon, and allowing\nmore behaviors to be explained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mostafavi_M/0/1/0/all/0/1\">Moeen Mostafavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porter_M/0/1/0/all/0/1\">Michael D. Porter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_D/0/1/0/all/0/1\">Dawn T. Robinson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"QALD-9-plus: A Multilingual Dataset for Question Answering over DBpedia and Wikidata Translated by Native Speakers. (arXiv:2202.00120v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00120","description":"<p>The ability to have the same experience for different user groups (i.e.,\naccessibility) is one of the most important characteristics of Web-based\nsystems. The same is true for Knowledge Graph Question Answering (KGQA) systems\nthat provide the access to Semantic Web data via natural language interface.\nWhile following our research agenda on the multilingual aspect of accessibility\nof KGQA systems, we identified several ongoing challenges. One of them is the\nlack of multilingual KGQA benchmarks. In this work, we extend one of the most\npopular KGQA benchmarks - QALD-9 by introducing high-quality questions'\ntranslations to 8 languages provided by native speakers, and transferring the\nSPARQL queries of QALD-9 from DBpedia to Wikidata, s.t., the usability and\nrelevance of the dataset is strongly increased. Five of the languages -\nArmenian, Ukrainian, Lithuanian, Bashkir and Belarusian - to our best knowledge\nwere never considered in KGQA research community before. The latter two of the\nlanguages are considered as \"endangered\" by UNESCO. We call the extended\ndataset QALD-9-plus and made it available online\nhttps://github.com/Perevalov/qald_9_plus.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Perevalov_A/0/1/0/all/0/1\">Aleksandr Perevalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diefenbach_D/0/1/0/all/0/1\">Dennis Diefenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1\">Ricardo Usbeck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Both_A/0/1/0/all/0/1\">Andreas Both</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Annotation and Querying Framework based on Semi-structured Ayurvedic Text. (arXiv:2202.00216v1 [cs.IR])","link":"http://arxiv.org/abs/2202.00216","description":"<p>Knowledge bases (KB) are an important resource in a number of natural\nlanguage processing (NLP) and information retrieval (IR) tasks, such as\nsemantic search, automated question-answering etc. They are also useful for\nresearchers trying to gain information from a text. Unfortunately, however, the\nstate-of-the-art in Sanskrit NLP does not yet allow automated construction of\nknowledge bases due to unavailability or lack of sufficient accuracy of tools\nand methods. Thus, in this work, we describe our efforts on manual annotation\nof Sanskrit text for the purpose of knowledge graph (KG) creation. We choose\nthe chapter Dhanyavarga from Bhavaprakashanighantu of the Ayurvedic text\nBhavaprakasha for annotation. The constructed knowledge graph contains 410\nentities and 764 relationships. Since Bhavaprakashanighantu is a technical\nglossary text that describes various properties of different substances, we\ndevelop an elaborate ontology to capture the semantics of the entity and\nrelationship types present in the text. To query the knowledge graph, we design\n31 query templates that cover most of the common question patterns. For both\nmanual annotation and querying, we customize the Sangrahaka framework\npreviously developed by us. The entire system including the dataset is\navailable from https://sanskrit.iitk.ac.in/ayurveda/ . We hope that the\nknowledge graph that we have created through manual annotation and subsequent\ncuration will help in development and testing of NLP tools in future as well as\nstudying of the Bhavaprakasanighantu text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Terdalkar_H/0/1/0/all/0/1\">Hrishikesh Terdalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arnab Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubey_M/0/1/0/all/0/1\">Madhulika Dubey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+S_R/0/1/0/all/0/1\">Ramamurthy S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Bhavna Naneria Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebFormer: The Web-page Transformer for Structure Information Extraction. (arXiv:2202.00217v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00217","description":"<p>Structure information extraction refers to the task of extracting structured\ntext fields from web pages, such as extracting a product offer from a shopping\npage including product title, description, brand and price. It is an important\nresearch topic which has been widely studied in document understanding and web\nsearch. Recent natural language models with sequence modeling have demonstrated\nstate-of-the-art performance on web information extraction. However,\neffectively serializing tokens from unstructured web pages is challenging in\npractice due to a variety of web layout patterns. Limited work has focused on\nmodeling the web layout for extracting the text fields. In this paper, we\nintroduce WebFormer, a Web-page transFormer model for structure information\nextraction from web documents. First, we design HTML tokens for each DOM node\nin the HTML by embedding representations from their neighboring tokens through\ngraph attention. Second, we construct rich attention patterns between HTML\ntokens and text tokens, which leverages the web layout for effective attention\nweight computation. We conduct an extensive set of experiments on SWDE and\nCommon Crawl benchmarks. Experimental results demonstrate the superior\nperformance of the proposed approach over several state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yi Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravula_A/0/1/0/all/0/1\">Anirudh Ravula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fuli Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongfang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning Over Multiple Domains in Natural Language Tasks. (arXiv:2202.00254v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00254","description":"<p>Studies of active learning traditionally assume the target and source data\nstem from a single domain. However, in realistic applications, practitioners\noften require active learning with multiple sources of out-of-distribution\ndata, where it is unclear a priori which data sources will help or hurt the\ntarget domain. We survey a wide variety of techniques in active learning (AL),\ndomain shift detection (DS), and multi-domain sampling to examine this\nchallenging setting for question answering and sentiment analysis. We ask (1)\nwhat family of methods are effective for this task? And, (2) what properties of\nselected examples and domains achieve strong results? Among 18 acquisition\nfunctions from 4 families of methods, we find H- Divergence methods, and\nparticularly our proposed variant DAL-E, yield effective results, averaging\n2-3% improvements over the random baseline. We also show the importance of a\ndiverse allocation of domains, as well as room-for-improvement of existing\nmethods on both domain and example selection. Our findings yield the first\ncomprehensive analysis of both existing and novel methods for practitioners\nfaced with multi-domain active learning for natural language tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisler_J/0/1/0/all/0/1\">Julia Reisler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_E/0/1/0/all/0/1\">Edward Greg Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yi Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Andrew Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XAlign: Cross-lingual Fact-to-Text Alignment and Generation for Low-Resource Languages. (arXiv:2202.00291v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00291","description":"<p>Multiple critical scenarios (like Wikipedia text generation given English\nInfoboxes) need automated generation of descriptive text in low resource (LR)\nlanguages from English fact triples. Previous work has focused on English\nfact-to-text (F2T) generation. To the best of our knowledge, there has been no\nprevious attempt on cross-lingual alignment or generation for LR languages.\nBuilding an effective cross-lingual F2T (XF2T) system requires alignment\nbetween English structured facts and LR sentences. We propose two unsupervised\nmethods for cross-lingual alignment. We contribute XALIGN, an XF2T dataset with\n0.45M pairs across 8 languages, of which 5402 pairs have been manually\nannotated. We also train strong baseline XF2T generation models on the XAlign\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abhishek_T/0/1/0/all/0/1\">Tushar Abhishek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sagare_S/0/1/0/all/0/1\">Shivprasad Sagare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_B/0/1/0/all/0/1\">Bhavyajeet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anubhav Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Manish Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1\">Vasudeva Varma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Research on Question Classification Methods in the Medical Field. (arXiv:2202.00298v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00298","description":"<p>Question classification is one of the important links in the research of\nquestion and answering system. The existing question classification models are\nmore trained on public data sets. At present, there is a lack of question\nclassification data sets in specific fields, especially in the medical field.\nTo make up for this gap, this paper presents a data set for question\nclassification in the medical field. Moreover, this paper proposes a\nmulti-dimensional extraction of the characteristics of the question by\ncombining multiple neural network models, and proposes a question\nclassification model based on multi-dimensional feature extraction. The\nexperimental results show that the proposed method can effectively improve the\nperformance of question classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinzhang Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural Language to Code Using Transformers. (arXiv:2202.00367v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00367","description":"<p>We tackle the problem of generating code snippets from natural language\ndescriptions using the CoNaLa dataset. We use the self-attention based\ntransformer architecture and show that it performs better than recurrent\nattention-based encoder decoder. Furthermore, we develop a modified form of\nback translation and use cycle consistent losses to train the model in an\nend-to-end fashion. We achieve a BLEU score of 16.99 beating the previously\nreported baseline of the CoNaLa challenge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_U/0/1/0/all/0/1\">Uday Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ailavarapu_V/0/1/0/all/0/1\">Venkata Ravi Teja Ailavarapu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Politics and Virality in the Time of Twitter: A Large-Scale Cross-Party Sentiment Analysis in Greece, Spain and United Kingdom. (arXiv:2202.00396v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00396","description":"<p>Social media has become extremely influential when it comes to policy making\nin modern societies especially in the western world (e.g., 48% of Europeans use\nsocial media every day or almost every day). Platforms such as Twitter allow\nusers to follow politicians, thus making citizens more involved in political\ndiscussion. In the same vein, politicians use Twitter to express their\nopinions, debate among others on current topics and promote their political\nagenda aiming to influence voter behaviour. Previous studies have shown that\ntweets conveying negative sentiment are likely to be retweeted more frequently.\nIn this paper, we attempt to analyse tweets from politicians from different\ncountries and explore if their tweets follow the same trend. Utilising\nstate-of-the-art pre-trained language models we performed sentiment analysis on\nmultilingual tweets collected from members of parliament of Greece, Spain and\nUnited Kingdom, including devolved administrations. We achieved this by\nsystematically exploring and analysing the differences between influential and\nless popular tweets. Our analysis indicates that politicians' negatively\ncharged tweets spread more widely, especially in more recent times, and\nhighlights interesting trends in the intersection of sentiment and popularity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1\">Dimosthenis Antypas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1\">Alun Preece</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collados_J/0/1/0/all/0/1\">Jose Camacho Collados</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Dependencies in Adversarial Attacks on Speech Recognition Systems. (arXiv:2202.00399v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00399","description":"<p>Automatic speech recognition (ASR) systems are ubiquitously present in our\ndaily devices. They are vulnerable to adversarial attacks, where manipulated\ninput samples fool the ASR system's recognition. While adversarial examples for\nvarious English ASR systems have already been analyzed, there exists no\ninter-language comparative vulnerability analysis.\n</p>\n<p>We compare the attackability of a German and an English ASR system, taking\nDeepspeech as an example. We investigate if one of the language models is more\nsusceptible to manipulations than the other. The results of our experiments\nsuggest statistically significant differences between English and German in\nterms of computational effort necessary for the successful generation of\nadversarial examples. This result encourages further research in\nlanguage-dependent characteristics in the robustness analysis of ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Markert_K/0/1/0/all/0/1\">Karla Markert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirdita_D/0/1/0/all/0/1\">Donika Mirdita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bottinger_K/0/1/0/all/0/1\">Konstantin B&#xf6;ttinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Inference Principles for Reasoning about Commonsense Causality. (arXiv:2202.00436v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00436","description":"<p>Commonsense causality reasoning (CCR) aims at identifying plausible causes\nand effects in natural language descriptions that are deemed reasonable by an\naverage person. Although being of great academic and practical interest, this\nproblem is still shadowed by the lack of a well-posed theoretical framework;\nexisting work usually relies on deep language models wholeheartedly, and is\npotentially susceptible to confounding co-occurrences. Motivated by classical\ncausal principles, we articulate the central question of CCR and draw parallels\nbetween human subjects in observational studies and natural languages to adopt\nCCR to the potential-outcomes framework, which is the first such attempt for\ncommonsense tasks. We propose a novel framework, ROCK, to Reason O(A)bout\nCommonsense K(C)ausality, which utilizes temporal signals as incidental\nsupervision, and balances confounding effects using temporal propensities that\nare analogous to propensity scores. The ROCK implementation is modular and\nzero-shot, and demonstrates good CCR capabilities on various datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiayao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Text Anonymization Benchmark (TAB): A Dedicated Corpus and Evaluation Framework for Text Anonymization. (arXiv:2202.00443v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00443","description":"<p>We present a novel benchmark and associated evaluation metrics for assessing\nthe performance of text anonymization methods. Text anonymization, defined as\nthe task of editing a text document to prevent the disclosure of personal\ninformation, currently suffers from a shortage of privacy-oriented annotated\ntext resources, making it difficult to properly evaluate the level of privacy\nprotection offered by various anonymization methods. This paper presents TAB\n(Text Anonymization Benchmark), a new, open-source annotated corpus developed\nto address this shortage. The corpus comprises 1,268 English-language court\ncases from the European Court of Human Rights (ECHR) enriched with\ncomprehensive annotations about the personal information appearing in each\ndocument, including their semantic category, identifier type, confidential\nattributes, and co-reference relations. Compared to previous work, the TAB\ncorpus is designed to go beyond traditional de-identification (which is limited\nto the detection of predefined semantic categories), and explicitly marks which\ntext spans ought to be masked in order to conceal the identity of the person to\nbe protected. Along with presenting the corpus and its annotation layers, we\nalso propose a set of evaluation metrics that are specifically tailored towards\nmeasuring the performance of text anonymization, both in terms of privacy\nprotection and utility preservation. We illustrate the use of the benchmark and\nthe proposed metrics by assessing the empirical performance of several baseline\ntext anonymization models. The full corpus along with its privacy-oriented\nannotation guidelines, evaluation scripts and baseline models are available on:\nhttps://github.com/NorskRegnesentral/text-anonymisation-benchmark\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pilan_I/0/1/0/all/0/1\">Ildik&#xf3; Pil&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lison_P/0/1/0/all/0/1\">Pierre Lison</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovrelid_L/0/1/0/all/0/1\">Lilja &#xd8;vrelid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papadopoulou_A/0/1/0/all/0/1\">Anthi Papadopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1\">David S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batet_M/0/1/0/all/0/1\">Montserrat Batet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TableQuery: Querying tabular data with natural language. (arXiv:2202.00454v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00454","description":"<p>This paper presents TableQuery, a novel tool for querying tabular data using\ndeep learning models pre-trained to answer questions on free text. Existing\ndeep learning methods for question answering on tabular data have various\nlimitations, such as having to feed the entire table as input into a neural\nnetwork model, making them unsuitable for most real-world applications. Since\nreal-world data might contain millions of rows, it may not entirely fit into\nthe memory. Moreover, data could be stored in live databases, which are updated\nin real-time, and it is impractical to serialize an entire database to a neural\nnetwork-friendly format each time it is updated. In TableQuery, we use deep\nlearning models pre-trained for question answering on free text to convert\nnatural language queries to structured queries, which can be run against a\ndatabase or a spreadsheet. This method eliminates the need for fitting the\nentire data into memory as well as serializing databases. Furthermore, deep\nlearning models pre-trained for question answering on free text are readily\navailable on platforms such as HuggingFace Model Hub (7). TableQuery does not\nrequire re-training; when a newly trained model for question answering with\nbetter performance is available, it can replace the existing model in\nTableQuery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abraham_A/0/1/0/all/0/1\">Abhijith Neil Abraham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_F/0/1/0/all/0/1\">Fariz Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_D/0/1/0/all/0/1\">Damanpreet Kaur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Invariable Semantical Representation from Language for Extensible Policy Generalization. (arXiv:2202.00466v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00466","description":"<p>Recently, incorporating natural language instructions into reinforcement\nlearning (RL) to learn semantically meaningful representations and foster\ngeneralization has caught many concerns. However, the semantical information in\nlanguage instructions is usually entangled with task-specific state\ninformation, which hampers the learning of semantically invariant and reusable\nrepresentations. In this paper, we propose a method to learn such\nrepresentations called element randomization, which extracts task-relevant but\nenvironment-agnostic semantics from instructions using a set of environments\nwith randomized elements, e.g., topological structures or textures, yet the\nsame language instruction. We theoretically prove the feasibility of learning\nsemantically invariant representations through randomization. In practice, we\naccordingly develop a hierarchy of policies, where a high-level policy is\ndesigned to modulate the behavior of a goal-conditioned low-level policy by\nproposing subgoals as semantically invariant representations. Experiments on\nchallenging long-horizon tasks show that (1) our low-level policy reliably\ngeneralizes to tasks against environment changes; (2) our hierarchical policy\nexhibits extensible generalization in unseen new tasks that can be decomposed\ninto several solvable sub-tasks; and (3) by storing and replaying language\ntrajectories as succinct policy representations, the agent can complete tasks\nin a one-shot fashion, i.e., once one successful trajectory has been attained.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yihan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jinsheng Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianrun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianren Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1\">Haichuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unified Multimodal Punctuation Restoration Framework for Mixed-Modality Corpus. (arXiv:2202.00468v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00468","description":"<p>The punctuation restoration task aims to correctly punctuate the output\ntranscriptions of automatic speech recognition systems. Previous punctuation\nmodels, either using text only or demanding the corresponding audio, tend to be\nconstrained by real scenes, where unpunctuated sentences are a mixture of those\nwith and without audio. This paper proposes a unified multimodal punctuation\nrestoration framework, named UniPunc, to punctuate the mixed sentences with a\nsingle model. UniPunc jointly represents audio and non-audio samples in a\nshared latent space, based on which the model learns a hybrid representation\nand punctuates both kinds of samples. We validate the effectiveness of the\nUniPunc on real-world datasets, which outperforms various strong baselines\n(e.g. BERT, MuSe) by at least 0.8 overall F1 scores, making a new\nstate-of-the-art. Extensive experiments show that UniPunc's design is a\npervasive solution: by grafting onto previous models, UniPunc enables them to\npunctuate on the mixed corpus. Our code is available at\ngithub.com/Yaoming95/UniPunc\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liwei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shanbo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient-guided Unsupervised Text Style Transfer via Contrastive Learning. (arXiv:2202.00469v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00469","description":"<p>Text style transfer is a challenging text generation problem, which aims at\naltering the style of a given sentence to a target one while keeping its\ncontent unchanged. Since there is a natural scarcity of parallel datasets,\nrecent works mainly focus on solving the problem in an unsupervised manner.\nHowever, previous gradient-based works generally suffer from the deficiencies\nas follows, namely: (1) Content migration. Previous approaches lack explicit\nmodeling of content invariance and are thus susceptible to content shift\nbetween the original sentence and the transferred one. (2) Style\nmisclassification. A natural drawback of the gradient-guided approaches is that\nthe inference process is homogeneous with a line of adversarial attack, making\nlatent optimization easily becomes an attack to the classifier due to\nmisclassification. This leads to difficulties in achieving high transfer\naccuracy. To address the problems, we propose a novel gradient-guided model\nthrough a contrastive paradigm for text style transfer, to explicitly gather\nsimilar semantic sentences, and to design a siamese-structure based style\nclassifier for alleviating such two issues, respectively. Experiments on two\ndatasets show the effectiveness of our proposed approach, as compared to the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chenghao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+wei_W/0/1/0/all/0/1\">Wei wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Assessment of the Impact of OCR Noise on Language Models. (arXiv:2202.00470v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00470","description":"<p>Neural language models are the backbone of modern-day natural language\nprocessing applications. Their use on textual heritage collections which have\nundergone Optical Character Recognition (OCR) is therefore also increasing.\nNevertheless, our understanding of the impact OCR noise could have on language\nmodels is still limited. We perform an assessment of the impact OCR noise has\non a variety of language models, using data in Dutch, English, French and\nGerman. We find that OCR noise poses a significant obstacle to language\nmodelling, with language models increasingly diverging from their noiseless\ntargets as OCR quality lowers. In the presence of small corpora, simpler models\nincluding PPMI and Word2Vec consistently outperform transformer-based models in\nthis respect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Todorov_K/0/1/0/all/0/1\">Konstantin Todorov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colavizza_G/0/1/0/all/0/1\">Giovanni Colavizza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal effect of racial bias in data and machine learning algorithms on user persuasiveness & discriminatory decision making: An Empirical Study. (arXiv:2202.00471v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00471","description":"<p>Language data and models demonstrate various types of bias, be it ethnic,\nreligious, gender, or socioeconomic. AI/NLP models, when trained on the\nracially biased dataset, AI/NLP models instigate poor model explainability,\ninfluence user experience during decision making and thus further magnifies\nsocietal biases, raising profound ethical implications for society. The\nmotivation of the study is to investigate how AI systems imbibe bias from data\nand produce unexplainable discriminatory outcomes and influence an individual's\narticulateness of system outcome due to the presence of racial bias features in\ndatasets. The design of the experiment involves studying the counterfactual\nimpact of racial bias features present in language datasets and its associated\neffect on the model outcome. A mixed research methodology is adopted to\ninvestigate the cross implication of biased model outcome on user experience,\neffect on decision-making through controlled lab experimentation. The findings\nprovide foundation support for correlating the implication of carry-over an\nartificial intelligence model solving NLP task due to biased concept presented\nin the dataset. Further, the research outcomes justify the negative influence\non users' persuasiveness that leads to alter the decision-making quotient of an\nindividual when trying to rely on the model outcome to act. The paper bridges\nthe gap across the harm caused in establishing poor customer trustworthiness\ndue to an inequitable system design and provides strong support for\nresearchers, policymakers, and data scientists to build responsible AI\nframeworks within organizations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_K/0/1/0/all/0/1\">Kinshuk Sengupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_P/0/1/0/all/0/1\">Praveen Ranjan Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Examples to Rules: Neural Guided Rule Synthesis for Information Extraction. (arXiv:2202.00475v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00475","description":"<p>While deep learning approaches to information extraction have had many\nsuccesses, they can be difficult to augment or maintain as needs shift.\nRule-based methods, on the other hand, can be more easily modified. However,\ncrafting rules requires expertise in linguistics and the domain of interest,\nmaking it infeasible for most users. Here we attempt to combine the advantages\nof these two directions while mitigating their drawbacks. We adapt recent\nadvances from the adjacent field of program synthesis to information\nextraction, synthesizing rules from provided examples. We use a\ntransformer-based architecture to guide an enumerative search, and show that\nthis reduces the number of steps that need to be explored before a rule is\nfound. Further, we show that without training the synthesis algorithm on the\nspecific domain, our synthesized rules achieve state-of-the-art performance on\nthe 1-shot scenario of a task that focuses on few-shot learning for relation\nclassification, and competitive performance in the 5-shot scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vacareanu_R/0/1/0/all/0/1\">Robert Vacareanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenzuela_Escarcega_M/0/1/0/all/0/1\">Marco A. Valenzuela-Escarcega</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barbosa_G/0/1/0/all/0/1\">George C. G. Barbosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharp_R/0/1/0/all/0/1\">Rebecca Sharp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring COVID-19 Related Stressors Using Topic Modeling. (arXiv:2202.00476v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00476","description":"<p>The COVID-19 pandemic has affected lives of people from different countries\nfor almost two years. The changes on lifestyles due to the pandemic may cause\npsychosocial stressors for individuals, and have a potential to lead to mental\nhealth problems. To provide high quality mental health supports, healthcare\norganization need to identify the COVID-19 specific stressors, and notice the\ntrends of prevalence of those stressors. This study aims to apply natural\nlanguage processing (NLP) on social media data to identify the psychosocial\nstressors during COVID-19 pandemic, and to analyze the trend on prevalence of\nstressors at different stages of the pandemic. We obtained dataset of 9266\nReddit posts from subreddit \\rCOVID19_support, from 14th Feb ,2020 to 19th July\n2021. We used Latent Dirichlet Allocation (LDA) topic model and lexicon methods\nto identify the topics that were mentioned on the subreddit. Our result\npresented a dashboard to visualize the trend of prevalence of topics about\ncovid-19 related stressors being discussed on social media platform. The result\ncould provide insights about the prevalence of pandemic related stressors\nduring different stages of COVID-19. The NLP techniques leveraged in this study\ncould also be applied to analyze event specific stressors in the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leung_Y/0/1/0/all/0/1\">Yue Tong Leung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalvati_F/0/1/0/all/0/1\">Farzad Khalvati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of Increased Time Intervals of Anti-Vaccine Tweets for COVID-19 Vaccine with BERT Model. (arXiv:2202.00477v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00477","description":"<p>The most effective of the solutions against Covid-19 is the various vaccines\ndeveloped. Distrust of vaccines can hinder the rapid and effective use of this\nremedy. One of the means of expressing the thoughts of society is social media.\nDetermining the time intervals during which anti-vaccination increases in\nsocial media can help institutions determine the strategy to be used in\ncombating anti-vaccination. Recording and tracking every tweet entered with\nhuman labor would be inefficient, so various automation solutions are needed.\nIn this study, The Bidirectional Encoder Representations from Transformers\n(BERT) model, which is a deep learning-based natural language processing (NLP)\nmodel, was used. In a dataset of 1506 tweets divided into four different\ncategories as news, irrelevant, anti-vaccine, and vaccine supporters, the model\nwas trained with a learning rate of 5e-6 for 25 epochs. To determine the\nintervals in which anti-vaccine tweets are concentrated, the categories to\nwhich 652840 tweets belong were determined by using the trained model. The\nchange of the determined categories overtime was visualized and the events that\ncould cause the change were determined. As a result of model training, in the\ntest dataset, the f-score of 0.81 and AUC values for different classes were\nobtained as 0.99,0.91, 0.92, 0.92, respectively. In this model, unlike the\nstudies in the literature, an auxiliary system is designed that provides data\nthat institutions can use when determining their strategy by measuring and\nvisualizing the frequency of anti-vaccine tweets in a time interval, different\nfrom detecting and censoring such tweets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kucuktas_U/0/1/0/all/0/1\">&#xdc;lk&#xfc; Tuncer K&#xfc;&#xe7;&#xfc;kta&#x15f;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uysal_F/0/1/0/all/0/1\">Fatih Uysal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hardalac_F/0/1/0/all/0/1\">F&#x131;rat Hardala&#xe7;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biri_I/0/1/0/all/0/1\">&#x130;smail Biri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NeuraHealthNLP: An Automated Screening Pipeline to Detect Undiagnosed Cognitive Impairment in Electronic Health Records with Deep Learning and Natural Language Processing. (arXiv:2202.00478v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00478","description":"<p>Dementia related cognitive impairment (CI) affects over 55 million people\nworldwide and is growing rapidly at the rate of one new case every 3 seconds.\nWith a recurring failure of clinical trials, early diagnosis is crucial, but\n75% of dementia cases go undiagnosed globally with up to 90% in\nlow-and-middle-income countries. Current diagnostic methods are notoriously\ncomplex, involving manual review of medical notes, numerous cognitive tests,\nexpensive brain scans or spinal fluid tests. Information relevant to CI is\noften found in the electronic health records (EHRs) and can provide vital clues\nfor early diagnosis, but a manual review by experts is tedious and error prone.\nThis project develops a novel state-of-the-art automated screening pipeline for\nscalable and high-speed discovery of undetected CI in EHRs. To understand the\nlinguistic context from complex language structures in EHR, a database of 8,656\nsequences was constructed to train attention-based deep learning natural\nlanguage processing model to classify sequences. A patient level prediction\nmodel based on logistic regression was developed using the sequence level\nclassifier. The deep learning system achieved 93% accuracy and AUC = 0.98 to\nidentify patients who had no earlier diagnosis, dementia-related diagnosis\ncode, or dementia-related medications in their EHR. These patients would have\notherwise gone undetected or detected too late. The EHR screening pipeline was\ndeployed in NeuraHealthNLP, a web application for automated and real-time CI\nscreening by simply uploading EHRs in a browser. NeuraHealthNLP is cheaper,\nfaster, more accessible, and outperforms current clinical methods including\ntext-based analytics and machine learning approaches. It makes early diagnosis\nviable in regions with scarce health care services but accessible internet or\ncellular services.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tyagi_T/0/1/0/all/0/1\">Tanish Tyagi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Matching based Customer Services Chatbot with Natural Language Understanding. (arXiv:2202.00480v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00480","description":"<p>Customer service is the lifeblood of any business. Excellent customer service\nnot only generates return business but also creates new customers. Looking at\nthe demanding market to provide a 24/7 service to customers, many organisations\nare increasingly engaged in popular social media and text messaging platforms\nsuch as WhatsApp and Facebook Messenger in providing a 24/7 service to\ncustomers in the current demanding market. In this paper, we present an intent\nmatching based customer services chatbot (IMCSC), which is capable of replacing\nthe customer service work of sales personnel, whilst interacting in a more\nnatural and human-like manner through the employment of Natural Language\nUnderstanding (NLU). The bot is able to answer the most common frequently asked\nquestions and we have also integrated features for the processing and exporting\nof customer orders to a Google Sheet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaidrata_A/0/1/0/all/0/1\">Alvin Chaidrata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafeeu_M/0/1/0/all/0/1\">Mariyam Imtha Shafeeu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chew_S/0/1/0/all/0/1\">Sze Ker Chew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiyuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cham_J/0/1/0/all/0/1\">Jin Sheng Cham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1\">Zi Li Yong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yap_U/0/1/0/all/0/1\">Uen Hsieh Yap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahrin_D/0/1/0/all/0/1\">Dania Imanina Binti Kamarul Bahrin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RabindraNet, Creating Literary Works in the Style of Rabindranath Tagore. (arXiv:2202.00481v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00481","description":"<p>Bengali literature has a rich history of hundreds of years with luminary\nfigures such as Rabindranath Tagore and Kazi Nazrul Islam. However, analytical\nworks involving the most recent advancements in NLP have barely scratched the\nsurface utilizing the enormous volume of the collected works from the writers\nof the language. In order to bring attention to the analytical study involving\nthe works of Bengali writers and spearhead the text generation endeavours in\nthe style of existing literature, we are introducing RabindraNet, a character\nlevel RNN model with stacked-LSTM layers trained on the works of Rabindranath\nTagore to produce literary works in his style for multiple genres. We created\nan extensive dataset as well by compiling the digitized works of Rabindranath\nTagore from authentic online sources and published as open source dataset on\ndata science platform Kaggle.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galib_A/0/1/0/all/0/1\">Asadullah Al Galib</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Auto-ABSA: Automatic Detection of Aspects in Aspect-Based Sentiment Analysis. (arXiv:2202.00484v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00484","description":"<p>After transformer is proposed, lots of pre-trained language models have been\ncome up with and sentiment analysis (SA) task has been improved. In this paper,\nwe proposed a method that uses an auxiliary sentence about aspects that the\nsentence contains to help sentiment prediction. The first is aspect detection,\nwhich uses a multi-aspects detection model to predict all aspects that the\nsentence has. Combining the predicted aspects and the original sentence as\nSentiment Analysis (SA) model's input. The second is to do out-of-domain\naspect-based sentiment analysis(ABSA), train sentiment classification model\nwith one kind of dataset and validate it with another kind of dataset. Finally,\nwe created two baselines, they use no aspect and all aspects as sentiment\nclassification model's input, respectively. Compare two baselines performance\nto our method, found that our method really makes sense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Teng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Theoretical Understanding of Word and Relation Representation. (arXiv:2202.00486v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00486","description":"<p>Representing words by vectors, or embeddings, enables computational reasoning\nand is foundational to automating natural language tasks. For example, if word\nembeddings of similar words contain similar values, word similarity can be\nreadily assessed, whereas judging that from their spelling is often impossible\n(e.g. cat /feline) and to predetermine and store similarities between all words\nis prohibitively time-consuming, memory intensive and subjective. We focus on\nword embeddings learned from text corpora and knowledge graphs. Several\nwell-known algorithms learn word embeddings from text on an unsupervised basis\nby learning to predict those words that occur around each word, e.g. word2vec\nand GloVe. Parameters of such word embeddings are known to reflect word\nco-occurrence statistics, but how they capture semantic meaning has been\nunclear. Knowledge graph representation models learn representations both of\nentities (words, people, places, etc.) and relations between them, typically by\ntraining a model to predict known facts in a supervised manner. Despite steady\nimprovements in fact prediction accuracy, little is understood of the latent\nstructure that enables this.\n</p>\n<p>The limited understanding of how latent semantic structure is encoded in the\ngeometry of word embeddings and knowledge graph representations makes a\nprincipled means of improving their performance, reliability or\ninterpretability unclear. To address this:\n</p>\n<p>1. we theoretically justify the empirical observation that particular\ngeometric relationships between word embeddings learned by algorithms such as\nword2vec and GloVe correspond to semantic relations between words; and\n</p>\n<p>2. we extend this correspondence between semantics and geometry to the\nentities and relations of knowledge graphs, providing a model for the latent\nstructure of knowledge graph representation linked to that of word embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1\">Carl Allen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Scaling and Transfer of Language Model Architectures for Machine Translation. (arXiv:2202.00528v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00528","description":"<p>Natural language understanding and generation models follow one of the two\ndominant architectural paradigms: language models (LMs) that process\nconcatenated sequences in a single stack of layers, and encoder-decoder models\n(EncDec) that utilize separate layer stacks for input and output processing. In\nmachine translation, EncDec has long been the favoured approach, but with few\nstudies investigating the performance of LMs. In this work, we thoroughly\nexamine the role of several architectural design choices on the performance of\nLMs on bilingual, (massively) multilingual and zero-shot translation tasks,\nunder systematic variations of data conditions and model sizes. Our results\nshow that: (i) Different LMs have different scaling properties, where\narchitectural differences often have a significant impact on model performance\nat small scales, but the performance gap narrows as the number of parameters\nincreases, (ii) Several design choices, including causal masking and\nlanguage-modeling objectives for the source sequence, have detrimental effects\non translation quality, and (iii) When paired with full-visible masking for\nsource sequences, LMs could perform on par with EncDec on supervised bilingual\nand multilingual translation tasks, and improve greatly on zero-shot directions\nby facilitating the reduction of off-target translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Biao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_B/0/1/0/all/0/1\">Behrooz Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bapna_A/0/1/0/all/0/1\">Ankur Bapna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_X/0/1/0/all/0/1\">Xavier Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jonathan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Novelty Controlled Paraphrase Generation with Retrieval Augmented Conditional Prompt Tuning. (arXiv:2202.00535v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00535","description":"<p>Paraphrase generation is a fundamental and long-standing task in natural\nlanguage processing. In this paper, we concentrate on two contributions to the\ntask: (1) we propose Retrieval Augmented Prompt Tuning (RAPT) as a\nparameter-efficient method to adapt large pre-trained language models for\nparaphrase generation; (2) we propose Novelty Conditioned RAPT (NC-RAPT) as a\nsimple model-agnostic method of using specialized prompt tokens for controlled\nparaphrase generation with varying levels of lexical novelty. By conducting\nextensive experiments on four datasets, we demonstrate the effectiveness of the\nproposed approaches for retaining the semantic content of the original text\nwhile inducing lexical novelty in the generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_J/0/1/0/all/0/1\">Jishnu Ray Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yong Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuyi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Maximum Batch Frobenius Norm for Multi-Domain Text Classification. (arXiv:2202.00537v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00537","description":"<p>Multi-domain text classification (MDTC) has obtained remarkable achievements\ndue to the advent of deep learning. Recently, many endeavors are devoted to\napplying adversarial learning to extract domain-invariant features to yield\nstate-of-the-art results. However, these methods still face one challenge:\ntransforming original features to be domain-invariant distorts the\ndistributions of the original features, degrading the discriminability of the\nlearned features. To address this issue, we first investigate the structure of\nthe batch classification output matrix and theoretically justify that the\ndiscriminability of the learned features has a positive correlation with the\nFrobenius norm of the batch output matrix. Based on this finding, we propose a\nmaximum batch Frobenius norm (MBF) method to boost the feature discriminability\nfor MDTC. Experiments on two MDTC benchmarks show that our MBF approach can\neffectively advance the performance of the state-of-the-art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkpen_D/0/1/0/all/0/1\">Diana Inkpen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Roby_A/0/1/0/all/0/1\">Ahmed El-Roby</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dominant Set-based Active Learning for Text Classification and its Application to Online Social Media. (arXiv:2202.00540v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00540","description":"<p>Recent advances in natural language processing (NLP) in online social media\nare evidently owed to large-scale datasets. However, labeling, storing, and\nprocessing a large number of textual data points, e.g., tweets, has remained\nchallenging. On top of that, in applications such as hate speech detection,\nlabeling a sufficiently large dataset containing offensive content can be\nmentally and emotionally taxing for human annotators. Thus, NLP methods that\ncan make the best use of significantly less labeled data points are of great\ninterest. In this paper, we present a novel pool-based active learning method\nthat can be used for the training of large unlabeled corpus with minimum\nannotation cost. For that, we propose to find the dominant sets of local\nclusters in the feature space. These sets represent maximally cohesive\nstructures in the data. Then, the samples that do not belong to any of the\ndominant sets are selected to be used to train the model, as they represent the\nboundaries of the local clusters and are more challenging to classify. Our\nproposed method does not have any parameters to be tuned, making it\ndataset-independent, and it can approximately achieve the same classification\naccuracy as full training data, with significantly fewer data points.\nAdditionally, our method achieves a higher performance in comparison to the\nstate-of-the-art active learning strategies. Furthermore, our proposed\nalgorithm is able to incorporate conventional active learning scores, such as\nuncertainty-based scores, into its selection criteria. We show the\neffectiveness of our method on different datasets and using different neural\nnetwork architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oghaz_T/0/1/0/all/0/1\">Toktam A. Oghaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garibay_I/0/1/0/all/0/1\">Ivan Garibay</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding the optimal human strategy for Wordle using maximum correct letter probabilities and reinforcement learning. (arXiv:2202.00557v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00557","description":"<p>Wordle is an online word puzzle game that gained viral popularity in January\n2022. The goal is to guess a hidden five letter word. After each guess, the\nplayer gains information about whether the letters they guessed are present in\nthe word, and whether they are in the correct position. Numerous blogs have\nsuggested guessing strategies and starting word lists that improve the chance\nof winning. Optimized algorithms can win 100% of games within five of the six\nallowed trials. However, it is infeasible for human players to use these\nalgorithms due to an inability to perfectly recall all known 5-letter words and\nperform complex calculations that optimize information gain. Here, we present\ntwo different methods for choosing starting words along with a framework for\ndiscovering the optimal human strategy based on reinforcement learning. Human\nWordle players can use the rules we discover to optimize their chance of\nwinning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Benton J. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_J/0/1/0/all/0/1\">Jesse G. Meyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEA-Base: A Benchmark for ASR of Spontaneous Hungarian. (arXiv:2202.00601v1 [eess.AS])","link":"http://arxiv.org/abs/2202.00601","description":"<p>Hungarian is spoken by 15 million people, still, easily accessible Automatic\nSpeech Recognition (ASR) benchmark datasets - especially for spontaneous speech\n- have been practically unavailable. In this paper, we introduce BEA-Base, a\nsubset of the BEA spoken Hungarian database comprising mostly spontaneous\nspeech of 140 speakers. It is built specifically to assess ASR, primarily for\nconversational AI applications. After defining the speech recognition subsets\nand task, several baselines - including classic HMM-DNN hybrid and end-to-end\napproaches augmented by cross-language transfer learning - are developed using\nopen-source toolkits. The best results obtained are based on multilingual\nself-supervised pretraining, achieving a 45% recognition error rate reduction\nas compared to the classical approach - without the application of an external\nlanguage model or additional supervised data. The results show the feasibility\nof using BEA-Base for training and evaluation of Hungarian speech recognition\nsystems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mihajlik_P/0/1/0/all/0/1\">P. Mihajlik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balog_A/0/1/0/all/0/1\">A. Balog</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Graczi_T/0/1/0/all/0/1\">T. E. Gr&#xe1;czi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohari_A/0/1/0/all/0/1\">A. Koh&#xe1;ri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tarjan_B/0/1/0/all/0/1\">B. Tarj&#xe1;n</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mady_K/0/1/0/all/0/1\">K. M&#xe1;dy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic of Cloud Computing services for Time Series workflows. (arXiv:2202.00609v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00609","description":"<p>Time series (TS) are present in many fields of knowledge, research, and\nengineering. The processing and analysis of TS are essential in order to\nextract knowledge from the data and to tackle forecasting or predictive\nmaintenance tasks among others The modeling of TS is a challenging task,\nrequiring high statistical expertise as well as outstanding knowledge about the\napplication of Data Mining(DM) and Machine Learning (ML) methods. The overall\nwork with TS is not limited to the linear application of several techniques,\nbut is composed of an open workflow of methods and tests. These workflow,\ndeveloped mainly on programming languages, are complicated to execute and run\neffectively on different systems, including Cloud Computing (CC) environments.\nThe adoption of CC can facilitate the integration and portability of services\nallowing to adopt solutions towards services Internet Technologies (IT)\nindustrialization. The definition and description of workflow services for TS\nopen up a new set of possibilities regarding the reduction of complexity in the\ndeployment of this type of issues in CC environments. In this sense, we have\ndesigned an effective proposal based on semantic modeling (or vocabulary) that\nprovides the full description of workflow for Time Series modeling as a CC\nservice. Our proposal includes a broad spectrum of the most extended\noperations, accommodating any workflow applied to classification, regression,\nor clustering problems for Time Series, as well as including evaluation\nmeasures, information, tests, or machine learning algorithms among others.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parra_Royon_M/0/1/0/all/0/1\">Manuel Parra-Roy&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldan_F/0/1/0/all/0/1\">Francisco Baldan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atemezing_G/0/1/0/all/0/1\">Ghislain Atemezing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benitez_J/0/1/0/all/0/1\">J.M. Benitez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FiNCAT: Financial Numeral Claim Analysis Tool. (arXiv:2202.00631v1 [q-fin.GN])","link":"http://arxiv.org/abs/2202.00631","description":"<p>While making investment decisions by reading financial documents, investors\nneed to differentiate between in-claim and outof-claim numerals. In this paper,\nwe present a tool which does it automatically. It extracts context embeddings\nof the numerals using one of the transformer based pre-trained language model\ncalled BERT. After this, it uses a Logistic Regression based model to detect\nwhether the numerals is in-claim or out-of-claim. We use FinNum-3 (English)\ndataset to train our model. After conducting rigorous experiments we achieve a\nMacro F1 score of 0.8223 on the validation set. We have open-sourced this tool\nand it can be accessed from\nhttps://github.com/sohomghosh/FiNCAT_Financial_Numeral_Claim_Analysis_Tool\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-fin/1/au:+Ghosh_S/0/1/0/all/0/1\">Sohom Ghosh</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Naskar_S/0/1/0/all/0/1\">Sudip Kumar Naskar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Typical Decoding for Natural Language Generation. (arXiv:2202.00666v1 [cs.CL])","link":"http://arxiv.org/abs/2202.00666","description":"<p>Despite achieving incredibly low perplexities on myriad natural language\ncorpora, today's language models still often underperform when used to generate\ntext. This dichotomy has puzzled the language generation community for the last\nfew years. In this work, we posit that the abstraction of natural language as a\ncommunication channel (\\`a la Shannon, 1948) can provide new insights into the\nbehaviors of probabilistic language generators, e.g., why high-probability\ntexts can be dull or repetitive. Humans use language as a means of\ncommunicating information, and do so in an efficient yet error-minimizing\nmanner, choosing each word in a string with this (perhaps subconscious) goal in\nmind. We propose that generation from probabilistic models should mimic this\nbehavior. Rather than always choosing words from the high-probability region of\nthe distribution--which have a low Shannon information content--we sample from\nthe set of words with an information content close to its expected value, i.e.,\nclose to the conditional entropy of our model. This decision criterion can be\nrealized through a simple and efficient implementation, which we call typical\nsampling. Automatic and human evaluations show that, in comparison to nucleus\nand top-k sampling, typical sampling offers competitive performance in terms of\nquality while consistently reducing the number of degenerate repetitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiher_G/0/1/0/all/0/1\">Gian Wiher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FlexParser -- the adaptive log file parser for continuous results in a changing world. (arXiv:2106.03170v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.03170","description":"<p>Any modern system writes events into files, called log files. Those contain\ncrucial information which are subject to various analyses. Examples range from\ncybersecurity, intrusion detection over usage analyses to trouble shooting.\nBefore data analysis is possible, desired information needs to be extracted\nfirst out of the semi-structured log messages. State-of-the-art event parsing\noften assumes static log events. However, any modern system is updated\nconsistently and with updates also log file structures can change. We call\nthose changes \"mutation\" and study parsing performance for different mutation\ncases. Latest research discovers mutations using anomaly detection post mortem,\nhowever, does not cover actual continuous parsing. Thus, we propose a novel and\nflexible parser, called FlexParser, which can extract desired values despite\ngradual changes in the log messages. It implies basic text preprocessing\nfollowed by a supervised Deep Learning method. We train a stateful LSTM on\nparsing one event per data set. Statefulness enforces the model to learn log\nmessage structures across several examples. Our model was tested on seven\ndifferent, publicly available log file data sets and various kinds of\nmutations. Exhibiting an average F1-Score of 0.98, it outperforms other Deep\nLearning methods as well as state-of-the-art unsupervised parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruecker_N/0/1/0/all/0/1\">Nadine Ruecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1\">Andreas Maier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Task-Oriented Dialog Modeling with Semi-Structured Knowledge Management. (arXiv:2106.11796v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.11796","description":"<p>Current task-oriented dialog (TOD) systems mostly manage structured knowledge\n(e.g. databases and tables) to guide the goal-oriented conversations. However,\nthey fall short of handling dialogs which also involve unstructured knowledge\n(e.g. reviews and documents). In this paper, we formulate a task of modeling\nTOD grounded on a fusion of structured and unstructured knowledge. To address\nthis task, we propose a TOD system with semi-structured knowledge management,\nSeKnow, which extends the belief state to manage knowledge with both structured\nand unstructured contents. Furthermore, we introduce two implementations of\nSeKnow based on a non-pretrained sequence-to-sequence model and a pretrained\nlanguage model, respectively. Both implementations use the end-to-end manner to\njointly optimize dialog modeling grounded on structured and unstructured\nknowledge. We conduct experiments on a modified version of MultiWOZ 2.1\ndataset, Mod-MultiWOZ 2.1, where dialogs are processed to involve\nsemi-structured knowledge. Experimental results show that SeKnow has strong\nperformances in both end-to-end dialog and intermediate knowledge management,\ncompared to existing TOD systems and their extensions with pipeline knowledge\nmanagement schemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1\">Silin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1\">Ryuichi Takanobu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Audiomer: A Convolutional Transformer For Keyword Spotting. (arXiv:2109.10252v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.10252","description":"<p>Transformers have seen an unprecedented rise in Natural Language Processing\nand Computer Vision tasks. However, in audio tasks, they are either infeasible\nto train due to extremely large sequence length of audio waveforms or incur a\nperformance penalty when trained on Fourier-based features. In this work, we\nintroduce an architecture, Audiomer, where we combine 1D Residual Networks with\nPerformer Attention to achieve state-of-the-art performance in keyword spotting\nwith raw audio waveforms, outperforming all previous methods while being\ncomputationally cheaper and parameter-efficient. Additionally, our model has\npractical advantages for speech processing, such as inference on arbitrarily\nlong audio clips owing to the absence of positional encoding. The code is\navailable at https://github.com/The-Learning-Machines/Audiomer-PyTorch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1\">Surya Kant Sahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitheran_S/0/1/0/all/0/1\">Sai Mitheran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamdar_J/0/1/0/all/0/1\">Juhi Kamdar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1\">Meet Gandhi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Structural Locality in Non-parametric Language Models. (arXiv:2110.02870v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02870","description":"<p>Structural locality is a ubiquitous feature of real-world datasets, wherein\ndata points are organized into local hierarchies. Some examples include topical\nclusters in text or project hierarchies in source code repositories. In this\npaper, we explore utilizing this structural locality within non-parametric\nlanguage models, which generate sequences that reference retrieved examples\nfrom an external source. We propose a simple yet effective approach for adding\nlocality information into such models by adding learned parameters that improve\nthe likelihood of retrieving examples from local neighborhoods. Experiments on\ntwo different domains, Java source code and Wikipedia text, demonstrate that\nlocality features improve model efficacy over models without access to these\nfeatures, with interesting differences. We also perform an analysis of how and\nwhere locality features contribute to improved performance and why the\ntraditionally used contextual similarity metrics alone are not enough to grasp\nthe locality structure.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Frank F. Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Junxian He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1\">Vincent J. Hellendoorn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CsFEVER and CTKFacts: Czech Datasets for Fact Verification. (arXiv:2201.11115v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11115","description":"<p>In this paper, we present two Czech datasets for automated fact-checking,\nwhich is a task commonly modeled as a classification of textual claim veracity\nw.r.t. a corpus of trusted ground truths. We consider 3 classes: SUPPORTS,\nREFUTES complemented with evidence documents or NEI (Not Enough Info) alone.\nOur first dataset, CsFEVER, has 127,328 claims. It is an automatically\ngenerated Czech version of the large-scale FEVER dataset built on top of\nWikipedia corpus. We take a hybrid approach of machine translation and document\nalignment; the approach, and the tools we provide, can be easily applied to\nother languages. The second dataset, CTKFacts of 3,097 claims, is annotated\nusing the corpus of 2.2M articles of Czech News Agency. We present its extended\nannotation methodology based on the FEVER approach. We analyze both datasets\nfor spurious cues - annotation patterns leading to model overfitting. CTKFacts\nis further examined for inter-annotator agreement, thoroughly cleaned, and a\ntypology of common annotator errors is extracted. Finally, we provide baseline\nmodels for all stages of the fact-checking pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Drchal_J/0/1/0/all/0/1\">Jan Drchal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullrich_H/0/1/0/all/0/1\">Herbert Ullrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rypar_M/0/1/0/all/0/1\">Martin R&#xfd;par</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincourova_H/0/1/0/all/0/1\">Hana Vincourov&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moravec_V/0/1/0/all/0/1\">V&#xe1;clav Moravec</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"BREAK: Bronchi Reconstruction by gEodesic transformation And sKeleton embedding. (arXiv:2202.00002v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00002","description":"<p>Airway segmentation is critical for virtual bronchoscopy and computer-aided\npulmonary disease analysis. In recent years, convolutional neural networks\n(CNNs) have been widely used to delineate the bronchial tree. However, the\nsegmentation results of the CNN-based methods usually include many\ndiscontinuous branches, which need manual repair in clinical use. A major\nreason for the breakages is that the appearance of the airway wall can be\naffected by the lung disease as well as the adjacency of the vessels, while the\nnetwork tends to overfit to these special patterns in the training set. To\nlearn robust features for these areas, we design a multi-branch framework that\nadopts the geodesic distance transform to capture the intensity changes between\nairway lumen and wall. Another reason for the breakages is the intra-class\nimbalance. Since the volume of the peripheral bronchi may be much smaller than\nthe large branches in an input patch, the common segmentation loss is not\nsensitive to the breakages among the distal branches. Therefore, in this paper,\na breakage-sensitive regularization term is designed and can be easily combined\nwith other loss functions. Extensive experiments are conducted on publicly\navailable datasets. Compared with state-of-the-art methods, our framework can\ndetect more branches while maintaining competitive segmentation performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Yu_W/0/1/0/all/0/1\">Weihao Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_H/0/1/0/all/0/1\">Hao Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1\">Minghui Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1\">Hanxiao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jiayuan Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jie Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Bitstream Metadata for Fast and Accurate Video Compression Correction. (arXiv:2202.00011v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00011","description":"<p>Video compression is a central feature of the modern internet powering\ntechnologies from social media to video conferencing. While video compression\ncontinues to mature, for many, and particularly for extreme, compression\nsettings, quality loss is still noticeable. These extreme settings nevertheless\nhave important applications to the efficient transmission of videos over\nbandwidth constrained or otherwise unstable connections. In this work, we\ndevelop a deep learning architecture capable of restoring detail to compressed\nvideos which leverages the underlying structure and motion information embedded\nin the video bitstream. We show that this improves restoration accuracy\ncompared to prior compression correction methods and is competitive when\ncompared with recent deep-learning-based video compression methods on\nrate-distortion while achieving higher throughput.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ehrlich_M/0/1/0/all/0/1\">Max Ehrlich</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Barker_J/0/1/0/all/0/1\">Jon Barker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Padmanabhan_N/0/1/0/all/0/1\">Namitha Padmanabhan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Davis_L/0/1/0/all/0/1\">Larry Davis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tao_A/0/1/0/all/0/1\">Andrew Tao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shrivastava_A/0/1/0/all/0/1\">Abhinav Shrivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Finding Directions in GAN's Latent Space for Neural Face Reenactment. (arXiv:2202.00046v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00046","description":"<p>This paper is on face/head reenactment where the goal is to transfer the\nfacial pose (3D head orientation and expression) of a target face to a source\nface. Previous methods focus on learning embedding networks for identity and\npose disentanglement which proves to be a rather hard task, degrading the\nquality of the generated images. We take a different approach, bypassing the\ntraining of such networks, by using (fine-tuned) pre-trained GANs which have\nbeen shown capable of producing high-quality facial images. Because GANs are\ncharacterized by weak controllability, the core of our approach is a method to\ndiscover which directions in latent GAN space are responsible for controlling\nfacial pose and expression variations. We present a simple pipeline to learn\nsuch directions with the aid of a 3D shape model which, by construction,\nalready captures disentangled directions for facial pose, identity and\nexpression. Moreover, we show that by embedding real images in the GAN latent\nspace, our method can be successfully used for the reenactment of real-world\nfaces. Our method features several favorable properties including using a\nsingle source image (one-shot) and enabling cross-person reenactment. Our\nqualitative and quantitative results show that our approach often produces\nreenacted faces of significantly higher quality than those produced by\nstate-of-the-art methods for the standard benchmarks of VoxCeleb1 &amp; 2.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bounareli_S/0/1/0/all/0/1\">Stella Bounareli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argyriou_V/0/1/0/all/0/1\">Vasileios Argyriou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzimiropoulos_G/0/1/0/all/0/1\">Georgios Tzimiropoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep-Disaster: Unsupervised Disaster Detection and Localization Using Visual Data. (arXiv:2202.00050v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00050","description":"<p>Social media plays a significant role in sharing essential information, which\nhelps humanitarian organizations in rescue operations during and after disaster\nincidents. However, developing an efficient method that can provide rapid\nanalysis of social media images in the early hours of disasters is still\nlargely an open problem, mainly due to the lack of suitable datasets and the\nsheer complexity of this task. In addition, supervised methods can not\ngeneralize well to novel disaster incidents. In this paper, inspired by the\nsuccess of Knowledge Distillation (KD) methods, we propose an unsupervised deep\nneural network to detect and localize damages in social media images. Our\nproposed KD architecture is a feature-based distillation approach that\ncomprises a pre-trained teacher and a smaller student network, with both\nnetworks having similar GAN architecture containing a generator and a\ndiscriminator. The student network is trained to emulate the behavior of the\nteacher on training input samples, which, in turn, contain images that do not\ninclude any damaged regions. Therefore, the student network only learns the\ndistribution of no damage data and would have different behavior from the\nteacher network-facing damages. To detect damage, we utilize the difference\nbetween features generated by two networks using a defined score function that\ndemonstrates the probability of damages occurring. Our experimental results on\nthe benchmark dataset confirm that our approach outperforms state-of-the-art\nmethods in detecting and localizing the damaged areas, especially for novel\ndisaster types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shekarizadeh_S/0/1/0/all/0/1\">Soroor Shekarizadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastgoo_R/0/1/0/all/0/1\">Razieh Rastgoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Kuwari_S/0/1/0/all/0/1\">Saif Al-Kuwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1\">Mohammad Sabokrou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoGeoLabel: Automated Label Generation for Geospatial Machine Learning. (arXiv:2202.00067v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00067","description":"<p>A key challenge of supervised learning is the availability of human-labeled\ndata. We evaluate a big data processing pipeline to auto-generate labels for\nremote sensing data. It is based on rasterized statistical features extracted\nfrom surveys such as e.g. LiDAR measurements. Using simple combinations of the\nrasterized statistical layers, it is demonstrated that multiple classes can be\ngenerated at accuracies of ~0.9. As proof of concept, we utilize the big\ngeo-data platform IBM PAIRS to dynamically generate such labels in dense urban\nareas with multiple land cover classes. The general method proposed here is\nplatform independent, and it can be adapted to generate labels for other\nsatellite modalities in order to enable machine learning on overhead imagery\nfor land use classification and object detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Albrecht_C/0/1/0/all/0/1\">Conrad M Albrecht</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Marianno_F/0/1/0/all/0/1\">Fernando Marianno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Klein_L/0/1/0/all/0/1\">Levente J Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Holistic Fine-grained GGS Characterization: From Detection to Unbalanced Classification. (arXiv:2202.00087v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00087","description":"<p>Recent studies have demonstrated the diagnostic and prognostic values of\nglobal glomerulosclerosis (GGS) in IgA nephropathy, aging, and end-stage renal\ndisease. However, the fine-grained quantitative analysis of multiple GGS\nsubtypes (e.g., obsolescent, solidified, and disappearing glomerulosclerosis)\nis typically a resource extensive manual process. Very few automatic methods,\nif any, have been developed to bridge this gap for such analytics. In this\npaper, we present a holistic pipeline to quantify GGS (with both detection and\nclassification) from a whole slide image in a fully automatic manner. In\naddition, we conduct the fine-grained classification for the sub-types of GGS.\nOur study releases the open-source quantitative analytical tool for\nfine-grained GGS characterization while tackling the technical challenges in\nunbalanced classification and integrating detection and classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yuzhe Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1\">Haichun Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Asad_Z/0/1/0/all/0/1\">Zuhayr Asad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheyu Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yao_T/0/1/0/all/0/1\">Tianyuan Yao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiachen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fogo_A/0/1/0/all/0/1\">Agnes B. Fogo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huo_Y/0/1/0/all/0/1\">Yuankai Huo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models. (arXiv:2202.00091v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00091","description":"<p>Despite our best efforts, deep learning models remain highly vulnerable to\neven tiny adversarial perturbations applied to the inputs. The ability to\nextract information from solely the output of a machine learning model to craft\nadversarial perturbations to black-box models is a practical threat against\nreal-world systems, such as autonomous cars or machine learning models exposed\nas a service (MLaaS). Of particular interest are sparse attacks. The\nrealization of sparse attacks in black-box models demonstrates that machine\nlearning models are more vulnerable than we believe. Because these attacks aim\nto minimize the number of perturbed pixels measured by l_0 norm-required to\nmislead a model by solely observing the decision (the predicted label) returned\nto a model query; the so-called decision-based attack setting. But, such an\nattack leads to an NP-hard optimization problem. We develop an evolution-based\nalgorithm-SparseEvo-for the problem and evaluate against both convolutional\ndeep neural networks and vision transformers. Notably, vision transformers are\nyet to be investigated under a decision-based attack setting. SparseEvo\nrequires significantly fewer model queries than the state-of-the-art sparse\nattack Pointwise for both untargeted and targeted attacks. The attack\nalgorithm, although conceptually simple, is also competitive with only a\nlimited query budget against the state-of-the-art gradient-based whitebox\nattacks in standard computer vision tasks such as ImageNet. Importantly, the\nquery efficient SparseEvo, along with decision-based attacks, in general, raise\nnew questions regarding the safety of deployed systems and poses new directions\nto study and understand the robustness of machine learning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1\">Viet Quoc Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbasnejad_E/0/1/0/all/0/1\">Ehsan Abbasnejad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_D/0/1/0/all/0/1\">Damith C. Ranasinghe</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Identification and Mapping of Surface Water Extent using Street-level Monitoring Videos. (arXiv:2202.00096v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00096","description":"<p>Urban flooding is becoming a common and devastating hazard to cause life loss\nand economic damage. Monitoring and understanding urban flooding in the local\nscale is a challenging task due to the complicated urban landscape, intricate\nhydraulic process, and the lack of high-quality and resolution data. The\nemerging smart city technology such as monitoring cameras provides an\nunprecedented opportunity to address the data issue. However, estimating the\nwater accumulation on the land surface based on the monitoring footage is\nunreliable using the traditional segmentation technique because the boundary of\nthe water accumulation, under the influence of varying weather, background, and\nillumination, is usually too fuzzy to identify, and the oblique angle and image\ndistortion in the video monitoring data prevents georeferencing and\nobject-based measurements. This paper presents a novel semi-supervised\nsegmentation scheme for surface water extent recognition from the footage of an\noblique monitoring camera. The semi-supervised segmentation algorithm was found\nsuitable to determine the water boundary and the monoplotting method was\nsuccessfully applied to georeference the pixels of the monitoring video for the\nvirtual quantification of the local drainage process. The correlation and\nmechanism-based analysis demonstrates the value of the proposed method in\nadvancing the understanding of local drainage hydraulics. The workflow and\ncreated methods in this study has a great potential to study other street-level\nand earth surface processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ruo-Qian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yangmin Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Real-Time Facial Expression Recognition using Facial Landmarks and Neural Networks. (arXiv:2202.00102v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00102","description":"<p>This paper presents a lightweight algorithm for feature extraction,\nclassification of seven different emotions, and facial expression recognition\nin a real-time manner based on static images of the human face. In this regard,\na Multi-Layer Perceptron (MLP) neural network is trained based on the foregoing\nalgorithm. In order to classify human faces, first, some pre-processing is\napplied to the input image, which can localize and cut out faces from it. In\nthe next step, a facial landmark detection library is used, which can detect\nthe landmarks of each face. Then, the human face is split into upper and lower\nfaces, which enables the extraction of the desired features from each part. In\nthe proposed model, both geometric and texture-based feature types are taken\ninto account. After the feature extraction phase, a normalized vector of\nfeatures is created. A 3-layer MLP is trained using these feature vectors,\nleading to 96% accuracy on the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Haghpanah_M/0/1/0/all/0/1\">Mohammad Amin Haghpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saeedizade_E/0/1/0/all/0/1\">Ehsan Saeedizade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masouleh_M/0/1/0/all/0/1\">Mehdi Tale Masouleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalhor_A/0/1/0/all/0/1\">Ahmad Kalhor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D Visualization and Spatial Data Mining for Analysis of LULC Images. (arXiv:2202.00123v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00123","description":"<p>The present study is an attempt made to create a new tool for the analysis of\nLand Use Land Cover (LUCL) images in 3D visualization. This study mainly uses\nspatial data mining techniques on high resolution LULC satellite imagery.\nVisualization of feature space allows exploration of patterns in the image data\nand insight into the classification process and related uncertainty. Visual\nData Mining provides added value to image classifications as the user can be\ninvolved in the classification process providing increased confidence in and\nunderstanding of the results. In this study, we present a prototype of image\nsegmentation, K-Means clustering and 3D visualization tool for visual data\nmining (VDM) of LUCL satellite imagery into volume visualization. This volume\nbased representation divides feature space into spheres or voxels. The\nvisualization tool is showcased in a classification study of high-resolution\nLULC imagery of Latur district (Maharashtra state, India) is used as sample\ndata.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kodge_B/0/1/0/all/0/1\">B. G. Kodge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning-Based Framework for Camera Calibration with Distortion Correction and High Precision Feature Detection. (arXiv:2202.00158v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00158","description":"<p>Camera calibration is a crucial technique which significantly influences the\nperformance of many robotic systems. Robustness and high precision have always\nbeen the pursuit of diverse calibration methods. State-of-the-art calibration\ntechniques based on classical Zhang's method, however, still suffer from\nenvironmental noise, radial lens distortion and sub-optimal parameter\nestimation. Therefore, in this paper, we propose a hybrid camera calibration\nframework which combines learning-based approaches with traditional methods to\nhandle these bottlenecks. In particular, this framework leverages\nlearning-based approaches to perform efficient distortion correction and robust\nchessboard corner coordinate encoding. For sub-pixel accuracy of corner\ndetection, a specially-designed coordinate decoding algorithm with embed\noutlier rejection mechanism is proposed. To avoid sub-optimal estimation\nresults, we improve the traditional parameter estimation by RANSAC algorithm\nand achieve stable results. Compared with two widely-used camera calibration\ntoolboxes, experiment results on both real and synthetic datasets manifest the\nbetter robustness and higher precision of the proposed framework. The massive\nsynthetic dataset is the basis of our framework's decent performance and will\nbe publicly available along with the code at\nhttps://github.com/Easonyesheng/CCS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yesheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1\">Dahong Qian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dilated Continuous Random Field for Semantic Segmentation. (arXiv:2202.00162v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00162","description":"<p>Mean field approximation methodology has laid the foundation of modern\nContinuous Random Field (CRF) based solutions for the refinement of semantic\nsegmentation. In this paper, we propose to relax the hard constraint of mean\nfield approximation - minimizing the energy term of each node from\nprobabilistic graphical model, by a global optimization with the proposed\ndilated sparse convolution module (DSConv). In addition, adaptive global\naverage-pooling and adaptive global max-pooling are implemented as replacements\nof fully connected layers. In order to integrate DSConv, we design an\nend-to-end, time-efficient DilatedCRF pipeline. The unary energy term is\nderived either from pre-softmax and post-softmax features, or the predicted\naffordance map using a conventional classifier, making it easier to implement\nDilatedCRF for varieties of classifiers. We also present superior experimental\nresults of proposed approach on the suction dataset comparing to other\nCRF-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mo_X/0/1/0/all/0/1\">Xi Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiangyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Cuncong Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kaidong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajid_U/0/1/0/all/0/1\">Usman Sajid</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DexVIP: Learning Dexterous Grasping with Human Hand Pose Priors from Video. (arXiv:2202.00164v1 [cs.RO])","link":"http://arxiv.org/abs/2202.00164","description":"<p>Dexterous multi-fingered robotic hands have a formidable action space, yet\ntheir morphological similarity to the human hand holds immense potential to\naccelerate robot learning. We propose DexVIP, an approach to learn dexterous\nrobotic grasping from human-object interactions present in in-the-wild YouTube\nvideos. We do this by curating grasp images from human-object interaction\nvideos and imposing a prior over the agent's hand pose when learning to grasp\nwith deep reinforcement learning. A key advantage of our method is that the\nlearned policy is able to leverage free-form in-the-wild visual data. As a\nresult, it can easily scale to new objects, and it sidesteps the standard\npractice of collecting human demonstrations in a lab -- a much more expensive\nand indirect way to capture human expertise. Through experiments on 27 objects\nwith a 30-DoF simulated robot hand, we demonstrate that DexVIP compares\nfavorably to existing approaches that lack a hand pose prior or rely on\nspecialized tele-operation equipment to obtain human demonstrations, while also\nbeing faster to train. Project page:\nhttps://vision.cs.utexas.edu/projects/dexvip-dexterous-grasp-pose-prior\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mandikal_P/0/1/0/all/0/1\">Priyanka Mandikal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1\">Kristen Grauman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fractional Motion Estimation for Point Cloud Compression. (arXiv:2202.00172v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00172","description":"<p>Motivated by the success of fractional pixel motion in video coding, we\nexplore the design of motion estimation with fractional-voxel resolution for\ncompression of color attributes of dynamic 3D point clouds. Our proposed\nblock-based fractional-voxel motion estimation scheme takes into account the\nfundamental differences between point clouds and videos, i.e., the irregularity\nof the distribution of voxels within a frame and across frames. We show that\nmotion compensation can benefit from the higher resolution reference and more\naccurate displacements provided by fractional precision. Our proposed scheme\nsignificantly outperforms comparable methods that only use integer motion. The\nproposed scheme can be combined with and add sizeable gains to state-of-the-art\nsystems that use transforms such as Region Adaptive Graph Fourier Transform and\nRegion Adaptive Haar Transform.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Hong_H/0/1/0/all/0/1\">Haoran Hong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pavez_E/0/1/0/all/0/1\">Eduardo Pavez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ortega_A/0/1/0/all/0/1\">Antonio Ortega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_R/0/1/0/all/0/1\">Ryosuke Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nonaka_K/0/1/0/all/0/1\">Keisuke Nonaka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blind Image Deconvolution Using Variational Deep Image Prior. (arXiv:2202.00179v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00179","description":"<p>Conventional deconvolution methods utilize hand-crafted image priors to\nconstrain the optimization. While deep-learning-based methods have simplified\nthe optimization by end-to-end training, they fail to generalize well to blurs\nunseen in the training dataset. Thus, training image-specific models is\nimportant for higher generalization. Deep image prior (DIP) provides an\napproach to optimize the weights of a randomly initialized network with a\nsingle degraded image by maximum a posteriori (MAP), which shows that the\narchitecture of a network can serve as the hand-crafted image prior. Different\nfrom the conventional hand-crafted image priors that are statistically\nobtained, it is hard to find a proper network architecture because the\nrelationship between images and their corresponding network architectures is\nunclear. As a result, the network architecture cannot provide enough constraint\nfor the latent sharp image. This paper proposes a new variational deep image\nprior (VDIP) for blind image deconvolution, which exploits additive\nhand-crafted image priors on latent sharp images and approximates a\ndistribution for each pixel to avoid suboptimal solutions. Our mathematical\nanalysis shows that the proposed method can better constrain the optimization.\nThe experimental results further demonstrate that the generated images have\nbetter quality than that of the original DIP on benchmark datasets. The source\ncode of our VDIP is available at\nhttps://github.com/Dong-Huo/VDIP-Deconvolution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Masoumzadeh_A/0/1/0/all/0/1\">Abbas Masoumzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kushol_R/0/1/0/all/0/1\">Rafsanjany Kushol</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLA-NeRF: Category-Level Articulated Neural Radiance Field. (arXiv:2202.00181v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00181","description":"<p>We propose CLA-NeRF -- a Category-Level Articulated Neural Radiance Field\nthat can perform view synthesis, part segmentation, and articulated pose\nestimation. CLA-NeRF is trained at the object category level using no CAD\nmodels and no depth, but a set of RGB images with ground truth camera poses and\npart segments. During inference, it only takes a few RGB views (i.e., few-shot)\nof an unseen 3D object instance within the known category to infer the object\npart segmentation and the neural radiance field. Given an articulated pose as\ninput, CLA-NeRF can perform articulation-aware volume rendering to generate the\ncorresponding RGB image at any camera pose. Moreover, the articulated pose of\nan object can be estimated via inverse rendering. In our experiments, we\nevaluate the framework across five categories on both synthetic and real-world\ndata. In all cases, our method shows realistic deformation results and accurate\narticulated pose estimation. We believe that both few-shot articulated object\nrendering and articulated pose estimation open doors for robots to perceive and\ninteract with unseen articulated objects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tseng_W/0/1/0/all/0/1\">Wei-Cheng Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1\">Hung-Ju Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised 3D Object Detection via Temporal Graph Neural Networks. (arXiv:2202.00182v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00182","description":"<p>3D object detection plays an important role in autonomous driving and other\nrobotics applications. However, these detectors usually require training on\nlarge amounts of annotated data that is expensive and time-consuming to\ncollect. Instead, we propose leveraging large amounts of unlabeled point cloud\nvideos by semi-supervised learning of 3D object detectors via temporal graph\nneural networks. Our insight is that temporal smoothing can create more\naccurate detection results on unlabeled data, and these smoothed detections can\nthen be used to retrain the detector. We learn to perform this temporal\nreasoning with a graph neural network, where edges represent the relationship\nbetween candidate detections in different time frames. After semi-supervised\nlearning, our method achieves state-of-the-art detection performance on the\nchallenging nuScenes and H3D benchmarks, compared to baselines trained on the\nsame amount of labeled data. Project and code are released at\nhttps://www.jianrenw.com/SOD-TGNN/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gang_H/0/1/0/all/0/1\">Haiming Gang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ancha_S/0/1/0/all/0/1\">Siddarth Ancha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi-Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ATEK: Augmenting Transformers with Expert Knowledge for Indoor Layout Synthesis. (arXiv:2202.00185v1 [cs.GR])","link":"http://arxiv.org/abs/2202.00185","description":"<p>We address the problem of indoor layout synthesis, which is a topic of\ncontinuing research interest in computer graphics. The newest works made\nsignificant progress using data-driven generative methods; however, these\napproaches rely on suitable datasets. In practice, desirable layout properties\nmay not exist in a dataset, for instance, specific expert knowledge can be\nmissing in the data. We propose a method that combines expert knowledge, for\nexample, knowledge about ergonomics, with a data-driven generator based on the\npopular Transformer architecture. The knowledge is given as differentiable\nscalar functions, which can be used both as weights or as additional terms in\nthe loss function. Using this knowledge, the synthesized layouts can be biased\nto exhibit desirable properties, even if these properties are not present in\nthe dataset. Our approach can also alleviate problems of lack of data and\nimperfections in the data. Our work aims to improve generative machine learning\nfor modeling and provide novel tools for designers and amateurs for the problem\nof interior layout creation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leimer_K/0/1/0/all/0/1\">Kurt Leimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1\">Paul Guerrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_T/0/1/0/all/0/1\">Tomer Weiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Musialski_P/0/1/0/all/0/1\">Przemyslaw Musialski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recognition-Aware Learned Image Compression. (arXiv:2202.00198v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00198","description":"<p>Learned image compression methods generally optimize a rate-distortion loss,\ntrading off improvements in visual distortion for added bitrate. Increasingly,\nhowever, compressed imagery is used as an input to deep learning networks for\nvarious tasks such as classification, object detection, and superresolution. We\npropose a recognition-aware learned compression method, which optimizes a\nrate-distortion loss alongside a task-specific loss, jointly learning\ncompression and recognition networks. We augment a hierarchical\nautoencoder-based compression network with an EfficientNet recognition model\nand use two hyperparameters to trade off between distortion, bitrate, and\nrecognition performance. We characterize the classification accuracy of our\nproposed method as a function of bitrate and find that for low bitrates our\nmethod achieves as much as 26% higher recognition accuracy at equivalent\nbitrates compared to traditional methods such as Better Portable Graphics\n(BPG).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kawawa_Beaudan_M/0/1/0/all/0/1\">Maxime Kawawa-Beaudan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roggenkemper_R/0/1/0/all/0/1\">Ryan Roggenkemper</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zakhor_A/0/1/0/all/0/1\">Avideh Zakhor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling multiple scattering with deep learning: application to strain mapping from electron diffraction patterns. (arXiv:2202.00204v1 [cond-mat.mtrl-sci])","link":"http://arxiv.org/abs/2202.00204","description":"<p>Implementation of a fast, robust, and fully-automated pipeline for crystal\nstructure determination and underlying strain mapping for crystalline materials\nis important for many technological applications. Scanning electron\nnanodiffraction offers a procedure for identifying and collecting strain maps\nwith good accuracy and high spatial resolutions. However, the application of\nthis technique is limited, particularly in thick samples where the electron\nbeam can undergo multiple scattering, which introduces signal nonlinearities.\nDeep learning methods have the potential to invert these complex signals, but\nprevious implementations are often trained only on specific crystal systems or\na small subset of the crystal structure and microscope parameter phase space.\nIn this study, we implement a Fourier space, complex-valued deep neural network\ncalled FCU-Net, to invert highly nonlinear electron diffraction patterns into\nthe corresponding quantitative structure factor images. We trained the FCU-Net\nusing over 200,000 unique simulated dynamical diffraction patterns which\ninclude many different combinations of crystal structures, orientations,\nthicknesses, microscope parameters, and common experimental artifacts. We\nevaluated the trained FCU-Net model against simulated and experimental 4D-STEM\ndiffraction datasets, where it substantially out-performs conventional analysis\nmethods. Our simulated diffraction pattern library, implementation of FCU-Net,\nand trained model weights are freely available in open source repositories, and\ncan be adapted to many different diffraction measurement problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cond-mat/1/au:+Munshi_J/0/1/0/all/0/1\">Joydeep Munshi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Rakowski_A/0/1/0/all/0/1\">Alexander Rakowski</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Savitzky_B/0/1/0/all/0/1\">Benjamin H Savitzky</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zeltmann_S/0/1/0/all/0/1\">Steven E Zeltmann</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ciston_J/0/1/0/all/0/1\">Jim Ciston</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Henderson_M/0/1/0/all/0/1\">Matthew Henderson</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Cholia_S/0/1/0/all/0/1\">Shreyas Cholia</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Minor_A/0/1/0/all/0/1\">Andrew M Minor</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Chan_M/0/1/0/all/0/1\">Maria KY Chan</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Ophus_C/0/1/0/all/0/1\">Colin Ophus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ISNet: Costless and Implicit Image Segmentation for Deep Classifiers, with Application in COVID-19 Detection. (arXiv:2202.00232v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00232","description":"<p>In this work we propose a novel deep neural network (DNN) architecture,\nISNet, to solve the task of image segmentation followed by classification,\nsubstituting the common pipeline of two networks by a single model. We designed\nthe ISNet for high flexibility and performance: it allows virtually any\nclassification neural network architecture to analyze a common image as if it\nhad been previously segmented. Furthermore, in relation to the original\nclassifier, the ISNet does not cause any increment in computational cost or\narchitectural changes at run-time. To accomplish this, we introduce the concept\nof optimizing DNNs for relevance segmentation in heatmaps created by Layer-wise\nRelevance Propagation (LRP), which proves to be equivalent to the\nclassification of previously segmented images. We apply an ISNet based on a\nDenseNet121 classifier to solve the task of COVID-19 detection in chest X-rays.\nWe compare the model to a U-net (performing lung segmentation) followed by a\nDenseNet121, and to a standalone DenseNet121. Due to the implicit segmentation,\nthe ISNet precisely ignored the X-ray regions outside of the lungs; it achieved\n94.5 +/-4.1% mean accuracy with an external database, showing strong\ngeneralization capability and surpassing the other models' performances by 6 to\n7.9%. ISNet presents a fast and light methodology to perform classification\npreceded by segmentation, while also being more accurate than standard\npipelines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bassi_P/0/1/0/all/0/1\">Pedro R.A.S. Bassi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Imitation Learning from Video using a State Observer. (arXiv:2202.00243v1 [cs.RO])","link":"http://arxiv.org/abs/2202.00243","description":"<p>The imitation learning research community has recently made significant\nprogress towards the goal of enabling artificial agents to imitate behaviors\nfrom video demonstrations alone. However, current state-of-the-art approaches\ndeveloped for this problem exhibit high sample complexity due, in part, to the\nhigh-dimensional nature of video observations. Towards addressing this issue,\nwe introduce here a new algorithm called Visual Generative Adversarial\nImitation from Observation using a State Observer VGAIfO-SO. At its core,\nVGAIfO-SO seeks to address sample inefficiency using a novel, self-supervised\nstate observer, which provides estimates of lower-dimensional proprioceptive\nstate representations from high-dimensional images. We show experimentally in\nseveral continuous control environments that VGAIfO-SO is more sample efficient\nthan other IfO algorithms at learning from video-only demonstrations and can\nsometimes even achieve performance close to the Generative Adversarial\nImitation from Observation (GAIfO) algorithm that has privileged access to the\ndemonstrator's proprioceptive state information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karnan_H/0/1/0/all/0/1\">Haresh Karnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warnell_G/0/1/0/all/0/1\">Garrett Warnell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torabi_F/0/1/0/all/0/1\">Faraz Torabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1\">Peter Stone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics. (arXiv:2202.00259v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00259","description":"<p>Human-Object Interaction (HOI) detection is an essential task to understand\nhuman-centric images from a fine-grained perspective. Although end-to-end HOI\ndetection models thrive, their paradigm of parallel human/object detection and\nverb class prediction loses two-stage methods' merit: object-guided hierarchy.\nThe object in one HOI triplet gives direct clues to the verb to be predicted.\nIn this paper, we aim to boost end-to-end models with object-guided statistical\npriors. Specifically, We propose to utilize a Verb Semantic Model (VSM) and use\nsemantic aggregation to profit from this object-guided hierarchy. Similarity KL\n(SKL) loss is proposed to optimize VSM to align with the HOI dataset's priors.\nTo overcome the static semantic embedding problem, we propose to generate\ncross-modality-aware visual and semantic features by Cross-Modal Calibration\n(CMC). The above modules combined composes Object-guided Cross-modal\nCalibration Network (OCN). Experiments conducted on two popular HOI detection\nbenchmarks demonstrate the significance of incorporating the statistical prior\nknowledge and produce state-of-the-art performances. More detailed analysis\nindicates proposed modules serve as a stronger verb predictor and a more\nsuperior method of utilizing prior knowledge. The codes are available at\n\\url{https://github.com/JacobYuan7/OCN-HOI-Benchmark}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1\">Hangjie Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liangpeng Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fully Online Meta-Learning Without Task Boundaries. (arXiv:2202.00263v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00263","description":"<p>While deep networks can learn complex functions such as classifiers,\ndetectors, and trackers, many applications require models that continually\nadapt to changing input distributions, changing tasks, and changing\nenvironmental conditions. Indeed, this ability to continuously accrue knowledge\nand use past experience to learn new tasks quickly in continual settings is one\nof the key properties of an intelligent system. For complex and\nhigh-dimensional problems, simply updating the model continually with standard\nlearning algorithms such as gradient descent may result in slow adaptation.\nMeta-learning can provide a powerful tool to accelerate adaptation yet is\nconventionally studied in batch settings. In this paper, we study how\nmeta-learning can be applied to tackle online problems of this nature,\nsimultaneously adapting to changing tasks and input distributions and\nmeta-training the model in order to adapt more quickly in the future. Extending\nmeta-learning into the online setting presents its own challenges, and although\nseveral prior methods have studied related problems, they generally require a\ndiscrete notion of tasks, with known ground-truth task boundaries. Such methods\ntypically adapt to each task in sequence, resetting the model between tasks,\nrather than adapting continuously across tasks. In many real-world settings,\nsuch discrete boundaries are unavailable, and may not even exist. To address\nthese settings, we propose a Fully Online Meta-Learning (FOML) algorithm, which\ndoes not require any ground truth knowledge about the task boundaries and stays\nfully online without resetting back to pre-trained weights. Our experiments\nshow that FOML was able to learn new tasks faster than the state-of-the-art\nonline learning methods on Rainbow-MNIST, CIFAR100 and CELEBA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajasegaran_J/0/1/0/all/0/1\">Jathushan Rajasegaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1\">Chesea Finn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Access Control of Object Detection Models Using Encrypted Feature Maps. (arXiv:2202.00265v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00265","description":"<p>In this paper, we propose an access control method for object detection\nmodels. The use of encrypted images or encrypted feature maps has been\ndemonstrated to be effective in access control of models from unauthorized\naccess. However, the effectiveness of the approach has been confirmed in only\nimage classification models and semantic segmentation models, but not in object\ndetection models. In this paper, the use of encrypted feature maps is shown to\nbe effective in access control of object detection models for the first time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagamori_T/0/1/0/all/0/1\">Teru Nagamori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1\">Hiroki Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maung_A/0/1/0/all/0/1\">April Pyone Maung Maung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1\">Hitoshi Kiya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets. (arXiv:2202.00273v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00273","description":"<p>Computer graphics has experienced a recent surge of data-centric approaches\nfor photorealistic and controllable content creation. StyleGAN in particular\nsets new standards for generative modeling regarding image quality and\ncontrollability. However, StyleGAN's performance severely degrades on large\nunstructured datasets such as ImageNet. StyleGAN was designed for\ncontrollability; hence, prior works suspect its restrictive design to be\nunsuitable for diverse datasets. In contrast, we find the main limiting factor\nto be the current training strategy. Following the recently introduced\nProjected GAN paradigm, we leverage powerful neural network priors and a\nprogressive growing strategy to successfully train the latest StyleGAN3\ngenerator on ImageNet. Our final model, StyleGAN-XL, sets a new\nstate-of-the-art on large-scale image synthesis and is the first to generate\nimages at a resolution of $1024^2$ at such a dataset scale. We demonstrate that\nthis model can invert and edit images beyond the narrow domain of portraits or\nspecific object classes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sauer_A/0/1/0/all/0/1\">Axel Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarz_K/0/1/0/all/0/1\">Katja Schwarz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Laplacian2Mesh: Laplacian-Based Mesh Understanding. (arXiv:2202.00307v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00307","description":"<p>Geometric deep learning has sparked a rising interest in computer graphics to\nperform shape understanding tasks, such as shape classification and semantic\nsegmentation on three-dimensional (3D) geometric surfaces. Previous works\nexplored the significant direction by defining the operations of convolution\nand pooling on triangle meshes, but most methods explicitly utilized the graph\nconnection structure of the mesh. Motivated by the geometric spectral surface\nreconstruction theory, we introduce a novel and flexible convolutional neural\nnetwork (CNN) model, called Laplacian2Mesh, for 3D triangle mesh, which maps\nthe features of mesh in the Euclidean space to the multi-dimensional\nLaplacian-Beltrami space, which is similar to the multi-resolution input in 2D\nCNN. Mesh pooling is applied to expand the receptive field of the network by\nthe multi-space transformation of Laplacian which retains the surface topology,\nand channel self-attention convolutions are applied in the new space. Since\nimplicitly using the intrinsic geodesic connections of the mesh through the\nadjacency matrix, we do not consider the number of the neighbors of the\nvertices, thereby mesh data with different numbers of vertices can be input.\nExperiments on various learning tasks applied to 3D meshes demonstrate the\neffectiveness and efficiency of Laplacian2Mesh.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiujie Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Junjie Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shuangmin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhenyu Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1\">Shiqing Xin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"From Explanations to Segmentation: Using Explainable AI for Image Segmentation. (arXiv:2202.00315v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00315","description":"<p>The new era of image segmentation leveraging the power of Deep Neural Nets\n(DNNs) comes with a price tag: to train a neural network for pixel-wise\nsegmentation, a large amount of training samples has to be manually labeled on\npixel-precision. In this work, we address this by following an indirect\nsolution. We build upon the advances of the Explainable AI (XAI) community and\nextract a pixel-wise binary segmentation from the output of the Layer-wise\nRelevance Propagation (LRP) explaining the decision of a classification\nnetwork. We show that we achieve similar results compared to an established\nU-Net segmentation architecture, while the generation of the training data is\nsignificantly simplified. The proposed method can be trained in a weakly\nsupervised fashion, as the training samples must be only labeled on\nimage-level, at the same time enabling the output of a segmentation mask. This\nmakes it especially applicable to a wider range of real applications where\ntedious pixel-level labelling is often not possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seibold_C/0/1/0/all/0/1\">Clemens Seibold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunzel_J/0/1/0/all/0/1\">Johannes K&#xfc;nzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1\">Anna Hilsmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1\">Peter Eisert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filtered-CoPhy: Unsupervised Learning of Counterfactual Physics in Pixel Space. (arXiv:2202.00368v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00368","description":"<p>Learning causal relationships in high-dimensional data (images, videos) is a\nhard task, as they are often defined on low dimensional manifolds and must be\nextracted from complex signals dominated by appearance, lighting, textures and\nalso spurious correlations in the data. We present a method for learning\ncounterfactual reasoning of physical processes in pixel space, which requires\nthe prediction of the impact of interventions on initial conditions. Going\nbeyond the identification of structural relationships, we deal with the\nchallenging problem of forecasting raw video over long horizons. Our method\ndoes not require the knowledge or supervision of any ground truth positions or\nother object or scene properties. Our model learns and acts on a suitable\nhybrid latent representation based on a combination of dense features, sets of\n2D keypoints and an additional latent vector per keypoint. We show that this\nbetter captures the dynamics of physical processes than purely dense or sparse\nrepresentations. We introduce a new challenging and carefully designed\ncounterfactual benchmark for predictions in pixel space and outperform strong\nbaselines in physics-inspired ML and video prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Janny_S/0/1/0/all/0/1\">Steeven Janny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baradel_F/0/1/0/all/0/1\">Fabien Baradel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neverova_N/0/1/0/all/0/1\">Natalia Neverova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadri_M/0/1/0/all/0/1\">Madiha Nadri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_C/0/1/0/all/0/1\">Christian Wolf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Comparative Study of Calibration Methods for Imbalanced Class Incremental Learning. (arXiv:2202.00386v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00386","description":"<p>Deep learning approaches are successful in a wide range of AI problems and in\nparticular for visual recognition tasks. However, there are still open problems\namong which is the capacity to handle streams of visual information and the\nmanagement of class imbalance in datasets. Existing research approaches these\ntwo problems separately while they co-occur in real world applications. Here,\nwe study the problem of learning incrementally from imbalanced datasets. We\nfocus on algorithms which have a constant deep model complexity and use a\nbounded memory to store exemplars of old classes across incremental states.\nSince memory is bounded, old classes are learned with fewer images than new\nclasses and an imbalance due to incremental learning is added to the initial\ndataset imbalance. A score prediction bias in favor of new classes appears and\nwe evaluate a comprehensive set of score calibration methods to reduce it.\nEvaluation is carried with three datasets, using two dataset imbalance\nconfigurations and three bounded memory sizes. Results show that most\ncalibration methods have beneficial effect and that they are most useful for\nlower bounded memory sizes, which are most interesting in practice. As a\nsecondary contribution, we remove the usual distillation component from the\nloss function of incremental learning algorithms. We show that simpler vanilla\nfine tuning is a stronger backbone for imbalanced incremental learning\nalgorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_U/0/1/0/all/0/1\">Umang Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1\">Adrian Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belouadah_E/0/1/0/all/0/1\">Eden Belouadah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minority Class Oriented Active Learning for Imbalanced Datasets. (arXiv:2202.00390v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00390","description":"<p>Active learning aims to optimize the dataset annotation process when\nresources are constrained. Most existing methods are designed for balanced\ndatasets. Their practical applicability is limited by the fact that a majority\nof real-life datasets are actually imbalanced. Here, we introduce a new active\nlearning method which is designed for imbalanced datasets. It favors samples\nlikely to be in minority classes so as to reduce the imbalance of the labeled\nsubset and create a better representation for these classes. We also compare\ntwo training schemes for active learning: (1) the one commonly deployed in deep\nactive learning using model fine tuning for each iteration and (2) a scheme\nwhich is inspired by transfer learning and exploits generic pre-trained models\nand train shallow classifiers for each iteration. Evaluation is run with three\nimbalanced datasets. Results show that the proposed active learning method\noutperforms competitive baselines. Equally interesting, they also indicate that\nthe transfer learning training scheme outperforms model fine tuning if features\nare transferable from the generic dataset to the unlabeled one. This last\nresult is surprising and should encourage the community to explore the design\nof deep active learning methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aggarwal_U/0/1/0/all/0/1\">Umang Aggarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1\">Adrian Popescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1\">C&#xe9;line Hudelot</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAESR: Conditional Autoencoder and Super-Resolution for Learned Spatial Scalability. (arXiv:2202.00416v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00416","description":"<p>In this paper, we present CAESR, an hybrid learning-based coding approach for\nspatial scalability based on the versatile video coding (VVC) standard. Our\nframework considers a low-resolution signal encoded with VVC intra-mode as a\nbase-layer (BL), and a deep conditional autoencoder with hyperprior (AE-HP) as\nan enhancement-layer (EL) model. The EL encoder takes as inputs both the\nupscaled BL reconstruction and the original image. Our approach relies on\nconditional coding that learns the optimal mixture of the source and the\nupscaled BL image, enabling better performance than residual coding. On the\ndecoder side, a super-resolution (SR) module is used to recover high-resolution\ndetails and invert the conditional coding process. Experimental results have\nshown that our solution is competitive with the VVC full-resolution intra\ncoding while being scalable.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Bonnineau_C/0/1/0/all/0/1\">Charles Bonnineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Travers_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Travers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sidaty_N/0/1/0/all/0/1\">Naty Sidaty</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aubie_J/0/1/0/all/0/1\">Jean-Yves Aubi&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deforges_O/0/1/0/all/0/1\">Olivier Deforges</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review of Serial and Parallel Min-Cut/Max-Flow Algorithms for Computer Vision. (arXiv:2202.00418v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00418","description":"<p>Minimum cut / maximum flow (min-cut/max-flow) algorithms are used to solve a\nvariety of problems in computer vision and thus significant effort has been put\ninto developing fast min-cut/max-flow algorithms. This makes it difficult to\nchoose an optimal algorithm for a given problem - especially for parallel\nalgorithms, which have not been thoroughly compared. In this paper, we review\nthe state-of-the-art min-cut/max-flow algorithms for unstructured graphs in\ncomputer vision. We evaluate run time performance and memory use of various\nimplementations of both serial and parallel algorithms on a set of graph cut\nproblems. Our results show that the Hochbaum pseudoflow algorithm is the\nfastest serial algorithm closely followed by the Excesses Incremental Breadth\nFirst Search algorithm, while the Boykov-Kolmogorov algorithm is the most\nmemory efficient. The best parallel algorithm is the adaptive bottom-up merging\napproach by Liu and Sun. Additionally, we show significant variations in\nperformance between different implementations the same algorithms highlighting\nthe importance of low-level implementation details. Finally, we note that\nexisting parallel min-cut/max-flow algorithms can significantly outperform\nserial algorithms on large problems but suffers from added overhead on small to\nmedium problems. Implementations of all algorithms are available at\nhttps://github.com/patmjen/maxflow_algorithms\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jensen_P/0/1/0/all/0/1\">Patrick M. Jensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeppesen_N/0/1/0/all/0/1\">Niels Jeppesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_A/0/1/0/all/0/1\">Anders B. Dahl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahl_V/0/1/0/all/0/1\">Vedrana A. Dahl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sinogram Enhancement with Generative Adversarial Networks using Shape Priors. (arXiv:2202.00419v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00419","description":"<p>Compensating scarce measurements by inferring them from computational models\nis a way to address ill-posed inverse problems. We tackle Limited Angle\nTomography by completing the set of acquisitions using a generative model and\nprior-knowledge about the scanned object. Using a Generative Adversarial\nNetwork as model and Computer-Assisted Design data as shape prior, we\ndemonstrate a quantitative and qualitative advantage of our technique over\nother state-of-the-art methods. Inferring a substantial number of consecutive\nmissing measurements, we offer an alternative to other image inpainting\ntechniques that fall short of providing a satisfying answer to our research\nquestion: can X-Ray exposition be reduced by using generative models to infer\nlacking measurements?\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Valat_E/0/1/0/all/0/1\">Emilien Valat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Farrahi_K/0/1/0/all/0/1\">Katayoun Farrahi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blumensath_T/0/1/0/all/0/1\">Thomas Blumensath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual Attentive Fusion for Incremental Learning in Semantic Segmentation. (arXiv:2202.00432v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00432","description":"<p>Over the past years, semantic segmentation, as many other tasks in computer\nvision, benefited from the progress in deep neural networks, resulting in\nsignificantly improved performance. However, deep architectures trained with\ngradient-based techniques suffer from catastrophic forgetting, which is the\ntendency to forget previously learned knowledge while learning new tasks.\nAiming at devising strategies to counteract this effect, incremental learning\napproaches have gained popularity over the past years. However, the first\nincremental learning methods for semantic segmentation appeared only recently.\nWhile effective, these approaches do not account for a crucial aspect in\npixel-level dense prediction problems, i.e. the role of attention mechanisms.\nTo fill this gap, in this paper we introduce a novel attentive feature\ndistillation approach to mitigate catastrophic forgetting while accounting for\nsemantic spatial- and channel-level dependencies. Furthermore, we propose a\n{continual attentive fusion} structure, which takes advantage of the attention\nlearned from the new and the old tasks while learning features for the new\ntask. Finally, we also introduce a novel strategy to account for the background\nclass in the distillation loss, thus preventing biased predictions. We\ndemonstrate the effectiveness of our approach with an extensive evaluation on\nPascal-VOC 2012 and ADE20K, setting a new state of the art.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guanglei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1\">Enrico Fini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1\">Dan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1\">Paolo Rota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Mingli Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1\">Elisa Ricci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Order Networks for Action Unit Detection. (arXiv:2202.00446v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00446","description":"<p>Deep multi-task methods, where several tasks are learned within a single\nnetwork, have recently attracted increasing attention. Burning point of this\nattention is their capacity to capture inter-task relationships. Current\napproaches either only rely on weight sharing, or add explicit dependency\nmodelling by decomposing the task joint distribution using Bayes chain rule. If\nthe latter strategy yields comprehensive inter-task relationships modelling, it\nrequires imposing an arbitrary order into an unordered task set. Most\nimportantly, this sequence ordering choice has been identified as a critical\nsource of performance variations. In this paper, we present Multi-Order Network\n(MONET), a multi-task learning method with joint task order optimization. MONET\nuses a differentiable order selection based on soft order modelling inside\nBirkhoff's polytope to jointly learn task-wise recurrent modules with their\noptimal chaining order. Furthermore, we introduce warm up and order dropout to\nenhance order selection by encouraging order exploration. Experimentally, we\nfirst validate MONET capacity to retrieve the optimal order in a toy\nenvironment. Second, we use an attribute detection scenario to show that MONET\noutperforms existing multi-task baselines on a wide range of dependency\nsettings. Finally, we demonstrate that MONET significantly extends\nstate-of-the-art performance in Facial Action Unit detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tallec_G/0/1/0/all/0/1\">Gauthier Tallec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1\">Kevin Bailly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sim2Real Object-Centric Keypoint Detection and Description. (arXiv:2202.00448v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00448","description":"<p>Keypoint detection and description play a central role in computer vision.\nMost existing methods are in the form of scene-level prediction, without\nreturning the object classes of different keypoints. In this paper, we propose\nthe object-centric formulation, which, beyond the conventional setting,\nrequires further identifying which object each interest point belongs to. With\nsuch fine-grained information, our framework enables more downstream\npotentials, such as object-level matching and pose estimation in a clustered\nenvironment. To get around the difficulty of label collection in the real\nworld, we develop a sim2real contrastive learning mechanism that can generalize\nthe model trained in simulation to real-world applications. The novelties of\nour training method are three-fold: (i) we integrate the uncertainty into the\nlearning framework to improve feature description of hard cases, e.g.,\nless-textured or symmetric patches; (ii) we decouple the object descriptor into\ntwo output branches -- intra-object salience and inter-object distinctness,\nresulting in a better pixel-wise description; (iii) we enforce cross-view\nsemantic consistency for enhanced robustness in representation learning.\nComprehensive experiments on image matching and 6D pose estimation verify the\nencouraging generalization ability of our method from simulation to reality.\nParticularly for 6D pose estimation, our method significantly outperforms\ntypical unsupervised/sim2real methods, achieving a closer gap with the fully\nsupervised counterpart.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1\">Chengliang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1\">Jinshan Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1\">Xiaodong Mu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wenbing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Feature Attribution: An Information-Theoretic Perspective. (arXiv:2202.00449v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00449","description":"<p>With a variety of local feature attribution methods being proposed in recent\nyears, follow-up work suggested several evaluation strategies. To assess the\nattribution quality across different attribution techniques, the most popular\namong these evaluation strategies in the image domain use pixel perturbations.\nHowever, recent advances discovered that different evaluation strategies\nproduce conflicting rankings of attribution methods and can be prohibitively\nexpensive to compute. In this work, we present an information-theoretic\nanalysis of evaluation strategies based on pixel perturbations. Our findings\nreveal that the results output by different evaluation strategies are strongly\naffected by information leakage through the shape of the removed pixels as\nopposed to their actual values. Using our theoretical insights, we propose a\nnovel evaluation framework termed Remove and Debias (ROAD) which offers two\ncontributions: First, it mitigates the impact of the confounders, which entails\nhigher consistency among evaluation strategies. Second, ROAD does not require\nthe computationally expensive retraining step and saves up to 99% in\ncomputational costs compared to the state-of-the-art. Our source code is\navailable at https://github.com/tleemann/road_evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1\">Yao Rong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leemann_T/0/1/0/all/0/1\">Tobias Leemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borisov_V/0/1/0/all/0/1\">Vadim Borisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1\">Gjergji Kasneci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1\">Enkelejda Kasneci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HCSC: Hierarchical Contrastive Selective Coding. (arXiv:2202.00455v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00455","description":"<p>Hierarchical semantic structures naturally exist in an image dataset, in\nwhich several semantically relevant image clusters can be further integrated\ninto a larger cluster with coarser-grained semantics. Capturing such structures\nwith image representations can greatly benefit the semantic understanding on\nvarious downstream tasks. Existing contrastive representation learning methods\nlack such an important model capability. In addition, the negative pairs used\nin these methods are not guaranteed to be semantically distinct, which could\nfurther hamper the structural correctness of learned image representations. To\ntackle these limitations, we propose a novel contrastive learning framework\ncalled Hierarchical Contrastive Selective Coding (HCSC). In this framework, a\nset of hierarchical prototypes are constructed and also dynamically updated to\nrepresent the hierarchical semantic structures underlying the data in the\nlatent space. To make image representations better fit such semantic\nstructures, we employ and further improve conventional instance-wise and\nprototypical contrastive learning via an elaborate pair selection scheme. This\nscheme seeks to select more diverse positive pairs with similar semantics and\nmore precise negative pairs with truly distinct semantics. On extensive\ndownstream tasks, we verify the superior performance of HCSC over\nstate-of-the-art contrastive methods, and the effectiveness of major model\ncomponents is proved by plentiful analytical studies. Our source code and model\nweights are available at https://github.com/gyfastas/HCSC\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuanfan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Minghao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiawen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1\">Bingbing Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xuanyu Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenbang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A generalizable approach based on U-Net model for automatic Intra retinal cyst segmentation in SD-OCT images. (arXiv:2202.00465v1 [eess.IV])","link":"http://arxiv.org/abs/2202.00465","description":"<p>Intra retinal fluids or Cysts are one of the important symptoms of macular\npathologies that are efficiently visualized in OCT images. Automatic\nsegmentation of these abnormalities has been widely investigated in medical\nimage processing studies. In this paper, we propose a new U-Net-based approach\nfor Intra retinal cyst segmentation across different vendors that improves some\nof the challenges faced by previous deep-based techniques. The proposed method\nhas two main steps: 1- prior information embedding and input data adjustment,\nand 2- IRC segmentation model. In the first step, we inject the information\ninto the network in a way that overcomes some of the network limitations in\nreceiving data and learning important contextual knowledge. And in the next\nstep, we introduced a connection module between encoder and decoder parts of\nthe standard U-Net architecture that transfers information more effectively\nfrom the encoder to the decoder part. Two public datasets namely OPTIMA and\nKERMANY were employed to evaluate the proposed method. Results showed that the\nproposed method is an efficient vendor-independent approach for IRC\nsegmentation with mean Dice values of 0.78 and 0.81 on the OPTIMA and KERMANY\ndatasets, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ganjee_R/0/1/0/all/0/1\">Razieh Ganjee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moghaddam_M/0/1/0/all/0/1\">Mohsen Ebrahimi Moghaddam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nourinia_R/0/1/0/all/0/1\">Ramin Nourinia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The impact of removing head movements on audio-visual speech enhancement. (arXiv:2202.00538v1 [cs.SD])","link":"http://arxiv.org/abs/2202.00538","description":"<p>This paper investigates the impact of head movements on audio-visual speech\nenhancement (AVSE). Although being a common conversational feature, head\nmovements have been ignored by past and recent studies: they challenge today's\nlearning-based methods as they often degrade the performance of models that are\ntrained on clean, frontal, and steady face images. To alleviate this problem,\nwe propose to use robust face frontalization (RFF) in combination with an AVSE\nmethod based on a variational auto-encoder (VAE) model. We briefly describe the\nbasic ingredients of the proposed pipeline and we perform experiments with a\nrecently released audio-visual dataset. In the light of these experiments, and\nbased on three standard metrics, namely STOI, PESQ and SI-SDR, we conclude that\nRFF improves the performance of AVSE by a considerable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhiqi Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadeghi_M/0/1/0/all/0/1\">Mostafa Sadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1\">Radu Horaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alameda_Pineda_X/0/1/0/all/0/1\">Xavier Alameda-Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donley_J/0/1/0/all/0/1\">Jacob Donley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Anurag Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Development of a neural network to recognize standards and features from 3D CAD models. (arXiv:2202.00573v1 [cs.GR])","link":"http://arxiv.org/abs/2202.00573","description":"<p>Focus of this work is to recognize standards and further features directly\nfrom 3D CAD models. For this reason, a neural network was trained to recognize\nnine classes of machine elements. After the system identified a part as a\nstandard, like a hexagon head screw after the DIN EN ISO 8676, it accesses the\ngeometrical information of the CAD system via the Application Programming\nInterface (API). In the API, the system searches for necessary information to\ndescribe the part appropriately. Based on this information standardized parts\ncan be recognized in detail and supplemented with further information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neb_A/0/1/0/all/0/1\">Alexander Neb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briki_I/0/1/0/all/0/1\">Iyed Briki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schoenhof_R/0/1/0/all/0/1\">Raoul Schoenhof</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fishing for User Data in Large-Batch Federated Learning via Gradient Magnification. (arXiv:2202.00580v1 [cs.LG])","link":"http://arxiv.org/abs/2202.00580","description":"<p>Federated learning (FL) has rapidly risen in popularity due to its promise of\nprivacy and efficiency. Previous works have exposed privacy vulnerabilities in\nthe FL pipeline by recovering user data from gradient updates. However,\nexisting attacks fail to address realistic settings because they either 1)\nrequire a `toy' settings with very small batch sizes, or 2) require unrealistic\nand conspicuous architecture modifications. We introduce a new strategy that\ndramatically elevates existing attacks to operate on batches of arbitrarily\nlarge size, and without architectural modifications. Our model-agnostic\nstrategy only requires modifications to the model parameters sent to the user,\nwhich is a realistic threat model in many scenarios. We demonstrate the\nstrategy in challenging large-scale settings, obtaining high-fidelity data\nextraction in both cross-device and cross-silo federated learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1\">Yuxin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1\">Jonas Geiping</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fowl_L/0/1/0/all/0/1\">Liam Fowl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1\">Micah Goldblum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Datamodels: Predicting Predictions from Training Data. (arXiv:2202.00622v1 [stat.ML])","link":"http://arxiv.org/abs/2202.00622","description":"<p>We present a conceptual framework, datamodeling, for analyzing the behavior\nof a model class in terms of the training data. For any fixed \"target\" example\n$x$, training set $S$, and learning algorithm, a datamodel is a parameterized\nfunction $2^S \\to \\mathbb{R}$ that for any subset of $S' \\subset S$ -- using\nonly information about which examples of $S$ are contained in $S'$ -- predicts\nthe outcome of training a model on $S'$ and evaluating on $x$. Despite the\npotential complexity of the underlying process being approximated (e.g.,\nend-to-end training and evaluation of deep neural networks), we show that even\nsimple linear datamodels can successfully predict model outputs. We then\ndemonstrate that datamodels give rise to a variety of applications, such as:\naccurately predicting the effect of dataset counterfactuals; identifying\nbrittle predictions; finding semantically similar examples; quantifying\ntrain-test leakage; and embedding data into a well-behaved and feature-rich\nrepresentation space. Data for this paper (including pre-computed datamodels as\nwell as raw predictions from four million trained deep neural networks) is\navailable at https://github.com/MadryLab/datamodels-data .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ilyas_A/0/1/0/all/0/1\">Andrew Ilyas</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Park_S/0/1/0/all/0/1\">Sung Min Park</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Engstrom_L/0/1/0/all/0/1\">Logan Engstrom</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Leclerc_G/0/1/0/all/0/1\">Guillaume Leclerc</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Madry_A/0/1/0/all/0/1\">Aleksander Madry</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing the Amount of Real World Data for Object Detector Training with Synthetic Data. (arXiv:2202.00632v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00632","description":"<p>A number of studies have investigated the training of neural networks with\nsynthetic data for applications in the real world. The aim of this study is to\nquantify how much real world data can be saved when using a mixed dataset of\nsynthetic and real world data. By modeling the relationship between the number\nof training examples and detection performance by a simple power law, we find\nthat the need for real world data can be reduced by up to 70% without\nsacrificing detection performance. The training of object detection networks is\nespecially enhanced by enriching the mixed dataset with classes\nunderrepresented in the real world dataset. The results indicate that mixed\ndatasets with real world data ratios between 5% and 20% reduce the need for\nreal world data the most without reducing the detection performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burdorf_S/0/1/0/all/0/1\">Sven Burdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plum_K/0/1/0/all/0/1\">Karoline Plum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasenklever_D/0/1/0/all/0/1\">Daniel Hasenklever</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AI-based Medical e-Diagnosis for Fast and Automatic Ventricular Volume Measurement in the Patients with Normal Pressure Hydrocephalus. (arXiv:2202.00650v1 [physics.med-ph])","link":"http://arxiv.org/abs/2202.00650","description":"<p>Based on CT and MRI images acquired from normal pressure hydrocephalus (NPH)\npatients, using machine learning methods, we aim to establish a multi-modal and\nhigh-performance automatic ventricle segmentation method to achieve efficient\nand accurate automatic measurement of the ventricular volume. First, we extract\nthe brain CT and MRI images of 143 definite NPH patients. Second, we manually\nlabel the ventricular volume (VV) and intracranial volume (ICV). Then, we use\nmachine learning method to extract features and establish automatic ventricle\nsegmentation model. Finally, we verify the reliability of the model and\nachieved automatic measurement of VV and ICV. In CT images, the Dice similarity\ncoefficient (DSC), Intraclass Correlation Coefficient (ICC), Pearson\ncorrelation, and Bland-Altman analysis of the automatic and manual segmentation\nresult of the VV were 0.95, 0.99, 0.99, and 4.2$\\pm$2.6 respectively. The\nresults of ICV were 0.96, 0.99, 0.99, and 6.0$\\pm$3.8 respectively. The whole\nprocess takes 3.4$\\pm$0.3 seconds. In MRI images, the DSC, ICC, Pearson\ncorrelation, and Bland-Altman analysis of the automatic and manual segmentation\nresult of the VV were 0.94, 0.99, 0.99, and 2.0$\\pm$0.6 respectively. The\nresults of ICV were 0.93, 0.99, 0.99, and 7.9$\\pm$3.8 respectively. The whole\nprocess took 1.9$\\pm$0.1 seconds. We have established a multi-modal and\nhigh-performance automatic ventricle segmentation method to achieve efficient\nand accurate automatic measurement of the ventricular volume of NPH patients.\nThis can help clinicians quickly and accurately understand the situation of NPH\npatient's ventricles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Zhou_X/0/1/0/all/0/1\">Xi Zhou</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ye_Q/0/1/0/all/0/1\">Qinghao Ye</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_X/0/1/0/all/0/1\">Xiaolin Yang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chen_J/0/1/0/all/0/1\">Jiakuan Chen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ma_H/0/1/0/all/0/1\">Haiqin Ma</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Ser_J/0/1/0/all/0/1\">Javier Del Ser</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stay Positive: Non-Negative Image Synthesis for Augmented Reality. (arXiv:2202.00659v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00659","description":"<p>In applications such as optical see-through and projector augmented reality,\nproducing images amounts to solving non-negative image generation, where one\ncan only add light to an existing image. Most image generation methods,\nhowever, are ill-suited to this problem setting, as they make the assumption\nthat one can assign arbitrary color to each pixel. In fact, naive application\nof existing methods fails even in simple domains such as MNIST digits, since\none cannot create darker pixels by adding light. We know, however, that the\nhuman visual system can be fooled by optical illusions involving certain\nspatial configurations of brightness and contrast. Our key insight is that one\ncan leverage this behavior to produce high quality images with negligible\nartifacts. For example, we can create the illusion of darker patches by\nbrightening surrounding pixels. We propose a novel optimization procedure to\nproduce images that satisfy both semantic and non-negativity constraints. Our\napproach can incorporate existing state-of-the-art methods, and exhibits strong\nperformance in a variety of tasks including image-to-image translation and\nstyle transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1\">Katie Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Guandao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xian_W/0/1/0/all/0/1\">Wenqi Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraldsson_H/0/1/0/all/0/1\">Harald Haraldsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hariharan_B/0/1/0/all/0/1\">Bharath Hariharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belongie_S/0/1/0/all/0/1\">Serge Belongie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Interactron: Embodied Adaptive Object Detection. (arXiv:2202.00660v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00660","description":"<p>Over the years various methods have been proposed for the problem of object\ndetection. Recently, we have witnessed great strides in this domain owing to\nthe emergence of powerful deep neural networks. However, there are typically\ntwo main assumptions common among these approaches. First, the model is trained\non a fixed training set and is evaluated on a pre-recorded test set. Second,\nthe model is kept frozen after the training phase, so no further updates are\nperformed after the training is finished. These two assumptions limit the\napplicability of these methods to real-world settings. In this paper, we\npropose Interactron, a method for adaptive object detection in an interactive\nsetting, where the goal is to perform object detection in images observed by an\nembodied agent navigating in different environments. Our idea is to continue\ntraining during inference and adapt the model at test time without any explicit\nsupervision via interacting with the environment. Our adaptive object detection\nmodel provides a 11.8 point improvement in AP (and 19.1 points in AP50) over\nDETR, a recent, high-performance object detector. Moreover, we show that our\nobject detection model adapts to environments with completely different\nappearance characteristics, and its performance is on par with a model trained\nwith full supervision for those environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kotar_K/0/1/0/all/0/1\">Klemen Kotar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1\">Roozbeh Mottaghi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Kernelized Dense Geometric Matching. (arXiv:2202.00667v1 [cs.CV])","link":"http://arxiv.org/abs/2202.00667","description":"<p>Dense geometric matching is a challenging computer vision task, requiring\naccurate correspondences under extreme variations in viewpoint and\nillumination, even for low-texture regions. In this task, finding accurate\nglobal correspondences is essential for later refinement stages. The current\nlearning based paradigm is to perform global fixed-size correlation, followed\nby flattening and convolution to predict correspondences. In this work, we\nconsider the problem from a different perspective and propose to formulate\nglobal correspondence estimation as a continuous probabilistic regression task\nusing deep kernels, yielding a novel approach to learning dense\ncorrespondences. Our full approach, \\textbf{D}eep \\textbf{K}ernelized\n\\textbf{M}atching, achieves significant improvements compared to the\nstate-of-the-art on the competitive HPatches and YFCC100m benchmarks, and we\ndissect the gains of our contributions in a thorough ablation study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Edstedt_J/0/1/0/all/0/1\">Johan Edstedt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extract an essential skeleton of a character as a graph from a character image. (arXiv:1506.05068v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1506.05068","description":"<p>This paper aims to make a graph representing an essential skeleton of a\ncharacter from an image that includes a machine printed or a handwritten\ncharacter using growing neural gas (GNG) method and relative network graph\n(RNG) algorithm. The visual system in our brain can recognize printed\ncharacters and handwritten characters easily, robustly, and precisely. How does\nour brain robustly recognize characters? The visual processing in our brain\nuses the essential features of an object, such as crosses and corners. These\nfeatures will be helpful for character recognition by a computer. However,\nextraction of the features is difficult. If the skeleton of a character is\nrepresented as a graph, we can more easily extract the features. To extract the\nskeleton of a character as a graph from an image, this paper proposes the new\napproach using GNG and RNG algorithm. I achieved to extract skeleton graphs\nfrom images including distorted, noisy, and handwritten characters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fujita_K/0/1/0/all/0/1\">Kazuhisa Fujita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Iterative Augmentation of Visual Evidence for Weakly-Supervised Lesion Localization in Deep Interpretability Frameworks: Application to Color Fundus Images. (arXiv:1910.07373v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1910.07373","description":"<p>Interpretability of deep learning (DL) systems is gaining attention in\nmedical imaging to increase experts' trust in the obtained predictions and\nfacilitate their integration in clinical settings. We propose a deep\nvisualization method to generate interpretability of DL classification tasks in\nmedical imaging by means of visual evidence augmentation. The proposed method\niteratively unveils abnormalities based on the prediction of a classifier\ntrained only with image-level labels. For each image, initial visual evidence\nof the prediction is extracted with a given visual attribution technique. This\nprovides localization of abnormalities that are then removed through selective\ninpainting. We iteratively apply this procedure until the system considers the\nimage as normal. This yields augmented visual evidence, including less\ndiscriminative lesions which were not detected at first but should be\nconsidered for final diagnosis. We apply the method to grading of two retinal\ndiseases in color fundus images: diabetic retinopathy (DR) and age-related\nmacular degeneration (AMD). We evaluate the generated visual evidence and the\nperformance of weakly-supervised localization of different types of DR and AMD\nabnormalities, both qualitatively and quantitatively. We show that the\naugmented visual evidence of the predictions highlights the biomarkers\nconsidered by experts for diagnosis and improves the final localization\nperformance. It results in a relative increase of 11.2+/-2.0% per image\nregarding sensitivity averaged at 10 false positives/image on average, when\napplied to different classification tasks, visual attribution techniques and\nnetwork architectures. This makes the proposed method a useful tool for\nexhaustive visual support of DL classifiers in medical imaging.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Gonzalo_C/0/1/0/all/0/1\">Cristina Gonz&#xe1;lez-Gonzalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liefers_B/0/1/0/all/0/1\">Bart Liefers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginneken_B/0/1/0/all/0/1\">Bram van Ginneken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_C/0/1/0/all/0/1\">Clara I. S&#xe1;nchez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning for Free-Hand Sketch: A Survey. (arXiv:2001.02600v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2001.02600","description":"<p>Free-hand sketches are highly illustrative, and have been widely used by\nhumans to depict objects or stories from ancient times to the present. The\nrecent prevalence of touchscreen devices has made sketch creation a much easier\ntask than ever and consequently made sketch-oriented applications increasingly\npopular. The progress of deep learning has immensely benefited free-hand sketch\nresearch and applications. This paper presents a comprehensive survey of the\ndeep learning techniques oriented at free-hand sketch data, and the\napplications that they enable. The main contents of this survey include: (i) A\ndiscussion of the intrinsic traits and unique challenges of free-hand sketch,\nto highlight the essential differences between sketch data and other data\nmodalities, e.g., natural photos. (ii) A review of the developments of\nfree-hand sketch research in the deep learning era, by surveying existing\ndatasets, research topics, and the state-of-the-art methods through a detailed\ntaxonomy and experimental evaluation. (iii) Promotion of future work via a\ndiscussion of bottlenecks, open problems, and potential research directions for\nthe community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Peng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hospedales_T/0/1/0/all/0/1\">Timothy M. Hospedales</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qiyue Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ulixes: Facial Recognition Privacy with Adversarial Machine Learning. (arXiv:2010.10242v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2010.10242","description":"<p>Facial recognition tools are becoming exceptionally accurate in identifying\npeople from images. However, this comes at the cost of privacy for users of\nonline services with photo management (e.g. social media platforms).\nParticularly troubling is the ability to leverage unsupervised learning to\nrecognize faces even when the user has not labeled their images. In this paper\nwe propose Ulixes, a strategy to generate visually non-invasive facial noise\nmasks that yield adversarial examples, preventing the formation of identifiable\nuser clusters in the embedding space of facial encoders. This is applicable\neven when a user is unmasked and labeled images are available online. We\ndemonstrate the effectiveness of Ulixes by showing that various classification\nand clustering methods cannot reliably label the adversarial examples we\ngenerate. We also study the effects of Ulixes in various black-box settings and\ncompare it to the current state of the art in adversarial machine learning.\nFinally, we challenge the effectiveness of Ulixes against adversarially trained\nmodels and show that it is robust to countermeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cilloni_T/0/1/0/all/0/1\">Thomas Cilloni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walter_C/0/1/0/all/0/1\">Charles Walter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleming_C/0/1/0/all/0/1\">Charles Fleming</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DOC2PPT: Automatic Presentation Slides Generation from Scientific Documents. (arXiv:2101.11796v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2101.11796","description":"<p>Creating presentation materials requires complex multimodal reasoning skills\nto summarize key concepts and arrange them in a logical and visually pleasing\nmanner. Can machines learn to emulate this laborious process? We present a\nnovel task and approach for document-to-slide generation. Solving this involves\ndocument summarization, image and text retrieval, slide structure and layout\nprediction to arrange key elements in a form suitable for presentation. We\npropose a hierarchical sequence-to-sequence approach to tackle our task in an\nend-to-end manner. Our approach exploits the inherent structures within\ndocuments and slides and incorporates paraphrasing and layout prediction\nmodules to generate slides. To help accelerate research in this domain, we\nrelease a dataset about 6K paired documents and slide decks used in our\nexperiments. We show that our approach outperforms strong baselines and\nproduces slides with rich content and aligned imagery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1\">Tsu-Jui Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1\">Daniel McDuff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yale Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exact Hypergraph Matching Algorithm for Nuclear Identification in Embryonic Caenorhabditis elegans. (arXiv:2104.10003v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10003","description":"<p>Finding an optimal correspondence between point sets is a common task in\ncomputer vision. Existing techniques assume relatively simple relationships\namong points and do not guarantee an optimal match. We introduce an algorithm\ncapable of exactly solving point set matching by modeling the task as\nhypergraph matching. The algorithm extends the classical branch and bound\nparadigm to select and aggregate vertices under a proposed decomposition of the\nmultilinear objective function. The methodology is motivated by Caenorhabditis\nelegans, a model organism used frequently in developmental biology and\nneurobiology. The embryonic C. elegans contains seam cells that can act as\nfiducial markers allowing the identification of other nuclei during embryo\ndevelopment. The proposed algorithm identifies seam cells more accurately than\nestablished point-set matching methods, while providing a framework to approach\nother similarly complex point set matching tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauziere_A/0/1/0/all/0/1\">Andrew Lauziere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christensen_R/0/1/0/all/0/1\">Ryan Christensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shroff_H/0/1/0/all/0/1\">Hari Shroff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balan_R/0/1/0/all/0/1\">Radu Balan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Transformer for Accurate and Reliable Salient Object Detection. (arXiv:2104.10127v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10127","description":"<p>In this paper, we conduct extensive research on exploring the contribution of\ntransformers to salient object detection, achieving both accurate and reliable\nsaliency predictions. We first investigate transformers for accurate salient\nobject detection with deterministic neural networks, and explain that the\neffective structure modeling and global context modeling abilities lead to its\nsuperior performance compared with the CNN based frameworks. Then, we design\nstochastic networks to evaluate the transformers' ability in reliable salient\nobject detection. We observe that both CNN and transformer based frameworks\nsuffer greatly from the over-confidence issue, where the models tend to\ngenerate wrong predictions with high confidence, leading to over-confident\npredictions or a poorly-calibrated model. To estimate the calibration degree of\nboth CNN- and transformer-based frameworks for reliable saliency prediction, we\nintroduce generative adversarial network (GAN) based models to identify the\nover-confident regions by sampling from the latent space. Specifically, we\npresent the inferential generative adversarial network (iGAN). Different from\nthe conventional GAN based framework, which defines the distribution of the\nlatent variable as fixed standard normal distribution N(0,1), the proposed\n\"iGAN\" infers the latent variable by gradient-based Markov Chain Monte Carlo\n(MCMC), namely Langevin dynamics. We apply the proposed inferential generative\nadversarial network (iGAN) to both fully and weakly supervised salient object\ndetection, and explain that iGAN within the transformer framework leads to both\naccurate and reliable salient object detection. The source code and\nexperimental results are publicly available via our project page:\nhttps://github.com/fupiao1998/TrasformerSOD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhexiong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust 3D Cell Segmentation: Extending the View of Cellpose. (arXiv:2105.00794v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2105.00794","description":"<p>Increasing data set sizes of 3D microscopy imaging experiments demand for an\nautomation of segmentation processes to be able to extract meaningful\nbiomedical information. Due to the shortage of annotated 3D image data that can\nbe used for machine learning-based approaches, 3D segmentation approaches are\nrequired to be robust and to generalize well to unseen data. The Cellpose\napproach proposed by Stringer et al. proved to be such a generalist approach\nfor cell instance segmentation tasks. In this paper, we extend the Cellpose\napproach to improve segmentation accuracy on 3D image data and we further show\nhow the formulation of the gradient maps can be simplified while still being\nrobust and reaching similar segmentation accuracy. The code is publicly\navailable and was integrated into two established open-source applications that\nallow using the 3D extension of Cellpose without any programming knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Eschweiler_D/0/1/0/all/0/1\">Dennis Eschweiler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Smith_R/0/1/0/all/0/1\">Richard S. Smith</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stegmaier_J/0/1/0/all/0/1\">Johannes Stegmaier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continual 3D Convolutional Neural Networks for Real-time Processing of Videos. (arXiv:2106.00050v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.00050","description":"<p>This paper introduces Continual 3D Convolutional Neural Networks (Co3D CNNs),\na new computational formulation of spatio-temporal 3D CNNs, in which videos are\nprocessed frame-by-frame rather than by clip. In online processing tasks\ndemanding frame-wise predictions, Co3D CNNs dispense with the computational\nredundancies of regular 3D CNNs, namely the repeated convolutions over frames,\nwhich appear in overlapping clips. We show that Continual 3D CNNs can reuse\npreexisting 3D-CNN weights to reduce the per-prediction floating point\noperations (FLOPs) in proportion to the temporal receptive field while\nretaining similar memory requirements and accuracy. This is validated with\nmultiple models on the Kinetics-400 and Charades datasets with remarkable\nresults: Continual X3D models attain state-of-the-art complexity/accuracy\ntrade-offs on Kinetics-400 with 12.1-15.3x reductions of FLOPs and 2.3-3.8%\nimprovements in accuracy compared to regular X3D models while reducing peak\nmemory consumption by up to 48%. Moreover, we investigate the transient\nresponse of Co3D CNNs at start-up and perform an extensive benchmark of\non-hardware processing speed and accuracy for publicly available 3D CNNs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hedegaard_L/0/1/0/all/0/1\">Lukas Hedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topology-Preserved Human Reconstruction with Details. (arXiv:2106.06313v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.06313","description":"<p>It is challenging to directly estimate the human geometry from a single image\ndue to the high diversity and complexity of body shapes with the various\nclothing styles. Most of model-based approaches are limited to predict the\nshape and pose of a minimally clothed body with over-smoothing surface. While\ncapturing the fine detailed geometries, the model-free methods are lack of the\nfixed mesh topology. To address these issues, we propose a novel\ntopology-preserved human reconstruction approach by bridging the gap between\nmodel-based and model-free human reconstruction. We present an end-to-end\nneural network that simultaneously predicts the pixel-aligned implicit surface\nand an explicit mesh model built by graph convolutional neural network.\nExperiments on DeepHuman and our collected dataset showed that our approach is\neffective. The code will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Lixiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jianke Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Robustness via Fisher-Rao Regularization. (arXiv:2106.06685v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2106.06685","description":"<p>Adversarial robustness has become a topic of growing interest in machine\nlearning since it was observed that neural networks tend to be brittle. We\npropose an information-geometric formulation of adversarial defense and\nintroduce FIRE, a new Fisher-Rao regularization for the categorical\ncross-entropy loss, which is based on the geodesic distance between the softmax\noutputs corresponding to natural and perturbed input features. Based on the\ninformation-geometric properties of the class of softmax distributions, we\nderive an explicit characterization of the Fisher-Rao Distance (FRD) for the\nbinary and multiclass cases, and draw some interesting properties as well as\nconnections with standard regularization metrics. Furthermore, for a simple\nlinear and Gaussian model, we show that all Pareto-optimal points in the\naccuracy-robustness region can be reached by FIRE while other state-of-the-art\nmethods fail. Empirically, we evaluate the performance of various classifiers\ntrained with the proposed loss on standard datasets, showing up to a\nsimultaneous 1\\% of improvement in terms of clean and robust performances while\nreducing the training time by 20\\% over the best-performing methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picot_M/0/1/0/all/0/1\">Marine Picot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Messina_F/0/1/0/all/0/1\">Francisco Messina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boudiaf_M/0/1/0/all/0/1\">Malik Boudiaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_F/0/1/0/all/0/1\">Fabrice Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1\">Ismail Ben Ayed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1\">Pablo Piantanida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-Clustering Point Clouds of Crop Fields Using Scalable Methods. (arXiv:2107.10950v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10950","description":"<p>In order to apply the recent successes of machine learning and automated\nplant phenotyping on a large scale using agricultural robotics, efficient and\ngeneral algorithms must be designed to intelligently split crop fields into\nsmall, yet actionable, portions that can then be processed by more complex\nalgorithms. In this paper, we notice a similarity between the current\nstate-of-the-art for separating corn plants and a commonly used density-based\nclustering algorithm, Quickshift. Exploiting this similarity we propose a\nnumber of novel, application-specific algorithms with the goal of producing a\ngeneral and scalable field segmentation algorithm. The novel algorithms\nproposed in this work are shown to produce quantitatively better results than\nthe current state-of-the-art while being less sensitive to input parameters and\nmaintaining the same algorithmic time complexity. When incorporated into\nfield-scale phenotyping systems, the proposed algorithms should work as a\ndrop-in replacement that can greatly improve the accuracy of results while\nensuring that performance and scalability remain undiminished.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nelson_H/0/1/0/all/0/1\">Henry J. Nelson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papanikolopoulos_N/0/1/0/all/0/1\">Nikolaos Papanikolopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning the shape of female breasts: an open-access 3D statistical shape model of the female breast built from 110 breast scans. (arXiv:2107.13463v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.13463","description":"<p>We present the Regensburg Breast Shape Model (RBSM) -- a 3D statistical shape\nmodel of the female breast built from 110 breast scans acquired in a standing\nposition, and the first publicly available. Together with the model, a fully\nautomated, pairwise surface registration pipeline used to establish dense\ncorrespondence among 3D breast scans is introduced. Our method is\ncomputationally efficient and requires only four landmarks to guide the\nregistration process. A major challenge when modeling female breasts from\nsurface-only 3D breast scans is the non-separability of breast and thorax. In\norder to weaken the strong coupling between breast and surrounding areas, we\npropose to minimize the variance outside the breast region as much as possible.\nTo achieve this goal, a novel concept called breast probability masks (BPMs) is\nintroduced. A BPM assigns probabilities to each point of a 3D breast scan,\ntelling how likely it is that a particular point belongs to the breast area.\nDuring registration, we use BPMs to align the template to the target as\naccurately as possible inside the breast region and only roughly outside. This\nsimple yet effective strategy significantly reduces the unwanted variance\noutside the breast region, leading to better statistical shape models in which\nbreast shapes are quite well decoupled from the thorax. The RBSM is thus able\nto produce a variety of different breast shapes as independently as possible\nfrom the shape of the thorax. Our systematic experimental evaluation reveals a\ngeneralization ability of 0.17 mm and a specificity of 2.8 mm. To underline the\nexpressiveness of the proposed model, we finally demonstrate in two showcase\napplications how the RBSM can be used for surgical outcome simulation and the\nprediction of a missing breast from the remaining one. Our model is available\nat https://www.rbsm.re-mic.de/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Weiherer_M/0/1/0/all/0/1\">Maximilian Weiherer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eigenberger_A/0/1/0/all/0/1\">Andreas Eigenberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Egger_B/0/1/0/all/0/1\">Bernhard Egger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brebant_V/0/1/0/all/0/1\">Vanessa Br&#xe9;bant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prantl_L/0/1/0/all/0/1\">Lukas Prantl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palm_C/0/1/0/all/0/1\">Christoph Palm</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RealisticHands: A Hybrid Model for 3D Hand Reconstruction. (arXiv:2108.13995v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13995","description":"<p>Estimating 3D hand meshes from RGB images robustly is a highly desirable\ntask, made challenging due to the numerous degrees of freedom, and issues such\nas self similarity and occlusions. Previous methods generally either use\nparametric 3D hand models or follow a model-free approach. While the former can\nbe considered more robust, e.g. to occlusions, they are less expressive. We\npropose a hybrid approach, utilizing a deep neural network and differential\nrendering based optimization to demonstrably achieve the best of both worlds.\nIn addition, we explore Virtual Reality (VR) as an application. Most VR\nheadsets are nowadays equipped with multiple cameras, which we can leverage by\nextending our method to the egocentric stereo domain. This extension proves to\nbe more resilient to the above mentioned issues. Finally, as a use-case, we\nshow that the improved image-model alignment can be used to acquire the user's\nhand texture, which leads to a more realistic virtual hand representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seeber_M/0/1/0/all/0/1\">Michael Seeber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poranne_R/0/1/0/all/0/1\">Roi Poranne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polleyfeys_M/0/1/0/all/0/1\">Marc Polleyfeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1\">Martin R. Oswald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reiterative Domain Aware Multi-Target Adaptation. (arXiv:2109.00919v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.00919","description":"<p>Most domain adaptation methods focus on single-source-single-target\nadaptation settings. Multi-target domain adaptation is a powerful extension in\nwhich a single classifier is learned for multiple unlabeled target domains. To\nbuild a multi-target classifier, it is important to have: a feature extractor\nthat generalizes well across domains; and effective aggregation of features\nfrom the labeled source and different unlabeled target domains. Towards the\nfirst, we use the recently popular Transformer as a feature extraction\nbackbone. Towards the second, we use a co-teaching-based approach using a\ndual-classifier head, one of which is based on the graph neural network. The\nproposed approach uses a sequential adaptation strategy that adapts one domain\nat a time starting from the target domains that are more similar to the source,\nassuming that the network finds it easier to adapt to such target domains.\nAfter adapting on each target, samples with a softmax-based confidence score\ngreater than a threshold are added to the pseudo-source, thus aggregating\nknowledge from different domains. However, softmax is not entirely trustworthy\nas a confidence score and may generate a high score for unreliable samples if\ntrained for many iterations. To mitigate this effect, we adopt a reiterative\napproach, where we reduce target adaptation iterations, however, reiterate\nmultiple times over the target domains. The experimental evaluation on the\nOffice-Home, Office-31 and DomainNet datasets shows significant improvement\nover the existing methods. We have achieved 10.7$\\%$ average improvement in\nOffice-Home dataset over the state-of-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sudipan Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_N/0/1/0/all/0/1\">Nasrullah Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiao Xiang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuation of Famous Art with AI: A Conditional Adversarial Network Inpainting Approach. (arXiv:2110.09170v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.09170","description":"<p>Much of the state-of-the-art in image synthesis inspired by real artwork are\neither entirely generative by filtered random noise or inspired by the transfer\nof style. This work explores the application of image inpainting to continue\nfamous artworks and produce generative art with a Conditional GAN. During the\ntraining stage of the process, the borders of images are cropped, leaving only\nthe centre. An inpainting GAN is then tasked with learning to reconstruct the\noriginal image from the centre crop by way of minimising both adversarial and\nabsolute difference losses, which are analysed by both their Fr\\'echet\nInception Distances and manual observations which are presented. Once the\nnetwork is trained, images are then resized rather than cropped and presented\nas input to the generator. Following the learning process, the generator then\ncreates new images by continuing from the edges of the original piece. Three\nexperiments are performed with datasets of 4766 landscape paintings\n(impressionism and romanticism), 1167 Ukiyo-e works from the Japanese Edo\nperiod, and 4968 abstract artworks. Results show that geometry and texture\n(including canvas and paint) as well as scenery such as sky, clouds, water,\nland (including hills and mountains), grass, and flowers are implemented by the\ngenerator when extending real artworks. In the Ukiyo-e experiments, it was\nobserved that features such as written text were generated even in cases where\nthe original image did not have any, due to the presence of an unpainted border\nwithin the input image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1\">Jordan J. Bird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Wide Neural Networks Forget Less Catastrophically. (arXiv:2110.11526v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.11526","description":"<p>A primary focus area in continual learning research is alleviating the\n\"catastrophic forgetting\" problem in neural networks by designing new\nalgorithms that are more robust to the distribution shifts. While the recent\nprogress in continual learning literature is encouraging, our understanding of\nwhat properties of neural networks contribute to catastrophic forgetting is\nstill limited. To address this, instead of focusing on continual learning\nalgorithms, in this work, we focus on the model itself and study the impact of\n\"width\" of the neural network architecture on catastrophic forgetting, and show\nthat width has a surprisingly significant effect on forgetting. To explain this\neffect, we study the learning dynamics of the network from various perspectives\nsuch as gradient orthogonality, sparsity, and lazy training regime. We provide\npotential explanations that are consistent with the empirical results across\ndifferent architectures and continual learning benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzadeh_S/0/1/0/all/0/1\">Seyed Iman Mirzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhry_A/0/1/0/all/0/1\">Arslan Chaudhry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Dong Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Huiyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorur_D/0/1/0/all/0/1\">Dilan Gorur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farajtabar_M/0/1/0/all/0/1\">Mehrdad Farajtabar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Online Mutual Adaptation of Deep Depth Prediction and Visual SLAM. (arXiv:2111.04096v3 [cs.RO] UPDATED)","link":"http://arxiv.org/abs/2111.04096","description":"<p>The ability of accurate depth prediction by a convolutional neural network\n(CNN) is a major challenge for its wide use in practical visual simultaneous\nlocalization and mapping (SLAM) applications, such as enhanced camera tracking\nand dense mapping. This paper is set out to answer the following question: Can\nwe tune a depth prediction CNN with the help of a visual SLAM algorithm even if\nthe CNN is not trained for the current operating environment in order to\nbenefit the SLAM performance? To this end, we propose a novel online adaptation\nframework consisting of two complementary processes: a SLAM algorithm that is\nused to generate keyframes to fine-tune the depth prediction and another\nalgorithm that uses the online adapted depth to improve map quality. Once the\npotential noisy map points are removed, we perform global photometric bundle\nadjustment (BA) to improve the overall SLAM performance. Experimental results\non both benchmark datasets and a real robot in our own experimental\nenvironments show that our proposed method improves the overall SLAM accuracy.\nWhile regularization has been shown to be effective in multi-task\nclassification problems, we present experimental results and an ablation study\nto show the effectiveness of regularization in preventing catastrophic\nforgetting in the online adaptation of depth prediction, a single-task\nregression problem. In addition, we compare our online adaptation framework\nagainst the state-of-the-art pre-trained depth prediction CNNs to show that our\nonline adapted depth prediction CNN outperforms the depth prediction CNNs that\nhave been trained on a large collection of datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Loo_S/0/1/0/all/0/1\">Shing Yan Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakeri_M/0/1/0/all/0/1\">Moein Shakeri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sai Hong Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashohor_S/0/1/0/all/0/1\">Syamsiah Mashohor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hong Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CLAWS: Contrastive Learning with hard Attention and Weak Supervision. (arXiv:2112.00847v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.00847","description":"<p>Learning effective visual representations without human supervision is a\nlong-standing problem in computer vision. Recent advances in self-supervised\nlearning algorithms have utilized contrastive learning, with methods such as\nSimCLR, which applies a composition of augmentations to an image, and minimizes\na contrastive loss between the two augmented images. In this paper, we present\nCLAWS, an annotation-efficient learning framework, addressing the problem of\nmanually labeling large-scale agricultural datasets along with potential\napplications such as anomaly detection and plant growth analytics. CLAWS uses a\nnetwork backbone inspired by SimCLR and weak supervision to investigate the\neffect of contrastive learning within class clusters. In addition, we inject a\nhard attention mask to the cropped input image before maximizing agreement\nbetween the image pairs using a contrastive loss function. This mask forces the\nnetwork to focus on pertinent object features and ignore background features.\nWe compare results between a supervised SimCLR and CLAWS using an agricultural\ndataset with 227,060 samples consisting of 11 different crop classes. Our\nexperiments and extensive evaluations show that CLAWS achieves a competitive\nNMI score of 0.7325. Furthermore, CLAWS engenders the creation of low\ndimensional representations of very large datasets with minimal parameter\ntuning and forming well-defined clusters, which lends themselves to using\nefficient, transparent, and highly interpretable clustering methods such as\nGaussian Mixture Models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Herrera_Gerena_J/0/1/0/all/0/1\">Jansel Herrera-Gerena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundareswaran_R/0/1/0/all/0/1\">Ramakrishnan Sundareswaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Just_J/0/1/0/all/0/1\">John Just</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darr_M/0/1/0/all/0/1\">Matthew Darr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1\">Ali Jannesari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastSurferVINN: Building Resolution-Independence into Deep Learning Segmentation Methods -- A Solution for HighRes Brain MRI. (arXiv:2112.09654v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.09654","description":"<p>Leading neuroimaging studies have pushed 3T MRI acquisition resolutions below\n1.0 mm for improved structure definition and morphometry. Yet, only few,\ntime-intensive automated image analysis pipelines have been validated for\nhigh-resolution (HiRes) settings. Efficient deep learning approaches, on the\nother hand, rarely support more than one fixed resolution (usually 1.0 mm).\nFurthermore, the lack of a standard submillimeter resolution as well as limited\navailability of diverse HiRes data with sufficient coverage of scanner, age,\ndiseases, or genetic variance poses additional, unsolved challenges for\ntraining HiRes networks. Incorporating resolution-independence into deep\nlearning-based segmentation, i.e., the ability to segment images at their\nnative resolution across a range of different voxel sizes, promises to overcome\nthese challenges, yet no such approach currently exists. We now fill this gap\nby introducing a Voxelsize Independent Neural Network (VINN) for\nresolution-independent segmentation tasks and present FastSurferVINN, which (i)\nestablishes and implements resolution-independence for deep learning as the\nfirst method simultaneously supporting 0.7-1.0 mm whole brain segmentation,\n(ii) significantly outperforms state-of-the-art methods across resolutions, and\n(iii) mitigates the data imbalance problem present in HiRes datasets. Overall,\ninternal resolution-independence mutually benefits both HiRes and 1.0 mm MRI\nsegmentation. With our rigorously validated FastSurferVINN we distribute a\nrapid tool for morphometric neuroimage analysis. The VINN architecture,\nfurthermore, represents an efficient resolution-independent segmentation method\nfor wider application\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Henschel_L/0/1/0/all/0/1\">Leonie Henschel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kugler_D/0/1/0/all/0/1\">David K&#xfc;gler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reuter_M/0/1/0/all/0/1\">Martin Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSegFormer: Interactive Image Segmentation with Transformers. (arXiv:2112.11325v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.11325","description":"<p>We propose iSegFormer, a novel transformer-based approach for interactive\nimage segmentation. iSegFormer is built upon existing segmentation transformers\nwith user clicks as an additional input, allowing users to interactively and\niteratively refine the segmentation mask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When less is more: Simplifying inputs aids neural network understanding. (arXiv:2201.05610v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.05610","description":"<p>How do neural network image classifiers respond to simpler and simpler\ninputs? And what do such responses reveal about the learning process? To answer\nthese questions, we need a clear measure of input simplicity (or inversely,\ncomplexity), an optimization objective that correlates with simplification, and\na framework to incorporate such objective into training and inference. Lastly\nwe need a variety of testbeds to experiment and evaluate the impact of such\nsimplification on learning. In this work, we measure simplicity with the\nencoding bit size given by a pretrained generative model, and minimize the bit\nsize to simplify inputs in training and inference. We investigate the effect of\nsuch simplification in several scenarios: conventional training, dataset\ncondensation and post-hoc explanations. In all settings, inputs are simplified\nalong with the original classification task, and we investigate the trade-off\nbetween input simplicity and task performance. For images with injected\ndistractors, such simplification naturally removes superfluous information. For\ndataset condensation, we find that inputs can be simplified with almost no\naccuracy degradation. When used in post-hoc explanation, our learning-based\nsimplification approach offers a valuable new tool to explore the basis of\nnetwork decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schirrmeister_R/0/1/0/all/0/1\">Robin Tibor Schirrmeister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rosanne Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooker_S/0/1/0/all/0/1\">Sara Hooker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ball_T/0/1/0/all/0/1\">Tonio Ball</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The KFIoU Loss for Rotated Object Detection. (arXiv:2201.12558v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12558","description":"<p>Differing from the well-developed horizontal object detection area whereby\nthe computing-friendly IoU based loss is readily adopted and well fits with the\ndetection metrics. In contrast, rotation detectors often involve a more\ncomplicated loss based on SkewIoU which is unfriendly to gradient-based\ntraining. In this paper, we argue that one effective alternative is to devise\nan approximate loss who can achieve trend-level alignment with SkewIoU loss\ninstead of the strict value-level identity. Specifically, we model the objects\nas Gaussian distribution and adopt Kalman filter to inherently mimic the\nmechanism of SkewIoU by its definition, and show its alignment with the SkewIoU\nat trend-level. This is in contrast to recent Gaussian modeling based rotation\ndetectors e.g. GWD, KLD that involves a human-specified distribution distance\nmetric which requires additional hyperparameter tuning. The resulting new loss\ncalled KFIoU is easier to implement and works better compared with exact\nSkewIoU, thanks to its full differentiability and ability to handle the\nnon-overlapping cases. We further extend our technique to the 3-D case which\nalso suffers from the same issues as 2-D detection. Extensive results on\nvarious public datasets (2-D/3-D, aerial/text/face images) with different base\ndetectors show the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yue Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gefan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jirui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wentao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self Semi Supervised Neural Architecture Search for Semantic Segmentation. (arXiv:2201.12646v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.12646","description":"<p>In this paper, we propose a Neural Architecture Search strategy based on self\nsupervision and semi-supervised learning for the task of semantic segmentation.\nOur approach builds an optimized neural network (NN) model for this task by\njointly solving a jigsaw pretext task discovered with self-supervised learning\nover unlabeled training data, and, exploiting the structure of the unlabeled\ndata with semi-supervised learning. The search of the architecture of the NN\nmodel is performed by dynamic routing using a gradient descent algorithm.\nExperiments on the Cityscapes and PASCAL VOC 2012 datasets demonstrate that the\ndiscovered neural network is more efficient than a state-of-the-art\nhand-crafted NN model with four times less floating operations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pauletto_L/0/1/0/all/0/1\">Lo&#xef;c Pauletto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1\">Massih-Reza Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winckler_N/0/1/0/all/0/1\">Nicolas Winckler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-02-01T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}