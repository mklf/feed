{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-12-22T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"DB-BERT: a Database Tuning Tool that \"Reads the Manual\". (arXiv:2112.10925v1 [cs.DB])","link":"http://arxiv.org/abs/2112.10925","description":"<p>DB-BERT is a database tuning tool that exploits information gained via\nnatural language analysis of manuals and other relevant text documents. It uses\ntext to identify database system parameters to tune as well as recommended\nparameter values. DB-BERT applies large, pre-trained language models\n(specifically, the BERT model) for text analysis. During an initial training\nphase, it fine-tunes model weights in order to translate natural language hints\ninto recommended settings. At run time, DB-BERT learns to aggregate, adapt, and\nprioritize hints to achieve optimal performance for a specific database system\nand benchmark. Both phases are iterative and use reinforcement learning to\nguide the selection of tuning settings to evaluate (penalizing settings that\nthe database system rejects while rewarding settings that improve performance).\nIn our experiments, we leverage hundreds of text documents about database\ntuning as input for DB-BERT. We compare DB-BERT against various baselines,\nconsidering different benchmarks (TPC-C and TPC-H), metrics (throughput and run\ntime), as well as database systems (Postgres and MySQL). In all cases, DB-BERT\nfinds the best parameter settings among all compared methods. The code of\nDB-BERT is available online at https://itrummer.github.io/dbbert/.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Trummer_I/0/1/0/all/0/1\">Immanuel Trummer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion. (arXiv:2112.10936v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10936","description":"<p>In today's era of digital misinformation, we are increasingly faced with new\nthreats posed by video falsification techniques. Such falsifications range from\ncheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,\nsophisticated AI media synthesis methods), which are becoming perceptually\nindistinguishable from real videos. To tackle this challenge, we propose a\nmulti-modal semantic forensic approach to discover clues that go beyond\ndetecting discrepancies in visual quality, thereby handling both simpler\ncheapfakes and visually persuasive deepfakes. In this work, our goal is to\nverify that the purported person seen in the video is indeed themselves by\ndetecting anomalous correspondences between their facial movements and the\nwords they are saying. We leverage the idea of attribution to learn\nperson-specific biometric patterns that distinguish a given speaker from\nothers. We use interpretable Action Units (AUs) to capture a persons' face and\nhead movement as opposed to deep CNN visual features, and we are the first to\nuse word-conditioned facial motion analysis. Unlike existing person-specific\napproaches, our method is also effective against attacks that focus on lip\nmanipulation. We further demonstrate our method's effectiveness on a range of\nfakes not seen in training including those without video manipulation, that\nwere not addressed in prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shruti Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Evonne Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularizing End-to-End Speech Translation with Triangular Decomposition Agreement. (arXiv:2112.10991v1 [cs.CL])","link":"http://arxiv.org/abs/2112.10991","description":"<p>End-to-end speech-to-text translation~(E2E-ST) is becoming increasingly\npopular due to the potential of its less error propagation, lower latency, and\nfewer parameters. Given the triplet training corpus $\\langle speech,\ntranscription, translation\\rangle$, the conventional high-quality E2E-ST system\nleverages the $\\langle speech, transcription\\rangle$ pair to pre-train the\nmodel and then utilizes the $\\langle speech, translation\\rangle$ pair to\noptimize it further. However, this process only involves two-tuple data at each\nstage, and this loose coupling fails to fully exploit the association between\ntriplet data. In this paper, we attempt to model the joint probability of\ntranscription and translation based on the speech input to directly leverage\nsuch triplet data. Based on that, we propose a novel regularization method for\nmodel training to improve the agreement of dual-path decomposition within\ntriplet data, which should be equal in theory. To achieve this goal, we\nintroduce two Kullback-Leibler divergence regularization terms into the model\ntraining objective to reduce the mismatch between output probabilities of\ndual-path. Then the well-trained model can be naturally transformed as the\nE2E-ST models by the pre-defined early stop tag. Experiments on the MuST-C\nbenchmark demonstrate that our proposed approach significantly outperforms\nstate-of-the-art E2E-ST baselines on all 8 language pairs, while achieving\nbetter performance in the automatic speech recognition task. Our code is\nopen-sourced at https://github.com/duyichao/E2E-ST-TDA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tong Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Cross-Lingual Retrieval with Multilingual Text Encoders. (arXiv:2112.11031v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11031","description":"<p>In this work we present a systematic empirical study focused on the\nsuitability of the state-of-the-art multilingual encoders for cross-lingual\ndocument and sentence retrieval tasks across a number of diverse language\npairs. We first treat these models as multilingual text encoders and benchmark\ntheir performance in unsupervised ad-hoc sentence- and document-level CLIR. In\ncontrast to supervised language understanding, our results indicate that for\nunsupervised document-level CLIR -- a setup with no relevance judgments for\nIR-specific fine-tuning -- pretrained multilingual encoders on average fail to\nsignificantly outperform earlier models based on CLWEs. For sentence-level\nretrieval, we do obtain state-of-the-art performance: the peak scores, however,\nare met by multilingual encoders that have been further specialized, in a\nsupervised fashion, for sentence understanding tasks, rather than using their\nvanilla 'off-the-shelf' variants. Following these results, we introduce\nlocalized relevance matching for document-level CLIR, where we independently\nscore a query against document sections. In the second part, we evaluate\nmultilingual encoders fine-tuned in a supervised fashion (i.e., we learn to\nrank) on English relevance data in a series of zero-shot language and domain\ntransfer CLIR experiments. Our results show that supervised re-ranking rarely\nimproves the performance of multilingual transformers as unsupervised base\nrankers. Finally, only with in-domain contrastive fine-tuning (i.e., same\ndomain, only language transfer), we manage to improve the ranking quality. We\nuncover substantial empirical differences between cross-lingual retrieval\nresults and results of (zero-shot) cross-lingual transfer for monolingual\nretrieval in target languages, which point to \"monolingual overfitting\" of\nretrieval models trained on monolingual data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Litschko_R/0/1/0/all/0/1\">Robert Litschko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponzetto_S/0/1/0/all/0/1\">Simone Paolo Ponzetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting Job Titles from Job Descriptions with Multi-label Text Classification. (arXiv:2112.11052v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11052","description":"<p>Finding a suitable job and hunting for eligible candidates are important to\njob seeking and human resource agencies. With the vast information about job\ndescriptions, employees and employers need assistance to automatically detect\njob titles based on job description texts. In this paper, we propose the\nmulti-label classification approach for predicting relevant job titles from job\ndescription texts, and implement the Bi-GRU-LSTM-CNN with different pre-trained\nlanguage models to apply for the job titles prediction problem. The BERT with\nmultilingual pre-trained model obtains the highest result by F1-scores on both\ndevelopment and test sets, which are 62.20% on the development set, and 47.44%\non the test set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Trung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1\">Hanh Hong Phuc Vo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Inference Approach To Question Answering Over Knowledge Graphs. (arXiv:2112.11070v1 [cs.LG])","link":"http://arxiv.org/abs/2112.11070","description":"<p>Knowledge Graphs (KG) act as a great tool for holding distilled information\nfrom large natural language text corpora. The problem of natural language\nquerying over knowledge graphs is essential for the human consumption of this\ninformation. This problem is typically addressed by converting the natural\nlanguage query to a structured query and then firing the structured query on\nthe KG. Direct answering models over knowledge graphs in literature are very\nfew. The query conversion models and direct models both require specific\ntraining data pertaining to the domain of the knowledge graph. In this work, we\nconvert the problem of natural language querying over knowledge graphs to an\ninference problem over premise-hypothesis pairs. Using trained deep learning\nmodels for the converted proxy inferencing problem, we provide the solution for\nthe original natural language querying problem. Our method achieves over 90%\naccuracy on MetaQA dataset, beating the existing state-of-the-art. We also\npropose a model for inferencing called Hierarchical Recurrent Path\nEncoder(HRPE). The inferencing models can be fine-tuned to be used across\ndomains with less training data. Our approach does not require large\ndomain-specific training data for querying on new knowledge graphs from\ndifferent domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Aayushee Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Annervaz_K/0/1/0/all/0/1\">K.M. Annervaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dukkipati_A/0/1/0/all/0/1\">Ambedkar Dukkipati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1\">Shubhashis Sengupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-oriented Dialogue Systems: performance vs. quality-optima, a review. (arXiv:2112.11176v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11176","description":"<p>Task-oriented dialogue systems (TODS) are continuing to rise in popularity as\nvarious industries find ways to effectively harness their capabilities, saving\nboth time and money. However, even state-of-the-art TODS are not yet reaching\ntheir full potential. TODS typically have a primary design focus on completing\nthe task at hand, so the metric of task-resolution should take priority. Other\nconversational quality attributes that may point to the success, or otherwise,\nof the dialogue, may be ignored. This can cause interactions between human and\ndialogue system that leave the user dissatisfied or frustrated. This paper\nexplores the literature on evaluative frameworks of dialogue systems and the\nrole of conversational quality attributes in dialogue systems, looking at if,\nhow, and where they are utilised, and examining their correlation with the\nperformance of the dialogue system.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fellows_R/0/1/0/all/0/1\">Ryan Fellows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ihshaish_H/0/1/0/all/0/1\">Hisham Ihshaish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Battle_S/0/1/0/all/0/1\">Steve Battle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haines_C/0/1/0/all/0/1\">Ciaran Haines</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mayhew_P/0/1/0/all/0/1\">Peter Mayhew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deza_J/0/1/0/all/0/1\">J. Ignacio Deza</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fake News Detection Tools and Methods -- A Review. (arXiv:2112.11185v1 [cs.CY])","link":"http://arxiv.org/abs/2112.11185","description":"<p>In the past decade, the social networks platforms and micro-blogging sites\nsuch as Facebook, Twitter, Instagram, and Weibo have become an integral part of\nour day-to-day activities and is widely used all over the world by billions of\nusers to share their views and circulate information in the form of messages,\npictures, and videos. These are even used by government agencies to spread\nimportant information through their verified Facebook accounts and official\nTwitter handles, as they can reach a huge population within a limited time\nwindow. However, many deceptive activities like propaganda and rumor can\nmislead users on a daily basis. In these COVID times, fake news and rumors are\nvery prevalent and are shared in a huge number which has created chaos in this\ntough time. And hence, the need for Fake News Detection in the present scenario\nis inevitable. In this paper, we survey the recent literature about different\napproaches to detect fake news over the Internet. In particular, we firstly\ndiscuss fake news and the various terms related to it that have been considered\nin the literature. Secondly, we highlight the various publicly available\ndatasets and various online tools that are available and can debunk Fake News\nin real-time. Thirdly, we describe fake news detection methods based on two\nbroader areas i.e., its content and the social context. Finally, we provide a\ncomparison of various techniques that are used to debunk fake news.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hangloo_S/0/1/0/all/0/1\">Sakshini Hangloo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_B/0/1/0/all/0/1\">Bhavna Arora</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrast and Generation Make BART a Good Dialogue Emotion Recognizer. (arXiv:2112.11202v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11202","description":"<p>In dialogue systems, utterances with similar semantics may have distinctive\nemotions under different contexts. Therefore, modeling long-range contextual\nemotional relationships with speaker dependency plays a crucial part in\ndialogue emotion recognition. Meanwhile, distinguishing the different emotion\ncategories is non-trivial since they usually have semantically similar\nsentiments. To this end, we adopt supervised contrastive learning to make\ndifferent emotions mutually exclusive to identify similar emotions better.\nMeanwhile, we utilize an auxiliary response generation task to enhance the\nmodel's ability of handling context information, thereby forcing the model to\nrecognize emotions with similar semantics in diverse contexts. To achieve these\nobjectives, we use the pre-trained encoder-decoder model BART as our backbone\nmodel since it is very suitable for both understanding and generation tasks.\nThe experiments on four datasets demonstrate that our proposed model obtains\nsignificantly more favorable results than the state-of-the-art model in\ndialogue emotion recognition. The ablation study further demonstrates the\neffectiveness of supervised contrastive loss and generative loss.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shimin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How are cities pledging net zero? A computational approach to analyzing subnational climate strategies. (arXiv:2112.11207v1 [cs.CY])","link":"http://arxiv.org/abs/2112.11207","description":"<p>Cities have become primary actors on climate change and are increasingly\nsetting goals aimed at net-zero emissions. The rapid proliferation of\nsubnational governments \"racing to zero\" emissions and articulating their own\nclimate mitigation plans warrants closer examination to understand how these\nactors intend to meet these goals. The scattered, incomplete and heterogeneous\nnature of city climate policy documents, however, has made their systemic\nanalysis challenging. We analyze 318 climate action documents from cities that\nhave pledged net-zero targets or joined a transnational climate initiative with\nthis goal using machine learning-based natural language processing (NLP)\ntechniques. We use these approaches to accomplish two primary goals: 1)\ndetermine text patterns that predict \"ambitious\" net-zero targets, where we\ndefine an ambitious target as one that encompasses a subnational government's\neconomy-wide emissions; and 2) perform a sectoral analysis to identify patterns\nand trade-offs in climate action themes (i.e., land-use, industry, buildings,\netc.). We find that cities that have defined ambitious climate actions tend to\nemphasize quantitative metrics and specific high-emitting sectors in their\nplans, supported by mentions of governance and citizen participation. Cities\npredominantly emphasize energy-related actions in their plans, particularly in\nthe buildings, transport and heating sectors, but often at the expense of other\nsectors, including land-use and climate impacts. The method presented in this\npaper provides a replicable, scalable approach to analyzing climate action\nplans and a first step towards facilitating cross-city learning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_S/0/1/0/all/0/1\">Siddharth Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_A/0/1/0/all/0/1\">Angel Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+French_I/0/1/0/all/0/1\">Ian French</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Elwin Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An ASP-based Approach to Answering Natural Language Questions for Texts. (arXiv:2112.11241v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11241","description":"<p>An approach based on answer set programming (ASP) is proposed in this paper\nfor representing knowledge generated from natural language texts. Knowledge in\na text is modeled using a Neo Davidsonian-like formalism, which is then\nrepresented as an answer set program. Relevant commonsense knowledge is\nadditionally imported from resources such as WordNet and represented in ASP.\nThe resulting knowledge-base can then be used to perform reasoning with the\nhelp of an ASP system. This approach can facilitate many natural language tasks\nsuch as automated question answering, text summarization, and automated\nquestion generation. ASP-based representation of techniques such as default\nreasoning, hierarchical knowledge organization, preferences over defaults,\netc., are used to model commonsense reasoning methods required to accomplish\nthese tasks. In this paper, we describe the CASPR system that we have developed\nto automate the task of answering natural language questions given English\ntext. CASPR can be regarded as a system that answers questions by\n\"understanding\" the text and has been tested on the SQuAD data set, with\npromising results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pendharkar_D/0/1/0/all/0/1\">Dhruva Pendharkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basu_K/0/1/0/all/0/1\">Kinjal Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakerin_F/0/1/0/all/0/1\">Farhad Shakerin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1\">Gopal Gupta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Graph Contrastive Pretraining for Text Classification. (arXiv:2112.11389v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11389","description":"<p>Contrastive pretraining techniques for text classification has been largely\nstudied in an unsupervised setting. However, oftentimes labeled data from\nrelated tasks which share label semantics with current task is available. We\nhypothesize that using this labeled data effectively can lead to better\ngeneralization on current task. In this paper, we propose a novel way to\neffectively utilize labeled data from related tasks with a graph based\nsupervised contrastive learning approach. We formulate a token-graph by\nextrapolating the supervised information from examples to tokens. Our\nformulation results in an embedding space where tokens with high/low\nprobability of belonging to same class are near/further-away from one another.\nWe also develop detailed theoretical insights which serve as a motivation for\nour method. In our experiments with $13$ datasets, we show our method\noutperforms pretraining schemes by $2.5\\%$ and also example-level contrastive\nlearning based formulation by $1.8\\%$ on average. In addition, we show\ncross-domain effectiveness of our method in a zero-shot setting by $3.91\\%$ on\naverage. Lastly, we also demonstrate our method can be used as a noisy teacher\nin a knowledge distillation setting to significantly improve performance of\ntransformer based models in low labeled data regime by $4.57\\%$ on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Samujjwal Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1\">Subhadeep Maji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1\">Maunendra Sankar Desarkar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Voice Quality and Pitch Features in Transformer-Based Speech Recognition. (arXiv:2112.11391v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11391","description":"<p>Jitter and shimmer measurements have shown to be carriers of voice quality\nand prosodic information which enhance the performance of tasks like speaker\nrecognition, diarization or automatic speech recognition (ASR). However, such\nfeatures have been seldom used in the context of neural-based ASR, where\nspectral features often prevail. In this work, we study the effects of\nincorporating voice quality and pitch features altogether and separately to a\nTransformer-based ASR model, with the intuition that the attention mechanisms\nmight exploit latent prosodic traits. For doing so, we propose separated\nconvolutional front-ends for prosodic and spectral features, showing that this\narchitectural choice yields better results than simple concatenation of such\npitch and voice quality features to mel-spectrogram filterbanks. Furthermore,\nwe find mean Word Error Rate relative reductions of up to 5.6% with the\nLibriSpeech benchmark. Such findings motivate further research on the\napplication of prosody knowledge for increasing the robustness of\nTransformer-based ASR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cambara_G/0/1/0/all/0/1\">Guillermo C&#xe1;mbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1\">Jordi Luque</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Farrus_M/0/1/0/all/0/1\">Mireia Farr&#xfa;s</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lyric document embeddings for music tagging. (arXiv:2112.11436v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11436","description":"<p>We present an empirical study on embedding the lyrics of a song into a\nfixed-dimensional feature for the purpose of music tagging. Five methods of\ncomputing token-level and four methods of computing document-level\nrepresentations are trained on an industrial-scale dataset of tens of millions\nof songs. We compare simple averaging of pretrained embeddings to modern\nrecurrent and attention-based neural architectures. Evaluating on a wide range\nof tagging tasks such as genre classification, explicit content identification\nand era detection, we find that averaging word embeddings outperform more\ncomplex architectures in many downstream metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+McVicar_M/0/1/0/all/0/1\">Matt McVicar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giorgi_B/0/1/0/all/0/1\">Bruno Di Giorgi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dundar_B/0/1/0/all/0/1\">Baris Dundar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mauch_M/0/1/0/all/0/1\">Matthias Mauch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixed Precision Low-bit Quantization of Neural Network Language Models for Speech Recognition. (arXiv:2112.11438v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11438","description":"<p>State-of-the-art language models (LMs) represented by long-short term memory\nrecurrent neural networks (LSTM-RNNs) and Transformers are becoming\nincreasingly complex and expensive for practical applications. Low-bit neural\nnetwork quantization provides a powerful solution to dramatically reduce their\nmodel size. Current quantization methods are based on uniform precision and\nfail to account for the varying performance sensitivity at different parts of\nLMs to quantization errors. To this end, novel mixed precision neural network\nLM quantization methods are proposed in this paper. The optimal local precision\nchoices for LSTM-RNN and Transformer based neural LMs are automatically learned\nusing three techniques. The first two approaches are based on quantization\nsensitivity metrics in the form of either the KL-divergence measured between\nfull precision and quantized LMs, or Hessian trace weighted quantization\nperturbation that can be approximated efficiently using matrix free techniques.\nThe third approach is based on mixed precision neural architecture search. In\norder to overcome the difficulty in using gradient descent methods to directly\nestimate discrete quantized weights, alternating direction methods of\nmultipliers (ADMM) are used to efficiently train quantized LMs. Experiments\nwere conducted on state-of-the-art LF-MMI CNN-TDNN systems featuring speed\nperturbation, i-Vector and learning hidden unit contribution (LHUC) based\nspeaker adaptation on two tasks: Switchboard telephone speech and AMI meeting\ntranscription. The proposed mixed precision quantization techniques achieved\n\"lossless\" quantization on both tasks, by producing model size compression\nratios of up to approximately 16 times over the full precision LSTM and\nTransformer baseline LMs, while incurring no statistically significant word\nerror rate increase.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Junhao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianwei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Shoukang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xunying Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Drug-Related Information Extraction from French Clinical Documents: ReLyfe Approach. (arXiv:2112.11439v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11439","description":"<p>Structuring medical data in France remains a challenge mainly because of the\nlack of medical data due to privacy concerns and the lack of methods and\napproaches on processing the French language. One of these challenges is\nstructuring drug-related information in French clinical documents. To our\nknowledge, over the last decade, there are less than five relevant papers that\nstudy French prescriptions. This paper proposes a new approach for extracting\ndrug-related information from French clinical scanned documents while\npreserving patients' privacy. In addition, we deployed our method in a health\ndata management platform where it is used to structure drug medical data and\nhelp patients organize their drug schedules. It can be implemented on any web\nor mobile platform. This work closes the gap between theoretical and practical\nwork by creating an application adapted to real production problems. It is a\ncombination of a rule-based phase and a Deep Learning approach. Finally,\nnumerical results show the outperformance and relevance of the proposed\nmethodology.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alwan_A/0/1/0/all/0/1\">Azzam Alwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Attias_M/0/1/0/all/0/1\">Maayane Attias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubin_L/0/1/0/all/0/1\">Larry Rubin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bakri_A/0/1/0/all/0/1\">Adnan El Bakri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NLP Techniques for Water Quality Analysis in Social Media Content. (arXiv:2112.11441v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11441","description":"<p>This paper presents our contributions to the MediaEval 2021 task namely\n\"WaterMM: Water Quality in Social Multimedia\". The task aims at analyzing\nsocial media posts relevant to water quality with particular focus on the\naspects like watercolor, smell, taste, and related illnesses. To this aim, a\nmultimodal dataset containing both textual and visual information along with\nmeta-data is provided. Considering the quality and quantity of available\ncontent, we mainly focus on textual information by employing three different\nmodels individually and jointly in a late-fusion manner. These models include\n(i) Bidirectional Encoder Representations from Transformers (BERT), (ii)\nRobustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and a (iii) custom\nLong short-term memory (LSTM) model obtaining an overall F1-score of 0.794,\n0.717, 0.663 on the official test set, respectively. In the fusion scheme, all\nthe models are treated equally and no significant improvement is observed in\nthe performance over the best performing individual model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ayub_M/0/1/0/all/0/1\">Muhammad Asif Ayub</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Khubaib Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_K/0/1/0/all/0/1\">Kashif Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1\">Nasir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1\">Ala Al-Fuqaha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deliberation of Streaming RNN-Transducer by Non-autoregressive Decoding. (arXiv:2112.11442v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11442","description":"<p>We propose to deliberate the hypothesis alignment of a streaming RNN-T model\nwith the previously proposed Align-Refine non-autoregressive decoding method\nand its improved versions. The method performs a few refinement steps, where\neach step shares a transformer decoder that attends to both text features\n(extracted from alignments) and audio features, and outputs complete updated\nalignments. The transformer decoder is trained with the CTC loss which\nfacilitates parallel greedy decoding, and performs full-context attention to\ncapture label dependencies. We improve Align-Refine by introducing cascaded\nencoder that captures more audio context before refinement, and alignment\naugmentation which enforces learning label dependency. We show that,\nconditioned on hypothesis alignments of a streaming RNN-T model, our method\nobtains significantly more accurate recognition results than the first-pass\nRNN-T, with only small amount of model parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara Sainath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESAN: Efficient Sentiment Analysis Network of A-Shares Research Reports for Stock Price Prediction. (arXiv:2112.11444v1 [cs.AI])","link":"http://arxiv.org/abs/2112.11444","description":"<p>In this paper, we are going to develop a natural language processing model to\nhelp us to predict stocks in the long term. The whole network includes two\nmodules. The first module is a natural language processing model which seeks\nout reliable factors from input reports. While the other is a time-series\nforecasting model which takes the factors as input and aims to predict stocks\nearnings yield. To indicate the efficiency of our model to combine the\nsentiment analysis module and the time-series forecasting module, we name our\nmethod ESAN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wanrong Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shufan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mengxun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1\">Jiarui Ou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controversy Detection: a Text and Graph Neural Network Based Approach. (arXiv:2112.11445v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11445","description":"<p>Controversial content refers to any content that attracts both positive and\nnegative feedback. Its automatic identification, especially on social media, is\na challenging task as it should be done on a large number of continuously\nevolving posts, covering a large variety of topics. Most of the existing\napproaches rely on the graph structure of a topic-discussion and/or the content\nof messages. This paper proposes a controversy detection approach based on both\ngraph structure of a discussion and text features. Our proposed approach relies\non Graph Neural Network (gnn) to encode the graph representation (including its\ntexts) in an embedding vector before performing a graph classification task.\nThe latter will classify the post as controversial or not. Two controversy\ndetection strategies are proposed. The first one is based on a hierarchical\ngraph representation learning. Graph user nodes are embedded hierarchically and\niteratively to compute the whole graph embedding vector. The second one is\nbased on the attention mechanism, which allows each user node to give more or\nless importance to its neighbors when computing node embeddings. We conduct\nexperiments to evaluate our approach using different real-world datasets.\nConducted experiments show the positive impact of combining textual features\nand structural information in terms of performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benslimane_S/0/1/0/all/0/1\">Samy Benslimane</a> (ADVANSE, LIRMM), <a href=\"http://arxiv.org/find/cs/1/au:+Aze_J/0/1/0/all/0/1\">J&#xe9;rome Az&#xe9;</a> (ADVANSE, LIRMM), <a href=\"http://arxiv.org/find/cs/1/au:+Bringay_S/0/1/0/all/0/1\">Sandra Bringay</a> (UPVM, ADVANSE, LIRMM), <a href=\"http://arxiv.org/find/cs/1/au:+Servajean_M/0/1/0/all/0/1\">Maximilien Servajean</a> (LIRMM, ADVANSE, UPVM), <a href=\"http://arxiv.org/find/cs/1/au:+Mollevi_C/0/1/0/all/0/1\">Caroline Mollevi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher. (arXiv:2112.11446v1 [cs.CL])","link":"http://arxiv.org/abs/2112.11446","description":"<p>Language modelling provides a step towards intelligent communication systems\nby harnessing large repositories of written human knowledge to better predict\nand understand the world. In this paper, we present an analysis of\nTransformer-based language model performance across a wide range of model\nscales -- from models with tens of millions of parameters up to a 280 billion\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\nachieving state-of-the-art performance across the majority. Gains from scale\nare largest in areas such as reading comprehension, fact-checking, and the\nidentification of toxic language, but logical and mathematical reasoning see\nless benefit. We provide a holistic analysis of the training dataset and\nmodel's behaviour, covering the intersection of model scale with bias and\ntoxicity. Finally we discuss the application of language models to AI safety\nand the mitigation of downstream harms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rae_J/0/1/0/all/0/1\">Jack W. Rae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borgeaud_S/0/1/0/all/0/1\">Sebastian Borgeaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1\">Trevor Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Millican_K/0/1/0/all/0/1\">Katie Millican</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jordan Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1\">Francis Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aslanides_J/0/1/0/all/0/1\">John Aslanides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_S/0/1/0/all/0/1\">Sarah Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ring_R/0/1/0/all/0/1\">Roman Ring</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Young_S/0/1/0/all/0/1\">Susannah Young</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rutherford_E/0/1/0/all/0/1\">Eliza Rutherford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennigan_T/0/1/0/all/0/1\">Tom Hennigan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cassirer_A/0/1/0/all/0/1\">Albin Cassirer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Powell_R/0/1/0/all/0/1\">Richard Powell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Driessche_G/0/1/0/all/0/1\">George van den Driessche</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendricks_L/0/1/0/all/0/1\">Lisa Anne Hendricks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rauh_M/0/1/0/all/0/1\">Maribeth Rauh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1\">Po-Sen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glaese_A/0/1/0/all/0/1\">Amelia Glaese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welbl_J/0/1/0/all/0/1\">Johannes Welbl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dathathri_S/0/1/0/all/0/1\">Sumanth Dathathri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Saffron Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uesato_J/0/1/0/all/0/1\">Jonathan Uesato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mellor_J/0/1/0/all/0/1\">John Mellor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higgins_I/0/1/0/all/0/1\">Irina Higgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Creswell_A/0/1/0/all/0/1\">Antonia Creswell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAleese_N/0/1/0/all/0/1\">Nat McAleese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Amy Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elsen_E/0/1/0/all/0/1\">Erich Elsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayakumar_S/0/1/0/all/0/1\">Siddhant Jayakumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buchatskaya_E/0/1/0/all/0/1\">Elena Buchatskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Budden_D/0/1/0/all/0/1\">David Budden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutherland_E/0/1/0/all/0/1\">Esme Sutherland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simonyan_K/0/1/0/all/0/1\">Karen Simonyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1\">Michela Paganini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sifre_L/0/1/0/all/0/1\">Laurent Sifre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martens_L/0/1/0/all/0/1\">Lena Martens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuncoro_A/0/1/0/all/0/1\">Adhiguna Kuncoro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nematzadeh_A/0/1/0/all/0/1\">Aida Nematzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gribovskaya_E/0/1/0/all/0/1\">Elena Gribovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donato_D/0/1/0/all/0/1\">Domenic Donato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazaridou_A/0/1/0/all/0/1\">Angeliki Lazaridou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mensch_A/0/1/0/all/0/1\">Arthur Mensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lespiau_J/0/1/0/all/0/1\">Jean-Baptiste Lespiau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grigorev_N/0/1/0/all/0/1\">Nikolai Grigorev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritz_D/0/1/0/all/0/1\">Doug Fritz</a>, et al. (31 additional authors not shown)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexicon-constrained Copying Network for Chinese Abstractive Summarization. (arXiv:2010.08197v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.08197","description":"<p>Copy mechanism allows sequence-to-sequence models to choose words from the\ninput and put them directly into the output, which is finding increasing use in\nabstractive summarization. However, since there is no explicit delimiter in\nChinese sentences, most existing models for Chinese abstractive summarization\ncan only perform character copy, resulting in inefficient. To solve this\nproblem, we propose a lexicon-constrained copying network that models\nmulti-granularity in both encoder and decoder. On the source side, words and\ncharacters are aggregated into the same input memory using a Transformerbased\nencoder. On the target side, the decoder can copy either a character or a\nmulti-character word at each time step, and the decoding process is guided by a\nword-enhanced search algorithm that facilitates the parallel computation and\nencourages the model to copy more words. Moreover, we adopt a word selector to\nintegrate keyword information. Experiments results on a Chinese social media\ndataset show that our model can work standalone or with the word selector. Both\nforms can outperform previous character-based models and achieve competitive\nperformances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wan_B/0/1/0/all/0/1\">Boyan Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohail_M/0/1/0/all/0/1\">Mishal Sohail</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning Based on Natural Language Processing to Detect Cardiac Failure in Clinical Narratives. (arXiv:2104.03934v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03934","description":"<p>The purpose of the study presented herein is to develop a machine learning\nalgorithm based on natural language processing that automatically detects\nwhether a patient has a cardiac failure or a healthy condition by using\nphysician notes in Research Data Warehouse at CHU Sainte Justine Hospital.\nFirst, a word representation learning technique was employed by using\nbag-of-word (BoW), term frequency inverse document frequency (TFIDF), and\nneural word embeddings (word2vec). Each representation technique aims to retain\nthe words semantic and syntactic analysis in critical care data. It helps to\nenrich the mutual information for the word representation and leads to an\nadvantage for further appropriate analysis steps. Second, a machine learning\nclassifier was used to detect the patients condition for either cardiac failure\nor stable patient through the created word representation vector space from the\nprevious step. This machine learning approach is based on a supervised binary\nclassification algorithm, including logistic regression (LR), Gaussian\nNaive-Bayes (GaussianNB), and multilayer perceptron neural network (MLPNN).\nTechnically, it mainly optimizes the empirical loss during training the\nclassifiers. As a result, an automatic learning algorithm would be accomplished\nto draw a high classification performance, including accuracy (acc), precision\n(pre), recall (rec), and F1 score (f1). The results show that the combination\nof TFIDF and MLPNN always outperformed other combinations with all overall\nperformance. In the case without any feature selection, the proposed framework\nyielded an overall classification performance with acc, pre, rec, and f1 of 84%\nand 82%, 85%, and 83%, respectively. Significantly, if the feature selection\nwas well applied, the overall performance would finally improve up to 4% for\neach evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thanh-Dung Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noumeir_R/0/1/0/all/0/1\">Rita Noumeir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rambaud_J/0/1/0/all/0/1\">Jerome Rambaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sans_G/0/1/0/all/0/1\">Guillaume Sans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jouvet_P/0/1/0/all/0/1\">Philippe Jouvet</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards General Natural Language Understanding with Probabilistic Worldbuilding. (arXiv:2105.02486v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.02486","description":"<p>We introduce the Probabilistic Worldbuilding Model (PWM), a new\nfully-symbolic Bayesian model of semantic parsing and reasoning, as a first\nstep in a research program toward more domain- and task-general NLU and AI.\nHumans create internal mental models of their observations which greatly aid in\ntheir ability to understand and reason about a large variety of problems. In\nPWM, the meanings of sentences, acquired facts about the world, and\nintermediate steps in reasoning are all expressed in a human-readable formal\nlanguage, with the design goal of interpretability. PWM is Bayesian, designed\nspecifically to be able to generalize to new domains and new tasks. We derive\nand implement an inference algorithm that reads sentences by parsing and\nabducing updates to its latent world model that capture the semantics of those\nsentences, and evaluate it on two out-of-domain question-answering datasets:\n(1) ProofWriter and (2) a new dataset we call FictionalGeoQA, designed to be\nmore representative of real language but still simple enough to focus on\nevaluating reasoning ability, while being robust against heuristics. Our method\noutperforms baselines on both, thereby demonstrating its value as a\nproof-of-concept.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1\">Abulhair Saparov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom M. Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Cross-Lingual Stance Detection with Sentiment-Based Pre-Training. (arXiv:2109.06050v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.06050","description":"<p>The goal of stance detection is to determine the viewpoint expressed in a\npiece of text towards a target. These viewpoints or contexts are often\nexpressed in many different languages depending on the user and the platform,\nwhich can be a local news outlet, a social media platform, a news forum, etc.\nMost research in stance detection, however, has been limited to working with a\nsingle language and on a few limited targets, with little work on cross-lingual\nstance detection. Moreover, non-English sources of labelled data are often\nscarce and present additional challenges. Recently, large multilingual language\nmodels have substantially improved the performance on many non-English tasks,\nespecially such with limited numbers of examples. This highlights the\nimportance of model pre-training and its ability to learn from few examples. In\nthis paper, we present the most comprehensive study of cross-lingual stance\ndetection to date: we experiment with 15 diverse datasets in 12 languages from\n6 language families, and with 6 low-resource evaluation settings each. For our\nexperiments, we build on pattern-exploiting training, proposing the addition of\na novel label encoder to simplify the verbalisation procedure. We further\npropose sentiment-based generation of stance data for pre-training, which shows\nsizeable improvement of more than 6% F1 absolute in low-shot settings compared\nto several strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive String Representation Learning using Synthetic Data. (arXiv:2110.04217v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.04217","description":"<p>String representation Learning (SRL) is an important task in the field of\nNatural Language Processing, but it remains under-explored. The goal of SRL is\nto learn dense and low-dimensional vectors (or embeddings) for encoding\ncharacter sequences. The learned representation from this task can be used in\nmany downstream application tasks such as string similarity matching or lexical\nnormalization. In this paper, we propose a new method for to train a SRL model\nby only using synthetic data. Our approach makes use of Contrastive Learning in\norder to maximize similarity between related strings while minimizing it for\nunrelated strings. We demonstrate the effectiveness of our approach by\nevaluating the learned representation on the task of string similarity\nmatching. Codes, data and pretrained models will be made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaratiana_U/0/1/0/all/0/1\">Urchade Zaratiana</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Words the Quanta of Human Language? Extending the Domain of Quantum Cognition. (arXiv:2110.04913v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2110.04913","description":"<p>In previous research, we showed that 'texts that tell a story' exhibit a\nstatistical structure that is not Maxwell-Boltzmann but Bose-Einstein. Our\nexplanation is that this is due to the presence of 'indistinguishability' in\nhuman language as a result of the same words in different parts of the story\nbeing indistinguishable from one another. In the current article, we set out to\nprovide an explanation for this Bose-Einstein statistics. We show that it is\nthe presence of 'meaning' in 'stories' that gives rise to the lack of\nindependence characteristic of Bose-Einstein, and provides conclusive evidence\nthat 'words can be considered the quanta of human language', structurally\nsimilar to how 'photons are the quanta of light'. Using several studies on\nentanglement from our Brussels research group, we also show that it is also the\npresence of 'meaning' in texts that makes the von Neumann entropy of a total\ntext smaller relative to the entropy of the words composing it. We explain how\nthe new insights in this article fit in with the research domain called\n'quantum cognition', where quantum probability models and quantum vector spaces\nare used in human cognition, and are also relevant to the use of quantum\nstructures in information retrieval and natural language processing, and how\nthey introduce 'quantization' and 'Bose-Einstein statistics' as relevant\nquantum effects there. Inspired by the conceptuality interpretation of quantum\nmechanics, and relying on the new insights, we put forward hypotheses about the\nnature of physical reality. In doing so, we note how this new type of decrease\nin entropy, and its explanation, may be important for the development of\nquantum thermodynamics. We likewise note how it can also give rise to an\noriginal explanatory picture of the nature of physical reality on the surface\nof planet Earth, in which human culture emerges as a reinforcing continuation\nof life.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Aerts_D/0/1/0/all/0/1\">Diederik Aerts</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Beltran_L/0/1/0/all/0/1\">Lester Beltran</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Much Coffee Was Consumed During EMNLP 2019? Fermi Problems: A New Reasoning Challenge for AI. (arXiv:2110.14207v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.14207","description":"<p>Many real-world problems require the combined application of multiple\nreasoning abilities employing suitable abstractions, commonsense knowledge, and\ncreative synthesis of problem-solving strategies. To help advance AI systems\ntowards such capabilities, we propose a new reasoning challenge, namely Fermi\nProblems (FPs), which are questions whose answers can only be approximately\nestimated because their precise computation is either impractical or\nimpossible. For example, \"How much would the sea level rise if all ice in the\nworld melted?\" FPs are commonly used in quizzes and interviews to bring out and\nevaluate the creative reasoning abilities of humans. To do the same for AI\nsystems, we present two datasets: 1) A collection of 1k real-world FPs sourced\nfrom quizzes and olympiads; and 2) a bank of 10k synthetic FPs of intermediate\ncomplexity to serve as a sandbox for the harder real-world challenge. In\naddition to question answer pairs, the datasets contain detailed solutions in\nthe form of an executable program and supporting facts, helping in supervision\nand evaluation of intermediate steps. We demonstrate that even extensively\nfine-tuned large scale language models perform poorly on these datasets, on\naverage making estimates that are off by two orders of magnitude. Our\ncontribution is thus the crystallization of several unsolved AI problems into a\nsingle, new challenge that we hope will spur further advances in building\nsystems that can reason.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandrasekaran_A/0/1/0/all/0/1\">Arjun Chandrasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1\">Peter Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Selecting Parallel In-domain Sentences for Neural Machine Translation Using Monolingual Texts. (arXiv:2112.06096v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.06096","description":"<p>Continuously-growing data volumes lead to larger generic models. Specific\nuse-cases are usually left out, since generic models tend to perform poorly in\ndomain-specific cases. Our work addresses this gap with a method for selecting\nin-domain data from generic-domain (parallel text) corpora, for the task of\nmachine translation. The proposed method ranks sentences in parallel\ngeneral-domain data according to their cosine similarity with a monolingual\ndomain-specific data set. We then select the top K sentences with the highest\nsimilarity score to train a new machine translation system tuned to the\nspecific in-domain data. Our experimental results show that models trained on\nthis in-domain data outperform models trained on generic or a mixture of\ngeneric and domain data. That is, our method selects high-quality\ndomain-specific training instances at low computational cost and data size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharami_J/0/1/0/all/0/1\">Javad Pourmostafa Roshan Sharami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shterionov_D/0/1/0/all/0/1\">Dimitar Shterionov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spronck_P/0/1/0/all/0/1\">Pieter Spronck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Isometric MT: Neural Machine Translation for Automatic Dubbing. (arXiv:2112.08682v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2112.08682","description":"<p>Automatic dubbing (AD) is among the use cases where translations should fit a\ngiven length template in order to achieve synchronicity between source and\ntarget speech. For neural machine translation (MT), generating translations of\nlength close to the source length (e.g. within +-10% in character count), while\npreserving quality is a challenging task. Controlling NMT output length comes\nat a cost to translation quality which is usually mitigated with a two step\napproach of generation of n-best hypotheses and then re-ranking them based on\nlength and quality. This work, introduces a self-learning approach that allows\na transformer model to directly learn to generate outputs that closely match\nthe source length, in short isometric MT. In particular, our approach for\nisometric MT does not require to generate multiple hypotheses nor any auxiliary\nscoring function. We report results on four language pairs (English - French,\nItalian, German, Spanish) with a publicly available benchmark based on TED Talk\ndata. Both automatic and manual evaluations show that our self-learning\napproach to performs on par with more complex isometric MT approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakew_S/0/1/0/all/0/1\">Surafel M. Lakew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1\">Yogesh Virkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_P/0/1/0/all/0/1\">Prashant Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1\">Marcello Federico</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"HarmoFL: Harmonizing Local and Global Drifts in Federated Learning on Heterogeneous Medical Images. (arXiv:2112.10775v1 [eess.IV])","link":"http://arxiv.org/abs/2112.10775","description":"<p>Multiple medical institutions collaboratively training a model using\nfederated learning (FL) has become a promising solution for maximizing the\npotential of data-driven models, yet the non-independent and identically\ndistributed (non-iid) data in medical images is still an outstanding challenge\nin real-world practice. The feature heterogeneity caused by diverse scanners or\nprotocols introduces a drift in the learning process, in both local (client)\nand global (server) optimizations, which harms the convergence as well as model\nperformance. Many previous works have attempted to address the non-iid issue by\ntackling the drift locally or globally, but how to jointly solve the two\nessentially coupled drifts is still unclear. In this work, we concentrate on\nhandling both local and global drifts and introduce a new harmonizing framework\ncalled HarmoFL. First, we propose to mitigate the local update drift by\nnormalizing amplitudes of images transformed into the frequency domain to mimic\na unified imaging setting, in order to generate a harmonized feature space\nacross local clients. Second, based on harmonized features, we design a client\nweight perturbation guiding each local model to reach a flat optimum, where a\nneighborhood area of the local optimal solution has a uniformly low loss.\nWithout any extra communication cost, the perturbation assists the global model\nto optimize towards a converged optimal solution by aggregating several local\nflat optima. We have theoretically analyzed the proposed method and empirically\nconducted extensive experiments on three medical image classification and\nsegmentation tasks, showing that HarmoFL outperforms a set of recent\nstate-of-the-art methods with promising convergence behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Jiang_M/0/1/0/all/0/1\">Meirui Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lite Vision Transformer with Enhanced Self-Attention. (arXiv:2112.10809v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10809","description":"<p>Despite the impressive representation capacity of vision transformer models,\ncurrent light-weight vision transformer models still suffer from inconsistent\nand incorrect dense predictions at local regions. We suspect that the power of\ntheir self-attention mechanism is limited in shallower and thinner networks. We\npropose Lite Vision Transformer (LVT), a novel light-weight transformer network\nwith two enhanced self-attention mechanisms to improve the model performances\nfor mobile deployment. For the low-level features, we introduce Convolutional\nSelf-Attention (CSA). Unlike previous approaches of merging convolution and\nself-attention, CSA introduces local self-attention into the convolution within\na kernel of size 3x3 to enrich low-level features in the first stage of LVT.\nFor the high-level features, we propose Recursive Atrous Self-Attention (RASA),\nwhich utilizes the multi-scale context when calculating the similarity map and\na recursive mechanism to increase the representation capability with marginal\nextra parameter cost. The superiority of LVT is demonstrated on ImageNet\nrecognition, ADE20K semantic segmentation, and COCO panoptic segmentation. The\ncode is made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chenglin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yilin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">He Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zijun Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1\">Alan Yuille</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Sketch for All: One-Shot Personalized Sketch Segmentation. (arXiv:2112.10838v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10838","description":"<p>We present the first one-shot personalized sketch segmentation method. We aim\nto segment all sketches belonging to the same category provisioned with a\nsingle sketch with a given part annotation while (i) preserving the parts\nsemantics embedded in the exemplar, and (ii) being robust to input style and\nabstraction. We refer to this scenario as personalized. With that, we\nimportantly enable a much-desired personalization capability for downstream\nfine-grained sketch analysis tasks. To train a robust segmentation module, we\ndeform the exemplar sketch to each of the available sketches of the same\ncategory. Our method generalizes to sketches not observed during training. Our\ncentral contribution is a sketch-specific hierarchical deformation network.\nGiven a multi-level sketch-strokes encoding obtained via a graph convolutional\nnetwork, our method estimates rigid-body transformation from the reference to\nthe exemplar, on the upper level. Finer deformation from the exemplar to the\nglobally warped reference sketch is further obtained through stroke-wise\ndeformations, on the lower level. Both levels of deformation are guided by mean\nsquared distances between the keypoints learned without supervision, ensuring\nthat the stroke semantics are preserved. We evaluate our method against the\nstate-of-the-art segmentation and perceptual grouping baselines re-purposed for\nthe one-shot setting and against two few-shot 3D shape segmentation methods. We\nshow that our method outperforms all the alternatives by more than 10% on\naverage. Ablation studies further demonstrate that our method is robust to\npersonalization: changes in input part semantics and style differences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_A/0/1/0/all/0/1\">Anran Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1\">Yulia Gryaditskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tao Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Zhe Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Encoding Hierarchical Information in Neural Networks helps in Subpopulation Shift. (arXiv:2112.10844v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10844","description":"<p>Over the past decade, deep neural networks have proven to be adept in image\nclassification tasks, often surpassing humans in terms of accuracy. However,\nstandard neural networks often fail to understand the concept of hierarchical\nstructures and dependencies among different classes for vision related tasks.\nHumans on the other hand, seem to learn categories conceptually, progressively\ngrowing from understanding high-level concepts down to granular levels of\ncategories. One of the issues arising from the inability of neural networks to\nencode such dependencies within its learned structure is that of subpopulation\nshift -- where models are queried with novel unseen classes taken from a\nshifted population of the training set categories. Since the neural network\ntreats each class as independent from all others, it struggles to categorize\nshifting populations that are dependent at higher levels of the hierarchy. In\nthis work, we study the aforementioned problems through the lens of a novel\nconditional supervised training framework. We tackle subpopulation shift by a\nstructured learning procedure that incorporates hierarchical information\nconditionally through labels. Furthermore, we introduce a notion of graphical\ndistance to model the catastrophic effect of mispredictions. We show that\nlearning in this structured hierarchical manner results in networks that are\nmore robust against subpopulation shifts, with an improvement of around ~2% in\nterms of accuracy and around 8.5\\% in terms of graphical distance over standard\nmodels on subpopulation shift benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Amitangshu Mukherjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_I/0/1/0/all/0/1\">Isha Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translational Concept Embedding for Generalized Compositional Zero-shot Learning. (arXiv:2112.10871v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10871","description":"<p>Generalized compositional zero-shot learning means to learn composed concepts\nof attribute-object pairs in a zero-shot fashion, where a model is trained on a\nset of seen concepts and tested on a combined set of seen and unseen concepts.\nThis task is very challenging because of not only the gap between seen and\nunseen concepts but also the contextual dependency between attributes and\nobjects. This paper introduces a new approach, termed translational concept\nembedding, to solve these two difficulties in a unified framework. It models\nthe effect of applying an attribute to an object as adding a translational\nattribute feature to an object prototype. We explicitly take into account of\nthe contextual dependency between attributes and objects by generating\ntranslational attribute features conditionally dependent on the object\nprototypes. Furthermore, we design a ratio variance constraint loss to promote\nthe model's generalization ability on unseen concepts. It regularizes the\ndistances between concepts by utilizing knowledge from their pretrained word\nembeddings. We evaluate the performance of our model under both the unbiased\nand biased concept classification tasks, and show that our model is able to\nachieve good balance in predicting unseen and seen concepts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">He Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1\">Wei Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiawei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spatiotemporal Motion Synchronization for Snowboard Big Air. (arXiv:2112.10909v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10909","description":"<p>During the training for snowboard big air, one of the most popular winter\nsports, athletes and coaches extensively shoot and check their jump attempts\nusing a single camera or smartphone. However, by watching videos sequentially,\nit is difficult to compare the precise difference in performance between two\ntrials. Therefore, side-by-side display or overlay of two videos may be helpful\nfor training. To accomplish this, the spatial and temporal alignment of\nmultiple performances must be ensured. In this study, we propose a conventional\nbut plausible solution using the existing image processing techniques for\nsnowboard big air training. We conducted interviews with expert snowboarders\nwho stated that the spatiotemporally aligned videos enabled them to precisely\nidentify slight differences in body movements. The results suggest that the\nproposed method can be used during the training of snowboard big air.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Matsumura_S/0/1/0/all/0/1\">Seiji Matsumura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikami_D/0/1/0/all/0/1\">Dan Mikami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saijo_N/0/1/0/all/0/1\">Naoki Saijo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashino_M/0/1/0/all/0/1\">Makio Kashino</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch Those Words: Video Falsification Detection Using Word-Conditioned Facial Motion. (arXiv:2112.10936v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10936","description":"<p>In today's era of digital misinformation, we are increasingly faced with new\nthreats posed by video falsification techniques. Such falsifications range from\ncheapfakes (e.g., lookalikes or audio dubbing) to deepfakes (e.g.,\nsophisticated AI media synthesis methods), which are becoming perceptually\nindistinguishable from real videos. To tackle this challenge, we propose a\nmulti-modal semantic forensic approach to discover clues that go beyond\ndetecting discrepancies in visual quality, thereby handling both simpler\ncheapfakes and visually persuasive deepfakes. In this work, our goal is to\nverify that the purported person seen in the video is indeed themselves by\ndetecting anomalous correspondences between their facial movements and the\nwords they are saying. We leverage the idea of attribution to learn\nperson-specific biometric patterns that distinguish a given speaker from\nothers. We use interpretable Action Units (AUs) to capture a persons' face and\nhead movement as opposed to deep CNN visual features, and we are the first to\nuse word-conditioned facial motion analysis. Unlike existing person-specific\napproaches, our method is also effective against attacks that focus on lip\nmanipulation. We further demonstrate our method's effectiveness on a range of\nfakes not seen in training including those without video manipulation, that\nwere not addressed in prior work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Shruti Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Liwen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Evonne Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structured Semantic Transfer for Multi-Label Recognition with Partial Labels. (arXiv:2112.10941v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10941","description":"<p>Multi-label image recognition is a fundamental yet practical task because\nreal-world images inherently possess multiple semantic labels. However, it is\ndifficult to collect large-scale multi-label annotations due to the complexity\nof both the input images and output label spaces. To reduce the annotation\ncost, we propose a structured semantic transfer (SST) framework that enables\ntraining multi-label recognition models with partial labels, i.e., merely some\nlabels are known while other labels are missing (also called unknown labels)\nper image. The framework consists of two complementary transfer modules that\nexplore within-image and cross-image semantic correlations to transfer\nknowledge of known labels to generate pseudo labels for unknown labels.\nSpecifically, an intra-image semantic transfer module learns image-specific\nlabel co-occurrence matrix and maps the known labels to complement unknown\nlabels based on this matrix. Meanwhile, a cross-image transfer module learns\ncategory-specific feature similarities and helps complement unknown labels with\nhigh similarities. Finally, both known and generated labels are used to train\nthe multi-label recognition models. Extensive experiments on the Microsoft\nCOCO, Visual Genome and Pascal VOC datasets show that the proposed SST\nframework obtains superior performance over current state-of-the-art\nalgorithms. Codes are available at\n\\url{https://github.com/HCPLab-SYSU/SST-MLR-PL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianshui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pu_T/0/1/0/all/0/1\">Tao Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hefeng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pixel-Stega: Generative Image Steganography Based on Autoregressive Models. (arXiv:2112.10945v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10945","description":"<p>In this letter, we explored generative image steganography based on\nautoregressive models. We proposed Pixel-Stega, which implements pixel-level\ninformation hiding with autoregressive models and arithmetic coding algorithm.\nFirstly, one of the autoregressive models, PixelCNN++, is utilized to produce\nexplicit conditional probability distribution of each pixel. Secondly, secret\nmessages are encoded to the selection of pixels through steganographic sampling\n(stegosampling) based on arithmetic coding. We carried out qualitative and\nquantitative assessment on gray-scale and colour image datasets. Experimental\nresults show that Pixel-Stega is able to embed secret messages adaptively\naccording to the entropy of the pixels to achieve both high embedding capacity\n(up to 4.3 bpp) and nearly perfect imperceptibility (about 50% detection\naccuracy).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Siyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhongliang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_H/0/1/0/all/0/1\">Haoqin Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jinshuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Image Transmission for Scene Classification in Unmanned Aerial Systems. (arXiv:2112.10948v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10948","description":"<p>The vigorous developments of Internet of Things make it possible to extend\nits computing and storage capabilities to computing tasks in the aerial system\nwith collaboration of cloud and edge, especially for artificial intelligence\n(AI) tasks based on deep learning (DL). Collecting a large amount of\nimage/video data, Unmanned aerial vehicles (UAVs) can only handover intelligent\nanalysis tasks to the back-end mobile edge computing (MEC) server due to their\nlimited storage and computing capabilities. How to efficiently transmit the\nmost correlated information for the AI model is a challenging topic. Inspired\nby the task-oriented communication in recent years, we propose a new aerial\nimage transmission paradigm for the scene classification task. A lightweight\nmodel is developed on the front-end UAV for semantic blocks transmission with\nperception of images and channel conditions. In order to achieve the tradeoff\nbetween transmission latency and classification accuracy, deep reinforcement\nlearning (DRL) is used to explore the semantic blocks which have the best\ncontribution to the back-end classifier under various channel conditions.\nExperimental results show that the proposed method can significantly improve\nclassification accuracy compared to the fixed transmission strategy and\ntraditional content perception methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1\">Xu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1\">Bin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jie Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhijin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">F. Richard Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous-Time Video Generation via Learning Motion Dynamics with Neural ODE. (arXiv:2112.10960v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10960","description":"<p>In order to perform unconditional video generation, we must learn the\ndistribution of the real-world videos. In an effort to synthesize high-quality\nvideos, various studies attempted to learn a mapping function between noise and\nvideos, including recent efforts to separate motion distribution and appearance\ndistribution. Previous methods, however, learn motion dynamics in discretized,\nfixed-interval timesteps, which is contrary to the continuous nature of motion\nof a physical body. In this paper, we propose a novel video generation approach\nthat learns separate distributions for motion and appearance, the former\nmodeled by neural ODE to learn natural motion dynamics. Specifically, we employ\na two-stage approach where the first stage converts a noise vector to a\nsequence of keypoints in arbitrary frame rates, and the second stage\nsynthesizes videos based on the given keypoints sequence and the appearance\nnoise vector. Our model not only quantitatively outperforms recent baselines\nfor video generation, but also demonstrates versatile functionality such as\ndynamic frame rate manipulation and motion transfer between two datasets, thus\nopening new doors to diverse video generation applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kangyeol Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Junsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joonseok Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sookyung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1\">Jaegul Choo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Edward Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nonlinear Transform Source-Channel Coding for Semantic Communications. (arXiv:2112.10961v1 [cs.IT])","link":"http://arxiv.org/abs/2112.10961","description":"<p>In this paper, we propose a new class of high-efficient deep joint\nsource-channel coding methods that can closely adapt to the source distribution\nunder the nonlinear transform, it can be collected under the name nonlinear\ntransform source-channel coding (NTSCC). In the considered model, the\ntransmitter first learns a nonlinear analysis transform to map the source data\ninto latent space, then transmits the latent representation to the receiver via\ndeep joint source-channel coding. Our model incorporates the nonlinear\ntransform as a strong prior to effectively extract the source semantic features\nand provide side information for source-channel coding. Unlike existing\nconventional deep joint source-channel coding methods, the proposed NTSCC\nessentially learns both the source latent representation and an entropy model\nas the prior on the latent representation. Accordingly, novel adaptive rate\ntransmission and hyperprior-aided codec refinement mechanisms are developed to\nupgrade deep joint source-channel coding. The whole system design is formulated\nas an optimization problem whose goal is to minimize the end-to-end\ntransmission rate-distortion performance under established perceptual quality\nmetrics. Across simple example sources and test image sources, we find that the\nproposed NTSCC transmission method generally outperforms both the analog\ntransmission using the standard deep joint source-channel coding and the\nclassical separation-based digital transmission. Notably, the proposed NTSCC\nmethod can potentially support future semantic communications due to its\nvigorous content-aware ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jincheng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sixian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1\">Kailin Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_Z/0/1/0/all/0/1\">Zhongwei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xiaoqi Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_K/0/1/0/all/0/1\">Kai Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Ping Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRPN: Making CNN Dynamically Handle Scale Variation. (arXiv:2112.10963v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10963","description":"<p>Based on our observations of infrared targets, serious scale variation along\nwithin sequence frames has high-frequently occurred. In this paper, we propose\na dynamic re-parameterization network (DRPN) to deal with the scale variation\nand balance the detection precision between small targets and large targets in\ninfrared datasets. DRPN adopts the multiple branches with different sizes of\nconvolution kernels and the dynamic convolution strategy. Multiple branches\nwith different sizes of convolution kernels have different sizes of receptive\nfields. Dynamic convolution strategy makes DRPN adaptively weight multiple\nbranches. DRPN can dynamically adjust the receptive field according to the\nscale variation of the target. Besides, in order to maintain effective\ninference in the test phase, the multi-branch structure is further converted to\na single-branch structure via the re-parameterization technique after training.\nExtensive experiments on FLIR, KAIST, and InfraPlane datasets demonstrate the\neffectiveness of our proposed DRPN. The experimental results show that\ndetectors using the proposed DRPN as the basic structure rather than SKNet or\nTridentNet obtained the best performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jingchao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Haitao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhengwei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1\">Yi Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bofan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizing Interactive Backpropagating Refinement for Dense Prediction. (arXiv:2112.10969v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10969","description":"<p>As deep neural networks become the state-of-the-art approach in the field of\ncomputer vision for dense prediction tasks, many methods have been developed\nfor automatic estimation of the target outputs given the visual inputs.\nAlthough the estimation accuracy of the proposed automatic methods continues to\nimprove, interactive refinement is oftentimes necessary for further correction.\nRecently, feature backpropagating refinement scheme (\\text{\\textit{f}-BRS}) has\nbeen proposed for the task of interactive segmentation, which enables efficient\noptimization of a small set of auxiliary variables inserted into the pretrained\nnetwork to produce object segmentation that better aligns with user inputs.\nHowever, the proposed auxiliary variables only contain channel-wise scale and\nbias, limiting the optimization to global refinement only. In this work, in\norder to generalize backpropagating refinement for a wide range of dense\nprediction tasks, we introduce a set of G-BRS (Generalized Backpropagating\nRefinement Scheme) layers that enable both global and localized refinement for\nthe following tasks: interactive segmentation, semantic segmentation, image\nmatting and monocular depth estimation. Experiments on SBD, Cityscapes,\nMapillary Vista, Composition-1k and NYU-Depth-V2 show that our method can\nsuccessfully generalize and significantly improve performance of existing\npretrained state-of-the-art models with only a few clicks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1\">Fanqing Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_T/0/1/0/all/0/1\">Tony Martinez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ACGNet: Action Complement Graph Network for Weakly-supervised Temporal Action Localization. (arXiv:2112.10977v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10977","description":"<p>Weakly-supervised temporal action localization (WTAL) in untrimmed videos has\nemerged as a practical but challenging task since only video-level labels are\navailable. Existing approaches typically leverage off-the-shelf segment-level\nfeatures, which suffer from spatial incompleteness and temporal incoherence,\nthus limiting their performance. In this paper, we tackle this problem from a\nnew perspective by enhancing segment-level representations with a simple yet\neffective graph convolutional network, namely action complement graph network\n(ACGNet). It facilitates the current video segment to perceive spatial-temporal\ndependencies from others that potentially convey complementary clues,\nimplicitly mitigating the negative effects caused by the two issues above. By\nthis means, the segment-level features are more discriminative and robust to\nspatial-temporal variations, contributing to higher localization accuracies.\nMore importantly, the proposed ACGNet works as a universal module that can be\nflexibly plugged into different WTAL frameworks, while maintaining the\nend-to-end training fashion. Extensive experiments are conducted on the\nTHUMOS'14 and ActivityNet1.2 benchmarks, where the state-of-the-art results\nclearly demonstrate the superiority of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jie Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Di Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Few-Shot Semantic Segmentation: All You Need is Fine-Tuning. (arXiv:2112.10982v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10982","description":"<p>Generalized few-shot semantic segmentation was introduced to move beyond only\nevaluating few-shot segmentation models on novel classes to include testing\ntheir ability to remember base classes. While all approaches currently are\nbased on meta-learning, they perform poorly and saturate in learning after\nobserving only a few shots. We propose the first fine-tuning solution, and\ndemonstrate that it addresses the saturation problem while achieving\nstate-of-art results on two datasets, PASCAL-$5^i$ and COCO-$20^i$. We also\nshow it outperforms existing methods whether fine-tuning multiple final layers\nor only the final layer. Finally, we present a triplet loss regularization that\nshows how to redistribute the balance of performance between novel and base\ncategories so that there is a smaller gap between them.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Myers_Dean_J/0/1/0/all/0/1\">Josh Myers-Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yinan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1\">Brian Price</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1\">Scott Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurari_D/0/1/0/all/0/1\">Danna Gurari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned ISTA with Error-based Thresholding for Adaptive Sparse Coding. (arXiv:2112.10985v1 [cs.LG])","link":"http://arxiv.org/abs/2112.10985","description":"<p>The learned iterative shrinkage thresholding algorithm (LISTA) introduces\ndeep unfolding models with learnable thresholds in some shrinkage functions for\nsparse coding. Drawing on some theoretical insights, we advocate an error-based\nthresholding (EBT) mechanism for LISTA, which leverages a function of the\nlayer-wise reconstruction error to suggest an appropriate threshold value for\neach observation on each layer. We show that the EBT mechanism well\ndisentangles the learnable parameters in the shrinkage functions from the\nreconstruction errors, making them more adaptive to the various observations.\nWith rigorous theoretical analyses, we show that the proposed EBT can lead to a\nfaster convergence on the basis of LISTA and its variants, in addition to its\nhigher adaptivity. Extensive experimental results confirm our theoretical\nanalyses and verify the effectiveness of our methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1\">Kailun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mapping industrial poultry operations at scale with deep learning and aerial imagery. (arXiv:2112.10988v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10988","description":"<p>Concentrated Animal Feeding Operations (CAFOs) pose serious risks to air,\nwater, and public health, but have proven to be challenging to regulate. The\nU.S. Government Accountability Office notes that a basic challenge is the lack\nof comprehensive location information on CAFOs. We use the USDA's National\nAgricultural Imagery Program (NAIP) 1m/pixel aerial imagery to detect poultry\nCAFOs across the continental United States. We train convolutional neural\nnetwork (CNN) models to identify individual poultry barns and apply the best\nperforming model to over 42 TB of imagery to create the first national,\nopen-source dataset of poultry CAFOs. We validate the model predictions against\nheld-out validation set on poultry CAFO facility locations from 10 hand-labeled\ncounties in California and demonstrate that this approach has significant\npotential to fill gaps in environmental monitoring.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chugg_B/0/1/0/all/0/1\">Ben Chugg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Brandon Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expansion-Squeeze-Excitation Fusion Network for Elderly Activity Recognition. (arXiv:2112.10992v1 [cs.CV])","link":"http://arxiv.org/abs/2112.10992","description":"<p>This work focuses on the task of elderly activity recognition, which is a\nchallenging task due to the existence of individual actions and human-object\ninteractions in elderly activities. Thus, we attempt to effectively aggregate\nthe discriminative information of actions and interactions from both RGB videos\nand skeleton sequences by attentively fusing multi-modal features. Recently,\nsome nonlinear multi-modal fusion approaches are proposed by utilizing\nnonlinear attention mechanism that is extended from Squeeze-and-Excitation\nNetworks (SENet). Inspired by this, we propose a novel\nExpansion-Squeeze-Excitation Fusion Network (ESE-FN) to effectively address the\nproblem of elderly activity recognition, which learns modal and channel-wise\nExpansion-Squeeze-Excitation (ESE) attentions for attentively fusing the\nmulti-modal features in the modal and channel-wise ways. Furthermore, we design\na new Multi-modal Loss (ML) to keep the consistency between the single-modal\nfeatures and the fused multi-modal features by adding the penalty of difference\nbetween the minimum prediction losses on single modalities and the prediction\nloss on the fused modality. Finally, we conduct experiments on a largest-scale\nelderly activity dataset, i.e., ETRI-Activity3D (including 110,000+ videos, and\n50+ categories), to demonstrate that the proposed ESE-FN achieves the best\naccuracy compared with the state-of-the-art methods. In addition, more\nextensive experimental results show that the proposed ESE-FN is also comparable\nto the other methods in terms of normal action recognition task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1\">Xiangbo Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jiawen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1\">Rui Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yan Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point spread function estimation for blind image deblurring problems based on framelet transform. (arXiv:2112.11004v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11004","description":"<p>One of the most important issues in the image processing is the approximation\nof the image that has been lost due to the blurring process. These types of\nmatters are divided into non-blind and blind problems. The second type of\nproblem is more complex in terms of calculations than the first problems due to\nthe unknown of original image and point spread function estimation. In the\npresent paper, an algorithm based on coarse-to-fine iterative by $l_0-\\alpha\nl_1$ regularization and framelet transform is introduced to approximate the\nspread function estimation. Framelet transfer improves the restored kernel due\nto the decomposition of the kernel to different frequencies. Also in the\nproposed model fraction gradient operator is used instead of ordinary gradient\noperator. The proposed method is investigated on different kinds of images such\nas text, face, natural. The output of the proposed method reflects the\neffectiveness of the proposed algorithm in restoring the images from blind\nproblems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvaz_R/0/1/0/all/0/1\">Reza Parvaz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MPViT: Multi-Path Vision Transformer for Dense Prediction. (arXiv:2112.11010v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11010","description":"<p>Dense computer vision tasks such as object detection and segmentation require\neffective multi-scale feature representation for detecting or classifying\nobjects or regions with varying sizes. While Convolutional Neural Networks\n(CNNs) have been the dominant architectures for such tasks, recently introduced\nVision Transformers (ViTs) aim to replace them as a backbone. Similar to CNNs,\nViTs build a simple multi-stage structure (i.e., fine-to-coarse) for\nmulti-scale representation with single-scale patches. In this work, with a\ndifferent perspective from existing Transformers, we explore multi-scale patch\nembedding and multi-path structure, constructing the Multi-Path Vision\nTransformer (MPViT). MPViT embeds features of the same size~(i.e., sequence\nlength) with patches of different scales simultaneously by using overlapping\nconvolutional patch embedding. Tokens of different scales are then\nindependently fed into the Transformer encoders via multiple paths and the\nresulting features are aggregated, enabling both fine and coarse feature\nrepresentations at the same feature level. Thanks to the diverse, multi-scale\nfeature representations, our MPViTs scaling from tiny~(5M) to base~(73M)\nconsistently achieve superior performance over state-of-the-art Vision\nTransformers on ImageNet classification, object detection, instance\nsegmentation, and semantic segmentation. These extensive results demonstrate\nthat MPViT can serve as a versatile backbone network for various vision tasks.\nCode will be made publicly available at \\url{https://git.io/MPViT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jonghee Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willette_J/0/1/0/all/0/1\">Jeff Willette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits. (arXiv:2112.11014v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11014","description":"<p>We obtain a personal signature of a person's learning progress in a\nself-neuromodulation task, guided by functional MRI (fMRI). The signature is\nbased on predicting the activity of the Amygdala in a second neurofeedback\nsession, given a similar fMRI-derived brain state in the first session. The\nprediction is made by a deep neural network, which is trained on the entire\ntraining cohort of patients. This signal, which is indicative of a person's\nprogress in performing the task of Amygdala modulation, is aggregated across\nmultiple prototypical brain states and then classified by a linear classifier\nto various personal and clinical indications. The predictive power of the\nobtained signature is stronger than previous approaches for obtaining a\npersonal signature from fMRI neurofeedback and provides an indication that a\nperson's learning pattern may be used as a diagnostic tool. Our code has been\nmade available, and data would be shared, subject to ethical approvals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leibovitz_R/0/1/0/all/0/1\">Rotem Leibovitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Osin_J/0/1/0/all/0/1\">Jhonathan Osin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevitch_G/0/1/0/all/0/1\">Guy Gurevitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hendler_T/0/1/0/all/0/1\">Talma Hendler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Theoretical View of Linear Backpropagation and Its Convergence. (arXiv:2112.11018v1 [cs.LG])","link":"http://arxiv.org/abs/2112.11018","description":"<p>Backpropagation is widely used for calculating gradients in deep neural\nnetworks (DNNs). Applied often along with stochastic gradient descent (SGD) or\nits variants, backpropagation is considered as a de-facto choice in a variety\nof machine learning tasks including DNN training and adversarial\nattack/defense. Recently, a linear variant of BP named LinBP was introduced for\ngenerating more transferable adversarial examples for black-box adversarial\nattacks, by Guo et al. Yet, it has not been theoretically studied and the\nconvergence analysis of such a method is lacking. This paper serves as a\ncomplement and somewhat an extension to Guo et al.'s paper, by providing\ntheoretical analyses on LinBP in neural-network-involved learning tasks\nincluding adversarial attack and model training. We demonstrate that, somewhat\nsurprisingly, LinBP can lead to faster convergence in these tasks in the same\nhyper-parameter settings, compared to BP. We confirm our theoretical results\nwith extensive experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haodi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SOIT: Segmenting Objects with Instance-Aware Transformers. (arXiv:2112.11037v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11037","description":"<p>This paper presents an end-to-end instance segmentation framework, termed\nSOIT, that Segments Objects with Instance-aware Transformers. Inspired by\nDETR~\\cite{carion2020end}, our method views instance segmentation as a direct\nset prediction problem and effectively removes the need for many hand-crafted\ncomponents like RoI cropping, one-to-many label assignment, and non-maximum\nsuppression (NMS). In SOIT, multiple queries are learned to directly reason a\nset of object embeddings of semantic category, bounding-box location, and\npixel-wise mask in parallel under the global image context. The class and\nbounding-box can be easily embedded by a fixed-length vector. The pixel-wise\nmask, especially, is embedded by a group of parameters to construct a\nlightweight instance-aware transformer. Afterward, a full-resolution mask is\nproduced by the instance-aware transformer without involving any RoI-based\noperation. Overall, SOIT introduces a simple single-stage instance segmentation\nframework that is both RoI- and NMS-free. Experimental results on the MS COCO\ndataset demonstrate that SOIT outperforms state-of-the-art instance\nsegmentation approaches significantly. Moreover, the joint learning of multiple\ntasks in a unified query embedding can also substantially improve the detection\nperformance. Code is available at \\url{https://github.com/yuxiaodongHRI/SOIT}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xiaodong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1\">Dahu Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xing Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1\">Ye Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1\">Tingqun Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wenming Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Geometry-Aware Unsupervised Domain Adaptation. (arXiv:2112.11041v1 [cs.LG])","link":"http://arxiv.org/abs/2112.11041","description":"<p>Unsupervised Domain Adaptation (UDA) aims to transfer the knowledge from the\nlabeled source domain to the unlabeled target domain in the presence of dataset\nshift. Most existing methods cannot address the domain alignment and class\ndiscrimination well, which may distort the intrinsic data structure for\ndownstream tasks (e.g., classification). To this end, we propose a novel\ngeometry-aware model to learn the transferability and discriminability\nsimultaneously via nuclear norm optimization. We introduce the domain coherence\nand class orthogonality for UDA from the perspective of subspace geometry. The\ndomain coherence will ensure the model has a larger capacity for learning\nseparable representations, and class orthogonality will minimize the\ncorrelation between clusters to alleviate the misalignment. So, they are\nconsistent and can benefit from each other. Besides, we provide a theoretical\ninsight into the norm-based learning literature in UDA, which ensures the\ninterpretability of our model. We show that the norms of domains and clusters\nare expected to be larger and smaller to enhance the transferability and\ndiscriminability, respectively. Extensive experimental results on standard UDA\ndatasets demonstrate the effectiveness of our theory and model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">You-Wei Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chuan-Xian Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zi-Ying Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Image Complexity in Macro-Level Neural Network Design for Medical Image Segmentation. (arXiv:2112.11065v1 [eess.IV])","link":"http://arxiv.org/abs/2112.11065","description":"<p>Recent progress in encoder-decoder neural network architecture design has led\nto significant performance improvements in a wide range of medical image\nsegmentation tasks. However, state-of-the-art networks for a given task may be\ntoo computationally demanding to run on affordable hardware, and thus users\noften resort to practical workarounds by modifying various macro-level design\naspects. Two common examples are downsampling of the input images and reducing\nthe network depth to meet computer memory constraints. In this paper we\ninvestigate the effects of these changes on segmentation performance and show\nthat image complexity can be used as a guideline in choosing what is best for a\ngiven dataset. We consider four statistical measures to quantify image\ncomplexity and evaluate their suitability on ten different public datasets. For\nthe purpose of our experiments we also propose two new encoder-decoder\narchitectures representing shallow and deep networks that are more memory\nefficient than currently popular networks. Our results suggest that median\nfrequency is the best complexity measure in deciding about an acceptable input\ndownsampling factor and network depth. For high-complexity datasets, a shallow\nnetwork running on the original images may yield better segmentation results\nthan a deep network running on downsampled images, whereas the opposite may be\nthe case for low-complexity images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_T/0/1/0/all/0/1\">Tariq M. Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naqvi_S/0/1/0/all/0/1\">Syed S. Naqvi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meijering_E/0/1/0/all/0/1\">Erik Meijering</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RC-Net: A Convolutional Neural Network for Retinal Vessel Segmentation. (arXiv:2112.11078v1 [eess.IV])","link":"http://arxiv.org/abs/2112.11078","description":"<p>Over recent years, increasingly complex approaches based on sophisticated\nconvolutional neural network architectures have been slowly pushing performance\non well-established benchmark datasets. In this paper, we take a step back to\nexamine the real need for such complexity. We present RC-Net, a fully\nconvolutional network, where the number of filters per layer is optimized to\nreduce feature overlapping and complexity. We also used skip connections to\nkeep spatial information loss to a minimum by keeping the number of pooling\noperations in the network to a minimum. Two publicly available retinal vessel\nsegmentation datasets were used in our experiments. In our experiments, RC-Net\nis quite competitive, outperforming alternatives vessels segmentation methods\nwith two or even three orders of magnitude less trainable parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Khan_T/0/1/0/all/0/1\">Tariq M Khan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Robles_Kelly_A/0/1/0/all/0/1\">Antonio Robles-Kelly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Naqvi_S/0/1/0/all/0/1\">Syed S. Naqvi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RepMLPNet: Hierarchical Vision MLP with Re-parameterized Locality. (arXiv:2112.11081v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11081","description":"<p>Compared to convolutional layers, fully-connected (FC) layers are better at\nmodeling the long-range dependencies but worse at capturing the local patterns,\nhence usually less favored for image recognition. In this paper, we propose a\nmethodology, Locality Injection, to incorporate local priors into an FC layer\nvia merging the trained parameters of a parallel conv kernel into the FC\nkernel. Locality Injection can be viewed as a novel Structural\nRe-parameterization method since it equivalently converts the structures via\ntransforming the parameters. Based on that, we propose a multi-layer-perceptron\n(MLP) block named RepMLP Block, which uses three FC layers to extract features,\nand a novel architecture named RepMLPNet. The hierarchical design distinguishes\nRepMLPNet from the other concurrently proposed vision MLPs. As it produces\nfeature maps of different levels, it qualifies as a backbone model for\ndownstream tasks like semantic segmentation. Our results reveal that 1)\nLocality Injection is a general methodology for MLP models; 2) RepMLPNet has\nfavorable accuracy-efficiency trade-off compared to the other MLPs; 3)\nRepMLPNet is the first MLP that seamlessly transfer to Cityscapes semantic\nsegmentation. The code and models are available at\nhttps://github.com/DingXiaoH/RepMLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1\">Xiaohan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Honghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jungong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can We Use Neural Regularization to Solve Depth Super-Resolution?. (arXiv:2112.11085v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11085","description":"<p>Depth maps captured with commodity sensors often require super-resolution to\nbe used in applications. In this work we study a super-resolution approach\nbased on a variational problem statement with Tikhonov regularization where the\nregularizer is parametrized with a deep neural network. This approach was\npreviously applied successfully in photoacoustic tomography. We experimentally\nshow that its application to depth map super-resolution is difficult, and\nprovide suggestions about the reasons for that.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gazdieva_M/0/1/0/all/0/1\">Milena Gazdieva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voynov_O/0/1/0/all/0/1\">Oleg Voynov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemov_A/0/1/0/all/0/1\">Alexey Artemov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Youyi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velho_L/0/1/0/all/0/1\">Luiz Velho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EPNet++: Cascade Bi-directional Fusion for Multi-Modal 3D Object Detection. (arXiv:2112.11088v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11088","description":"<p>Recently, fusing the LiDAR point cloud and camera image to improve the\nperformance and robustness of 3D object detection has received more and more\nattention, as these two modalities naturally possess strong complementarity. In\nthis paper, we propose EPNet++ for multi-modal 3D object detection by\nintroducing a novel Cascade Bi-directional Fusion~(CB-Fusion) module and a\nMulti-Modal Consistency~(MC) loss. More concretely, the proposed CB-Fusion\nmodule boosts the plentiful semantic information of point features with the\nimage features in a cascade bi-directional interaction fusion manner, leading\nto more comprehensive and discriminative feature representations. The MC loss\nexplicitly guarantees the consistency between predicted scores from two\nmodalities to obtain more comprehensive and reliable confidence scores. The\nexperiment results on the KITTI, JRDB and SUN-RGBD datasets demonstrate the\nsuperiority of EPNet++ over the state-of-the-art methods. Besides, we emphasize\na critical but easily overlooked problem, which is to explore the performance\nand robustness of a 3D detector in a sparser scene. Extensive experiments\npresent that EPNet++ outperforms the existing SOTA methods with remarkable\nmargins in highly sparse point cloud cases, which might be an available\ndirection to reduce the expensive cost of LiDAR sensors. Code will be released\nin the future.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tengteng~Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bingling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiwu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1\">Xiang Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Registration of Forest Point Clouds by Global Matching of Relative Stem Positions. (arXiv:2112.11121v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11121","description":"<p>Registering point clouds of forest environments is an essential prerequisite\nfor LiDAR applications in precision forestry. State-of-the-art methods for\nforest point cloud registration require the extraction of individual tree\nattributes, and they have an efficiency bottleneck when dealing with point\nclouds of real-world forests with dense trees. We propose an automatic, robust,\nand efficient method for the registration of forest point clouds. Our approach\nfirst locates tree stems from raw point clouds and then matches the stems based\non their relative spatial relationship to determine the registration\ntransformation. In contrast to existing methods, our algorithm requires no\nextra individual tree attributes and has linear complexity to the number of\ntrees in the environment, allowing it to align point clouds of large forest\nenvironments. Extensive experiments have revealed that our method is superior\nto the state-of-the-art methods regarding registration accuracy and robustness,\nand it significantly outperforms existing techniques in terms of efficiency.\nBesides, we introduce a new benchmark dataset that complements the very few\nexisting open datasets for the development and evaluation of registration\nmethods for forest point clouds.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xufei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zexin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xiaojun Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoter_J/0/1/0/all/0/1\">Jantien Stoter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenbin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhenlun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nan_L/0/1/0/all/0/1\">Liangliang Nan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Human Motion Prediction via Stochastic Differential Equations. (arXiv:2112.11124v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11124","description":"<p>Human motion understanding and prediction is an integral aspect in our\npursuit of machine intelligence and human-machine interaction systems. Current\nmethods typically pursue a kinematics modeling approach, relying heavily upon\nprior anatomical knowledge and constraints. However, such an approach is hard\nto generalize to different skeletal model representations, and also tends to be\ninadequate in accounting for the dynamic range and complexity of motion, thus\nhindering predictive accuracy. In this work, we propose a novel approach in\nmodeling the motion prediction problem based on stochastic differential\nequations and path integrals. The motion profile of each skeletal joint is\nformulated as a basic stochastic variable and modeled with the Langevin\nequation. We develop a strategy of employing GANs to simulate path integrals\nthat amounts to optimizing over possible future paths. We conduct experiments\nin two large benchmark datasets, Human 3.6M and CMU MoCap. It is highlighted\nthat our approach achieves a 12.48% accuracy improvement over current\nstate-of-the-art methods in average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_K/0/1/0/all/0/1\">Kedi Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenguang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haipeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1\">Yuyu Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cloud Sphere: A 3D Shape Representation via Progressive Deformation. (arXiv:2112.11133v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11133","description":"<p>In the area of 3D shape analysis, the geometric properties of a shape have\nlong been studied. Instead of directly extracting representative features using\nexpert-designed descriptors or end-to-end deep neural networks, this paper is\ndedicated to discovering distinctive information from the shape formation\nprocess. Concretely, a spherical point cloud served as the template is\nprogressively deformed to fit the target shape in a coarse-to-fine manner.\nDuring the shape formation process, several checkpoints are inserted to\nfacilitate recording and investigating the intermediate stages. For each stage,\nthe offset field is evaluated as a stage-aware description. The summation of\nthe offsets throughout the shape formation process can completely define the\ntarget shape in terms of geometry. In this perspective, one can derive the\npoint-wise shape correspondence from the template inexpensively, which benefits\nvarious graphic applications. In this paper, the Progressive Deformation-based\nAuto-Encoder (PDAE) is proposed to learn the stage-aware description through a\ncoarse-to-fine shape fitting task. Experimental results show that the proposed\nPDAE has the ability to reconstruct 3D shapes with high fidelity, and\nconsistent topology is preserved in the multi-stage deformation process.\nAdditional applications based on the stage-aware description are performed,\ndemonstrating its universality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zongji Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yunfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PONet: Robust 3D Human Pose Estimation via Learning Orientations Only. (arXiv:2112.11153v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11153","description":"<p>Conventional 3D human pose estimation relies on first detecting 2D body\nkeypoints and then solving the 2D to 3D correspondence problem.Despite the\npromising results, this learning paradigm is highly dependent on the quality of\nthe 2D keypoint detector, which is inevitably fragile to occlusions and\nout-of-image absences.In this paper,we propose a novel Pose Orientation Net\n(PONet) that is able to robustly estimate 3D pose by learning orientations\nonly, hence bypassing the error-prone keypoint detector in the absence of image\nevidence. For images with partially invisible limbs, PONet estimates the 3D\norientation of these limbs by taking advantage of the local image evidence to\nrecover the 3D pose.Moreover, PONet is competent to infer full 3D poses even\nfrom images with completely invisible limbs, by exploiting the orientation\ncorrelation between visible limbs to complement the estimated poses,further\nimproving the robustness of 3D pose estimation.We evaluate our method on\nmultiple datasets, including Human3.6M, MPII, MPI-INF-3DHP, and 3DPW. Our\nmethod achieves results on par with state-of-the-art techniques in ideal\nsettings, yet significantly eliminates the dependency on keypoint detectors and\nthe corresponding computation burden. In highly challenging scenarios, such as\ntruncation and erasing, our method performs very robustly and yields much\nsuperior results as compared to state of the art,demonstrating its potential\nfor real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaoli Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinchao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization. (arXiv:2112.11177v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11177","description":"<p>For medical image segmentation, imagine if a model was only trained using MR\nimages in source domain, how about its performance to directly segment CT\nimages in target domain? This setting, namely generalizable cross-modality\nsegmentation, owning its clinical potential, is much more challenging than\nother related settings, e.g., domain adaptation. To achieve this goal, we in\nthis paper propose a novel dual-normalization module by leveraging the\naugmented source-similar and source-dissimilar images during our generalizable\nsegmentation. To be specific, given a single source domain, aiming to simulate\nthe possible appearance change in unseen target domains, we first utilize a\nnonlinear transformation to augment source-similar and source-dissimilar\nimages. Then, to sufficiently exploit these two types of augmentations, our\nproposed dual-normalization based model employs a shared backbone yet\nindependent batch normalization layer for separate normalization. Afterwards,\nwe put forward a style-based selection scheme to automatically choose the\nappropriate path in the test stage. Extensive experiments on three publicly\navailable datasets, i.e., BraTS, Cross-Modality Cardiac and Abdominal\nMulti-Organ dataset, have demonstrated that our method outperforms other\nstate-of-the-art domain generalization methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Ziqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1\">Dong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yinghuan Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-Based Sensor Fusion for Human Activity Recognition Using IMU Signals. (arXiv:2112.11224v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11224","description":"<p>Human Activity Recognition (HAR) using wearable devices such as smart watches\nembedded with Inertial Measurement Unit (IMU) sensors has various applications\nrelevant to our daily life, such as workout tracking and health monitoring. In\nthis paper, we propose a novel attention-based approach to human activity\nrecognition using multiple IMU sensors worn at different body locations.\nFirstly, a sensor-wise feature extraction module is designed to extract the\nmost discriminative features from individual sensors with Convolutional Neural\nNetworks (CNNs). Secondly, an attention-based fusion mechanism is developed to\nlearn the importance of sensors at different body locations and to generate an\nattentive feature representation. Finally, an inter-sensor feature extraction\nmodule is applied to learn the inter-sensor correlations, which are connected\nto a classifier to output the predicted classes of activities. The proposed\napproach is evaluated using five public datasets and it outperforms\nstate-of-the-art methods on a wide variety of activity categories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1\">Wenjin Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haodong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moniruzzaman_M/0/1/0/all/0/1\">Md Moniruzzaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leu_M/0/1/0/all/0/1\">Ming C. Leu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1\">Zhaozheng Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Robustness with Image Filtering. (arXiv:2112.11235v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11235","description":"<p>Adversarial robustness is one of the most challenging problems in Deep\nLearning and Computer Vision research. All the state-of-the-art techniques\nrequire a time-consuming procedure that creates cleverly perturbed images. Due\nto its cost, many solutions have been proposed to avoid Adversarial Training.\nHowever, all these attempts proved ineffective as the attacker manages to\nexploit spurious correlations among pixels to trigger brittle features\nimplicitly learned by the model. This paper first introduces a new image\nfiltering scheme called Image-Graph Extractor (IGE) that extracts the\nfundamental nodes of an image and their connections through a graph structure.\nBy leveraging the IGE representation, we build a new defense method, Filtering\nAs a Defense, that does not allow the attacker to entangle pixels to create\nmalicious patterns. Moreover, we show that data augmentation with filtered\nimages effectively improves the model's robustness to data corruption. We\nvalidate our techniques on CIFAR-10, CIFAR-100, and ImageNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Terzi_M/0/1/0/all/0/1\">Matteo Terzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carletti_M/0/1/0/all/0/1\">Mattia Carletti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susto_G/0/1/0/all/0/1\">Gian Antonio Susto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised deep learning techniques for powdery mildew recognition based on multispectral imaging. (arXiv:2112.11242v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11242","description":"<p>Objectives. Sustainable management of plant diseases is an open challenge\nwhich has relevant economic and environmental impact. Optimal strategies rely\non human expertise for field scouting under favourable conditions to assess the\ncurrent presence and extent of disease symptoms. This labor-intensive task is\ncomplicated by the large field area to be scouted, combined with the\nmillimeter-scale size of the early symptoms to be detected. In view of this,\nimage-based detection of early disease symptoms is an attractive approach to\nautomate this process, enabling a potential high throughput monitoring at\nsustainable costs.\n</p>\n<p>Methods. Deep learning has been successfully applied in various domains to\nobtain an automatic selection of the relevant image features by learning\nfilters via a training procedure. Deep learning has recently entered also the\ndomain of plant disease detection: following this idea, in this work we present\na deep learning approach to automatically recognize powdery mildew on cucumber\nleaves. We focus on unsupervised deep learning techniques applied to\nmultispectral imaging data and we propose the use of autoencoder architectures\nto investigate two strategies for disease detection: i) clusterization of\nfeatures in a compressed space; ii) anomaly detection.\n</p>\n<p>Results. The two proposed approaches have been assessed by quantitative\nindices. The clusterization approach is not fully capable by itself to provide\naccurate predictions but it does cater relevant information. Anomaly detection\nhas instead a significant potential of resolution which could be further\nexploited as a prior for supervised architectures with a very limited number of\nlabeled samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Benfenati_A/0/1/0/all/0/1\">Alessandro Benfenati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Causin_P/0/1/0/all/0/1\">Paola Causin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oberti_R/0/1/0/all/0/1\">Roberto Oberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stefanello_G/0/1/0/all/0/1\">Giovanni Stefanello</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Projected Sliced Wasserstein Autoencoder-based Hyperspectral Images Anomaly Detection. (arXiv:2112.11243v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11243","description":"<p>Anomaly detection refers to identifying the observation that deviates from\nthe normal pattern, which has been an active research area in various domains.\nRecently, the increasing data scale, complexity, and dimension turns the\ntraditional representation and statistical-based outlier detection method into\nchallenging. In this paper, we leverage the generative model in hyperspectral\nimages anomaly detection. The gist is to model the distribution of the normal\ndata, while the out-of-distribution sample can be viewed as the outlier. At\nfirst, the variational inference-based anomaly detection methods are\ninvestigated. We theoretically and empirically find that they are unstable due\nto the strong notion of distance ($f$-divergence) served as the regularization.\nSecondly, this paper introduces sliced Wasserstein distance, which is a weaker\ndistribution measure compared with f-divergence. However, the number of\nrandomly slicing poses a difficulty to estimate the true distance. In the end,\nwe propose a projected sliced Wasserstein (PSW) autoencoder-based anomaly\nscreening method. In particular, we leverage a computation-friendly\neigen-decomposition method to find the principal component as slicing the\nhigh-dimensional data. Furthermore, our proposed distance can be calculated\nwith the closed-form, even the prior distribution is not Gaussian.\nComprehensive experiments conducted on various real-world hyperspectral anomaly\ndetection benchmarks demonstrate the superior performance of our proposed\nmethod.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaonan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Q. M. Jonathan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yimin Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hateful Memes Challenge: An Enhanced Multimodal Framework. (arXiv:2112.11244v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11244","description":"<p>Hateful Meme Challenge proposed by Facebook AI has attracted contestants\naround the world. The challenge focuses on detecting hateful speech in\nmultimodal memes. Various state-of-the-art deep learning models have been\napplied to this problem and the performance on challenge's leaderboard has also\nbeen constantly improved. In this paper, we enhance the hateful detection\nframework, including utilizing Detectron for feature extraction, exploring\ndifferent setups of VisualBERT and UNITER models with different loss functions,\nresearching the association between the hateful memes and the sensitive text\nfeatures, and finally building ensemble method to boost model performance. The\nAUROC of our fine-tuned VisualBERT, UNITER, and ensemble method achieves 0.765,\n0.790, and 0.803 on the challenge's test set, respectively, which beats the\nbaseline models. Our code is available at\nhttps://github.com/yatingtian/hateful-meme\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_A/0/1/0/all/0/1\">Aijing Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bingjun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jiaqi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yating Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Photo-realistic Images from LiDAR Point Clouds with Generative Adversarial Networks. (arXiv:2112.11245v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11245","description":"<p>We examined the feasibility of generative adversarial networks (GANs) to\ngenerate photo-realistic images from LiDAR point clouds. For this purpose, we\ncreated a dataset of point cloud image pairs and trained the GAN to predict\nphotorealistic images from LiDAR point clouds containing reflectance and\ndistance information. Our models learned how to predict realistically looking\nimages from just point cloud data, even images with black cars. Black cars are\ndifficult to detect directly from point clouds because of their low level of\nreflectivity. This approach might be used in the future to perform visual\nobject recognition on photorealistic images generated from LiDAR point clouds.\nIn addition to the conventional LiDAR system, a second system that generates\nphotorealistic images from LiDAR point clouds would run simultaneously for\nvisual object recognition in real-time. In this way, we might preserve the\nsupremacy of LiDAR and benefit from using photo-realistic images for visual\nobject recognition without the usage of any camera. In addition, this approach\ncould be used to colorize point clouds without the usage of any camera images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mor_N/0/1/0/all/0/1\">Nuriel Shalom Mor</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image quality enhancement of embedded holograms in holographic information hiding using deep neural networks. (arXiv:2112.11246v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11246","description":"<p>Holographic information hiding is a technique for embedding holograms or\nimages into another hologram, used for copyright protection and steganography\nof holograms. Using deep neural networks, we offer a way to improve the visual\nquality of embedded holograms. The brightness of an embedded hologram is set to\na fraction of that of the host hologram, resulting in a barely damaged\nreconstructed image of the host hologram. However, it is difficult to perceive\nbecause the embedded hologram's reconstructed image is darker than the\nreconstructed host image. In this study, we use deep neural networks to restore\nthe darkened image.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shimobaba_T/0/1/0/all/0/1\">Tomoyoshi Shimobaba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oshima_S/0/1/0/all/0/1\">Sota Oshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakue_T/0/1/0/all/0/1\">Takashi Kakue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ito_a/0/1/0/all/0/1\">and Tomoyoshi Ito</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PointCaps: Raw Point Cloud Processing using Capsule Networks with Euclidean Distance Routing. (arXiv:2112.11258v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11258","description":"<p>Raw point cloud processing using capsule networks is widely adopted in\nclassification, reconstruction, and segmentation due to its ability to preserve\nspatial agreement of the input data. However, most of the existing capsule\nbased network approaches are computationally heavy and fail at representing the\nentire point cloud as a single capsule. We address these limitations in\nexisting capsule network based approaches by proposing PointCaps, a novel\nconvolutional capsule architecture with parameter sharing. Along with\nPointCaps, we propose a novel Euclidean distance routing algorithm and a\nclass-independent latent representation. The latent representation captures\nphysically interpretable geometric parameters of the point cloud, with dynamic\nEuclidean routing, PointCaps well-represents the spatial (point-to-part)\nrelationships of points. PointCaps has a significantly lower number of\nparameters and requires a significantly lower number of FLOPs while achieving\nbetter reconstruction with comparable classification and segmentation accuracy\nfor raw point clouds compared to state-of-the-art capsule networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Denipitiyage_D/0/1/0/all/0/1\">Dishanika Denipitiyage</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayasundara_V/0/1/0/all/0/1\">Vinoj Jayasundara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigo_R/0/1/0/all/0/1\">Ranga Rodrigo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edussooriya_C/0/1/0/all/0/1\">Chamira U. S. Edussooriya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"High-Fidelity Point Cloud Completion with Low-Resolution Recovery and Noise-Aware Upsampling. (arXiv:2112.11271v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11271","description":"<p>Completing an unordered partial point cloud is a challenging task. Existing\napproaches that rely on decoding a latent feature to recover the complete\nshape, often lead to the completed point cloud being over-smoothing, losing\ndetails, and noisy. Instead of decoding a whole shape, we propose to decode and\nrefine a low-resolution (low-res) point cloud first, and then performs a\npatch-wise noise-aware upsampling rather than interpolating the whole sparse\npoint cloud at once, which tends to lose details. Regarding the possibility of\nlacking details of the initially decoded low-res point cloud, we propose an\niterative refinement to recover the geometric details and a symmetrization\nprocess to preserve the trustworthy information from the input partial point\ncloud. After obtaining a sparse and complete point cloud, we propose a\npatch-wise upsampling strategy. Patch-based upsampling allows to better recover\nfine details unlike decoding a whole shape, however, the existing upsampling\nmethods are not applicable to completion task due to the data discrepancy\n(i.e., input sparse data here is not from ground-truth). Therefore, we propose\na patch extraction approach to generate training patch pairs between the sparse\nand ground-truth point clouds, and an outlier removal step to suppress the\nnoisy points from the sparse point cloud. Together with the low-res recovery,\nour whole method is able to achieve high-fidelity point cloud completion.\nComprehensive evaluations are provided to demonstrate the effectiveness of the\nproposed method and its individual components.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Ren-Wu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Ling-Xiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Review of Face Presentation Attack Detection Competitions. (arXiv:2112.11290v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11290","description":"<p>Face presentation attack detection (PAD) has received increasing attention\never since the vulnerabilities to spoofing have been widely recognized. The\nstate of the art in unimodal and multi-modal face anti-spoofing has been\nassessed in eight international competitions organized in conjunction with\nmajor biometrics and computer vision conferences in 2011, 2013, 2017, 2019,\n2020 and 2021, each introducing new challenges to the research community. In\nthis chapter, we present the design and results of the five latest competitions\nfrom 2019 until 2021. The first two challenges aimed to evaluate the\neffectiveness of face PAD in multi-modal setup introducing near-infrared (NIR)\nand depth modalities in addition to colour camera data, while the latest three\ncompetitions focused on evaluating domain and attack type generalization\nabilities of face PAD algorithms operating on conventional colour images and\nvideos. We also discuss the lessons learnt from the competitions and future\nchallenges in the field in general.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komulainen_J/0/1/0/all/0/1\">Jukka Komulainen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Neural Video Compression. (arXiv:2112.11312v1 [cs.LG])","link":"http://arxiv.org/abs/2112.11312","description":"<p>We propose a method to compress full-resolution video sequences with implicit\nneural representations. Each frame is represented as a neural network that maps\ncoordinate positions to pixel values. We use a separate implicit network to\nmodulate the coordinate inputs, which enables efficient motion compensation\nbetween frames. Together with a small residual network, this allows us to\nefficiently compress P-frames relative to the previous frame. We further lower\nthe bitrate by storing the network weights with learned integer quantization.\nOur method, which we call implicit pixel flow (IPF), offers several\nsimplifications over established neural video codecs: it does not require the\nreceiver to have access to a pretrained neural network, does not use expensive\ninterpolation-based warping operations, and does not require a separate\ntraining dataset. We demonstrate the feasibility of neural implicit compression\non image and video data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunfan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozendaal_T/0/1/0/all/0/1\">Ties van Rozendaal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brehmer_J/0/1/0/all/0/1\">Johann Brehmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nagel_M/0/1/0/all/0/1\">Markus Nagel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Taco Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iSegFormer: Interactive Image Segmentation with Transformers. (arXiv:2112.11325v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11325","description":"<p>We propose iSegFormer, a novel transformer-based approach for interactive\nimage segmentation. iSegFormer is built upon existing segmentation transformers\nwith user clicks as an additional input, allowing users to interactively and\niteratively refine the segmentation mask.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multispectral image fusion by super pixel statistics. (arXiv:2112.11329v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11329","description":"<p>Multispectral image fusion is a fundamental problem of remote sensing and\nimage processing. This problem is addressed by both classic and deep learning\napproaches. This paper is focused on the classic solutions and introduces a new\nnovel approach to this family. The proposed method carries out multispectral\nimage fusion based on the content of the fused images. It relies on analysis\nbased on the level of information on segmented superpixels in the fused inputs.\nSpecifically, I address the task of visible color RGB to Near-Infrared (NIR)\nfusion. The RGB image captures the color of the scene while the NIR captures\ndetails and sees beyond haze and clouds. Since each channel senses different\ninformation of the scene, their fusion is challenging and interesting. The\nproposed method is designed to produce a fusion that contains both advantages\nof each spectra. This manuscript experiments show that the proposed method is\nvisually informative with respect to other classic fusion methods which can be\nrun fastly on embedded devices with no need for heavy computation resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ofir_N/0/1/0/all/0/1\">Nati Ofir</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PrimSeq: a deep learning-based pipeline to quantitate rehabilitation training. (arXiv:2112.11330v1 [cs.LG])","link":"http://arxiv.org/abs/2112.11330","description":"<p>Stroke rehabilitation seeks to increase neuroplasticity through the repeated\npractice of functional motions, but may have minimal impact on recovery because\nof insufficient repetitions. The optimal training content and quantity are\ncurrently unknown because no practical tools exist to measure them. Here, we\npresent PrimSeq, a pipeline to classify and count functional motions trained in\nstroke rehabilitation. Our approach integrates wearable sensors to capture\nupper-body motion, a deep learning model to predict motion sequences, and an\nalgorithm to tally motions. The trained model accurately decomposes\nrehabilitation activities into component functional motions, outperforming\ncompetitive machine learning methods. PrimSeq furthermore quantifies these\nmotions at a fraction of the time and labor costs of human experts. We\ndemonstrate the capabilities of PrimSeq in previously unseen stroke patients\nwith a range of upper extremity motor impairment. We expect that these advances\nwill support the rigorous measurement required for quantitative dosing trials\nin stroke rehabilitation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parnandi_A/0/1/0/all/0/1\">Avinash Parnandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaku_A/0/1/0/all/0/1\">Aakash Kaku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesan_A/0/1/0/all/0/1\">Anita Venkatesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandit_N/0/1/0/all/0/1\">Natasha Pandit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wirtanen_A/0/1/0/all/0/1\">Audre Wirtanen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajamohan_H/0/1/0/all/0/1\">Haresh Rajamohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramanan_K/0/1/0/all/0/1\">Kannan Venkataramanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nilsen_D/0/1/0/all/0/1\">Dawn Nilsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Granda_C/0/1/0/all/0/1\">Carlos Fernandez-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schambra_H/0/1/0/all/0/1\">Heidi Schambra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Learning Based 3D Point Cloud Regression for Estimating Forest Biomass. (arXiv:2112.11335v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11335","description":"<p>Knowledge of forest biomass stocks and their development is important for\nimplementing effective climate change mitigation measures. It is needed for\nstudying the processes driving af-, re-, and deforestation and is a\nprerequisite for carbon-accounting. Remote sensing using airborne LiDAR can be\nused to measure vegetation biomass at large scale. We present deep learning\nsystems for predicting wood volume, above-ground biomass (AGB), and\nsubsequently carbon directly from 3D LiDAR point cloud data. We devise\ndifferent neural network architectures for point cloud regression and evaluate\nthem on remote sensing data of areas for which AGB estimates have been obtained\nfrom field measurements in a national forest inventory. Our adaptation of\nMinkowski convolutional neural networks for regression gave the best results.\nThe deep neural networks produced significantly more accurate wood volume, AGB,\nand carbon estimates compared to state-of-the-art approaches operating on basic\nstatistics of the point clouds, and we expect this finding to have a strong\nimpact on LiDAR-based analyses of terrestrial ecosystem dynamics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oehmcke_S/0/1/0/all/0/1\">Stefan Oehmcke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revenga_J/0/1/0/all/0/1\">Jaime Revenga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nord_Larsen_T/0/1/0/all/0/1\">Thomas Nord-Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trepekli_K/0/1/0/all/0/1\">Katerina Trepekli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gieseke_F/0/1/0/all/0/1\">Fabian Gieseke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1\">Christian Igel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Transferable End-to-end Room Layout Estimation via Implicit Encoding. (arXiv:2112.11340v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11340","description":"<p>We study the problem of estimating room layouts from a single panorama image.\nMost former works have two stages: feature extraction and parametric model\nfitting. Here we propose an end-to-end method that directly predicts parametric\nlayouts from an input panorama image. It exploits an implicit encoding\nprocedure that embeds parametric layouts into a latent space. Then learning a\nmapping from images to this latent space makes end-to-end room layout\nestimation possible. However end-to-end methods have several notorious\ndrawbacks despite many intriguing properties. A widely raised criticism is that\nthey are troubled with dataset bias and do not transfer to unfamiliar domains.\nOur study echos this common belief. To this end, we propose to use semantic\nboundary prediction maps as an intermediate domain. It brings significant\nperformance boost on four benchmarks (Structured3D, PanoContext, S3DIS, and\nMatterport3D), notably in the zero-shot transfer setting. Code, data, and\nmodels will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranftl_R/0/1/0/all/0/1\">Rene Ranftl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yurong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1\">Hongbin Zha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of Articulated Objects. (arXiv:2112.11347v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11347","description":"<p>Rendering articulated objects while controlling their poses is critical to\napplications such as virtual reality or animation for movies. Manipulating the\npose of an object, however, requires the understanding of its underlying\nstructure, that is, its joints and how they interact with each other.\nUnfortunately, assuming the structure to be known, as existing methods do,\nprecludes the ability to work on new object categories. We propose to learn\nboth the appearance and the structure of previously unseen articulated objects\nby observing them move from multiple views, with no additional supervision,\nsuch as joints annotations, or information about the structure. Our insight is\nthat adjacent parts that move relative to each other must be connected by a\njoint. To leverage this observation, we model the object parts in 3D as\nellipsoids, which allows us to identify joints. We combine this explicit\nrepresentation with an implicit one that compensates for the approximation\nintroduced. We show that our method works for different structures, from\nquadrupeds, to single-arm robots, to humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Noguchi_A/0/1/0/all/0/1\">Atsuhiro Noguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_U/0/1/0/all/0/1\">Umar Iqbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1\">Jonathan Tremblay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1\">Tatsuya Harada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallo_O/0/1/0/all/0/1\">Orazio Gallo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Object Detection Using Knowledge Graph Embeddings. (arXiv:2112.11366v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11366","description":"<p>Object recognition for the most part has been approached as a one-hot problem\nthat treats classes to be discrete and unrelated. Each image region has to be\nassigned to one member of a set of objects, including a background class,\ndisregarding any similarities in the object types. In this work, we compare the\nerror statistics of the class embeddings learned from a one-hot approach with\nsemantically structured embeddings from natural language processing or\nknowledge graphs that are widely applied in open world object detection.\nExtensive experimental results on multiple knowledge-embeddings as well as\ndistance metrics indicate that knowledge-based class representations result in\nmore semantically grounded misclassifications while performing on par compared\nto one-hot methods on the challenging COCO and Cityscapes object detection\nbenchmarks. We generalize our findings to multiple object detection\narchitectures by proposing a knowledge-embedded design for keypoint-based and\ntransformer-based object detection architectures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lang_C/0/1/0/all/0/1\">Christopher Lang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1\">Alexander Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1\">Abhinav Valada</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shape from Polarization for Complex Scenes in the Wild. (arXiv:2112.11377v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11377","description":"<p>We present a new data-driven approach with physics-based priors to\nscene-level normal estimation from a single polarization image. Existing shape\nfrom polarization (SfP) works mainly focus on estimating the normal of a single\nobject rather than complex scenes in the wild. A key barrier to high-quality\nscene-level SfP is the lack of real-world SfP data in complex scenes. Hence, we\ncontribute the first real-world scene-level SfP dataset with paired input\npolarization images and ground-truth normal maps. Then we propose a\nlearning-based framework with a multi-head self-attention module and viewing\nencoding, which is designed to handle increasing polarization ambiguities\ncaused by complex materials and non-orthographic projection in scene-level SfP.\nOur trained model can be generalized to far-field outdoor scenes as the\nrelationship between polarized light and surface normals is not affected by\ndistance. Experimental results demonstrate that our approach significantly\noutperforms existing SfP models on two datasets. Our dataset and source code\nwill be publicly available at \\url{https://github.com/ChenyangLEI/sfp-wild}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1\">Chenyang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1\">Chenyang Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaxin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_N/0/1/0/all/0/1\">Na Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A novel approach for the automated segmentation and volume quantification of cardiac fats on computed tomography. (arXiv:2112.11381v1 [eess.IV])","link":"http://arxiv.org/abs/2112.11381","description":"<p>The deposits of fat on the surroundings of the heart are correlated to\nseveral health risk factors such as atherosclerosis, carotid stiffness,\ncoronary artery calcification, atrial fibrillation and many others. These\ndeposits vary unrelated to obesity, which reinforces its direct segmentation\nfor further quantification. However, manual segmentation of these fats has not\nbeen widely deployed in clinical practice due to the required human workload\nand consequential high cost of physicians and technicians. In this work, we\npropose a unified method for an autonomous segmentation and quantification of\ntwo types of cardiac fats. The segmented fats are termed epicardial and\nmediastinal, and stand apart from each other by the pericardium. Much effort\nwas devoted to achieve minimal user intervention. The proposed methodology\nmainly comprises registration and classification algorithms to perform the\ndesired segmentation. We compare the performance of several classification\nalgorithms on this task, including neural networks, probabilistic models and\ndecision tree algorithms. Experimental results of the proposed methodology have\nshown that the mean accuracy regarding both epicardial and mediastinal fats is\n98.5% (99.5% if the features are normalized), with a mean true positive rate of\n98.0%. In average, the Dice similarity index was equal to 97.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Rodrigues_E/0/1/0/all/0/1\">&#xc9;rick Oliveira Rodrigues</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morais_F/0/1/0/all/0/1\">FFC Morais</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Morais_N/0/1/0/all/0/1\">NAOS Morais</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Conci_L/0/1/0/all/0/1\">LS Conci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neto_L/0/1/0/all/0/1\">LV Neto</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Conci_A/0/1/0/all/0/1\">Aura Conci</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sports Video: Fine-Grained Action Detection and Classification of Table Tennis Strokes from Videos for MediaEval 2021. (arXiv:2112.11384v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11384","description":"<p>Sports video analysis is a prevalent research topic due to the variety of\napplication areas, ranging from multimedia intelligent devices with\nuser-tailored digests up to analysis of athletes' performance. The Sports Video\ntask is part of the MediaEval 2021 benchmark. This task tackles fine-grained\naction detection and classification from videos. The focus is on recordings of\ntable tennis games. Running since 2019, the task has offered a classification\nchallenge from untrimmed video recorded in natural conditions with known\ntemporal boundaries for each stroke. This year, the dataset is extended and\noffers, in addition, a detection challenge from untrimmed videos without\nannotations. This work aims at creating tools for sports coaches and players in\norder to analyze sports performance. Movement analysis and player profiling may\nbe built upon such technology to enrich the training experience of athletes and\nimprove their performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1\">Pierre-Etienne Martin</a> (LaBRI, MPI-EVA, UB), <a href=\"http://arxiv.org/find/cs/1/au:+Calandre_J/0/1/0/all/0/1\">Jordan Calandre</a> (MIA), <a href=\"http://arxiv.org/find/cs/1/au:+Mansencal_B/0/1/0/all/0/1\">Boris Mansencal</a> (LaBRI), <a href=\"http://arxiv.org/find/cs/1/au:+Benois_Pineau_J/0/1/0/all/0/1\">Jenny Benois-Pineau</a> (LaBRI), <a href=\"http://arxiv.org/find/cs/1/au:+Peteri_R/0/1/0/all/0/1\">Renaud P&#xe9;teri</a> (MIA), <a href=\"http://arxiv.org/find/cs/1/au:+Mascarilla_L/0/1/0/all/0/1\">Laurent Mascarilla</a> (MIA), <a href=\"http://arxiv.org/find/cs/1/au:+Morlier_J/0/1/0/all/0/1\">Julien Morlier</a> (IMS)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ADJUST: A Dictionary-Based Joint Reconstruction and Unmixing Method for Spectral Tomography. (arXiv:2112.11406v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11406","description":"<p>Advances in multi-spectral detectors are causing a paradigm shift in X-ray\nComputed Tomography (CT). Spectral information acquired from these detectors\ncan be used to extract volumetric material composition maps of the object of\ninterest. If the materials and their spectral responses are known a priori, the\nimage reconstruction step is rather straightforward. If they are not known,\nhowever, the maps as well as the responses need to be estimated jointly. A\nconventional workflow in spectral CT involves performing volume reconstruction\nfollowed by material decomposition, or vice versa. However, these methods\ninherently suffer from the ill-posedness of the joint reconstruction problem.\nTo resolve this issue, we propose `A Dictionary-based Joint reconstruction and\nUnmixing method for Spectral Tomography' (ADJUST). Our formulation relies on\nforming a dictionary of spectral signatures of materials common in CT and prior\nknowledge of the number of materials present in an object. In particular, we\ndecompose the spectral volume linearly in terms of spatial material maps, a\nspectral dictionary, and the indicator of materials for the dictionary\nelements. We propose a memory-efficient accelerated alternating proximal\ngradient method to find an approximate solution to the resulting bi-convex\nproblem. From numerical demonstrations on several synthetic phantoms, we\nobserve that ADJUST performs exceedingly well when compared to other\nstate-of-the-art methods. Additionally, we address the robustness of ADJUST\nagainst limited measurement patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeegers_M/0/1/0/all/0/1\">Math&#xe9; T. Zeegers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadu_A/0/1/0/all/0/1\">Ajinkya Kadu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeuwen_T/0/1/0/all/0/1\">Tristan van Leeuwen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batenburg_K/0/1/0/all/0/1\">Kees Joost Batenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StyleSDF: High-Resolution 3D-Consistent Image and Geometry Generation. (arXiv:2112.11427v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11427","description":"<p>We introduce a high resolution, 3D-consistent image and shape generation\ntechnique which we call StyleSDF. Our method is trained on single-view RGB data\nonly, and stands on the shoulders of StyleGAN2 for image generation, while\nsolving two main challenges in 3D-aware GANs: 1) high-resolution,\nview-consistent generation of the RGB images, and 2) detailed 3D shape. We\nachieve this by merging a SDF-based 3D representation with a style-based 2D\ngenerator. Our 3D implicit network renders low-resolution feature maps, from\nwhich the style-based network generates view-consistent, 1024x1024 images.\nNotably, our SDF-based 3D modeling defines detailed 3D surfaces, leading to\nconsistent volume rendering. Our method shows higher quality results compared\nto state of the art in terms of visual and geometric quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Or_El_R/0/1/0/all/0/1\">Roy Or-El</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xuan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1\">Mengyi Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1\">Eli Shechtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jeong Joon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemelmacher_Shlizerman_I/0/1/0/all/0/1\">Ira Kemelmacher-Shlizerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Queries for Efficient Local Attention. (arXiv:2112.11435v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11435","description":"<p>Vision Transformers (ViT) serve as powerful vision models. Unlike\nconvolutional neural networks, which dominated vision research in previous\nyears, vision transformers enjoy the ability to capture long-range dependencies\nin the data. Nonetheless, an integral part of any transformer architecture, the\nself-attention mechanism, suffers from high latency and inefficient memory\nutilization, making it less suitable for high-resolution input images. To\nalleviate these shortcomings, hierarchical vision models locally employ\nself-attention on non-interleaving windows. This relaxation reduces the\ncomplexity to be linear in the input size; however, it limits the cross-window\ninteraction, hurting the model performance. In this paper, we propose a new\nshift-invariant local attention layer, called query and attend (QnA), that\naggregates the input locally in an overlapping manner, much like convolutions.\nThe key idea behind QnA is to introduce learned queries, which allow fast and\nefficient implementation. We verify the effectiveness of our layer by\nincorporating it into a hierarchical vision transformer model. We show\nimprovements in speed and memory complexity while achieving comparable accuracy\nwith state-of-the-art models. Finally, our layer scales especially well with\nwindow size, requiring up-to x10 less memory while being up-to x5 faster than\nexisting methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1\">Moab Arar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1\">Ariel Shamir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1\">Amit H. Bermano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Modality Distillation via Learning the teacher's modality-level Gram Matrix. (arXiv:2112.11447v1 [cs.AI])","link":"http://arxiv.org/abs/2112.11447","description":"<p>In the context of multi-modality knowledge distillation research, the\nexisting methods was mainly focus on the problem of only learning teacher final\noutput. Thus, there are still deep differences between the teacher network and\nthe student network. It is necessary to force the student network to learn the\nmodality relationship information of the teacher network. To effectively\nexploit transfering knowledge from teachers to students, a novel modality\nrelation distillation paradigm by modeling the relationship information among\ndifferent modality are adopted, that is learning the teacher modality-level\nGram Matrix.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Peng Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Max-Margin Contrastive Learning. (arXiv:2112.11450v1 [cs.LG])","link":"http://arxiv.org/abs/2112.11450","description":"<p>Standard contrastive learning approaches usually require a large number of\nnegatives for effective unsupervised learning and often exhibit slow\nconvergence. We suspect this behavior is due to the suboptimal selection of\nnegatives used for offering contrast to the positives. We counter this\ndifficulty by taking inspiration from support vector machines (SVMs) to present\nmax-margin contrastive learning (MMCL). Our approach selects negatives as the\nsparse support vectors obtained via a quadratic optimization problem, and\ncontrastiveness is enforced by maximizing the decision margin. As SVM\noptimization can be computationally demanding, especially in an end-to-end\nsetting, we present simplifications that alleviate the computational burden. We\nvalidate our approach on standard vision benchmark datasets, demonstrating\nbetter performance in unsupervised representation learning over\nstate-of-the-art, while having better empirical convergence properties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Anshul Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1\">Suvrit Sra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappa_R/0/1/0/all/0/1\">Rama Chellappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GOAL: Generating 4D Whole-Body Motion for Hand-Object Grasping. (arXiv:2112.11454v1 [cs.CV])","link":"http://arxiv.org/abs/2112.11454","description":"<p>Generating digital humans that move realistically has many applications and\nis widely studied, but existing methods focus on the major limbs of the body,\nignoring the hands and head. Hands have been separately studied but the focus\nhas been on generating realistic static grasps of objects. To synthesize\nvirtual characters that interact with the world, we need to generate full-body\nmotions and realistic hand grasps simultaneously. Both sub-problems are\nchallenging on their own and, together, the state-space of poses is\nsignificantly larger, the scales of hand and body motions differ, and the\nwhole-body posture and the hand grasp must agree, satisfy physical constraints,\nand be plausible. Additionally, the head is involved because the avatar must\nlook at the object to interact with it. For the first time, we address the\nproblem of generating full-body, hand and head motions of an avatar grasping an\nunknown object. As input, our method, called GOAL, takes a 3D object, its\nposition, and a starting 3D body pose and shape. GOAL outputs a sequence of\nwhole-body poses using two novel networks. First, GNet generates a goal\nwhole-body grasp with a realistic body, head, arm, and hand pose, as well as\nhand-object contact. Second, MNet generates the motion between the starting and\ngoal pose. This is challenging, as it requires the avatar to walk towards the\nobject with foot-ground contact, orient the head towards it, reach out, and\ngrasp it with a realistic hand pose and hand-object contact. To achieve this\nthe networks exploit a representation that combines SMPL-X body parameters and\n3D vertex offsets. We train and evaluate GOAL, both qualitatively and\nquantitatively, on the GRAB dataset. Results show that GOAL generalizes well to\nunseen objects, outperforming baselines. GOAL takes a step towards synthesizing\nrealistic full-body object grasping.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Taheri_O/0/1/0/all/0/1\">Omid Taheri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choutas_V/0/1/0/all/0/1\">Vasileios Choutas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzionas_D/0/1/0/all/0/1\">Dimitrios Tzionas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TPPO: A Novel Trajectory Predictor with Pseudo Oracle. (arXiv:2002.01852v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2002.01852","description":"<p>Forecasting pedestrian trajectories in dynamic scenes remains a critical\nproblem in various applications, such as autonomous driving and socially aware\nrobots. Such forecasting is challenging due to human-human and human-object\ninteractions and future uncertainties caused by human randomness. Generative\nmodel-based methods handle future uncertainties by sampling a latent variable.\nHowever, few studies explored the generation of the latent variable. In this\nwork, we propose the Trajectory Predictor with Pseudo Oracle (TPPO), which is a\ngenerative model-based trajectory predictor. The first pseudo oracle is\npedestrians' moving directions, and the second one is the latent variable\nestimated from ground truth trajectories. A social attention module is used to\naggregate neighbors' interactions based on the correlation between pedestrians'\nmoving directions and future trajectories. This correlation is inspired by the\nfact that pedestrians' future trajectories are often influenced by pedestrians\nin front. A latent variable predictor is proposed to estimate latent variable\ndistributions from observed and ground-truth trajectories. Moreover, the gap\nbetween these two distributions is minimized during training. Therefore, the\nlatent variable predictor can estimate the latent variable from observed\ntrajectories to approximate that estimated from ground-truth trajectories. We\ncompare the performance of TPPO with related methods on several public\ndatasets. Results demonstrate that TPPO outperforms state-of-the-art methods\nwith low average and final displacement errors. The ablation study shows that\nthe prediction performance will not dramatically decrease as sampling times\ndecline during tests.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Biao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1\">Caizhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1\">Ching-yao Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaofeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial images for the primate brain. (arXiv:2011.05623v2 [q-bio.NC] UPDATED)","link":"http://arxiv.org/abs/2011.05623","description":"<p>Convolutional neural networks (CNNs) are vulnerable to adversarial attack,\nthe phenomenon that adding minuscule noise to an image can fool CNNs into\nmisclassifying it. Because this noise is nearly imperceptible to human viewers,\nbiological vision is assumed to be robust to adversarial attack. Despite this\napparent difference in robustness, CNNs are currently the best models of\nbiological vision, revealing a gap in explaining how the brain responds to\nadversarial images. Indeed, sensitivity to adversarial attack has not been\nmeasured for biological vision under normal conditions, nor have attack methods\nbeen specifically designed to affect biological vision. We studied the effects\nof adversarial attack on primate vision, measuring both monkey neuronal\nresponses and human behavior. Adversarial images were created by modifying\nimages from one category(such as human faces) to look like a target\ncategory(such as monkey faces), while limiting pixel value change. We tested\nthree attack directions via several attack methods, including directly using\nCNN adversarial images and using a CNN-based predictive model to guide monkey\nvisual neuron responses. We considered a wide range of image change magnitudes\nthat covered attack success rates up to&gt;90%. We found that adversarial images\ndesigned for CNNs were ineffective in attacking primate vision. Even when\nconsidering the best attack method, primate vision was more robust to\nadversarial attack than an ensemble of CNNs, requiring over 100-fold larger\nimage change to attack successfully. The success of individual attack methods\nand images was correlated between monkey neurons and human behavior, but was\nless correlated between either and CNN categorization. Consistently, CNN-based\nmodels of neurons, when trained on natural images, did not generalize to\nexplain neuronal responses to adversarial images.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/q-bio/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xiao_W/0/1/0/all/0/1\">Will Xiao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Tay_F/0/1/0/all/0/1\">Francis E.H. Tay</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Feng_J/0/1/0/all/0/1\">Jiashi Feng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Livingstone_M/0/1/0/all/0/1\">Margaret S. Livingstone</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"iToF2dToF: A Robust and Flexible Representation for Data-Driven Time-of-Flight Imaging. (arXiv:2103.07087v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.07087","description":"<p>Indirect Time-of-Flight (iToF) cameras are a promising depth sensing\ntechnology. However, they are prone to errors caused by multi-path interference\n(MPI) and low signal-to-noise ratio (SNR). Traditional methods, after\ndenoising, mitigate MPI by estimating a transient image that encodes depths.\nRecently, data-driven methods that jointly denoise and mitigate MPI have become\nstate-of-the-art without using the intermediate transient representation. In\nthis paper, we propose to revisit the transient representation. Using\ndata-driven priors, we interpolate/extrapolate iToF frequencies and use them to\nestimate the transient image. Given direct ToF (dToF) sensors capture transient\nimages, we name our method iToF2dToF. The transient representation is flexible.\nIt can be integrated with different rule-based depth sensing algorithms that\nare robust to low SNR and can deal with ambiguous scenarios that arise in\npractice (e.g., specular MPI, optical cross-talk). We demonstrate the benefits\nof iToF2dToF over previous methods in real depth sensing scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Barragan_F/0/1/0/all/0/1\">Felipe Gutierrez-Barragan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huaijin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mohit Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velten_A/0/1/0/all/0/1\">Andreas Velten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jinwei Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Trainable Multi-Instance Pose Estimation with Transformers. (arXiv:2103.12115v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2103.12115","description":"<p>We propose an end-to-end trainable approach for multi-instance pose\nestimation, called POET (POse Estimation Transformer). Combining a\nconvolutional neural network with a transformer encoder-decoder architecture,\nwe formulate multiinstance pose estimation from images as a direct set\nprediction problem. Our model is able to directly regress the pose of all\nindividuals, utilizing a bipartite matching scheme. POET is trained using a\nnovel set-based global loss that consists of a keypoint loss, a visibility loss\nand a class loss. POET reasons about the relations between multiple detected\nindividuals and the full image context to directly predict their poses in\nparallel. We show that POET achieves high accuracy on the COCO keypoint\ndetection task while having less parameters and higher inference speed than\nother bottom-up and top-down approaches. Moreover, we show successful transfer\nlearning when applying POET to animal pose estimation. To the best of our\nknowledge, this model is the first end-to-end trainable multi-instance pose\nestimation method and we hope it will serve as a simple and promising\nalternative.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stoffl_L/0/1/0/all/0/1\">Lucas Stoffl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidal_M/0/1/0/all/0/1\">Maxime Vidal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathis_A/0/1/0/all/0/1\">Alexander Mathis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scene Graphs: A Survey of Generations and Applications. (arXiv:2104.01111v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01111","description":"<p>Scene graph is a structured representation of a scene that can clearly\nexpress the objects, attributes, and relationships between objects in the\nscene. As computer vision technology continues to develop, people are no longer\nsatisfied with simply detecting and recognizing objects in images; instead,\npeople look forward to a higher level of understanding and reasoning about\nvisual scenes. For example, given an image, we want to not only detect and\nrecognize objects in the image, but also know the relationship between objects\n(visual relationship detection), and generate a text description (image\ncaptioning) based on the image content. Alternatively, we might want the\nmachine to tell us what the little girl in the image is doing (Visual Question\nAnswering (VQA)), or even remove the dog from the image and find similar images\n(image editing and retrieval), etc. These tasks require a higher level of\nunderstanding and reasoning for image vision tasks. The scene graph is just\nsuch a powerful tool for scene understanding. Therefore, scene graphs have\nattracted the attention of a large number of researchers, and related research\nis often cross-modal, complex, and rapidly developing. However, no relatively\nsystematic survey of scene graphs exists at present. To this end, this survey\nconducts a comprehensive investigation of the current scene graph research.\nMore specifically, we first summarized the general definition of the scene\ngraph, then conducted a comprehensive and systematic discussion on the\ngeneration method of the scene graph (SGG) and the SGG with the aid of prior\nknowledge. We then investigated the main applications of scene graphs and\nsummarized the most commonly used datasets. Finally, we provide some insights\ninto the future development of scene graphs. We believe this will be a very\nhelpful foundation for future research on scene graphs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1\">Xiaojun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengzhen Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhihui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaojiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauptmann_A/0/1/0/all/0/1\">Alex Hauptmann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nested Hierarchical Transformer: Towards Accurate, Data-Efficient andInterpretable Visual Understanding. (arXiv:2105.12723v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.12723","description":"<p>Hierarchical structures are popular in recent vision transformers, however,\nthey require sophisticated designs and massive datasets to work well. In this\npaper, we explore the idea of nesting basic local transformers on\nnon-overlapping image blocks and aggregating them in a hierarchical way. We\nfind that the block aggregation function plays a critical role in enabling\ncross-block non-local information communication. This observation leads us to\ndesign a simplified architecture that requires minor code changes upon the\noriginal vision transformer. The benefits of the proposed judiciously-selected\ndesign are threefold: (1) NesT converges faster and requires much less training\ndata to achieve good generalization on both ImageNet and small datasets like\nCIFAR; (2) when extending our key ideas to image generation, NesT leads to a\nstrong decoder that is 8$\\times$ faster than previous transformer-based\ngenerators; and (3) we show that decoupling the feature learning and\nabstraction processes via this nested hierarchy in our design enables\nconstructing a novel method (named GradCAT) for visually interpreting the\nlearned model. Source code is available\nhttps://github.com/google-research/nested-transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1\">Long Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arik_S/0/1/0/all/0/1\">Sercan O. Arik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1\">Tomas Pfister</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02320","description":"<p>Few-shot segmentation aims to train a segmentation model that can fast adapt\nto novel classes with few exemplars. The conventional training paradigm is to\nlearn to make predictions on query images conditioned on the features from\nsupport images. Previous methods only utilized the semantic-level prototypes of\nsupport images as conditional information. These methods cannot utilize all\npixel-wise support information for the query predictions, which is however\ncritical for the segmentation task. In this paper, we focus on utilizing\npixel-wise relationships between support and query images to facilitate the\nfew-shot segmentation task. We design a novel Cycle-Consistent TRansformer\n(CyCTR) module to aggregate pixel-wise support features into query ones. CyCTR\nperforms cross-attention between features from different images, i.e. support\nand query images. We observe that there may exist unexpected irrelevant\npixel-level support features. Directly performing cross-attention may aggregate\nthese features from support to query and bias the query features. Thus, we\npropose using a novel cycle-consistent attention mechanism to filter out\npossible harmful support features and encourage query features to attend to the\nmost informative pixels from support images. Experiments on all few-shot\nsegmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable\nimprovement compared to previous state-of-the-art methods. Specifically, on\nPascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for\n5-shot segmentation, outperforming previous state-of-the-art methods by 4.6%\nand 7.1% respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Gengwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Guoliang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dynamic Spatial-temporal Attention Network for Early Anticipation of Traffic Accidents. (arXiv:2106.10197v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.10197","description":"<p>The rapid advancement of sensor technologies and artificial intelligence are\ncreating new opportunities for traffic safety enhancement. Dashboard cameras\n(dashcams) have been widely deployed on both human driving vehicles and\nautomated driving vehicles. A computational intelligence model that can\naccurately and promptly predict accidents from the dashcam video will enhance\nthe preparedness for accident prevention. The spatial-temporal interaction of\ntraffic agents is complex. Visual cues for predicting a future accident are\nembedded deeply in dashcam video data. Therefore, the early anticipation of\ntraffic accidents remains a challenge. Inspired by the attention behavior of\nhumans in visually perceiving accident risks, this paper proposes a Dynamic\nSpatial-Temporal Attention (DSTA) network for the early accident anticipation\nfrom dashcam videos. The DSTA-network learns to select discriminative temporal\nsegments of a video sequence with a Dynamic Temporal Attention (DTA) module. It\nalso learns to focus on the informative spatial regions of frames with a\nDynamic Spatial Attention (DSA) module. A Gated Recurrent Unit (GRU) is trained\njointly with the attention modules to predict the probability of a future\naccident. The evaluation of the DSTA-network on two benchmark datasets confirms\nthat it has exceeded the state-of-the-art performance. A thorough ablation\nstudy that assesses the DSTA-network at the component level reveals how the\nnetwork achieves such performance. Furthermore, this paper proposes a method to\nfuse the prediction scores from two complementary models and verifies its\neffectiveness in further boosting the performance of early accident\nanticipation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1\">Muhammad Monjurul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Ruwen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhaozheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Deep Learning Technique for Video Segmentation. (arXiv:2107.01153v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.01153","description":"<p>Video segmentation, i.e., partitioning video frames into multiple segments or\nobjects, plays a critical role in a broad range of practical applications,\ne.g., visual effect assistance in movie, scene understanding in autonomous\ndriving, and virtual background creation in video conferencing, to name a few.\nRecently, due to the renaissance of connectionism in computer vision, there has\nbeen an influx of numerous deep learning based approaches that have been\ndedicated to video segmentation and delivered compelling performance. In this\nsurvey, we comprehensively review two basic lines of research in this area,\ni.e., generic object segmentation (of unknown categories) in videos and video\nsemantic segmentation, by introducing their respective task settings,\nbackground concepts, perceived need, development history, and main challenges.\nWe also provide a detailed overview of representative literature on both\nmethods and datasets. Additionally, we present quantitative performance\ncomparisons of the reviewed methods on benchmark datasets. At last, we point\nout a set of unsolved open issues in this field, and suggest possible\nopportunities for further research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ANCER: Anisotropic Certification via Sample-wise Volume Maximization. (arXiv:2107.04570v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2107.04570","description":"<p>Randomized smoothing has recently emerged as an effective tool that enables\ncertification of deep neural network classifiers at scale. All prior art on\nrandomized smoothing has focused on isotropic $\\ell_p$ certification, which has\nthe advantage of yielding certificates that can be easily compared among\nisotropic methods via $\\ell_p$-norm radius. However, isotropic certification\nlimits the region that can be certified around an input to worst-case\nadversaries, i.e., it cannot reason about other \"close\", potentially large,\nconstant prediction safe regions. To alleviate this issue, (i) we theoretically\nextend the isotropic randomized smoothing $\\ell_1$ and $\\ell_2$ certificates to\ntheir generalized anisotropic counterparts following a simplified analysis.\nMoreover, (ii) we propose evaluation metrics allowing for the comparison of\ngeneral certificates - a certificate is superior to another if it certifies a\nsuperset region - with the quantification of each certificate through the\nvolume of the certified region. We introduce ANCER, a practical framework for\nobtaining anisotropic certificates for a given test set sample via volume\nmaximization. Our empirical results demonstrate that ANCER achieves\nstate-of-the-art $\\ell_1$ and $\\ell_2$ certified accuracy on both CIFAR-10 and\nImageNet at multiple radii, while certifying substantially larger regions in\nterms of volume, thus highlighting the benefits of moving away from isotropic\nanalysis. Our code is available in https://github.com/MotasemAlfarra/ANCER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eiras_F/0/1/0/all/0/1\">Francisco Eiras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Pawan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1\">Puneet K. Dokania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SynthSeg: Domain Randomisation for Segmentation of Brain Scans of any Contrast and Resolution. (arXiv:2107.09559v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2107.09559","description":"<p>Despite advances in data augmentation and transfer learning, convolutional\nneural networks (CNNs) difficultly generalise to unseen domains. When\nsegmenting brain scans, CNNs are highly sensitive to changes in resolution and\ncontrast: even within the same MRI modality, performance can decrease across\ndatasets. Here we introduce SynthSeg, the first segmentation CNN agnostic to\ncontrast and resolution. SynthSeg is trained with synthetic data sampled from a\ngenerative model conditioned on segmentations. Crucially, we adopt a domain\nrandomisation strategy where we fully randomise the contrast and resolution of\nthe synthetic training data. Consequently, SynthSeg can segment real scans of\nany target domain without retraining or fine-tuning, which enables, for the\nfirst time, analysis of huge amounts of heterogeneous clinical data. Because\nSynthSeg only requires segmentations to be trained (no images), it can learn\nfrom labels obtained by automated methods on subjects of different populations\n(e.g., ageing and diseased), thus achieving robustness to a wide range of\nmorphological variability. We demonstrate SynthSeg on 5,300 scans of six\nmodalities and ten resolutions, where it exhibits unparalleled generalisation\ncompared with supervised CNNs, state-of-the-art domain adaptation, and Bayesian\nsegmentation. Finally, we demonstrate the generalisability of SynthSeg by\napplying it to cardiac MRI and CT segmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Billot_B/0/1/0/all/0/1\">Benjamin Billot</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Greve_D/0/1/0/all/0/1\">Douglas N. Greve</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puonti_O/0/1/0/all/0/1\">Oula Puonti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thielscher_A/0/1/0/all/0/1\">Axel Thielscher</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1\">Koen Van Leemput</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fischl_B/0/1/0/all/0/1\">Bruce Fischl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalca_A/0/1/0/all/0/1\">Adrian V. Dalca</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1\">Juan Eugenio Iglesias</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ELSED: Enhanced Line SEgment Drawing. (arXiv:2108.03144v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.03144","description":"<p>Detecting local features, such as corners, segments or blobs, is the first\nstep in the pipeline of many Computer Vision applications. Its speed is crucial\nfor real-time applications. In this paper we present ELSED, the fastest line\nsegment detector in the literature. The key for its efficiency is a local\nsegment growing algorithm that connects gradient-aligned pixels in presence of\nsmall discontinuities. The proposed algorithm not only runs in devices with\nvery low end hardware, but may also be parametrized to foster the detection of\nshort or longer segments, depending on the task at hand. We also introduce new\nmetrics to evaluate the accuracy and repeatability of segment detectors. In our\nexperiments with different public benchmarks we prove that our method accounts\nthe highest repeatability and it is the most efficient in the literature. In\nthe experiments we quantify the accuracy traded for such gain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1\">Iago Su&#xe1;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1\">Jos&#xe9; M. Buenaposada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1\">Luis Baumela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Significance of Question Encoder Sequence Model in the Out-of-Distribution Performance in Visual Question Answering. (arXiv:2108.12585v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.12585","description":"<p>Generalizing beyond the experiences has a significant role in developing\npractical AI systems. It has been shown that current Visual Question Answering\n(VQA) models are over-dependent on the language-priors (spurious correlations\nbetween question-types and their most frequent answers) from the train set and\npose poor performance on Out-of-Distribution (OOD) test sets. This conduct\nlimits their generalizability and restricts them from being utilized in\nreal-world situations. This paper shows that the sequence model architecture\nused in the question-encoder has a significant role in the generalizability of\nVQA models. To demonstrate this, we performed a detailed analysis of various\nexisting RNN-based and Transformer-based question-encoders, and along, we\nproposed a novel Graph attention network (GAT)-based question-encoder. Our\nstudy found that a better choice of sequence model in the question-encoder\nimproves the generalizability of VQA models even without using any additional\nrelatively complex bias-mitigation approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+KV_G/0/1/0/all/0/1\">Gouthaman KV</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Anurag Mittal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Optimal Target Shape for LiDAR Pose Estimation. (arXiv:2109.01181v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.01181","description":"<p>Targets are essential in problems such as object tracking in cluttered or\ntextureless environments, camera (and multi-sensor) calibration tasks, and\nsimultaneous localization and mapping (SLAM). Target shapes for these tasks\ntypically are symmetric (square, rectangular, or circular) and work well for\nstructured, dense sensor data such as pixel arrays (i.e., image). However,\nsymmetric shapes lead to pose ambiguity when using sparse sensor data such as\nLiDAR point clouds and suffer from the quantization uncertainty of the LiDAR.\nThis paper introduces the concept of optimizing target shape to remove pose\nambiguity for LiDAR point clouds. A target is designed to induce large\ngradients at edge points under rotation and translation relative to the LiDAR\nto ameliorate the quantization uncertainty associated with point cloud\nsparseness. Moreover, given a target shape, we present a means that leverages\nthe target's geometry to estimate the target's vertices while globally\nestimating the pose. Both the simulation and the experimental results (verified\nby a motion capture system) confirm that by using the optimal shape and the\nglobal solver, we achieve centimeter error in translation and a few degrees in\nrotation even when a partially illuminated target is placed 30 meters away. All\nthe implementations and datasets are available at\nhttps://github.com/UMich-BipedLab/optimal_shape_global_pose_estimation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiunn-Kai Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_W/0/1/0/all/0/1\">William Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grizzle_J/0/1/0/all/0/1\">Jessy W. Grizzle</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Tumor Segmentation through Layer Decomposition. (arXiv:2109.03230v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.03230","description":"<p>In this paper, we target self-supervised representation learning for\nzero-shot tumor segmentation. We make the following contributions: First, we\nadvocate a zero-shot setting, where models from pre-training should be directly\napplicable for the downstream task, without using any manual annotations.\nSecond, we take inspiration from \"layer-decomposition\", and innovate on the\ntraining regime with simulated tumor data. Third, we conduct extensive ablation\nstudies to analyse the critical components in data simulation, and validate the\nnecessity of different proxy tasks. We demonstrate that, with sufficient\ntexture randomization in simulation, model trained on synthetic data can\neffortlessly generalise to segment real tumor data. Forth, our approach\nachieves superior results for zero-shot tumor segmentation on different\ndownstream datasets, BraTS2018 for brain tumor segmentation and LiTS2017 for\nliver tumor segmentation. While evaluating the model transferability for tumor\nsegmentation under a low-annotation regime, the proposed approach also\noutperforms all existing self-supervised approaches, opening up the usage of\nself-supervised learning in practical scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoman Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1\">Weidi Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1\">Chaoqin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanfeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD. (arXiv:2109.07497v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.07497","description":"<p>We propose a new computationally-efficient first-order algorithm for\nModel-Agnostic Meta-Learning (MAML). The key enabling technique is to interpret\nMAML as a bilevel optimization (BLO) problem and leverage the sign-based\nSGD(signSGD) as a lower-level optimizer of BLO. We show that MAML, through the\nlens of signSGD-oriented BLO, naturally yields an alternating optimization\nscheme that just requires first-order gradients of a learned meta-model. We\nterm the resulting MAML algorithm Sign-MAML. Compared to the conventional\nfirst-order MAML (FO-MAML) algorithm, Sign-MAML is theoretically-grounded as it\ndoes not impose any assumption on the absence of second-order derivatives\nduring meta training. In practice, we show that Sign-MAML outperforms FO-MAML\nin various few-shot image classification tasks, and compared to MAML, it\nachieves a much more graceful tradeoff between classification accuracy and\ncomputation efficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1\">Chen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1\">Parikshit Ram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting 3D shapes, masks, and properties of materials, liquids, and objects inside transparent containers, using the TransProteus CGI dataset. (arXiv:2109.07577v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.07577","description":"<p>We present TransProteus, a dataset, and methods for predicting the 3D\nstructure, masks, and properties of materials, liquids, and objects inside\ntransparent vessels from a single image without prior knowledge of the image\nsource and camera parameters. Manipulating materials in transparent containers\nis essential in many fields and depends heavily on vision. This work supplies a\nnew procedurally generated dataset consisting of 50k images of liquids and\nsolid objects inside transparent containers. The image annotations include 3D\nmodels, material properties (color/transparency/roughness...), and segmentation\nmasks for the vessel and its content. The synthetic (CGI) part of the dataset\nwas procedurally generated using 13k different objects, 500 different\nenvironments (HDRI), and 1450 material textures (PBR) combined with simulated\nliquids and procedurally generated vessels. In addition, we supply 104\nreal-world images of objects inside transparent vessels with depth maps of both\nthe vessel and its content. We propose a camera agnostic method that predicts\n3D models from an image as an XYZ map. This allows the trained net to predict\nthe 3D model as a map with XYZ coordinates per pixel without prior knowledge of\nthe image source. To calculate the training loss, we use the distance between\npairs of points inside the 3D model instead of the absolute XYZ coordinates.\nThis makes the loss function translation invariant. We use this to predict 3D\nmodels of vessels and their content from a single image. Finally, we\ndemonstrate a net that uses a single image to predict the material properties\nof the vessel content and surface.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Eppel_S/0/1/0/all/0/1\">Sagi Eppel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Ru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspuru_Guzik_A/0/1/0/all/0/1\">Alan Aspuru-Guzik</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMBI: A Large Multiview Dataset of Human Body Expressions and Benchmark Challenge. (arXiv:2110.00119v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2110.00119","description":"<p>This paper presents a new large multiview dataset called HUMBI for human body\nexpressions with natural clothing. The goal of HUMBI is to facilitate modeling\nview-specific appearance and geometry of five primary body signals including\ngaze, face, hand, body, and garment from assorted people. 107 synchronized HD\ncameras are used to capture 772 distinctive subjects across gender, ethnicity,\nage, and style. With the multiview image streams, we reconstruct high fidelity\nbody expressions using 3D mesh models, which allows representing view-specific\nappearance. We demonstrate that HUMBI is highly effective in learning and\nreconstructing a complete human model and is complementary to the existing\ndatasets of human body expressions with limited views and subjects such as\nMPII-Gaze, Multi-PIE, Human3.6M, and Panoptic Studio datasets. Based on HUMBI,\nwe formulate a new benchmark challenge of a pose-guided appearance rendering\ntask that aims to substantially extend photorealism in modeling diverse human\nexpressions in 3D, which is the key enabling factor of authentic social\ntele-presence. HUMBI is publicly available at <a href=\"http://humbi-data.net\">this http URL</a>\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1\">Jae Shin Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhixuan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jaesik Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyun Soo Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SkullEngine: A Multi-stage CNN Framework for Collaborative CBCT Image Segmentation and Landmark Detection. (arXiv:2110.03828v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2110.03828","description":"<p>We propose a multi-stage coarse-to-fine CNN-based framework, called\nSkullEngine, for high-resolution segmentation and large-scale landmark\ndetection through a collaborative, integrated, and scalable JSD model and three\nsegmentation and landmark detection refinement models. We evaluated our\nframework on a clinical dataset consisting of 170 CBCT/CT images for the task\nof segmenting 2 bones (midface and mandible) and detecting 175 clinically\ncommon landmarks on bones, teeth, and soft tissues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1\">Qin Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_H/0/1/0/all/0/1\">Han Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lian_C/0/1/0/all/0/1\">Chunfeng Lian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xiaoyang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_D/0/1/0/all/0/1\">Deqiang Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_L/0/1/0/all/0/1\">Lei Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_T/0/1/0/all/0/1\">Tianshu Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gateno_J/0/1/0/all/0/1\">Jaime Gateno</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yap_P/0/1/0/all/0/1\">Pew-Thian Yap</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_J/0/1/0/all/0/1\">James J. Xia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HIERMATCH: Leveraging Label Hierarchies for Improving Semi-Supervised Learning. (arXiv:2111.00164v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.00164","description":"<p>Semi-supervised learning approaches have emerged as an active area of\nresearch to combat the challenge of obtaining large amounts of annotated data.\nTowards the goal of improving the performance of semi-supervised learning\nmethods, we propose a novel framework, HIERMATCH, a semi-supervised approach\nthat leverages hierarchical information to reduce labeling costs and performs\nas well as a vanilla semi-supervised learning method. Hierarchical information\nis often available as prior knowledge in the form of coarse labels (e.g.,\nwoodpeckers) for images with fine-grained labels (e.g., downy woodpeckers or\ngolden-fronted woodpeckers). However, the use of supervision using coarse\ncategory labels to improve semi-supervised techniques has not been explored. In\nthe absence of fine-grained labels, HIERMATCH exploits the label hierarchy and\nuses coarse class labels as a weak supervisory signal. Additionally, HIERMATCH\nis a generic-approach to improve any semisupervised learning framework, we\ndemonstrate this using our results on recent state-of-the-art techniques\nMixMatch and FixMatch. We evaluate the efficacy of HIERMATCH on two benchmark\ndatasets, namely CIFAR-100 and NABirds. HIERMATCH can reduce the usage of\nfine-grained labels by 50% on CIFAR-100 with only a marginal drop of 0.59% in\ntop-1 accuracy as compared to MixMatch. Code:\nhttps://github.com/07Agarg/HIERMATCH\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Ashima Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagga_S/0/1/0/all/0/1\">Shaurya Bagga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_Y/0/1/0/all/0/1\">Yashvardhan Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_S/0/1/0/all/0/1\">Saket Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoised Internal Models: a Brain-Inspired Autoencoder against Adversarial Attacks. (arXiv:2111.10844v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.10844","description":"<p>Despite its great success, deep learning severely suffers from robustness;\nthat is, deep neural networks are very vulnerable to adversarial attacks, even\nthe simplest ones. Inspired by recent advances in brain science, we propose the\nDenoised Internal Models (DIM), a novel generative autoencoder-based model to\ntackle this challenge. Simulating the pipeline in the human brain for visual\nsignal processing, DIM adopts a two-stage approach. In the first stage, DIM\nuses a denoiser to reduce the noise and the dimensions of inputs, reflecting\nthe information pre-processing in the thalamus. Inspired from the sparse coding\nof memory-related traces in the primary visual cortex, the second stage\nproduces a set of internal models, one for each category. We evaluate DIM over\n42 adversarial attacks, showing that DIM effectively defenses against all the\nattacks and outperforms the SOTA on the overall robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kaiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xingyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yurui Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Ge Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiachen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1\">Chunxu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jisong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yi Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REPLICA: Enhanced Feature Pyramid Network by Local Image Translation and Conjunct Attention for High-Resolution Breast Tumor Detection. (arXiv:2111.11546v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.11546","description":"<p>We introduce an improvement to the feature pyramid network of standard object\ndetection models. We call our method enhanced featuRE Pyramid network by Local\nImage translation and Conjunct Attention, or REPLICA. REPLICA improves object\ndetection performance by simultaneously (1) generating realistic but fake\nimages with simulated objects to mitigate the data-hungry problem of the\nattention mechanism, and (2) advancing the detection model architecture through\na novel modification of attention on image feature patches. Specifically, we\nuse a convolutional autoencoder as a generator to create new images by\ninjecting objects into images via local interpolation and reconstruction of\ntheir features extracted in hidden layers. Then due to the larger number of\nsimulated images, we use a visual transformer to enhance outputs of each ResNet\nlayer that serve as inputs to a feature pyramid network. We apply our\nmethodology to the problem of detecting lesions in Digital Breast Tomosynthesis\nscans (DBT), a high-resolution medical imaging modality crucial in breast\ncancer screening. We demonstrate qualitatively and quantitatively that REPLICA\ncan improve the accuracy of tumor detection using our enhanced standard object\ndetection framework via experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Haoyu Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1\">Nicholas Konz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Hanxue Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazurowski_M/0/1/0/all/0/1\">Maciej A. Mazurowski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"360-DFPE: Leveraging Monocular 360-Layouts for Direct Floor Plan Estimation. (arXiv:2112.06180v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.06180","description":"<p>We present 360-DFPE, a sequential floor plan estimation method that directly\ntakes 360-images as input without relying on active sensors or 3D information.\nOur approach leverages a loosely coupled integration between a monocular visual\nSLAM solution and a monocular 360-room layout approach, which estimate camera\nposes and layout geometries, respectively. Since our task is to sequentially\ncapture the floor plan using monocular images, the entire scene structure, room\ninstances, and room shapes are unknown. To tackle these challenges, we first\nhandle the scale difference between visual odometry and layout geometry via\nformulating an entropy minimization process, which enables us to directly align\n360-layouts without knowing the entire scene in advance. Second, to\nsequentially identify individual rooms, we propose a novel room identification\nalgorithm that tracks every room along the camera exploration using geometry\ninformation. Lastly, to estimate the final shape of the room, we propose a\nshortest path algorithm with an iterative coarse-to-fine strategy, which\nimproves prior formulations with higher accuracy and faster run-time. Moreover,\nwe collect a new floor plan dataset with challenging large-scale scenes,\nproviding both point clouds and sequential 360-image information. Experimental\nresults show that our monocular solution achieves favorable performance against\nthe current state-of-the-art algorithms that rely on active sensors and require\nthe entire scene reconstruction data in advance. Our code and dataset will be\nreleased soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solarte_B/0/1/0/all/0/1\">Bolivar Solarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yueh-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chin-Hsuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1\">Yi-Hsuan Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Sea Bubble Stream Characterization Using Wide-Baseline Stereo Photogrammetry. (arXiv:2112.07414v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.07414","description":"<p>Reliable quantification of natural and anthropogenic gas release (e.g.\\\nCO$_2$, methane) from the seafloor into the ocean, and ultimately, the\natmosphere, is a challenging task. While ship-based echo sounders allow\ndetection of free gas in the water even from a larger distance, exact\nquantification requires parameters such as rise speed and bubble size\ndistribution not obtainable by such sensors. Optical methods are complementary\nin the sense that they can provide high temporal and spatial resolution of\nsingle bubbles or bubble streams from close distance. In this contribution we\nintroduce a complete instrument and evaluation method for optical bubble stream\ncharacterization. The dedicated instrument employs a high-speed deep sea stereo\ncamera system that can record terabytes of bubble imagery when deployed at a\nseep site for later automated analysis. Bubble characteristics can be obtained\nfor short sequences of few minutes, then relocating the instrument to other\nlocations, or in autonomous mode of intervals up to several days, in order to\ncapture variations due to current and pressure changes and across tidal cycles.\nBeside reporting the steps to make bubble characterization robust and\nautonomous, we carefully evaluate the reachable accuracy and propose a novel\ncalibration procedure that, due to the lack of point correspondences, uses only\nthe silhouettes of bubbles. The system has been operated successfully in up to\n1000m water depth in the Pacific Ocean to assess methane fluxes. Besides sample\nresults we also report failure cases and lessons learnt during development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+She_M/0/1/0/all/0/1\">Mengkun She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yifan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_T/0/1/0/all/0/1\">Tim Wei&#xdf;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greinert_J/0/1/0/all/0/1\">Jens Greinert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koser_K/0/1/0/all/0/1\">Kevin K&#xf6;ser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TransZero++: Cross Attribute-Guided Transformer for Zero-Shot Learning. (arXiv:2112.08643v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.08643","description":"<p>Zero-shot learning (ZSL) tackles the novel class recognition problem by\ntransferring semantic knowledge from seen classes to unseen ones. Existing\nattention-based models have struggled to learn inferior region features in a\nsingle image by solely using unidirectional attention, which ignore the\ntransferability and discriminative attribute localization of visual features.\nIn this paper, we propose a cross attribute-guided Transformer network, termed\nTransZero++, to refine visual features and learn accurate attribute\nlocalization for semantic-augmented visual embedding representations in ZSL.\nTransZero++ consists of an attribute$\\rightarrow$visual Transformer sub-net\n(AVT) and a visual$\\rightarrow$attribute Transformer sub-net (VAT).\nSpecifically, AVT first takes a feature augmentation encoder to alleviate the\ncross-dataset problem, and improves the transferability of visual features by\nreducing the entangled relative geometry relationships among region features.\nThen, an attribute$\\rightarrow$visual decoder is employed to localize the image\nregions most relevant to each attribute in a given image for attribute-based\nvisual feature representations. Analogously, VAT uses the similar feature\naugmentation encoder to refine the visual features, which are further applied\nin visual$\\rightarrow$attribute decoder to learn visual-based attribute\nfeatures. By further introducing semantical collaborative losses, the two\nattribute-guided transformers teach each other to learn semantic-augmented\nvisual embeddings via semantical collaborative learning. Extensive experiments\nshow that TransZero++ achieves the new state-of-the-art results on three\nchallenging ZSL benchmarks. The codes are available at:\n\\url{https://github.com/shiming-chen/TransZero_pp}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Shiming Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1\">Ziming Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guo-Sen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1\">Shuicheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learned Half-Quadratic Splitting Network for Magnetic Resonance Image Reconstruction. (arXiv:2112.09760v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.09760","description":"<p>Magnetic Resonance (MR) image reconstruction from highly undersampled\n$k$-space data is critical in accelerated MR imaging (MRI) techniques. In\nrecent years, deep learning-based methods have shown great potential in this\ntask. This paper proposes a learned half-quadratic splitting algorithm for MR\nimage reconstruction and implements the algorithm in an unrolled deep learning\nnetwork architecture. We compare the performance of our proposed method on a\npublic cardiac MR dataset against DC-CNN and LPDNet, and our method outperforms\nother methods in both quantitative results and qualitative results with fewer\nmodel parameters and faster reconstruction speed. Finally, we enlarge our model\nto achieve superior reconstruction quality, and the improvement is $1.76$ dB\nand $2.74$ dB over LPDNet in peak signal-to-noise ratio on $5\\times$ and\n$10\\times$ acceleration, respectively. Code for our method is publicly\navailable at https://github.com/hellopipu/HQS-Net.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Xin_B/0/1/0/all/0/1\">Bingyu Xin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phan_T/0/1/0/all/0/1\">Timothy S. Phan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Axel_L/0/1/0/all/0/1\">Leon Axel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MoCaNet: Motion Retargeting in-the-wild via Canonicalization Networks. (arXiv:2112.10082v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10082","description":"<p>We present a novel framework that brings the 3D motion retargeting task from\ncontrolled environments to in-the-wild scenarios. In particular, our method is\ncapable of retargeting body motion from a character in a 2D monocular video to\na 3D character without using any motion capture system or 3D reconstruction\nprocedure. It is designed to leverage massive online videos for unsupervised\ntraining, needless of 3D annotations or motion-body pairing information. The\nproposed method is built upon two novel canonicalization operations, structure\ncanonicalization and view canonicalization. Trained with the canonicalization\noperations and the derived regularizations, our method learns to factorize a\nskeleton sequence into three independent semantic subspaces, i.e., motion,\nstructure, and view angle. The disentangled representation enables motion\nretargeting from 2D to 3D with high precision. Our method achieves superior\nperformance on motion transfer benchmarks with large body variations and\nchallenging actions. Notably, the canonicalized skeleton sequence could serve\nas a disentangled and interpretable representation of human motion that\nbenefits action analysis and motion retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wentao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoqian Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Di_Z/0/1/0/all/0/1\">Ziang Di</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wayne Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yizhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1\">Chen Change Loy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Deep Learning Based Workflow for Detection of Lung Nodules With Chest Radiograph. (arXiv:2112.10184v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2112.10184","description":"<p>PURPOSE: This study aimed to develop a deep learning-based tool to detect and\nlocalize lung nodules with chest radiographs(CXRs). We expected it to enhance\nthe efficiency of interpreting CXRs and reduce the possibilities of delayed\ndiagnosis of lung cancer.\n</p>\n<p>MATERIALS AND METHODS: We collected CXRs from NCKUH database and VBD, an\nopen-source medical image dataset, as our training and validation data. A\nnumber of CXRs from the Ministry of Health and Welfare(MOHW) database served as\nour test data. We built a segmentation model to identify lung areas from CXRs,\nand sliced them into 16 patches. Physicians labeled the CXRs by clicking the\npatches. These labeled patches were then used to train and fine-tune a deep\nneural network(DNN) model, classifying the patches as positive or negative.\nFinally, we test the DNN model with the lung patches of CXRs from MOHW.\n</p>\n<p>RESULTS: Our segmentation model identified the lung regions well from the\nwhole CXR. The Intersection over Union(IoU) between the ground truth and the\nsegmentation result was 0.9228. In addition, our DNN model achieved a\nsensitivity of 0.81, specificity of 0.82, and AUROC of 0.869 in 98 of 125\ncases. For the other 27 difficult cases, the sensitivity was 0.54, specificity\n0.494, and AUROC 0.682. Overall, we obtained a sensitivity of 0.78, specificity\nof 0.79, and AUROC 0.837.\n</p>\n<p>CONCLUSIONS: Our two-step workflow is comparable to state-of-the-art\nalgorithms in the sensitivity and specificity of localizing lung nodules from\nCXRs. Notably, our workflow provides an efficient way for specialists to label\nthe data, which is valuable for relevant researches because of the relative\nrarity of labeled medical image data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Tai_Y/0/1/0/all/0/1\">Yang Tai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1\">Yu-Wen Fang</a> (Same contribution), <a href=\"http://arxiv.org/find/eess/1/au:+Su_F/0/1/0/all/0/1\">Fang-Yi Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiang_J/0/1/0/all/0/1\">Jung-Hsien Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Product Re-identification System in Fully Automated Defect Detection. (arXiv:2112.10324v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10324","description":"<p>In this work, we introduce a method and present an improved neural work to\nperform product re-identification, which is an essential core function of a\nfully automated product defect detection system. Our method is based on feature\ndistance. It is the combination of feature extraction neural networks, such as\nVGG16, AlexNet, with an image search engine - Vearch. The dataset that we used\nto develop product re-identification systems is a water-bottle dataset that\nconsists of 400 images of 18 types of water bottles. This is a small dataset,\nwhich was the biggest challenge of our work. However, the combination of neural\nnetworks with Vearch shows potential to tackle the product re-identification\nproblems. Especially, our new neural network - AlphaAlexNet that a neural\nnetwork was improved based on AlexNet could improve the production\nidentification accuracy by four percent. This indicates that an ideal\nproduction identification accuracy could be achieved when efficient feature\nextraction methods could be introduced and redesigned for image feature\nextractions of nearly identical products. In order to solve the biggest\nchallenges caused by the small size of the dataset and the difficult nature of\nidentifying productions that have little differences from each other. In our\nfuture work, we propose a new roadmap to tackle nearly-identical production\nidentifications: to introduce or develop new algorithms that need very few\nimages to train themselves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenggui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Li Bin Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Label Noise for Image Retrieval by Selecting Interactions. (arXiv:2112.10453v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.10453","description":"<p>Learning with noisy labels is an active research area for image\nclassification. However, the effect of noisy labels on image retrieval has been\nless studied. In this work, we propose a noise-resistant method for image\nretrieval named Teacher-based Selection of Interactions, T-SINT, which\nidentifies noisy interactions, ie. elements in the distance matrix, and selects\ncorrect positive and negative interactions to be considered in the retrieval\nloss by using a teacher-based training setup which contributes to the\nstability. As a result, it consistently outperforms state-of-the-art methods on\nhigh noise rates across benchmark datasets with synthetic noise and more\nrealistic noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ibrahimi_S/0/1/0/all/0/1\">Sarah Ibrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sors_A/0/1/0/all/0/1\">Arnaud Sors</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezende_R/0/1/0/all/0/1\">Rafael Sampaio de Rezende</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"General Greedy De-bias Learning. (arXiv:2112.10572v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.10572","description":"<p>Neural networks often make predictions relying on the spurious correlations\nfrom the datasets rather than the intrinsic properties of the task of interest,\nfacing sharp degradation on out-of-distribution (OOD) test data. Existing\nde-bias learning frameworks try to capture specific dataset bias by bias\nannotations, they fail to handle complicated OOD scenarios. Others implicitly\nidentify the dataset bias by the special design on the low capability biased\nmodel or the loss, but they degrade when the training and testing data are from\nthe same distribution. In this paper, we propose a General Greedy De-bias\nlearning framework (GGD), which greedily trains the biased models and the base\nmodel like gradient descent in functional space. It encourages the base model\nto focus on examples that are hard to solve with biased models, thus remaining\nrobust against spurious correlations in the test stage. GGD largely improves\nmodels' OOD generalization ability on various tasks, but sometimes\nover-estimates the bias level and degrades on the in-distribution test. We\nfurther re-analyze the ensemble process of GGD and introduce the Curriculum\nRegularization into GGD inspired by curriculum learning, which achieves a good\ntrade-off between in-distribution and out-of-distribution performance.\nExtensive experiments on image classification, adversarial question answering,\nand visual question answering demonstrate the effectiveness of our method. GGD\ncan learn a more robust base model under the settings of both task-specific\nbiased models with prior knowledge and self-ensemble biased model without prior\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xinzhe Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1\">Chi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qingming Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Microfossil Identificationvia Deep Metric Learning. (arXiv:2112.09490v1 [cs.CV] CROSS LISTED)","link":"http://arxiv.org/abs/2112.09490","description":"<p>We apply deep metric learning for the first time to the prob-lem of\nclassifying planktic foraminifer shells on microscopic images. This species\nrecognition task is an important information source and scientific pillar for\nreconstructing past climates. All foraminifer CNN recognition pipelines in the\nliterature produce black-box classifiers that lack visualisation options for\nhuman experts and cannot be applied to open set problems. Here, we benchmark\nmetric learning against these pipelines, produce the first scientific\nvisualisation of the phenotypic planktic foraminifer morphology space, and\ndemonstrate that metric learning can be used to cluster species unseen during\ntraining. We show that metric learning out-performs all published CNN-based\nstate-of-the-art benchmarks in this domain. We evaluate our approach on the\n34,640 expert-annotated images of the Endless Forams public library of 35\nmodern planktic foraminifera species. Our results on this data show leading 92%\naccuracy (at 0.84 F1-score) in reproducing expert labels on withheld test data,\nand 66.5% accuracy (at 0.70 F1-score) when clustering species never encountered\nin training. We conclude that metric learning is highly effective for this\ndomain and serves as an important tool towards expert-in-the-loop automation of\nmicrofossil identification. Key code, network weights, and data splits are\npublished with this paper for full reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karaderi_T/0/1/0/all/0/1\">Tayfun Karaderi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghardt_T/0/1/0/all/0/1\">Tilo Burghardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiang_A/0/1/0/all/0/1\">Allison Y. Hsiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramaer_J/0/1/0/all/0/1\">Jacob Ramaer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_D/0/1/0/all/0/1\">Daniela N. Schmidt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-12-21T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}}]}]}