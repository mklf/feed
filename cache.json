{"site_title":"M.D.Arxiv","project_name":"notfeed","project_version":"0.2.9","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2022-01-31T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Recursive Decoding: A Situated Cognition Approach to Compositional Generation in Grounded Language Understanding. (arXiv:2201.11766v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11766","description":"<p>Compositional generalization is a troubling blind spot for neural language\nmodels. Recent efforts have presented techniques for improving a model's\nability to encode novel combinations of known inputs, but less work has focused\non generating novel combinations of known outputs. Here we focus on this latter\n\"decode-side\" form of generalization in the context of gSCAN, a synthetic\nbenchmark for compositional generalization in grounded language understanding.\nWe present Recursive Decoding (RD), a novel procedure for training and using\nseq2seq models, targeted towards decode-side generalization. Rather than\ngenerating an entire output sequence in one pass, models are trained to predict\none token at a time. Inputs (i.e., the external gSCAN environment) are then\nincrementally updated based on predicted tokens, and re-encoded for the next\ndecoder time step. RD thus decomposes a complex, out-of-distribution sequence\ngeneration task into a series of incremental predictions that each resemble\nwhat the model has already seen during training. RD yields dramatic improvement\non two previously neglected generalization tasks in gSCAN. We provide analyses\nto elucidate these gains over failure of a baseline, and then discuss\nimplications for generalization in naturalistic grounded language\nunderstanding, and seq2seq more generally.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setzler_M/0/1/0/all/0/1\">Matthew Setzler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1\">Scott Howland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_L/0/1/0/all/0/1\">Lauren Phillips</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Going Extreme: Comparative Analysis of Hate Speech in Parler and Gab. (arXiv:2201.11770v1 [cs.SI])","link":"http://arxiv.org/abs/2201.11770","description":"<p>Social platforms such as Gab and Parler, branded as `free-speech' networks,\nhave seen a significant growth of their user base in recent years. This\npopularity is mainly attributed to the stricter moderation enforced by\nmainstream platforms such as Twitter, Facebook, and Reddit. In this work we\nprovide the first large scale analysis of hate-speech on Parler.\n</p>\n<p>We experiment with an array of algorithms for hate-speech detection,\ndemonstrating limitations of transfer learning in that domain, given the\nillusive and ever changing nature of the ways hate-speech is delivered. In\norder to improve classification accuracy we annotated 10K Parler posts, which\nwe use to fine-tune a BERT classifier. Classification of individual posts is\nthen leveraged for the classification of millions of users via label\npropagation over the social network. Classifying users by their propensity to\ndisseminate hate, we find that hate mongers make 16.1\\% of Parler active users,\nand that they have distinct characteristics comparing to other user groups. We\nfind that hate mongers are more active, more central and express distinct\nlevels of sentiment and convey a distinct array of emotions like anger and\nsadness. We further complement our analysis by comparing the trends discovered\nin Parler and those found in Gab.\n</p>\n<p>To the best of our knowledge, this is among the first works to analyze hate\nspeech in Parler in a quantitative manner and on the user level, and the first\nannotated dataset to be made available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Israeli_A/0/1/0/all/0/1\">Abraham Israeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsur_O/0/1/0/all/0/1\">Oren Tsur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Visual Transfer Learning using Knowledge Graphs. (arXiv:2201.11794v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11794","description":"<p>Recent approaches of computer vision utilize deep learning methods as they\nperform quite well if training and testing domains follow the same underlying\ndata distribution. However, it has been shown that minor variations in the\nimages that occur when using these methods in the real world can lead to\nunpredictable errors. Transfer learning is the area of machine learning that\ntries to prevent these errors. Especially, approaches that augment image data\nusing auxiliary knowledge encoded in language embeddings or knowledge graphs\n(KGs) have achieved promising results in recent years. This survey focuses on\nvisual transfer learning approaches using KGs. KGs can represent auxiliary\nknowledge either in an underlying graph-structured schema or in a vector-based\nknowledge graph embedding. Intending to enable the reader to solve visual\ntransfer learning problems with the help of specific KG-DL configurations we\nstart with a description of relevant modeling structures of a KG of various\nexpressions, such as directed labeled graphs, hypergraphs, and hyper-relational\ngraphs. We explain the notion of feature extractor, while specifically\nreferring to visual and semantic features. We provide a broad overview of\nknowledge graph embedding methods and describe several joint training\nobjectives suitable to combine them with high dimensional visual embeddings.\nThe main section introduces four different categories on how a KG can be\ncombined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge\nGraph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as\na Peer. To help researchers find evaluation benchmarks, we provide an overview\nof generic KGs and a set of image processing datasets and benchmarks including\nvarious types of auxiliary knowledge. Last, we summarize related surveys and\ngive an outlook about challenges and open issues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment-Aware Automatic Speech Recognition pre-training for enhanced Speech Emotion Recognition. (arXiv:2201.11826v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11826","description":"<p>We propose a novel multi-task pre-training method for Speech Emotion\nRecognition (SER). We pre-train SER model simultaneously on Automatic Speech\nRecognition (ASR) and sentiment classification tasks to make the acoustic ASR\nmodel more ``emotion aware''. We generate targets for the sentiment\nclassification using text-to-sentiment model trained on publicly available\ndata. Finally, we fine-tune the acoustic ASR on emotion annotated speech data.\nWe evaluated the proposed approach on the MSP-Podcast dataset, where we\nachieved the best reported concordance correlation coefficient (CCC) of 0.41\nfor valence prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghriss_A/0/1/0/all/0/1\">Ayoub Ghriss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozgic_V/0/1/0/all/0/1\">Viktor Rozgic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shriberg_E/0/1/0/all/0/1\">Elizabeth Shriberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chao Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Clinical-Longformer and Clinical-BigBird: Transformers for long clinical sequences. (arXiv:2201.11838v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11838","description":"<p>Transformers-based models, such as BERT, have dramatically improved the\nperformance for various natural language processing tasks. The clinical\nknowledge enriched model, namely ClinicalBERT, also achieved state-of-the-art\nresults when performed on clinical named entity recognition and natural\nlanguage inference tasks. One of the core limitations of these transformers is\nthe substantial memory consumption due to their full self-attention mechanism.\nTo overcome this, long sequence transformer models, e.g. Longformer and\nBigBird, were proposed with the idea of sparse attention mechanism to reduce\nthe memory usage from quadratic to the sequence length to a linear scale. These\nmodels extended the maximum input sequence length from 512 to 4096, which\nenhanced the ability of modeling long-term dependency and consequently achieved\noptimal results in a variety of tasks. Inspired by the success of these long\nsequence transformer models, we introduce two domain enriched language models,\nnamely Clinical-Longformer and Clinical-BigBird, which are pre-trained from\nlarge-scale clinical corpora. We evaluate both pre-trained models using 10\nbaseline tasks including named entity recognition, question answering, and\ndocument classification tasks. The results demonstrate that Clinical-Longformer\nand Clinical-BigBird consistently and significantly outperform ClinicalBERT as\nwell as other short-sequence transformers in all downstream tasks. We have made\nthe pre-trained models available for public download at:\n[https://huggingface.co/yikuan8/Clinical-Longformer].\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yikuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wehbe_R/0/1/0/all/0/1\">Ramsey M. Wehbe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_F/0/1/0/all/0/1\">Faraz S. Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanyin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yuan Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural-FST Class Language Model for End-to-End Speech Recognition. (arXiv:2201.11867v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11867","description":"<p>We propose Neural-FST Class Language Model (NFCLM) for end-to-end speech\nrecognition, a novel method that combines neural network language models\n(NNLMs) and finite state transducers (FSTs) in a mathematically consistent\nframework. Our method utilizes a background NNLM which models generic\nbackground text together with a collection of domain-specific entities modeled\nas individual FSTs. Each output token is generated by a mixture of these\ncomponents; the mixture weights are estimated with a separately trained neural\ndecider. We show that NFCLM significantly outperforms NNLM by 15.8% relative in\nterms of Word Error Rate. NFCLM achieves similar performance as traditional\nNNLM and FST shallow fusion while being less prone to overbiasing and 12 times\nmore compact, making it more suitable for on-device usage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruguier_A/0/1/0/all/0/1\">Antoine Bruguier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_D/0/1/0/all/0/1\">Duc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhavalkar_R/0/1/0/all/0/1\">Rohit Prabhavalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dangna Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Eun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_F/0/1/0/all/0/1\">Fuchun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalinli_O/0/1/0/all/0/1\">Ozlem Kalinli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seltzer_M/0/1/0/all/0/1\">Michael L. Seltzer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple-Source Domain Adaptation via Coordinated Domain Encoders and Paired Classifiers. (arXiv:2201.11870v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11870","description":"<p>We present a novel multiple-source unsupervised model for text classification\nunder domain shift. Our model exploits the update rates in document\nrepresentations to dynamically integrate domain encoders. It also employs a\nprobabilistic heuristic to infer the error rate in the target domain in order\nto pair source classifiers. Our heuristic exploits data transformation cost and\nthe classifier accuracy in the target feature space. We have used real world\nscenarios of Domain Adaptation to evaluate the efficacy of our algorithm. We\nalso used pretrained multi-layer transformers as the document encoder in the\nexperiments to demonstrate whether the improvement achieved by domain\nadaptation models can be delivered by out-of-the-box language model\npretraining. The experiments testify that our model is the top performing\napproach in this setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karisani_P/0/1/0/all/0/1\">Payam Karisani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Entity Mention Detection for Targetted Twitter Streams with Global Contextual Embeddings. (arXiv:2201.11885v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11885","description":"<p>Microblogging sites, like Twitter, have emerged as ubiquitous sources of\ninformation. Two important tasks related to the automatic extraction and\nanalysis of information in Microblogs are Entity Mention Detection (EMD) and\nEntity Detection (ED). The state-of-the-art EMD systems aim to model the\nnon-literary nature of microblog text by training upon offline static datasets.\nThey extract a combination of surface-level features -- orthographic, lexical,\nand semantic -- from individual messages for noisy text modeling and entity\nextraction. But given the constantly evolving nature of microblog streams,\ndetecting all entity mentions from such varying yet limited context of short\nmessages remains a difficult problem. To this end, we propose a framework named\nEMD Globalizer, better suited for the execution of EMD learners on microblog\nstreams. It deviates from the processing of isolated microblog messages by\nexisting EMD systems, where learned knowledge from the immediate context of a\nmessage is used to suggest entities. After an initial extraction of entity\ncandidates by an EMD system, the proposed framework leverages occurrence mining\nto find additional candidate mentions that are missed during this first\ndetection. Aggregating the local contextual representations of these mentions,\na global embedding is drawn from the collective context of an entity candidate\nwithin a stream. The global embeddings are then utilized to separate entities\nwithin the candidates from false positives. All mentions of said entities from\nthe stream are produced in the framework's final outputs. Our experiments show\nthat EMD Globalizer can enhance the effectiveness of all existing EMD systems\nthat we tested (on average by 25.61%) with a small additional computational\noverhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhowmick_S/0/1/0/all/0/1\">Satadisha Saha Bhowmick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_E/0/1/0/all/0/1\">Eduard C. Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_W/0/1/0/all/0/1\">Weiyi Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The CARE Dataset for Affective Response Detection. (arXiv:2201.11895v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11895","description":"<p>Social media plays an increasing role in our communication with friends and\nfamily, and our consumption of information and entertainment. Hence, to design\neffective ranking functions for posts on social media, it would be useful to\npredict the affective response to a post (e.g., whether the user is likely to\nbe humored, inspired, angered, informed). Similar to work on emotion\nrecognition (which focuses on the affect of the publisher of the post), the\ntraditional approach to recognizing affective response would involve an\nexpensive investment in human annotation of training data.\n</p>\n<p>We introduce CARE$_{db}$, a dataset of 230k social media posts annotated\naccording to 7 affective responses using the Common Affective Response\nExpression (CARE) method. The CARE method is a means of leveraging the signal\nthat is present in comments that are posted in response to a post, providing\nhigh-precision evidence about the affective response of the readers to the post\nwithout human annotation. Unlike human annotation, the annotation process we\ndescribe here can be iterated upon to expand the coverage of the method,\nparticularly for new affective responses. We present experiments that\ndemonstrate that the CARE annotations compare favorably with crowd-sourced\nannotations. Finally, we use CARE$_{db}$ to train competitive BERT-based models\nfor predicting affective response as well as emotion detection, demonstrating\nthe utility of the dataset for related tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jane A. Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1\">Alon Y. Halevy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models. (arXiv:2201.11903v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11903","description":"<p>Although scaling up language model size has reliably improved performance on\na range of NLP tasks, even the largest models currently struggle with certain\nreasoning tasks such as math word problems, symbolic manipulation, and\ncommonsense reasoning. This paper explores the ability of language models to\ngenerate a coherent chain of thought -- a series of short sentences that mimic\nthe reasoning process a person might have when responding to a question.\nExperiments show that inducing a chain of thought via prompting can enable\nsufficiently large language models to better perform reasoning tasks that\notherwise have flat scaling curves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuezhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1\">Dale Schuurmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosma_M/0/1/0/all/0/1\">Maarten Bosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1\">Ed Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Secure and Efficient Federated Learning Framework for NLP. (arXiv:2201.11934v1 [cs.CR])","link":"http://arxiv.org/abs/2201.11934","description":"<p>In this work, we consider the problem of designing secure and efficient\nfederated learning (FL) frameworks. Existing solutions either involve a trusted\naggregator or require heavyweight cryptographic primitives, which degrades\nperformance significantly. Moreover, many existing secure FL designs work only\nunder the restrictive assumption that none of the clients can be dropped out\nfrom the training protocol. To tackle these problems, we propose SEFL, a secure\nand efficient FL framework that (1) eliminates the need for the trusted\nentities; (2) achieves similar and even better model accuracy compared with\nexisting FL designs; (3) is resilient to client dropouts. Through extensive\nexperimental studies on natural language processing (NLP) tasks, we demonstrate\nthat the SEFL achieves comparable accuracy compared to existing FL solutions,\nand the proposed pruning technique can improve runtime performance up to 13.7x.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jieren Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xianrui Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Ji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Sheng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_F/0/1/0/all/0/1\">Fei Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1\">Sanguthevar Rajasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiffGAN-TTS: High-Fidelity and Efficient Text-to-Speech with Denoising Diffusion GANs. (arXiv:2201.11972v1 [eess.AS])","link":"http://arxiv.org/abs/2201.11972","description":"<p>Denoising diffusion probabilistic models (DDPMs) are expressive generative\nmodels that have been used to solve a variety of speech synthesis problems.\nHowever, because of their high sampling costs, DDPMs are difficult to use in\nreal-time speech processing applications. In this paper, we introduce\nDiffGAN-TTS, a novel DDPM-based text-to-speech (TTS) model achieving\nhigh-fidelity and efficient speech synthesis. DiffGAN-TTS is based on denoising\ndiffusion generative adversarial networks (GANs), which adopt an\nadversarially-trained expressive model to approximate the denoising\ndistribution. We show with multi-speaker TTS experiments that DiffGAN-TTS can\ngenerate high-fidelity speech samples within only 4 denoising steps. We present\nan active shallow diffusion mechanism to further speed up inference. A\ntwo-stage training scheme is proposed, with a basic TTS acoustic model trained\nat stage one providing valuable prior information for a DDPM trained at stage\ntwo. Our experiments show that DiffGAN-TTS can achieve high synthesis\nperformance with only 1 denoising step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model. (arXiv:2201.11990v1 [cs.CL])","link":"http://arxiv.org/abs/2201.11990","description":"<p>Pretrained general-purpose language models can achieve state-of-the-art\naccuracies in various natural language processing domains by adapting to\ndownstream tasks via zero-shot, few-shot and fine-tuning techniques. Because of\ntheir success, the size of these models has increased rapidly, requiring\nhigh-performance hardware, software, and algorithmic techniques to enable\ntraining such large models. As the result of a joint effort between Microsoft\nand NVIDIA, we present details on the training of the largest monolithic\ntransformer based language model, Megatron-Turing NLG 530B (MT-NLG), with 530\nbillion parameters. In this paper, we first focus on the infrastructure as well\nas the 3D parallelism methodology used to train this model using DeepSpeed and\nMegatron. Next, we detail the training process, the design of our training\ncorpus, and our data curation techniques, which we believe is a key ingredient\nto the success of the model. Finally, we discuss various evaluation results, as\nwell as other interesting observations and new properties exhibited by MT-NLG.\nWe demonstrate that MT-NLG achieves superior zero-, one-, and few-shot learning\naccuracies on several NLP benchmarks and establishes new state-of-the-art\nresults. We believe that our contributions will help further the development of\nlarge-scale training infrastructures, large-scale language models, and natural\nlanguage generations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1\">Shaden Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1\">Mostofa Patwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norick_B/0/1/0/all/0/1\">Brandon Norick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeGresley_P/0/1/0/all/0/1\">Patrick LeGresley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Casper_J/0/1/0/all/0/1\">Jared Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabhumoye_S/0/1/0/all/0/1\">Shrimai Prabhumoye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerveas_G/0/1/0/all/0/1\">George Zerveas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korthikanti_V/0/1/0/all/0/1\">Vijay Korthikanti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1\">Elton Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Child_R/0/1/0/all/0/1\">Rewon Child</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aminabadi_R/0/1/0/all/0/1\">Reza Yazdani Aminabadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernauer_J/0/1/0/all/0/1\">Julie Bernauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1\">Mohammad Shoeybi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Houston_M/0/1/0/all/0/1\">Michael Houston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwary_S/0/1/0/all/0/1\">Saurabh Tiwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1\">Bryan Catanzaro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linear Adversarial Concept Erasure. (arXiv:2201.12091v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12091","description":"<p>Modern neural models trained on textual data rely on pre-trained\nrepresentations that emerge without direct supervision. As these\nrepresentations are increasingly being used in real-world applications, the\ninability to \\emph{control} their content becomes an increasingly important\nproblem.\n</p>\n<p>We formulate the problem of identifying and erasing a linear subspace that\ncorresponds to a given concept, in order to prevent linear predictors from\nrecovering the concept. We model this problem as a constrained, linear minimax\ngame, and show that existing solutions are generally not optimal for this task.\nWe derive a closed-form solution for certain objectives, and propose a convex\nrelaxation, R-LACE, that works well for others. When evaluated in the context\nof binary gender removal, the method recovers a low-dimensional subspace whose\nremoval mitigates bias by intrinsic and extrinsic evaluation. We show that the\nmethod -- despite being linear -- is highly expressive, effectively mitigating\nbias in deep nonlinear classifiers while maintaining tractability and\ninterpretability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Twiton_M/0/1/0/all/0/1\">Michael Twiton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PCL: Peer-Contrastive Learning with Diverse Augmentations for Unsupervised Sentence Embeddings. (arXiv:2201.12093v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12093","description":"<p>Learning sentence embeddings in an unsupervised manner is fundamental in\nnatural language processing. Recent common practice is to couple pre-trained\nlanguage models with unsupervised contrastive learning, whose success relies on\naugmenting a sentence with a semantically-close positive instance to construct\ncontrastive pairs. Nonetheless, existing approaches usually depend on a\nmono-augmenting strategy, which causes learning shortcuts towards the\naugmenting biases and thus corrupts the quality of sentence embeddings. A\nstraightforward solution is resorting to more diverse positives from a\nmulti-augmenting strategy, while an open question remains about how to\nunsupervisedly learn from the diverse positives but with uneven augmenting\nqualities in the text field. As one answer, we propose a novel Peer-Contrastive\nLearning (PCL) with diverse augmentations. PCL constructs diverse contrastive\npositives and negatives at the group level for unsupervised sentence\nembeddings. PCL can perform peer-positive contrast as well as peer-network\ncooperation, which offers an inherent anti-bias ability and an effective way to\nlearn from diverse augmentations. Experiments on STS benchmarks verify the\neffectiveness of our PCL against its competitors in unsupervised sentence\nembeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1\">Qiyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1\">Chongyang Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Can Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xiubo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving End-to-End Models for Set Prediction in Spoken Language Understanding. (arXiv:2201.12105v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12105","description":"<p>The goal of spoken language understanding (SLU) systems is to determine the\nmeaning of the input speech signal, unlike speech recognition which aims to\nproduce verbatim transcripts. Advances in end-to-end (E2E) speech modeling have\nmade it possible to train solely on semantic entities, which are far cheaper to\ncollect than verbatim transcripts. We focus on this set prediction problem,\nwhere entity order is unspecified. Using two classes of E2E models, RNN\ntransducers and attention based encoder-decoders, we show that these models\nwork best when the training entity sequence is arranged in spoken order. To\nimprove E2E SLU models when entity spoken order is unknown, we propose a novel\ndata augmentation technique along with an implicit attention based alignment\nmethod to infer the spoken order. F1 scores significantly increased by more\nthan 11% for RNN-T and about 2% for attention based encoder-decoder SLU models,\noutperforming previously reported results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1\">Hong-Kwang J. Kuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1\">Zoltan Tuske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Protum: A New Method For Prompt Tuning Based on \"[MASK]\". (arXiv:2201.12109v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12109","description":"<p>Recently, prompt tuning \\cite{lester2021power} has gradually become a new\nparadigm for NLP, which only depends on the representation of the words by\nfreezing the parameters of pre-trained language models (PLMs) to obtain\nremarkable performance on downstream tasks. It maintains the consistency of\nMasked Language Model (MLM) \\cite{devlin2018bert} task in the process of\npre-training, and avoids some issues that may happened during fine-tuning.\nNaturally, we consider that the \"[MASK]\" tokens carry more useful information\nthan other tokens because the model combines with context to predict the masked\ntokens. Among the current prompt tuning methods, there will be a serious\nproblem of random composition of the answer tokens in prediction when they\npredict multiple words so that they have to map tokens to labels with the help\nverbalizer. In response to the above issue, we propose a new \\textbf{Pro}mpt\n\\textbf{Tu}ning based on \"[\\textbf{M}ASK]\" (\\textbf{Protum}) method in this\npaper, which constructs a classification task through the information carried\nby the hidden layer of \"[MASK]\" tokens and then predicts the labels directly\nrather than the answer tokens. At the same time, we explore how different\nhidden layers under \"[MASK]\" impact on our classification model on many\ndifferent data sets. Finally, we find that our \\textbf{Protum} can achieve much\nbetter performance than fine-tuning after continuous pre-training with less\ntime consumption. Our model facilitates the practical application of large\nmodels in NLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1\">Pan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanru Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12114","description":"<p>Attention mechanisms are dominating the explainability of deep models. They\nproduce probability distributions over the input, which are widely deemed as\nfeature-importance indicators. However, in this paper, we find one critical\nlimitation in attention explanations: weakness in identifying the polarity of\nfeature impact. This would be somehow misleading -- features with higher\nattention weights may not faithfully contribute to model predictions; instead,\nthey can impose suppression effects. With this finding, we reflect on the\nexplainability of current attention-based techniques, such as\nAttentio$\\odot$Gradient and LRP-based attention explanations. We first propose\nan actionable diagnostic methodology (henceforth faithfulness violation test)\nto measure the consistency between explanation weights and the impact polarity.\nThrough the extensive experiments, we then show that most tested explanation\nmethods are unexpectedly hindered by the faithfulness violation issue,\nespecially the raw attention. Empirical analyses on the factors affecting\nviolation issues further provide useful observations for adopting explanation\nmethods in attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Wikipedia Help Offline Reinforcement Learning?. (arXiv:2201.12122v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12122","description":"<p>Fine-tuning reinforcement learning (RL) models has been challenging because\nof a lack of large scale off-the-shelf datasets as well as high variance in\ntransferability among different environments. Recent work has looked at\ntackling offline RL from the perspective of sequence modeling with improved\nresults as result of the introduction of the Transformer architecture. However,\nwhen the model is trained from scratch, it suffers from slow convergence\nspeeds. In this paper, we look to take advantage of this formulation of\nreinforcement learning as sequence modeling and investigate the transferability\nof pre-trained sequence models on other domains (vision, language) when\nfinetuned on offline RL tasks (control, games). To this end, we also propose\ntechniques to improve transfer between these domains. Results show consistent\nperformance gains in terms of both convergence speed and reward on a variety of\nenvironments, accelerating training by 3-6x and achieving state-of-the-art\nperformance in a variety of tasks using Wikipedia-pretrained and GPT2 language\nmodels. We hope that this work not only brings light to the potentials of\nleveraging generic sequence modeling techniques and pre-trained models for RL,\nbut also inspires future work on sharing knowledge between generative modeling\ntasks of completely different domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamada_Y/0/1/0/all/0/1\">Yutaro Yamada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1\">Shixiang Shane Gu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing language context confusion for end-to-end code-switching automatic speech recognition. (arXiv:2201.12155v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12155","description":"<p>Code-switching is about dealing with alternative languages in the\ncommunication process. Training end-to-end (E2E) automatic speech recognition\n(ASR) systems for code-switching is known to be a challenging problem because\nof the lack of data compounded by the increased language context confusion due\nto the presence of more than one language. In this paper, we propose a\nlanguage-related attention mechanism to reduce multilingual context confusion\nfor the E2E code-switching ASR model based on the Equivalence Constraint Theory\n(EC). The linguistic theory requires that any monolingual fragment that occurs\nin the code-switching sentence must occur in one of the monolingual sentences.\nIt establishes a bridge between monolingual data and code-switching data. By\ncalculating the respective attention of multiple languages, our method can\nefficiently transfer language knowledge from rich monolingual data. We evaluate\nour method on ASRU 2019 Mandarin-English code-switching challenge dataset.\nCompared with the baseline model, the proposed method achieves 11.37% relative\nmix error rate reduction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1\">Jiangyan Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhengkun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_J/0/1/0/all/0/1\">Jianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1\">Yu Ting Yeung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1\">Liqun Deng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Concept Erasure in Kernel Space. (arXiv:2201.12191v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12191","description":"<p>The representation space of neural models for textual data emerges in an\nunsupervised manner during training. Understanding how human-interpretable\nconcepts, such as gender, are encoded in these representations would improve\nthe ability of users to \\emph{control} the content of these representations and\nanalyze the working of the models that rely on them. One prominent approach to\nthe control problem is the identification and removal of linear concept\nsubspaces -- subspaces in the representation space that correspond to a given\nconcept. While those are tractable and interpretable, neural network do not\nnecessarily represent concepts in linear subspaces.\n</p>\n<p>We propose a kernalization of the linear concept-removal objective of\n[Ravfogel et al. 2022], and show that it is effective in guarding against the\nability of certain nonlinear adversaries to recover the concept. Interestingly,\nour findings suggest that the division between linear and nonlinear models is\noverly simplistic: when considering the concept of binary gender and its\nneutralization, we do not find a single kernel space that exclusively contains\nall the concept-related information. It is therefore challenging to protect\nagainst \\emph{all} nonlinear adversaries at once.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vargas_F/0/1/0/all/0/1\">Francisco Vargas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards a Broad Coverage Named Entity Resource: A Data-Efficient Approach for Many Diverse Languages. (arXiv:2201.12219v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12219","description":"<p>Parallel corpora are ideal for extracting a multilingual named entity (MNE)\nresource, i.e., a dataset of names translated into multiple languages. Prior\nwork on extracting MNE datasets from parallel corpora required resources such\nas large monolingual corpora or word aligners that are unavailable or perform\npoorly for underresourced languages. We present CLC-BN, a new method for\ncreating an MNE resource, and apply it to the Parallel Bible Corpus, a corpus\nof more than 1000 languages. CLC-BN learns a neural transliteration model from\nparallel-corpus statistics, without requiring any other bilingual resources,\nword aligners, or seed data. Experimental results show that CLC-BN clearly\noutperforms prior work. We release an MNE resource for 1340 languages and\ndemonstrate its effectiveness in two downstream tasks: knowledge graph\naugmentation and bilingual lexicon induction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Severini_S/0/1/0/all/0/1\">Silvia Severini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Cooperative Networks for Natural Language Generation. (arXiv:2201.12320v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12320","description":"<p>Generative Adversarial Networks (GANs) have known a tremendous success for\nmany continuous generation tasks, especially in the field of image generation.\nHowever, for discrete outputs such as language, optimizing GANs remains an open\nproblem with many instabilities, as no gradient can be properly back-propagated\nfrom the discriminator output to the generator parameters. An alternative is to\nlearn the generator network via reinforcement learning, using the discriminator\nsignal as a reward, but such a technique suffers from moving rewards and\nvanishing gradient problems. Finally, it often falls short compared to direct\nmaximum-likelihood approaches. In this paper, we introduce Generative\nCooperative Networks, in which the discriminator architecture is cooperatively\nused along with the generation policy to output samples of realistic texts for\nthe task at hand. We give theoretical guarantees of convergence for our\napproach, and study various efficient decoding schemes to empirically achieve\nstate-of-the-art results in two main NLG tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claveau_V/0/1/0/all/0/1\">Vincent Claveau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1\">Ewa Kijak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarizing Differences between Text Distributions with Natural Language. (arXiv:2201.12323v1 [cs.CL])","link":"http://arxiv.org/abs/2201.12323","description":"<p>How do two distributions of texts differ? Humans are slow at answering this,\nsince discovering patterns might require tediously reading through hundreds of\nsamples. We propose to automatically summarize the differences by \"learning a\nnatural language hypothesis\": given two distributions $D_{0}$ and $D_{1}$, we\nsearch for a description that is more often true for $D_{1}$, e.g., \"is\nmilitary-related.\" To tackle this problem, we fine-tune GPT-3 to propose\ndescriptions with the prompt: \"[samples of $D_{0}$] + [samples of $D_{1}$] +\nthe difference between them is _____\". We then re-rank the descriptions by\nchecking how often they hold on a larger set of samples with a learned\nverifier. On a benchmark of 54 real-world binary classification tasks, while\nGPT-3 Curie (13B) only generates a description similar to human annotation 7%\nof the time, the performance reaches 61% with fine-tuning and re-ranking, and\nour best system using GPT-3 Davinci (175B) reaches 76%. We apply our system to\ndescribe distribution shifts, debug dataset shortcuts, summarize unknown tasks,\nand label text clusters, and present analyses based on automatically generated\ndescriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snell_C/0/1/0/all/0/1\">Charlie Snell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Look It Up: Bilingual Dictionaries Improve Neural Machine Translation. (arXiv:2010.05997v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.05997","description":"<p>Despite advances in neural machine translation (NMT) quality, rare words\ncontinue to be problematic. For humans, the solution to the rare-word problem\nhas long been dictionaries, but dictionaries cannot be straightforwardly\nincorporated into NMT. In this paper, we describe a new method for \"attaching\"\ndictionary definitions to rare words so that the network can learn the best way\nto use them. We demonstrate improvements of up to 1.8 BLEU using bilingual\ndictionaries.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1\">Xing Jie Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CNN Encoding of Acoustic Parameters for Prominence Detection. (arXiv:2104.05488v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05488","description":"<p>Expressive reading, considered the defining attribute of oral reading\nfluency, comprises the prosodic realization of phrasing and prominence. In the\ncontext of evaluating oral reading, it helps to establish the speaker's\ncomprehension of the text. We consider a labeled dataset of children's reading\nrecordings for the speaker-independent detection of prominent words using\nacoustic-prosodic and lexico-syntactic features. A previous well-tuned random\nforest ensemble predictor is replaced by an RNN sequence classifier to exploit\npotential context dependency across the longer utterance. Further, deep\nlearning is applied to obtain word-level features from low-level acoustic\ncontours of fundamental frequency, intensity and spectral shape in an\nend-to-end fashion. Performance comparisons are presented across the different\nfeature types and across different feature learning architectures for prominent\nword prediction to draw insights wherever possible.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sabu_K/0/1/0/all/0/1\">Kamini Sabu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vaidya_M/0/1/0/all/0/1\">Mithilesh Vaidya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_P/0/1/0/all/0/1\">Preeti Rao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CEAR: Cross-Entity Aware Reranker for Knowledge Base Completion. (arXiv:2104.08741v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08741","description":"<p>Pre-trained language models (LMs) like BERT have shown to store factual\nknowledge about the world. This knowledge can be used to augment the\ninformation present in Knowledge Bases, which tend to be incomplete. However,\nprior attempts at using BERT for task of Knowledge Base Completion (KBC)\nresulted in performance worse than embedding based techniques that rely only on\nthe graph structure. In this work we develop a novel model, Cross-Entity Aware\nReranker (CEAR), that uses BERT to re-rank the output of existing KBC models\nwith cross-entity attention. Unlike prior work that scores each entity\nindependently, CEAR uses BERT to score the entities together, which is\neffective for exploiting its factual knowledge. CEAR achieves a new state of\nart for the OLPBench dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kolluru_K/0/1/0/all/0/1\">Keshav Kolluru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_M/0/1/0/all/0/1\">Mayank Singh Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nandwani_Y/0/1/0/all/0/1\">Yatin Nandwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singla_P/0/1/0/all/0/1\">Parag Singla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1\">Mausam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Incorporating Word Sense Disambiguation in Neural Language Models. (arXiv:2106.07967v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.07967","description":"<p>We present two supervised (pre-)training methods to incorporate gloss\ndefinitions from lexical resources into neural language models (LMs). The\ntraining improves our models' performance for Word Sense Disambiguation (WSD)\nbut also benefits general language understanding tasks while adding almost no\nparameters. We evaluate our techniques with seven different neural LMs and find\nthat XLNet is more suitable for WSD than BERT. Our best-performing methods\nexceeds state-of-the-art WSD techniques on the SemCor 3.0 dataset by 0.5% F1\nand increase BERT's performance on the GLUE benchmark by 1.1% on average.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahle_J/0/1/0/all/0/1\">Jan Philip Wahle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruas_T/0/1/0/all/0/1\">Terry Ruas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meuschke_N/0/1/0/all/0/1\">Norman Meuschke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-conditioning pre-trained language models. (arXiv:2110.02802v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2110.02802","description":"<p>In this paper we aim to investigate the mechanisms that guide text generation\nwith pre-trained Transformer-based Language Models (TLMs). Grounded on the\nProduct of Experts formulation by Hinton (1999), we describe a generative\nmechanism that exploits expert units which naturally exist in TLMs. Such units\nare responsible for detecting concepts in the input and conditioning text\ngeneration on such concepts. We describe how to identify expert units and how\nto activate them during inference in order to induce any desired concept in the\ngenerated output. We find that the activation of a surprisingly small amount of\nunits is sufficient to steer text generation (as little as 3 units in a model\nwith 345M parameters). While the objective of this work is to learn more about\nhow TLMs work, we show that our method is effective for conditioning without\nfine-tuning or using extra parameters, even on fine-grained homograph concepts.\nAdditionally, we show that our method can be used to correct gender bias\npresent in the output of TLMs and achieves gender parity for all evaluated\ncontexts. We compare our method with FUDGE and PPLM-BoW, and show that our\napproach is able to achieve gender parity at a lower perplexity. The proposed\nmethod is accessible to a wide audience thanks to its simplicity and minimal\ncompute needs. The findings in this paper are a step forward in understanding\nthe generative mechanisms of TLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suau_X/0/1/0/all/0/1\">Xavier Suau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zappella_L/0/1/0/all/0/1\">Luca Zappella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apostoloff_N/0/1/0/all/0/1\">Nicholas Apostoloff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NATURE: Natural Auxiliary Text Utterances for Realistic Spoken Language Evaluation. (arXiv:2111.05196v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2111.05196","description":"<p>Slot-filling and intent detection are the backbone of conversational agents\nsuch as voice assistants, and are active areas of research. Even though\nstate-of-the-art techniques on publicly available benchmarks show impressive\nperformance, their ability to generalize to realistic scenarios is yet to be\ndemonstrated. In this work, we present NATURE, a set of simple spoken-language\noriented transformations, applied to the evaluation set of datasets, to\nintroduce human spoken language variations while preserving the semantics of an\nutterance. We apply NATURE to common slot-filling and intent detection\nbenchmarks and demonstrate that simple perturbations from the standard\nevaluation set by NATURE can deteriorate model performance significantly.\nThrough our experiments we demonstrate that when NATURE operators are applied\nto evaluation set of popular benchmarks the model accuracy can drop by up to\n40%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alfonso_Hermelo_D/0/1/0/all/0/1\">David Alfonso-Hermelo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1\">Ahmad Rashid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaddar_A/0/1/0/all/0/1\">Abbas Ghaddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlais_P/0/1/0/all/0/1\">Philippe Langlais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual Sentiment Analysis. (arXiv:2201.08277v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.08277","description":"<p>Sentiment analysis is one of the most widely studied applications in NLP, but\nmost work focuses on languages with large amounts of data. We introduce the\nfirst large-scale human-annotated Twitter sentiment dataset for the four most\nwidely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and\nYor\\`ub\\'a ) consisting of around 30,000 annotated tweets per language (and\n14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed\ntweets. We propose text collection, filtering, processing and labeling methods\nthat enable us to create datasets for these low-resource languages. We evaluate\na rangeof pre-trained models and transfer strategies on the dataset. We find\nthat language-specific models and language-adaptivefine-tuning generally\nperform best. We release the datasets, trained models, sentiment lexicons, and\ncode to incentivizeresearch on sentiment analysis in under-represented\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_S/0/1/0/all/0/1\">Shamsuddeen Hassan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1\">David Ifeoluwa Adelani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1\">Ibrahim Said Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdulmumin_I/0/1/0/all/0/1\">Idris Abdulmumin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bello_B/0/1/0/all/0/1\">Bello Shehu Bello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Monojit Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1\">Chris Chinenye Emezue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdullahi_S/0/1/0/all/0/1\">Saheed Salahudeen Abdullahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aremu_A/0/1/0/all/0/1\">Anuoluwapo Aremu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeorge_A/0/1/0/all/0/1\">Alipio Jeorge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brazdil_P/0/1/0/all/0/1\">Pavel Brazdil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DiscoScore: Evaluating Text Generation with BERT and Discourse Coherence. (arXiv:2201.11176v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2201.11176","description":"<p>Recently, there has been a growing interest in designing text generation\nsystems from a discourse coherence perspective, e.g., modeling the\ninterdependence between sentences. Still, recent BERT-based evaluation metrics\ncannot recognize coherence and fail to punish incoherent elements in system\noutputs. In this work, we introduce DiscoScore, a parametrized discourse\nmetric, which uses BERT to model discourse coherence from different\nperspectives, driven by Centering theory. Our experiments encompass 16\nnon-discourse and discourse metrics, including DiscoScore and popular coherence\nmodels, evaluated on summarization and document-level machine translation (MT).\nWe find that (i) the majority of BERT-based metrics correlate much worse with\nhuman rated coherence than early discourse metrics, invented a decade ago; (ii)\nthe recent state-of-the-art BARTScore is weak when operated at system level --\nwhich is particularly problematic as systems are typically compared in this\nmanner. DiscoScore, in contrast, achieves strong system-level correlation with\nhuman ratings, not only in coherence but also in factual consistency and other\naspects, and surpasses BARTScore by over 10 correlation points on average.\nFurther, aiming to understand DiscoScore, we provide justifications to the\nimportance of discourse coherence for evaluation metrics, and explain the\nsuperiority of one variant over another. Our code is available at\n\\url{https://github.com/AIPHES/DiscoScore}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strube_M/0/1/0/all/0/1\">Michael Strube</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1\">Steffen Eger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discovering Phonetic Inventories with Crosslingual Automatic Speech Recognition. (arXiv:2201.11207v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2201.11207","description":"<p>The high cost of data acquisition makes Automatic Speech Recognition (ASR)\nmodel training problematic for most existing languages, including languages\nthat do not even have a written script, or for which the phone inventories\nremain unknown. Past works explored multilingual training, transfer learning,\nas well as zero-shot learning in order to build ASR systems for these\nlow-resource languages. While it has been shown that the pooling of resources\nfrom multiple languages is helpful, we have not yet seen a successful\napplication of an ASR model to a language unseen during training. A crucial\nstep in the adaptation of ASR from seen to unseen languages is the creation of\nthe phone inventory of the unseen language. The ultimate goal of our work is to\nbuild the phone inventory of a language unseen during training in an\nunsupervised way without any knowledge about the language. In this paper, we 1)\ninvestigate the influence of different factors (i.e., model architecture,\nphonotactic model, type of speech representation) on phone recognition in an\nunknown language; 2) provide an analysis of which phones transfer well across\nlanguages and which do not in order to understand the limitations of and areas\nfor further improvement for automatic phone inventory creation; and 3) present\ndifferent methods to build a phone inventory of an unseen language in an\nunsupervised way. To that end, we conducted mono-, multi-, and crosslingual\nexperiments on a set of 13 phonetically diverse languages and several in-depth\nanalyses. We found a number of universal phone tokens (IPA symbols) that are\nwell-recognized cross-linguistically. Through a detailed analysis of results,\nwe conclude that unique sounds, similar sounds, and tone languages remain a\nmajor challenge for phonetic inventory discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zelasko_P/0/1/0/all/0/1\">Piotr &#x17b;elasko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Siyuan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Velazquez_L/0/1/0/all/0/1\">Laureano Moro Velazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abavisani_A/0/1/0/all/0/1\">Ali Abavisani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhati_S/0/1/0/all/0/1\">Saurabhchand Bhati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1\">Odette Scharenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_Johnson_M/0/1/0/all/0/1\">Mark Hasegawa-Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dehak_N/0/1/0/all/0/1\">Najim Dehak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/"}},{"title":"cs.CV updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CV","description":"Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Unsupervised Denoising of Retinal OCT with Diffusion Probabilistic Model. (arXiv:2201.11760v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11760","description":"<p>Optical coherence tomography (OCT) is a prevalent non-invasive imaging method\nwhich provides high resolution volumetric visualization of retina. However, its\ninherent defect, the speckle noise, can seriously deteriorate the tissue\nvisibility in OCT. Deep learning based approaches have been widely used for\nimage restoration, but most of these require a noise-free reference image for\nsupervision. In this study, we present a diffusion probabilistic model that is\nfully unsupervised to learn from noise instead of signal. A diffusion process\nis defined by adding a sequence of Gaussian noise to self-fused OCT b-scans.\nThen the reverse process of diffusion, modeled by a Markov chain, provides an\nadjustable level of denoising. Our experiment results demonstrate that our\nmethod can significantly improve the image quality with a simple working\npipeline and a small amount of training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1\">Dewei Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1\">Yuankai K. Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1\">Ipek Oguz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Analysis of Recurrent Learning Algorithms In Neural Lossy Image Compression Systems. (arXiv:2201.11782v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11782","description":"<p>Recent advances in deep learning have resulted in image compression\nalgorithms that outperform JPEG and JPEG 2000 on the standard Kodak benchmark.\nHowever, they are slow to train (due to backprop-through-time) and, to the best\nof our knowledge, have not been systematically evaluated on a large variety of\ndatasets. In this paper, we perform the first large-scale comparison of recent\nstate-of-the-art hybrid neural compression algorithms, while exploring the\neffects of alternative training strategies (when applicable). The hybrid\nrecurrent neural decoder is a former state-of-the-art model (recently overtaken\nby a Google model) that can be trained using backprop-through-time (BPTT) or\nwith alternative algorithms like sparse attentive backtracking (SAB), unbiased\nonline recurrent optimization (UORO), and real-time recurrent learning (RTRL).\nWe compare these training alternatives along with the Google models (GOOG and\nE2E) on 6 benchmark datasets. Surprisingly, we found that the model trained\nwith SAB performs better (outperforming even BPTT), resulting in faster\nconvergence and a better peak signal-to-noise ratio.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1\">Daniel Kifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giles_L/0/1/0/all/0/1\">Lee Giles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Denoising Diffusion Restoration Models. (arXiv:2201.11793v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11793","description":"<p>Many interesting tasks in image restoration can be cast as linear inverse\nproblems. A recent family of approaches for solving these problems uses\nstochastic algorithms that sample from the posterior distribution of natural\nimages given the measurements. However, efficient solutions often require\nproblem-specific supervised training to model the posterior, whereas\nunsupervised methods that are not problem-specific typically rely on\ninefficient iterative methods. This work addresses these issues by introducing\nDenoising Diffusion Restoration Models (DDRM), an efficient, unsupervised\nposterior sampling method. Motivated by variational inference, DDRM takes\nadvantage of a pre-trained denoising diffusion generative model for solving any\nlinear inverse problem. We demonstrate DDRM's versatility on several image\ndatasets for super-resolution, deblurring, inpainting, and colorization under\nvarious amounts of measurement noise. DDRM outperforms the current leading\nunsupervised methods on the diverse ImageNet dataset in reconstruction quality,\nperceptual quality, and runtime, being 5x faster than the nearest competitor.\nDDRM also generalizes well for natural images out of the distribution of the\nobserved ImageNet training set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Kawar_B/0/1/0/all/0/1\">Bahjat Kawar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Elad_M/0/1/0/all/0/1\">Michael Elad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ermon_S/0/1/0/all/0/1\">Stefano Ermon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_J/0/1/0/all/0/1\">Jiaming Song</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Visual Transfer Learning using Knowledge Graphs. (arXiv:2201.11794v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11794","description":"<p>Recent approaches of computer vision utilize deep learning methods as they\nperform quite well if training and testing domains follow the same underlying\ndata distribution. However, it has been shown that minor variations in the\nimages that occur when using these methods in the real world can lead to\nunpredictable errors. Transfer learning is the area of machine learning that\ntries to prevent these errors. Especially, approaches that augment image data\nusing auxiliary knowledge encoded in language embeddings or knowledge graphs\n(KGs) have achieved promising results in recent years. This survey focuses on\nvisual transfer learning approaches using KGs. KGs can represent auxiliary\nknowledge either in an underlying graph-structured schema or in a vector-based\nknowledge graph embedding. Intending to enable the reader to solve visual\ntransfer learning problems with the help of specific KG-DL configurations we\nstart with a description of relevant modeling structures of a KG of various\nexpressions, such as directed labeled graphs, hypergraphs, and hyper-relational\ngraphs. We explain the notion of feature extractor, while specifically\nreferring to visual and semantic features. We provide a broad overview of\nknowledge graph embedding methods and describe several joint training\nobjectives suitable to combine them with high dimensional visual embeddings.\nThe main section introduces four different categories on how a KG can be\ncombined with a DL pipeline: 1) Knowledge Graph as a Reviewer; 2) Knowledge\nGraph as a Trainee; 3) Knowledge Graph as a Trainer; and 4) Knowledge Graph as\na Peer. To help researchers find evaluation benchmarks, we provide an overview\nof generic KGs and a set of image processing datasets and benchmarks including\nvarious types of auxiliary knowledge. Last, we summarize related surveys and\ngive an outlook about challenges and open issues for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Monka_S/0/1/0/all/0/1\">Sebastian Monka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halilaj_L/0/1/0/all/0/1\">Lavdim Halilaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rettinger_A/0/1/0/all/0/1\">Achim Rettinger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural JPEG: End-to-End Image Compression Leveraging a Standard JPEG Encoder-Decoder. (arXiv:2201.11795v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11795","description":"<p>Recent advances in deep learning have led to superhuman performance across a\nvariety of applications. Recently, these methods have been successfully\nemployed to improve the rate-distortion performance in the task of image\ncompression. However, current methods either use additional post-processing\nblocks on the decoder end to improve compression or propose an end-to-end\ncompression scheme based on heuristics. For the majority of these, the trained\ndeep neural networks (DNNs) are not compatible with standard encoders and would\nbe difficult to deply on personal computers and cellphones. In light of this,\nwe propose a system that learns to improve the encoding performance by\nenhancing its internal neural representations on both the encoder and decoder\nends, an approach we call Neural JPEG. We propose frequency domain pre-editing\nand post-editing methods to optimize the distribution of the DCT coefficients\nat both encoder and decoder ends in order to improve the standard compression\n(JPEG) method. Moreover, we design and integrate a scheme for jointly learning\nquantization tables within this hybrid neural compression framework.Experiments\ndemonstrate that our approach successfully improves the rate-distortion\nperformance over JPEG across various quality metrics, such as PSNR and MS-SSIM,\nand generates visually appealing images with better color retention quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Mali_A/0/1/0/all/0/1\">Ankur Mali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kifer_D/0/1/0/all/0/1\">Daniel Kifer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Giles_L/0/1/0/all/0/1\">Lee Giles</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks. (arXiv:2201.11808v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11808","description":"<p>Despite the state-of-the-art performance of deep convolutional neural\nnetworks, they are susceptible to bias and malfunction in unseen situations.\nThe complex computation behind their reasoning is not sufficiently\nhuman-understandable to develop trust. External explainer methods have tried to\ninterpret the network decisions in a human-understandable way, but they are\naccused of fallacies due to their assumptions and simplifications. On the other\nside, the inherent self-interpretability of models, while being more robust to\nthe mentioned fallacies, cannot be applied to the already trained models. In\nthis work, we propose a new attention-based pooling layer, called Local\nAttention Pooling (LAP), that accomplishes self-interpretability and the\npossibility for knowledge injection while improving the model's performance.\nMoreover, several weakly-supervised knowledge injection methodologies are\nprovided to enhance the process of training. We verified our claims by\nevaluating several LAP-extended models on three different datasets, including\nImagenet. The proposed framework offers more valid human-understandable and\nmore faithful-to-the-model interpretations than the commonly used white-box\nexplainer methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Modegh_R/0/1/0/all/0/1\">Rassa Ghavami Modegh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimi_A/0/1/0/all/0/1\">Ahmad Salimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rabiee_H/0/1/0/all/0/1\">Hamid R. Rabiee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Transfer Learning and Optimized CNN Based Intrusion Detection System for Internet of Vehicles. (arXiv:2201.11812v1 [cs.CR])","link":"http://arxiv.org/abs/2201.11812","description":"<p>Modern vehicles, including autonomous vehicles and connected vehicles, are\nincreasingly connected to the external world, which enables various\nfunctionalities and services. However, the improving connectivity also\nincreases the attack surfaces of the Internet of Vehicles (IoV), causing its\nvulnerabilities to cyber-threats. Due to the lack of authentication and\nencryption procedures in vehicular networks, Intrusion Detection Systems (IDSs)\nare essential approaches to protect modern vehicle systems from network\nattacks. In this paper, a transfer learning and ensemble learning-based IDS is\nproposed for IoV systems using convolutional neural networks (CNNs) and\nhyper-parameter optimization techniques. In the experiments, the proposed IDS\nhas demonstrated over 99.25% detection rates and F1-scores on two well-known\npublic benchmark IoV security datasets: the Car-Hacking dataset and the\nCICIDS2017 dataset. This shows the effectiveness of the proposed IDS for\ncyber-attack detection in both intra-vehicle and external vehicular networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Li Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shami_A/0/1/0/all/0/1\">Abdallah Shami</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pressure Eye: In-bed Contact Pressure Estimation via Contact-less Imaging. (arXiv:2201.11828v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11828","description":"<p>Computer vision has achieved great success in interpreting semantic meanings\nfrom images, yet estimating underlying (non-visual) physical properties of an\nobject is often limited to their bulk values rather than reconstructing a dense\nmap. In this work, we present our pressure eye (PEye) approach to estimate\ncontact pressure between a human body and the surface she is lying on with high\nresolution from vision signals directly. PEye approach could ultimately enable\nthe prediction and early detection of pressure ulcers in bed-bound patients,\nthat currently depends on the use of expensive pressure mats. Our PEye network\nis configured in a dual encoding shared decoding form to fuse visual cues and\nsome relevant physical parameters in order to reconstruct high resolution\npressure maps (PMs). We also present a pixel-wise resampling approach based on\nNaive Bayes assumption to further enhance the PM regression performance. A\npercentage of correct sensing (PCS) tailored for sensing estimation accuracy\nevaluation is also proposed which provides another perspective for performance\nevaluation under varying error tolerances. We tested our approach via a series\nof extensive experiments using multimodal sensing technologies to collect data\nfrom 102 subjects while lying on a bed. The individual's high resolution\ncontact pressure data could be estimated from their RGB or long wavelength\ninfrared (LWIR) images with 91.8% and 91.2% estimation accuracies in\n$PCS_{efs0.1}$ criteria, superior to state-of-the-art methods in the related\nimage regression/translation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shuangjun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1\">Sarah Ostadabbas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discriminative Supervised Subspace Learning for Cross-modal Retrieval. (arXiv:2201.11843v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11843","description":"<p>Nowadays the measure between heterogeneous data is still an open problem for\ncross-modal retrieval. The core of cross-modal retrieval is how to measure the\nsimilarity between different types of data. Many approaches have been developed\nto solve the problem. As one of the mainstream, approaches based on subspace\nlearning pay attention to learning a common subspace where the similarity among\nmulti-modal data can be measured directly. However, many of the existing\napproaches only focus on learning a latent subspace. They ignore the full use\nof discriminative information so that the semantically structural information\nis not well preserved. Therefore satisfactory results can not be achieved as\nexpected. We in this paper propose a discriminative supervised subspace\nlearning for cross-modal retrieval(DS2L), to make full use of discriminative\ninformation and better preserve the semantically structural information.\nSpecifically, we first construct a shared semantic graph to preserve the\nsemantic structure within each modality. Subsequently, the Hilbert-Schmidt\nIndependence Criterion(HSIC) is introduced to preserve the consistence between\nfeature-similarity and semantic-similarity of samples. Thirdly, we introduce a\nsimilarity preservation term, thus our model can compensate for the\nshortcomings of insufficient use of discriminative data and better preserve the\nsemantically structural information within each modality. The experimental\nresults obtained on three well-known benchmark datasets demonstrate the\neffectiveness and competitiveness of the proposed method against the compared\nclassic subspace learning approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiao-Jun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donglin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speckle-based optical cryptosystem and its application for human face recognition via deep learning. (arXiv:2201.11844v1 [cs.CR])","link":"http://arxiv.org/abs/2201.11844","description":"<p>Face recognition has recently become ubiquitous in many scenes for\nauthentication or security purposes. Meanwhile, there are increasing concerns\nabout the privacy of face images, which are sensitive biometric data that\nshould be carefully protected. Software-based cryptosystems are widely adopted\nnowadays to encrypt face images, but the security level is limited by\ninsufficient digital secret key length or computing power. Hardware-based\noptical cryptosystems can generate enormously longer secret keys and enable\nencryption at light speed, but most reported optical methods, such as double\nrandom phase encryption, are less compatible with other systems due to system\ncomplexity. In this study, a plain yet high-efficient speckle-based optical\ncryptosystem is proposed and implemented. A scattering ground glass is\nexploited to generate physical secret keys of gigabit length and encrypt face\nimages via seemingly random optical speckles at light speed. Face images can\nthen be decrypted from the random speckles by a well-trained decryption neural\nnetwork, such that face recognition can be realized with up to 98% accuracy.\nThe proposed cryptosystem has wide applicability, and it may open a new avenue\nfor high-security complex information encryption and decryption by utilizing\noptical speckles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huanhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhipeng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woo_C/0/1/0/all/0/1\">Chi Man Woo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1\">Tianting Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shengfu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yuanjin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Honglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Jie Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_P/0/1/0/all/0/1\">Puxiang Lai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards an Automatic Diagnosis of Peripheral and Central Palsy Using Machine Learning on Facial Features. (arXiv:2201.11852v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11852","description":"<p>Central palsy is a form of facial paralysis that requires urgent medical\nattention and has to be differentiated from other, similar conditions such as\nperipheral palsy. To aid in fast and accurate diagnosis of this condition, we\npropose a machine learning approach to automatically classify peripheral and\ncentral facial palsy. The Palda dataset is used, which contains 103 peripheral\npalsy images, 40 central palsy, and 60 healthy people. Experiments are run on\nfive machine learning algorithms. The best performing algorithms were found to\nbe the SVM (total accuracy of 85.1%) and the Gaussian naive Bayes (80.7%). The\nlowest false negative rate on central palsy was achieved by the naive Bayes\napproach (80% compared to 70%). This condition could prove to be the most\nsevere, and thus its sensitivity is another good way to compare algorithms. By\nextrapolation, a dataset size of 334 total pictures is estimated to achieve a\ncentral palsy sensitivity of 95%. All code used for these machine learning\nexperiments is freely available online at https://github.com/cvvletter/palsy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vletter_C/0/1/0/all/0/1\">C.V. Vletter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burger_H/0/1/0/all/0/1\">H.L. Burger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alers_H/0/1/0/all/0/1\">H. Alers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sourlos_N/0/1/0/all/0/1\">N. Sourlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Ars_Z/0/1/0/all/0/1\">Z. Al-Ars</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using Shape Metrics to Describe 2D Data Points. (arXiv:2201.11857v1 [cs.LG])","link":"http://arxiv.org/abs/2201.11857","description":"<p>Traditional machine learning (ML) algorithms, such as multiple regression,\nrequire human analysts to make decisions on how to treat the data. These\ndecisions can make the model building process subjective and difficult to\nreplicate for those who did not build the model. Deep learning approaches\nbenefit by allowing the model to learn what features are important once the\nhuman analyst builds the architecture. Thus, a method for automating certain\nhuman decisions for traditional ML modeling would help to improve the\nreproducibility and remove subjective aspects of the model building process. To\nthat end, we propose to use shape metrics to describe 2D data to help make\nanalyses more explainable and interpretable. The proposed approach provides a\nfoundation to help automate various aspects of model building in an\ninterpretable and explainable fashion. This is particularly important in\napplications in the medical community where the `right to explainability' is\ncrucial. We provide various simulated data sets ranging from probability\ndistributions, functions, and model quality control checks (such as QQ-Plots\nand residual analyses from ordinary least squares) to showcase the breadth of\nthis approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lamberti_W/0/1/0/all/0/1\">William Franz Lamberti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification of White Blood Cell Leukemia with Low Number of Interpretable and Explainable Features. (arXiv:2201.11864v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11864","description":"<p>White Blood Cell (WBC) Leukaemia is detected through image-based\nclassification. Convolutional Neural Networks are used to learn the features\nneeded to classify images of cells a malignant or healthy. However, this type\nof model requires learning a large number of parameters and is difficult to\ninterpret and explain. Explainable AI (XAI) attempts to alleviate this issue by\nproviding insights to how models make decisions. Therefore, we present an XAI\nmodel which uses only 24 explainable and interpretable features and is highly\ncompetitive to other approaches by outperforming them by about 4.38\\%. Further,\nour approach provides insight into which variables are the most important for\nthe classification of the cells. This insight provides evidence that when labs\ntreat the WBCs differently, the importance of various metrics changes\nsubstantially. Understanding the important features for classification is vital\nin medical imaging diagnosis and, by extension, understanding the AI models\nbuilt in scientific pursuits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Lamberti_W/0/1/0/all/0/1\">William Franz Lamberti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Calibrating Histopathology Image Classifiers using Label Smoothing. (arXiv:2201.11866v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11866","description":"<p>The classification of histopathology images fundamentally differs from\ntraditional image classification tasks because histopathology images naturally\nexhibit a range of diagnostic features, resulting in a diverse range of\nannotator agreement levels. However, examples with high annotator disagreement\nare often either assigned the majority label or discarded entirely when\ntraining histopathology image classifiers. This widespread practice often\nyields classifiers that do not account for example difficulty and exhibit poor\nmodel calibration. In this paper, we ask: can we improve model calibration by\nendowing histopathology image classifiers with inductive biases about example\ndifficulty?\n</p>\n<p>We propose several label smoothing methods that utilize per-image annotator\nagreement. Though our methods are simple, we find that they substantially\nimprove model calibration, while maintaining (or even improving) accuracy. For\ncolorectal polyp classification, a common yet challenging task in\ngastrointestinal pathology, we find that our proposed agreement-aware label\nsmoothing methods reduce calibration error by almost 70%. Moreover, we find\nthat using model confidence as a proxy for annotator agreement also improves\ncalibration and accuracy, suggesting that datasets without multiple annotators\ncan still benefit from our proposed label smoothing methods via our proposed\nconfidence-aware label smoothing methods.\n</p>\n<p>Given the importance of calibration (especially in histopathology image\nanalysis), the improvements from our proposed techniques merit further\nexploration and potential implementation in other histopathology image\nclassification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jerry Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Torresani_L/0/1/0/all/0/1\">Lorenzo Torresani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hassanpour_S/0/1/0/all/0/1\">Saeed Hassanpour</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Infrastructure-Based Object Detection and Tracking for Cooperative Driving Automation: A Survey. (arXiv:2201.11871v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11871","description":"<p>Object detection plays a fundamental role in enabling Cooperative Driving\nAutomation (CDA), which is regarded as the revolutionary solution to addressing\nsafety, mobility, and sustainability issues of contemporary transportation\nsystems. Although current computer vision technologies could provide\nsatisfactory object detection results in occlusion-free scenarios, the\nperception performance of onboard sensors could be inevitably limited by the\nrange and occlusion. Owing to flexible position and pose for sensor\ninstallation, infrastructure-based detection and tracking systems can enhance\nthe perception capability for connected vehicles and thus quickly become one of\nthe most popular research topics. In this paper, we review the research\nprogress for infrastructure-based object detection and tracking systems.\nArchitectures of roadside perception systems based on different types of\nsensors are reviewed to show a high-level description of the workflows for\ninfrastructure-based perception systems. Roadside sensors and different\nperception methodologies are reviewed and analyzed with detailed literature to\nprovide a low-level explanation for specific methods followed by Datasets and\nSimulators to draw an overall landscape of infrastructure-based object\ndetection and tracking methods. Discussions are conducted to point out current\nopportunities, open problems, and anticipated future trends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1\">Zhengwei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guoyuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xuewei Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yongkang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oguchi_K/0/1/0/all/0/1\">Kentaro Oguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barth_M/0/1/0/all/0/1\">Matthew J. Barth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Indicative Image Retrieval: Turning Blackbox Learning into Grey. (arXiv:2201.11898v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11898","description":"<p>Deep learning became the game changer for image retrieval soon after it was\nintroduced. It promotes the feature extraction (by representation learning) as\nthe core of image retrieval, with the relevance/matching evaluation being\ndegenerated into simple similarity metrics. In many applications, we need the\nmatching evidence to be indicated rather than just have the ranked list (e.g.,\nthe locations of the target proteins/cells/lesions in medical images). It is\nlike the matched words need to be highlighted in search engines. However, this\nis not easy to implement without explicit relevance/matching modeling. The deep\nrepresentation learning models are not feasible because of their blackbox\nnature. In this paper, we revisit the importance of relevance/matching modeling\nin deep learning era with an indicative retrieval setting. The study shows that\nit is possible to skip the representation learning and model the matching\nevidence directly. By removing the dependency on the pre-trained models, it has\navoided a lot of related issues (e.g., the domain gap between classification\nand retrieval, the detail-diffusion caused by convolution, and so on). More\nimportantly, the study demonstrates that the matching can be explicitly modeled\nand backtracked later for generating the matching evidence indications. It can\nimprove the explainability of deep inference. Our method obtains a best\nperformance in literature on both Oxford-5k and Paris-6k, and sets a new record\nof 97.77% on Oxford-5k (97.81% on Paris-6k) without extracting any deep\nfeatures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xulu Zhang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenqun Yang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xiaoyong Wei</a> (1 and 3) ((1) Sichuan University, (2) Chinese University of Hong Kong, (3) Hong Kong Polytechnic Univeristy)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stereo Matching with Cost Volume based Sparse Disparity Propagation. (arXiv:2201.11937v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11937","description":"<p>Stereo matching is crucial for binocular stereo vision. Existing methods\nmainly focus on simple disparity map fusion to improve stereo matching, which\nrequire multiple dense or sparse disparity maps. In this paper, we propose a\nsimple yet novel scheme, termed feature disparity propagation, to improve\ngeneral stereo matching based on matching cost volume and sparse matching\nfeature points. Specifically, our scheme first calculates a reliable sparse\ndisparity map by local feature matching, and then refines the disparity map by\npropagating reliable disparities to neighboring pixels in the matching cost\ndomain. In addition, considering the gradient and multi-scale information of\nlocal disparity regions, we present a $\\rho$-Census cost measure based on the\nwell-known AD-Census, which guarantees the robustness of cost volume even\nwithout the cost aggregation step. Extensive experiments on Middlebury stereo\nbenchmark V3 demonstrate that our scheme achieves promising performance\ncomparable to state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wei Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xiaojiang Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DICP: Doppler Iterative Closest Point Algorithm. (arXiv:2201.11944v1 [cs.RO])","link":"http://arxiv.org/abs/2201.11944","description":"<p>In this paper, we present a novel algorithm for point cloud registration for\nrange sensors capable of measuring per-return instantaneous radial velocity:\nDoppler ICP. Existing variants of ICP that solely rely on geometry or other\nfeatures generally fail to estimate the motion of the sensor correctly in\nscenarios that have non-distinctive features and/or repetitive geometric\nstructures such as hallways, tunnels, highways, and bridges. We propose a new\nDoppler velocity objective function that exploits the compatibility of each\npoint's Doppler measurement and the sensor's current motion estimate. We\njointly optimize the Doppler velocity objective function and the geometric\nobjective function which sufficiently constrains the point cloud alignment\nproblem even in feature-denied environments. Furthermore, the correspondence\nmatches used for the alignment are improved by pruning away the points from\ndynamic targets which generally degrade the ICP solution. We evaluate our\nmethod on data collected from real sensors and from simulation. Our results\nshow a significant performance improvement in terms of the registration\naccuracy with the added benefit of faster convergence guided by the Doppler\nvelocity gradients.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hexsel_B/0/1/0/all/0/1\">Bruno Hexsel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vhavle_H/0/1/0/all/0/1\">Heethesh Vhavle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Shuffle Augmentation of Features from Unlabeled Data for Unsupervised Domain Adaptation. (arXiv:2201.11963v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11963","description":"<p>Unsupervised Domain Adaptation (UDA), a branch of transfer learning where\nlabels for target samples are unavailable, has been widely researched and\ndeveloped in recent years with the help of adversarially trained models.\nAlthough existing UDA algorithms are able to guide neural networks to extract\ntransferable and discriminative features, classifiers are merely trained under\nthe supervision of labeled source data. Given the inevitable discrepancy\nbetween source and target domains, the classifiers can hardly be aware of the\ntarget classification boundaries. In this paper, Shuffle Augmentation of\nFeatures (SAF), a novel UDA framework, is proposed to address the problem by\nproviding the classifier with supervisory signals from target feature\nrepresentations. SAF learns from the target samples, adaptively distills\nclass-aware target features, and implicitly guides the classifier to find\ncomprehensive class borders. Demonstrated by extensive experiments, the SAF\nmodule can be integrated into any existing adversarial UDA models to achieve\nperformance improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Changwei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoran Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1\">Han Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Cheng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianshuo Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalized Visual Quality Assessment of GAN-Generated Face Images. (arXiv:2201.11975v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11975","description":"<p>Recent years have witnessed the dramatically increased interest in face\ngeneration with generative adversarial networks (GANs). A number of successful\nGAN algorithms have been developed to produce vivid face images towards\ndifferent application scenarios. However, little work has been dedicated to\nautomatic quality assessment of such GAN-generated face images (GFIs), even\nless have been devoted to generalized and robust quality assessment of GFIs\ngenerated with unseen GAN model. Herein, we make the first attempt to study the\nsubjective and objective quality towards generalized quality assessment of\nGFIs. More specifically, we establish a large-scale database consisting of GFIs\nfrom four GAN algorithms, the pseudo labels from image quality assessment (IQA)\nmeasures, as well as the human opinion scores via subjective testing.\nSubsequently, we develop a quality assessment model that is able to deliver\naccurate quality predictions for GFIs from both available and unseen GAN\nalgorithms based on meta-learning. In particular, to learn shared knowledge\nfrom GFIs pairs that are born of limited GAN algorithms, we develop the\nconvolutional block attention (CBA) and facial attributes-based analysis (ABA)\nmodules, ensuring that the learned knowledge tends to be consistent with human\nvisual perception. Extensive experiments exhibit that the proposed model\nachieves better performance compared with the state-of-the-art IQA models, and\nis capable of retaining the effectiveness when evaluating GFIs from the unseen\nGAN algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_Z/0/1/0/all/0/1\">Zhangkai Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Baoliang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanli Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Computer-aided Recognition and Assessment of a Porous Bioelastomer on Ultrasound Images for Regenerative Medicine Applications. (arXiv:2201.11987v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11987","description":"<p>Biodegradable elastic scaffolds have attracted more and more attention in the\nfield of soft tissue repair and tissue engineering. These scaffolds made of\nporous bioelastomers support tissue ingrowth along with their own degradation.\nIt is necessary to develop a computer-aided analyzing method based on\nultrasound images to identify the degradation performance of the scaffold, not\nonly to obviate the need to do destructive testing, but also to monitor the\nscaffold's degradation and tissue ingrowth over time. It is difficult using a\nsingle traditional image processing algorithm to extract continuous and\naccurate contour of a porous bioelastomer. This paper proposes a joint\nalgorithm for the bioelastomer's contour detection and a texture feature\nextraction method for monitoring the degradation behavior of the bioelastomer.\nMean-shift clustering method is used to obtain the bioelastomer's and native\ntissue's clustering feature information. Then the OTSU image binarization\nmethod automatically selects the optimal threshold value to convert the\ngrayscale ultrasound image into a binary image. The Canny edge detector is used\nto extract the complete bioelastomer's contour. The first-order and\nsecond-order statistical features of texture are extracted. The proposed joint\nalgorithm not only achieves the ideal extraction of the bioelastomer's contours\nin ultrasound images, but also gives valuable feedback of the degradation\nbehavior of the bioelastomer at the implant site based on the changes of\ntexture characteristics and contour area. The preliminary results of this study\nsuggest that the proposed computer-aided image processing techniques have\nvalues and potentials in the non-invasive analysis of tissue scaffolds in vivo\nbased on ultrasound images and may help tissue engineers evaluate the tissue\nscaffold's degradation and cellular ingrowth progress and improve the scaffold\ndesigns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Dun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_K/0/1/0/all/0/1\">Kaixuan Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1\">Yanying Zhu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jia Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dreglea_A/0/1/0/all/0/1\">Aliona Dreglea</a>, <a href=\"http://arxiv.org/find/eess/1/au:+You_Z/0/1/0/all/0/1\">Zhengwei You</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jiao Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hybrid Contrastive Learning with Cluster Ensemble for Unsupervised Person Re-identification. (arXiv:2201.11995v1 [cs.CV])","link":"http://arxiv.org/abs/2201.11995","description":"<p>Unsupervised person re-identification (ReID) aims to match a query image of a\npedestrian to the images in gallery set without supervision labels. The most\npopular approaches to tackle unsupervised person ReID are usually performing a\nclustering algorithm to yield pseudo labels at first and then exploit the\npseudo labels to train a deep neural network. However, the pseudo labels are\nnoisy and sensitive to the hyper-parameter(s) in clustering algorithm. In this\npaper, we propose a Hybrid Contrastive Learning (HCL) approach for unsupervised\nperson ReID, which is based on a hybrid between instance-level and\ncluster-level contrastive loss functions. Moreover, we present a\nMulti-Granularity Clustering Ensemble based Hybrid Contrastive Learning\n(MGCE-HCL) approach, which adopts a multi-granularity clustering ensemble\nstrategy to mine priority information among the pseudo positive sample pairs\nand defines a priority-weighted hybrid contrastive loss for better tolerating\nthe noises in the pseudo positive samples. We conduct extensive experiments on\ntwo benchmark datasets Market-1501 and DukeMTMC-reID. Experimental results\nvalidate the effectiveness of our proposals.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">He Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingkun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chun-Guang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Networks for Image and Video Super-Resolution. (arXiv:2201.11996v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11996","description":"<p>Efficiency of gradient propagation in intermediate layers of convolutional\nneural networks is of key importance for super-resolution task. To this end, we\npropose a deep architecture for single image super-resolution (SISR), which is\nbuilt using efficient convolutional units we refer to as mixed-dense connection\nblocks (MDCB). The design of MDCB combines the strengths of both residual and\ndense connection strategies, while overcoming their limitations. To enable\nsuper-resolution for multiple factors, we propose a scale-recurrent framework\nwhich reutilizes the filters learnt for lower scale factors recursively for\nhigher factors. This leads to improved performance and promotes parametric\nefficiency for higher factors. We train two versions of our network to enhance\ncomplementary image qualities using different loss configurations. We further\nemploy our network for video super-resolution task, where our network learns to\naggregate information from multiple frames and maintain spatio-temporal\nconsistency. The proposed networks lead to qualitative and quantitative\nimprovements over state-of-the-art techniques on image and video\nsuper-resolution benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandal_S/0/1/0/all/0/1\">Srimanta Mandal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Image Superresolution using Scale-Recurrent Dense Network. (arXiv:2201.11998v1 [eess.IV])","link":"http://arxiv.org/abs/2201.11998","description":"<p>Recent advances in the design of convolutional neural network (CNN) have\nyielded significant improvements in the performance of image super-resolution\n(SR). The boost in performance can be attributed to the presence of residual or\ndense connections within the intermediate layers of these networks. The\nefficient combination of such connections can reduce the number of parameters\ndrastically while maintaining the restoration quality. In this paper, we\npropose a scale recurrent SR architecture built upon units containing series of\ndense connections within a residual block (Residual Dense Blocks (RDBs)) that\nallow extraction of abundant local features from the image. Our scale recurrent\ndesign delivers competitive performance for higher scale factors while being\nparametrically more efficient as compared to current state-of-the-art\napproaches. To further improve the performance of our network, we employ\nmultiple residual connections in intermediate layers (referred to as\nMulti-Residual Dense Blocks), which improves gradient propagation in existing\nlayers. Recent works have discovered that conventional loss functions can guide\na network to produce results which have high PSNRs but are perceptually\ninferior. We mitigate this issue by utilizing a Generative Adversarial Network\n(GAN) based framework and deep feature (VGG) losses to train our network. We\nexperimentally demonstrate that different weighted combinations of the VGG loss\nand the adversarial loss enable our network outputs to traverse along the\nperception-distortion curve. The proposed networks perform favorably against\nexisting methods, both perceptually and objectively (PSNR-based) with fewer\nparameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mandal_S/0/1/0/all/0/1\">Srimanta Mandal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unfolding a blurred image. (arXiv:2201.12010v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12010","description":"<p>We present a solution for the goal of extracting a video from a single motion\nblurred image to sequentially reconstruct the clear views of a scene as beheld\nby the camera during the time of exposure. We first learn motion representation\nfrom sharp videos in an unsupervised manner through training of a convolutional\nrecurrent video autoencoder network that performs a surrogate task of video\nreconstruction. Once trained, it is employed for guided training of a motion\nencoder for blurred images. This network extracts embedded motion information\nfrom the blurred image to generate a sharp video in conjunction with the\ntrained recurrent video decoder. As an intermediate step, we also design an\nefficient architecture that enables real-time single image deblurring and\noutperforms competing methods across all factors: accuracy, speed, and\ncompactness. Experiments on real scenes and standard datasets demonstrate the\nsuperiority of our framework over the state-of-the-art and its ability to\ngenerate a plausible sequence of temporally consistent sharp frames.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Purohit_K/0/1/0/all/0/1\">Kuldeep Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1\">Anshul Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajagopalan_A/0/1/0/all/0/1\">A. N. Rajagopalan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RGB-D SLAM Using Attention Guided Frame Association. (arXiv:2201.12047v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12047","description":"<p>Deep learning models as an emerging topic have shown great progress in\nvarious fields. Especially, visualization tools such as class activation\nmapping methods provided visual explanation on the reasoning of convolutional\nneural networks (CNNs). By using the gradients of the network layers, it is\npossible to demonstrate where the networks pay attention during a specific\nimage recognition task. Moreover, these gradients can be integrated with CNN\nfeatures for localizing more generalized task dependent attentive (salient)\nobjects in scenes. Despite this progress, there is not much explicit usage of\nthis gradient (network attention) information to integrate with CNN\nrepresentations for object semantics. This can be very useful for visual tasks\nsuch as simultaneous localization and mapping (SLAM) where CNN representations\nof spatially attentive object locations may lead to improved performance.\nTherefore, in this work, we propose the use of task specific network attention\nfor RGB-D indoor SLAM. To do so, we integrate layer-wise object attention\ninformation (layer gradients) with CNN layer representations to improve frame\nassociation performance in a state-of-the-art RGB-D indoor SLAM method.\nExperiments show promising initial results with improved performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Caglayan_A/0/1/0/all/0/1\">Ali Caglayan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imamoglu_N/0/1/0/all/0/1\">Nevrez Imamoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guclu_O/0/1/0/all/0/1\">Oguzhan Guclu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serhatoglu_A/0/1/0/all/0/1\">Ali Osman Serhatoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weimin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Can_A/0/1/0/all/0/1\">Ahmet Burak Can</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakamura_R/0/1/0/all/0/1\">Ryosuke Nakamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detection of fake faces in videos. (arXiv:2201.12051v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12051","description":"<p>: Deep learning methodologies have been used to create applications that can\ncause threats to privacy, democracy and national security and could be used to\nfurther amplify malicious activities. One of those deep learning-powered\napplications in recent times is synthesized videos of famous personalities.\nAccording to Forbes, Generative Adversarial Networks(GANs) generated fake\nvideos growing exponentially every year and the organization known as Deeptrace\nhad estimated an increase of deepfakes by 84% from the year 2018 to 2019. They\nare used to generate and modify human faces, where most of the existing fake\nvideos are of prurient non-consensual nature, of which its estimates to be\naround 96% and some carried out impersonating personalities for cyber crime. In\nthis paper, available video datasets are identified and a pretrained model\nBlazeFace is used to detect faces, and a ResNet and Xception ensembled\narchitectured neural network trained on the dataset to achieve the goal of\ndetection of fake faces in videos. The model is optimized over a loss value and\nlog loss values and evaluated over its F1 score. Over a sample of data, it is\nobserved that focal loss provides better accuracy, F1 score and loss as the\ngamma of the focal loss becomes a hyper parameter. This provides a k-folded\naccuracy of around 91% at its peak in a training cycle with the real world\naccuracy subjected to change over time as the model decays.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shamanth_M/0/1/0/all/0/1\">M. Shamanth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathias_R/0/1/0/all/0/1\">Russel Mathias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MN_D/0/1/0/all/0/1\">Dr Vijayalakshmi MN</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"You Only Cut Once: Boosting Data Augmentation with a Single Cut. (arXiv:2201.12078v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12078","description":"<p>We present You Only Cut Once (YOCO) for performing data augmentations. YOCO\ncuts one image into two pieces and performs data augmentations individually\nwithin each piece. Applying YOCO improves the diversity of the augmentation per\nsample and encourages neural networks to recognize objects from partial\ninformation. YOCO enjoys the properties of parameter-free, easy usage, and\nboosting almost all augmentations for free. Thorough experiments are conducted\nto evaluate its effectiveness. We first demonstrate that YOCO can be seamlessly\napplied to varying data augmentations, neural network architectures, and brings\nperformance gains on CIFAR and ImageNet classification tasks, sometimes\nsurpassing conventional image-level augmentation by large margins. Moreover, we\nshow YOCO benefits contrastive pre-training toward a more powerful\nrepresentation that can be better transferred to multiple downstream tasks.\nFinally, we study a number of variants of YOCO and empirically analyze the\nperformance for respective settings. Code is available at GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Junlin Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengfei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weihao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1\">Jie Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1\">Ian Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongdong Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DynaMixer: A Vision MLP Architecture with Dynamic Mixing. (arXiv:2201.12083v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12083","description":"<p>Recently, MLP-like vision models have achieved promising performances on\nmainstream visual recognition tasks. In contrast with vision transformers and\nCNNs, the success of MLP-like models shows that simple information fusion\noperations among tokens and channels can yield a good representation power for\ndeep recognition models. However, existing MLP-like models fuse tokens through\nstatic fusion operations, lacking adaptability to the contents of the tokens to\nbe mixed. Thus, customary information fusion procedures are not effective\nenough. To this end, this paper presents an efficient MLP-like network\narchitecture, dubbed DynaMixer, resorting to dynamic information fusion.\nCritically, we propose a procedure, on which the DynaMixer model relies, to\ndynamically generate mixing matrices by leveraging the contents of all the\ntokens to be mixed. To reduce the time complexity and improve the robustness, a\ndimensionality reduction technique and a multi-segment fusion mechanism are\nadopted. Our proposed DynaMixer model (97M parameters) achieves 84.3\\% top-1\naccuracy on the ImageNet-1K dataset without extra training data, performing\nfavorably against the state-of-the-art vision MLP models. When the number of\nparameters is reduced to 26M, it still achieves 82.7\\% top-1 accuracy,\nsurpassing the existing MLP-like models with a similar capacity. The\nimplementation of DynaMixer will be made available to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenhao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yiming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Li Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yibing Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Psychophysical Evaluation of Human Performance in Detecting Digital Face Image Manipulations. (arXiv:2201.12084v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12084","description":"<p>In recent years, increasing deployment of face recognition technology in\nsecurity-critical settings, such as border control or law enforcement, has led\nto considerable interest in the vulnerability of face recognition systems to\nattacks utilising legitimate documents, which are issued on the basis of\ndigitally manipulated face images. As automated manipulation and attack\ndetection remains a challenging task, conventional processes with human\ninspectors performing identity verification remain indispensable. These\ncircumstances merit a closer investigation of human capabilities in detecting\nmanipulated face images, as previous work in this field is sparse and often\nconcentrated only on specific scenarios and biometric characteristics.\n</p>\n<p>This work introduces a web-based, remote visual discrimination experiment on\nthe basis of principles adopted from the field of psychophysics and\nsubsequently discusses interdisciplinary opportunities with the aim of\nexamining human proficiency in detecting different types of digitally\nmanipulated face images, specifically face swapping, morphing, and retouching.\nIn addition to analysing appropriate performance measures, a possible metric of\ndetectability is explored. Experimental data of 306 probands indicate that\ndetection performance is widely distributed across the population and detection\nof certain types of face image manipulations is much more challenging than\nothers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nichols_R/0/1/0/all/0/1\">Robert Nichols</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. (arXiv:2201.12086v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12086","description":"<p>Vision-Language Pre-training (VLP) has advanced the performance for many\nvision-language tasks. However, most existing pre-trained models only excel in\neither understanding-based tasks or generation-based tasks. Furthermore,\nperformance improvement has been largely achieved by scaling up the dataset\nwith noisy image-text pairs collected from the web, which is a suboptimal\nsource of supervision. In this paper, we propose BLIP, a new VLP framework\nwhich transfers flexibly to both vision-language understanding and generation\ntasks. BLIP effectively utilizes the noisy web data by bootstrapping the\ncaptions, where a captioner generates synthetic captions and a filter removes\nthe noisy ones. We achieve state-of-the-art results on a wide range of\nvision-language tasks, such as image-text retrieval (+2.7% in average\nrecall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score).\nBLIP also demonstrates strong generalization ability when directly transferred\nto video-language tasks in a zero-shot manner. Code, models, and datasets are\nreleased at https://github.com/salesforce/BLIP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junnan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dongxu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label uncertainty-guided multi-stream model for disease screening. (arXiv:2201.12089v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12089","description":"<p>The annotation of disease severity for medical image datasets often relies on\ncollaborative decisions from multiple human graders. The intra-observer\nvariability derived from individual differences always persists in this\nprocess, yet the influence is often underestimated. In this paper, we cast the\nintra-observer variability as an uncertainty problem and incorporate the label\nuncertainty information as guidance into the disease screening model to improve\nthe final decision. The main idea is dividing the images into simple and hard\ncases by uncertainty information, and then developing a multi-stream network to\ndeal with different cases separately. Particularly, for hard cases, we\nstrengthen the network's capacity in capturing the correct disease features and\nresisting the interference of uncertainty. Experiments on a fundus image-based\nglaucoma screening case study show that the proposed model outperforms several\nbaselines, especially in screening hard cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1\">Zongyuan Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingguang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xiaotong Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neighborhood-aware Geometric Encoding Network for Point Cloud Registration. (arXiv:2201.12094v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12094","description":"<p>The distinguishing geometric features determine the success of point cloud\nregistration. However, most point clouds are partially overlapping, corrupted\nby noise, and comprised of indistinguishable surfaces, which makes it a\nchallenge to extract discriminative features. Here, we propose the\nNeighborhood-aware Geometric Encoding Network (NgeNet) for accurate point cloud\nregistration. NgeNet utilizes a geometric guided encoding module to take\ngeometric characteristics into consideration, a multi-scale architecture to\nfocus on the semantically rich regions in different scales, and a consistent\nvoting strategy to select features with proper neighborhood size and reject the\nspecious features. The awareness of adaptive neighborhood points is obtained\nthrough the multi-scale architecture accompanied by voting. Specifically, the\nproposed techniques in NgeNet are model-agnostic, which could be easily\nmigrated to other networks. Comprehensive experiments on indoor, outdoor and\nobject-centric synthetic datasets demonstrate that NgeNet surpasses all of the\npublished state-of-the-art methods. The code will be available at\nhttps://github.com/zhulf0804/NgeNet.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Lifa Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1\">Haining Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Changwei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Renmin Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Owner-member Relationship with Graph Convolution Network in Fisheye Camera System. (arXiv:2201.12099v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12099","description":"<p>The owner-member relationship between wheels and vehicles contributes\nsignificantly to the 3D perception of vehicles, especially in embedded\nenvironments. However, to leverage this relationship we must face two major\nchallenges: i) Traditional IoU-based heuristics have difficulty handling\noccluded traffic congestion scenarios. ii) The effectiveness and applicability\nof the solution in a vehicle-mounted system is difficult. To address these\nissues, we propose an innovative relationship prediction method, DeepWORD, by\ndesigning a graph convolutional network (GCN). Specifically, to improve the\ninformation richness, we use feature maps with local correlation as input to\nthe nodes. Subsequently, we introduce a graph attention network (GAT) to\ndynamically correct the a priori estimation bias. Finally, we designed a\ndataset as a large-scale benchmark which has annotated owner-member\nrelationship, called WORD. In the experiments we learned that the proposed\nmethod achieved state-of-the-art accuracy and real-time performance. The WORD\ndataset is made publicly available at\nhttps://github.com/NamespaceMain/ownermember-relationship-dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zizhang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jason Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1\">Tianhao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Feature Visualization within an Automated Design Assessment leveraging Explainable Artificial Intelligence Methods. (arXiv:2201.12107v1 [cs.AI])","link":"http://arxiv.org/abs/2201.12107","description":"<p>Not only automation of manufacturing processes but also automation of\nautomation procedures itself become increasingly relevant to automation\nresearch. In this context, automated capability assessment, mainly leveraged by\ndeep learning systems driven from 3D CAD data, have been presented. Current\nassessment systems may be able to assess CAD data with regards to abstract\nfeatures, e.g. the ability to automatically separate components from bulk\ngoods, or the presence of gripping surfaces. Nevertheless, they suffer from the\nfactor of black box systems, where an assessment can be learned and generated\neasily, but without any geometrical indicator about the reasons of the system's\ndecision. By utilizing explainable AI (xAI) methods, we attempt to open up the\nblack box. Explainable AI methods have been used in order to assess whether a\nneural network has successfully learned a given task or to analyze which\nfeatures of an input might lead to an adversarial attack. These methods aim to\nderive additional insights into a neural network, by analyzing patterns from a\ngiven input and its impact to the network output. Within the NeuroCAD Project,\nxAI methods are used to identify geometrical features which are associated with\na certain abstract feature. Within this work, a sensitivity analysis (SA), the\nlayer-wise relevance propagation (LRP), the Gradient-weighted Class Activation\nMapping (Grad-CAM) method as well as the Local Interpretable Model-Agnostic\nExplanations (LIME) have been implemented in the NeuroCAD environment, allowing\nnot only to assess CAD models but also to identify features which have been\nrelevant for the network decision. In the medium run, this might enable to\nidentify regions of interest supporting product designers to optimize their\nmodels with regards to assembly processes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schonhof_R/0/1/0/all/0/1\">Raoul Sch&#xf6;nhof</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Werner_A/0/1/0/all/0/1\">Artem Werner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elstner_J/0/1/0/all/0/1\">Jannes Elstner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zopcsak_B/0/1/0/all/0/1\">Boldizsar Zopcsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awad_R/0/1/0/all/0/1\">Ramez Awad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco Huber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Attention-Model Explainability through Faithfulness Violation Test. (arXiv:2201.12114v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12114","description":"<p>Attention mechanisms are dominating the explainability of deep models. They\nproduce probability distributions over the input, which are widely deemed as\nfeature-importance indicators. However, in this paper, we find one critical\nlimitation in attention explanations: weakness in identifying the polarity of\nfeature impact. This would be somehow misleading -- features with higher\nattention weights may not faithfully contribute to model predictions; instead,\nthey can impose suppression effects. With this finding, we reflect on the\nexplainability of current attention-based techniques, such as\nAttentio$\\odot$Gradient and LRP-based attention explanations. We first propose\nan actionable diagnostic methodology (henceforth faithfulness violation test)\nto measure the consistency between explanation weights and the impact polarity.\nThrough the extensive experiments, we then show that most tested explanation\nmethods are unexpectedly hindered by the faithfulness violation issue,\nespecially the raw attention. Empirical analyses on the factors affecting\nviolation issues further provide useful observations for adopting explanation\nmethods in attention models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yangyang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1\">Chenqi Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DELAUNAY: a dataset of abstract art for psychophysical and machine learning research. (arXiv:2201.12123v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12123","description":"<p>Image datasets are commonly used in psychophysical experiments and in machine\nlearning research. Most publicly available datasets are comprised of images of\nrealistic and natural objects. However, while typical machine learning models\nlack any domain specific knowledge about natural objects, humans can leverage\nprior experience for such data, making comparisons between artificial and\nnatural learning challenging. Here, we introduce DELAUNAY, a dataset of\nabstract paintings and non-figurative art objects labelled by the artists'\nnames. This dataset provides a middle ground between natural images and\nartificial patterns and can thus be used in a variety of contexts, for example\nto investigate the sample efficiency of humans and artificial neural networks.\nFinally, we train an off-the-shelf convolutional neural network on DELAUNAY,\nhighlighting several of its intriguing features.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gontier_C/0/1/0/all/0/1\">Camille Gontier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_J/0/1/0/all/0/1\">Jakob Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petrovici_M/0/1/0/all/0/1\">Mihai A. Petrovici</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"O-ViT: Orthogonal Vision Transformer. (arXiv:2201.12133v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12133","description":"<p>Inspired by the tremendous success of the self-attention mechanism in natural\nlanguage processing, the Vision Transformer (ViT) creatively applies it to\nimage patch sequences and achieves incredible performance. However, the scaled\ndot-product self-attention of ViT brings about scale ambiguity to the structure\nof the original feature space. To address this problem, we propose a novel\nmethod named Orthogonal Vision Transformer (O-ViT), to optimize ViT from the\ngeometric perspective. O-ViT limits parameters of self-attention blocks to be\non the norm-keeping orthogonal manifold, which can keep the geometry of the\nfeature space. Moreover, O-ViT achieves both orthogonal constraints and cheap\noptimization overhead by adopting a surjective mapping between the orthogonal\ngroup and its Lie algebra.We have conducted comparative experiments on image\nrecognition tasks to demonstrate O-ViT's validity and experiments show that\nO-ViT can boost the performance of ViT by up to 3.6%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1\">Yanhong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yingjie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xian Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingsong Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carotid artery wall segmentation in ultrasound image sequences using a deep convolutional neural network. (arXiv:2201.12152v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12152","description":"<p>The objective of this study is the segmentation of the intima-media complex\nof the common carotid artery, on longitudinal ultrasound images, to measure its\nthickness. We propose a fully automatic region-based segmentation method,\ninvolving a supervised region-based deep-learning approach based on a dilated\nU-net network. It was trained and evaluated using a 5-fold cross-validation on\na multicenter database composed of 2176 images annotated by two experts. The\nresulting mean absolute difference (&lt;120 um) compared to reference annotations\nwas less than the inter-observer variability (180 um). With a 98.7% success\nrate, i.e., only 1.3% cases requiring manual correction, the proposed method\nhas been shown to be robust and thus may be recommended for use in clinical\npractice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Laine_N/0/1/0/all/0/1\">Nolann Lain&#xe9;</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zahnd_G/0/1/0/all/0/1\">Guillaume Zahnd</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liebgott_H/0/1/0/all/0/1\">Herv &#xe9; Liebgott</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Orkisz_M/0/1/0/all/0/1\">Maciej Orkisz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Single-shot Depth Estimation using Perceptual Reconstruction. (arXiv:2201.12170v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12170","description":"<p>Real-time estimation of actual object depth is a module that is essential to\nperforming various autonomous system tasks such as 3D reconstruction, scene\nunderstanding and condition assessment of machinery parts. During the last\ndecade of machine learning, extensive deployment of deep learning methods to\ncomputer vision tasks has yielded approaches that succeed in achieving\nrealistic depth synthesis out of a simple RGB modality. While most of these\nmodels are based on paired depth data or availability of video sequences and\nstereo images, methods for single-view depth synthesis in a fully unsupervised\nsetting have hardly been explored. This study presents the most recent advances\nin the field of generative neural networks, leveraging them to perform fully\nunsupervised single-shot depth synthesis. Two generators for RGB-to-depth and\ndepth-to-RGB transfer are implemented and simultaneously optimized using the\nWasserstein-1 distance and a novel perceptual reconstruction term. To ensure\nthat the proposed method is plausible, we comprehensively evaluate the models\nusing industrial surface depth data as well as the Texas 3D Face Recognition\nDatabase and the SURREAL dataset that records body depth. The success observed\nin this study suggests the great potential for unsupervised single-shot depth\nestimation in real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Angermann_C/0/1/0/all/0/1\">Christoph Angermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwab_M/0/1/0/all/0/1\">Matthias Schwab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haltmeier_M/0/1/0/all/0/1\">Markus Haltmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laubichler_C/0/1/0/all/0/1\">Christian Laubichler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsson_S/0/1/0/all/0/1\">Steinbj&#xf6;rn J&#xf3;nsson</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plug & Play Attacks: Towards Robust and Flexible Model Inversion Attacks. (arXiv:2201.12179v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12179","description":"<p>Model inversion attacks (MIAs) aim to create synthetic images that reflect\nthe class-wise characteristics from a target classifier's training data by\nexploiting the model's learned knowledge. Previous research has developed\ngenerative MIAs using generative adversarial networks (GANs) as image priors\nthat are tailored to a specific target model. This makes the attacks time- and\nresource-consuming, inflexible, and susceptible to distributional shifts\nbetween datasets. To overcome these drawbacks, we present Plug &amp; Play Attacks\nthat loosen the dependency between the target model and image prior and enable\nthe use of a single trained GAN to attack a broad range of targets with only\nminor attack adjustments needed. Moreover, we show that powerful MIAs are\npossible even with publicly available pre-trained GANs and under strong\ndistributional shifts, whereas previous approaches fail to produce meaningful\nresults. Our extensive evaluation confirms the improved robustness and\nflexibility of Plug &amp; Play Attacks and their ability to create high-quality\nimages revealing sensitive class characteristics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1\">Lukas Struppek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1\">Dominik Hintersdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Correia_A/0/1/0/all/0/1\">Antonio De Almeida Correia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Antonia Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1\">Kristian Kersting</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A tomographic workflow to enable deep learning for X-ray based foreign object detection. (arXiv:2201.12184v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12184","description":"<p>Detection of unwanted (`foreign') objects within products is a common\nprocedure in many branches of industry for maintaining production quality.\nX-ray imaging is a fast, non-invasive and widely applicable method for foreign\nobject detection. Deep learning has recently emerged as a powerful approach for\nrecognizing patterns in radiographs (i.e., X-ray images), enabling automated\nX-ray based foreign object detection. However, these methods require a large\nnumber of training examples and manual annotation of these examples is a\nsubjective and laborious task. In this work, we propose a Computed Tomography\n(CT) based method for producing training data for supervised learning of\nforeign object detection, with minimal labour requirements. In our approach, a\nfew representative objects are CT scanned and reconstructed in 3D. The\nradiographs that have been acquired as part of the CT-scan data serve as input\nfor the machine learning method. High-quality ground truth locations of the\nforeign objects are obtained through accurate 3D reconstructions and\nsegmentations. Using these segmented volumes, corresponding 2D segmentations\nare obtained by creating virtual projections. We outline the benefits of\nobjectively and reproducibly generating training data in this way compared to\nconventional radiograph annotation. In addition, we show how the accuracy\ndepends on the number of objects used for the CT reconstructions. The results\nshow that in this workflow generally only a relatively small number of\nrepresentative objects (i.e., fewer than 10) are needed to achieve adequate\ndetection performance in an industrial setting. Moreover, for real experimental\ndata we show that the workflow leads to higher foreign object detection\naccuracies than with standard radiograph annotation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeegers_M/0/1/0/all/0/1\">Math&#xe9; T. Zeegers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeuwen_T/0/1/0/all/0/1\">Tristan van Leeuwen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pelt_D/0/1/0/all/0/1\">Dani&#xeb;l M. Pelt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coban_S/0/1/0/all/0/1\">Sophia Bethany Coban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liere_R/0/1/0/all/0/1\">Robert van Liere</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batenburg_K/0/1/0/all/0/1\">Kees Joost Batenburg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M\\\"{o}bius Convolutions for Spherical CNNs. (arXiv:2201.12212v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12212","description":"<p>M\\\"{o}bius transformations play an important role in both geometry and\nspherical image processing -- they are the group of conformal automorphisms of\n2D surfaces and the spherical equivalent of homographies. Here we present a\nnovel, M\\\"{o}bius-equivariant spherical convolution operator which we call\nM\\\"{o}bius convolution, and with it, develop the foundations for\nM\\\"{o}bius-equivariant spherical CNNs. Our approach is based on a simple\nobservation: to achieve equivariance, we only need to consider the\nlower-dimensional subgroup which transforms the positions of points as seen in\nthe frames of their neighbors. To efficiently compute M\\\"{o}bius convolutions\nat scale we derive an approximation of the action of the transformations on\nspherical filters, allowing us to compute our convolutions in the spectral\ndomain with the fast Spherical Harmonic Transform. The resulting framework is\nboth flexible and descriptive, and we demonstrate its utility by achieving\npromising results in both shape classification and image segmentation tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mitchel_T/0/1/0/all/0/1\">Thomas W. Mitchel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1\">Noam Aigerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1\">Vladimir G. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kazhdan_M/0/1/0/all/0/1\">Michael Kazhdan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-paced learning to improve text row detection in historical documents with missing lables. (arXiv:2201.12216v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12216","description":"<p>An important preliminary step of optical character recognition systems is the\ndetection of text rows. To address this task in the context of historical data\nwith missing labels, we propose a self-paced learning algorithm capable of\nimproving the row detection performance. We conjecture that pages with more\nground-truth bounding boxes are less likely to have missing annotations. Based\non this hypothesis, we sort the training examples in descending order with\nrespect to the number of ground-truth bounding boxes, and organize them into k\nbatches. Using our self-paced learning method, we train a row detector over k\niterations, progressively adding batches with less ground-truth annotations. At\neach iteration, we combine the ground-truth bounding boxes with pseudo-bounding\nboxes (bounding boxes predicted by the model itself) using non-maximum\nsuppression, and we include the resulting annotations at the next training\niteration. We demonstrate that our self-paced learning strategy brings\nsignificant performance gains on two data sets of historical documents,\nimproving the average precision of YOLOv4 with more than 12% on one data set\nand 39% on the other.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghadamiyan_L/0/1/0/all/0/1\">Lida Ghadamiyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_M/0/1/0/all/0/1\">Marius Popescu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixing Implicit and Explicit Deep Learning with Skip DEQs and Infinite Time Neural ODEs (Continuous DEQs). (arXiv:2201.12240v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12240","description":"<p>Implicit deep learning architectures, like Neural ODEs and Deep Equilibrium\nModels (DEQs), separate the definition of a layer from the description of its\nsolution process. While implicit layers allow features such as depth to adapt\nto new scenarios and inputs automatically, this adaptivity makes its\ncomputational expense challenging to predict. Numerous authors have noted that\nimplicit layer techniques can be more computationally intensive than explicit\nlayer methods. In this manuscript, we address the question: is there a way to\nsimultaneously achieve the robustness of implicit layers while allowing the\nreduced computational expense of an explicit layer? To solve this we develop\nSkip DEQ, an implicit-explicit (IMEX) layer that simultaneously trains an\nexplicit prediction followed by an implicit correction. We show that training\nthis explicit layer is free and even decreases the training time by 2.5x and\nprediction time by 3.4x. We then further increase the \"implicitness\" of the DEQ\nby redefining the method in terms of an infinite time neural ODE which\nparadoxically decreases the training cost over a standard neural ODE by not\nrequiring backpropagation through time. We demonstrate how the resulting\nContinuous Skip DEQ architecture trains more robustly than the original DEQ\nwhile achieving faster training and prediction times. Together, this manuscript\nshows how bridging the dichotomy of implicit and explicit deep learning can\ncombine the advantages of both techniques.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_A/0/1/0/all/0/1\">Avik Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edelman_A/0/1/0/all/0/1\">Alan Edelman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rackauckas_C/0/1/0/all/0/1\">Christopher Rackauckas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Review on Deep-Learning Algorithms for Fetal Ultrasound-Image Analysis. (arXiv:2201.12260v1 [eess.IV])","link":"http://arxiv.org/abs/2201.12260","description":"<p>Deep-learning (DL) algorithms are becoming the standard for processing\nultrasound (US) fetal images. Despite a large number of survey papers already\npresent in this field, most of them are focusing on a broader area of\nmedical-image analysis or not covering all fetal US DL applications. This paper\nsurveys the most recent work in the field, with a total of 145 research papers\npublished after 2017. Each paper is analyzed and commented on from both the\nmethodology and application perspective. We categorized the papers in (i) fetal\nstandard-plane detection, (ii) anatomical-structure analysis, and (iii)\nbiometry parameter estimation. For each category, main limitations and open\nissues are presented. Summary tables are included to facilitate the comparison\namong the different approaches. Publicly-available datasets and performance\nmetrics commonly used to assess algorithm performance are summarized, too. This\npaper ends with a critical summary of the current state of the art on DL\nalgorithms for fetal US image analysis and a discussion on current challenges\nthat have to be tackled by researchers working in the field to translate the\nresearch methodology into the actual clinical practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Fiorentino_M/0/1/0/all/0/1\">Maria Chiara Fiorentino</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Villani_F/0/1/0/all/0/1\">Francesca Pia Villani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cosmo_M/0/1/0/all/0/1\">Mariachiara Di Cosmo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frontoni_E/0/1/0/all/0/1\">Emanuele Frontoni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moccia_S/0/1/0/all/0/1\">Sara Moccia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"3D-FlowNet: Event-based optical flow estimation with 3D representation. (arXiv:2201.12265v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12265","description":"<p>Event-based cameras can overpass frame-based cameras limitations for\nimportant tasks such as high-speed motion detection during self-driving cars\nnavigation in low illumination conditions. The event cameras' high temporal\nresolution and high dynamic range, allow them to work in fast motion and\nextreme light scenarios. However, conventional computer vision methods, such as\nDeep Neural Networks, are not well adapted to work with event data as they are\nasynchronous and discrete. Moreover, the traditional 2D-encoding representation\nmethods for event data, sacrifice the time resolution. In this paper, we first\nimprove the 2D-encoding representation by expanding it into three dimensions to\nbetter preserve the temporal distribution of the events. We then propose\n3D-FlowNet, a novel network architecture that can process the 3D input\nrepresentation and output optical flow estimations according to the new\nencoding methods. A self-supervised training strategy is adopted to compensate\nthe lack of labeled datasets for the event-based camera. Finally, the proposed\nnetwork is trained and evaluated with the Multi-Vehicle Stereo Event Camera\n(MVSEC) dataset. The results show that our 3D-FlowNet outperforms\nstate-of-the-art approaches with less training epoch (30 compared to 100 of\nSpike-FlowNet).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Haixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1\">Minh-Quan Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fremont_V/0/1/0/all/0/1\">Vincent Fremont</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HSADML: Hyper-Sphere Angular Deep Metric based Learning for Brain Tumor Classification. (arXiv:2201.12269v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12269","description":"<p>Brain Tumors are abnormal mass of clustered cells penetrating regions of\nbrain. Their timely identification and classification help doctors to provide\nappropriate treatment. However, Classifi-cation of Brain Tumors is quite\nintricate because of high-intra class similarity and low-inter class\nvariability. Due to morphological similarity amongst various MRI-Slices of\ndifferent classes the challenge deepens more. This all leads to hampering\ngeneralizability of classification models. To this end, this paper proposes\nHSADML, a novel framework which enables deep metric learning (DML) using\nSphereFace Loss. SphereFace loss embeds the features into a\nhyperspheric-manifold and then imposes margin on the embeddings to enhance\ndifferentiability between the classes. With utilization of SphereFace loss\nbased deep metric learning it is ensured that samples from class clustered\ntogether while the different ones are pushed apart. Results reflects the\npromi-nence in the approach, the proposed framework achieved state-of-the-art\n98.69% validation accu-racy using k-NN (k=1) and this is significantly higher\nthan normal SoftMax Loss training which though obtains 98.47% validation\naccuracy but that too with limited inter-class separability and intra-class\ncloseness. Experimental analysis done over various classifiers and loss\nfunction set-tings suggests potential in the approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1\">Aman Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1\">Vibhav Prakash Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Conventional Vision Models on Neuromorphic Fall Detection and Action Recognition Dataset. (arXiv:2201.12285v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12285","description":"<p>Neuromorphic vision-based sensors are gaining popularity in recent years with\ntheir ability to capture Spatio-temporal events with low power sensing. These\nsensors record events or spikes over traditional cameras which helps in\npreserving the privacy of the subject being recorded. These events are captured\nas per-pixel brightness changes and the output data stream is encoded with\ntime, location, and pixel intensity change information. This paper proposes and\nbenchmarks the performance of fine-tuned conventional vision models on\nneuromorphic human action recognition and fall detection datasets. The\nSpatio-temporal event streams from the Dynamic Vision Sensing cameras are\nencoded into a standard sequence image frames. These video frames are used for\nbenchmarking conventional deep learning-based architectures. In this proposed\napproach, we fine-tuned the state-of-the-art vision models for this Dynamic\nVision Sensing (DVS) application and named these models as DVS-R2+1D, DVS-CSN,\nDVS-C2D, DVS-SlowFast, DVS-X3D, and DVS-MViT. Upon comparing the performance of\nthese models, we see the current state-of-the-art MViT based architecture\nDVS-MViT outperforms all the other models with an accuracy of 0.958 and an F-1\nscore of 0.958. The second best is the DVS-C2D with an accuracy of 0.916 and an\nF-1 score of 0.916. Third and Fourth are DVS-R2+1D and DVS-SlowFast with an\naccuracy of 0.875 and 0.833 and F-1 score of 0.875 and 0.861 respectively.\nDVS-CSN and DVS-X3D were the least performing models with an accuracy of 0.708\nand 0.625 and an F1 score of 0.722 and 0.625 respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_K/0/1/0/all/0/1\">Karthik Sivarama Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_K/0/1/0/all/0/1\">Koushik Sivarama Krishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VRT: A Video Restoration Transformer. (arXiv:2201.12288v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12288","description":"<p>Video restoration (e.g., video super-resolution) aims to restore high-quality\nframes from low-quality frames. Different from single image restoration, video\nrestoration generally requires to utilize temporal information from multiple\nadjacent but usually misaligned video frames. Existing deep methods generally\ntackle with this by exploiting a sliding window strategy or a recurrent\narchitecture, which either is restricted by frame-by-frame restoration or lacks\nlong-range modelling ability. In this paper, we propose a Video Restoration\nTransformer (VRT) with parallel frame prediction and long-range temporal\ndependency modelling abilities. More specifically, VRT is composed of multiple\nscales, each of which consists of two kinds of modules: temporal mutual self\nattention (TMSA) and parallel warping. TMSA divides the video into small clips,\non which mutual attention is applied for joint motion estimation, feature\nalignment and feature fusion, while self attention is used for feature\nextraction. To enable cross-clip interactions, the video sequence is shifted\nfor every other layer. Besides, parallel warping is used to further fuse\ninformation from neighboring frames by parallel feature warping. Experimental\nresults on three tasks, including video super-resolution, video deblurring and\nvideo denoising, demonstrate that VRT outperforms the state-of-the-art methods\nby large margins ($\\textbf{up to 2.16dB}$) on nine benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jingyun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jiezhang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1\">Yuchen Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranjan_R/0/1/0/all/0/1\">Rakesh Ranjan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yawei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions. (arXiv:2201.12296v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12296","description":"<p>Deep neural networks on 3D point cloud data have been widely used in the real\nworld, especially in safety-critical applications. However, their robustness\nagainst corruptions is less studied. In this paper, we present ModelNet40-C,\nthe first comprehensive benchmark on 3D point cloud corruption robustness,\nconsisting of 15 common and realistic corruptions. Our evaluation shows a\nsignificant gap between the performances on ModelNet40 and ModelNet40-C for\nstate-of-the-art (SOTA) models. To reduce the gap, we propose a simple but\neffective method by combining PointCutMix-R and TENT after evaluating a wide\nrange of augmentation and test-time adaptation strategies. We identify a number\nof critical insights for future studies on corruption robustness in point cloud\nrecognition. For instance, we unveil that Transformer-based architectures with\nproper training recipes achieve the strongest robustness. We hope our in-depth\nanalysis will motivate the development of robust training strategies or\narchitecture designs in the 3D point cloud domain. Our codebase and dataset are\nincluded in https://github.com/jiachens/ModelNet40-C\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiachen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingzhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhiding Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chaowei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Z. Morley Mao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"REET: Robustness Evaluation and Enhancement Toolbox for Computational Pathology. (arXiv:2201.12311v1 [cs.LG])","link":"http://arxiv.org/abs/2201.12311","description":"<p>Motivation: Digitization of pathology laboratories through digital slide\nscanners and advances in deep learning approaches for objective histological\nassessment have resulted in rapid progress in the field of computational\npathology (CPath) with wide-ranging applications in medical and pharmaceutical\nresearch as well as clinical workflows. However, the estimation of robustness\nof CPath models to variations in input images is an open problem with a\nsignificant impact on the down-stream practical applicability, deployment and\nacceptability of these approaches. Furthermore, development of domain-specific\nstrategies for enhancement of robustness of such models is of prime importance\nas well.\n</p>\n<p>Implementation and Availability: In this work, we propose the first\ndomain-specific Robustness Evaluation and Enhancement Toolbox (REET) for\ncomputational pathology applications. It provides a suite of algorithmic\nstrategies for enabling robustness assessment of predictive models with respect\nto specialized image transformations such as staining, compression, focusing,\nblurring, changes in spatial resolution, brightness variations, geometric\nchanges as well as pixel-level adversarial perturbations. Furthermore, REET\nalso enables efficient and robust training of deep learning pipelines in\ncomputational pathology. REET is implemented in Python and is available at the\nfollowing URL: https://github.com/alexjfoote/reetoolbox.\n</p>\n<p>Contact: Fayyaz.minhas@warwick.ac.uk\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Foote_A/0/1/0/all/0/1\">Alex Foote</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asif_A/0/1/0/all/0/1\">Amina Asif</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minhas_F/0/1/0/all/0/1\">Fayyaz Minhas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAB-DETR: Dynamic Anchor Boxes are Better Queries for DETR. (arXiv:2201.12329v1 [cs.CV])","link":"http://arxiv.org/abs/2201.12329","description":"<p>We present in this paper a novel query formulation using dynamic anchor boxes\nfor DETR (DEtection TRansformer) and offer a deeper understanding of the role\nof queries in DETR. This new formulation directly uses box coordinates as\nqueries in Transformer decoders and dynamically updates them layer-by-layer.\nUsing box coordinates not only helps using explicit positional priors to\nimprove the query-to-feature similarity and eliminate the slow training\nconvergence issue in DETR, but also allows us to modulate the positional\nattention map using the box width and height information. Such a design makes\nit clear that queries in DETR can be implemented as performing soft ROI pooling\nlayer-by-layer in a cascade manner. As a result, it leads to the best\nperformance on MS-COCO benchmark among the DETR-like detection models under the\nsame setting, e.g., AP 45.7\\% using ResNet50-DC5 as backbone trained in 50\nepochs. We also conducted extensive experiments to confirm our analysis and\nverify the effectiveness of our methods. Code is available at\n\\url{https://github.com/SlongLiu/DAB-DETR}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Feng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianbiao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ASMCNN: An Efficient Brain Extraction Using Active Shape Model and Convolutional Neural Networks. (arXiv:1802.01268v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/1802.01268","description":"<p>Brain extraction (skull stripping) is a challenging problem in neuroimaging.\nIt is due to the variability in conditions from data acquisition or\nabnormalities in images, making brain morphology and intensity characteristics\nchangeable and complicated. In this paper, we propose an algorithm for skull\nstripping in Magnetic Resonance Imaging (MRI) scans, namely ASMCNN, by\ncombining the Active Shape Model (ASM) and Convolutional Neural Network (CNN)\nfor taking full of their advantages to achieve remarkable results. Instead of\nworking with 3D structures, we process 2D image sequences in the sagittal\nplane. First, we divide images into different groups such that, in each group,\nshapes and structures of brain boundaries have similar appearances. Second, a\nmodified version of ASM is used to detect brain boundaries by utilizing prior\nknowledge of each group. Finally, CNN and post-processing methods, including\nConditional Random Field (CRF), Gaussian processes, and several special rules\nare applied to refine the segmentation contours. Experimental results show that\nour proposed method outperforms current state-of-the-art algorithms by a\nsignificant margin in all experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy H. M. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy M. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_M/0/1/0/all/0/1\">Mai T. N. Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khanh T. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Triet_N/0/1/0/all/0/1\">Nguyen A. Triet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_P/0/1/0/all/0/1\">Pham T. Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Topological Filter for Learning with Label Noise. (arXiv:2012.04835v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.04835","description":"<p>Noisy labels can impair the performance of deep neural networks. To tackle\nthis problem, in this paper, we propose a new method for filtering label noise.\nUnlike most existing methods relying on the posterior probability of a noisy\nclassifier, we focus on the much richer spatial behavior of data in the latent\nrepresentational space. By leveraging the high-order topological information of\ndata, we are able to collect most of the clean data and train a high-quality\nmodel. Theoretically we prove that this topological approach is guaranteed to\ncollect the clean data with high probability. Empirical results show that our\nmethod outperforms the state-of-the-arts and is robust to a broad spectrum of\nnoise types and levels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pengxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Songzhu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goswami_M/0/1/0/all/0/1\">Mayank Goswami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simplicial Complex Representation Learning. (arXiv:2103.04046v4 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2103.04046","description":"<p>Simplicial complexes form an important class of topological spaces that are\nfrequently used in many application areas such as computer-aided design,\ncomputer graphics, and simulation. Representation learning on graphs, which are\njust 1-d simplicial complexes, has witnessed a great attention in recent years.\nHowever, there has not been enough effort to extend representation learning to\nhigher dimensional simplicial objects due to the additional complexity these\nobjects hold, especially when it comes to entire-simplicial complex\nrepresentation learning. In this work, we propose a method for simplicial\ncomplex-level representation learning that embeds a simplicial complex to a\nuniversal embedding space in a way that complex-to-complex proximity is\npreserved. Our method uses our novel geometric message passing schemes to learn\nan entire simplicial complex representation in an end-to-end fashion. We\ndemonstrate the proposed model on publicly available mesh dataset. To the best\nof our knowledge, this work presents the first method for learning simplicial\ncomplex-level representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1\">Mustafa Hajij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1\">Ghada Zamzmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maroulas_V/0/1/0/all/0/1\">Vasileios Maroulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papamarkou_T/0/1/0/all/0/1\">Theodore Papamarkou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xuanting Cai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effects of Spectral Dimensionality Reduction on Hyperspectral Pixel Classification: A Case Study. (arXiv:2104.00788v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.00788","description":"<p>This paper presents a systematic study of the effects of hyperspectral pixel\ndimensionality reduction on the pixel classification task. We use five\ndimensionality reduction methods -- PCA, KPCA, ICA, AE, and DAE -- to compress\n301-dimensional hyperspectral pixels. Compressed pixels are subsequently used\nto perform pixel classifications. Pixel classification accuracies together with\ncompression method, compression rates, and reconstruction errors provide a new\nlens to study the suitability of a compression method for the task of pixel\nclassification. We use three high-resolution hyperspectral image datasets,\nrepresenting three common landscape types (i.e. urban, transitional suburban,\nand forests) collected by the Remote Sensing and Spatial Ecosystem Modeling\nlaboratory of the University of Toronto. We found that PCA, KPCA, and ICA post\ngreater signal reconstruction capability; however, when compression rates are\nmore than 90\\% these methods show lower classification scores. AE and DAE\nmethods post better classification accuracy at 95\\% compression rate, however\ntheir performance drops as compression rate approaches 97\\%. Our results\nsuggest that both the compression method and the compression rate are important\nconsiderations when designing a hyperspectral pixel classification pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mantripragada_K/0/1/0/all/0/1\">Kiran Mantripragada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dao_P/0/1/0/all/0/1\">Phuong D. Dao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuhong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qureshi_F/0/1/0/all/0/1\">Faisal Z. Qureshi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TATL: Task Agnostic Transfer Learning for Skin Attributes Detection. (arXiv:2104.01641v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.01641","description":"<p>Existing skin attributes detection methods usually initialize with a\npre-trained Imagenet network and then fine-tune on a medical target task.\nHowever, we argue that such approaches are suboptimal because medical datasets\nare largely different from ImageNet and often contain limited training samples.\nIn this work, we propose \\emph{Task Agnostic Transfer Learning (TATL)}, a novel\nframework motivated by dermatologists' behaviors in the skincare context. TATL\nlearns an attribute-agnostic segmenter that detects lesion skin regions and\nthen transfers this knowledge to a set of attribute-specific classifiers to\ndetect each particular attribute. Since TATL's attribute-agnostic segmenter\nonly detects skin attribute regions, it enjoys ample data from all attributes,\nallows transferring knowledge among features, and compensates for the lack of\ntraining data from rare attributes. We conduct extensive experiments to\nevaluate the proposed TATL transfer learning mechanism with various neural\nnetwork architectures on two popular skin attributes detection benchmarks. The\nempirical results show that TATL not only works well with multiple\narchitectures but also can achieve state-of-the-art performances while enjoying\nminimal model and computational complexities. We also provide theoretical\ninsights and explanations for why our transfer learning framework performs well\nin practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Duy M. H. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thu T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_H/0/1/0/all/0/1\">Huong Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1\">Quang Pham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Manh-Duy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1\">Daniel Sonntag</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multiple Sclerosis Lesion Analysis in Brain Magnetic Resonance Images: Techniques and Clinical Applications. (arXiv:2104.10029v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.10029","description":"<p>Multiple sclerosis (MS) is a chronic inflammatory and degenerative disease of\nthe central nervous system, characterized by the appearance of focal lesions in\nthe white and gray matter that topographically correlate with an individual\npatient's neurological symptoms and signs. Magnetic resonance imaging (MRI)\nprovides detailed in-vivo structural information, permitting the quantification\nand categorization of MS lesions that critically inform disease management.\nTraditionally, MS lesions have been manually annotated on 2D MRI slices, a\nprocess that is inefficient and prone to inter-/intra-observer errors.\nRecently, automated statistical imaging analysis techniques have been proposed\nto detect and segment MS lesions based on MRI voxel intensity. However, their\neffectiveness is limited by the heterogeneity of both MRI data acquisition\ntechniques and the appearance of MS lesions. By learning complex lesion\nrepresentations directly from images, deep learning techniques have achieved\nremarkable breakthroughs in the MS lesion segmentation task. Here, we provide a\ncomprehensive review of state-of-the-art automatic statistical and\ndeep-learning MS segmentation methods and discuss current and future clinical\napplications. Further, we review technical strategies, such as domain\nadaptation, to enhance MS lesion segmentation in real-world clinical settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1\">Yang Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabezas_M/0/1/0/all/0/1\">Mariano Cabezas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zihao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnett_M/0/1/0/all/0/1\">Michael Barnett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenyu Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Carrying out CNN Channel Pruning in a White Box. (arXiv:2104.11883v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.11883","description":"<p>Channel Pruning has been long studied to compress CNNs, which significantly\nreduces the overall computation. Prior works implement channel pruning in an\nunexplainable manner, which tends to reduce the final classification errors\nwhile failing to consider the internal influence of each channel. In this\npaper, we conduct channel pruning in a white box. Through deep visualization of\nfeature maps activated by different channels, we observe that different\nchannels have a varying contribution to different categories in image\nclassification. Inspired by this, we choose to preserve channels contributing\nto most categories. Specifically, to model the contribution of each channel to\ndifferentiating categories, we develop a class-wise mask for each channel,\nimplemented in a dynamic training manner w.r.t. the input image's category. On\nthe basis of the learned class-wise mask, we perform a global voting mechanism\nto remove channels with less category discrimination. Lastly, a fine-tuning\nprocess is conducted to recover the performance of the pruned model. To our\nbest knowledge, it is the first time that CNN interpretability theory is\nconsidered to guide channel pruning. Extensive experiments on representative\nimage classification tasks demonstrate the superiority of our White-Box over\nmany state-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with\neven 0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box\nachieves a 45.6% FLOPs reduction with only a small loss of 0.83% in the top-1\naccuracy for ResNet-50.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning. (arXiv:2105.04906v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2105.04906","description":"<p>Recent self-supervised methods for image representation learning are based on\nmaximizing the agreement between embedding vectors from different views of the\nsame image. A trivial solution is obtained when the encoder outputs constant\nvectors. This collapse problem is often avoided through implicit biases in the\nlearning architecture, that often lack a clear justification or interpretation.\nIn this paper, we introduce VICReg (Variance-Invariance-Covariance\nRegularization), a method that explicitly avoids the collapse problem with a\nsimple regularization term on the variance of the embeddings along each\ndimension individually. VICReg combines the variance term with a decorrelation\nmechanism based on redundancy reduction and covariance regularization, and\nachieves results on par with the state of the art on several downstream tasks.\nIn addition, we show that incorporating our new variance term into other\nmethods helps stabilize the training and leads to performance improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bardes_A/0/1/0/all/0/1\">Adrien Bardes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponce_J/0/1/0/all/0/1\">Jean Ponce</a>, <a href=\"http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1\">Yann LeCun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.02701","description":"<p>Recent advances in brain clearing and imaging have made it possible to image\nentire mammalian brains at sub-micron resolution. These images offer the\npotential to assemble brain-wide atlases of neuron morphology, but manual\nneuron reconstruction remains a bottleneck. Several automatic reconstruction\nalgorithms exist, but most focus on single neuron images. In this paper, we\npresent a probabilistic reconstruction method, ViterBrain, which combines a\nhidden Markov state process that encodes neuron geometry with a random field\nappearance model of neuron fluorescence. Our method utilizes dynamic\nprogramming to compute the global maximizers of what we call the \"most\nprobable\" neuron path. Our most probable estimation method models the task of\nreconstructing neuronal processes in the presence of other neurons, and thus is\napplicable in images with several neurons. Our method operates on image\nsegmentations in order to leverage cutting edge computer vision technology. We\napplied our algorithm to imperfect image segmentations where false negatives\nsevered neuronal processes, and showed that it can follow axons in the presence\nof noise or nearby neurons. Additionally, it creates a framework where users\ncan intervene to, for example, fit start and endpoints. The code used in this\nwork is available in our open-source Python package brainlit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1\">Thomas L. Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1\">Daniel J. Tward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1\">Ulrich Mueller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1\">Joshua T. Vogelstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1\">Michael I. Miller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conditional COT-GAN for Video Prediction with Kernel Smoothing. (arXiv:2106.05658v2 [stat.ML] UPDATED)","link":"http://arxiv.org/abs/2106.05658","description":"<p>Causal Optimal Transport (COT) results from imposing a temporal causality\nconstraint on classic optimal transport problems, which naturally generates a\nnew concept of distances between distributions on path spaces. The first\napplication of the COT theory for sequential learning was given in Xu et al.\n(2020), where COT-GAN was introduced as an adversarial algorithm to train\nimplicit generative models optimized for producing sequential data. Relying on\n(Xu et al., 2020), the contribution of the present paper is twofold. First, we\ndevelop a conditional version of COT-GAN suitable for sequence prediction. This\nmeans that the dataset is now used in order to learn how a sequence will evolve\ngiven the observation of its past evolution. Second, we improve on the\nconvergence results by working with modifications of the empirical measures via\nkernel smoothing due to (Pflug and Pichler (2016)). The resulting kernel\nconditional COT-GAN algorithm is illustrated with an application for video\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Xu_T/0/1/0/all/0/1\">Tianlin Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Acciaio_B/0/1/0/all/0/1\">Beatrice Acciaio</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Steerable 3D Spherical Neurons. (arXiv:2106.13863v3 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.13863","description":"<p>Emerging from low-level vision theory, steerable filters found their\ncounterpart in prior work on steerable convolutional neural networks\nequivariant to rigid transformations. In our work, we propose a steerable\nfeed-forward learning-based approach that consists of neurons with spherical\ndecision surfaces and operates on point clouds. Such spherical neurons are\nobtained by conformal embedding of Euclidean space and have recently been\nrevisited in the context of learning representations of point sets. Focusing on\n3D geometry, we exploit the isometry property of spherical neurons and derive a\n3D steerability constraint. After training spherical neurons to classify point\nclouds in a canonical orientation, we use a tetrahedron basis to quadruplicate\nthe neurons and construct rotation-equivariant spherical filter banks. We then\napply the derived constraint to interpolate the filter bank outputs and, thus,\nobtain a rotation-invariant network. Finally, we use a synthetic point set and\nreal-world 3D skeleton data to verify our theoretical findings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Convolution for Re-ranking in Person Re-identification. (arXiv:2107.02220v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.02220","description":"<p>Nowadays, deep learning is widely applied to extract features for similarity\ncomputation in person re-identification (re-ID) and have achieved great\nsuccess. However, due to the non-overlapping between training and testing IDs,\nthe difference between the data used for model training and the testing data\nmakes the performance of learned feature degraded during testing. Hence,\nre-ranking is proposed to mitigate this issue and various algorithms have been\ndeveloped. However, most of existing re-ranking methods focus on replacing the\nEuclidean distance with sophisticated distance metrics, which are not friendly\nto downstream tasks and hard to be used for fast retrieval of massive data in\nreal applications. In this work, we propose a graph-based re-ranking method to\nimprove learned features while still keeping Euclidean distance as the\nsimilarity metric. Inspired by graph convolution networks, we develop an\noperator to propagate features over an appropriate graph. Since graph is the\nessential key for the propagation, two important criteria are considered for\ndesigning the graph, and three different graphs are explored accordingly.\nFurthermore, a simple yet effective method is proposed to generate a profile\nvector for each tracklet in videos, which helps extend our method to video\nre-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501,\nDuke, and MARS, demonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuqi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1\">Qian Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Weihua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1\">Rong Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few Shots Are All You Need: A Progressive Few Shot Learning Approach for Low Resource Handwriting Recognition. (arXiv:2107.10064v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2107.10064","description":"<p>Handwritten text recognition in low resource scenarios, such as manuscripts\nwith rare alphabets, is a challenging problem. The main difficulty comes from\nthe very few annotated data and the limited linguistic information (e.g.\ndictionaries and language models). Thus, we propose a few-shot learning-based\nhandwriting recognition approach that significantly reduces the human labor\nannotation process, requiring only few images of each alphabet symbol. The\nmethod consists in detecting all the symbols of a given alphabet in a textline\nimage and decoding the obtained similarity scores to the final sequence of\ntranscribed symbols. Our model is first pretrained on synthetic line images\ngenerated from any alphabet, even though different from the target domain. A\nsecond training step is then applied to diminish the gap between the source and\ntarget data. Since this retraining would require annotation of thousands of\nhandwritten symbols together with their bounding boxes, we propose to avoid\nsuch human effort through an unsupervised progressive learning approach that\nautomatically assigns pseudo-labels to the non-annotated data. The evaluation\non different manuscript datasets show that our model can lead to competitive\nresults with a significant reduction in human effort. The code will be publicly\navailable in this repository: \\url{https://github.com/dali92002/HTRbyMatching}\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1\">Mohamed Ali Souibgui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fornes_A/0/1/0/all/0/1\">Alicia Forn&#xe9;s</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kessentini_Y/0/1/0/all/0/1\">Yousri Kessentini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Megyesi_B/0/1/0/all/0/1\">Be&#xe1;ta Megyesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting of Head Pose Estimation by Knowledge Distillation. (arXiv:2108.09183v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09183","description":"<p>We propose a response-based method of knowledge distillation (KD) for the\nhead pose estimation problem. A student model trained by the proposed KD\nachieves results better than a teacher model, which is atypical for the\nresponse-based method. Our method consists of two stages. In the first stage,\nwe trained the base neural network (NN), which has one regression head and four\nregression via classification (RvC) heads. We build the convolutional ensemble\nover the base NN using offsets of face bounding boxes over a regular grid. In\nthe second stage, we perform KD from the convolutional ensemble into the final\nNN with one RvC head. The KD improves the results by an average of 7.7\\%\ncompared to base NN. This feature makes it possible to use KD as a booster and\neffectively train deeper NNs. NNs trained by our KD method partially improved\nthe state-of-the-art results. KD-ResNet152 has the best results, and\nKD-ResNet18 has a better result on the AFLW2000 dataset than any previous\nmethod.We have made publicly available trained NNs and face bounding boxes for\nthe 300W-LP, AFLW, AFLW2000, and BIWI datasets.Our method potentially can be\neffective for other regression problems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sheka_A/0/1/0/all/0/1\">Andrey Sheka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samun_V/0/1/0/all/0/1\">Victor Samun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KTVQA: Generalized use of External Knowledge to empower Scene Text in Text-VQA. (arXiv:2108.09717v6 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.09717","description":"<p>The open-ended question answering task of Text-VQA requires reading and\nreasoning about local, often previously unseen, scene-text content of an image.\nWe address this zero-shot nature of the problem by proposing the generalized\nuse of external knowledge to augment our understanding of the said scene-text.\nWe design a framework to extract, validate, and reason with knowledge using a\nstandard multimodal transformer for vision language understanding tasks.\nThrough empirical evidence and qualitative results, we demonstrate how external\nknowledge can highlight instance-only cues and thus help deal with training\ndata bias, improve answer entity type correctness, and detect multiword named\nentities. We generate results comparable to the state-of-the-art on three\npublicly available datasets, under the constraints of similar upstream OCR\nsystems and training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1\">Arka Ujjal Dey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1\">Ernest Valveny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harit_G/0/1/0/all/0/1\">Gaurav Harit</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Consistent Relative Confidence and Label-Free Model Selection for Convolutional Neural Networks. (arXiv:2108.11845v5 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.11845","description":"<p>This letter is concerned with image classification with deep convolutional\nneural networks (CNNs). The focus is on the following question: given a set of\ncandidate CNN models, how to select the right one with the best generalization\nproperty for the current task? Present model selection methods require access\nto a batch of labeled data for computing a pre-specified performance metric,\nsuch as the cross-entropy loss, the classification error rate, the negative\nlog-likelihood. In many practical cases, labels are not available in time as\nlabeling itself is a time-consuming and expensive task. To this end, this\nletter presents an approach to CNN model selection using only unlabeled data.\nThis method is developed based on a principle termed consistent relative\nconfidence. The effectiveness and efficiency of the proposed method are\ndemonstrated by experiments using benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bin Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"InSeGAN: A Generative Approach to Segmenting Identical Instances in Depth Images. (arXiv:2108.13865v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2108.13865","description":"<p>In this paper, we present InSeGAN, an unsupervised 3D generative adversarial\nnetwork (GAN) for segmenting (nearly) identical instances of rigid objects in\ndepth images. Using an analysis-by-synthesis approach, we design a novel GAN\narchitecture to synthesize a multiple-instance depth image with independent\ncontrol over each instance. InSeGAN takes in a set of code vectors (e.g.,\nrandom noise vectors), each encoding the 3D pose of an object that is\nrepresented by a learned implicit object template. The generator has two\ndistinct modules. The first module, the instance feature generator, uses each\nencoded pose to transform the implicit template into a feature map\nrepresentation of each object instance. The second module, the depth image\nrenderer, aggregates all of the single-instance feature maps output by the\nfirst module and generates a multiple-instance depth image. A discriminator\ndistinguishes the generated multiple-instance depth images from the\ndistribution of true depth images. To use our model for instance segmentation,\nwe propose an instance pose encoder that learns to take in a generated depth\nimage and reproduce the pose code vectors for all of the object instances. To\nevaluate our approach, we introduce a new synthetic dataset, \"Insta-10\",\nconsisting of 100,000 depth images, each with 5 instances of an object from one\nof 10 classes. Our experiments on Insta-10, as well as on real-world noisy\ndepth images, show that InSeGAN achieves state-of-the-art performance, often\noutperforming prior methods by large margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1\">Anoop Cherian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pais_G/0/1/0/all/0/1\">Goncalo Dias Pais</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Siddarth Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marks_T/0/1/0/all/0/1\">Tim K. Marks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_A/0/1/0/all/0/1\">Alan Sullivan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Blood vessel segmentation in en-face OCTA images: a frequency based method. (arXiv:2109.06116v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2109.06116","description":"<p>Optical coherence tomography angiography (OCTA) is a novel noninvasive\nimaging modality for visualization of retinal blood flow in the human retina.\nUsing specific OCTA imaging biomarkers for the identification of pathologies,\nautomated image segmentations of the blood vessels can improve subsequent\nanalysis and diagnosis. We present a novel segmentation method for vessel\ndensity identification based on frequency representations of the image, in\nparticular, using so-called Gabor filter banks. The algorithm is evaluated\nqualitatively and quantitatively on an OCTA image in-house data set from $10$\neyes acquired by a Cirrus HD-OCT device. Qualitatively, the segmentation\noutcomes received very good visual evaluation feedback by experts.\nQuantitatively, we compared the resulting vessel density values with automated\nin-built values provided by the device. The results underline the visual\nevaluation. For the evaluation of the FAZ identification substep, manual\nannotations of $2$ expert graders were used, showing that our results coincide\nwell in visual and quantitative manners. Lastly, we suggest the computation of\nadaptive local vessel density maps that allow straightforward analysis of\nretinal blood flow in a local manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Breger_A/0/1/0/all/0/1\">Anna Breger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldbach_F/0/1/0/all/0/1\">Felix Goldbach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gerendas_B/0/1/0/all/0/1\">Bianca S. Gerendas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1\">Ursula Schmidt-Erfurth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ehler_M/0/1/0/all/0/1\">Martin Ehler</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modern Evolution Strategies for Creativity: Fitting Concrete Images and Abstract Concepts. (arXiv:2109.08857v2 [cs.NE] UPDATED)","link":"http://arxiv.org/abs/2109.08857","description":"<p>Evolutionary algorithms have been used in the digital art scene since the\n1970s. A popular application of genetic algorithms is to optimize the\nprocedural placement of vector graphic primitives to resemble a given painting.\nIn recent years, deep learning-based approaches have also been proposed to\ngenerate procedural drawings, which can be optimized using gradient descent. In\nthis work, we revisit the use of evolutionary algorithms for computational\ncreativity. We find that modern evolution strategies (ES) algorithms, when\ntasked with the placement of shapes, offer large improvements in both quality\nand efficiency compared to traditional genetic algorithms, and even comparable\nto gradient-based methods. We demonstrate that ES is also well suited at\noptimizing the placement of shapes to fit the CLIP model, and can produce\ndiverse, distinct geometric abstractions that are aligned with human\ninterpretation of language. Videos and demo: https://es-clip.github.io/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingtao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_D/0/1/0/all/0/1\">David Ha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DAAS: Differentiable Architecture and Augmentation Policy Search. (arXiv:2109.15273v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2109.15273","description":"<p>Neural architecture search (NAS) has been an active direction of automatic\nmachine learning (Auto-ML), aiming to explore efficient network structures. The\nsearched architecture is evaluated by training on datasets with fixed data\naugmentation policies. However, recent works on auto-augmentation show that the\nsuited augmentation policies can vary over different structures. Therefore,\nthis work considers the possible coupling between neural architectures and data\naugmentation and proposes an effective algorithm jointly searching for them.\nSpecifically, 1) for the NAS task, we adopt a single-path based differentiable\nmethod with Gumbel-softmax reparameterization strategy due to its memory\nefficiency; 2) for the auto-augmentation task, we introduce a novel search\nmethod based on policy gradient algorithm, which can significantly reduce the\ncomputation complexity. Our approach achieves 97.91% accuracy on CIFAR-10 and\n76.6% Top-1 accuracy on ImageNet dataset, showing the outstanding performance\nof our search algorithm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoxing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiangxiang Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Junchi Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaokang Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Thing to Fool them All: Generating Interpretable, Universal, and Physically-Realizable Adversarial Features. (arXiv:2110.03605v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2110.03605","description":"<p>It is well understood that modern deep networks are vulnerable to adversarial\nattacks. However, conventional attack methods fail to produce adversarial\nperturbations that are intelligible to humans, and they pose limited threats in\nthe physical world. To study feature-class associations in networks and better\nunderstand their vulnerability to attacks in the real world, we develop\nfeature-level adversarial perturbations using deep image generators and a novel\noptimization objective. We term these feature-fool attacks. We show that they\nare versatile and use them to generate targeted feature-level attacks at the\nImageNet scale that are simultaneously interpretable, universal to any source\nimage, and physically-realizable. These attacks reveal spurious,\nsemantically-describable feature/class associations that can be exploited by\nnovel combinations of objects. We use them to guide the design of \"copy/paste\"\nadversaries in which one natural image is pasted into another to cause a\ntargeted misclassification. Code is available at\nhttps://github.com/thestephencasper/feature_fool.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1\">Stephen Casper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeau_M/0/1/0/all/0/1\">Max Nadeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1\">Gabriel Kreiman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Roadmap on Signal Processing for Next Generation Measurement Systems. (arXiv:2111.02493v3 [eess.SP] UPDATED)","link":"http://arxiv.org/abs/2111.02493","description":"<p>Signal processing is a fundamental component of almost any sensor-enabled\nsystem, with a wide range of applications across different scientific\ndisciplines. Time series data, images, and video sequences comprise\nrepresentative forms of signals that can be enhanced and analysed for\ninformation extraction and quantification. The recent advances in artificial\nintelligence and machine learning are shifting the research attention towards\nintelligent, data-driven, signal processing. This roadmap presents a critical\noverview of the state-of-the-art methods and applications aiming to highlight\nfuture challenges and research opportunities towards next generation\nmeasurement systems. It covers a broad spectrum of topics ranging from basic to\nindustrial research, organized in concise thematic sections that reflect the\ntrends and the impacts of current and future developments per research field.\nFurthermore, it offers guidance to researchers and funding agencies in\nidentifying new prospects.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Iakovidis_D/0/1/0/all/0/1\">D.K. Iakovidis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ooi_M/0/1/0/all/0/1\">M. Ooi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuang_Y/0/1/0/all/0/1\">Y.C. Kuang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Demidenko_S/0/1/0/all/0/1\">S. Demidenko</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shestakov_A/0/1/0/all/0/1\">A. Shestakov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sinitsin_V/0/1/0/all/0/1\">V. Sinitsin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Henry_M/0/1/0/all/0/1\">M. Henry</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sciacchitano_A/0/1/0/all/0/1\">A. Sciacchitano</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Discetti_A/0/1/0/all/0/1\">A. Discetti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Donati_S/0/1/0/all/0/1\">S. Donati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norgia_M/0/1/0/all/0/1\">M. Norgia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Menychtas_A/0/1/0/all/0/1\">A. Menychtas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maglogiannis_I/0/1/0/all/0/1\">I. Maglogiannis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wriessnegger_S/0/1/0/all/0/1\">S.C. Wriessnegger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chacon_L/0/1/0/all/0/1\">L.A. Barradas Chacon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimas_G/0/1/0/all/0/1\">G. Dimas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Filos_D/0/1/0/all/0/1\">D. Filos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aletras_A/0/1/0/all/0/1\">A.H. Aletras</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toger_J/0/1/0/all/0/1\">J. T&#xf6;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_F/0/1/0/all/0/1\">F. Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_S/0/1/0/all/0/1\">S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uhl_A/0/1/0/all/0/1\">A. Uhl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paziewski_J/0/1/0/all/0/1\">J. Paziewski</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geng_J/0/1/0/all/0/1\">J. Geng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fioranelli_F/0/1/0/all/0/1\">F. Fioranelli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Narayanan_R/0/1/0/all/0/1\">R.M. Narayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fernandez_C/0/1/0/all/0/1\">C. Fernandez</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stiller_C/0/1/0/all/0/1\">C. Stiller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malamousi_K/0/1/0/all/0/1\">K. Malamousi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kamnis_S/0/1/0/all/0/1\">S. Kamnis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delibasis_K/0/1/0/all/0/1\">K. Delibasis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">D. Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">J. Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_R/0/1/0/all/0/1\">R.X. Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeltaConv: Anisotropic Geometric Deep Learning with Exterior Calculus. (arXiv:2111.08799v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2111.08799","description":"<p>Learning from 3D point-cloud data has rapidly gained momentum, motivated by\nthe success of deep learning on images and the increased availability of 3D\ndata. In this paper, we aim to construct anisotropic convolutions that work\ndirectly on the surface derived from a point cloud. This is challenging because\nof the lack of a global coordinate system for tangential directions on\nsurfaces. We introduce a new convolution operator called DeltaConv, which\ncombines geometric operators from exterior calculus to enable the construction\nof anisotropic filters on point clouds. Because these operators are defined on\nscalar- and vector-fields, we separate the network into a scalar- and a\nvector-stream, which are connected by the operators. The vector stream enables\nthe network to explicitly represent, evaluate, and process directional\ninformation. Our convolutions are robust and simple to implement and show\nimproved accuracy compared to state-of-the-art approaches on several\nbenchmarks, while also speeding up training and inference.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiersma_R/0/1/0/all/0/1\">Ruben Wiersma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasikun_A/0/1/0/all/0/1\">Ahmad Nasikun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisemann_E/0/1/0/all/0/1\">Elmar Eisemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hildebrandt_K/0/1/0/all/0/1\">Klaus Hildebrandt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep Neural Networks for Rank-Consistent Ordinal Regression Based On Conditional Probabilities. (arXiv:2111.08851v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2111.08851","description":"<p>In recent times, deep neural networks achieved outstanding predictive\nperformance on various classification and pattern recognition tasks. However,\nmany real-world prediction problems have ordinal response variables, and this\nordering information is ignored by conventional classification losses such as\nthe multi-category cross-entropy. Ordinal regression methods for deep neural\nnetworks address this. One such method is the CORAL method, which is based on\nan earlier binary label extension framework and achieves rank consistency among\nits output layer tasks by imposing a weight-sharing constraint. However, while\nearlier experiments showed that CORAL's rank consistency is beneficial for\nperformance, the weight-sharing constraint could severely restrict the\nexpressiveness of a deep neural network. In this paper, we propose an\nalternative method for rank-consistent ordinal regression that does not require\na weight-sharing constraint in a neural network's fully connected output layer.\nWe achieve this rank consistency by a novel training scheme using conditional\ntraining sets to obtain the unconditional rank probabilities through applying\nthe chain rule for conditional probability distributions. Experiments on\nvarious datasets demonstrate the efficacy of the proposed method to utilize the\nordinal target information, and the absence of the weight-sharing restriction\nimproves the performance substantially compared to the CORAL reference\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1\">Xintong Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1\">Wenzhi Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raschka_S/0/1/0/all/0/1\">Sebastian Raschka</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hamiltonian Operator Disentanglement of Content and Motion in Image Sequences. (arXiv:2112.01641v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01641","description":"<p>We introduce a deep generative model for image sequences that reliably\nfactorise the latent space into content and motion variables. To model the\ndiverse dynamics, we split the motion space into subspaces and introduce a\nunique Hamiltonian operator for each subspace. The Hamiltonian formulation\nprovides reversible dynamics that constrain the evolution of the motion path\nalong the low-dimensional manifold and conserves learnt invariant properties.\nThe explicit split of the motion space decomposes the Hamiltonian into symmetry\ngroups and gives long-term separability of the dynamics. This split also means\nwe can learn content representations that are easy to interpret and control. We\ndemonstrate the utility of our model by swapping the motion of two videos,\ngenerating long term sequences of various actions from a given image,\nunconditional sequence generation and image rotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1\">Asif Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storkey_A/0/1/0/all/0/1\">Amos Storkey</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Action Units That Constitute Trainable Micro-expressions (and A Large-scale Synthetic Dataset). (arXiv:2112.01730v4 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2112.01730","description":"<p>Because of the expensive data collection process, micro-expression (MiE)\ndatasets are generally much smaller in scale than those in other computer\nvision fields, rendering large-scale training less feasible. This paper\ndevelops a protocol to automatically synthesize MiE training data that 1) are\nof a large scale and 2) allow us to train accurate recognition models for\nreal-world test data. Specifically, we discover three types of Action Units\n(AUs) that can constitute trainable MiEs. These AUs come from real-world MiEs,\nearly frames of macro-expression videos, and the relationship between AUs and\nexpression categories defined by human expert knowledge. With these AUs, our\nprotocol then employs large numbers of face images of various identities and an\noff-the-shelf face generator for MiE synthesis, yielding the MiE-X dataset. MiE\nrecognition models are trained or pre-trained on MiE-X and evaluated on\nreal-world test sets, where competitive accuracy is obtained. Experimental\nresults not only validate the effectiveness of these AUs and our MiE-X dataset\nbut also reveal some critical properties of MiEs: they generalize across faces,\nare close to early-stage macro-expressions, and can be manually defined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Curse of Zero Task Diversity: On the Failure of Transfer Learning to Outperform MAML and their Empirical Equivalence. (arXiv:2112.13121v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2112.13121","description":"<p>Recently, it has been observed that a transfer learning solution might be all\nwe need to solve many few-shot learning benchmarks -- thus raising important\nquestions about when and how meta-learning algorithms should be deployed. In\nthis paper, we seek to clarify these questions by proposing a novel metric --\nthe diversity coefficient -- to measure the diversity of tasks in a few-shot\nlearning benchmark. We hypothesize that the diversity coefficient of the\nfew-shot learning benchmark is predictive of whether meta-learning solutions\nwill succeed or not. Using the diversity coefficient, we show that the\nMiniImagenet benchmark has zero diversity. This novel insight contextualizes\nclaims that transfer learning solutions are better than meta-learned solutions.\nSpecifically, we empirically find that a diversity coefficient of zero\ncorrelates with a high similarity between transfer learning and Model-Agnostic\nMeta-Learning (MAML) learned solutions in terms of meta-accuracy (at meta-test\ntime). Therefore, we conjecture meta-learned solutions have the same meta-test\nperformance as transfer learning when the diversity coefficient is zero. Our\nwork provides the first test of whether diversity correlates with meta-learning\nsuccess.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miranda_B/0/1/0/all/0/1\">Brando Miranda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Xiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1\">Sanmi Koyejo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block Walsh-Hadamard Transform Based Binary Layers in Deep Neural Networks. (arXiv:2201.02711v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2201.02711","description":"<p>Convolution has been the core operation of modern deep neural networks. It is\nwell-known that convolutions can be implemented in the Fourier Transform\ndomain. In this paper, we propose to use binary block Walsh-Hadamard transform\n(WHT) instead of the Fourier transform. We use WHT-based binary layers to\nreplace some of the regular convolution layers in deep neural networks. We\nutilize both one-dimensional (1-D) and two-dimensional (2-D) binary WHTs in\nthis paper. In both 1-D and 2-D layers, we compute the binary WHT of the input\nfeature map and denoise the WHT domain coefficients using a nonlinearity which\nis obtained by combining soft-thresholding with the tanh function. After\ndenoising, we compute the inverse WHT. We use 1D-WHT to replace the $1\\times 1$\nconvolutional layers, and 2D-WHT layers can replace the 3$\\times$3 convolution\nlayers and Squeeze-and-Excite layers. 2D-WHT layers with trainable weights can\nbe also inserted before the Global Average Pooling (GAP) layers to assist the\ndense layers. In this way, we can reduce the number of trainable parameters\nsignificantly with a slight decrease in trainable parameters. In this paper, we\nimplement the WHT layers into MobileNet-V2, MobileNet-V3-Large, and ResNet to\nreduce the number of parameters significantly with negligible accuracy loss.\nMoreover, according to our speed test, the 2D-FWHT layer runs about 24 times as\nfast as the regular $3\\times 3$ convolution with 19.51\\% less RAM usage in an\nNVIDIA Jetson Nano experiment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badawi_D/0/1/0/all/0/1\">Diaa Badawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1\">Ahmet Enis Cetin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Sneak Attack on Segmentation of Medical Images Using Deep Neural Network Classifiers. (arXiv:2201.02771v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.02771","description":"<p>Instead of using current deep-learning segmentation models (like the UNet and\nvariants), we approach the segmentation problem using trained Convolutional\nNeural Network (CNN) classifiers, which automatically extract important\nfeatures from images for classification. Those extracted features can be\nvisualized and formed into heatmaps using Gradient-weighted Class Activation\nMapping (Grad-CAM). This study tested whether the heatmaps could be used to\nsegment the classified targets. We also proposed an evaluation method for the\nheatmaps; that is, to re-train the CNN classifier using images filtered by\nheatmaps and examine its performance. We used the mean-Dice coefficient to\nevaluate segmentation results. Results from our experiments show that heatmaps\ncan locate and segment partial tumor areas. But use of only the heatmaps from\nCNN classifiers may not be an optimal approach for segmentation. We have\nverified that the predictions of CNN classifiers mainly depend on tumor areas,\nand dark regions in Grad-CAM's heatmaps also contribute to classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guan_S/0/1/0/all/0/1\">Shuyue Guan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Loew_M/0/1/0/all/0/1\">Murray Loew</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DCNGAN: A Deformable Convolutional-Based GAN with QP Adaptation for Perceptual Quality Enhancement of Compressed Video. (arXiv:2201.08944v3 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.08944","description":"<p>In this paper, we propose a deformable convolution-based generative\nadversarial network (DCNGAN) for perceptual quality enhancement of compressed\nvideos. DCNGAN is also adaptive to the quantization parameters (QPs). Compared\nwith optical flows, deformable convolutions are more effective and efficient to\nalign frames. Deformable convolutions can operate on multiple frames, thus\nleveraging more temporal information, which is beneficial for enhancing the\nperceptual quality of compressed videos. Instead of aligning frames in a\npairwise manner, the deformable convolution can process multiple frames\nsimultaneously, which leads to lower computational complexity. Experimental\nresults demonstrate that the proposed DCNGAN outperforms other state-of-the-art\ncompressed video quality enhancement algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1\">Saiping Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Herranz_L/0/1/0/all/0/1\">Luis Herranz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mrak_M/0/1/0/all/0/1\">Marta Mrak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Blanch_M/0/1/0/all/0/1\">Marc Gorriz Blanch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wan_S/0/1/0/all/0/1\">Shuai Wan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_F/0/1/0/all/0/1\">Fuzheng Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReconFormer: Accelerated MRI Reconstruction Using Recurrent Transformer. (arXiv:2201.09376v2 [eess.IV] UPDATED)","link":"http://arxiv.org/abs/2201.09376","description":"<p>Accelerating magnetic resonance image (MRI) reconstruction process is a\nchallenging ill-posed inverse problem due to the excessive under-sampling\noperation in k-space. In this paper, we propose a recurrent transformer model,\nnamely ReconFormer, for MRI reconstruction which can iteratively reconstruct\nhigh fertility magnetic resonance images from highly under-sampled k-space\ndata. In particular, the proposed architecture is built upon Recurrent Pyramid\nTransformer Layers (RPTL), which jointly exploits intrinsic multi-scale\ninformation at every architecture unit as well as the dependencies of the deep\nfeature correlation through recurrent states. Moreover, the proposed\nReconFormer is lightweight since it employs the recurrent structure for its\nparameter efficiency. We validate the effectiveness of ReconFormer on multiple\ndatasets with different magnetic resonance sequences and show that it achieves\nsignificant improvements over the state-of-the-art methods with better\nparameter efficiency. Implementation code will be available in\nhttps://github.com/guopengf/ReconFormer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1\">Pengfei Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mei_Y/0/1/0/all/0/1\">Yiqun Mei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1\">Jinyuan Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_S/0/1/0/all/0/1\">Shanshan Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patel_V/0/1/0/all/0/1\">Vishal M. Patel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Comparison of Evaluation Metrics for Landmark Detection in CMR Images. (arXiv:2201.10410v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10410","description":"<p>Cardiac Magnetic Resonance (CMR) images are widely used for cardiac diagnosis\nand ventricular assessment. Extracting specific landmarks like the right\nventricular insertion points is of importance for spatial alignment and 3D\nmodeling. The automatic detection of such landmarks has been tackled by\nmultiple groups using Deep Learning, but relatively little attention has been\npaid to the failure cases of evaluation metrics in this field. In this work, we\nextended the public ACDC dataset with additional labels of the right\nventricular insertion points and compare different variants of a heatmap-based\nlandmark detection pipeline. In this comparison, we demonstrate very likely\npitfalls of apparently simple detection and localisation metrics which\nhighlights the importance of a clear detection strategy and the definition of\nan upper limit for localisation-based metrics. Our preliminary results indicate\nthat a combination of different metrics is necessary, as they yield different\nwinners for method comparison. Additionally, they highlight the need of a\ncomprehensive metric description and evaluation standardisation, especially for\nthe error cases where no metrics could be computed or where no lower/upper\nboundary of a metric exists. Code and labels:\nhttps://github.com/Cardio-AI/rvip_landmark_detection\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koehler_S/0/1/0/all/0/1\">Sven Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharan_L/0/1/0/all/0/1\">Lalith Sharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhm_J/0/1/0/all/0/1\">Julian Kuhm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanaat_A/0/1/0/all/0/1\">Arman Ghanaat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordejeva_J/0/1/0/all/0/1\">Jelizaveta Gordejeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_N/0/1/0/all/0/1\">Nike K. Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grell_N/0/1/0/all/0/1\">Niko M. Grell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andre_F/0/1/0/all/0/1\">Florian Andr&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelhardt_S/0/1/0/all/0/1\">Sandy Engelhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Class-Aware Generative Adversarial Transformers for Medical Image Segmentation. (arXiv:2201.10737v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.10737","description":"<p>Transformers have made remarkable progress towards modeling long-range\ndependencies within the medical image analysis domain. However, current\ntransformer-based models suffer from several disadvantages: (1) existing\nmethods fail to capture the important features of the images due to the naive\ntokenization scheme; (2) the models suffer from information loss because they\nonly consider single-scale feature representations; and (3) the segmentation\nlabel maps generated by the models are not accurate enough without considering\nrich semantic contexts and anatomical textures. In this work, we present\nCA-GANformer, a novel type of generative adversarial transformers, for medical\nimage segmentation. First, we take advantage of the pyramid structure to\nconstruct multi-scale representations and handle multi-scale variations. We\nthen design a novel class-aware transformer module to better learn the\ndiscriminative regions of objects with semantic structures. Lastly, we utilize\nan adversarial training strategy that boosts segmentation accuracy and\ncorrespondingly allows a transformer-based discriminator to capture high-level\nsemantically correlated contents and low-level anatomical features. Our\nexperiments demonstrate that CA-GANformer dramatically outperforms previous\nstate-of-the-art transformer-based approaches on three benchmarks, obtaining\n2.54%-5.88% absolute improvements in Dice over previous models. Further\nqualitative experiments provide a more detailed picture of the model's inner\nworkings, shed light on the challenges in improved transparency, and\ndemonstrate that transfer learning can greatly improve performance and reduce\nthe size of medical image datasets in training, making CA-GANformer a strong\nstarting point for downstream medical image analysis tasks. Codes and models\nwill be available to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1\">Ruihan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinchali_S/0/1/0/all/0/1\">Sandeep Chinchali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staib_L/0/1/0/all/0/1\">Lawrence Staib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duncan_J/0/1/0/all/0/1\">James S. Duncan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Checklist: Towards Testable Error Analysis of Image Models to Help System Designers Interrogate Model Capabilities. (arXiv:2201.11674v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2201.11674","description":"<p>Using large pre-trained models for image recognition tasks is becoming\nincreasingly common owing to the well acknowledged success of recent models\nlike vision transformers and other CNN-based models like VGG and Resnet. The\nhigh accuracy of these models on benchmark tasks has translated into their\npractical use across many domains including safety-critical applications like\nautonomous driving and medical diagnostics. Despite their widespread use, image\nmodels have been shown to be fragile to changes in the operating environment,\nbringing their robustness into question. There is an urgent need for methods\nthat systematically characterise and quantify the capabilities of these models\nto help designers understand and provide guarantees about their safety and\nrobustness. In this paper, we propose Vision Checklist, a framework aimed at\ninterrogating the capabilities of a model in order to produce a report that can\nbe used by a system designer for robustness evaluations. This framework\nproposes a set of perturbation operations that can be applied on the underlying\ndata to generate test samples of different types. The perturbations reflect\npotential changes in operating environments, and interrogate various properties\nranging from the strictly quantitative to more qualitative. Our framework is\nevaluated on multiple datasets like Tinyimagenet, CIFAR10, CIFAR100 and\nCamelyon17 and for models like ViT and Resnet. Our Vision Checklist proposes a\nspecific set of evaluations that can be integrated into the previously proposed\nconcept of a model card. Robustness evaluations like our checklist will be\ncrucial in future safety evaluations of visual perception modules, and be\nuseful for a wide range of stakeholders including designers, deployers, and\nregulators involved in the certification of these systems. Source code of\nVision Checklist would be open for public use.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xin Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Legastelois_B/0/1/0/all/0/1\">Benedicte Legastelois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_B/0/1/0/all/0/1\">Bhargavi Ganesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1\">Ajitha Rajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chockler_H/0/1/0/all/0/1\">Hana Chockler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belle_V/0/1/0/all/0/1\">Vaishak Belle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_S/0/1/0/all/0/1\">Stuart Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1\">Subramanian Ramamoorthy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2022-01-30T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computer Vision and Pattern Recognition"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}